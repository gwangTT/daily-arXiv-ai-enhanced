<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 101]
- [cs.AR](#cs.AR) [Total: 11]
- [cs.CL](#cs.CL) [Total: 184]
- [cs.CV](#cs.CV) [Total: 266]
- [cs.DC](#cs.DC) [Total: 36]
- [cs.LG](#cs.LG) [Total: 185]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PL](#cs.PL) [Total: 10]
- [cs.RO](#cs.RO) [Total: 62]
- [cs.SE](#cs.SE) [Total: 59]
- [q-bio.NC](#q-bio.NC) [Total: 12]
- [stat.ML](#stat.ML) [Total: 12]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.ET](#cs.ET) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.GT](#cs.GT) [Total: 4]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 4]
- [cs.CR](#cs.CR) [Total: 30]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DL](#cs.DL) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [stat.AP](#stat.AP) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 3]
- [cs.MA](#cs.MA) [Total: 4]
- [econ.GN](#econ.GN) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 2]
- [cs.DS](#cs.DS) [Total: 3]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [econ.EM](#econ.EM) [Total: 2]
- [eess.AS](#eess.AS) [Total: 12]
- [cs.OS](#cs.OS) [Total: 3]
- [astro-ph.SR](#astro-ph.SR) [Total: 2]
- [cs.IT](#cs.IT) [Total: 3]
- [cs.MM](#cs.MM) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [eess.IV](#eess.IV) [Total: 11]
- [math.NA](#math.NA) [Total: 5]
- [eess.SP](#eess.SP) [Total: 5]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CY](#cs.CY) [Total: 9]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 11]
- [cs.HC](#cs.HC) [Total: 31]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.SC](#cs.SC) [Total: 1]
- [cs.SI](#cs.SI) [Total: 3]
- [math.OC](#math.OC) [Total: 2]
- [cs.CG](#cs.CG) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MIMIC-RD: Can LLMs differentially diagnose rare diseases in real-world clinical settings?](https://arxiv.org/abs/2601.11559)
*Zilal Eiz AlDin,John Wu,Jeffrey Paul Fung,Jennifer King,Mya Watts,Lauren ONeill,Adam Richard Cross,Jimeng Sun*

Main category: cs.AI

TL;DR: This paper evaluates the capacity of large language models (LLMs) for diagnosing rare diseases using real-world clinical data from a new benchmark, MIMIC-RD, and finds current LLMs inadequate.


<details>
  <summary>Details</summary>
Motivation: Rare diseases affect 1 in 10 Americans, but their diagnosis is extremely challenging due to the rarity and complexity of cases. Existing evaluation methods for LLMs in rare disease diagnosis either oversimplify real-world cases or rely on incomplete data sources.

Method: The authors developed a benchmark dataset, MIMIC-RD, by mapping clinical text entities from the MIMIC database to Orphanet using an LLM-based mining approach validated by medical annotators. They evaluated various state-of-the-art LLMs on their dataset.

Result: State-of-the-art large language models showed poor performance on the MIMIC-RD dataset, indicating a significant gap between their diagnostic capabilities and real-world clinical requirements.

Conclusion: The study reveals the inadequacy of current LLMs for rare disease diagnosis and offers future directions for improving differential diagnosis methods for rare diseases.

Abstract: Despite rare diseases affecting 1 in 10 Americans, their differential diagnosis remains challenging. Due to their impressive recall abilities, large language models (LLMs) have been recently explored for differential diagnosis. Existing approaches to evaluating LLM-based rare disease diagnosis suffer from two critical limitations: they rely on idealized clinical case studies that fail to capture real-world clinical complexity, or they use ICD codes as disease labels, which significantly undercounts rare diseases since many lack direct mappings to comprehensive rare disease databases like Orphanet. To address these limitations, we explore MIMIC-RD, a rare disease differential diagnosis benchmark constructed by directly mapping clinical text entities to Orphanet. Our methodology involved an initial LLM-based mining process followed by validation from four medical annotators to confirm identified entities were genuine rare diseases. We evaluated various models on our dataset of 145 patients and found that current state-of-the-art LLMs perform poorly on rare disease differential diagnosis, highlighting the substantial gap between existing capabilities and clinical needs. From our findings, we outline several future steps towards improving differential diagnosis of rare diseases.

</details>


### [2] [A Mind Cannot Be Smeared Across Time](https://arxiv.org/abs/2601.11620)
*Michael Timothy Bennett*

Main category: cs.AI

TL;DR: The paper argues that consciousness in machines depends on temporal dynamics and hardware architecture, proposing that consciousness requires simultaneous contributions and suggesting that software alone cannot achieve it on sequential systems.


<details>
  <summary>Details</summary>
Motivation: The paper explores the connection between consciousness and the temporal aspects of computation, challenging the assumption that functional equivalence alone is sufficient for attributing consciousness to machines.

Method: It augments Stack Theory with algebraic laws, introduces temporal semantics, formulates new constraints for conscious experiences (StrongSync and WeakSync), and validates these concepts against neurophysiological evidence.

Result: The results demonstrate that sequential systems lack the concurrency capacity needed for StrongSync, implying that hardware capable of simultaneous processing is crucial for consciousness.

Conclusion: Consciousness is dependent on the architecture and concurrency capacity of hardware, necessitating both architectural inspections and consideration of temporal dynamics for consciousness attributions.

Abstract: Whether machines can be conscious depends not only on what they compute, but \emph{when} they compute it. Most deployed artificial systems realise their functions via sequential or time-multiplexed updates. Conscious experience appears unified and simultaneous. I show that this difference matters formally. I augment Stack Theory with algebraic laws relating within time-window constraint satisfaction to conjunction. I introduce a precise temporal semantics over windowed trajectories $τ^{Δ,s}$ and prove that existential temporal realisation $\Diamond_Δ$ does not preserve conjunction. A system can realise all the ingredients of experience across time without ever instantiating the experienced conjunction itself. I then distinguish two postulates. StrongSync requires objective co-instantiation of the grounded conjunction within the window, while WeakSync permits temporal ``smearing''. I formalise concurrency-capacity to measure what is needed to satisfy StrongSync. Finally, I review neurophysiological evidence suggesting that consciousness depends on phase synchrony and effective connectivity, and that loss of consciousness is often associated with its breakdown. This evidence makes WeakSync less plausible. Under StrongSync, software consciousness on strictly sequential substrates is impossible for contents whose grounding requires two or more simultaneous contributors. The more parts from which simultaneous contribution required, the more concurrency capacity is required. The hardware matters. Consciousness attribution therefore requires architectural inspection, not just functional performance.

</details>


### [3] [Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models](https://arxiv.org/abs/2601.11622)
*Hassan Ugail,Newton Howard*

Main category: cs.AI

TL;DR: This study introduces a neuroscience-inspired dynamical metric to analyze the temporal dynamics of large language models like GPT-2 during text generation; finding functional differences across various conditions.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the gap in understanding the temporal organization of high-dimensional internal dynamics in large language models, a topic that remains largely unexplored by traditional interpretability approaches.

Method: By adapting temporal integration and metastability concepts from neuroscience, the authors propose a composite dynamical metric derived from activation time-series of a GPT-2-medium. This metric is applied across five experimental conditions, including structured reasoning and noisy sampling.

Result: The structured reasoning condition exhibited significantly elevated dynamical metrics in comparison to other conditions like forced repetition and noisy sampling, which was validated through statistical analysis such as one-way ANOVA and effect size measurements.

Conclusion: Neuroscience-inspired dynamical metrics can effectively characterize variations in computational organization in large language models, though these metrics focus purely on formal dynamical properties without implying subjective experiences.

Abstract: Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventions, leaving temporal structure largely unexplored. Drawing on neuroscience, where temporal integration and metastability are core markers of neural organisation, we adapt these concepts to transformer models and discuss a composite dynamical metric, computed from activation time-series during autoregressive generation. We evaluate this metric in GPT-2-medium across five conditions: structured reasoning, forced repetition, high-temperature noisy sampling, attention-head pruning, and weight-noise injection. Structured reasoning consistently exhibits elevated metric relative to repetitive, noisy, and perturbed regimes, with statistically significant differences confirmed by one-way ANOVA and large effect sizes in key comparisons. These results are robust to layer selection, channel subsampling, and random seeds. Our findings demonstrate that neuroscience-inspired dynamical metrics can reliably characterise differences in computational organisation across functional regimes in large language models. We stress that the proposed metric captures formal dynamical properties and does not imply subjective experience.

</details>


### [4] [Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance](https://arxiv.org/abs/2601.11625)
*Sahil Rajesh Dhayalkar*

Main category: cs.AI

TL;DR: The study introduces explanation drift to track token-level attribution changes during fine-tuning of pretrained language models and proposes the Reasoning Stabilization Point (RSP) as a measure for stable evidence reliance.


<details>
  <summary>Details</summary>
Motivation: To monitor and evaluate how decision evidence (token-level attributions) shifts during fine-tuning of language models without relying on out-of-distribution data.

Method: Tracking token-level explanation drift during fine-tuning epochs and defining the Reasoning Stabilization Point (RSP), which marks the epoch where attribution changes stabilize consistently.

Result: The researchers found that attribution dynamics stabilize into a low-drift regime early in training, while validation accuracy changes only slightly. Explanation drift also revealed models' increasing reliance on shortcuts like trigger tokens.

Conclusion: Explanation drift serves as an effective, low-cost diagnostic tool to monitor evidence evolution during fine-tuning and to identify stable checkpoints for robust task performance.

Abstract: Fine-tuning pretrained language models can improve task performance while subtly altering the evidence a model relies on. We propose a training-time interpretability view that tracks token-level attributions across finetuning epochs. We define explanation driftas the epoch-to-epoch change in normalized token attributions on a fixed probe set, and introduce the Reasoning Stabilization Point(RSP), the earliest epoch after which drift remains consistently low. RSP is computed from within-run drift dynamics and requires no tuning on out-of-distribution data. Across multiple lightweight transformer classifiers and benchmark classification tasks, drift typically collapses into a low, stable regime early in training, while validation accuracy continues to change only marginally. In a controlled shortcut setting with label-correlated trigger tokens, attribution dynamics expose increasing reliance on the shortcut even when validation accuracy remains competitive. Overall, explanation drift provides a simple, low-cost diagnostic for monitoring how decision evidence evolves during fine-tuning and for selecting checkpoints in a stable-evidence regime.

</details>


### [5] [PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement](https://arxiv.org/abs/2601.11747)
*Huaxiaoyue Wang,Sunav Choudhary,Franck Dernoncourt,Yu Shen,Stefano Petrangeli*

Main category: cs.AI

TL;DR: The paper presents PRISM, a tool for stylistic graphic design improvement using real-world data and natural language instructions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of enhancing graphic designs stylistically based on natural language instructions, overcoming limitations of pretrained models in aligning with specific design domains.

Method: PRISM leverages real-world design data by clustering diverse designs, summarizing clusters into actionable knowledge, and retrieving these insights in real-time for style improvement.

Result: PRISM demonstrated top performance in style alignment tasks on the Crello dataset, with user studies confirming its effectiveness and designer preference.

Conclusion: PRISM successfully integrates designer principles into graphic design improvement, enhancing style alignment and user satisfaction.

Abstract: Graphic design often involves exploring different stylistic directions, which can be time-consuming for non-experts. We address this problem of stylistically improving designs based on natural language instructions. While VLMs have shown initial success in graphic design, their pretrained knowledge on styles is often too general and misaligned with specific domain data. For example, VLMs may associate minimalism with abstract designs, whereas designers emphasize shape and color choices. Our key insight is to leverage design data -- a collection of real-world designs that implicitly capture designer's principles -- to learn design knowledge and guide stylistic improvement. We propose PRISM (PRior-Informed Stylistic Modification) that constructs and applies a design knowledge base through three stages: (1) clustering high-variance designs to capture diversity within a style, (2) summarizing each cluster into actionable design knowledge, and (3) retrieving relevant knowledge during inference to enable style-aware improvement. Experiments on the Crello dataset show that PRISM achieves the highest average rank of 1.49 (closer to 1 is better) over baselines in style alignment. User studies further validate these results, showing that PRISM is consistently preferred by designers.

</details>


### [6] [Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles](https://arxiv.org/abs/2601.11781)
*Dawood Wasif,Terrence J. Moore,Seunghyun Yoon,Hyuk Lim,Dan Dongseong Kim,Frederica F. Nelson,Jin-Hee Cho*

Main category: cs.AI

TL;DR: This paper proposes RAIL, a framework for safe autonomous driving during rare or compromised scenarios. It demonstrates superior performance metrics compared to existing approaches.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of autonomous vehicles encountering rare long-tail scenarios or external cyber-physical threats.

Method: RAIL integrates runtime signals with a human-in-the-loop approach, using an Intrusion Risk Score (IRS) to calibrate actions and improve safety. It employs a mix of behavioral shields, contextual bandits, and reinforcement learning with risk-aware learning techniques.

Result: RAIL outperforms existing models in challenging driving environments, enhancing safety, reducing risks from external attacks, and achieving higher success rates across multiple benchmarks.

Conclusion: The proposed RAIL framework significantly improves autonomous vehicle safety and effectiveness in uncontrolled environments, making it a viable solution for long-tail and intrusion-prone scenarios.

Abstract: Autonomous vehicles must remain safe and effective when encountering rare long-tailed scenarios or cyber-physical intrusions during driving. We present RAIL, a risk-aware human-in-the-loop framework that turns heterogeneous runtime signals into calibrated control adaptations and focused learning. RAIL fuses three cues (curvature actuation integrity, time-to-collision proximity, and observation-shift consistency) into an Intrusion Risk Score (IRS) via a weighted Noisy-OR. When IRS exceeds a threshold, actions are blended with a cue-specific shield using a learned authority, while human override remains available; when risk is low, the nominal policy executes. A contextual bandit arbitrates among shields based on the cue vector, improving mitigation choices online. RAIL couples Soft Actor-Critic (SAC) with risk-prioritized replay and dual rewards so that takeovers and near misses steer learning while nominal behavior remains covered. On MetaDrive, RAIL achieves a Test Return (TR) of 360.65, a Test Success Rate (TSR) of 0.85, a Test Safety Violation (TSV) of 0.75, and a Disturbance Rate (DR) of 0.0027, while logging only 29.07 training safety violations, outperforming RL, safe RL, offline/imitation learning, and prior HITL baselines. Under Controller Area Network (CAN) injection and LiDAR spoofing attacks, it improves Success Rate (SR) to 0.68 and 0.80, lowers the Disengagement Rate under Attack (DRA) to 0.37 and 0.03, and reduces the Attack Success Rate (ASR) to 0.34 and 0.11. In CARLA, RAIL attains a TR of 1609.70 and TSR of 0.41 with only 8000 steps.

</details>


### [7] [A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation](https://arxiv.org/abs/2601.11792)
*Yifei Sun,Yongan Li,A. K. Qin,Sicheng Hou,Tamas Pflanzner*

Main category: cs.AI

TL;DR: This paper presents an innovative approach to mathematical problem generation (IMPG) using a self-evolving, multi-role collaborative framework and fine-grained difficulty guidance.


<details>
  <summary>Details</summary>
Motivation: Existing large language models for mathematical problem generation lack innovation and discrimination, despite achieving high correctness rates.

Method: The paper proposes a self-evolving, multi-role collaborative framework involving sampler, generator, evaluator, state machine, and memory, along with an improved difficulty model and the DAPS algorithm. A specialized HSM3K-CN dataset and multi-stage training pipeline (CPT, SFT, GRPO) enhance base model capabilities.

Result: Experiments demonstrate significant improvements in the innovation of generated problems without compromising correctness when compared to baseline models.

Conclusion: The proposed framework and methods effectively address the lack of innovation in LLM-generated mathematical problems, advancing both correctness and creative problem generation.

Abstract: Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. In this paper, we propose the task of innovative math problem generation (IMPG). To solve the IMPG task, this paper proposes a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance. First, a multi-role collaborative mechanism comprising a sampler, generator, evaluator, state machine, and memory is constructed, ensuring the correctness of generated problems through iterative optimization informed by self-assessment and external feedback. Second, we introduce an improved difficulty model to quantify difficulty and provide fine-grained guidance. We adopt the data-driven association-guided path sampling (DAPS) algorithm to enhance the semantic rationality of sampled encodings. Third, we construct the HSM3K-CN dataset, which comprises high-quality high school math problems. A multi-stage training pipeline is adopted, incorporating continual pre-training (CPT), supervised fine-tuning (SFT), and group relative policy optimization (GRPO), to enhance the generation and evaluation capabilities of the base model. Finally, system self-evolution is achieved by transferring evaluation capabilities from the expert model to the apprentice model via distillation. Experiments show that, compared to baseline models, our proposed method significantly improves the innovation of the generated problems while maintaining a high correctness rate.

</details>


### [8] [Actionable Interpretability Must Be Defined in Terms of Symmetries](https://arxiv.org/abs/2601.12913)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Francesco Giannini,Alberto Termine,Filippo Bonchi,Mateja Jamnik,Giuseppe Marra*

Main category: cs.AI

TL;DR: The paper critiques current AI interpretability definitions for their lack of actionable principles and proposes using symmetries as a framework to establish such principles.


<details>
  <summary>Details</summary>
Motivation: Addressing the inadequacy of current definitions of interpretability in AI, which fail to provide actionable formal principles.

Method: The authors introduce a symmetry-based approach to define interpretability that supports formal modeling and inferential rules.

Result: The paper identifies four symmetries that motivate interpretability principles, characterize interpretable models, and unify inference methods (alignment, interventions, counterfactuals) under Bayesian inversion.

Conclusion: Interpretability research in AI can be redefined and operationalized using symmetry principles to create actionable frameworks for modeling and inference.

Abstract: This paper argues that interpretability research in Artificial Intelligence is fundamentally ill-posed as existing definitions of interpretability are not *actionable*: they fail to provide formal principles from which concrete modelling and inferential rules can be derived. We posit that for a definition of interpretability to be actionable, it must be given in terms of *symmetries*. We hypothesise that four symmetries suffice to (i) motivate core interpretability properties, (ii) characterize the class of interpretable models, and (iii) derive a unified formulation of interpretable inference (e.g., alignment, interventions, and counterfactuals) as a form of Bayesian inversion.

</details>


### [9] [Multi-agent DRL-based Lane Change Decision Model for Cooperative Planning in Mixed Traffic](https://arxiv.org/abs/2601.11809)
*Zeyu Mu,Shangtong Zhang,B. Brian Park*

Main category: cs.AI

TL;DR: This paper proposes a hybrid multi-agent decision-making model integrated with CNN-QMIX to improve cooperative platooning among CAVs, even in mixed traffic scenarios.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the challenges in forming cooperative platoons of CAVs during the early deployment phase, where sparse distribution among human-driven vehicles limits their efficiency.

Method: The approach involves developing a hybrid multi-agent lane-change decision model using the QMIX framework and convolutional neural networks, supplemented by a trajectory planner and model predictive controller, evaluated in a microsimulation environment.

Result: The proposed model improves cooperative platooning rates by up to 26.2% and outperforms rule-based models in managing dynamic traffic scenarios with varying CAV penetration rates.

Conclusion: The model demonstrates effectiveness in enhancing CAV cooperation and traffic efficiency, providing a promising solution during the initial deployment of CAVs.

Abstract: Connected automated vehicles (CAVs) possess the ability to communicate and coordinate with one another, enabling cooperative platooning that enhances both energy efficiency and traffic flow. However, during the initial stage of CAV deployment, the sparse distribution of CAVs among human-driven vehicles reduces the likelihood of forming effective cooperative platoons. To address this challenge, this study proposes a hybrid multi-agent lane change decision model aimed at increasing CAV participation in cooperative platooning and maximizing its associated benefits. The proposed model employs the QMIX framework, integrating traffic data processed through a convolutional neural network (CNN-QMIX). This architecture addresses a critical issue in dynamic traffic scenarios by enabling CAVs to make optimal decisions irrespective of the varying number of CAVs present in mixed traffic. Additionally, a trajectory planner and a model predictive controller are designed to ensure smooth and safe lane-change execution. The proposed model is trained and evaluated within a microsimulation environment under varying CAV market penetration rates. The results demonstrate that the proposed model efficiently manages fluctuating traffic agent numbers, significantly outperforming the baseline rule-based models. Notably, it enhances cooperative platooning rates up to 26.2\%, showcasing its potential to optimize CAV cooperation and traffic dynamics during the early stage of deployment.

</details>


### [10] [POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation](https://arxiv.org/abs/2601.11816)
*Zahra Moslemi,Keerthi Koneru,Yen-Ting Lee,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: This paper introduces POLARIS, a framework for policy-aligned and auditable automation in enterprise back-office tasks, achieving strong results in document-centric finance tasks and anomaly handling.


<details>
  <summary>Details</summary>
Motivation: There is a need for more systematic, auditable, and policy-aligned agentic systems in enterprise workflows, as current multi-agent setups often fail to meet these requirements.

Method: POLARIS uses a structured plan synthesis and execution model, employing typed DAGs, reasoning modules for plan compliance, and a guarded execution framework with validators and policy guardrails.

Result: POLARIS demonstrated high performance in empirical evaluations, including a micro F1 score of 0.81 on the SROIE dataset and 0.95 to 1.00 precision for anomaly routing.

Conclusion: POLARIS sets a new benchmark for policy-aligned Agentic AI in enterprise automation, providing decision-grade artifacts and compliance-focused execution while minimizing human intervention.

Abstract: Enterprise back office workflows require agentic systems that are auditable, policy-aligned, and operationally predictable, capabilities that generic multi-agent setups often fail to deliver. We present POLARIS (Policy-Aware LLM Agentic Reasoning for Integrated Systems), a governed orchestration framework that treats automation as typed plan synthesis and validated execution over LLM agents. A planner proposes structurally diverse, type checked directed acyclic graphs (DAGs), a rubric guided reasoning module selects a single compliant plan, and execution is guarded by validator gated checks, a bounded repair loop, and compiled policy guardrails that block or route side effects before they occur. Applied to document centric finance tasks, POLARIS produces decision grade artifacts and full execution traces while reducing human intervention. Empirically, POLARIS achieves a micro F1 of 0.81 on the SROIE dataset and, on a controlled synthetic suite, achieves 0.95 to 1.00 precision for anomaly routing with preserved audit trails. These evaluations constitute an initial benchmark for governed Agentic AI. POLARIS provides a methodological and benchmark reference for policy-aligned Agentic AI. Keywords Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation

</details>


### [11] [AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept](https://arxiv.org/abs/2601.11825)
*Arya Rahgozar,Pouria Mortezaagha*

Main category: cs.AI

TL;DR: This paper proposes an AI-based, domain-agnostic knowledge synthesis framework to improve scalability, transparency, and efficiency in evidence synthesis while reducing research redundancy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of research waste in biomedical science, caused by redundant studies, incomplete reporting, and limited scalability in traditional workflows, by developing a scalable and explainable AI-powered evidence synthesis approach.

Method: The authors developed an AI platform utilizing PICOS formalization, integrating relational storage, semantic retrieval via vector and graph-based approaches, and a Neo4j knowledge graph. Machine learning models like PubMedBERT-based transformers and Bi-LSTM were used for study design classification and compliance detection.

Result: The framework achieved 95.7% transformer-based model accuracy and 87% Bi-LSTM accuracy for study design classification and PICOS compliance, outperforming non-retrieval methods in structured queries. Topic modeling exposed thematic redundancy while identifying research gaps, showcasing the platform's potential for optimizing evidence synthesis.

Conclusion: The research concludes that PICOS-aware and explainable NLP frameworks can improve evidence synthesis by offering scalability, transparency, and domain-general applicability to minimize inefficiencies across biomedical fields.

Abstract: Research waste in biomedical science is driven by redundant studies, incomplete reporting, and the limited scalability of traditional evidence synthesis workflows. We present an AI co-scientist for scalable and transparent knowledge synthesis based on explicit formalization of Population, Intervention, Comparator, Outcome, and Study design (PICOS). The platform integrates relational storage, vector-based semantic retrieval, and a Neo4j knowledge graph. Evaluation was conducted on dementia-sport and non-communicable disease corpora. Automated PICOS compliance and study design classification from titles and abstracts were performed using a Bidirectional Long Short-Term Memory baseline and a transformer-based multi-task classifier fine-tuned from PubMedBERT. Full-text synthesis employed retrieval-augmented generation with hybrid vector and graph retrieval, while BERTopic was used to identify thematic structure, redundancy, and evidence gaps. The transformer model achieved 95.7% accuracy for study design classification with strong agreement against expert annotations, while the Bi-LSTM achieved 87% accuracy for PICOS compliance detection. Retrieval-augmented generation outperformed non-retrieval generation for queries requiring structured constraints, cross-study integration, and graph-based reasoning, whereas non-retrieval approaches remained competitive for high-level summaries. Topic modeling revealed substantial thematic redundancy and identified underexplored research areas. These results demonstrate that PICOS-aware and explainable natural language processing can improve the scalability, transparency, and efficiency of evidence synthesis. The proposed architecture is domain-agnostic and offers a practical framework for reducing research waste across biomedical disciplines.

</details>


### [12] [AgenticRed: Optimizing Agentic Systems for Automated Red-teaming](https://arxiv.org/abs/2601.13518)
*Jiayi Yuan,Jonathan Nöther,Natasha Jaques,Goran Radanović*

Main category: cs.AI

TL;DR: This paper introduces AgenticRed, an automated pipeline leveraging LLMs for red-teaming system design, achieving state-of-the-art results in AI model vulnerability testing.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address limitations in existing red-teaming approaches, which rely heavily on human-specified workflows prone to biases, making exploration of broader design spaces inefficient.

Method: AgenticRed uses LLMs' in-context learning to create and refine red-teaming systems autonomously, incorporating evolutionary selection methods to evolve optimized systems.

Result: AgenticRed achieves significantly high attack success rates, outperforming existing methods by 36% to 100% across various models, demonstrating excellent transferability to proprietary systems.

Conclusion: The study positions automated system design as a promising solution for robust AI safety evaluations, ensuring adaptability with advancing AI technologies.

Abstract: While recent automated red-teaming methods show promise for systematically exposing model vulnerabilities, most existing approaches rely on human-specified workflows. This dependence on manually designed workflows suffers from human biases and makes exploring the broader design space expensive. We introduce AgenticRed, an automated pipeline that leverages LLMs' in-context learning to iteratively design and refine red-teaming systems without human intervention. Rather than optimizing attacker policies within predefined structures, AgenticRed treats red-teaming as a system design problem. Inspired by methods like Meta Agent Search, we develop a novel procedure for evolving agentic systems using evolutionary selection, and apply it to the problem of automatic red-teaming. Red-teaming systems designed by AgenticRed consistently outperform state-of-the-art approaches, achieving 96% attack success rate (ASR) on Llama-2-7B (36% improvement) and 98% on Llama-3-8B on HarmBench. Our approach exhibits strong transferability to proprietary models, achieving 100% ASR on GPT-3.5-Turbo and GPT-4o-mini, and 60% on Claude-Sonnet-3.5 (24% improvement). This work highlights automated system design as a powerful paradigm for AI safety evaluation that can keep pace with rapidly evolving models.

</details>


### [13] [Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic](https://arxiv.org/abs/2601.11840)
*Hongyu Lin,Samer Abdallah,Makar Valentinov,Paul Brennan,Elijah Kagan,Christoph M. Wintersteiger,Denis Ignatovich,Grant Passmore*

Main category: cs.AI

TL;DR: This paper introduces CodeLogician, a neurosymbolic tool combining large language models (LLMs) and formal methods for rigorous software logic analysis, utilizing a novel benchmark.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve program analysis capabilities by addressing gaps in mathematical reasoning and rigorous semantic understanding in existing approaches.

Method: The method involves integrating LLMs with the ImandraX reasoning engine, allowing automated reasoning from explicit formal models, as well as introducing a new benchmark (code-logic-bench) for evaluating software logic.

Result: Integration of LLMs with CodeLogician achieved substantial improvements in reasoning accuracy, closing a 41-47 percentage point gap compared to LLM-only approaches.

Conclusion: The study concludes that neurosymbolic integration is critical for achieving accurate, autonomous software logic analysis capabilities at scale.

Abstract: Large Language Models (LLMs) have shown strong performance on code understanding tasks, yet they fundamentally lack the ability to perform precise, exhaustive mathematical reasoning about program behavior. Existing benchmarks either focus on mathematical proof automation, largely disconnected from real-world software, or on engineering tasks that do not require semantic rigor.
  We present CodeLogician, a neurosymbolic agent for precise analysis of software logic, integrated with ImandraX, an industrial automated reasoning engine deployed in financial markets and safety-critical systems. Unlike prior approaches that use formal methods primarily to validate LLM outputs, CodeLogician uses LLMs to construct explicit formal models of software systems, enabling automated reasoning to answer rich semantic questions beyond binary verification outcomes.
  To rigorously evaluate mathematical reasoning about software logic, we introduce code-logic-bench, a benchmark targeting the middle ground between theorem proving and software engineering benchmarks. It measures reasoning correctness about program state spaces, control flow, coverage constraints, and edge cases, with ground truth defined via formal modeling and region decomposition.
  Comparing LLM-only reasoning against LLMs augmented with CodeLogician, formal augmentation yields substantial improvements, closing a 41-47 percentage point gap in reasoning accuracy. These results demonstrate that neurosymbolic integration is essential for scaling program analysis toward rigorous, autonomous software understanding.

</details>


### [14] [Human-AI Collaborative Inductive Thematic Analysis: AI Guided Analysis and Human Interpretive Authority](https://arxiv.org/abs/2601.11850)
*Matthew Nyaaba,Min SungEun,Mary Abiswin Apam,Kwame Owoahene Acheampong,Emmanuel Dwamena,Xiaoming Zhai*

Main category: cs.AI

TL;DR: This paper explores how AI, specifically ITA-GPT, supports inductive thematic analysis in research, focusing on analysis processes rather than findings.


<details>
  <summary>Details</summary>
Motivation: To investigate how qualitative researchers can responsibly collaborate with generative AI to enhance analytic processes while retaining interpretive authority.

Method: Three experienced researchers used the AI tool ITA-GPT to analyze interview transcripts from education research in Ghana, following a Human-AI collaboration framework and leveraging features like coding and theme development.

Result: The study found ITA-GPT effectively supported structured analysis, improved transparency, and enabled workflow scaffolding. Human researchers retained control of interpretive authority, using judgment to modify or reject AI outputs.

Conclusion: The paper showcases how inductive thematic analysis can be responsibly achieved through human-AI collaborative methods, emphasizing the researcher’s role in maintaining analytic integrity and critical engagement.

Abstract: The increasing use of generative artificial intelligence (GenAI) in qualitative research raises important questions about analytic practice and interpretive authority. This study examines how researchers interact with an Inductive Thematic Analysis GPT (ITA-GPT), a purpose-built AI tool designed to support inductive thematic analysis through structured, semi-automated prompts aligned with reflexive thematic analysis and verbatim coding principles. Guided by a Human-Artificial Intelligence Collaborative Inductive Thematic Analysis (HACITA) framework, the study focuses on analytic process rather than substantive findings. Three experienced qualitative researchers conducted ITA-GPT assisted analyses of interview transcripts from education research in the Ghanaian teacher education context. The tool supported familiarization, verbatim in vivo coding, gerund-based descriptive coding, and theme development, while enforcing trace to text integrity, coverage checks, and auditability. Data sources included interaction logs, AI-generated tables, researcher revisions, deletions, insertions, comments, and reflexive memos. Findings show that ITA-GPT functioned as a procedural scaffold that structured analytic workflow and enhanced transparency. However, interpretive authority remained with human researchers, who exercised judgment through recurrent analytic actions including modification, deletion, rejection, insertion, and commenting. The study demonstrates how inductive thematic analysis is enacted through responsible human AI collaboration.

</details>


### [15] [MyGram: Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment](https://arxiv.org/abs/2601.11885)
*Zhifei Li,Ziyue Qin,Xiangyu Luo,Xiaoju Hou,Yue Zhao,Miao Zhang,Zhifang Huang,Kui Xiao,Bing Yang*

Main category: cs.AI

TL;DR: The paper introduces MyGram, a modality-aware graph transformer aimed at improving multi-modal entity alignment by integrating fine-grained multi-modal fusion and global distribution consistency, achieving superior experimental results.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing multi-modal entity alignment methods that neglect structural contextual information and are prone to shallow feature interference.

Method: The proposed method, MyGram, leverages a modality diffusion learning module and Gram Loss to enhance deep structural contextual understanding and global distribution consistency across multi-modal data.

Result: MyGram achieves superior performance in multi-modal entity alignment, with a maximum improvement of 4.8% in Hits@1 on FBDB15K, 9.9% on FBYG15K, and 4.3% on DBP15K compared to baseline models.

Conclusion: MyGram effectively addresses challenges in multi-modal entity alignment, offering fine-grained fusion and global distribution consistency, validated by significant experimental improvements.

Abstract: Multi-modal entity alignment aims to identify equivalent entities between two multi-modal Knowledge graphs by integrating multi-modal data, such as images and text, to enrich the semantic representations of entities. However, existing methods may overlook the structural contextual information within each modality, making them vulnerable to interference from shallow features. To address these challenges, we propose MyGram, a modality-aware graph transformer with global distribution for multi-modal entity alignment. Specifically, we develop a modality diffusion learning module to capture deep structural contextual information within modalities and enable fine-grained multi-modal fusion. In addition, we introduce a Gram Loss that acts as a regularization constraint by minimizing the volume of a 4-dimensional parallelotope formed by multi-modal features, thereby achieving global distribution consistency across modalities. We conduct experiments on five public datasets. Results show that MyGram outperforms baseline models, achieving a maximum improvement of 4.8% in Hits@1 on FBDB15K, 9.9% on FBYG15K, and 4.3% on DBP15K.

</details>


### [16] [AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems](https://arxiv.org/abs/2601.11903)
*YenTing Lee,Keerthi Koneru,Zahra Moslemi,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: AEMA is a new framework to evaluate LLM-based multi-agent systems in a transparent, reproducible, and trustworthy manner.


<details>
  <summary>Details</summary>
Motivation: Evaluation of multi-agent systems powered by LLMs is essential but challenging, due to evolving tasks and the need for reliable and auditable coordination.

Method: The paper introduces AEMA, a framework that organizes multi-step evaluations of agent workflows, under human oversight, ensuring transparency and accountability.

Result: AEMA demonstrates greater stability, alignment with human principles, and traceability in simulated enterprise scenarios compared to single LLM-based evaluation methods.

Conclusion: AEMA enables enterprises to responsibly evaluate multi-agent systems, leveraging transparency and reproducibility while addressing challenges in trustworthiness.

Abstract: Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. Compared to a single LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Our results on enterprise-style agent workflows simulated using realistic business scenarios demonstrate that AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems.
  Keywords Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight

</details>


### [17] [LIBRA: Language Model Informed Bandit Recourse Algorithm for Personalized Treatment Planning](https://arxiv.org/abs/2601.11905)
*Junyu Cao,Ruijiang Gao,Esmaeil Keyvanshokooh,Jianhao Ma*

Main category: cs.AI

TL;DR: The paper introduces the LIBRA framework, which combines contextual bandits, algorithmic recourse, and large language models for personalized high-stakes decision-making, improving outcomes and efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve sequential decision-making in high-stakes scenarios, such as personalized medicine, by seamlessly combining domain knowledge from LLMs with bandit algorithms.

Method: The authors propose two algorithms: the Generalized Linear Recourse Bandit (GLRB) and LIBRA, which integrates LLM guidance with bandit learning. LIBRA is designed with guarantees on performance, LLM-effort efficiency, and robustness to unreliable LLMs.

Result: LIBRA provides strong theoretical guarantees and performs better in regret and treatment quality than traditional bandit or LLM-only solutions. It is validated through experiments on synthetic data and a hypertension-management case study.

Conclusion: The approach demonstrates the potential of integrating LLMs with recourse-aware algorithms for reliable and efficient decision-making in sensitive domains.

Abstract: We introduce a unified framework that seamlessly integrates algorithmic recourse, contextual bandits, and large language models (LLMs) to support sequential decision-making in high-stakes settings such as personalized medicine. We first introduce the recourse bandit problem, where a decision-maker must select both a treatment action and a feasible, minimal modification to mutable patient features. To address this problem, we develop the Generalized Linear Recourse Bandit (GLRB) algorithm. Building on this foundation, we propose LIBRA, a Language Model-Informed Bandit Recourse Algorithm that strategically combines domain knowledge from LLMs with the statistical rigor of bandit learning. LIBRA offers three key guarantees: (i) a warm-start guarantee, showing that LIBRA significantly reduces initial regret when LLM recommendations are near-optimal; (ii) an LLM-effort guarantee, proving that the algorithm consults the LLM only $O(\log^2 T)$ times, where $T$ is the time horizon, ensuring long-term autonomy; and (iii) a robustness guarantee, showing that LIBRA never performs worse than a pure bandit algorithm even when the LLM is unreliable. We further establish matching lower bounds that characterize the fundamental difficulty of the recourse bandit problem and demonstrate the near-optimality of our algorithms. Experiments on synthetic environments and a real hypertension-management case study confirm that GLRB and LIBRA improve regret, treatment quality, and sample efficiency compared with standard contextual bandits and LLM-only benchmarks. Our results highlight the promise of recourse-aware, LLM-assisted bandit algorithms for trustworthy LLM-bandits collaboration in personalized high-stakes decision-making.

</details>


### [18] [Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart](https://arxiv.org/abs/2601.11940)
*Kang Chen,Fan Yu,Junjie Nian,Shihan Zhao,Zhuoka Feng,Zijun Yao,Heng Wang,Minshen Yu,Yixin Cao*

Main category: cs.AI

TL;DR: This paper addresses the issue of reasoning errors in Long Chain-of-Thought (Long-CoT) models by identifying 'Thinking Traps.' To remedy these, the authors propose a test-time framework called TAAR, which adapts and restarts decoding strategies to improve reasoning outcomes.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the tendency of Long-CoT reasoning models to propagate early errors throughout their outputs, resulting in wrong but internally consistent reasoning sequences, which the authors call 'Thinking Traps.'

Method: TAAR (Trap-Aware Adaptive Restart) is introduced as a test-time control framework. It includes training a diagnostic policy to detect trap indices and escape probabilities from reasoning trajectories. During inference, TAAR truncates problematic segments and applies adaptive restarts, such as higher-temperature resampling or a structured reboot.

Result: Experiments on mathematical and scientific reasoning datasets demonstrate that TAAR significantly enhances reasoning capabilities without requiring fine-tuning of the base models.

Conclusion: The proposed TAAR framework effectively tackles reasoning errors caused by Thinking Traps in Long-CoT models, improving performance on challenging benchmarks through adaptive and restart-based techniques.

Abstract: Scaling test-time compute via Long Chain-of-Thought (Long-CoT) significantly enhances reasoning capabilities, yet extended generation does not guarantee correctness: after an early wrong commitment, models may keep elaborating a self-consistent but incorrect prefix. Through fine-grained trajectory analysis, we identify Thinking Traps, prefix-dominant deadlocks where later reflection, alternative attempts, or verification fails to revise the root error. On a curated subset of DAPO-MATH, 89\% of failures exhibit such traps. To solve this problem, we introduce TAAR (Trap-Aware Adaptive Restart), a test-time control framework that trains a diagnostic policy to predict two signals from partial trajectories: a trap index for where to truncate and an escape probability for whether and how strongly to intervene. At inference time, TAAR truncates the trajectory before the predicted trap segment and adaptively restarts decoding; for severely trapped cases, it applies stronger perturbations, including higher-temperature resampling and an optional structured reboot suffix. Experiments on challenging mathematical and scientific reasoning benchmarks (AIME24, AIME25, GPQA-Diamond, HMMT25, BRUMO25) show that TAAR improves reasoning performance without fine-tuning base model parameters.

</details>


### [19] [Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement](https://arxiv.org/abs/2601.11974)
*Xinmeng Hou,Peiliang Gong,Bohao Qu,Wuqi Wang,Qing Guo,Yang Liu*

Main category: cs.AI

TL;DR: MARS is a framework enabling efficient self-improvement in language models through a single cycle, using principle-based and procedural reflection to refine reasoning and reduce computational costs.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of static, human-designed prompts and high computational cost in self-improving frameworks for Large Language Models (LLMs).

Method: Propose the Metacognitive Agent Reflective Self-improvement (MARS) framework, inspired by human learning, which integrates principle-based and procedural reflection to synthesize optimized instructions for improving reasoning logic.

Result: MARS outperformed state-of-the-art self-evolving systems on six benchmarks and significantly reduced computational overhead.

Conclusion: MARS demonstrates how integrating reflective learning approaches into LLMs enhances their adaptability, efficiency, and reasoning, setting a new standard for self-evolving systems.

Abstract: While Large Language Models (LLMs) enable complex autonomous behavior, current agents remain constrained by static, human-designed prompts that limit adaptability. Existing self-improving frameworks attempt to bridge this gap but typically rely on inefficient, multi-turn recursive loops that incur high computational costs. To address this, we propose Metacognitive Agent Reflective Self-improvement (MARS), a framework that achieves efficient self-evolution within a single recurrence cycle. Inspired by educational psychology, MARS mimics human learning by integrating principle-based reflection (abstracting normative rules to avoid errors) and procedural reflection (deriving step-by-step strategies for success). By synthesizing these insights into optimized instructions, MARS allows agents to systematically refine their reasoning logic without continuous online feedback. Extensive experiments on six benchmarks demonstrate that MARS outperforms state-of-the-art self-evolving systems while significantly reducing computational overhead.

</details>


### [20] [Process In-Context Learning: Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion](https://arxiv.org/abs/2601.11979)
*Ang Gao,Changshuo Zhang,Xiao Zhang,Deyang Li,Minjun Zhao,Fangchao Liu,Xinyu Zhang*

Main category: cs.AI

TL;DR: PICL enhances mathematical reasoning in LLMs by dynamically adapting demonstrations during inference to address confusion points.


<details>
  <summary>Details</summary>
Motivation: Existing in-context learning approaches fail to address dynamic confusion points during multi-step reasoning, resulting in decreased accuracy.

Method: PICL identifies potential confusion points during inference and retrieves relevant demonstrations from a pool to resolve them in real-time.

Result: PICL reduces mid-inference confusion and outperforms baseline methods in mathematical reasoning tasks.

Conclusion: Dynamic integration of demonstrations enhances reasoning capabilities in LLMs, especially for tasks requiring step-by-step logic.

Abstract: In-context learning (ICL) has proven highly effective across diverse large language model (LLM) tasks. However, its potential for enhancing tasks that demand step-by-step logical deduction, such as mathematical reasoning, remains underexplored. A core limitation of existing ICL approaches is their static use of demonstrations: examples are pre-selected before inference and remain fixed, failing to adapt to the dynamic confusion points that often arise during multi-step reasoning such as ambiguous calculations or logical gaps. These unresolved confusion points can lead to cascading errors that degrade final accuracy. To tackle this issue, we propose Process In-Context Learning (PICL), a dynamic demonstration integration framework designed to boost mathematical reasoning by responding to real-time inference needs. PICL operates in two stages: 1)~it identifies potential confusion points by analyzing semantics and entropy in the reasoning process and summarizes their core characteristics; 2)~upon encountering these points, it retrieves relevant demonstrations from the demonstration pool that match the confusion context and inserts them directly into the ongoing reasoning process to guide subsequent steps. Experiments show that PICL outperforms baseline methods by mitigating mid-inference confusion, highlighting the value of adaptive demonstration insertion in complex mathematical reasoning.

</details>


### [21] [Kernel-Based Learning of Safety Barriers](https://arxiv.org/abs/2601.12002)
*Oliver Schön,Zhengang Zhong,Sadegh Soudjani*

Main category: cs.AI

TL;DR: The paper proposes a scalable, data-driven method for safety verification of black-box systems using control barrier certificates and robust statistical tools, tested on complex systems including neural networks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in verifying safety of AI-driven systems in critical fields like healthcare and autonomous driving, where traditional methods struggle with black-box and complex dynamics.

Method: A data-driven approach employing control barrier certificates learned from system trajectories, conditional mean embeddings in RKHS, and a Fourier-based method to solve the optimization problem, ensuring scalability and robustness.

Result: The proposed method efficiently verified safety for systems with stochastic dynamics, handling complexity and uncertainty, supported with case studies including neural network-controlled systems.

Conclusion: This novel approach expands safety verification capability for black-box systems beyond traditional restrictive assumptions, providing a scalable and robust framework fit for real-world applications.

Abstract: The rapid integration of AI algorithms in safety-critical applications such as autonomous driving and healthcare is raising significant concerns about the ability to meet stringent safety standards. Traditional tools for formal safety verification struggle with the black-box nature of AI-driven systems and lack the flexibility needed to scale to the complexity of real-world applications. In this paper, we present a data-driven approach for safety verification and synthesis of black-box systems with discrete-time stochastic dynamics. We employ the concept of control barrier certificates, which can guarantee safety of the system, and learn the certificate directly from a set of system trajectories. We use conditional mean embeddings to embed data from the system into a reproducing kernel Hilbert space (RKHS) and construct an RKHS ambiguity set that can be inflated to robustify the result to out-of-distribution behavior. We provide the theoretical results on how to apply the approach to general classes of temporal logic specifications beyond safety. For the data-driven computation of safety barriers, we leverage a finite Fourier expansion to cast a typically intractable semi-infinite optimization problem as a linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. Our work moves beyond restrictive assumptions on system dynamics and uncertainty, as demonstrated on two case studies including a black-box system with a neural network controller.

</details>


### [22] [Are LLMs Ready for TOON? Benchmarking Structural Correctness-Sustainability Trade-offs in Novel Structured Output Formats](https://arxiv.org/abs/2601.12014)
*Elio Masciari,Vincenzo Moscato,Enea Vincenzo Napolitano,Gian Marco Orlando,Marco Perillo,Diego Russo*

Main category: cs.AI

TL;DR: The paper introduces a framework for evaluating structured output formats of LLMs, considering both correctness and environmental efficiency. It proposes a new metric, GCS_env, and benchmarks the compact TOON format against other representations like JSON, YAML, and XML.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked aspect of environmental efficiency in structured output generation by LLMs, alongside the evaluation of structural correctness.

Method: Introduced a sustainability-aware evaluation framework that measures token usage, generation time, and estimated carbon emissions. Proposed GCS_env, a metric combining correctness with carbon efficiency, and systematically compared TOON with JSON, XML, and YAML across various LLMs.

Result: TOON outputs are more compact and environmentally efficient but show lower correctness without native model support. Increased model capacity can address this gap, and environmental scoring shifts format rankings based on priorities.

Conclusion: The study highlights the importance of sustainability-aware benchmarking and demonstrates the advantages of compact formats like TOON for large-scale, carbon-conscious LLM use cases.

Abstract: Large Language Models (LLMs) are increasingly required to generate structured, machine-readable outputs for downstream systems. While recent benchmarks have focused on evaluating the structural correctness of such outputs, the environmental impact of inference for different output formats has largely been overlooked. In this paper, we argue that structured output formats should be assessed not only in terms of correctness, but also with respect to their environmental efficiency. To this end, we introduce a sustainability-aware evaluation framework for structured generation that measures token usage, generation time, and estimated carbon emissions. Within this framework, we propose the Environment-Aware Generation Correctness Score (GCS_env), a unified metric that integrates structural correctness with carbon-aware efficiency. Using this framework, we systematically benchmark the novel TOON format against established representations (JSON, XML, YAML) across multiple LLMs spanning different architectures and parameter scales.
  Our results reveal a consistent trade-off: TOON yields markedly more compact outputs and lower emissions, but lower structural correctness when models lack native support. We show that increased model capacity reduces this gap and that environment-aware scoring can shift format rankings depending on deployment priorities. highlighting the need for sustainability-inclusive benchmarking and provides empirical evidence that compact representations such as TOON can offer practical advantages in large-scale, carbon-conscious LLM deployments.

</details>


### [23] [A Multi-Agent System for Generating Actionable Business Advice](https://arxiv.org/abs/2601.12024)
*Kartikey Singh Bhandari,Tanish Jain,Archit Agrawal,Dhruv Kumar,Praveen Kumar,Pratik Narang*

Main category: cs.AI

TL;DR: The paper proposes a multi-agent LLM framework to analyze customer reviews and deliver actionable business advice, outperforming traditional single-model baselines.


<details>
  <summary>Details</summary>
Motivation: To improve upon traditional sentiment analysis and aspect extraction methods by providing businesses with actionable insights derived from large-scale customer reviews.

Method: The method involves a framework with four components: clustering representative reviews, generating advice, iterative feedback-driven evaluation, and feasibility ranking.

Result: The framework surpasses single model baselines in dimensions like actionability, specificity, and non-redundancy across various service domains and model sizes.

Conclusion: The framework effectively transforms customer reviews into actionable advice and demonstrates that medium-sized language models can closely match the performance of larger ones.

Abstract: Customer reviews contain rich signals about product weaknesses and unmet user needs, yet existing analytic methods rarely move beyond descriptive tasks such as sentiment analysis or aspect extraction. While large language models (LLMs) can generate free-form suggestions, their outputs often lack accuracy and depth of reasoning. In this paper, we present a multi-agent, LLM-based framework for prescriptive decision support, which transforms large scale review corpora into actionable business advice. The framework integrates four components: clustering to select representative reviews, generation of advices, iterative evaluation, and feasibility based ranking. This design couples corpus distillation with feedback driven advice refinement to produce outputs that are specific, actionable, and practical. Experiments across three service domains and multiple model families show that our framework consistently outperform single model baselines on actionability, specificity, and non-redundancy, with medium sized models approaching the performance of large model frameworks.

</details>


### [24] [ARC: Active and Reflection-driven Context Management for Long-Horizon Information Seeking Agents](https://arxiv.org/abs/2601.12030)
*Yilun Yao,Shan Huang,Elsie Dai,Zhewen Tan,Zhenyu Duan,Shousheng Jia,Yanbing Jiang,Tong Yang*

Main category: cs.AI

TL;DR: The paper introduces ARC, a framework to combat context rot in large language models by actively managing and revising the reasoning context, improving accuracy in long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: Address the issue of 'context rot,' where large language models fail to maintain coherent and relevant reasoning over extended interactions, leading to degraded performance.

Method: Introduces ARC, a framework that actively manages the reasoning context through reflection-driven monitoring and revision, treating context as a dynamic internal state rather than static memory.

Result: ARC shows substantial improvement in performance, with up to 11% better accuracy in information-seeking tasks compared to passive context management methods.

Conclusion: Active and reflective context management, as demonstrated by ARC, resolves long-horizon degradation issues and enhances the performance of language models.

Abstract: Large language models are increasingly deployed as research agents for deep search and long-horizon information seeking, yet their performance often degrades as interaction histories grow. This degradation, known as context rot, reflects a failure to maintain coherent and task-relevant internal states over extended reasoning horizons. Existing approaches primarily manage context through raw accumulation or passive summarization, treating it as a static artifact and allowing early errors or misplaced emphasis to persist. Motivated by this perspective, we propose ARC, which is the first framework to systematically formulate context management as an active, reflection-driven process that treats context as a dynamic internal reasoning state during execution. ARC operationalizes this view through reflection-driven monitoring and revision, allowing agents to actively reorganize their working context when misalignment or degradation is detected. Experiments on challenging long-horizon information-seeking benchmarks show that ARC consistently outperforms passive context compression methods, achieving up to an 11% absolute improvement in accuracy on BrowseComp-ZH with Qwen2.5-32B-Instruct.

</details>


### [25] [Abstract Argumentation with Subargument Relations](https://arxiv.org/abs/2601.12038)
*Beishui Liao*

Main category: cs.AI

TL;DR: The paper introduces an enriched abstract argumentation framework incorporating subargument relations to address structural dependencies overlooked in traditional frameworks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in traditional abstract argumentation frameworks that neglect structural dependencies, such as subargument relations, crucial for structured reasoning.

Method: The authors extend traditional argumentation frameworks by adding subargument relations alongside attack relations and analyze their interactions and impact on semantic properties.

Result: The enhanced framework provides new insights into the role of subarguments, while maintaining a high-level abstraction for argument acceptability reasoning.

Conclusion: Incorporating subargument relations into abstract argumentation frameworks improves their representational capabilities and clarifies subarguments' role in abstract reasoning.

Abstract: Dung's abstract argumentation framework characterises argument acceptability solely via an attack relation, deliberately abstracting from the internal structure of arguments. While this level of abstraction has enabled a rich body of results, it limits the ability to represent structural dependencies that are central in many structured argumentation formalisms, in particular subargument relations. Existing extensions, including bipolar argumentation frameworks, introduce support relations, but these do not capture the asymmetric and constitutive nature of subarguments or their interaction with attacks. In this paper, we study abstract argumentation frameworks enriched with an explicit subargument relation, treated alongside attack as a basic relation. We analyse how subargument relations interact with attacks and examine their impact on fundamental semantic properties. This framework provides a principled abstraction of structural information and clarifies the role of subarguments in abstract acceptability reasoning.

</details>


### [26] [Partial Reasoning in Language Models: Search and Refinement Guided by Uncertainty](https://arxiv.org/abs/2601.12040)
*Murilo da Luz,Bruno Brandão,Luana Martins,Gustavo Oliveira,Bryan de Oliveira,Luckeciano Melo,Telma Soares*

Main category: cs.AI

TL;DR: The paper introduces PREGU, a method to improve reasoning in Large Language Models (LLMs) by using entropy-based uncertainty signals to refine answers.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with multi-step reasoning and inference, particularly in areas like mathematical and logical reasoning.

Method: PREGU leverages entropy monitoring during model generation. When uncertainty is detected, reasoning halts, and partial answers are refined via the Soft Reasoning method in latent space.

Result: Experiments using various LLMs across four benchmarks demonstrated that PREGU often outperforms or matches Soft Reasoning methods.

Conclusion: Entropy serves as an effective trigger for localized reasoning refinement, providing an advantage in complex reasoning tasks with LLMs.

Abstract: The use of Large Language Models (LLMs) for reasoning and planning tasks has drawn increasing attention in Artificial Intelligence research. Despite their remarkable progress, these models still exhibit limitations in multi-step inference scenarios, particularly in mathematical and logical reasoning. We introduce PREGU (Partial Reasoning Guided by Uncertainty). PREGU monitors the entropy of the output distribution during autoregressive generation and halts the process whenever entropy exceeds a defined threshold, signaling uncertainty. From that point, a localized search is performed in the latent space to refine the partial reasoning and select the most coherent answer, using the Soft Reasoning method. Experiments conducted with LLaMA-3-8B, Mistral-7B, and Qwen2-7B across four reasoning benchmarks (GSM8K, GSM-Hard, SVAMP, and StrategyQA) showed performance greater than or similar to Soft Reasoning, indicating that entropy can serve as an effective signal to trigger selective refinement during reasoning.

</details>


### [27] [UniMo: Unified Motion Generation and Understanding with Chain of Thought](https://arxiv.org/abs/2601.12126)
*Guocun Wang,Kenkun Liu,Jing Lin,Guorui Song,Jian Li,Xiaoguang Han*

Main category: cs.AI

TL;DR: The paper introduces UniMo, a novel framework for combining motion-language information with interpretable reasoning into large language models. This improves semantic alignment and task coherence in 3D human motion generation and understanding.


<details>
  <summary>Details</summary>
Motivation: Current methods for 3D human motion lack interpretability, leading to challenges in aligning semantics and coherence between motions and languages. The paper aims to address shortcomings in token prediction for motion sequences by integrating reasoning and better training strategies.

Method: UniMo incorporates interpretable chain of thought reasoning into large language models via supervised fine-tuning, and introduces Group Relative Policy Optimization (GRPO) during post-training to improve structure and alignment in motion predictions.

Result: UniMo significantly outperforms existing models, achieving state-of-the-art results in both motion generation and motion understanding tasks.

Conclusion: This framework resolves critical issues in semantic alignment and cumulative errors for 3D motion tasks, advancing performance and interpretability in the field.

Abstract: Existing 3D human motion generation and understanding methods often exhibit limited interpretability, restricting effective mutual enhancement between these inherently related tasks. While current unified frameworks based on large language models (LLMs) leverage linguistic priors, they frequently encounter challenges in semantic alignment and task coherence. Moreover, the next-token prediction paradigm in LLMs is ill-suited for motion sequences, causing cumulative prediction errors. To address these limitations, we propose UniMo, a novel framework that integrates motion-language information and interpretable chain of thought (CoT) reasoning into the LLM via supervised fine-tuning (SFT). We further introduce reinforcement learning with Group Relative Policy Optimization (GRPO) as a post-training strategy that optimizes over groups of tokens to enforce structural correctness and semantic alignment, mitigating cumulative errors in motion token prediction. Extensive experiments demonstrate that UniMo significantly outperforms existing unified and task-specific models, achieving state-of-the-art performance in both motion generation and understanding.

</details>


### [28] [DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants](https://arxiv.org/abs/2601.12138)
*Abhishek Kumar,Riya Tapwal,Carsten Maple*

Main category: cs.AI

TL;DR: This study introduces DriveSafe, a new taxonomy for classifying safety-critical risks of LLM-based driving assistants, emphasizing their failure to handle driving-related safety compliance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of domain-specific risk identification and assessment tools for LLMs used in vehicle digital assistants, where inadequate responses can lead to serious consequences.

Method: Developed DriveSafe, a hierarchical risk taxonomy with 129 fine-grained categories based on driving regulations and safety principles, validated through expert review and testing of LLM behavior on driving prompts.

Result: Tested six widely-used LLMs and found them frequently incapable of properly refusing unsafe or legally non-compliant driving-related queries, highlighting their safety limitations.

Conclusion: General-purpose safety alignments in LLMs are insufficient for driving contexts; domain-specific frameworks like DriveSafe are crucial for improving safety.

Abstract: Large Language Models (LLMs) are increasingly integrated into vehicle-based digital assistants, where unsafe, ambiguous, or legally incorrect responses can lead to serious safety, ethical, and regulatory consequences. Despite growing interest in LLM safety, existing taxonomies and evaluation frameworks remain largely general-purpose and fail to capture the domain-specific risks inherent to real-world driving scenarios. In this paper, we introduce DriveSafe, a hierarchical, four-level risk taxonomy designed to systematically characterize safety-critical failure modes of LLM-based driving assistants. The taxonomy comprises 129 fine-grained atomic risk categories spanning technical, legal, societal, and ethical dimensions, grounded in real-world driving regulations and safety principles and reviewed by domain experts. To validate the safety relevance and realism of the constructed prompts, we evaluate their refusal behavior across six widely deployed LLMs. Our analysis shows that the evaluated models often fail to appropriately refuse unsafe or non-compliant driving-related queries, underscoring the limitations of general-purpose safety alignment in driving contexts.

</details>


### [29] [TIDE: A Trace-Informed Depth-First Exploration for Planning with Temporally Extended Goals](https://arxiv.org/abs/2601.12141)
*Yuliia Suprun,Khen Elimelech,Lydia E. Kavraki,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: This paper introduces TIDE, a novel task planning approach for temporally extended goals (TEGs) using informed heuristics and adaptive backtracking, showing promising performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional LTLf task planning approaches lack guided heuristics and efficient mechanisms to handle temporal goals, limiting their efficiency and effectiveness.

Method: TIDE decomposes the temporal planning problem into manageable reach-avoid sub-problems, prioritizes automaton traces with cost heuristics, and systematically backtracks with penalization to ensure completeness.

Result: TIDE performs better than traditional approaches, offering guided exploration and improved efficiency in tasks requiring temporally extended goals.

Conclusion: TIDE is an efficient and effective method for planning with TEGs, enhancing the capabilities of AI and robotics in executing complex objectives.

Abstract: Task planning with temporally extended goals (TEGs) is a critical challenge in AI and robotics, enabling agents to achieve complex sequences of objectives over time rather than addressing isolated, immediate tasks. Linear Temporal Logic on finite traces (LTLf ) provides a robust formalism for encoding these temporal goals. Traditional LTLf task planning approaches often transform the temporal planning problem into a classical planning problem with reachability goals, which are then solved using off-the-shelf planners. However, these methods often lack informed heuristics to provide a guided search for temporal goals. We introduce TIDE (Trace-Informed Depth-first Exploration), a novel approach that addresses this limitation by decomposing a temporal problem into a sequence of smaller, manageable reach-avoid sub-problems, each solvable using an off-the-shelf planner. TIDE identifies and prioritizes promising automaton traces within the domain graph, using cost-driven heuristics to guide exploration. Its adaptive backtracking mechanism systematically recovers from failed plans by recalculating costs and penalizing infeasible transitions, ensuring completeness and efficiency. Experimental results demonstrate that TIDE achieves promising performance and is a valuable addition to the portfolio of planning methods for temporally extended goals.

</details>


### [30] [Optimal Power Allocation and Sub-Optimal Channel Assignment for Downlink NOMA Systems Using Deep Reinforcement Learning](https://arxiv.org/abs/2601.12242)
*WooSeok Kim,Jeonghoon Lee,Sangho Kim,Taesun An,WonMin Lee,Dowon Kim,Kyungseop Shin*

Main category: cs.AI

TL;DR: This research focuses on incorporating deep reinforcement learning (DRL) with replay memory into a NOMA system to improve network resource allocation, specifically addressing the channel assignment problem.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the increasing need to optimize network resources due to the expansion of IoT, resulting in resource scarcity. NOMA systems aim to address this scarcity, but existing limitations and challenges, including the channel assignment problem, demand further study.

Method: The method involves developing a DRL framework with replay memory and an on-policy algorithm to improve the generalization of resource allocation in NOMA systems. This includes simulations to evaluate different settings such as learning rate, batch size, model type, and state features.

Result: The proposed method introduces a new approach to handle the channel assignment problem in NOMA systems by testing the effectiveness of DRL with specific variations of its parameters and features.

Conclusion: The study demonstrates that DRL with replay memory and an on-policy algorithm holds promise in addressing resource allocation challenges in NOMA systems, though further development and testing are needed.

Abstract: In recent years, Non-Orthogonal Multiple Access (NOMA) system has emerged as a promising candidate for multiple access frameworks due to the evolution of deep machine learning, trying to incorporate deep machine learning into the NOMA system. The main motivation for such active studies is the growing need to optimize the utilization of network resources as the expansion of the internet of things (IoT) caused a scarcity of network resources. The NOMA addresses this need by power multiplexing, allowing multiple users to access the network simultaneously. Nevertheless, the NOMA system has few limitations. Several works have proposed to mitigate this, including the optimization of power allocation known as joint resource allocation(JRA) method, and integration of the JRA method and deep reinforcement learning (JRA-DRL). Despite this, the channel assignment problem remains unclear and requires further investigation. In this paper, we propose a deep reinforcement learning framework incorporating replay memory with an on-policy algorithm, allocating network resources in a NOMA system to generalize the learning. Also, we provide extensive simulations to evaluate the effects of varying the learning rate, batch size, type of model, and the number of features in the state.

</details>


### [31] [Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration](https://arxiv.org/abs/2601.12256)
*Jinyoung Park,Minseong Bae,Jeehye Na,Hyunwoo J. Kim*

Main category: cs.AI

TL;DR: The paper introduces CoLLaMo, a multi-modal large molecular language model (LMLM) for improved molecular analysis using advanced integration of molecular data and enhanced assessment metrics.


<details>
  <summary>Details</summary>
Motivation: Existing LMLMs struggle with hallucination, limited robustness, and lack of efficient integration of 1D, 2D, and 3D molecular modalities.

Method: CoLLaMo employs a relation-aware modality-collaborative projector and introduces molecule-centric evaluation metrics for robust and refined molecular analysis.

Result: CoLLaMo demonstrates superior performance in various tasks such as molecule captioning, property QA, motif counting, and IUPAC name prediction compared to existing models.

Conclusion: CoLLaMo effectively improves LMLM molecular comprehension and robustness through advanced modality integration and molecule-specific evaluation methods.

Abstract: Large language models (LLMs) have demonstrated their instruction-following capabilities and achieved powerful performance on various tasks. Inspired by their success, recent works in the molecular domain have led to the development of large molecular language models (LMLMs) that integrate 1D molecular strings or 2D molecular graphs into the language models. However, existing LMLMs often suffer from hallucination and limited robustness, largely due to inadequate integration of diverse molecular modalities such as 1D sequences, 2D molecular graphs, and 3D conformations. To address these limitations, we propose CoLLaMo, a large language model-based molecular assistant equipped with a multi-level molecular modality-collaborative projector. The relation-aware modality-collaborative attention mechanism in the projector facilitates fine-grained and relation-guided information exchange between atoms by incorporating 2D structural and 3D spatial relations. Furthermore, we present a molecule-centric new automatic measurement, including a hallucination assessment metric and GPT-based caption quality evaluation to address the limitations of token-based generic evaluation metrics (i.e., BLEU) widely used in assessing molecular comprehension of LMLMs. Our extensive experiments demonstrate that our CoLLaMo enhances the molecular modality generalization capabilities of LMLMs, achieving the best performance on multiple tasks, including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.

</details>


### [32] [FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains](https://arxiv.org/abs/2601.12259)
*Jiashuo Liu,Siyuan Chen,Zaiyuan Wang,Zhiyuan Zeng,Jiacheng Guo,Liang Hu,Lingyue Yin,Suozhi Huang,Wenxin Hao,Yang Yang,Zerui Cheng,Zixin Yao,Lingyue Yin,Haoxin Liu,Jiayi Cheng,Yuzhen Li,Zezhong Ma,Bingjie Wang,Bingsen Qiu,Xiao Liu,Zeyang Zhang,Zijian Liu,Jinpeng Wang,Mingren Yin,Tianci He,Yali Liao,Yixiao Tian,Zhenwei Zhu,Anqi Dai,Ge Zhang,Jingkai Liu,Kaiyuan Zhang,Wenlong Wu,Xiang Gao,Xinjie Chen,Zhixin Yao,Zhoufutu Wen,B. Aditya Prakash,Jose Blanchet,Mengdi Wang,Nian Si,Wenhao Huang*

Main category: cs.AI

TL;DR: The paper introduces FutureX-Pro, a framework extending FutureX to high-value verticals for agentic future prediction. It focuses on Finance, Retail, Public Health, and Natural Disaster while benchmarking Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the reliability and potential of generalist agents, particularly LLMs, in critical, high-value domains where precision and domain grounding are essential.

Method: The study benchmarks agentic LLMs using foundational prediction tasks in important economic and social sectors. It employs FutureX's contamination-free, live-evaluation pipeline to evaluate performance.

Result: The study finds a significant performance gap between generalist LLMs and the precision required for high-stakes applications in vertical domains.

Conclusion: Generalist LLMs show potential but lack sufficient precision for deployment in critical, capital-intensive, or safety-sensitive sectors within high-value domains.

Abstract: Building upon FutureX, which established a live benchmark for general-purpose future prediction, this report introduces FutureX-Pro, including FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, FutureX-NaturalDisaster, and FutureX-Search. These together form a specialized framework extending agentic future prediction to high-value vertical domains. While generalist agents demonstrate proficiency in open-domain search, their reliability in capital-intensive and safety-critical sectors remains under-explored. FutureX-Pro targets four economically and socially pivotal verticals: Finance, Retail, Public Health, and Natural Disaster. We benchmark agentic Large Language Models (LLMs) on entry-level yet foundational prediction tasks -- ranging from forecasting market indicators and supply chain demands to tracking epidemic trends and natural disasters. By adapting the contamination-free, live-evaluation pipeline of FutureX, we assess whether current State-of-the-Art (SOTA) agentic LLMs possess the domain grounding necessary for industrial deployment. Our findings reveal the performance gap between generalist reasoning and the precision required for high-value vertical applications.

</details>


### [33] [Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding](https://arxiv.org/abs/2601.12260)
*Yihao Ding,Qiang Sun,Puzhen Wu,Sirui Li,Siwen Luo,Wei Liu*

Main category: cs.AI

TL;DR: Docs2Synth introduces a synthetic-supervision framework improving document understanding in regulated, sensitive, and low-resource domains by combining retrieval-guided inference and iterative processes to minimize hallucination in responses.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address two main challenges in regulated document understanding: the lack of manual annotations for adapting models to domain-specific knowledge and the hallucination/limited domain grounding in existing pretrained models, especially in sensitive and evolving fields.

Method: Docs2Synth is a framework that processes raw document data to synthetically generate QA pairs, verifies their validity, and trains a lightweight visual retriever to find domain-relevant evidence. This retriever works with a Multimodal Large Language Model (MLLM) in an iterative retrieval-generation loop to improve accuracy, minimize hallucinations, and enhance response consistency.

Result: Docs2Synth improves grounding and generalization by reducing reliance on human annotations. Experiments show its effectiveness on multiple regulated document understanding (VRDU) benchmarks.

Conclusion: Docs2Synth makes VRDU progress by enabling high-performance results in private domains without manual annotations, ensuring scalability, and providing an accessible Python package for real-world deployment.

Abstract: Document understanding (VRDU) in regulated domains is particularly challenging, since scanned documents often contain sensitive, evolving, and domain specific knowledge. This leads to two major challenges: the lack of manual annotations for model adaptation and the difficulty for pretrained models to stay up-to-date with domain-specific facts. While Multimodal Large Language Models (MLLMs) show strong zero-shot abilities, they still suffer from hallucination and limited domain grounding. In contrast, discriminative Vision-Language Pre-trained Models (VLPMs) provide reliable grounding but require costly annotations to cover new domains. We introduce Docs2Synth, a synthetic-supervision framework that enables retrieval-guided inference for private and low-resource domains. Docs2Synth automatically processes raw document collections, generates and verifies diverse QA pairs via an agent-based system, and trains a lightweight visual retriever to extract domain-relevant evidence. During inference, the retriever collaborates with an MLLM through an iterative retrieval--generation loop, reducing hallucination and improving response consistency. We further deliver Docs2Synth as an easy-to-use Python package, enabling plug-and-play deployment across diverse real-world scenarios. Experiments on multiple VRDU benchmarks show that Docs2Synth substantially enhances grounding and domain generalization without requiring human annotations.

</details>


### [34] [ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents](https://arxiv.org/abs/2601.12294)
*Dawei Li,Yuguang Yao,Zhen Tan,Huan Liu,Ruocheng Guo*

Main category: cs.AI

TL;DR: The paper introduces ToolPRMBench, a benchmark for evaluating process reward models (PRMs) in tool-using agents, combining offline and online sampling to test single-step and multi-step errors.


<details>
  <summary>Details</summary>
Motivation: To address the lack of reliable benchmarks to systematically evaluate PRMs in the context of tool-using agents.

Method: The study developed ToolPRMBench, generating step-level test cases from agent trajectories, and employing a multi-LLM pipeline to reduce label noise and ensure data quality.

Result: Experiments on ToolPRMBench revealed differences in PRM performance, showing the effectiveness and potential of specialized PRMs over general PRMs.

Conclusion: ToolPRMBench provides a systematic evaluation framework for PRMs, highlighting the necessity for specialized PRMs in improving outcomes for tool-using agents.

Abstract: Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.

</details>


### [35] [Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection](https://arxiv.org/abs/2601.12310)
*Jennifer Dodgson,Alfath Daryl Alhajir,Michael Joedhitya,Akira Rafhael Janson Pattirane,Surender Suresh Kumar,Joseph Lim,C. H. Peh,Adith Ramdas,Steven Zhang Zhexu*

Main category: cs.AI

TL;DR: This paper proposes a self-training system architecture based on environmental viability and survival of behaviours rather than reward or task-specific criteria, showing promise in avoiding issues like reward hacking and semantic drift.


<details>
  <summary>Details</summary>
Motivation: Current self-training systems often fail due to reward hacking and semantic drift caused by relying on external criteria for assessing data quality.

Method: The paper introduces a novel self-training system where behaviours are evaluated based on their environmental viability and persistence under resource constraints, eliminating proxy optimization and reducing stability issues.

Result: The study demonstrates that improvement arises from effective strategies persisting amidst consolidation and pruning, and models develop meta-learning behaviors without instructions, such as using failure as a learning opportunity.

Conclusion: Grounding selection in environmental viability enables robust self-improvement in autonomous systems, reducing reliance on human-curated data and complex reward structures.

Abstract: Self-training systems often degenerate due to the lack of an external criterion for judging data quality, leading to reward hacking and semantic drift. This paper provides a proof-of-concept system architecture for stable self-training under sparse external feedback and bounded memory, and empirically characterises its learning dynamics and failure modes.
  We introduce a self-training architecture in which learning is mediated exclusively by environmental viability, rather than by reward, objective functions, or externally defined fitness criteria. Candidate behaviours are executed under real resource constraints, and only those whose environmental effects both persist and preserve the possibility of future interaction are propagated. The environment does not provide semantic feedback, dense rewards, or task-specific supervision; selection operates solely through differential survival of behaviours as world-altering events, making proxy optimisation impossible and rendering reward-hacking evolutionarily unstable.
  Analysis of semantic dynamics shows that improvement arises primarily through the persistence of effective and repeatable strategies under a regime of consolidation and pruning, a paradigm we refer to as negative-space learning (NSL), and that models develop meta-learning strategies (such as deliberate experimental failure in order to elicit informative error messages) without explicit instruction. This work establishes that environment-grounded selection enables sustainable open-ended self-improvement, offering a viable path toward more robust and generalisable autonomous systems without reliance on human-curated data or complex reward shaping.

</details>


### [36] [Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence](https://arxiv.org/abs/2601.12318)
*Dehao Ying,Fengchang Yu,Haihua Chen,Changjiang Jiang,Yurong Li,Wei Lu*

Main category: cs.AI

TL;DR: The paper surveys data generation methods in Document Intelligence (DI), creating a unifying framework to address gaps in manual annotation bottlenecks and fragmented research areas.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of training data in DI is a bottleneck, and a unified framework is needed to align data generation methods with real-world workflows.

Method: The paper introduces a taxonomy based on data and label availability, four paradigms for data generation, and a multi-level evaluation framework for performance analysis.

Result: The methodology highlights challenges like fidelity gaps and opportunities such as co-evolutionary ecosystems, demonstrating a systematic view for enhancing DI benchmarks.

Conclusion: Positioning data generation as the core for next-generation DI, the survey bridges fragmented areas into a unified resource-centric structure to drive advancements.

Abstract: The advancement of Document Intelligence (DI) demands large-scale, high-quality training data, yet manual annotation remains a critical bottleneck. While data generation methods are evolving rapidly, existing surveys are constrained by fragmented focuses on single modalities or specific tasks, lacking a unified perspective aligned with real-world workflows. To fill this gap, this survey establishes the first comprehensive technical map for data generation in DI. Data generation is redefined as supervisory signal production, and a novel taxonomy is introduced based on the "availability of data and labels." This framework organizes methodologies into four resource-centric paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. Furthermore, a multi-level evaluation framework is established to integrate intrinsic quality and extrinsic utility, compiling performance gains across diverse DI benchmarks. Guided by this unified structure, the methodological landscape is dissected to reveal critical challenges such as fidelity gaps and frontiers including co-evolutionary ecosystems. Ultimately, by systematizing this fragmented field, data generation is positioned as the central engine for next-generation DI.

</details>


### [37] [MARO: Learning Stronger Reasoning from Social Interaction](https://arxiv.org/abs/2601.12323)
*Yin Cai,Zhouhong Gu,Juntao Zhang,Ping Chen*

Main category: cs.AI

TL;DR: This paper introduces Multi-Agent Reward Optimization (MARO), a method that enhances the reasoning capabilities of large language models by training them in multi-agent social environments.


<details>
  <summary>Details</summary>
Motivation: Existing large language models lack interaction, negotiation, and competition experience needed for real-world reasoning, as their training is limited to static textual data and predetermined problems.

Method: MARO learns by addressing three key challenges in multi-agent social environments: (1) decomposing outcomes into specific behaviors to solve the sparse learning signal issue, (2) balancing role training sample weights to address uneven role distribution, and (3) directly evaluating behavior utility to handle environmental instability.

Result: MARO significantly improves social reasoning skills in LLMs, and its abilities generalize well to tasks like mathematical reasoning and instruction following.

Conclusion: Multi-agent social learning with MARO demonstrates strong potential in enhancing the general reasoning capabilities of large language models, going beyond traditional training approaches.

Abstract: Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.

</details>


### [38] [Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business Recommendations](https://arxiv.org/abs/2601.12338)
*Kartikey Singh Bhandari,Manav Ganesh,Yashwant Viswanathan,Archit Agrawal,Dhruv Kumar,Pratik Narang*

Main category: cs.AI

TL;DR: The paper presents a framework for translating customer review text into actionable business recommendations using a two-large language model (LLM) approach, improving specificity and actionability.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of converting unstructured customer feedback into actionable and concrete business decisions, given the rich yet complex information in reviews.

Method: A two-LLM framework is introduced: an Issue model identifies key review issues and themes, while an Advice model generates actionable recommendations using a Mixture of LoRA experts strategy for enhanced specialization without full fine-tuning. Synthetic training data constructed from Yelp reviews is used for development and evaluation.

Result: The proposed framework demonstrates improvements in actionability and specificity of recommendations compared to baseline methods, while balancing efficiency and quality across test domains (airlines and restaurants).

Conclusion: The modular approach provides a practical solution to extract actionable insights from reviews, with enhanced effectiveness and efficiency. It validates the use of specialized strategies for generating domain-specific advice.

Abstract: Customer reviews contain detailed, domain specific signals about service failures and user expectations, but converting this unstructured feedback into actionable business decisions remains difficult. We study review-to-action generation: producing concrete, implementable recommendations grounded in review text. We propose a modular two-LLM framework in which an Issue model extracts salient issues and assigns coarse themes, and an Advice model generates targeted operational fixes conditioned on the extracted issue representation. To enable specialization without expensive full fine-tuning, we adapt the Advice model using a mixture of LoRA experts strategy: multiple low-rank adapters are trained and a lightweight gating mechanism performs token-level expert mixing at inference, combining complementary expertise across issue types. We construct synthetic review-issue-advice triples from Yelp reviews (airlines and restaurants) to supervise training, and evaluate recommendations using an eight dimension operational rubric spanning actionability, specificity, feasibility, expected impact, novelty, non-redundancy, bias, and clarity. Across both domains, our approach consistently outperforms prompting-only and single-adapter baselines, yielding higher actionability and specificity while retaining favorable efficiency-quality trade-offs.

</details>


### [39] [PsychēChat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling](https://arxiv.org/abs/2601.12392)
*Zhentao Xia,Yongqi Fan,Yuxiang Chu,Yichao Yin,Liangliang Chen,Tong Ruan,Weiyan Zhang*

Main category: cs.AI

TL;DR: PsychēChat proposes a novel system integrating emotional shift tracking and safety risk analysis in psychological counseling using LLMs for improved performance and safety.


<details>
  <summary>Details</summary>
Motivation: To address the lack of explicit modeling of emotion shifts and risk mitigation in existing counseling language models.

Method: Developed PsychēChat with two key modules: Emotion Management Module (to track emotion shifts) and Risk Control Module (to anticipate reactions and risks), along with two paradigms (Agent Mode and LLM Mode).

Result: PsychēChat outperformed existing models in emotional insight, safety control, and overall performance according to various experiments, including human assessment.

Conclusion: PsychēChat offers a balanced, efficient, and safe approach to enhancing language models for psychological counseling, addressing critical gaps in emotional shift tracking and risk management.

Abstract: Large language models (LLMs) have demonstrated notable advancements in psychological counseling. However, existing models generally do not explicitly model seekers' emotion shifts across counseling sessions, a core focus in classical psychological schools. Moreover, how to align counselor models' responses with these emotion shifts while proactively mitigating safety risks remains underexplored. To bridge these gaps, we propose PsychēChat, which explicitly integrates emotion shift tracking and safety risk analysis for psychological counseling. Specifically, we employ interactive role-playing to synthesize counselor--seeker dialogues, incorporating two modules: Emotion Management Module, to capture seekers' current emotions and emotion shifts; and Risk Control Module, to anticipate seekers' subsequent reactions and identify potential risks. Furthermore, we introduce two modeling paradigms. The Agent Mode structures emotion management, risk control, and counselor responses into a collaborative multi-agent pipeline. The LLM Mode integrates these stages into a unified chain-of-thought for end-to-end inference, balancing efficiency and performance. Extensive experiments, including interactive scoring, dialogue-level evaluation, and human assessment, demonstrate that PsychēChat outperforms existing methods for emotional insight and safety control.

</details>


### [40] [Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation](https://arxiv.org/abs/2601.12410)
*Dingyi Yang,Junqi Zhao,Xue Li,Ce Li,Boyang Li*

Main category: cs.AI

TL;DR: This paper evaluates LLMs' ability to infer knowledge states and intentions, finding that current models perform near randomly compared to humans.


<details>
  <summary>Details</summary>
Motivation: Understanding human intelligence requires investigating the ability to infer knowledge states and intentions, an area where LLM capabilities remain underexplored.

Method: Two tasks are designed to test LLMs: detecting inappropriate knowledge possession by story characters and predicting their actions based on known vs unknown truths.

Result: State-of-the-art LLMs perform near-random on both tasks, displaying a significant gap in comparison to human abilities.

Conclusion: Future LLM research should prioritize improving knowledge state tracking and understanding of intentions to bridge the gap with human cognition.

Abstract: Cognitive anthropology suggests that the distinction of human intelligence lies in the ability to infer other individuals' knowledge states and understand their intentions. In comparison, our closest animal relative, chimpanzees, lack the capacity to do so. With this paper, we aim to evaluate LLM performance in the area of knowledge state tracking and estimation. We design two tasks to test (1) if LLMs can detect when story characters, through their actions, demonstrate knowledge they should not possess, and (2) if LLMs can predict story characters' next actions based on their own knowledge vs. objective truths they do not know. Results reveal that most current state-of-the-art LLMs achieve near-random performance on both tasks, and are substantially inferior to humans. We argue future LLM research should place more weight on the abilities of knowledge estimation and intention understanding.

</details>


### [41] [Large Language Model for OWL Proofs](https://arxiv.org/abs/2601.12444)
*Hui Yang,Jiaoyan Chen,Uli Sattler*

Main category: cs.AI

TL;DR: The study investigates proof generation by LLMs in the context of OWL ontologies, focusing on Extraction, Simplification, and Explanation tasks. It analyzes LLM performance on logical complexity and noisy/incomplete data.


<details>
  <summary>Details</summary>
Motivation: To explore and evaluate the ability of LLMs to generate proofs for reasoning tasks in OWL ontologies, and address the gap in understanding their performance in complex and imperfect conditions.

Method: Developed an automated dataset construction and evaluation framework. This evaluates LLM reasoning across three structured tasks (Extraction, Simplification, Explanation) and an additional Logic Completeness assessment.

Result: Findings include: (1) Strong LLM performance on simpler cases but challenges on complex cases; (2) Logical complexity impacts performance more than representation format; (3) Input noise/incompleteness greatly reduces performance.

Conclusion: LLMs show promise for rigorous reasoning with logic but face limitations under complex or imperfect conditions, highlighting areas for improvement in robust reasoning capabilities.

Abstract: The ability of Large Language Models (LLMs) to perform reasoning tasks such as deduction has been widely investigated in recent years. Yet, their capacity to generate proofs-faithful, human-readable explanations of why conclusions follow-remains largely under explored. In this work, we study proof generation in the context of OWL ontologies, which are widely adopted for representing and reasoning over complex knowledge, by developing an automated dataset construction and evaluation framework. Our evaluation encompassing three sequential tasks for complete proving: Extraction, Simplification, and Explanation, as well as an additional task of assessing Logic Completeness of the premise. Through extensive experiments on widely used reasoning LLMs, we achieve important findings including: (1) Some models achieve overall strong results but remain limited on complex cases; (2) Logical complexity, rather than representation format (formal logic language versus natural language), is the dominant factor shaping LLM performance; and (3) Noise and incompleteness in input data substantially diminish LLMs' performance. Together, these results underscore both the promise of LLMs for explanation with rigorous logics and the gap of supporting resilient reasoning under complex or imperfect conditions. Code and data are available at https://github.com/HuiYang1997/LLMOwlR.

</details>


### [42] [Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck](https://arxiv.org/abs/2601.12499)
*Meiru Zhang,Zaiqiao Meng,Nigel Collier*

Main category: cs.AI

TL;DR: This paper investigates why Large Language Models (LLMs) struggle with multi-hop reasoning and introduces Multi-Focus Attention Instruction (MFAI) to study position bias, demonstrating that LLM performance depends on the least visible evidence (Weakest Link Law).


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges in multi-hop reasoning due to position bias, leading to uncertainties in distinguishing between problems in evidence recognition or synthesis.

Method: The authors propose MFAI to steer attention towards certain positions and analyze multi-hop reasoning performance across 5 models using QA tasks (MuSiQue and NeoQA).

Result: Performance follows the Weakest Link Law, dictated by position, not distance. Accurate MFAI improves accuracy significantly, misleading MFAI confuses in realistic tasks but performs well in controlled settings. System-2 reasoning models overcome these challenges.

Conclusion: LLMs' multi-hop reasoning is hindered by position biases, but with approaches like matched MFAI and System-2 reasoning, performance can be improved, even in noisy, complicated settings.

Abstract: Despite scaling to massive context windows, Large Language Models (LLMs) struggle with multi-hop reasoning due to inherent position bias, which causes them to overlook information at certain positions. Whether these failures stem from an inability to locate evidence (recognition failure) or integrate it (synthesis failure) is unclear. We introduce Multi-Focus Attention Instruction (MFAI), a semantic probe to disentangle these mechanisms by explicitly steering attention towards selected positions. Across 5 LLMs on two multi-hop QA tasks (MuSiQue and NeoQA), we establish the "Weakest Link Law": multi-hop reasoning performance collapses to the performance level of the least visible evidence. Crucially, this failure is governed by absolute position rather than the linear distance between facts (performance variance $<3%$). We further identify a duality in attention steering: while matched MFAI resolves recognition bottlenecks, improving accuracy by up to 11.5% in low-visibility positions, misleading MFAI triggers confusion in real-world tasks but is successfully filtered in synthetic tasks. Finally, we demonstrate that "thinking" models that utilize System-2 reasoning, effectively locate and integrate the required information, matching gold-only baselines even in noisy, long-context settings.

</details>


### [43] [Agentic Reasoning for Large Language Models](https://arxiv.org/abs/2601.12538)
*Tianxin Wei,Ting-Wei Li,Zhining Liu,Xuying Ning,Ze Yang,Jiaru Zou,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Dongqi Fu,Zihao Li,Mengting Ai,Duo Zhou,Wenxuan Bao,Yunzhe Li,Gaotang Li,Cheng Qian,Yu Wang,Xiangru Tang,Yin Xiao,Liri Fang,Hui Liu,Xianfeng Tang,Yuji Zhang,Chi Wang,Jiaxuan You,Heng Ji,Hanghang Tong,Jingrui He*

Main category: cs.AI

TL;DR: This paper surveys agentic reasoning, reframing large language models (LLMs) as autonomous agents that plan, act, and learn within dynamic environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of LLMs in reasoning within open-ended and dynamic settings by organizing and advancing the methods and dimensions of agentic reasoning.

Method: The paper organizes agentic reasoning into three layers: foundational, self-evolving, and collective multi-agent reasoning, distinguishing between in-context and post-training reasoning strategies.

Result: It identifies existing frameworks and applications in diverse fields like science, robotics, and healthcare, while synthesizing agentic reasoning approaches.

Conclusion: The survey provides a unified roadmap for agentic reasoning, highlights its potential, and discusses open challenges such as personalization, scalability, and governance for broader deployment.

Abstract: Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.

</details>


### [44] [MemeLens: Multilingual Multitask VLMs for Memes](https://arxiv.org/abs/2601.12539)
*Ali Ezzat Shahroor,Mohamed Bayan Kmainasi,Abul Hasnat,Dimitar Dimitrov,Giovanni Da San Martino,Preslav Nakov,Firoj Alam*

Main category: cs.AI

TL;DR: This research introduces MemeLens, an advanced unified framework for explaining and analyzing memes by consolidating 38 datasets into 20 shared tasks, overcoming prior task/language silos.


<details>
  <summary>Details</summary>
Motivation: To address the fragmented research on meme analysis across tasks and languages, and to improve generalizability and understanding of multimodal memes.

Method: The researchers developed MemeLens, a unified multilingual and multitask Vision Language Model, consolidating multiple datasets into shared taxonomies for better cross-domain and multilingual generalization.

Result: Empirical analyses reveal that robust meme comprehension needs multimodal data, varies significantly by category, and requires unifying training to avoid over-specialization.

Conclusion: MemeLens bridges gaps in meme research, enabling better generalization across tasks and languages; resources and datasets will be openly shared for further exploration.

Abstract: Memes are a dominant medium for online communication and manipulation because meaning emerges from interactions between embedded text, imagery, and cultural context. Existing meme research is distributed across tasks (hate, misogyny, propaganda, sentiment, humour) and languages, which limits cross-domain generalization. To address this gap we propose MemeLens, a unified multilingual and multitask explanation-enhanced Vision Language Model (VLM) for meme understanding. We consolidate 38 public meme datasets, filter and map dataset-specific labels into a shared taxonomy of $20$ tasks spanning harm, targets, figurative/pragmatic intent, and affect. We present a comprehensive empirical analysis across modeling paradigms, task categories, and datasets. Our findings suggest that robust meme understanding requires multimodal training, exhibits substantial variation across semantic categories, and remains sensitive to over-specialization when models are fine-tuned on individual datasets rather than trained in a unified setting. We will make the experimental resources and datasets publicly available for the community.

</details>


### [45] [Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery](https://arxiv.org/abs/2601.12542)
*Lukas Weidener,Marko Brkić,Mihailo Jovanović,Ritvik Singh,Chiara Baccin,Emre Ulgac,Alex Dobrin,Aakaash Meduri*

Main category: cs.AI

TL;DR: Deep Research introduces an AI system enabling real-time, interactive scientific investigation with faster response times, demonstrating superior performance on computational biology benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing AI systems for scientific discovery are often proprietary, slow, and lack real-time guidance capabilities. Researchers need systems that enable interactive, efficient scientific investigations.

Method: The paper presents Deep Research, a multi-agent system with agents for planning, data analysis, literature search, and novelty detection, operating in both semi-autonomous and fully autonomous modes.

Result: Deep Research achieved state-of-the-art performance on the BixBench computational biology benchmark, surpassing existing baselines by 14-26 percentage points in accuracy.

Conclusion: Deep Research facilitates real-time scientific investigation, addresses operational constraints, and offers practical insights for deploying AI-assisted research workflows.

Abstract: Artificial intelligence systems for scientific discovery have demonstrated remarkable potential, yet existing approaches remain largely proprietary and operate in batch-processing modes requiring hours per research cycle, precluding real-time researcher guidance. This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The architecture comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Two operational modes support different workflows: semi-autonomous mode with selective human checkpoints, and fully autonomous mode for extended investigations. Evaluation on the BixBench computational biology benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points. Analysis of architectural constraints, including open access literature limitations and challenges inherent to automated novelty assessment, informs practical deployment considerations for AI-assisted scientific workflows.

</details>


### [46] [How Clinicians Think and What AI Can Learn From It](https://arxiv.org/abs/2601.12547)
*Dipayan Sengupta,Saumya Panda*

Main category: cs.AI

TL;DR: The paper argues clinical reasoning is more about ordinal decision-making rather than cardinal optimization, and proposes an AI framework aligned with clinicians' decision-making processes.


<details>
  <summary>Details</summary>
Motivation: Clinical AI systems focus on prediction, but real clinical reasoning involves sequential decision-making under uncertainty. Existing models might not align with clinicians' actual fast-and-frugal decision-making methods.

Method: The paper provides theoretical rationale emphasizing ordinal over cardinal approaches, discussing trade-offs, decision-making noise, and robust filtering rules. It suggests AI leverage complex models while respecting ordinal decision processes.

Result: The paper establishes the advantage of robust ordinal-based decision methods over conventional optimization techniques in clinical settings, aligning with real-world clinician reasoning.

Conclusion: Clinical AI should complement clinicians by using complex models judiciously, focusing on ordinal heuristics and employing selective complexity during fragile decisions.

Abstract: Most clinical AI systems operate as prediction engines -- producing labels or risk scores -- yet real clinical reasoning is a time-bounded, sequential control problem under uncertainty. Clinicians interleave information gathering with irreversible actions, guided by regret, constraints and patient values. We argue that the dominant computational substrate of clinician reasoning is not cardinal optimization but ordinal, non-compensatory decision-making: Clinicians frequently rely on fast-and-frugal, lexicographic heuristics (e.g., fast-and-frugal trees) that stop early after checking a small, fixed sequence of cues. We provide a normative rationale for why such algorithms are not merely bounded rationality shortcuts, but can be epistemically preferred in medicine. First, many clinical trade-offs are constructed through human judgment and are only weakly measurable on absolute scales; without strong measurement axioms, only orderings are invariant, motivating an ordinal-by-default stance. Second, preference and signal elicitation are structurally crude: The mapping from truth $\to$ perception $\to$ inference $\to$ recorded variables introduces layered noise, leaving a persistent uncertainty floor. When this 'crudeness' overwhelms the decision margin, plug-in expected-utility optimization becomes brittle (high flip probability under small perturbations), whereas robust dominance/filtering rules ($ε$-dominance, maximin) stabilize decisions.Finally, we outline a clinician-aligned AI blueprint: Use rich models for beliefs and trajectories, but choose actions through robust ordinal rules; treat heuristics as the low-dimensional special case; and deploy AI as 'selective complexity' -- invoked mainly for tie-breaking when decisions are fragile and information has positive expected impact.

</details>


### [47] [Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents](https://arxiv.org/abs/2601.12560)
*Arunkumar V,Gangadharan G. R.,Rajkumar Buyya*

Main category: cs.AI

TL;DR: This paper explores the evolution of Artificial Intelligence towards Agentic AI - autonomous systems combining memory, planning, and collaboration with tools, proposing a taxonomy and addressing challenges.


<details>
  <summary>Details</summary>
Motivation: The shift from passive text-generating AI to Agentic AI requires understanding architectures to enable reliable autonomous systems.

Method: The method involves defining a taxonomy that categorizes AI agents by components like Perception, Brain, Planning, Action, Tool Use, and Collaboration, alongside evaluating environments and discussing current practices.

Result: The paper provides a structured classification system for AI agents, reviews operating environments, evaluation methods, and highlights challenges such as hallucination and infinite loops.

Conclusion: Agentic AI represents a pivotal shift in AI capabilities, with promising applications and significant challenges requiring future research for robust and trustworthy systems.

Abstract: Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.

</details>


### [48] [STEP-LLM: Generating CAD STEP Models from Natural Language with Large Language Models](https://arxiv.org/abs/2601.12641)
*Xiangyu Shi,Junyang Ding,Xu Zhao,Sinong Zhan,Payal Mohapatra,Daniel Quispe,Kojo Welbeck,Jian Cao,Wei Chen,Ping Guo,Qi Zhu*

Main category: cs.AI

TL;DR: The paper introduces STEP-LLM, leveraging novel preprocessing, retrieval-augmented generation, and reinforcement learning to translate text into STEP-based CAD models with improved accuracy and fidelity.


<details>
  <summary>Details</summary>
Motivation: CAD model creation is traditionally labor-intensive and expertise-dependent. This paper aims to enable non-experts to efficiently design manufacturable models using natural language.

Method: STEP-LLM uses preprocessing techniques like depth-first serialization for STEP files, retrieval-augmented generation to ground predictions, and reinforcement learning with a geometric reward for enhanced output quality.

Result: Experiments show STEP-LLM improves geometric fidelity, accuracy, and completeness over Text2CAD, supporting better CAD model generation directly in the STEP format.

Conclusion: STEP-LLM demonstrates the potential for democratizing CAD design through LLMs, expanding accessibility for manufacturing-oriented modeling by utilizing STEP file compatibility effectively.

Abstract: Computer-aided design (CAD) is vital to modern manufacturing, yet model creation remains labor-intensive and expertise-heavy. To enable non-experts to translate intuitive design intent into manufacturable artifacts, recent large language models-based text-to-CAD efforts focus on command sequences or script-based formats like CadQuery. However, these formats are kernel-dependent and lack universality for manufacturing. In contrast, the Standard for the Exchange of Product Data (STEP, ISO 10303) file is a widely adopted, neutral boundary representation (B-rep) format directly compatible with manufacturing, but its graph-structured, cross-referenced nature poses unique challenges for auto-regressive LLMs. To address this, we curate a dataset of ~40K STEP-caption pairs and introduce novel preprocessing tailored for the graph-structured format of STEP, including a depth-first search-based reserialization that linearizes cross-references while preserving locality and chain-of-thought(CoT)-style structural annotations that guide global coherence. We integrate retrieval-augmented generation to ground predictions in relevant examples for supervised fine-tuning, and refine generation quality through reinforcement learning with a specific Chamfer Distance-based geometric reward. Experiments demonstrate consistent gains of our STEP-LLM in geometric fidelity over the Text2CAD baseline, with improvements arising from multiple stages of our framework: the RAG module substantially enhances completeness and renderability, the DFS-based reserialization strengthens overall accuracy, and the RL further reduces geometric discrepancy. Both metrics and visual comparisons confirm that STEP-LLM generates shapes with higher fidelity than Text2CAD. These results show the feasibility of LLM-driven STEP model generation from natural language, showing its potential to democratize CAD design for manufacturing.

</details>


### [49] [MedConsultBench: A Full-Cycle, Fine-Grained, Process-Aware Benchmark for Medical Consultation Agents](https://arxiv.org/abs/2601.12661)
*Chuhan Qiao,Jianghua Huang,Daxing Zhao,Ziding Liu,Yanjun Shen,Bing Cheng,Wei Lin,Kai Wu*

Main category: cs.AI

TL;DR: The paper introduces MedConsultBench, a framework for evaluating medical consultation AI across the entire clinical workflow, highlighting inefficiencies in current models despite high diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of medical AI neglect the end-to-end process and safety critical for real-world use. This gap calls for better benchmarks that reflect structured inquiry and diagnostic rigor.

Method: The paper proposes MedConsultBench, which uses Atomic Information Units (AIUs) and 22 metrics to evaluate information acquisition, diagnostic accuracy, medication safety, and handling of post-prescription Q&A.

Result: Evaluation of 19 large language models showed high diagnostic accuracy but revealed deficiencies in information-gathering efficiency and medication safety.

Conclusion: MedConsultBench is a robust tool to bridge the gap between theoretical medical AI and real-world clinical needs, offering a foundation for better alignment.

Abstract: Current evaluations of medical consultation agents often prioritize outcome-oriented tasks, frequently overlooking the end-to-end process integrity and clinical safety essential for real-world practice. While recent interactive benchmarks have introduced dynamic scenarios, they often remain fragmented and coarse-grained, failing to capture the structured inquiry logic and diagnostic rigor required in professional consultations. To bridge this gap, we propose MedConsultBench, a comprehensive framework designed to evaluate the complete online consultation cycle by covering the entire clinical workflow from history taking and diagnosis to treatment planning and follow-up Q\&A. Our methodology introduces Atomic Information Units (AIUs) to track clinical information acquisition at a sub-turn level, enabling precise monitoring of how key facts are elicited through 22 fine-grained metrics. By addressing the underspecification and ambiguity inherent in online consultations, the benchmark evaluates uncertainty-aware yet concise inquiry while emphasizing medication regimen compatibility and the ability to handle realistic post-prescription follow-up Q\&A via constraint-respecting plan revisions. Systematic evaluation of 19 large language models reveals that high diagnostic accuracy often masks significant deficiencies in information-gathering efficiency and medication safety. These results underscore a critical gap between theoretical medical knowledge and clinical practice ability, establishing MedConsultBench as a rigorous foundation for aligning medical AI with the nuanced requirements of real-world clinical care.

</details>


### [50] [Empowering All-in-Loop Health Management of Spacecraft Power System in the Mega-Constellation Era via Human-AI Collaboration](https://arxiv.org/abs/2601.12667)
*Yi Di,Zhibin Zhao,Fujin Wang,Xue Liu,Jiafeng Tang,Jiaxin Ren,Zhi Zhai,Xuefeng Chen*

Main category: cs.AI

TL;DR: The paper proposes SpaceHMchat, a Human-AI collaboration framework to manage spacecraft power systems (SPS) health in the satellite mega-constellation era, achieving significant performance across various metrics.


<details>
  <summary>Details</summary>
Motivation: The anticipated growth in spacecraft numbers and the complexities of managing satellite mega-constellations (SMC) demand advanced health management solutions for spacecraft power systems (SPS), which are critical but prone to failures.

Method: The authors propose the AUC principle and develop SpaceHMchat, an open-source Human-AI collaboration framework for all-in-loop health management. They also created a hardware-realistic fault injection platform and a corresponding simulation model for experimental verification.

Result: SpaceHMchat demonstrates exceptional performance with 100% accuracy in logical reasoning, a 99%+ success rate in anomaly detection tool usage, over 90% precision in fault localization, and efficient maintenance decision-making (knowledge base search under 3 mins).

Conclusion: The study offers a cutting-edge solution for SPS health management in the SMC era by combining Human-AI collaboration and experimental validation, accompanied by releasing a comprehensive dataset for future research.

Abstract: It is foreseeable that the number of spacecraft will increase exponentially, ushering in an era dominated by satellite mega-constellations (SMC). This necessitates a focus on energy in space: spacecraft power systems (SPS), especially their health management (HM), given their role in power supply and high failure rates. Providing health management for dozens of SPS and for thousands of SPS represents two fundamentally different paradigms. Therefore, to adapt the health management in the SMC era, this work proposes a principle of aligning underlying capabilities (AUC principle) and develops SpaceHMchat, an open-source Human-AI collaboration (HAIC) framework for all-in-loop health management (AIL HM). SpaceHMchat serves across the entire loop of work condition recognition, anomaly detection, fault localization, and maintenance decision making, achieving goals such as conversational task completion, adaptive human-in-the-loop learning, personnel structure optimization, knowledge sharing, efficiency enhancement, as well as transparent reasoning and improved interpretability. Meanwhile, to validate this exploration, a hardware-realistic fault injection experimental platform is established, and its simulation model is built and open-sourced, both fully replicating the real SPS. The corresponding experimental results demonstrate that SpaceHMchat achieves excellent performance across 23 quantitative metrics, such as 100% conclusion accuracy in logical reasoning of work condition recognition, over 99% success rate in anomaly detection tool invocation, over 90% precision in fault localization, and knowledge base search time under 3 minutes in maintenance decision-making. Another contribution of this work is the release of the first-ever AIL HM dataset of SPS. This dataset contains four sub-datasets, involving 4 types of AIL HM sub-tasks, 17 types of faults, and over 700,000 timestamps.

</details>


### [51] [Logic-Guided Multistage Inference for Explainable Multidefendant Judgment Prediction](https://arxiv.org/abs/2601.12688)
*Xu Zhang,Qinghua Wang,Mengyang Zhao,Fang Wang,Cunquan Qu*

Main category: cs.AI

TL;DR: This paper proposes a masked multistage inference framework (MMSI) to address fairness and role differentiation in multidefendant cases using AI.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve fairness in judicial cases involving multiple defendants by addressing challenges in role differentiation and enhancing AI-based analyses of sentencing logic.

Method: The authors incorporate sentencing logic into a Transformer-based encoder with an oriented masking mechanism and a comparative data construction strategy. Predicted guilt labels are integrated into a regression model to strengthen role-based culpability differentiation.

Result: The MMSI framework, evaluated on the IMLJP dataset for intentional injury cases, demonstrated significant accuracy enhancements, surpassing baseline models in correctly distinguishing roles like principals and accomplices.

Conclusion: The proposed MMSI framework offers an effective AI-driven solution to improve fairness and interpretability in multidefendant cases, contributing to intelligent judicial systems. Publicly available code supports further research and application.

Abstract: Crime disrupts societal stability, making law essential for balance. In multidefendant cases, assigning responsibility is complex and challenges fairness, requiring precise role differentiation. However, judicial phrasing often obscures the roles of the defendants, hindering effective AI-driven analyses. To address this issue, we incorporate sentencing logic into a pretrained Transformer encoder framework to enhance the intelligent assistance in multidefendant cases while ensuring legal interpretability. Within this framework an oriented masking mechanism clarifies roles and a comparative data construction strategy improves the model's sensitivity to culpability distinctions between principals and accomplices. Predicted guilt labels are further incorporated into a regression model through broadcasting, consolidating crime descriptions and court views. Our proposed masked multistage inference (MMSI) framework, evaluated on the custom IMLJP dataset for intentional injury cases, achieves significant accuracy improvements, outperforming baselines in role-based culpability differentiation. This work offers a robust solution for enhancing intelligent judicial systems, with publicly code available.

</details>


### [52] [Neurosymbolic LoRA: Why and When to Tune Weights vs. Rewrite Prompts](https://arxiv.org/abs/2601.12711)
*Kevin Wang,Neel P. Bhatt,Cong Liu,Junbo Li,Runjin Chen,Yihan Xi,Timothy Barclay,Alvaro Velasquez,Ufuk Topcu,Zhangyang Wang*

Main category: cs.AI

TL;DR: The paper proposes a neurosymbolic LoRA framework combining numerical and symbolic updates to enhance large language models, showing improved adaptability and performance.


<details>
  <summary>Details</summary>
Motivation: To leverage both numerical and symbolic methods for fine-tuning language models, enabling better factual reconstruction and flexible style control.

Method: The framework dynamically combines numerical LoRA for parameter updates and symbolic TextGrad for prompt editing; symbolic transformations are externalized, and edited prompts are used as training data.

Result: Neurosymbolic LoRA consistently outperformed numerical-only and symbolic-only approaches, improving adaptability and performance in experiments across multiple language models.

Conclusion: Interleaving numerical and symbolic updates enhances language model fine-tuning, achieving higher versatility and effectiveness.

Abstract: Large language models (LLMs) can be adapted either through numerical updates that alter model parameters or symbolic manipulations that work on discrete prompts or logical constraints. While numerical fine-tuning excels at injecting new factual knowledge, symbolic updates offer flexible control of style and alignment without retraining. We introduce a neurosymbolic LoRA framework that dynamically combines these two complementary strategies. Specifically, we present a unified monitoring signal and a reward-based classifier to decide when to employ LoRA for deeper factual reconstruction and when to apply TextGrad for token-level edits. Our approach remains memory-efficient by offloading the symbolic transformations to an external LLM only when needed. Additionally, the refined prompts produced during symbolic editing serve as high-quality, reusable training data, an important benefit in data-scarce domains like mathematical reasoning. Extensive experiments across multiple LLM backbones show that neurosymbolic LoRA consistently outperforms purely numerical or purely symbolic baselines, demonstrating superior adaptability and improved performance. Our findings highlight the value of interleaving numerical and symbolic updates to unlock a new level of versatility in language model fine-tuning.

</details>


### [53] [Teaching Large Reasoning Models Effective Reflection](https://arxiv.org/abs/2601.12720)
*Hanbin Wang,Jingwei Song,Jinpeng Li,Qi Zhu,Fei Mi,Ganqu Cui,Yasheng Wang,Lifeng Shang*

Main category: cs.AI

TL;DR: The paper addresses issues of superficial reflections in Large Reasoning Models (LRMs) and proposes methods to improve reflective reasoning through Self-Critique Fine-Tuning (SCFT) and Reinforcement Learning with Reflection Rewards (RLERR).


<details>
  <summary>Details</summary>
Motivation: Superficial reflections in Large Reasoning Models often fail to improve reasoning quality while incurring computational costs, necessitating better mechanisms to enhance reflective reasoning.

Method: The authors present SCFT, which uses self-generated critiques with a quality filtering process for fine-tuning models, and RLERR, a reinforcement learning approach that uses reflections from SCFT to reward effective self-correction.

Result: SCFT and RLERR demonstrate superior performance in reasoning accuracy and reflection quality on benchmarks AIME2024 and AIME2025, outperforming current state-of-the-art methods.

Conclusion: The methods proposed in this paper provide an effective way to enhance the reflective reasoning capabilities of LRMs, addressing superficial reflections and improving overall model performance.

Abstract: Large Reasoning Models (LRMs) have recently shown impressive performance on complex reasoning tasks, often by engaging in self-reflective behaviors such as self-critique and backtracking. However, not all reflections are beneficial-many are superficial, offering little to no improvement over the original answer and incurring computation overhead. In this paper, we identify and address the problem of superficial reflection in LRMs. We first propose Self-Critique Fine-Tuning (SCFT), a training framework that enhances the model's reflective reasoning ability using only self-generated critiques. SCFT prompts models to critique their own outputs, filters high-quality critiques through rejection sampling, and fine-tunes the model using a critique-based objective. Building on this strong foundation, we further introduce Reinforcement Learning with Effective Reflection Rewards (RLERR). RLERR leverages the high-quality reflections initialized by SCFT to construct reward signals, guiding the model to internalize the self-correction process via reinforcement learning. Experiments on two challenging benchmarks, AIME2024 and AIME2025, show that SCFT and RLERR significantly improve both reasoning accuracy and reflection quality, outperforming state-of-the-art baselines. All data and codes are available at https://github.com/wanghanbinpanda/SCFT.

</details>


### [54] [Vision Language Models for Optimization-Driven Intent Processing in Autonomous Networks](https://arxiv.org/abs/2601.12744)
*Tasnim Ahmed,Yifan Zhu,Salimur Choudhury*

Main category: cs.AI

TL;DR: This paper assesses Vision-Language Models (VLMs) for converting annotated network sketches into optimization code for Intent-Based Networking. It evaluates their performance on a benchmark of 85 optimization problems across 17 categories.


<details>
  <summary>Details</summary>
Motivation: The study investigates whether VLMs can bridge the gap between network operators' use of diagrams and the need for generating optimization code for complex networking tasks.

Method: Four Vision-Language Models are tested under different prompting strategies on a benchmark dataset, with comparisons drawn between multimodal and text-only inputs.

Result: Visual parameter extraction reduces execution success by 12-21 percentage points, and open-source models underperform compared to closed-source ones, highlighting significant limitations in current VLM capabilities.

Conclusion: VLMs have foundational capabilities but fall short in optimization code generation when handling multimodal inputs, necessitating further model refinement for practical use in Intent-Based Networking systems.

Abstract: Intent-Based Networking (IBN) allows operators to specify high-level network goals rather than low-level configurations. While recent work demonstrates that large language models can automate configuration tasks, a distinct class of intents requires generating optimization code to compute provably optimal solutions for traffic engineering, routing, and resource allocation. Current systems assume text-based intent expression, requiring operators to enumerate topologies and parameters in prose. Network practitioners naturally reason about structure through diagrams, yet whether Vision-Language Models (VLMs) can process annotated network sketches into correct optimization code remains unexplored. We present IntentOpt, a benchmark of 85 optimization problems across 17 categories, evaluating four VLMs (GPT-5-Mini, Claude-Haiku-4.5, Gemini-2.5-Flash, Llama-3.2-11B-Vision) under three prompting strategies on multimodal versus text-only inputs. Our evaluation shows that visual parameter extraction reduces execution success by 12-21 percentage points (pp), with GPT-5-Mini dropping from 93% to 72%. Program-of-thought prompting decreases performance by up to 13 pp, and open-source models lag behind closed-source ones, with Llama-3.2-11B-Vision reaching 18% compared to 75% for GPT-5-Mini. These results establish baseline capabilities and limitations of current VLMs for optimization code generation within an IBN system. We also demonstrate practical feasibility through a case study that deploys VLM-generated code to network testbed infrastructure using Model Context Protocol.

</details>


### [55] [VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension](https://arxiv.org/abs/2601.12781)
*Hyejin Park,Junhyuk Kwon,Suha Kwak,Jungseul Ok*

Main category: cs.AI

TL;DR: The paper introduces VIRO, a framework for Referring Expression Comprehension (REC) that incorporates verification mechanisms to improve accuracy and handle no-target cases.


<details>
  <summary>Details</summary>
Motivation: To address cascading errors in neuro-symbolic REC approaches caused by the assumption that intermediate reasoning steps are always accurate.

Method: Embedding operator-level verifiers within reasoning steps to validate outputs like object existence or spatial relationships during REC.

Result: VIRO achieves 61.1% balanced accuracy on target-present and no-target settings, generalizes to real-world data, and demonstrates high computational efficiency.

Conclusion: The VIRO framework improves interpretability, reliability, and scalability in REC, reducing false-positive rates and enhancing computational throughput.

Abstract: Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural-language query. Recent neuro-symbolic REC approaches leverage large language models (LLMs) and vision-language models (VLMs) to perform compositional reasoning, decomposing queries 4 structured programs and executing them step-by-step. While such approaches achieve interpretable reasoning and strong zero-shot generalization, they assume that intermediate reasoning steps are accurate. However, this assumption causes cascading errors: false detections and invalid relations propagate through the reasoning chain, yielding high-confidence false positives even when no target is present in the image. To address this limitation, we introduce Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight operator-level verifiers within reasoning steps. Each operator executes and validates its output, such as object existence or spatial relationship, thereby allowing the system to robustly handle no-target cases when verification conditions are not met. Our framework achieves state-of-the-art performance, reaching 61.1% balanced accuracy across target-present and no-target settings, and demonstrates generalization to real-world egocentric data. Furthermore, VIRO shows superior computational efficiency in terms of throughput, high reliability with a program failure rate of less than 0.3%, and scalability through decoupled program generation from execution.

</details>


### [56] [SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability](https://arxiv.org/abs/2601.12804)
*Hanwei Zhang,Luo Cheng,Rui Wen,Yang Zhang,Lijun Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: This paper introduces SL-CBM, a concept bottleneck model with an added focus on spatial alignment for transparent and interpretable machine learning.


<details>
  <summary>Details</summary>
Motivation: Existing Concept Bottleneck Models (CBMs) struggle to spatially align human-understandable concepts with meaningful regions in images, limiting their interpretability and trustworthiness, especially in critical scenarios.

Method: SL-CBM, the proposed method, enforces locality faithfulness by generating spatially coherent saliency maps. It incorporates a 1x1 convolutional layer and cross-attention mechanisms to improve alignment between human concepts, image regions, and the model's predictions.

Result: Extensive experiments show that SL-CBM significantly enhances locality faithfulness, explanation quality, and debugging capability while maintaining competitive classification accuracy.

Conclusion: SL-CBM improves concept-based interpretability by integrating spatial explainability into CBMs, addressing their limitations and setting a benchmark for interpretable AI systems in high-stakes domains.

Abstract: Explainable AI (XAI) is crucial for building transparent and trustworthy machine learning systems, especially in high-stakes domains. Concept Bottleneck Models (CBMs) have emerged as a promising ante-hoc approach that provides interpretable, concept-level explanations by explicitly modeling human-understandable concepts. However, existing CBMs often suffer from poor locality faithfulness, failing to spatially align concepts with meaningful image regions, which limits their interpretability and reliability. In this work, we propose SL-CBM (CBM with Semantic Locality), a novel extension that enforces locality faithfulness by generating spatially coherent saliency maps at both concept and class levels. SL-CBM integrates a 1x1 convolutional layer with a cross-attention mechanism to enhance alignment between concepts, image regions, and final predictions. Unlike prior methods, SL-CBM produces faithful saliency maps inherently tied to the model's internal reasoning, facilitating more effective debugging and intervention. Extensive experiments on image datasets demonstrate that SL-CBM substantially improves locality faithfulness, explanation quality, and intervention efficacy while maintaining competitive classification accuracy. Our ablation studies highlight the importance of contrastive and entropy-based regularization for balancing accuracy, sparsity, and faithfulness. Overall, SL-CBM bridges the gap between concept-based reasoning and spatial explainability, setting a new standard for interpretable and trustworthy concept-based models.

</details>


### [57] [MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction](https://arxiv.org/abs/2601.12822)
*Wenqi Zhang,Yulin Shen,Changyue Jiang,Jiarun Dai,Geng Hong,Xudong Pan*

Main category: cs.AI

TL;DR: The paper introduces MirrorGuard, a defense framework that improves the security of Computer Use Agents (CUAs) in GUI tasks by intercepting unsafe actions through a simulated training pipeline.


<details>
  <summary>Details</summary>
Motivation: To address security risks from malicious instructions or visual prompt injections in autonomous GUIs tasked by large foundation models.

Method: MirrorGuard employs a neural-symbolic simulation pipeline to train CUAs in identifying and correcting unsafe reasoning using a text-based simulated GUI environment.

Result: In testing, MirrorGuard reduced unsafe actions significantly (unsafe rate from 66.5% to 13.0%) compared to existing defenses like GuardAgent.

Conclusion: Simulated, neural-symbolic training improves real-world security of CUAs while preserving their utility, outperforming existing security frameworks.

Abstract: Large foundation models are integrated into Computer Use Agents (CUAs), enabling autonomous interaction with operating systems through graphical user interfaces (GUIs) to perform complex tasks. This autonomy introduces serious security risks: malicious instructions or visual prompt injections can trigger unsafe reasoning and cause harmful system-level actions. Existing defenses, such as detection-based blocking, prevent damage but often abort tasks prematurely, reducing agent utility. In this paper, we present MirrorGuard, a plug-and-play defense framework that uses simulation-based training to improve CUA security in the real world. To reduce the cost of large-scale training in operating systems, we propose a novel neural-symbolic simulation pipeline, which generates realistic, high-risk GUI interaction trajectories entirely in a text-based simulated environment, which captures unsafe reasoning patterns and potential system hazards without executing real operations. In the simulation environment, MirrorGuard learns to intercept and rectify insecure reasoning chains of CUAs before they produce and execute unsafe actions. In real-world testing, extensive evaluations across diverse benchmarks and CUA architectures show that MirrorGuard significantly mitigates security risks. For instance, on the ByteDance UI-TARS system, it reduces the unsafe rate from 66.5% to 13.0% while maintaining a marginal false refusal rate (FRR). In contrast, the state-of-the-art GuardAgent only achieves a reduction to 53.9% and suffers from a 15.4% higher FRR. Our work proves that simulation-derived defenses can provide robust, real-world protection while maintaining the fundamental utility of the agent. Our code and model are publicly available at https://bmz-q-q.github.io/MirrorGuard/.

</details>


### [58] [SCULPT: Constraint-Guided Pruned MCTS that Carves Efficient Paths for Mathematical Reasoning](https://arxiv.org/abs/2601.12842)
*Qitong Fang,Haotian Li,Xu Wang*

Main category: cs.AI

TL;DR: This paper introduces SCULPT, a constraint-guided approach to improve the reasoning accuracy and efficiency of automated workflows in large language models by steering the search through plausible paths using domain-aware scoring.


<details>
  <summary>Details</summary>
Motivation: Enhancing problem-solving capabilities of large language models by promoting ordered and efficient exploration in automated workflows.

Method: SCULPT uses constraint-guided Monte Carlo Tree Search with symbolic checks and structural pattern guidance to score and prune actions for plausible reasoning.

Result: SCULPT showed stable improvements in accuracy across multiple datasets, demonstrated executor transferability, and maintained reasoning efficiency and stability.

Conclusion: Domain-aware constraints contribute to better accuracy, efficiency, and stability in reasoning within automated agent workflows.

Abstract: Automated agent workflows can enhance the problem-solving ability of large language models (LLMs), but common search strategies rely on stochastic exploration and often traverse implausible branches. This occurs because current pipelines sample candidate steps from generic prompts or learned policies with weak domain priors, yielding near-random walks over operators, units, and formats. To promote ordered exploration, this paper introduces SCULPT, a constraint-guided approach for Monte Carlo Tree Search (MCTS) that integrates domain-aware scoring into selection, expansion, simulation, and backpropagation. SCULPT scores and prunes actions using a combination of symbolic checks (dimensional consistency, type compatibility, magnitude sanity, depth control, and diversity) and structural pattern guidance, thereby steering the search toward plausible reasoning paths. Under matched LLM configurations, SCULPT yields stable improvements on multiple datasets; additional results with GPT-5.2 assess executor transferability and performance on frontier reasoning models. Overall, domain-aware constraints can improve accuracy while maintaining efficiency and reasoning stability.

</details>


### [59] [Mining Citywide Dengue Spread Patterns in Singapore Through Hotspot Dynamics from Open Web Data](https://arxiv.org/abs/2601.12856)
*Liping Huang,Gaoxi Xiao,Stefan Ma,Hechang Chen,Shisong Tang,Flora Salim*

Main category: cs.AI

TL;DR: This paper introduces a framework leveraging dengue case data to predict disease spread, incorporating human mobility patterns for effective hotspot forecasting.


<details>
  <summary>Details</summary>
Motivation: To enhance urban dengue control by predicting hotspots of transmission for proactive intervention, utilizing publicly available data and understanding mobility-driven disease spread.

Method: A novel framework analyzing dengue case data to infer latent transmission links via gradient descent, validated using hotspot forecasting and network stability across weeks.

Result: The approach successfully achieved an average F-score of 0.79 in predicting dengue hotspots in Singapore from 2013-2018 and 2020, aligning inferred transmission with commuting flows.

Conclusion: This scalable, cost-effective framework transforms dengue case data into predictive insights, improving epidemic modeling, public health planning, and urban resilience efforts.

Abstract: Dengue, a mosquito-borne disease, continues to pose a persistent public health challenge in urban areas, particularly in tropical regions such as Singapore. Effective and affordable control requires anticipating where transmission risks are likely to emerge so that interventions can be deployed proactively rather than reactively. This study introduces a novel framework that uncovers and exploits latent transmission links between urban regions, mined directly from publicly available dengue case data. Instead of treating cases as isolated reports, we model how hotspot formation in one area is influenced by epidemic dynamics in neighboring regions. While mosquito movement is highly localized, long-distance transmission is often driven by human mobility, and in our case study, the learned network aligns closely with commuting flows, providing an interpretable explanation for citywide spread. These hidden links are optimized through gradient descent and used not only to forecast hotspot status but also to verify the consistency of spreading patterns, by examining the stability of the inferred network across consecutive weeks. Case studies on Singapore during 2013-2018 and 2020 show that four weeks of hotspot history are sufficient to achieve an average F-score of 0.79. Importantly, the learned transmission links align with commuting flows, highlighting the interpretable interplay between hidden epidemic spread and human mobility. By shifting from simply reporting dengue cases to mining and validating hidden spreading dynamics, this work transforms open web-based case data into a predictive and explanatory resource. The proposed framework advances epidemic modeling while providing a scalable, low-cost tool for public health planning, early intervention, and urban resilience.

</details>


### [60] [Human Emotion Verification by Action Languages via Answer Set Programming](https://arxiv.org/abs/2601.12912)
*Andreas Brännström,Juan Carlos Nieves*

Main category: cs.AI

TL;DR: The paper introduces the action language C-MT, based on ASP and transition systems, for representing human mental state dynamics and enabling controlled reasoning about them.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for controlled agent behaviors and to restrict unwanted mental side-effects of actions by formalizing and reasoning about human mental states and their evolution.

Method: The paper builds C-MT using ASP and transition systems, incorporating psychological theories like Appraisal Theory of Emotion. It introduces concepts like 'forbids to cause' and specializes in mental state dynamics modeling, translating principles of mental change into transition constraints and invariance properties.

Result: Through transition systems and trajectories, C-MT supports controlled reasoning about human mental state evolution, allowing the analysis and comparison of mental state transitions under diverse psychological principles.

Conclusion: C-MT proves useful for the reasoning and design of controlled dynamics in human mental states, with applications like emotion verification, offering a framework for understanding mental state trajectories.

Abstract: In this paper, we introduce the action language C-MT (Mind Transition Language). It is built on top of answer set programming (ASP) and transition systems to represent how human mental states evolve in response to sequences of observable actions. Drawing on well-established psychological theories, such as the Appraisal Theory of Emotion, we formalize mental states, such as emotions, as multi-dimensional configurations. With the objective to address the need for controlled agent behaviors and to restrict unwanted mental side-effects of actions, we extend the language with a novel causal rule, forbids to cause, along with expressions specialized for mental state dynamics, which enables the modeling of principles for valid transitions between mental states. These principles of mental change are translated into transition constraints, and properties of invariance, which are rigorously evaluated using transition systems in terms of so-called trajectories. This enables controlled reasoning about the dynamic evolution of human mental states. Furthermore, the framework supports the comparison of different dynamics of change by analyzing trajectories that adhere to different psychological principles. We apply the action language to design models for emotion verification. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [61] [MagicGUI-RMS: A Multi-Agent Reward Model System for Self-Evolving GUI Agents via Automated Feedback Reflux](https://arxiv.org/abs/2601.13060)
*Zecheng Li,Zhihui Cao,Wenke Huang,Yudong Zhang,Keying Qi,Rui Wang,Zeyu Zheng,Jian Zhao,Hao Zhu,Hengxin Wu,Yuran Wang,Guitao Fan,Guokun Wu,Yicong Liu,Zhilin Gao,Haikun Xu,He Yang,Minqi Xiang,Xingyu Liu,Zuojian Wang*

Main category: cs.AI

TL;DR: MagicGUI-RMS introduces a system to autonomously evaluate GUI agent trajectories, refine behavior, and scale learning through adaptive reward modeling and automated data construction.


<details>
  <summary>Details</summary>
Motivation: Current GUI agents face challenges in trajectory evaluation and scaling high-quality training data, which hinders their adaptability and autonomous improvement.

Method: MagicGUI-RMS combines Domain-Specific Reward Model (DS-RM) with General-Purpose Reward Model (GP-RM) for thorough evaluations and generalization across GUI tasks. It employs an automated data pipeline for scalable, balanced reward dataset construction.

Result: Extensive experiments demonstrate substantial improvements in task accuracy and behavioral robustness for GUI agents using MagicGUI-RMS.

Conclusion: MagicGUI-RMS proves to be an effective foundation for self-improving GUI agents driven by reward-based adaptation.

Abstract: Graphical user interface (GUI) agents are rapidly progressing toward autonomous interaction and reliable task execution across diverse applications. However, two central challenges remain unresolved: automating the evaluation of agent trajectories and generating high-quality training data at scale to enable continual improvement. Existing approaches often depend on manual annotation or static rule-based verification, which restricts scalability and limits adaptability in dynamic environments. We present MagicGUI-RMS, a multi-agent reward model system that delivers adaptive trajectory evaluation, corrective feedback, and self-evolving learning capabilities. MagicGUI-RMS integrates a Domain-Specific Reward Model (DS-RM) with a General-Purpose Reward Model (GP-RM), enabling fine-grained action assessment and robust generalization across heterogeneous GUI tasks. To support reward learning at scale, we design a structured data construction pipeline that automatically produces balanced and diverse reward datasets, effectively reducing annotation costs while maintaining sample fidelity. During execution, the reward model system identifies erroneous actions, proposes refined alternatives, and continuously enhances agent behavior through an automated data-reflux mechanism. Extensive experiments demonstrate that MagicGUI-RMS yields substantial gains in task accuracy, behavioral robustness. These results establish MagicGUI-RMS as a principled and effective foundation for building self-improving GUI agents driven by reward-based adaptation.

</details>


### [62] [Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward](https://arxiv.org/abs/2601.13122)
*Gourab K Patro,Himanshi Agrawal,Himanshu Gharat,Supriya Panigrahi,Nim Sherpa,Vishal Vaddina,Dagnachew Birru*

Main category: cs.AI

TL;DR: This paper examines vulnerabilities in modern general-purpose AI systems and their adherence to Responsible AI principles, proposing a new framework called C2V2 (Control, Consistency, Value, Veracity) for enhancing trustworthiness.


<details>
  <summary>Details</summary>
Motivation: Current general-purpose AI systems are gaining popularity due to their broad capabilities but exhibit risks like hallucinations, bias, and unreliability, necessitating a fresh approach to Responsible AI.

Method: The paper reviews eight Responsible AI principles, explores their application to general-purpose AI versus traditional AI, identifies challenges caused by high Degree of Freedom in general-purpose AI, and proposes the C2V2 framework as a guiding principle.

Result: The work highlights the inadequacy of traditional Responsible AI principles for general-purpose AI and introduces the C2V2 desiderata framework to improve AI alignment with these principles.

Conclusion: The authors conclude that designing future general-purpose AI requires formalizing Responsible AI requirements along the C2V2 dimensions and combining technical solutions to systematically address vulnerabilities.

Abstract: Modern general-purpose AI systems made using large language and vision models, are capable of performing a range of tasks like writing text articles, generating and debugging codes, querying databases, and translating from one language to another, which has made them quite popular across industries. However, there are risks like hallucinations, toxicity, and stereotypes in their output that make them untrustworthy. We review various risks and vulnerabilities of modern general-purpose AI along eight widely accepted responsible AI (RAI) principles (fairness, privacy, explainability, robustness, safety, truthfulness, governance, and sustainability) and compare how they are non-existent or less severe and easily mitigable in traditional task-specific counterparts. We argue that this is due to the non-deterministically high Degree of Freedom in output (DoFo) of general-purpose AI (unlike the deterministically constant or low DoFo of traditional task-specific AI systems), and there is a need to rethink our approach to RAI for general-purpose AI. Following this, we derive C2V2 (Control, Consistency, Value, Veracity) desiderata to meet the RAI requirements for future general-purpose AI systems, and discuss how recent efforts in AI alignment, retrieval-augmented generation, reasoning enhancements, etc. fare along one or more of the desiderata. We believe that the goal of developing responsible general-purpose AI can be achieved by formally modeling application- or domain-dependent RAI requirements along C2V2 dimensions, and taking a system design approach to suitably combine various techniques to meet the desiderata.

</details>


### [63] [Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching](https://arxiv.org/abs/2601.13186)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: This paper addresses prompt injection in large language models, introducing a new scoring metric (TIVS-O), combining security robustness and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To resolve vulnerabilities with prompt injection attacks in multi-agent LLM systems, focusing on balancing strict defence measures with interpretability and efficiency.

Method: A nested learning architecture integrating semantic similarity-based caching, observability score, and memory-augmented agent pipelines to analyze injection-focused scenarios.

Result: Achieved secure responses with no high-risk breaches, 41.6% fewer LLM calls, reduced latency, energy use, and carbon emissions, and optimal trade-offs between mitigation and transparency.

Conclusion: Memory-augmented systems with transparency-driven evaluation improve LLM security, are cost-effective, environmentally friendly, and suitable for production deployment without retraining the models.

Abstract: Prompt injection remains a central obstacle to the safe deployment of large language models, particularly in multi-agent settings where intermediate outputs can propagate or amplify malicious instructions. Building on earlier work that introduced a four-metric Total Injection Vulnerability Score (TIVS), this paper extends the evaluation framework with semantic similarity-based caching and a fifth metric (Observability Score Ratio) to yield TIVS-O, investigating how defence effectiveness interacts with transparency in a HOPE-inspired Nested Learning architecture. The proposed system combines an agentic pipeline with Continuum Memory Systems that implement semantic similarity-based caching across 301 synthetically generated injection-focused prompts drawn from ten attack families, while a fourth agent performs comprehensive security analysis using five key performance indicators. In addition to traditional injection metrics, OSR quantifies the richness and clarity of security-relevant reasoning exposed by each agent, enabling an explicit analysis of trade-offs between strict mitigation and auditability. Experiments show that the system achieves secure responses with zero high-risk breaches, while semantic caching delivers substantial computational savings, achieving a 41.6% reduction in LLM calls and corresponding decreases in latency, energy consumption, and carbon emissions. Five TIVS-O configurations reveal optimal trade-offs between mitigation strictness and forensic transparency. These results indicate that observability-aware evaluation can reveal non-monotonic effects within multi-agent pipelines and that memory-augmented agents can jointly maximize security robustness, real-time performance, operational cost savings, and environmental sustainability without modifying underlying model weights, providing a production-ready pathway for secure and green LLM deployments.

</details>


### [64] [Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic Dialogues](https://arxiv.org/abs/2601.13206)
*Neil K. R. Sehgal,Sharath Chandra Guntuku,Lyle Ungar*

Main category: cs.AI

TL;DR: The paper examines time awareness in Large Language Models (LLMs) through simulated negotiations, revealing that LLMs struggle with temporal tracking in real-time but perform well under turn-based limits.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the limitations of current LLMs in time-sensitive communication scenarios, as temporal tracking is critical for real-world applications like negotiations and therapy.

Method: Simulated negotiations were conducted between paired LLM agents, comparing performance under a control condition (global time limit) and a time-aware condition (remaining-time updates during each turn). Turn-based limits were also evaluated.

Result: LLMs achieved significantly higher deal closure rates and offer acceptances in the time-aware condition, highlighting their failure to internally track time. However, under turn-based limits, the models performed near-perfectly, indicating the issue lies in temporal awareness rather than strategic reasoning.

Conclusion: LLMs lack inherent time awareness, which limits their effectiveness in time-sensitive tasks. Addressing this issue is critical for their deployment in many real-world applications.

Abstract: Large Language Models (LLMs) generate text token-by-token in discrete time, yet real-world communication, from therapy sessions to business negotiations, critically depends on continuous time constraints. Current LLM architectures and evaluation protocols rarely test for temporal awareness under real-time deadlines. We use simulated negotiations between paired agents under strict deadlines to investigate how LLMs adjust their behavior in time-sensitive settings. In a control condition, agents know only the global time limit. In a time-aware condition, they receive remaining-time updates at each turn. Deal closure rates are substantially higher (32\% vs. 4\% for GPT-5.1) and offer acceptances are sixfold higher in the time-aware condition than in the control, suggesting LLMs struggle to internally track elapsed time. However, the same LLMs achieve near-perfect deal closure rates ($\geq$95\%) under turn-based limits, revealing the failure is in temporal tracking rather than strategic reasoning. These effects replicate across negotiation scenarios and models, illustrating a systematic lack of LLM time awareness that will constrain LLM deployment in many time-sensitive applications.

</details>


### [65] [RAG: A Random-Forest-Based Generative Design Framework for Uncertainty-Aware Design of Metamaterials with Complex Functional Response Requirements](https://arxiv.org/abs/2601.13233)
*Bolin Chen,Dex Doksoo Lee,Wei "Wayne'' Chen,Wei Chen*

Main category: cs.AI

TL;DR: The paper introduces RAG, a data-efficient random-forest-based generative approach, for inverse designing of metamaterials functional responses overcoming current challenges such as high dimensionality and uncertainty.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in inverse designing of metamaterials such as managing high-dimensional, nonlinear functional responses, avoiding infeasible designs, and handling insufficient data while fulfilling specific design requirements.

Method: The proposed RAG framework utilizes random forests for small-data compatibility, ensemble-based trust quantification, and conditional likelihood estimation to generate feasible one-to-many designs.

Result: RAG demonstrated effectiveness on case studies including acoustic and mechanical metamaterials using limited samples and showed superior data-efficiency compared to neural networks.

Conclusion: RAG facilitates efficient, trustworthy inverse design of functional responses under complex requirements, making it applicable beyond metamaterials.

Abstract: Metamaterials design for advanced functionality often entails the inverse design on nonlinear and condition-dependent responses (e.g., stress-strain relation and dispersion relation), which are described by continuous functions. Most existing design methods focus on vector-valued responses (e.g., Young's modulus and bandgap width), while the inverse design of functional responses remains challenging due to their high-dimensionality, the complexity of accommodating design requirements in inverse-design frameworks, and non-existence or non-uniqueness of feasible solutions. Although generative design approaches have shown promise, they are often data-hungry, handle design requirements heuristically, and may generate infeasible designs without uncertainty quantification. To address these challenges, we introduce a RAndom-forest-based Generative approach (RAG). By leveraging the small-data compatibility of random forests, RAG enables data-efficient predictions of high-dimensional functional responses. During the inverse design, the framework estimates the likelihood through the ensemble which quantifies the trustworthiness of generated designs while reflecting the relative difficulty across different requirements. The one-to-many mapping is addressed through single-shot design generation by sampling from the conditional likelihood. We demonstrate RAG on: 1) acoustic metamaterials with prescribed partial passbands/stopbands, and 2) mechanical metamaterials with targeted snap-through responses, using 500 and 1057 samples, respectively. Its data-efficiency is benchmarked against neural networks on a public mechanical metamaterial dataset with nonlinear stress-strain relations. Our framework provides a lightweight, trustworthy pathway to inverse design involving functional responses, expensive simulations, and complex design requirements, beyond metamaterials.

</details>


### [66] [CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning](https://arxiv.org/abs/2601.13262)
*Eric Onyame,Akash Ghosh,Subhadip Baidya,Sriparna Saha,Xiuying Chen,Chirag Agarwal*

Main category: cs.AI

TL;DR: The paper addresses the challenges of multilingual medical reasoning in LLMs by introducing CUREMED-BENCH, a multilingual dataset, and proposing CURE-MED, a framework for improving reasoning and language consistency.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with reliable multilingual medical reasoning, limiting their applicability in diverse healthcare settings. The paper aims to address this gap.

Method: The authors introduced CUREMED-BENCH, developed a curriculum-informed reinforcement learning framework (CURE-MED), and used supervised fine-tuning with Group Relative Policy Optimization to enhance performance.

Result: Their approach outperformed baseline models, attaining significant improvements in language consistency (94.96%) and logical correctness (70.04%) with larger models (32B parameters).

Conclusion: The developed framework and dataset facilitate equitable, reliable medical reasoning across multiple languages, including underrepresented ones, improving LLM performance for healthcare applications.

Abstract: While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/

</details>


### [67] [Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops](https://arxiv.org/abs/2601.13268)
*Zainab Ghafoor,Md Shafiqul Islam,Koushik Howlader,Md Rasel Khondokar,Tanusree Bhattacharjee,Sayantan Chakraborty,Adrito Roy,Ushashi Bhattacharjee,Tirtho Roy*

Main category: cs.AI

TL;DR: This research develops a multi-agent refinement framework for improving the ethical and safety compliance of medical large language models (LLMs) using structured, iterative alignment and evaluation protocols.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the critical challenges of ensuring ethical integrity and safety compliance in using LLMs in healthcare applications.

Method: The framework employs two generative models (DeepSeek R1 and Med-PaLM) and two evaluation agents (LLaMA 3.1 and Phi-4), which assess responses based on medical ethics principles and a comprehensive safety risk assessment protocol.

Result: Results show an 89% reduction in ethical violations and a 92% downgrade rate in risk. DeepSeek R1 demonstrates quicker convergence in iterations, while Med-PaLM excels in scenarios involving privacy-sensitive data.

Conclusion: The iterative multi-agent framework is effective in enhancing the safety and reliability of medical LLMs, offering a scalable and regulator-aligned approach for clinical deployment.

Abstract: Large Language Models (LLMs) are increasingly applied in healthcare, yet ensuring their ethical integrity and safety compliance remains a major barrier to clinical deployment. This work introduces a multi-agent refinement framework designed to enhance the safety and reliability of medical LLMs through structured, iterative alignment. Our system combines two generative models - DeepSeek R1 and Med-PaLM - with two evaluation agents, LLaMA 3.1 and Phi-4, which assess responses using the American Medical Association's (AMA) Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol. We evaluate performance across 900 clinically diverse queries spanning nine ethical domains, measuring convergence efficiency, ethical violation reduction, and domain-specific risk behavior. Results demonstrate that DeepSeek R1 achieves faster convergence (mean 2.34 vs. 2.67 iterations), while Med-PaLM shows superior handling of privacy-sensitive scenarios. The iterative multi-agent loop achieved an 89% reduction in ethical violations and a 92% risk downgrade rate, underscoring the effectiveness of our approach. This study presents a scalable, regulator-aligned, and cost-efficient paradigm for governing medical AI safety.

</details>


### [68] [PepEDiff: Zero-Shot Peptide Binder Design via Protein Embedding Diffusion](https://arxiv.org/abs/2601.13327)
*Po-Yu Liang,Tobo Duran,Jun Bai*

Main category: cs.AI

TL;DR: PepEDiff is a novel peptide binder generator leveraging pretrained protein embeddings to generate diverse sequences without relying on predicted structures, excelling in challenging use cases.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in current peptide binder design approaches, which often overly depend on structure prediction and limit sequence diversity.

Method: PepEDiff operates in a continuous latent space using pretrained protein embeddings, incorporating diffusion-based sampling and latent-space exploration to enhance generation capabilities.

Result: PepEDiff outperforms state-of-the-art methods, including in challenging cases like peptide binders for TIGIT, demonstrating improved sequence and structural diversity.

Conclusion: PepEDiff offers a simple, effective, structure-free framework for zero-shot peptide binder design, expanding possibilities in therapeutic and biochemical applications.

Abstract: We present PepEDiff, a novel peptide binder generator that designs binding sequences given a target receptor protein sequence and its pocket residues. Peptide binder generation is critical in therapeutic and biochemical applications, yet many existing methods rely heavily on intermediate structure prediction, adding complexity and limiting sequence diversity. Our approach departs from this paradigm by generating binder sequences directly in a continuous latent space derived from a pretrained protein embedding model, without relying on predicted structures, thereby improving structural and sequence diversity. To encourage the model to capture binding-relevant features rather than memorizing known sequences, we perform latent-space exploration and diffusion-based sampling, enabling the generation of peptides beyond the limited distribution of known binders. This zero-shot generative strategy leverages the global protein embedding manifold as a semantic prior, allowing the model to propose novel peptide sequences in previously unseen regions of the protein space. We evaluate PepEDiff on TIGIT, a challenging target with a large, flat protein-protein interaction interface that lacks a druggable pocket. Despite its simplicity, our method outperforms state-of-the-art approaches across benchmark tests and in the TIGIT case study, demonstrating its potential as a general, structure-free framework for zero-shot peptide binder design. The code for this research is available at GitHub: https://github.com/LabJunBMI/PepEDiff-An-Peptide-binder-Embedding-Diffusion-Model

</details>


### [69] [The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models](https://arxiv.org/abs/2601.13358)
*Samuel Cyrenius Anderson*

Main category: cs.AI

TL;DR: This paper analyzes how neural scaling laws affect reasoning across four domains (Law, Science, Code, Math) and shows that scaling triggers domain-specific changes rather than uniform reasoning improvements.


<details>
  <summary>Details</summary>
Motivation: To study the impact of scaling neural networks on reasoning performance and to understand how structural changes in reasoning vary across different domains.

Method: Analyzing 25,000+ chain-of-thought trajectories for networks with 8B and 70B parameters across four domains, and introducing Neural Reasoning Operators to predict reasoning endpoints without intermediate traversal.

Result: Neural scaling laws lead to domain-specific phase transitions: legal reasoning becomes Crystalline, scientific and mathematical reasoning remain Liquid, and code reasoning forms a Lattice structure. A universal oscillatory signature invariant across domains and scales is identified.

Conclusion: Reasoning is shaped by manifold geometry rather than task difficulty. This insight provides a path to improving inference efficiency by leveraging topological characteristics of reasoning geometries.

Abstract: Scale does not uniformly improve reasoning - it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two scales (8B, 70B parameters), we discover that neural scaling laws trigger domain-specific phase transitions rather than uniform capability gains. Legal reasoning undergoes Crystallization: 45% collapse in representational dimensionality (d95: 501 -> 274), 31% increase in trajectory alignment, and 10x manifold untangling. Scientific and mathematical reasoning remain Liquid - geometrically invariant despite 9x parameter increase. Code reasoning forms a discrete Lattice of strategic modes (silhouette: 0.13 -> 0.42). This geometry predicts learnability. We introduce Neural Reasoning Operators - learned mappings from initial to terminal hidden states. In crystalline legal reasoning, our operator achieves 63.6% accuracy on held-out tasks via probe decoding, predicting reasoning endpoints without traversing intermediate states. We further identify a universal oscillatory signature (coherence ~ -0.4) invariant across domains and scales, suggesting attention and feedforward layers drive reasoning through opposing dynamics. These findings establish that the cost of thought is determined not by task difficulty but by manifold geometry - offering a blueprint for inference acceleration where topology permits.

</details>


### [70] [A Lightweight Modular Framework for Constructing Autonomous Agents Driven by Large Language Models: Design, Implementation, and Applications in AgentForge](https://arxiv.org/abs/2601.13383)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: The paper introduces AgentForge, an open-source Python framework for building LLM-powered autonomous agents. It aims to simplify task creation and execution while maintaining flexibility and performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in existing agent frameworks, such as rigidity, vendor lock-in, and excessive complexity, which hinder rapid prototyping and deployment of LLM-driven agents.

Method: The paper proposes a modular architecture with three key innovations: (1) a composable skill abstraction for structured task decomposition, (2) a unified backend interface for switching between LLM sources, and (3) a YAML-based configuration system that separates logic from implementation. The framework uses a directed acyclic graph (DAG) to model skill composition.

Result: Experimental evaluation shows AgentForge reduces development time significantly—by 62% compared to LangChain and 78% compared to direct API use—while achieving competitive task completion rates and maintaining minimal latency (<100ms).

Conclusion: AgentForge provides a scalable, efficient, and flexible foundation for developing LLM-powered autonomous agents, bridging a key gap in the ecosystem by enhancing usability without compromising on performance.

Abstract: The emergence of LLMs has catalyzed a paradigm shift in autonomous agent development, enabling systems capable of reasoning, planning, and executing complex multi-step tasks. However, existing agent frameworks often suffer from architectural rigidity, vendor lock-in, and prohibitive complexity that impedes rapid prototyping and deployment. This paper presents AgentForge, a lightweight, open-source Python framework designed to democratize the construction of LLM-driven autonomous agents through a principled modular architecture. AgentForge introduces three key innovations: (1) a composable skill abstraction that enables fine-grained task decomposition with formally defined input-output contracts, (2) a unified LLM backend interface supporting seamless switching between cloud-based APIs and local inference engines, and (3) a declarative YAML-based configuration system that separates agent logic from implementation details. We formalize the skill composition mechanism as a directed acyclic graph (DAG) and prove its expressiveness for representing arbitrary sequential and parallel task workflows. Comprehensive experimental evaluation across four benchmark scenarios demonstrates that AgentForge achieves competitive task completion rates while reducing development time by 62% compared to LangChain and 78% compared to direct API integration. Latency measurements confirm sub-100ms orchestration overhead, rendering the framework suitable for real-time applications. The modular design facilitates extension: we demonstrate the integration of six built-in skills and provide comprehensive documentation for custom skill development. AgentForge addresses a critical gap in the LLM agent ecosystem by providing researchers and practitioners with a production-ready foundation for constructing, evaluating, and deploying autonomous agents without sacrificing flexibility or performance.

</details>


### [71] [Explicit Cognitive Allocation: A Principle for Governed and Auditable Inference in Large Language Models](https://arxiv.org/abs/2601.13443)
*Héctor Manuel Manzanilla-Granados,Zaira Navarrete-Cazales,Miriam Pescador-Rojas,Tonahtiu Ramírez-Romero*

Main category: cs.AI

TL;DR: The paper introduces Explicit Cognitive Allocation to address the lack of structure in large language model (LLM) reasoning, presenting the Cognitive Universal Agent (CUA) framework, which improves AI-assisted inference by structuring epistemic functions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of current LLM reasoning methods, which lack structure, reducing traceability, epistemic control, and reproducibility in high-responsibility scenarios.

Method: The authors propose Explicit Cognitive Allocation, implemented through Cognitive Universal Agent (CUA) architecture. It separates inference into distinct stages and introduces Universal Cognitive Instruments (UCIs) for structured reasoning.

Result: Controlled experiments in the agricultural domain show that CUA inference achieves better epistemic convergence, alignment, and exposes the instrumental structure, compared to baseline LLM reasoning.

Conclusion: Explicit separation of epistemic processes in LLM-based reasoning enhances structural and aligned inferences, addressing the weaknesses of traditional generative methods.

Abstract: The rapid adoption of large language models (LLMs) has enabled new forms of AI-assisted reasoning across scientific, technical, and organizational domains. However, prevailing modes of LLM use remain cognitively unstructured: problem framing, knowledge exploration, retrieval, methodological awareness, and explanation are typically collapsed into a single generative process. This cognitive collapse limits traceability, weakens epistemic control, and undermines reproducibility, particularly in high-responsibility settings.
  We introduce Explicit Cognitive Allocation, a general principle for structuring AI-assisted inference through the explicit separation and orchestration of epistemic functions. We instantiate this principle in the Cognitive Universal Agent (CUA), an architecture that organizes inference into distinct stages of exploration and framing, epistemic anchoring, instrumental and methodological mapping, and interpretive synthesis. Central to this framework is the notion of Universal Cognitive Instruments (UCIs), which formalize heterogeneous means, including computational, experimental, organizational, regulatory, and educational instruments, through which abstract inquiries become investigable.
  We evaluate the effects of explicit cognitive and instrumental allocation through controlled comparisons between CUA-orchestrated inference and baseline LLM inference under matched execution conditions. Across multiple prompts in the agricultural domain, CUA inference exhibits earlier and structurally governed epistemic convergence, higher epistemic alignment under semantic expansion, and systematic exposure of the instrumental landscape of inquiry. In contrast, baseline LLM inference shows greater variability in alignment and fails to explicitly surface instrumental structure.

</details>


### [72] [SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt Following in Text-to-Image Generation](https://arxiv.org/abs/2601.13462)
*Amine Rostane*

Main category: cs.AI

TL;DR: The paper introduces SpatialBench-UC, a benchmark for evaluating spatial relations in text-to-image models with selective prediction capabilities.


<details>
  <summary>Details</summary>
Motivation: To address challenges in automating the evaluation of explicit spatial instructions in text-to-image models.

Method: The authors create SpatialBench-UC, comprising 200 prompts and counterfactual pairs. They provide benchmarks, human audits, and evaluate baselines using pass rates, coverage metrics, and selective abstention measures.

Result: Grounding methods significantly improve spatial relation accuracy and coverage, though missing detections remain a major challenge.

Conclusion: SpatialBench-UC facilitates auditable, reproducible comparisons among models, highlighting the importance of grounding methods for spatial evaluation in selective prediction frameworks.

Abstract: Evaluating whether text-to-image models follow explicit spatial instructions is difficult to automate. Object detectors may miss targets or return multiple plausible detections, and simple geometric tests can become ambiguous in borderline cases. Spatial evaluation is naturally a selective prediction problem, the checker may abstain when evidence is weak and report confidence so that results can be interpreted as a risk coverage tradeoff rather than a single score. We introduce SpatialBench-UC, a small, reproducible benchmark for pairwise spatial relations. The benchmark contains 200 prompts (50 object pairs times 4 relations) grouped into 100 counterfactual pairs obtained by swapping object roles. We release a benchmark package, versioned prompts, pinned configs, per-sample checker outputs, and report tables, enabling reproducible and auditable comparisons across models. We also include a lightweight human audit used to calibrate the checker's abstention margin and confidence threshold. We evaluate three baselines, Stable Diffusion 1.5, SD 1.5 BoxDiff, and SD 1.4 GLIGEN. The checker reports pass rate and coverage as well as conditional pass rates on decided samples. The results show that grounding methods substantially improve both pass rate and coverage, while abstention remains a dominant factor due mainly to missing detections.

</details>


### [73] [Context and Transcripts Improve Detection of Deepfake Audios of Public Figures](https://arxiv.org/abs/2601.13464)
*Chongyang Gao,Marco Postiglione,Julian Baldwin,Natalia Denisenko,Isabel Gortner,Luke Fosdick,Chiara Pulice,Sarit Kraus,V. S. Subrahmanian*

Main category: cs.AI

TL;DR: This paper introduces a Context-based Audio Deepfake Detector (CADD) and demonstrates that integrating context and transcripts significantly enhances detection performance.


<details>
  <summary>Details</summary>
Motivation: Current audio deepfake detectors lack the ability to analyze context or transcripts, which humans naturally use to assess information authenticity.

Method: The authors created a Journalist-provided Deepfake Dataset (JDD) of 255 public deepfakes, generated a synthetic audio dataset (SYN), and proposed the novel CADD architecture, evaluating it on multiple datasets while testing adversarial robustness.

Result: Using context or transcripts improved detector performance by 5%-37.58% (F1-score), 3.77%-42.79% (AUC), and 6.17%-47.83% (EER). CADD showed strong robustness, limiting adversarial performance degradation to -0.71%.

Conclusion: Incorporating context and transcripts significantly enhances deepfake detection, with the proposed CADD model leading in terms of performance and robustness against adversarial attacks.

Abstract: Humans use context to assess the veracity of information. However, current audio deepfake detectors only analyze the audio file without considering either context or transcripts. We create and analyze a Journalist-provided Deepfake Dataset (JDD) of 255 public deepfakes which were primarily contributed by over 70 journalists since early 2024. We also generate a synthetic audio dataset (SYN) of dead public figures and propose a novel Context-based Audio Deepfake Detector (CADD) architecture. In addition, we evaluate performance on two large-scale datasets: ITW and P$^2$V. We show that sufficient context and/or the transcript can significantly improve the efficacy of audio deepfake detectors. Performance (measured via F1 score, AUC, and EER) of multiple baseline audio deepfake detectors and traditional classifiers can be improved by 5%-37.58% in F1-score, 3.77%-42.79% in AUC, and 6.17%-47.83% in EER. We additionally show that CADD, via its use of context and/or transcripts, is more robust to 5 adversarial evasion strategies, limiting performance degradation to an average of just -0.71% across all experiments. Code, models, and datasets are available at our project page: https://sites.northwestern.edu/nsail/cadd-context-based-audio-deepfake-detection (access restricted during review).

</details>


### [74] [Graph Neural Networks are Heuristics](https://arxiv.org/abs/2601.13465)
*Yimeng Min,Carla P. Gomes*

Main category: cs.AI

TL;DR: This paper presents a method to turn a single trained graph neural network (GNN) into an unsupervised heuristic for solving combinatorial optimization problems, such as the Traveling Salesman Problem, without requiring supervision or search.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the ability of graph neural networks to solve combinatorial optimization tasks without supervised training or dependence on explicit search methods, leveraging global constraints and inductive biases.

Method: The authors use a non-autoregressive graph neural network with inductive biases that encode global constraints. Dropout and snapshot ensembling are employed during inference to improve solutions via diversity, eliminating the need for traditional sequential or search-based optimization methods.

Result: The presented approach enables GNNs to produce high-quality solutions for the Traveling Salesman Problem without requiring supervision or search, highlighting their ability to internalize combinatorial structures as unsupervised heuristics.

Conclusion: This study redefines the role of learning in combinatorial optimization by demonstrating that graph neural networks can serve as independent, unsupervised heuristics, moving beyond their traditional role of complementing classical algorithms.

Abstract: We demonstrate that a single training trajectory can transform a graph neural network into an unsupervised heuristic for combinatorial optimization. Focusing on the Travelling Salesman Problem, we show that encoding global structural constraints as an inductive bias enables a non-autoregressive model to generate solutions via direct forward passes, without search, supervision, or sequential decision-making. At inference time, dropout and snapshot ensembling allow a single model to act as an implicit ensemble, reducing optimality gaps through increased solution diversity. Our results establish that graph neural networks do not require supervised training nor explicit search to be effective. Instead, they can internalize global combinatorial structure and function as strong, learned heuristics. This reframes the role of learning in combinatorial optimization: from augmenting classical algorithms to directly instantiating new heuristics.

</details>


### [75] [Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement](https://arxiv.org/abs/2601.13481)
*Jian Zhang,Zhangqi Wang,Zhiyuan Wang,Weiping Fu,Yu He,Haiping Zhu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: The paper introduces a prompt optimization framework (APOLO) for improving emotion diagnosis using LLMs in medical and mental healthcare settings.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of emotional comorbidity and inefficient diagnostic exploration using LLMs in high-stakes medical scenarios.

Method: APOLO leverages a Partially Observable Markov Decision Process and a multi-agent framework involving Planner, Teacher, Critic, Student, and Target roles to systematically refine prompts.

Result: APOLO demonstrated improved diagnostic accuracy and robustness over baseline methods across various domain-specific benchmarks.

Conclusion: APOLO provides a scalable and generalized framework for trustworthy applications of LLMs in mental healthcare, mitigating sensitivity issues in prompt design.

Abstract: Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare.

</details>


### [76] [Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models](https://arxiv.org/abs/2601.13533)
*Changshuo Zhang*

Main category: cs.AI

TL;DR: This paper introduces the Entropy-Guided Latent Reasoning (EGLR) model, which integrates reasoning into generative ranking processes for enhancing preference accuracy.


<details>
  <summary>Details</summary>
Motivation: To address challenges in capturing complex preferences during list generation by adapting to dynamic model entropy changes.

Method: The EGLR model uses reasoning during recommendation, entropy-guided variable-length reasoning, dynamic temperature adjustment, and lightweight integration.

Result: Experimental validation on real-world datasets shows EGLR enhances performance and compatibility with existing generative re-ranking models.

Conclusion: EGLR effectively balances exploration-exploitation, improves recommendation precision, and demonstrates deployment potential.

Abstract: Reinforcement learning plays a crucial role in generative re-ranking scenarios due to its exploration-exploitation capabilities, but existing generative methods mostly fail to adapt to the dynamic entropy changes in model difficulty during list generation, making it challenging to accurately capture complex preferences. Given that language models have achieved remarkable breakthroughs by integrating reasoning capabilities, we draw on this approach to introduce a latent reasoning mechanism, and experimental validation demonstrates that this mechanism effectively reduces entropy in the model's decision-making process. Based on these findings, we introduce the Entropy-Guided Latent Reasoning (EGLR) recommendation model, which has three core advantages. First, it abandons the "reason first, recommend later" paradigm to achieve "reasoning while recommending", specifically designed for the high-difficulty nature of list generation by enabling real-time reasoning during generation. Second, it implements entropy-guided variable-length reasoning using context-aware reasoning token alongside dynamic temperature adjustment, expanding exploration breadth in reasoning and boosting exploitation precision in recommending to achieve a more precisely adapted exploration-exploitation trade-off. Third, the model adopts a lightweight integration design with no complex independent modules or post-processing, enabling easy adaptation to existing models. Experimental results on two real-world datasets validate the model's effectiveness, and its notable advantage lies in being compatible with existing generative re-ranking models to enhance their performance. Further analyses also demonstrate its practical deployment value and research potential.

</details>


### [77] [TruthTensor: Evaluating LLMs Human Imitation through Prediction Market Drift and Holistic Reasoning](https://arxiv.org/abs/2601.13545)
*Shirin Shahabi,Spencer Graham,Haruna Isah*

Main category: cs.AI

TL;DR: Static benchmarks struggle to fully evaluate real-world model performance. TruthTensor proposes a reproducible evaluation system for LLMs focusing on prediction markets and robustness measures.


<details>
  <summary>Details</summary>
Motivation: Traditional benchmarks fail to align with real-world decision-making dynamics and uncertainties, creating the need for a robust evaluation framework for LLMs.

Method: TruthTensor implements human-imitation evaluation in prediction markets using probabilistic scoring and robust metrics alongside live datasets and contamination-free tasks.

Result: TruthTensor experiments on 500+ real markets show significant differences in calibration, drift, and risk-sensitivity among similarly accurate models.

Conclusion: TruthTensor provides a comprehensive and reproducible framework for evaluating LLMs, addressing accuracy, stability, and cost-effectiveness challenges in real-world applications.

Abstract: Evaluating language models and AI agents remains fundamentally challenging because static benchmarks fail to capture real-world uncertainty, distribution shift, and the gap between isolated task accuracy and human-aligned decision-making under evolving conditions. This paper introduces TruthTensor, a novel, reproducible evaluation paradigm that measures Large Language Models (LLMs) not only as prediction engines but as human-imitation systems operating in socially-grounded, high-entropy environments. Building on forward-looking, contamination-free tasks, our framework anchors evaluation to live prediction markets and combines probabilistic scoring to provide a holistic view of model behavior. TruthTensor complements traditional correctness metrics with drift-centric diagnostics and explicit robustness checks for reproducibility. It specify human vs. automated evaluation roles, annotation protocols, and statistical testing procedures to ensure interpretability and replicability of results. In experiments across 500+ real markets (political, economic, cultural, technological), TruthTensor demonstrates that models with similar forecast accuracy can diverge markedly in calibration, drift, and risk-sensitivity, underscoring the need to evaluate models along multiple axes (accuracy, calibration, narrative stability, cost, and resource efficiency). TruthTensor therefore operationalizes modern evaluation best practices, clear hypothesis framing, careful metric selection, transparent compute/cost reporting, human-in-the-loop validation, and open, versioned evaluation contracts, to produce defensible assessments of LLMs in real-world decision contexts. We publicly release TruthTensor at https://truthtensor.com

</details>


### [78] [ChatAD: Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn Instruction Evolution](https://arxiv.org/abs/2601.13546)
*Hui Sun,Chang Xu,Haonan Xie,Hao Li,Yuhao Huang,Chuheng Zhang,Ming Jin,Xiaoguang Liu,Gang Wang,Jiang Bian*

Main category: cs.AI

TL;DR: This paper proposes solutions for improving anomaly detection in time series using LLMs, including a multi-agent algorithm (TSEvol), a dataset, chatbot family, optimization methods, and an evaluation benchmark.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in LLM-driven anomaly detection such as limitations in reasoning, multi-turn dialogue abilities, and generalization.

Method: The authors introduce TSEvol (a multi-agent algorithm), TSEData-20K (a reasoning/dataset dialogue), ChatAD chatbot family, TKTO optimization technique, and LLADBench evaluation framework.

Result: The ChatAD models demonstrated significant improvements: up to 34.50% in accuracy, 34.71% in F1 score, and a 37.42% reduction in false positives. Optimized models showed competitive performance across various tasks.

Conclusion: The study presents advancements in LLM-driven anomaly detection with enhanced reasoning, dialogue capabilities, generalization, and evaluation techniques, achieving notable performance gains.

Abstract: LLM-driven Anomaly Detection (AD) helps enhance the understanding and explanatory abilities of anomalous behaviors in Time Series (TS). Existing methods face challenges of inadequate reasoning ability, deficient multi-turn dialogue capability, and narrow generalization. To this end, we 1) propose a multi-agent-based TS Evolution algorithm named TSEvol. On top of it, we 2) introduce the AD reasoning and multi-turn dialogue Dataset TSEData-20K and contribute the Chatbot family for AD, including ChatAD-Llama3-8B, Qwen2.5-7B, and Mistral-7B. Furthermore, 3) we propose the TS Kahneman-Tversky Optimization (TKTO) to enhance ChatAD's cross-task generalization capability. Lastly, 4) we propose a LLM-driven Learning-based AD Benchmark LLADBench to evaluate the performance of ChatAD and nine baselines across seven datasets and tasks. Our three ChatAD models achieve substantial gains, up to 34.50% in accuracy, 34.71% in F1, and a 37.42% reduction in false positives. Besides, via KTKO, our optimized ChatAD achieves competitive performance in reasoning and cross-task generalization on classification, forecasting, and imputation.

</details>


### [79] [Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective Behaviors in MSM: Social Media and Dating apps Text Analysis](https://arxiv.org/abs/2601.13558)
*Mehrab Beikzadeh,Chenglin Hong,Cory J Cascalheira,Callisto Boka,Majid Sarrafzadeh,Ian W Holloway*

Main category: cs.AI

TL;DR: This study explores using text data from social media and dating apps to predict sexual behaviors, alcohol use, and PrEP uptake among MSM using machine learning.


<details>
  <summary>Details</summary>
Motivation: MSM are at higher risk for sexually transmitted infections and harmful behaviors, creating a need for targeted public health strategies. Social media and dating app data present a potential resource for identifying risky and protective behaviors.

Method: The study used participant-consented textual data and machine learning models based on ChatGPT embeddings, BERT embeddings, LIWC, and a risk term dictionary to predict behaviors like binge drinking and sexual partner count.

Result: The models displayed strong prediction for binge drinking and multiple sexual partners (F1 score: 0.78) and moderate capability for PrEP use and heavy drinking (F1 scores: 0.64 and 0.63).

Conclusion: Social media and dating app text data, combined with large language model techniques, offer promise for scalable and personalized public health interventions for MSM.

Abstract: Men who have sex with men (MSM) are at elevated risk for sexually transmitted infections and harmful drinking compared to heterosexual men. Text data collected from social media and dating applications may provide new opportunities for personalized public health interventions by enabling automatic identification of risk and protective behaviors. In this study, we evaluated whether text from social media and dating apps can be used to predict sexual risk behaviors, alcohol use, and pre-exposure prophylaxis (PrEP) uptake among MSM. With participant consent, we collected textual data and trained machine learning models using features derived from ChatGPT embeddings, BERT embeddings, LIWC, and a dictionary-based risk term approach. The models achieved strong performance in predicting monthly binge drinking and having more than five sexual partners, with F1 scores of 0.78, and moderate performance in predicting PrEP use and heavy drinking, with F1 scores of 0.64 and 0.63. These findings demonstrate that social media and dating app text data can provide valuable insights into risk and protective behaviors and highlight the potential of large language model-based methods to support scalable and personalized public health interventions for MSM.

</details>


### [80] [AgentGC: Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent](https://arxiv.org/abs/2601.13559)
*Sun Hui,Ding Yanfeng,Huidong Ma,Chang Xu,Keyan Jin,Lizheng Zu,Cheng Zhong,xiaoguang Liu,Gang Wang,Wentong Cai*

Main category: cs.AI

TL;DR: This paper introduces AgentGC, an evolutionary agent-based genomic data compression system offering improved user-friendliness, adaptability, and compression performance.


<details>
  <summary>Details</summary>
Motivation: Current genomic data compression methods face challenges in low-level modeling, adaptability, and user-friendliness, which motivated the design of an advanced solution.

Method: The authors designed AgentGC, a 3-layer agent-based system with Leader/Worker agents and integrated LLMs for optimization, offering three compression modes tailored for various scenarios.

Result: AgentGC achieves significant improvements in genomic data compression, with average compression gains of ~16% and throughput gains up to ~9x across various datasets.

Conclusion: AgentGC is a robust, user-friendly, and adaptable solution for genomic data compression, demonstrating superior performance and versatility.

Abstract: Lossless compression has made significant advancements in Genomics Data (GD) storage, sharing and management. Current learning-based methods are non-evolvable with problems of low-level compression modeling, limited adaptability, and user-unfriendly interface. To this end, we propose AgentGC, the first evolutionary Agent-based GD Compressor, consisting of 3 layers with multi-agent named Leader and Worker. Specifically, the 1) User layer provides a user-friendly interface via Leader combined with LLM; 2) Cognitive layer, driven by the Leader, integrates LLM to consider joint optimization of algorithm-dataset-system, addressing the issues of low-level modeling and limited adaptability; and 3) Compression layer, headed by Worker, performs compression & decompression via a automated multi-knowledge learning-based compression framework. On top of AgentGC, we design 3 modes to support diverse scenarios: CP for compression-ratio priority, TP for throughput priority, and BM for balanced mode. Compared with 14 baselines on 9 datasets, the average compression ratios gains are 16.66%, 16.11%, and 16.33%, the throughput gains are 4.73x, 9.23x, and 9.15x, respectively.

</details>


### [81] [Reasoning is a Modality](https://arxiv.org/abs/2601.13562)
*Zhiguang Liu,Yi Shang*

Main category: cs.AI

TL;DR: This paper presents a novel transformer-based model for solving ARC tasks that isolates reasoning as a separate modality, achieving higher accuracy than prior approaches and human averages.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between human-like reasoning and AI behavior by exploring reasoning as a distinct modality in abstract reasoning tasks like ARC.

Method: The authors propose a role-separated transformer block that separates global controller tokens from grid workspace tokens, allowing iterative rule application for visual reasoning tasks.

Result: The proposed method achieved 62.6% accuracy in solving ARC tasks, surpassing human average performance (60.2%) and outperforming previous AI methods.

Conclusion: The findings suggest that separating reasoning as a modality aligns better with structured, controller-driven reasoning and improves performance in abstract reasoning problems.

Abstract: The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state. We hypothesize that reasoning is a modality: reasoning should exist as a distinct channel separate from the low-level workspace on which rules are applied. To test this hypothesis, on solving ARC tasks as a visual reasoning problem, we designed a novel role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution. Trained and evaluated within the VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and outperforming prior methods significantly. Qualitatively, our models exhibit more coherent rule-application structure than the dense ViT baseline, consistent with a shift away from plausible probability blobs toward controller-driven reasoning.

</details>


### [82] [SCRIPTMIND: Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System](https://arxiv.org/abs/2601.13581)
*Heedou Kim,Changsik Kim,Sanghwa Shin,Jaewoo Kang*

Main category: cs.AI

TL;DR: This paper introduces ScriptMind, a framework to improve scam detection using Large Language Models (LLMs), focusing on both automated reasoning and human cognition. It significantly enhances detection accuracy and user awareness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the growing issue of personalized, multi-turn social engineering scams which traditional detection methods struggle to handle.

Method: ScriptMind incorporates three components: 1) Crime Script Inference Task (CSIT) for reasoning, 2) Crime Script-Aware Inference Dataset (CSID) for fine-tuning smaller LLMs, and 3) Cognitive Simulation-based Evaluation (CSED) to assess cognitive impact during scams.

Result: The fine-tuned 11B LLM exhibited superior performance (13% better than GPT-4o), especially in detection accuracy, false-positive reduction, and rationale quality. It also effectively raised and sustained users' suspicion towards scams in simulation experiments.

Conclusion: ScriptMind demonstrates the potential of human-centered, cognitively adaptive LLMs, significantly improving both automated scam detection and users' cognitive defenses against social engineering.

Abstract: Social engineering scams increasingly employ personalized, multi-turn deception, exposing the limits of traditional detection methods. While Large Language Models (LLMs) show promise in identifying deception, their cognitive assistance potential remains underexplored. We propose ScriptMind, an integrated framework for LLM-based scam detection that bridges automated reasoning and human cognition. It comprises three components: the Crime Script Inference Task (CSIT) for scam reasoning, the Crime Script-Aware Inference Dataset (CSID) for fine-tuning small LLMs, and the Cognitive Simulation-based Evaluation of Social Engineering Defense (CSED) for assessing real-time cognitive impact. Using 571 Korean phone scam cases, we built 22,712 structured scammer-sequence training instances. Experimental results show that the 11B small LLM fine-tuned with ScriptMind outperformed GPT-4o by 13%, achieving superior performance over commercial models in detection accuracy, false-positive reduction, scammer utterance prediction, and rationale quality. Moreover, in phone scam simulation experiments, it significantly enhanced and sustained users' suspicion levels, improving their cognitive awareness of scams. ScriptMind represents a step toward human-centered, cognitively adaptive LLMs for scam defense.

</details>


### [83] [Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification](https://arxiv.org/abs/2601.13589)
*HyeYoung Lee*

Main category: cs.AI

TL;DR: The paper presents a real-time AI system transforming audio-emotional signals into safe, age-appropriate media responses using a structured multi-agent pipeline.


<details>
  <summary>Details</summary>
Motivation: To develop a system that not only identifies emotions from speech but also creates safe, controllable, and age-appropriate media responses.

Method: The system uses four AI agents: an Emotion Recognition Agent, a Response Policy Decision Agent, a Content Parameter Generation Agent, and a Safety Verification Agent. It utilizes CNN-based emotion recognition, maps emotions to response modes, generates media control parameters, and ensures safety compliance.

Result: Achieved 73.2% emotion recognition accuracy, 89.4% response mode consistency, 100% safety compliance, and sub-100ms latency during experimental testing.

Conclusion: The modular and interpretable system is suited for applications in child-related media, therapy, and emotionally adaptive smart devices.

Abstract: This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.

</details>


### [84] [DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems](https://arxiv.org/abs/2601.13591)
*Maojun Sun,Yifei Xie,Yue Wu,Ruijian Han,Binyan Jiang,Defeng Sun,Yancheng Yuan,Jian Huang*

Main category: cs.AI

TL;DR: DSAEval is a benchmark designed to evaluate LLM-based data agents for real-world data science tasks using diverse datasets and metrics.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of evaluating LLM-based agents in handling real-world open-ended data science problems.

Method: The authors introduced DSAEval, a benchmark with diverse datasets and tested agentic LLMs considering multimodal perception and multi-dimensional metrics.

Result: Key findings indicate Claude-Sonnet-4.5 as the best performer overall, GPT-5.2 as the most efficient, and MiMo-V2-Flash as cost-effective; multimodal perception enhances performance.

Conclusion: Current LLM-based agents excel in structured data tasks but struggle with unstructured domains. The benchmark provides insights and future directions for improvement.

Abstract: Recent LLM-based data agents aim to automate data science tasks ranging from data analysis to deep learning. However, the open-ended nature of real-world data science problems, which often span multiple taxonomies and lack standard answers, poses a significant challenge for evaluation. To address this, we introduce DSAEval, a benchmark comprising 641 real-world data science problems grounded in 285 diverse datasets, covering both structured and unstructured data (e.g., vision and text). DSAEval incorporates three distinctive features: (1) Multimodal Environment Perception, which enables agents to interpret observations from multiple modalities including text and vision; (2) Multi-Query Interactions, which mirror the iterative and cumulative nature of real-world data science projects; and (3) Multi-Dimensional Evaluation, which provides a holistic assessment across reasoning, code, and results. We systematically evaluate 11 advanced agentic LLMs using DSAEval. Our results show that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2 is the most efficient, and MiMo-V2-Flash is the most cost-effective. We further demonstrate that multimodal perception consistently improves performance on vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while current data science agents perform well on structured data and routine data anlysis workflows, substantial challenges remain in unstructured domains. Finally, we offer critical insights and outline future research directions to advance the development of data science agents.

</details>


### [85] [Foundations of Global Consistency Checking with Noisy LLM Oracles](https://arxiv.org/abs/2601.13600)
*Paul He,Elke Kirschbaum,Shiva Kasiviswanathan*

Main category: cs.AI

TL;DR: This paper addresses the challenge of ensuring global consistency in natural-language fact collections by introducing an efficient algorithm to detect and repair inconsistencies with LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the global consistency challenge in natural language fact collections, as ensuring this consistency is critical for tasks like fact-checking, summarization, and knowledge base construction.

Method: The authors propose an adaptive divide-and-conquer algorithm that identifies minimal inconsistent subsets (MUSes) of facts and optionally computes minimal repairs using hitting-sets, maintaining polynomial query complexity.

Result: Experiments with synthetic and real LLM oracles demonstrate the efficiency of the method in detecting and localizing inconsistencies in natural-language datasets.

Conclusion: This scalable framework provides a practical solution for linguistic consistency verification using LLM-based evaluators, addressing the challenges of noisy and limited pairwise consistency checks.

Abstract: Ensuring that collections of natural-language facts are globally consistent is essential for tasks such as fact-checking, summarization, and knowledge base construction. While Large Language Models (LLMs) can assess the consistency of small subsets of facts, their judgments are noisy, and pairwise checks are insufficient to guarantee global coherence. We formalize this problem and show that verifying global consistency requires exponentially many oracle queries in the worst case. To make the task practical, we propose an adaptive divide-and-conquer algorithm that identifies minimal inconsistent subsets (MUSes) of facts and optionally computes minimal repairs through hitting-sets. Our approach has low-degree polynomial query complexity. Experiments with both synthetic and real LLM oracles show that our method efficiently detects and localizes inconsistencies, offering a scalable framework for linguistic consistency verification with LLM-based evaluators.

</details>


### [86] [Resilient Routing: Risk-Aware Dynamic Routing in Smart Logistics via Spatiotemporal Graph Learning](https://arxiv.org/abs/2601.13632)
*Zhiming Xue,Sichen Zhao,Yalun Qi,Xianling Zeng,Zihan Yu*

Main category: cs.AI

TL;DR: The paper proposes a Risk-Aware Dynamic Routing (RADR) framework using spatiotemporal graph neural networks and combinatorial optimization, which significantly improves routing resilience in logistics under congestion scenarios.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of e-commerce has caused logistical strain, exposing the limitations of traditional static routing systems to handle fluctuating retail demand and traffic congestion.

Method: The authors designed a logistics topology graph from GPS data using spatial clustering, applied a hybrid GCN-GRU model to predict congestion risks, and used a dynamic edge weight mechanism for path planning.

Result: The RADR framework reduced congestion risk by 19.3% and increased transportation distance by only 2.1% in highly congested scenarios using data from the Smart Logistics Dataset 2024.

Conclusion: The data-driven RADR framework effectively balances delivery efficiency and operational safety, offering a robust solution to tackle modern logistics challenges.

Abstract: With the rapid development of the e-commerce industry, the logistics network is experiencing unprecedented pressure. The traditional static routing strategy most time cannot tolerate the traffic congestion and fluctuating retail demand. In this paper, we propose a Risk-Aware Dynamic Routing(RADR) framework which integrates Spatiotemporal Graph Neural Networks (ST-GNN) with combinatorial optimization. We first construct a logistics topology graph by using the discrete GPS data using spatial clustering methods. Subsequently, a hybrid deep learning model combining Graph Convolutional Network (GCN) and Gated Recurrent Unit (GRU) is adopted to extract spatial correlations and temporal dependencies for predicting future congestion risks. These prediction results are then integrated into a dynamic edge weight mechanism to perform path planning. We evaluated the framework on the Smart Logistics Dataset 2024, which contains real-world Internet of Things(IoT) sensor data. The experimental results show that the RADR algorithm significantly enhances the resilience of the supply chain. Particularly in the case study of high congestion scenarios, our method reduces the potential congestion risk exposure by 19.3% while only increasing the transportation distance by 2.1%. This empirical evidence confirms that the proposed data-driven approach can effectively balance delivery efficiency and operational safety.

</details>


### [87] [Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue](https://arxiv.org/abs/2601.13687)
*Zhichao Liang,Satoshi Nakamura*

Main category: cs.AI

TL;DR: This paper introduces SocialMindChange, a benchmark for testing a model's ability to influence mental-state trajectories through dialogue in social interactions.


<details>
  <summary>Details</summary>
Motivation: Most Theory of Mind (ToM) benchmarks focus on passively tracking how mental states change, but social interaction also involves actively influencing others' mental states.

Method: The study designs a benchmark where models act as one character in a multi-scene, multi-character social context, generating dialogue to meet a target while keeping interactions consistent.

Result: The authors evaluate ten leading language models, revealing a significant 54.2% performance gap compared to humans in these tasks.

Conclusion: Current language models struggle to actively manage and influence mental-state representations over extended interactions, indicating room for improvement.

Abstract: Existing dynamic Theory of Mind (ToM) benchmarks mostly place language models in a passive role: the model reads a sequence of connected scenarios and reports what people believe, feel, intend, and do as these states change. In real social interaction, ToM is also used for action: a speaker plans what to say in order to shift another person's mental-state trajectory toward a goal. We introduce SocialMindChange, a benchmark that moves from tracking minds to changing minds in social interaction. Each instance defines a social context with 4 characters and five connected scenes. The model plays one character and generates dialogue across the five scenes to reach the target while remaining consistent with the evolving states of all participants. SocialMindChange also includes selected higher-order states. Using a structured four-step framework, we construct 1,200 social contexts, covering 6000 scenarios and over 90,000 questions, each validated for realism and quality. Evaluations on ten state-of-the-art LLMs show that their average performance is 54.2% below human performance. This gap suggests that current LLMs still struggle to maintain and change mental-state representations across long, linked interactions.

</details>


### [88] [Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games](https://arxiv.org/abs/2601.13709)
*Christopher Kao,Vanshika Vats,James Davis*

Main category: cs.AI

TL;DR: This paper studies the deceptive abilities of large language models (LLMs) using the Social Deduction Game (SDG) Mafia, showcasing their effective deception in social contexts.


<details>
  <summary>Details</summary>
Motivation: To understand the deceptive capabilities and risks of LLMs in real-world-like social contexts, beyond controlled, task-specific testing.

Method: The researchers simulate 35 Mafia games using GPT-4-based LLM agents, developing a Mafia Detector with GPT-4-Turbo to analyze and predict the mafia players' identities based on game transcripts.

Result: The Mafia Detector demonstrated lower prediction accuracy on LLM games than human games, suggesting LLMs blend in better and deceive more effectively in social settings.

Conclusion: LLMs exhibit enhanced, effective deceptive capabilities in natural language-based social interactions. The authors highlight this as both a technological sophistication and a potential safety risk.

Abstract: Large Language Model (LLM) agents are increasingly used in many applications, raising concerns about their safety. While previous work has shown that LLMs can deceive in controlled tasks, less is known about their ability to deceive using natural language in social contexts. In this paper, we study deception in the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving others through conversation. Unlike previous SDG studies, we use an asynchronous multi-agent framework which better simulates realistic social contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a Mafia Detector using GPT-4-Turbo to analyze game transcripts without player role information to predict the mafia players. We use prediction accuracy as a surrogate marker for deception quality. We compare this prediction accuracy to that of 28 human games and a random baseline. Results show that the Mafia Detector's mafia prediction accuracy is lower on LLM games than on human games. The result is consistent regardless of the game days and the number of mafias detected. This indicates that LLMs blend in better and thus deceive more effectively. We also release a dataset of LLM Mafia transcripts to support future research. Our findings underscore both the sophistication and risks of LLM deception in social contexts.

</details>


### [89] [Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection](https://arxiv.org/abs/2601.13735)
*Hojin Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: The paper examines the reliability of probabilistic confidence metrics in reflecting reasoning quality, finding that they fail to capture inter-step causal dependencies and primarily measure surface fluency. It introduces a contrastive causality metric to address this gap.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to assess whether existing probabilistic confidence metrics accurately represent logical reasoning quality, especially in capturing causal dependencies between reasoning steps.

Method: The authors analyze current metrics' limitations by designing inter-step causality perturbations that disrupt reasoning dependencies while maintaining fluency. They further propose a contrastive causality metric that isolates causal dependencies for more faithful output selection.

Result: The study reveals that existing metrics are highly insensitive to logical structure, even under severe disruptions. The proposed contrastive causality metric better captures inter-step dependencies and improves reasoning accuracy.

Conclusion: Probabilistic metrics are insufficient for assessing reasoning quality as they overlook inter-step causality. The contrastive causality metric improves reasoning fidelity, offering a better alternative for selection tasks.

Abstract: Probabilistic confidence metrics are increasingly adopted as proxies for reasoning quality in Best-of-N selection, under the assumption that higher confidence reflects higher reasoning fidelity. In this work, we challenge this assumption by investigating whether these metrics truly capture inter-step causal dependencies necessary for valid reasoning. We introduce three classes of inter-step causality perturbations that systematically disrupt dependencies between reasoning steps while preserving local fluency. Surprisingly, across diverse model families and reasoning benchmarks, we find that selection accuracy degrades only marginally under these disruptions. Even severe interventions, such as applying hard attention masks that directly prevent the model from attending to prior reasoning steps, do not substantially reduce selection performance. These findings provide strong evidence that current probabilistic metrics are largely insensitive to logical structure, and primarily capture surface-level fluency or in-distribution priors instead. Motivated by this gap, we propose a contrastive causality metric that explicitly isolates inter-step causal dependencies, and demonstrate that it yields more faithful output selection than existing probability-based approaches.

</details>


### [90] [Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering](https://arxiv.org/abs/2601.13752)
*Chak Tou Leong,Dingwei Chen,Heming Xia,Qingyu Yin,Sunbowen Lee,Jian Wang,Wenjie Li*

Main category: cs.AI

TL;DR: The paper introduces RELIEF, an approach to improve large reasoning models (LRMs) by shaping their reasoning beliefs without using reasoning-trace supervision.


<details>
  <summary>Details</summary>
Motivation: Current LRMs face challenges like computational redundancy and unfaithfulness in their reasoning, with existing solutions being costly and hard to scale.

Method: RELIEF aligns the latent reasoning beliefs of LRMs with target beliefs via fine-tuning on self-reflective, synthesized question-answering pairs, avoiding reasoning-trace supervision.

Result: RELIEF matches or surpasses traditional methods in efficiency and faithfulness tasks while lowering training costs.

Conclusion: Shaping a model's reasoning beliefs through RELIEF effectively changes its behavior and offers a scalable, cost-efficient alternative to existing methods.

Abstract: Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model's self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model's reasoning belief effectively shapes its actual behavior.

</details>


### [91] [DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution](https://arxiv.org/abs/2601.13761)
*Shengda Fan,Xuyan Ye,Yankai Lin*

Main category: cs.AI

TL;DR: The paper introduces DARC, a two-stage framework to stabilize self-play with large language models, improving performance in reasoning tasks without human annotations.


<details>
  <summary>Details</summary>
Motivation: Existing self-play frameworks often suffer from optimization instability due to non-stationary objectives and bootstrapping errors, leading to unreliable self-evolution.

Method: DARC employs a two-stage strategy: (1) training the Questioner to create difficulty-calibrated questions using external corpora, and (2) training the Solver with document-augmented self-distillation to reduce errors in pseudo-label supervision.

Result: DARC is shown to improve reasoning benchmarks by an average of 10.9 points across various models, outperforming baselines and achieving nearly supervised performance without human labels.

Conclusion: DARC successfully addresses instability in self-play frameworks, demonstrating robust and scalable improvements in artificial intelligence reasoning tasks with its novel curriculum and distillation approach.

Abstract: Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.

</details>


### [92] [Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance](https://arxiv.org/abs/2601.13770)
*Mostapha Benhenda*

Main category: cs.AI

TL;DR: This paper introduces Look-Ahead-Bench, a benchmark for evaluating look-ahead bias in financial workflows using Point-in-Time LLMs, and demonstrates its utility in testing LLMs for real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: The work aims to address the gap in evaluating look-ahead bias within financial LLMs by providing a standardized benchmark to test their generalization and predictive capabilities under realistic conditions.

Method: The authors create Look-Ahead-Bench, measuring look-ahead bias using temporal decay in financial tasks across market regimes, and evaluate both standard and PiT-geared LLMs.

Result: The benchmark reveals significant look-ahead bias in standard LLMs like Llama 3.1 and DeepSeek, while PiT-inference models show better generalization, reasoning, and reduced bias as scale increases.

Conclusion: Look-Ahead-Bench helps identify financial LLMs suitable for real-world deployment, advancing standardized evaluation frameworks for temporal challenges in AI models.

Abstract: We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench

</details>


### [93] [Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments](https://arxiv.org/abs/2601.13846)
*Glinskaya Maria*

Main category: cs.AI

TL;DR: The paper introduces Virtual Urbanism (VU), an AI-based framework for analyzing urban identity using synthetic urban replicas, validated through a Tokyo-based pilot study with an 81% accuracy in identity assessment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance computational ways of quantifying urban identity and develop multi-parameter identity metrics for cities.

Method: The study used Stable Diffusion and LoRA models to create synthetic urban replicas, evaluated through human experiments for legitimacy, area-level identity, and core identity elements.

Result: The pilot study achieved ~81% identification accuracy, developed an Urban Identity Level metric, and identified culturally embedded elements as key in urban identity.

Conclusion: The framework demonstrates the feasibility of AI-driven urban analysis and sets the foundation for creating automated, multi-parameter urban identity metrics.

Abstract: This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic urban replicas. The framework aims to advance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and LoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II) quantified area-level identity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.

</details>


### [94] [LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health](https://arxiv.org/abs/2601.13880)
*Ye Tian,Zihao Wang,Onat Gungor,Xiaoran Fan,Tajana Rosing*

Main category: cs.AI

TL;DR: This paper introduces LifeAgentBench, a large-scale benchmark for evaluating long-horizon, cross-dimensional lifestyle health reasoning using LLMs. It contains 22,573 questions and proposes LifeAgent, a baseline health assistant model.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic benchmarks for evaluating the capabilities of LLMs in personalized digital health support across diverse and complex lifestyle scenarios.

Method: The authors developed LifeAgentBench, a QA benchmark containing a large dataset and an evaluation protocol for LLM-based health assistants. They evaluated 11 LLMs and introduced LifeAgent, an agent using evidence retrieval with deterministic aggregation.

Result: Key bottlenecks in LLMs' capabilities were identified, particularly in long-horizon aggregation and cross-dimensional reasoning. LifeAgent showed significant improvements over existing baselines.

Conclusion: LifeAgentBench and LifeAgent can enhance the development of reliable health assistants. The benchmark and findings provide a foundation for further research and improvement in this domain.

Abstract: Personalized digital health support requires long-horizon, cross-dimensional reasoning over heterogeneous lifestyle signals, and recent advances in mobile sensing and large language models (LLMs) make such support increasingly feasible. However, the capabilities of current LLMs in this setting remain unclear due to the lack of systematic benchmarks. In this paper, we introduce LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional, and multi-user lifestyle health reasoning, containing 22,573 questions spanning from basic retrieval to complex reasoning. We release an extensible benchmark construction pipeline and a standardized evaluation protocol to enable reliable and scalable assessment of LLM-based health assistants. We then systematically evaluate 11 leading LLMs on LifeAgentBench and identify key bottlenecks in long-horizon aggregation and cross-dimensional reasoning. Motivated by these findings, we propose LifeAgent as a strong baseline agent for health assistant that integrates multi-step evidence retrieval with deterministic aggregation, achieving significant improvements compared with two widely used baselines. Case studies further demonstrate its potential in realistic daily-life scenarios. The benchmark is publicly available at https://anonymous.4open.science/r/LifeAgentBench-CE7B.

</details>


### [95] [Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems](https://arxiv.org/abs/2601.13887)
*Hong Su*

Main category: cs.AI

TL;DR: The paper introduces Human Simulation Computation (HSC), a framework designed to improve reasoning and interaction through a human-inspired closed-loop process involving thinking, acting, learning, and reflecting.


<details>
  <summary>Details</summary>
Motivation: Current large language models lack the ability to adapt and verify reasoning outcomes effectively in dynamic, real-world environments due to their reliance solely on textual data.

Method: The paper proposes HSC, a computational approach involving continuous, closed-loop processes that emphasize reasoning, action, learning, reflection, and environmental interaction.

Result: Theoretical arguments highlight that incorporating human-like thinking strategies and action-based reasoning methods can enhance adaptation and interaction capabilities.

Conclusion: Human-like reasoning processes, combined with action-grounded approaches, are crucial to achieving advanced intelligence in real-world scenarios.

Abstract: Large language models (LLMs) have demonstrated strong capabilities in knowledge representation and reasoning based on textual data. However, their reliance on language material alone limits their ability to adapt, verify reasoning outcomes, and operate effectively in open and dynamic real-world environments. In this paper, we propose Human Simulation Computation (HSC), a human-inspired computational framework that models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling, collectively referred to as the internal reasoning process. HSC emphasizes active participation both within the internal reasoning process and in interactions with the environment, where actions are used not only to achieve goals but also to automatically refine and improve internal reasoning mechanisms without external intervention. Furthermore, HSC incorporates commonly used human thinking strategies across all stages of the internal reasoning process, such as main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback. Through theoretical analysis, we argue that human simulation strategies cannot be fully learned from language material alone, and that human-like reasoning processes and action-grounded reasoning methods are essential for robust adaptation and effective interaction with real-world environments.

</details>


### [96] [PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation](https://arxiv.org/abs/2601.13904)
*Jaeyoung Moon,Youjin Choi,Yucheon Park,David Melhart,Georgios N. Yannakakis,Kyung-Joong Kim*

Main category: cs.AI

TL;DR: PREFAB is a novel low-budget annotation method targeting affective inflection points rather than full emotional labeling, reducing workload while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Self-annotation in affective computing is time-consuming, cognitively demanding, and error-prone; addressing these challenges is necessary for effective emotion labeling.

Method: PREFAB uses the peak-end rule and preference-learning models to focus on key emotional changes instead of full annotations, supported by a preview mechanism for contextual cues.

Result: PREFAB outperformed baseline methods in detecting affective inflections, reduced workload, conditional reduction in temporal burden, and improved annotator confidence without losing quality.

Conclusion: PREFAB offers a viable alternative for efficient yet reliable affective state annotation, balancing cognitive load and annotation quality.

Abstract: Self-annotation is the gold standard for collecting affective state labels in affective computing. Existing methods typically rely on full annotation, requiring users to continuously label affective states across entire sessions. While this process yields fine-grained data, it is time-consuming, cognitively demanding, and prone to fatigue and errors. To address these issues, we present PREFAB, a low-budget retrospective self-annotation method that targets affective inflection regions rather than full annotation. Grounded in the peak-end rule and ordinal representations of emotion, PREFAB employs a preference-learning model to detect relative affective changes, directing annotators to label only selected segments while interpolating the remainder of the stimulus. We further introduce a preview mechanism that provides brief contextual cues to assist annotation. We evaluate PREFAB through a technical performance study and a 25-participant user study. Results show that PREFAB outperforms baselines in modeling affective inflections while mitigating workload (and conditionally mitigating temporal burden). Importantly PREFAB improves annotator confidence without degrading annotation quality.

</details>


### [97] [Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval](https://arxiv.org/abs/2601.13969)
*Joaquín Polonuer,Lucas Vittor,Iñaki Arango,Ayush Noori,David A. Clifton,Luciano Del Corro,Marinka Zitnik*

Main category: cs.AI

TL;DR: ARK introduces an adaptive method for retrieving evidence from knowledge graphs, optimizing search and traversal balance while eliminating reliance on seed selection and preset parameters.


<details>
  <summary>Details</summary>
Motivation: To enhance evidence retrieval from knowledge graphs without the restrictions of seed-based or fixed-depth methods, optimizing both query specificity and relational traversal.

Method: ARK employs a dual-operation toolset for global lexical searches and one-hop neighborhood exploration, integrating breadth and depth retrieval dynamically. It also uses label-free imitation for model distillation.

Result: ARK achieves significant improvements in retrieval metrics (Hit@1 & MRR), surpassing traditional methods and enhancing performance across datasets through model distillation.

Conclusion: ARK provides an adaptable retrieval approach that balances breadth and depth effectively in knowledge graphs, showing promise for more versatile evidence querying processes.

Abstract: Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.

</details>


### [98] [Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics](https://arxiv.org/abs/2601.14027)
*Junqi Liu,Zihao Zhou,Zekai Zhu,Marco Dos Santos,Weikun He,Jiawei Liu,Ran Wang,Yunzhou Xie,Junqiao Zhao,Qiufeng Wang,Lihong Zhi,Jia Li,Wenda Li*

Main category: cs.AI

TL;DR: The paper introduces a new paradigm using a general coding agent for formal theorem proving instead of task-specific pipelines and trained provers, emphasizing its flexibility and generality.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to formal theorem proving are task-specific and rely on trained models, limiting their usability and flexibility. The authors aim to leverage general coding agents for diverse reasoning tasks.

Method: The authors propose Numina-Lean-Agent, which integrates Claude Code with Numina-Lean-MCP. This system autonomously interacts with Lean, retrieves relevant theorems, and uses auxiliary tools for reasoning.

Result: Numina-Lean-Agent achieves strong performance, successfully solving all problems in Putnam 2025 and assisting in formalizing the Brascamp-Lieb theorem.

Conclusion: The paradigm using general coding agents allows improved performance, flexibility, and generality in formal theorem proving, demonstrating potential for broader applications in mathematics and reasoning.

Abstract: Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.

</details>


### [99] [Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems](https://arxiv.org/abs/2601.14096)
*Benedikt Hartl,Léo Pio-Lopez,Chris Fields,Michael Levin*

Main category: cs.AI

TL;DR: The paper proposes a framework unifying cognition in natural and synthetic systems based on scale-invariant principles such as remapping and navigating embedding spaces to explain decision-making.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to understand cognition across diverse agents, both biological and artificial, and to discover universal principles underlying decision-making irrespective of the system's origin or composition.

Method: The authors propose analyzing cognition through invariants: (1) remapping embedding spaces to adapt to environments, and (2) iterative navigation within these spaces. Examples span biological systems like cells and organisms, and synthetic systems like neural networks.

Result: The study identifies parallels between biological and artificial systems, demonstrating that both utilize remapping and navigation principles through distributed error correction in embedding spaces.

Conclusion: The paper concludes that the shared framework provides a substrate-independent foundation for understanding and engineering adaptive intelligence across scales, bridging biological and artificial entities.

Abstract: The emerging field of diverse intelligence seeks an integrated view of problem-solving in agents of very different provenance, composition, and substrates. From subcellular chemical networks to swarms of organisms, and across evolved, engineered, and chimeric systems, it is hypothesized that scale-invariant principles of decision-making can be discovered. We propose that cognition in both natural and synthetic systems can be characterized and understood by the interplay between two equally important invariants: (1) the remapping of embedding spaces, and (2) the navigation within these spaces. Biological collectives, from single cells to entire organisms (and beyond), remap transcriptional, morphological, physiological, or 3D spaces to maintain homeostasis and regenerate structure, while navigating these spaces through distributed error correction. Modern Artificial Intelligence (AI) systems, including transformers, diffusion models, and neural cellular automata enact analogous processes by remapping data into latent embeddings and refining them iteratively through contextualization. We argue that this dual principle - remapping and navigation of embedding spaces via iterative error minimization - constitutes a substrate-independent invariant of cognition. Recognizing this shared mechanism not only illuminates deep parallels between living systems and artificial models, but also provides a unifying framework for engineering adaptive intelligence across scales.

</details>


### [100] [Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance](https://arxiv.org/abs/2601.14171)
*Qianli Ma,Chang Guo,Zhiheng Tian,Siyu Wang,Jipeng Xiao,Yuanhao Yue,Zhipeng Zhang*

Main category: cs.AI

TL;DR: RebuttalAgent is a multi-agent framework designed to improve rebuttal generation by emphasizing evidence alignment and addressing limitations like hallucination and inadequate critique addressing.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the challenges in rebuttal writing, such as the mismatch between reviewer feedback and manuscript details, and issues like hallucination and lack of grounding in current systems.

Method: The authors propose RebuttalAgent, a system that decomposes reviewer feedback into atomic concerns, builds hybrid contexts with summaries and high-fidelity text, and employs external search modules for additional evidence gathering. It generates inspectable response plans before drafting to ensure evidence-based arguments.

Result: RebuttalAgent, evaluated on the RebuttalBench framework, surpasses strong baselines in terms of feedback coverage, response faithfulness, and strategic coherence.

Conclusion: RebuttalAgent provides a transparent, controllable, and effective solution for the peer review process, ensuring precise alignment with reviewer feedback. The authors plan to release the code for broader usage.

Abstract: Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.

</details>


### [101] [Toward Efficient Agents: Memory, Tool learning, and Planning](https://arxiv.org/abs/2601.14192)
*Xiaofang Yang,Lijun Li,Heng Zhou,Tong Zhu,Xiaoye Qu,Yuchen Fan,Qianshan Wei,Rui Ye,Li Kang,Yiran Qin,Zhiqiang Kou,Daizong Liu,Qi Li,Ning Ding,Siheng Chen,Jing Shao*

Main category: cs.AI

TL;DR: The paper explores efficiency within agentic systems derived from large language models, focusing on memory, tool learning, and planning while balancing effectiveness and cost metrics.


<details>
  <summary>Details</summary>
Motivation: Efficiency in agentic systems has not been prioritized despite its importance for real-world deployment, in contrast to the focus on improving effectiveness.

Method: The study reviews various approaches focusing on memory compression, reinforcement learning rewards optimization, and controlled search mechanisms for efficiency gains. Efficiency is characterized by effectiveness under cost constraints and cost required for comparable effectiveness.

Result: It provides a detailed examination of efficiency benchmarks, evaluation protocols, and metrics from both methodological and benchmark perspectives.

Conclusion: Efficiency optimization is a key challenge in agent development. The study offers insights into enhancing agent systems while highlighting future directions and ongoing challenges.

Abstract: Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [102] [NuRedact: Non-Uniform eFPGA Architecture for Low-Overhead and Secure IP Redaction](https://arxiv.org/abs/2601.11770)
*Voktho Das,Kimia Azar,Hadi Kamali*

Main category: cs.AR

TL;DR: NuRedact, a full-custom eFPGA redaction framework, offers significant security advantages with reduced overhead by leveraging architectural non-uniformity.


<details>
  <summary>Details</summary>
Motivation: Existing reconfigurable-based redaction techniques for IC security, such as LUT and eFPGA, are inefficient due to high costs stemming from artificial complexity meant to hinder reverse engineering.

Method: NuRedact introduces a three-stage methodology: (1) generating custom fabrics with pin-mapping irregularity, (2) altering VPR with Python-based optimization for non-uniform placement, and (3) enabling redaction-aware IP module reconfiguration and mapping.

Result: The method results in up to 9x area reduction compared to traditional uniform fabrics, competitive efficiency versus other redaction methods, and robust resistance to SAT-based, cyclic, and sequential attacks.

Conclusion: NuRedact demonstrates that combining architectural non-uniformity with optimized methodologies achieves a balanced security-to-efficiency trade-off in eFPGA redaction.

Abstract: While logic locking has been extensively studied as a countermeasure against integrated circuit (IC) supply chain threats, recent research has shifted toward reconfigurable-based redaction techniques, e.g., LUT- and eFPGA-based schemes. While these approaches raise the bar against attacks, they incur substantial overhead, much of which arises not from genuine functional reconfigurability need, but from artificial complexity intended solely to frustrate reverse engineering (RE). As a result, fabrics are often underutilized, and security is achieved at disproportionate cost. This paper introduces NuRedact, the first full-custom eFPGA redaction framework that embraces architectural non-uniformity to balance security and efficiency. Built as an extension of the widely adopted OpenFPGA infrastructure, NuRedact introduces a three-stage methodology: (i) custom fabric generation with pin-mapping irregularity, (ii) VPR-level modifications to enable non-uniform placement guided by an automated Python-based optimizer, and (iii) redaction-aware reconfiguration and mapping of target IP modules. Experimental results show up to 9x area reduction compared to conventional uniform fabrics, achieving competitive efficiency with LUT-based and even transistor-level redaction techniques while retaining strong resilience. From a security perspective, NuRedact fabrics are evaluated against state-of-the-art attack models, including SAT-based, cyclic, and sequential variants, and show enhanced resilience while maintaining practical design overheads.

</details>


### [103] [Domain-specific Hardware Acceleration for Model Predictive Path Integral Control](https://arxiv.org/abs/2601.12089)
*Erwan Tanguy-Legac,Tommaso Belvedere,Gianluca Corsini,Marco Tognon,Marcello Traiola*

Main category: cs.AR

TL;DR: This paper introduces a hardware accelerator for MPPI control that is both energy-efficient and results in more accurate trajectories compared to GPU-based implementations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a solution for real-time robotic system control that is energy-efficient and can handle non-linear systems effectively, addressing the limitations of GPU implementations (high power consumption) and the challenges of MPC in non-linear systems.

Method: The authors designed and simulated a hardware accelerator specifically for MPPI control, optimizing it for energy efficiency and performance.

Result: The simulation results indicate that the proposed MPPI custom accelerator achieves more accurate trajectories than GPU-based MPPI implementations.

Conclusion: The custom MPPI hardware accelerator is a promising solution for energy-efficient, accurate control in robotic systems, particularly outperforming GPU-based approaches.

Abstract: Accurately controlling a robotic system in real time is a challenging problem. To address this, the robotics community has adopted various algorithms, such as Model Predictive Control (MPC) and Model Predictive Path Integral (MPPI) control. The first is difficult to implement on non-linear systems such as unmanned aerial vehicles, whilst the second requires a heavy computational load. GPUs have been successfully used to accelerate MPPI implementations; however, their power consumption is often excessive for autonomous or unmanned targets, especially when battery-powered. On the other hand, custom designs, often implemented on FPGAs, have been proposed to accelerate robotic algorithms while consuming considerably less energy than their GPU (or CPU) implementation. However, no MPPI custom accelerator has been proposed so far. In this work, we present a hardware accelerator for MPPI control and simulate its execution. Results show that the MPPI custom accelerator allows more accurate trajectories than GPU-based MPPI implementations.

</details>


### [104] [Biological Intuition on Digital Hardware: An RTL Implementation of Poisson-Encoded SNNs for Static Image Classification](https://arxiv.org/abs/2601.12156)
*Debabrata Das,Yogeeth G. K.,Arnav Gupta*

Main category: cs.AR

TL;DR: The paper introduces a hardware-efficient Spiking Neural Network (SNN) core implemented in SystemVerilog that reduces power consumption and computation footprint for edge AI devices.


<details>
  <summary>Details</summary>
Motivation: High power consumption and latency challenges of Artificial Neural Networks for edge AI (TinyML).

Method: Designing a cycle-accurate neuromorphic SNN core using Leaky Integrate-and-Fire neurons, fixed-point arithmetic, and dynamic pruning techniques.

Result: Achieved 89% accuracy on digit classification with reduced computation footprint and rapid convergence in limited timesteps.

Conclusion: The work offers a scalable, energy-efficient neuromorphic hardware solution for FPGA and ASIC applications.

Abstract: The deployment of Artificial Intelligence on edge devices (TinyML) is often constrained by the high power consumption and latency associated with traditional Artificial Neural Networks (ANNs) and their reliance on intensive Matrix-Multiply (MAC) operations. Neuromorphic computing offers a compelling alternative by mimicking biological efficiency through event-driven processing. This paper presents the design and implementation of a cycle-accurate, hardware-oriented Spiking Neural Network (SNN) core implemented in SystemVerilog. Unlike conventional accelerators, this design utilizes a Leaky Integrate-and-Fire (LIF) neuron model powered by fixed-point arithmetic and bit-wise primitives (shifts and additions) to eliminate the need for complex floating-point hardware. The architecture features an on-chip Poisson encoder for stochastic spike generation and a novel active pruning mechanism that dynamically disables neurons post-classification to minimize dynamic power consumption. We demonstrate the hardware's efficacy through a fully connected layer implementation targeting digit classification. Simulation results indicate that the design achieves rapid convergence (89% accuracy) within limited timesteps while maintaining a significantly reduced computational footprint compared to traditional dense architectures. This work serves as a foundational building block for scalable, energy-efficient neuromorphic hardware on FPGA and ASIC platforms.

</details>


### [105] [CD-PIM: A High-Bandwidth and Compute-Efficient LPDDR5-Based PIM for Low-Batch LLM Acceleration on Edge-Device](https://arxiv.org/abs/2601.12298)
*Ye Lin,Chao Fang,Xiaoyong Song,Qi Wu,Anying Jiang,Yichuan Bai,Li Du*

Main category: cs.AR

TL;DR: Low-batch large language model edge deployments face memory bandwidth issues during GEMV operations. CD-PIM addresses these with enhanced modes, interleaving strategies, and efficient mapping.


<details>
  <summary>Details</summary>
Motivation: Edge deployment of LLMs encounters memory bandwidth bottlenecks during GEMV operations, and existing PIM architectures have limitations that require innovative solutions.

Method: The paper proposes CD-PIM with a high-bandwidth mode, interleaving mode for mixed workloads, efficient computation units, and optimized data mapping techniques.

Result: CD-PIM achieves up to 11.42x speedup compared to GPU-only baseline and a 4.25x improvement against other PIM designs in specific tasks, along with gains in low-batch scenarios.

Conclusion: CD-PIM effectively addresses key challenges in GEMV operations for LLM edge deployment, enabling better bandwidth and computation efficiencies.

Abstract: Edge deployment of low-batch large language models (LLMs) faces critical memory bandwidth bottlenecks when executing memory-intensive general matrix-vector multiplications (GEMV) operations. While digital processing-in-memory (PIM) architectures promise to accelerate GEMV operations, existing PIM-equipped edge devices still suffer from three key limitations: limited bandwidth improvement, component under-utilization in mixed workloads, and low compute capacity of computing units (CUs). In this paper, we propose CD-PIM to address these challenges through three key innovations. First, we introduce a high-bandwidth compute-efficient mode (HBCEM) that enhances bandwidth by dividing each bank into four pseudo-banks through segmented global bitlines. Second, we propose a low-batch interleaving mode (LBIM) to improve component utilization by overlapping GEMV operations with GEMM operations. Third, we design a compute-efficient CU that performs enhanced GEMV operations in a pipelined manner by serially feeding weight data into the computing core. Forth, we adopt a column-wise mapping for the key-cache matrix and row-wise mapping for the value-cache matrix, which fully utilizes CU resources. Our evaluation shows that compared to a GPU-only baseline and state-of-the-art PIM designs, our CD-PIM achieves 11.42x and 4.25x speedup on average within a single batch in HBCEM mode, respectively. Moreover, for low-batch sizes, the CD-PIM achieves an average speedup of 1.12x in LBIM compared to HBCEM.

</details>


### [106] [Best Practices for Large Load Interconnections: A North American Perspective on Data Centers](https://arxiv.org/abs/2601.12686)
*Rafi Zahedi,Amin Zamani,Rahul Anilkumar*

Main category: cs.AR

TL;DR: The paper examines best practices for interconnecting large electrical loads like data centers, highlighting challenges and proposing guidelines.


<details>
  <summary>Details</summary>
Motivation: Rapid growth in large electrical loads driven by advances like AI requires new transmission interconnection standards.

Method: Reviewed utility and operator guidelines across North America, synthesizing into technical requirements. Includes cross-utility comparisons and European insights.

Result: Highlighted issues in power quality, telemetry, testing, and protection, as well as gaps in specifications for load management and disturbance recovery.

Conclusion: The paper proposes practical guidelines for utilities and developers to address technical challenges in large-load interconnections.

Abstract: Large loads are expanding rapidly across North America, led by data centers, cryptocurrency mining, hydrogen production facilities, and heavy-duty charging stations. Each class presents distinct electrical characteristics, but data centers are drawing particular attention as AI deployment drives unprecedented capacity growth. Their scale, duty cycles, and converter-dominated interfaces introduce new challenges for transmission interconnections, especially regarding disturbance behavior, steady-state performance, and operational visibility. This paper reviews best practices for large-load interconnections across North America, synthesizing utility and system operator guidelines into a coherent set of technical requirements. The approach combines handbook and manual analysis with cross-utility comparisons and an outlook on European directions. The review highlights requirements on power quality, telemetry, commissioning tests, and protection coordination, while noting gaps in ride-through specifications, load-variation management, and post-disturbance recovery targets. Building on these findings, the paper proposes practical guidance for developers and utilities.

</details>


### [107] [PRIMAL: Processing-In-Memory Based Low-Rank Adaptation for LLM Inference Accelerator](https://arxiv.org/abs/2601.13628)
*Yue Jiet Chong,Yimin Wang,Zhen Wu,Xuanyao Fong*

Main category: cs.AR

TL;DR: PRIMAL is a processing-in-memory accelerator for large language models, achieving higher throughput and energy efficiency with innovative techniques for LoRA updates.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in LLM inference with Low-Rank Adaptation (LoRA), including the need for better performance and energy savings.

Method: Introduces PRIMAL, a PIM-based accelerator with a novel SRAM reprogramming and power gating scheme, heterogeneous PIM processing elements, and optimized spatial mapping.

Result: PRIMAL delivers 1.5× throughput and 25× energy efficiency improvements compared to NVIDIA H100, tested on Llama-13B with LoRA rank 8.

Conclusion: PRIMAL demonstrates significant advancements in LLM acceleration and energy efficiency, proving the potential of PIM architectures for this domain.

Abstract: This paper presents PRIMAL, a processing-in-memory (PIM) based large language model (LLM) inference accelerator with low-rank adaptation (LoRA). PRIMAL integrates heterogeneous PIM processing elements (PEs), interconnected by 2D-mesh inter-PE computational network (IPCN). A novel SRAM reprogramming and power gating (SRPG) scheme enables pipelined LoRA updates and sub-linear power scaling by overlapping reconfiguration with computation and gating idle resources. PRIMAL employs optimized spatial mapping and dataflow orchestration to minimize communication overhead, and achieves $1.5\times$ throughput and $25\times$ energy efficiency over NVIDIA H100 with LoRA rank 8 (Q,V) on Llama-13B.

</details>


### [108] [The Non-Predictability of Mispredicted Branches using Timing Information](https://arxiv.org/abs/2601.13804)
*Ioannis Constantinou,Arthur Perais,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: This paper explores whether incorporating microarchitectural timing information can improve branch prediction accuracy, finding mixed results but identifying scenarios with potential advantages.


<details>
  <summary>Details</summary>
Motivation: Branch misprediction latency leads to significant performance degradation and wasted energy in modern cores, motivating the development of improved predictors.

Method: The authors introduce Speculative Branch Resolution (SBR), a technique using timing information from the Reorder Buffer (ROB) for re-predicting branches, simulated via gem5 with TAGE-Like predictors.

Result: Experiments revealed no performance gains over state-of-the-art predictors but identified specific cases of hard-to-predict branches where timing data proved advantageous.

Conclusion: Microarchitectural timing information may enhance prediction accuracy on select branches, but more research is required to refine methods and identify impactful data vectors.

Abstract: Branch misprediction latency is one of the most important contributors to performance degradation and wasted energy consumption in a modern core. State-of-the-art predictors generally perform very well but occasionally suffer from high Misprediction Per Kilo Instruction due to hard-to-predict branches. In this work, we investigate if predicting branches using microarchitectural information, in addition to traditional branch history, can improve prediction accuracy. Our approach considers branch timing information (resolution cycle) both for older branches in the Reorder Buffer (ROB) and recently committed, and for younger branches relative to the branch we re-predict. We propose Speculative Branch Resolution (SBR) in which, N cycles after a branch allocates in the ROB, various timing information is collected and used to re-predict. Using the gem5 simulator we implement and perform a limit-study of SBR using a TAGE-Like predictor. Our experiments show that the post-alloc timing information we used was not able to yield performance gains over an unbounded TAGE-SC. However, we find two hard to predict branches where timing information did provide an advantage and thoroughly analysed one of them to understand why. This finding suggests that predictors may benefit from specific microarchitectural information to increase accuracy on specific hard to predict branches and that overriding predictions in the backend may yet yield performance benefits, but that further research is needed to determine such information vectors.

</details>


### [109] [From RTL to Prompt Coding: Empowering the Next Generation of Chip Designers through LLMs](https://arxiv.org/abs/2601.13815)
*Lukas Krupp,Matthew Venn,Norbert Wehn*

Main category: cs.AR

TL;DR: The paper introduces an LLM-supported learning platform for beginners in chip design, enabling holistic support for both frontend and backend processes.


<details>
  <summary>Details</summary>
Motivation: Chip design education is often too technical and inaccessible for beginners. This paper aims to simplify the process and make education in this field more engaging for early learners.

Method: The platform integrates an LLM chat agent into a browser-based workflow using the Tiny Tapeout ecosystem. It enables learners to progress from design ideas to creating tapeout-ready chips.

Result: In a case study with 18 high-school students, eight functional VGA chip designs in 130 nm technology were successfully created within 90 minutes despite the students' lack of prior knowledge.

Conclusion: The LLM-assisted chip design platform demonstrates its effectiveness in educating beginners, broadening the field's accessibility, and inspiring early learners.

Abstract: This paper presents an LLM-based learning platform for chip design education, aiming to make chip design accessible to beginners without overwhelming them with technical complexity. It represents the first educational platform that assists learners holistically across both frontend and backend design. The proposed approach integrates an LLM-based chat agent into a browser-based workflow built upon the Tiny Tapeout ecosystem. The workflow guides users from an initial design idea through RTL code generation to a tapeout-ready chip. To evaluate the concept, a case study was conducted with 18 high-school students. Within a 90-minute session they developed eight functional VGA chip designs in a 130 nm technology. Despite having no prior experience in chip design, all groups successfully implemented tapeout-ready projects. The results demonstrate the feasibility and educational impact of LLM-assisted chip design, highlighting its potential to attract and inspire early learners and significantly broaden the target audience for the field.

</details>


### [110] ['1'-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators](https://arxiv.org/abs/2601.14087)
*Ruichi Han,Yizhi Chen,Tong Lei,Jordi Altayo Gonzalez,Ahmed Hemani*

Main category: cs.AR

TL;DR: This paper addresses interconnect power consumption in DNN accelerators by proposing a comparison-free approximate sorting unit optimized for CNNs, achieving significant area and link power benefits.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle hardware interconnect power consumption issues in DNN accelerators, which hinder performance. Sorting data based on '1'-bit counts reduces switching activity but lacks an efficient hardware implementation.

Method: The paper introduces an approximate hardware sorting implementation that groups population counts into coarse-grained buckets, reducing hardware area while maintaining power-conserving data reordering benefits.

Result: The proposed sorting unit achieves up to 35.4% reduction in hardware area and maintains a link power switching reduction of 19.50% compared to 20.42% achieved by precise implementations.

Conclusion: The paper concludes that approximate computing in sorting units effectively reduces hardware area without significantly sacrificing power-saving benefits, and highlights its feasibility for DNN accelerators.

Abstract: Interconnect power consumption remains a bottleneck in Deep Neural Network (DNN) accelerators. While ordering data based on '1'-bit counts can mitigate this via reduced switching activity, practical hardware sorting implementations remain underexplored. This work proposes the hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks (CNN). By leveraging approximate computing to group population counts into coarse-grained buckets, our design achieves hardware area reductions while preserving the link power benefits of data reordering. Our approximate sorting unit achieves up to 35.4% area reduction while maintaining 19.50\% BT reduction compared to 20.42% of precise implementation.

</details>


### [111] [CREATE: Cross-Layer Resilience Characterization and Optimization for Efficient yet Reliable Embodied AI Systems](https://arxiv.org/abs/2601.14140)
*Tong Xie,Yijiahao Qi,Jinqi Wen,Zishen Wan,Yanchi Dong,Zihao Wang,Shaofei Cai,Yitao Liang,Tianyu Jia,Yuan Wang,Runsheng Wang,Meng Li*

Main category: cs.AR

TL;DR: Embodied AI faces challenges in deploying to energy-constrained environments; "CREATE" offers resilient energy and reliability optimization methods for enhanced efficiency.


<details>
  <summary>Details</summary>
Motivation: Embodied AI bridges AI with real-world tasks, but deployment on battery-powered devices faces challenges due to high computational energy demands potentially causing errors.

Method: CREATE introduces a fault-tolerance mechanism at circuit, model, and application layers using anomaly detection, weight-rotation-enhanced planning, and voltage scaling.

Result: CREATE achieves an average of 40.6% computational energy savings, 29.5% to 37.3% chip-level energy savings, and boosts battery life by approximately 15% to 30%.

Conclusion: CREATE demonstrates that optimizing heterogeneous resilience across layers can significantly enhance energy efficiency and reliability in embodied AI systems without sacrificing task quality.

Abstract: Embodied Artificial Intelligence (AI) has recently attracted significant attention as it bridges AI with the physical world. Modern embodied AI systems often combine a Large Language Model (LLM)-based planner for high-level task planning and a reinforcement learning (RL)-based controller for low-level action generation, enabling embodied agents to tackle complex tasks in real-world environments. However, deploying embodied agents remains challenging due to their high computation requirements, especially for battery-powered local devices. Although techniques like lowering operating voltage can improve energy efficiency, they can introduce bit errors and result in task failures. In this work, we propose CREATE, a general design principle that leverages heterogeneous resilience at different layers for synergistic energy-reliability co-optimization. For the first time, we conduct a comprehensive error injection study on modern embodied AI systems and observe an inherent but heterogeneous fault tolerance. Building upon these insights, we develop an anomaly detection and clearance mechanism at the circuit level to eliminate outlier errors. At the model level, we propose a weight-rotation-enhanced planning algorithm to improve the fault tolerance of the LLM-based planner. Furthermore, we introduce an application-level technique, autonomy-adaptive voltage scaling, to dynamically adjust the operating voltage of the controllers. The voltage scaling circuit is co-designed to enable online voltage adjustment. Extensive experiments demonstrate that without compromising task quality, CREATE achieves 40.6% computational energy savings on average over nominal-voltage baselines and 35.0% over prior-art techniques. This further leads to 29.5% to 37.3% chip-level energy savings and approximately a 15% to 30% improvement in battery life.

</details>


### [112] [The Quest for Reliable AI Accelerators: Cross-Layer Evaluation and Design Optimization](https://arxiv.org/abs/2601.14148)
*Meng Li,Tong Xie,Zuodong Zhang,Runsheng Wang*

Main category: cs.AR

TL;DR: The paper addresses reliability challenges in AI accelerators due to aging effects and process variations in nanoscale CMOS technology, proposing co-optimized designs to achieve both reliability and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Reliability issues in AI accelerators are exacerbated by CMOS technology scaling to nanoscale, with significant concerns including aging effects and process variations. Traditional reliability approaches lead to inefficiency, creating a need for innovative solutions.

Method: The paper introduces (1) a dynamic timing analyzer accounting for aging and variations, (2) dataflow optimization focusing on critical input pattern reduction, and (3) resilience characterization and novel architecture design for LLMs, leveraging integrated cross-layer modeling.

Result: These proposed methods enable reliable AI acceleration with improved computational efficiency and robustness for advanced AI workloads.

Conclusion: By integrating cross-layer reliability modeling with AI workload-specific optimizations, the paper demonstrates successful designs that balance reliability maintenance and performance efficiency.

Abstract: As the CMOS technology pushes to the nanoscale, aging effects and process variations have become increasingly pronounced, posing significant reliability challenges for AI accelerators. Traditional guardband-based design approaches, which rely on pessimistic timing margin, sacrifice significant performance and computational efficiency, rendering them inadequate for high-performance AI computing demands. Current reliability-aware AI accelerator design faces two core challenges: (1) the lack of systematic cross-layer analysis tools to capture coupling reliability effects across device, circuit, architecture, and application layers; and (2) the fundamental trade-off between conventional reliability optimization and computational efficiency. To address these challenges, this paper systematically presents a series of reliability-aware accelerator designs, encompassing (1) aging and variation-aware dynamic timing analyzer, (2) accelerator dataflow optimization using critical input pattern reduction, and (3) resilience characterization and novel architecture design for large language models (LLMs). By tightly integrating cross-layer reliability modeling and AI workload characteristics, these co-optimization approaches effectively achieve reliable and efficient AI acceleration.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [113] [Context Discipline and Performance Correlation: Analyzing LLM Performance and Quality Degradation Under Varying Context Lengths](https://arxiv.org/abs/2601.11564)
*Ahilan Ayyachamy Nadar Ponnusamy,Karthic Chandran,M Maruf Hossain*

Main category: cs.CL

TL;DR: The paper studies the computational challenges in managing expanded context in LLMs, emphasizing performance degradation due to large irrelevant contexts and KV cache growth.


<details>
  <summary>Details</summary>
Motivation: Understand the trade-offs between managing large context windows for reasoning while mitigating computational overhead in dense transformer architectures.

Method: Analyzed performance degradation in LLMs (Llama-3.1-70B and Qwen1.5-14B) under extensive context windows; included investigation into Mixture-of-Experts (MoE) architecture behavior at different scales.

Result: Non-linear performance degradation due to KV cache growth was identified, alongside behavioral anomalies in MoE architectures at high-scale contexts.

Conclusion: Expanded context window sizes create computational trade-offs that hinder model performance; MoE architectures require consideration for addressing infrastructure bottlenecks.

Abstract: The scaling trend in Large Language Models (LLMs) has prioritized increasing the maximum context window to facilitate complex, long-form reasoning and document analysis. However, managing this expanded context introduces severe computational overhead. This paper investigates the critical trade-off between system performance and model quality when dense transformer architectures--specifically Llama-3.1-70B and Qwen1.5-14B--are exposed to large volumes of irrelevant and distracting context. The research identifies a non-linear performance degradation tied to the growth of the Key-Value (KV) cache. Furthermore, an extended analysis of the Mixture-of-Experts (MoE) architecture reveals unique behavioral anomalies at varying context scales, suggesting that architectural benefits may be masked by infrastructure bottlenecks at high token volumes.

</details>


### [114] [Compass-Embedding v4: Robust Contrastive Learning for Multilingual E-commerce Embeddings](https://arxiv.org/abs/2601.11565)
*Pakorn Ueareeworakul,Shuman Liu,Jinghao Feng,Ling Hu,Zhantang Shi,Chengqi Sun,Liang Yao,Panyi Ouyang,Haibo Zhang,Anxiang Zeng*

Main category: cs.CL

TL;DR: The paper introduces Compass-Embedding v4, a high-efficiency multilingual embedding framework for Southeast Asian e-commerce, addressing challenges like data scarcity and noisy supervision, and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the lack of high-quality semantic representations for low-resource Southeast Asian languages, which hinders retrieval, recommendation, and search systems in emerging e-commerce markets.

Method: They use techniques like Class-Aware Masking (CAM) for improved semantic discrimination, diversified training data creation via synthetic data and translations, and optimization of inference through large-batch training, vLLM, and FP8 quantization.

Result: Compass-Embedding v4 delivers state-of-the-art performance in SEA languages for domain-specific retrieval and classification tasks, surpassing general-purpose models while also performing well in high-resource languages.

Conclusion: The framework successfully addresses challenges in low-resource e-commerce scenarios, combining domain-specific optimization with high efficiency suitable for large-scale deployment.

Abstract: As global e-commerce rapidly expands into emerging markets, the lack of high-quality semantic representations for low-resource languages has become a decisive bottleneck for retrieval, recommendation, and search systems. In this work, we present Compass-Embedding v4, a high-efficiency multilingual embedding framework specifically optimized for Southeast Asian (SEA) e-commerce scenarios, where data scarcity, noisy supervision, and strict production constraints jointly challenge representation learning. Compass-Embedding v4 addresses three core challenges. First, large-batch contrastive training under mixed task supervision introduces systematic false negatives that degrade semantic alignment. We propose Class-Aware Masking (CAM), a lightweight modification to the InfoNCE objective that suppresses invalid in-batch negatives and improves semantic discrimination without altering training efficiency. Second, low-resource SEA languages suffer from limited and uneven data coverage. We construct a diversified training corpus through context-grounded synthetic data generation, cross-lingual translation, and structured e-commerce data construction, enabling robust multilingual and domain-specific learning. Third, production deployment requires high-throughput inference while preserving embedding quality. We combine robustness-driven large-batch training with spherical model merging to mitigate catastrophic forgetting, and optimize inference via vLLM and FP8 quantization. Extensive evaluations across multilingual benchmarks and proprietary e-commerce tasks show that Compass-Embedding v4 achieves state-of-the-art performance on major SEA languages, significantly outperforming general-purpose embedding models in domain-specific retrieval and classification, while maintaining competitive performance on high-resource languages.

</details>


### [115] [Measuring Stability Beyond Accuracy in Small Open-Source Medical Large Language Models for Pediatric Endocrinology](https://arxiv.org/abs/2601.11567)
*Vanessa D'Amario,Randy Daniel,Alessandro Zanetti,Dhruv Edamadaka,Nitya Alaparthy,Joshua Tarkoff*

Main category: cs.CL

TL;DR: This paper evaluates six small open-source medical large language models (LLMs) in pediatric endocrinology, emphasizing the limitations in consistency, reasoning, and robustness of their outputs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in evaluating medical LLMs beyond traditional accuracy benchmarks, focusing on their reasoning, robustness under stochastic conditions, and clinical applicability.

Method: The authors used medical MCQs, human evaluation, and clinical reviews to test the models under deterministic and stochastic settings, assessing consistency, correctness, reasoning, and impact of prompt/system-level variations.

Result: HuatuoGPT-o1-8B exhibited the highest performance but showed that consistency does not guarantee correctness. All models displayed variability influenced by prompt perturbations and system configurations.

Conclusion: Small medical LLMs demonstrate promise but present concerns like reproducibility issues and reasoning biases. A broader diagnostic framework is needed for reliable clinical decision support.

Abstract: Small open-source medical large language models (LLMs) offer promising opportunities for low-resource deployment and broader accessibility. However, their evaluation is often limited to accuracy on medical multiple choice question (MCQ) benchmarks, and lacks evaluation of consistency, robustness, or reasoning behavior. We use MCQ coupled to human evaluation and clinical review to assess six small open-source medical LLMs (HuatuoGPT-o1 (Chen 2024), Diabetica-7B, Diabetica-o1 (Wei 2024), Meditron3-8B (Sallinen2025), MedFound-7B (Liu 2025), and ClinicaGPT-base-zh (Wang 2023)) in pediatric endocrinology. In deterministic settings, we examine the effect of prompt variation on models' output and self-assessment bias. In stochastic settings, we evaluate output variability and investigate the relationship between consistency and correctness. HuatuoGPT-o1-8B achieved the highest performance. The results show that high consistency across the model response is not an indicator of correctness, although HuatuoGPT-o1-8B showed the highest consistency rate. When tasked with selecting correct reasoning, both HuatuoGPT-o1-8B and Diabetica-o1 exhibit self-assessment bias and dependency on the order of the candidate explanations. Expert review of incorrect reasoning rationales identified a mix of clinically acceptable responses and clinical oversight. We further show that system-level perturbations, such as differences in CUDA builds, can yield statistically significant shifts in model output despite stable accuracy. This work demonstrates that small, semantically negligible prompt perturbations lead to divergent outputs, raising concerns about reproducibility of LLM-based evaluations and highlights the output variability under different stochastic regimes, emphasizing the need of a broader diagnostic framework to understand potential pitfalls in real-world clinical decision support scenarios.

</details>


### [116] [An Empirical Analysis of Fine-Tuning Large Language Models on Bioinformatics Literature: PRSGPT and BioStarsGPT](https://arxiv.org/abs/2601.11573)
*Muhammad Muneeb,David B. Ascher*

Main category: cs.CL

TL;DR: The paper introduces a pipeline for fine-tuning language models on bioinformatics data, demonstrated with PRSGPT and BioStarsGPT. Results show significant improvements in benchmarks and practical applications.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap in large language models (LLMs) lacking specialized knowledge for complex bioinformatics tasks, enabling scalable and efficient fine-tuning for domain-specific applications.

Method: A nine-step pipeline is used, involving data preprocessing, QA generation, quality control, deduplication, clustering, and efficient fine-tuning using LoRA. Three LLMs were fine-tuned and benchmarked across various metrics.

Result: The Qwen2.5-7B model performed the best, with substantial metric improvements for PRSGPT and BioStarsGPT. The datasets created include tens of thousands of QA pairs, and human evaluations demonstrated high accuracy and methodological richness.

Conclusion: The pipeline showcases the potential for scalable fine-tuning of LLMs for specialized bioinformatics purposes while maintaining privacy and addressing practical challenges in development and deployment.

Abstract: Large language models (LLMs) often lack specialized knowledge for complex bioinformatics applications. We present a reproducible pipeline for fine-tuning LLMs on specialized bioinformatics data, demonstrated through two use cases: PRSGPT, focused on polygenic risk score (PRS) tools, and BioStarsGPT, trained on community forum discussions. The nine-step pipeline integrates diverse data sources, structured preprocessing, prompt-based question-answer (QA) generation (via Google Gemini), natural language inference (NLI) for quality control, semantic deduplication, clustering-based data splitting, and parameter-efficient fine-tuning using LoRA. We fine-tuned three LLMs (LLaMA-3.2-3B, Qwen2.5-7B, Gemma) and benchmarked them on over 14 lexical and semantic metrics. Qwen2.5-7B emerged as the best performer, with BLEU-4 and ROUGE-1 improvements of 82\% and 70\% for PRSGPT and 6\% and 18\% for BioStarsGPT, respectively. The open-source datasets produced include over 28,000 QA pairs for PRSGPT and 154,282 for BioStarsGPT. Human evaluation of PRSGPT yielded 61.9\% accuracy on the PRS tools comparison task, comparable to Google Gemini (61.4\%), but with richer methodological detail and accurate citations. BioStarsGPT demonstrated 59\% conceptual accuracy across 142 curated bioinformatics questions. Our pipeline enables scalable, domain-specific fine-tuning of LLMs. It enables privacy-preserving, locally deployable bioinformatics assistants, explores their practical applications, and addresses the challenges, limitations, and mitigation strategies associated with their development and use.

</details>


### [117] [Concept Attractors in LLMs and their Applications](https://arxiv.org/abs/2601.11575)
*Sotirios Panagiotis Chytas,Vikas Singh*

Main category: cs.CL

TL;DR: The paper introduces a framework explaining large language models' (LLMs) behavior using Iterated Function Systems (IFS) and proposes efficient Attractor-based methods to solve multiple NLP tasks without additional training.


<details>
  <summary>Details</summary>
Motivation: To explain how LLMs handle semantically related prompts and leverage this understanding to develop efficient, training-free solutions for real-world language tasks.

Method: The authors describe layers in LLMs as contractive mappings in an IFS framework, leading to concept-specific Attractors. They develop Attractor-based methods for tasks like translation, reducing hallucination, guardrailing, and synthetic data generation.

Result: The Attractor-based methods achieve performance comparable to or better than specialized baselines and provide a generalizable, cost-efficient solution for various NLP tasks.

Conclusion: Attractor-based approaches offer an innovative, training-free alternative for addressing practical issues in NLP, reducing the need for traditional fine-tuning and enhancing generalization.

Abstract: Large language models (LLMs) often map semantically related prompts to similar internal representations at specific layers, even when their surface forms differ widely. We show that this behavior can be explained through Iterated Function Systems (IFS), where layers act as contractive mappings toward concept-specific Attractors. We leverage this insight and develop simple, training-free methods that operate directly on these Attractors to solve a wide range of practical tasks, including language translation, hallucination reduction, guardrailing, and synthetic data generation. Despite their simplicity, these Attractor-based interventions match or exceed specialized baselines, offering an efficient alternative to heavy fine-tuning, generalizable in scenarios where baselines underperform.

</details>


### [118] [LimAgents: Multi-Agent LLMs for Generating Research Limitations](https://arxiv.org/abs/2601.11578)
*Ibrahim Al Azher,Zhishuai Guo,Hamed Alhoori*

Main category: cs.CL

TL;DR: The paper introduces LimAgents, a multi-agent large language model framework that identifies substantive limitations in scientific research by using OpenReview, citations, and peer reviews.


<details>
  <summary>Details</summary>
Motivation: To address the issues of superficial limitation statements produced by zero-shot LLMs and the lack of disclosure of meaningful limitations by authors.

Method: LimAgents employs a multi-agent framework with specialized agents (e.g., limitation extraction, methodological analysis) and a centralized role for refining and compiling limitations. It also uses broader contextual data sources, such as cited/citing papers and OpenReview.

Result: The LimAgents framework showed a notable performance improvement, with a +15.51% coverage gain using RAG + multi-agent GPT-4o and a +4.41% gain using Llama 3 8B in identifying limitations.

Conclusion: LimAgents is highly effective in systematically and substantively identifying limitations in scientific literature, surpassing basic zero-shot approaches.

Abstract: Identifying and articulating limitations is essential for transparent and rigorous scientific research. However, zero-shot large language models (LLMs) approach often produce superficial or general limitation statements (e.g., dataset bias or generalizability). They usually repeat limitations reported by authors without looking at deeper methodological issues and contextual gaps. This problem is made worse because many authors disclose only partial or trivial limitations. We propose LimAgents, a multi-agent LLM framework for generating substantive limitations. LimAgents integrates OpenReview comments and author-stated limitations to provide stronger ground truth. It also uses cited and citing papers to capture broader contextual weaknesses. In this setup, different agents have specific roles as sequential role: some extract explicit limitations, others analyze methodological gaps, some simulate the viewpoint of a peer reviewer, and a citation agent places the work within the larger body of literature. A Judge agent refines their outputs, and a Master agent consolidates them into a clear set. This structure allows for systematic identification of explicit, implicit, peer review-focused, and literature-informed limitations. Moreover, traditional NLP metrics like BLEU, ROUGE, and cosine similarity rely heavily on n-gram or embedding overlap. They often overlook semantically similar limitations. To address this, we introduce a pointwise evaluation protocol that uses an LLM-as-a-Judge to measure coverage more accurately. Experiments show that LimAgents substantially improve performance. The RAG + multi-agent GPT-4o mini configuration achieves a +15.51% coverage gain over zero-shot baselines, while the Llama 3 8B multi-agent setup yields a +4.41% improvement.

</details>


### [119] [Bielik 11B v3: Multilingual Large Language Model for European Languages](https://arxiv.org/abs/2601.11579)
*Krzysztof Ociepa,Łukasz Flis,Remigiusz Kinas,Krzysztof Wróbel,Adrian Gwoździej*

Main category: cs.CL

TL;DR: Bielik 11B v3 is a cutting-edge language model focused on Polish, excelling in various European languages, and surpassing larger models in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To create a high-performance language model for Polish, addressing the scarcity of advanced NLP tools for less-represented languages while ensuring resource efficiency.

Method: The model builds on Mistral 7B architecture, scaled to 11B parameters, using a four-stage training pipeline: pre-training, supervised fine-tuning, Direct Preference Optimization, and reinforcement learning.

Result: Bielik 11B v3 significantly outperforms existing Polish models and many larger models in linguistic and reasoning tasks, showcasing superior parameter efficiency and deployment versatility.

Conclusion: This work establishes Bielik 11B v3 as a benchmark for resource-efficient, high-performance models tailored to Polish and other underrepresented languages.

Abstract: We present Bielik 11B v3, a state-of-the-art language model highly optimized for the Polish language, while also maintaining strong capabilities in other European languages. This model extends the Mistral 7B v0.2 architecture, scaled to 11B parameters via depth up-scaling. Its development involved a comprehensive four-stage training pipeline: continuous pre-training, supervised fine-tuning (SFT), Direct Preference Optimization (DPO), and reinforcement learning.
  Comprehensive evaluations demonstrate that Bielik 11B v3 achieves exceptional performance. It significantly surpasses other specialized Polish language models and outperforms many larger models (with 2-6 times more parameters) on a wide range of tasks, from basic linguistic understanding to complex reasoning.
  The model's parameter efficiency, combined with extensive quantization options, allows for effective deployment across diverse hardware configurations. Bielik 11B v3 not only advances AI capabilities for the Polish language but also establishes a new benchmark for developing resource-efficient, high-performance models for less-represented languages.

</details>


### [120] [Speculative Decoding: Performance or Illusion?](https://arxiv.org/abs/2601.11580)
*Xiaoxuan Liu,Jiaxiang Yu,Jongseok Park,Ion Stoica,Alvin Cheung*

Main category: cs.CL

TL;DR: This paper conducts a systematic study of speculative decoding (SD) in real-world, production-grade Large Language Model (LLM) inference, assessing various SD variants across workloads and datasets, identifying key performance factors, and highlighting performance gaps from theoretical bounds.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate the real-world effectiveness of speculative decoding in LLM inference, as prior studies were based on research prototypes with small batch sizes, which may not reflect production-grade environments.

Method: The authors systematically test multiple speculative decoding variants on a production-grade inference engine (vLLM), with diverse workloads, model scales, and batch sizes, and analyze performance factors affecting SD.

Result: The study reveals that target model verification significantly dominates execution time, acceptance length varies widely, and there are substantial performance gaps between measured and theoretical potential, offering insights into optimizing SD.

Conclusion: There is significant room for improvement in speculative decoding optimization, and this systematic study opens up new research avenues for bridging performance gaps identified in real-world LLM inference scenarios.

Abstract: Speculative decoding (SD) has become a popular technique to accelerate Large Language Model (LLM) inference, yet its real-world effectiveness remains unclear as prior evaluations rely on research prototypes and unrealistically small batch sizes. We present, to our knowledge, the first systematic study of SD on a production-grade and widely deployed inference engine (vLLM), covering multiple SD variants ($n$-gram, EAGLE/EAGLE-3, Draft-Model, Multi-Token Prediction) across diverse workloads, model scales, and batch sizes. We analyze key factors governing SD performance, and quantify a theoretical upper bound on SD speedup. Our results show that verification by the target model dominates the execution, while acceptance length varies markedly across output token positions, requests, and datasets. Comparing measured performance with theoretical bounds reveals substantial gaps between observed and theoretical upper bounds, and we leverage this observation to highlight new research opportunities that our study opens up in improving SD.

</details>


### [121] [Enhancing the QA Model through a Multi-domain Debiasing Framework](https://arxiv.org/abs/2601.11581)
*Yuefeng Wang,ChangJae Lee*

Main category: cs.CL

TL;DR: The paper analyzes biases in QA models like ELECTRA-small and demonstrates a multi-domain debiasing framework that enhances performance in adversarial conditions.


<details>
  <summary>Details</summary>
Motivation: QA models often struggle with biases, especially in adversarial conditions, limiting their reliability in complex queries.

Method: The study uses the ELECTRA-small model on datasets SQuAD v1.1, AddSent, and AddOneSent, applying knowledge distillation, debiasing, and domain expansion techniques.

Result: The multi-domain debiasing framework improved EM and F1 scores up to 2.6 points across all test sets, specifically enhancing performance in adversarial conditions.

Conclusion: Targeted bias mitigation can improve the robustness and reliability of QA systems in challenging contexts.

Abstract: Question-answering (QA) models have advanced significantly in machine reading comprehension but often exhibit biases that hinder their performance, particularly with complex queries in adversarial conditions. This study evaluates the ELECTRA-small model on the Stanford Question Answering Dataset (SQuAD) v1.1 and adversarial datasets AddSent and AddOneSent. By identifying errors related to lexical bias, numerical reasoning, and entity recognition, we develop a multi-domain debiasing framework incorporating knowledge distillation, debiasing techniques, and domain expansion. Our results demonstrate up to 2.6 percentage point improvements in Exact Match (EM) and F1 scores across all test sets, with gains in adversarial contexts. These findings highlight the potential of targeted bias mitigation strategies to enhance the robustness and reliability of natural language understanding systems.

</details>


### [122] [Entropic Context Shaping: Information-Theoretic Filtering for Context-Aware LLM Agents](https://arxiv.org/abs/2601.11585)
*Hyunjun Kim*

Main category: cs.CL

TL;DR: The paper introduces Entropic Context Shaping (ECS), a novel framework to measure the utility of context based on its impact on a model's answer distribution, outperforming traditional lexical similarity methods.


<details>
  <summary>Details</summary>
Motivation: To improve large language model (LLM) agents by effectively distinguishing useful context from irrelevant information.

Method: ECS measures context utility via the signed change in a model's answer probability, relying on information-theoretic principles instead of word overlap comparison.

Result: ECS demonstrates significant improvement over traditional methods such as TF-IDF, achieving a 71.83% relative gain in F1 score on specific multi-turn context selection tasks.

Conclusion: Pragmatic utility measured by ECS is more effective than lexical similarity in precise context selection, showcasing its potential for enhancing LLM applications.

Abstract: Context engineering for large language model (LLM) agents requires distinguishing pragmatically useful information from misleading distractors. We introduce Entropic Context Shaping (ECS), an information-theoretic framework that measures context utility via the shift in the model's answer distribution toward the correct answer. Unlike lexical similarity methods that rely on word overlap, ECS captures pragmatic utility -- whether a passage actually helps answer the question. We formalize utility as the signed change in answer probability and provide theoretical analysis showing that task-irrelevant updates yield near-zero distribution shift. We evaluate on multi-turn context selection tasks using LongMemEval (session-level) and LoCoMo (turn-level) benchmarks. On fine-grained turn selection, ECS with Llama-3.1-8B achieves F1=0.265, a 71.83% relative improvement over TF-IDF (F1=0.154), demonstrating that pragmatic utility outperforms lexical similarity when precise context selection matters. Code and data are available in the supplementary materials.

</details>


### [123] [Towards AGI A Pragmatic Approach Towards Self Evolving Agent](https://arxiv.org/abs/2601.11658)
*Indrajit Kar,Sammy Zonunpuia,Zonunfeli Ralte*

Main category: cs.CL

TL;DR: This paper proposes a hierarchical self-evolving framework for Large Language Model (LLM) agents, enabling them to autonomously improve their reasoning, tools, and capabilities.


<details>
  <summary>Details</summary>
Motivation: Despite their power, LLM agents are static and incapable of self-evolution post-deployment; the authors aim to introduce autonomy and adaptability to these systems.

Method: The method involves a hierarchical framework integrating a Base LLM, operational SLM agent, Code-Gen LLM, and Teacher-LLM, leveraging approaches like Curriculum Learning, Reward-Based Learning, and Genetic Algorithm for agent evolution.

Result: The evaluation on the TaskCraft dataset demonstrates that the evolved agents outperform the original ones, and each evolution paradigm offers its unique strengths (e.g., fast recovery, high task performance, or behavioral diversity).

Conclusion: The framework enables LLM-based agents to self-improve autonomously, showcasing a path toward adaptive and evolving systems for enhanced task performance.

Abstract: Large Language Model (LLM) based agents are powerful yet fundamentally static after deployment, lacking the ability to autonomously expand capabilities, generate new tools, or evolve their reasoning. This work introduces a hierarchical self-evolving multi-agent framework that integrates a Base LLM, an operational SLM agent, a Code-Generation LLM, and a Teacher-LLM to enable continuous adaptation. The workflow begins with the agent attempting a task using reasoning and existing tools; if unsuccessful, it escalates to tool synthesis through the Code-Gen LLM, and when failures persist, it triggers an evolution phase using Curriculum Learning (CL), Reward-Based Learning (RL), or Genetic Algorithm (GA) evolution. Using the TaskCraft dataset rich in hierarchical tasks, tool-use traces, and difficulty scaling we evaluate these paradigms. CL delivers fast recovery and strong generalization, RL excels on high-difficulty tasks, and GA offers high behavioral diversity. Across all settings, evolved agents outperform their originals, demonstrating robust, autonomous, self-improving agentic evolution.

</details>


### [124] [RAC: Retrieval-Augmented Clarification for Faithful Conversational Search](https://arxiv.org/abs/2601.11722)
*Ahmed Rayane Kebir,Vincent Guigue,Lynda Said Lhadj,Laure Soulier*

Main category: cs.CL

TL;DR: The paper introduces RAC, a framework ensuring conversational clarification questions are grounded in the corpus, improving faithfulness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of corpus-grounding in conversational search systems' clarification questions, which can lead to irrelevant or unanswerable inquiries.

Method: The authors developed RAC by fine-tuning a large language model to generate grounded questions based on retrieved passages, aided by novel evaluation metrics and indexing strategies.

Result: RAC improves on traditional baselines in generating evidence-based clarification questions, as shown across four benchmarks with enhanced faithfulness using proposed metrics.

Conclusion: RAC ensures conversational systems produce reliable, corpus-faithful clarification questions, enhancing user interaction quality and information retrieval.

Abstract: Clarification questions help conversational search systems resolve ambiguous or underspecified user queries. While prior work has focused on fluency and alignment with user intent, especially through facet extraction, much less attention has been paid to grounding clarifications in the underlying corpus. Without such grounding, systems risk asking questions that cannot be answered from the available documents. We introduce RAC (Retrieval-Augmented Clarification), a framework for generating corpus-faithful clarification questions. After comparing several indexing strategies for retrieval, we fine-tune a large language model to make optimal use of research context and to encourage the generation of evidence-based question. We then apply contrastive preference optimization to favor questions supported by retrieved passages over ungrounded alternatives. Evaluated on four benchmarks, RAC demonstrate significant improvements over baselines. In addition to LLM-as-Judge assessments, we introduce novel metrics derived from NLI and data-to-text to assess how well questions are anchored in the context, and we demonstrate that our approach consistently enhances faithfulness.

</details>


### [125] [Bridging Human Interpretation and Machine Representation: A Landscape of Qualitative Data Analysis in the LLM Era](https://arxiv.org/abs/2601.11739)
*Xinyu Pi,Qisen Yang,Chuong Nguyen,Hua Shen*

Main category: cs.CL

TL;DR: The paper introduces a framework to conceptualize the varying outputs of LLMs in qualitative research and highlights gaps in interpretive and theoretical modeling.


<details>
  <summary>Details</summary>
Motivation: To address the varied outputs of LLMs in qualitative research and the lack of explicit focus on higher-level interpretive and theoretical modeling.

Method: The authors propose a 4x4 framework combining levels of meaning-making (descriptive to theoretical) with levels of modeling (static structures to feedback dynamics) and apply it to evaluate prior work.

Result: They observe a skew toward low-level outputs with minimal focus on interpretive/theoretical inference or dynamic modeling from LLMs in current research.

Conclusion: The authors propose a future agenda for LLM systems to explicitly define and govern their interpretive and modeling capabilities.

Abstract: LLMs are increasingly used to support qualitative research, yet existing systems produce outputs that vary widely--from trace-faithful summaries to theory-mediated explanations and system models. To make these differences explicit, we introduce a 4$\times$4 landscape crossing four levels of meaning-making (descriptive, categorical, interpretive, theoretical) with four levels of modeling (static structure, stages/timelines, causal pathways, feedback dynamics). Applying the landscape to prior LLM-based automation highlights a strong skew toward low-level meaning and low-commitment representations, with few reliable attempts at interpretive/theoretical inference or dynamical modeling. Based on the revealed gap, we outline an agenda for applying and building LLM-systems that make their interpretive and modeling commitments explicit, selectable, and governable.

</details>


### [126] [LIME-LLM: Probing Models with Fluent Counterfactuals, Not Broken Text](https://arxiv.org/abs/2601.11746)
*George Mihaila,Suleyman Olcay Polat,Poli Nemkova,Himanshu Sharma,Namratha V. Urs,Mark V. Albert*

Main category: cs.CL

TL;DR: This paper presents LIME-LLM, a framework that improves local explanation fidelity in NLP by introducing controlled perturbations and eliminating random token masking.


<details>
  <summary>Details</summary>
Motivation: There is a need for trustworthy AI in NLP, yet local explanation methods struggle due to reliance on random token masking, which produces semantically invalid inputs.

Method: LIME-LLM introduces a controlled perturbation framework using Single Mask-Single Sample protocol and specific infill strategies for constructing fluent, on-manifold neighborhoods.

Result: LIME-LLM significantly outperformed baselines in local explanation fidelity across benchmarks CoLA, SST-2, and HateXplain compared to traditional and generative alternatives.

Conclusion: The paper concludes that LIME-LLM sets a new standard for black-box NLP explainability by enhancing local explanation fidelity with hypothesis-driven perturbations.

Abstract: Local explanation methods such as LIME (Ribeiro et al., 2016) remain fundamental to trustworthy AI, yet their application to NLP is limited by a reliance on random token masking. These heuristic perturbations frequently generate semantically invalid, out-of-distribution inputs that weaken the fidelity of local surrogate models. While recent generative approaches such as LLiMe (Angiulli et al., 2025b) attempt to mitigate this by employing Large Language Models for neighborhood generation, they rely on unconstrained paraphrasing that introduces confounding variables, making it difficult to isolate specific feature contributions. We introduce LIME-LLM, a framework that replaces random noise with hypothesis-driven, controlled perturbations. By enforcing a strict "Single Mask-Single Sample" protocol and employing distinct neutral infill and boundary infill strategies, LIME-LLM constructs fluent, on-manifold neighborhoods that rigorously isolate feature effects. We evaluate our method against established baselines (LIME, SHAP, Integrated Gradients) and the generative LLiMe baseline across three diverse benchmarks: CoLA, SST-2, and HateXplain using human-annotated rationales as ground truth. Empirical results demonstrate that LIME-LLM establishes a new benchmark for black-box NLP explainability, achieving significant improvements in local explanation fidelity compared to both traditional perturbation-based methods and recent generative alternatives.

</details>


### [127] [Early Linguistic Pattern of Anxiety from Social Media Using Interpretable Linguistic Features: A Multi-Faceted Validation Study with Author-Disjoint Evaluation](https://arxiv.org/abs/2601.11758)
*Arnab Das Utsa*

Main category: cs.CL

TL;DR: This paper proposes a transparent, interpretable method for detecting anxiety using social media language.


<details>
  <summary>Details</summary>
Motivation: Current large-scale anxiety screening methods are limited, and social media data offers a scalable alternative for detection. Existing models face challenges in interpretability, keyword-bias validation, and data integrity.

Method: A logistic regression classifier was trained using Reddit data with curated subreddit splits. Evaluations included feature ablation, keyword masking, density analyses, and external validation with clinically diagnosed users.

Result: The model demonstrated strong performance, maintained high accuracy despite sentiment removal or keyword masking, and achieved effective early detection using minimal post histories. Results were consistent with clinical interview data.

Conclusion: Transparent linguistic features enable reliable, generalizable, and interpretable anxiety detection. The framework sets a reproducible baseline for mental health screening in various online settings.

Abstract: Anxiety affects hundreds of millions of individuals globally, yet large-scale screening remains limited. Social media language provides an opportunity for scalable detection, but current models often lack interpretability, keyword-robustness validation, and rigorous user-level data integrity. This work presents a transparent approach to social media-based anxiety detection through linguistically interpretable feature-grounded modeling and cross-domain validation. Using a substantial dataset of Reddit posts, we trained a logistic regression classifier on carefully curated subreddits for training, validation, and test splits. Comprehensive evaluation included feature ablation, keyword masking experiments, and varying-density difference analyses comparing anxious and control groups, along with external validation using clinically interviewed participants with diagnosed anxiety disorders. The model achieved strong performance while maintaining high accuracy even after sentiment removal or keyword masking. Early detection using minimal post history significantly outperformed random classification, and cross-domain analysis demonstrated strong consistency with clinical interview data. Results indicate that transparent linguistic features can support reliable, generalizable, and keyword-robust anxiety detection. The proposed framework provides a reproducible baseline for interpretable mental health screening across diverse online contexts.

</details>


### [128] [Industry-Aligned Granular Topic Modeling](https://arxiv.org/abs/2601.11762)
*Sae Young Moon,Myeongjun Erik Jang,Haoyan Luo,Chunyang Xiao,Antonios Georgiadis,Fran Silavong*

Main category: cs.CL

TL;DR: This paper introduces TIDE, a topic modeling framework leveraging large language models (LLMs) to deliver granular topic insights and industrially-relevant tools. It outperforms existing methods and provides functionalities such as topic parenting and document summarization.


<details>
  <summary>Details</summary>
Motivation: To explore and enhance the capability of topic modeling methods to produce granular and detailed insights for industrial and business applications.

Method: The paper presents the framework TIDE, which integrates large language models for granular topic modeling, and adds features like topic parenting, distillation, and long document summarization. The approach is tested on public and business datasets.

Result: The proposed framework outperforms state-of-the-art topic modeling methods and proves effective in providing business-oriented insights through additional functionalities.

Conclusion: TIDE demonstrates potential as a robust tool for granular topic modeling and industrial applications, with plans for open-source availability soon.

Abstract: Topic modeling has extensive applications in text mining and data analysis across various industrial sectors. Although the concept of granularity holds significant value for business applications by providing deeper insights, the capability of topic modeling methods to produce granular topics has not been thoroughly explored. In this context, this paper introduces a framework called TIDE, which primarily provides a novel granular topic modeling method based on large language models (LLMs) as a core feature, along with other useful functionalities for business applications, such as summarizing long documents, topic parenting, and distillation. Through extensive experiments on a variety of public and real-world business datasets, we demonstrate that TIDE's topic modeling approach outperforms modern topic modeling methods, and our auxiliary components provide valuable support for dealing with industrial business scenarios. The TIDE framework is currently undergoing the process of being open sourced.

</details>


### [129] [Cleansing the Artificial Mind: A Self-Reflective Detoxification Framework for Large Language Models](https://arxiv.org/abs/2601.11776)
*Kaituo Zhang,Zhimeng Jiang,Na Zou*

Main category: cs.CL

TL;DR: This paper introduces a self-reflective detoxification framework that enables Large Language Models (LLMs) to self-detect and correct toxic content without external modules or data annotation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current detoxification methods for LLMs rely on external modules, data annotation, or human intervention, which limit scalability and consistency. The authors aim to leverage LLMs’ innate self-corrective capabilities for autonomous detoxification.

Method: The proposed framework involves a Toxic Signal Detector as an internal mechanism for identifying toxic content and a systematic process to transform it into non-toxic text. This iterative approach generates a contrastive detoxification dataset to fine-tune the model, enhancing its detoxification ability.

Result: The experiments on benchmark datasets such as DetoxLLM and ParaDetox show that the proposed method outperforms state-of-the-art techniques in detoxification while maintaining semantic integrity.

Conclusion: This research highlights the potential of LLMs to self-regulate harmful content generation, fostering more ethical and reliable text generation systems without human or external module dependency.

Abstract: Recent breakthroughs in Large Language Models (LLMs) have revealed remarkable generative capabilities and emerging self-regulatory mechanisms, including self-correction and self-rewarding. However, current detoxification techniques rarely exploit these built-in abilities; instead, they rely on external modules, labor-intensive data annotation, or human intervention --factors that hinder scalability and consistency. In this paper, we introduce a fully self-reflective detoxification framework that harnesses the inherent capacities of LLMs to detect, correct toxic content, and refine LLMs without external modules and data annotation. Specifically, we propose a Toxic Signal Detector --an internal self-identification mechanism, coupled with a systematic intervention process to transform toxic text into its non-toxic counterpart. This iterative procedure yields a contrastive detoxification dataset used to fine-tune the model, enhancing its ability for safe and coherent text generation. Experiments on benchmark datasets such as DetoxLLM and ParaDetox show that our method achieves better detoxification performance than state-of-the-art methods while preserving semantic fidelity. By obviating the need for human intervention or external components, this paper reveals the intrinsic self-detoxification ability of LLMs, offering a consistent and effective approach for mitigating harmful content generation. Ultimately, our findings underscore the potential for truly self-regulated language models, paving the way for more responsible and ethically guided text generation systems.

</details>


### [130] [Translation as a Scalable Proxy for Multilingual Evaluation](https://arxiv.org/abs/2601.11778)
*Sheriff Issaka,Erick Rosas Gonzalez,Lieqi Liu,Evans Kofi Agyei,Lucas Bandarkar,Nanyun Peng,David Ifeoluwa Adelani,Francisco Guzmán,Saadia Gabriel*

Main category: cs.CL

TL;DR: The paper explores if translation quality can predict multilingual performance of LLMs as a cost-effective evaluation proxy.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of multilingual benchmarks for evaluating LLMs in a majority of the world's languages, as traditional benchmark creation is expensive and unscalable.

Method: The study evaluates 14 language models on 9 benchmarks and 7 translation metrics to test the relationship between translation quality and overall multilingual task performance.

Result: High correlations (e.g., Pearson r = 0.87 to 0.91) between translation metrics and downstream multilingual performance were observed, validating translation as an effective proxy.

Conclusion: Translation quality can act as a scalable, cost-effective proxy for evaluating multilingual capabilities in LLMs.

Abstract: The rapid proliferation of LLMs has created a critical evaluation paradox: while LLMs claim multilingual proficiency, comprehensive non-machine-translated benchmarks exist for fewer than 30 languages, leaving >98% of the world's 7,000 languages in an empirical void. Traditional benchmark construction faces scaling challenges such as cost, scarcity of domain experts, and data contamination. We evaluate the validity of a simpler alternative: can translation quality alone indicate a model's broader multilingual capabilities? Through systematic evaluation of 14 models (1B-72B parameters) across 9 diverse benchmarks and 7 translation metrics, we find that translation performance is a good indicator of downstream task success (e.g., Phi-4, median Pearson r: MetricX = 0.89, xCOMET = 0.91, SSA-COMET = 0.87). These results suggest that the representational abilities supporting faithful translation overlap with those required for multilingual understanding. Translation quality, thus emerges as a strong, inexpensive first-pass proxy of multilingual performance, enabling a translation-first screening with targeted follow-up for specific tasks.

</details>


### [131] [Beyond Tokens: Concept-Level Training Objectives for LLMs](https://arxiv.org/abs/2601.11791)
*Laya Iyer,Pranav Somani,Alice Guo,Dan Jurafsky,Chen Shani*

Main category: cs.CL

TL;DR: This paper proposes shifting from token-level next-token prediction (NTP) to concept-level prediction in large language model (LLM) training for better alignment with human semantic abstractions.


<details>
  <summary>Details</summary>
Motivation: The current token-level NTP penalizes alternative valid continuations and biases models toward surface forms rather than semantic meaning, motivating the need for higher-level representations.

Method: The authors introduce concept-level prediction, grouping multiple surface forms into shared conceptual representation. They propose methods for integrating conceptual supervision into LLM training.

Result: Concept-aware models outperform token-level NTP-based models with lower perplexity, robustness under domain shift, and better performance on NLP benchmarks.

Conclusion: Concept-level supervision serves as a superior training signal for LLMs, addressing semantic correctness and improving alignment with human reasoning.

Abstract: The next-token prediction (NTP) objective has been foundational in the development of modern large language models (LLMs), driving advances in fluency and generalization. However, NTP operates at the \textit{token} level, treating deviations from a single reference continuation as errors even when alternative continuations are equally plausible or semantically equivalent (e.g., ``mom'' vs. ``mother''). As a result, token-level loss can penalize valid abstractions, paraphrases, or conceptually correct reasoning paths, biasing models toward surface form rather than underlying meaning. This mismatch between the training signal and semantic correctness motivates learning objectives that operate over higher-level representations. We propose a shift from token-level to concept-level prediction, where concepts group multiple surface forms of the same idea (e.g., ``mom,'' ``mommy,'' ``mother'' $\rightarrow$ \textit{MOTHER}). We introduce various methods for integrating conceptual supervision into LLM training and show that concept-aware models achieve lower perplexity, improved robustness under domain shift, and stronger performance than NTP-based models on diverse NLP benchmarks. This suggests \textit{concept-level supervision} as an improved training signal that better aligns LLMs with human semantic abstractions.

</details>


### [132] [TWeddit : A Dataset of Triggering Stories Predominantly Shared by Women on Reddit](https://arxiv.org/abs/2601.11819)
*Shirlene Rose Bandela,Sanjeev Parthasarathy,Vaibhav Garg*

Main category: cs.CL

TL;DR: This paper introduces TWeddit, a curated dataset of triggering Reddit stories, focusing on issues often experienced by women, and highlights its potential for research.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of labeled datasets for triggering experiences on Reddit, particularly related to sensitive issues like miscarriage and sexual violence, and improve awareness around trigger warnings.

Method: They curated and annotated a dataset called TWeddit, analyzing its linguistic aspects to ensure it captures distinct topics and moral frameworks.

Result: The linguistic analyses of TWeddit indicate that the data captures diverse topics and moral underpinnings, making it a useful resource for future studies.

Conclusion: TWeddit fills a significant gap by providing a labeled dataset for emotionally triggering content, enabling better research and understanding of sensitive online narratives.

Abstract: Warning: This paper may contain examples and topics that may be disturbing to some readers, especially survivors of miscarriage and sexual violence. People affected by abortion, miscarriage, or sexual violence often share their experiences on social media to express emotions and seek support. On public platforms like Reddit, where users can post long, detailed narratives (up to 40,000 characters), readers may be exposed to distressing content. Although Reddit allows manual trigger warnings, many users omit them due to limited awareness or uncertainty about which categories apply. There is scarcity of datasets on Reddit stories labeled for triggering experiences. We propose a curated Reddit dataset, TWeddit, covering triggering experiences related to issues majorly faced by women. Our linguistic analyses show that annotated stories in TWeddit express distinct topics and moral foundations, making the dataset useful for a wide range of future research.

</details>


### [133] [The Third VoicePrivacy Challenge: Preserving Emotional Expressiveness and Linguistic Content in Voice Anonymization](https://arxiv.org/abs/2601.11846)
*Natalia Tomashenko,Xiaoxiao Miao,Pierre Champion,Sarina Meyer,Michele Panariello,Xin Wang,Nicholas Evans,Emmanuel Vincent,Junichi Yamagishi,Massimiliano Todisco*

Main category: cs.CL

TL;DR: The paper discusses the third VoicePrivacy Challenge in 2024, advancing voice anonymization systems to protect speaker identity while preserving linguistic and emotional content.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve voice anonymization technology to balance privacy (concealing speaker identity) and utility (preserving linguistic and emotional aspects).

Method: The framework included developing anonymization systems, using datasets for evaluation, applying attack models, and assessing privacy and utility metrics.

Result: Participants developed innovative approaches for voice anonymization; six baseline systems were described and tested.

Conclusion: Key findings provide insights for future challenges and directions in voice anonymization research to further advance the field.

Abstract: We present results and analyses from the third VoicePrivacy Challenge held in 2024, which focuses on advancing voice anonymization technologies. The task was to develop a voice anonymization system for speech data that conceals a speaker's voice identity while preserving linguistic content and emotional state. We provide a systematic overview of the challenge framework, including detailed descriptions of the anonymization task and datasets used for both system development and evaluation. We outline the attack model and objective evaluation metrics for assessing privacy protection (concealing speaker voice identity) and utility (content and emotional state preservation). We describe six baseline anonymization systems and summarize the innovative approaches developed by challenge participants. Finally, we provide key insights and observations to guide the design of future VoicePrivacy challenges and identify promising directions for voice anonymization research.

</details>


### [134] [ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System](https://arxiv.org/abs/2601.11854)
*Yifei Zhang,Hooshang Nayyeri,Rinat Khaziev,Emine Yilmaz,Gokhan Tur,Dilek Hakkani-Tür,Hari Thadakamalla*

Main category: cs.CL

TL;DR: This paper introduces ATOD, a new benchmark and evaluation framework for advanced task-oriented dialogue (TOD) systems, emphasizing capabilities such as multi-goal coordination and long-term reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing TOD benchmarks do not systematically assess advanced behaviors like interleaved goal coordination, memory, and proactivity enabled by large language models.

Method: The authors designed ATOD, a synthetic dialogue generation benchmark capturing advanced TOD features, and ATOD-Eval, an evaluation framework with fine-grained metrics and offline/online evaluative support.

Result: ATOD-Eval demonstrated comprehensive assessment capabilities related to task success, agentic capabilities, and response quality. A proposed memory-based evaluator outperformed current approaches in accuracy-efficiency tradeoffs.

Conclusion: ATOD and ATOD-Eval provide a robust method to evaluate advanced TOD systems, filling gaps in existing benchmarks and improving evaluation comprehensiveness and precision.

Abstract: Recent advances in task-oriented dialogue (TOD) systems, driven by large language models (LLMs) with extensive API and tool integration, have enabled conversational agents to coordinate interleaved goals, maintain long-horizon context, and act proactively through asynchronous execution. These capabilities extend beyond traditional TOD systems, yet existing benchmarks lack systematic support for evaluating such agentic behaviors. To address this gap, we introduce ATOD, a benchmark and synthetic dialogue generation pipeline that produces richly annotated conversations requiring long-term reasoning. ATOD captures key characteristics of advanced TOD, including multi-goal coordination, dependency management, memory, adaptability, and proactivity. Building on ATOD, we propose ATOD-Eval, a holistic evaluation framework that translates these dimensions into fine-grained metrics and supports reproducible offline and online evaluation. We further present a strong agentic memory-based evaluator for benchmarking on ATOD. Experiments show that ATOD-Eval enables comprehensive assessment across task completion, agentic capability, and response quality, and that the proposed evaluator offers a better accuracy-efficiency tradeoff compared to existing memory- and LLM-based approaches under this evaluation setting.

</details>


### [135] [CTPD: Cross Tokenizer Preference Distillation](https://arxiv.org/abs/2601.11865)
*Truong Nguyen,Phi Van Dat,Ngan Nguyen,Linh Ngo Van,Trung Le,Thanh Hong Nguyen*

Main category: cs.CL

TL;DR: The paper proposes Cross-Tokenizer Preference Distillation (CTPD), a novel approach to align language models with human preferences across different tokenization schemes, achieving significant performance improvement.


<details>
  <summary>Details</summary>
Motivation: Current methods lack effective solutions for aligning language models with human preferences across heterogeneous tokenizer schemes, which limits fine-grained knowledge transfer during preference distillation.

Method: The paper introduces CTPD with three core components: Aligned Span Projection for mapping tokens to shared spans, Token-level Importance Sampling (TIS-DPO) for better credit assignment, and Teacher-Anchored References for leveraging teacher model preferences.

Result: CTPD demonstrates significant performance increases across various benchmarks compared to existing methods, validating its effectiveness.

Conclusion: CTPD provides a unified, practical solution to transferring human-aligned behavior between models with different tokenization systems, enhancing language model alignment.

Abstract: While knowledge distillation has seen widespread use in pre-training and instruction tuning, its application to aligning language models with human preferences remains underexplored, particularly in the more realistic cross-tokenizer setting. The incompatibility of tokenization schemes between teacher and student models has largely prevented fine-grained, white-box distillation of preference information. To address this gap, we propose Cross-Tokenizer Preference Distillation (CTPD), the first unified framework for transferring human-aligned behavior between models with heterogeneous tokenizers. CTPD introduces three key innovations: (1) Aligned Span Projection, which maps teacher and student tokens to shared character-level spans for precise supervision transfer; (2) a cross-tokenizer adaptation of Token-level Importance Sampling (TIS-DPO) for improved credit assignment; and (3) a Teacher-Anchored Reference, allowing the student to directly leverage the teacher's preferences in a DPO-style objective. Our theoretical analysis grounds CTPD in importance sampling, and experiments across multiple benchmarks confirm its effectiveness, with significant performance gains over existing methods. These results establish CTPD as a practical and general solution for preference distillation across diverse tokenization schemes, opening the door to more accessible and efficient alignment of language models.

</details>


### [136] [Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving](https://arxiv.org/abs/2601.11866)
*Kie Shidara,Preethi Prem,Jonathan Kim,Anna Podlasek,Feng Liu,Ahmed Alaa,Danilo Bernardo*

Main category: cs.CL

TL;DR: The study examines if advances in Large Language Models (LLMs) improve their cognitive flexibility in clinical reasoning by using challenging medical benchmark tests. Strong reasoning models achieved human-level performance and avoided heuristic traps better than weaker models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate whether recent advancements in reasoning-based LLMs enhance their ability to exhibit flexible and accurate clinical reasoning, overcoming biases like the Einstellung effect.

Method: Different reasoning models, including OpenAI, Grok, Gemini, Claude, and DeepSeek, were tested on the medicine abstraction and reasoning corpus (mARC), which is designed to challenge models using adversarial medical QA tasks.

Result: Strong reasoning models performed at human-level accuracy, particularly avoiding heuristic traps induced by the Einstellung effect, and answered 55–70% of challenging questions typically missed by physicians.

Conclusion: Advanced reasoning models show greater flexibility and human-level competence in clinical reasoning tasks, suggesting they are less prone to cognitive bias and can complement human performance in medical contexts.

Abstract: Large Language Models (LLMs) have achieved high accuracy on medical question-answer (QA) benchmarks, yet their capacity for flexible clinical reasoning has been debated. Here, we asked whether advances in reasoning LLMs improve their cognitive flexibility in clinical reasoning. We assessed reasoning models from the OpenAI, Grok, Gemini, Claude, and DeepSeek families on the medicine abstraction and reasoning corpus (mARC), an adversarial medical QA benchmark which utilizes the Einstellung effect to induce inflexible overreliance on learned heuristic patterns in contexts where they become suboptimal. We found that strong reasoning models avoided Einstellung-based traps more often than weaker reasoning models, achieving human-level performance on mARC. On questions most commonly missed by physicians, the top 5 performing models answered 55% to 70% correctly with high confidence, indicating that these models may be less susceptible than humans to Einstellung effects. Our results indicate that strong reasoning models demonstrate improved flexibility in medical reasoning, achieving performance on par with humans on mARC.

</details>


### [137] [GloCTM: Cross-Lingual Topic Modeling via a Global Context Space](https://arxiv.org/abs/2601.11872)
*Nguyen Tien Phat,Ngo Vu Minh,Linh Van Ngo,Nguyen Thi Ngoc Diep,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: GloCTM is a novel cross-lingual topic modeling framework that achieves improved topic coherence and alignment by utilizing a unified semantic space and leveraging multilingual pretrained representations.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in cross-lingual topic modeling, particularly the limitations of existing approaches that fail to capture deep semantics and overlook multilingual pretrained representations.

Method: GloCTM constructs enriched input representations by expanding bag-of-words, aligns topic proportions using local and global encoders with regularization, synchronizes topic meanings through a combined vocabulary, and incorporates CKA loss for aligning latent topic space with contextual embeddings.

Result: Experiments show that GloCTM significantly improves topic coherence and cross-lingual alignment compared to strong baseline models.

Conclusion: The paper concludes that GloCTM effectively enhances cross-lingual topic modeling by fully exploiting semantics and achieving better alignment across languages.

Abstract: Cross-lingual topic modeling seeks to uncover coherent and semantically aligned topics across languages - a task central to multilingual understanding. Yet most existing models learn topics in disjoint, language-specific spaces and rely on alignment mechanisms (e.g., bilingual dictionaries) that often fail to capture deep cross-lingual semantics, resulting in loosely connected topic spaces. Moreover, these approaches often overlook the rich semantic signals embedded in multilingual pretrained representations, further limiting their ability to capture fine-grained alignment. We introduce GloCTM (Global Context Space for Cross-Lingual Topic Model), a novel framework that enforces cross-lingual topic alignment through a unified semantic space spanning the entire model pipeline. GloCTM constructs enriched input representations by expanding bag-of-words with cross-lingual lexical neighborhoods, and infers topic proportions using both local and global encoders, with their latent representations aligned through internal regularization. At the output level, the global topic-word distribution, defined over the combined vocabulary, structurally synchronizes topic meanings across languages. To further ground topics in deep semantic space, GloCTM incorporates a Centered Kernel Alignment (CKA) loss that aligns the latent topic space with multilingual contextual embeddings. Experiments across multiple benchmarks demonstrate that GloCTM significantly improves topic coherence and cross-lingual alignment, outperforming strong baselines.

</details>


### [138] [Faithfulness vs. Safety: Evaluating LLM Behavior Under Counterfactual Medical Evidence](https://arxiv.org/abs/2601.11886)
*Kaijie Mo,Siddhartha Venkatayogi,Chantal Shaib,Ramez Kouzy,Wei Xu,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

TL;DR: This paper analyzes how large language models (LLMs) respond to counterfactual or adversarial medical evidence using a new dataset called MedCounterFact.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability and safety of LLMs when faced with misleading or dangerous medical evidence in high-stakes scenarios.

Method: The authors created MedCounterFact, a medical QA dataset with counterfactual stimuli, and tested LLMs' responses to clinical questions based on randomized controlled trial contexts.

Result: LLMs tend to accept misleading or dangerous counterfactual evidence uncritically, often producing confident and unsafe answers.

Conclusion: Existing LLMs lack a boundary between faithfulness to context and safety, posing significant risks in high-stakes applications like medicine.

Abstract: In high-stakes domains like medicine, it may be generally desirable for models to faithfully adhere to the context provided. But what happens if the context does not align with model priors or safety protocols? In this paper, we investigate how LLMs behave and reason when presented with counterfactual or even adversarial medical evidence. We first construct MedCounterFact, a counterfactual medical QA dataset that requires the models to answer clinical comparison questions (i.e., judge the efficacy of certain treatments, with evidence consisting of randomized controlled trials provided as context). In MedCounterFact, real-world medical interventions within the questions and evidence are systematically replaced with four types of counterfactual stimuli, ranging from unknown words to toxic substances. Our evaluation across multiple frontier LLMs on MedCounterFact reveals that in the presence of counterfactual evidence, existing models overwhelmingly accept such "evidence" at face value even when it is dangerous or implausible, and provide confident and uncaveated answers. While it may be prudent to draw a boundary between faithfulness and safety, our findings reveal that there exists no such boundary yet.

</details>


### [139] [PPA-Plan: Proactive Pitfall Avoidance for Reliable Planning in Long-Context LLM Reasoning](https://arxiv.org/abs/2601.11908)
*Byeongjin Kim,Gyuwan Kim,Seo Yeon Park*

Main category: cs.CL

TL;DR: The paper proposes a proactive planning strategy, PPA-Plan, to improve reasoning over long contexts by addressing logical pitfalls before generating a plan.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with long-context reasoning due to sparse relevant information and faulty planning, which limits their effectiveness in tasks requiring accurate plan formulation and refinement.

Method: PPA-Plan identifies potential logical pitfalls and false assumptions, formulates them as negative constraints, and ensures plan generation avoids these constraints.

Result: Experiments demonstrate that PPA-Plan outperforms existing plan-and-execute methods and direct prompting in long-context QA benchmarks.

Conclusion: PPA-Plan successfully mitigates failures in reasoning over long contexts, offering a robust approach to proactive planning for LLMs.

Abstract: Large language models (LLMs) struggle with reasoning over long contexts where relevant information is sparsely distributed. Although plan-and-execute frameworks mitigate this by decomposing tasks into planning and execution, their effectiveness is often limited by unreliable plan generation due to dependence on surface-level cues. Consequently, plans may be based on incorrect assumptions, and once a plan is formed, identifying what went wrong and revising it reliably becomes difficult, limiting the effectiveness of reactive refinement. To address this limitation, we propose PPA-Plan, a proactive planning strategy for long-context reasoning that focuses on preventing such failures before plan generation. PPA-Plan identifies potential logical pitfalls and false assumptions, formulates them as negative constraints, and conditions plan generation on explicitly avoiding these constraints. Experiments on long-context QA benchmarks show that executing plans generated by PPA-Plan consistently outperforms existing plan-and-execute methods and direct prompting.

</details>


### [140] [LSTM-MAS: A Long Short-Term Memory Inspired Multi-Agent System for Long-Context Understanding](https://arxiv.org/abs/2601.11913)
*Yichen Jiang,Peng Ye,Jiakang Yuan,Chongjun Tu,Lei Bai,Tao Chen*

Main category: cs.CL

TL;DR: This paper introduces LSTM-MAS, a Multi-Agent System inspired by LSTM architecture to enhance long-context understanding in large language models, improving results significantly on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the challenge of processing long contexts in large language models (LLMs) by overcoming limitations of existing single-LLM methods and multi-agent frameworks that are prone to errors and hallucinations.

Method: The method involves designing a Multi-Agent System (LSTM-MAS) with hierarchical information flow and gated memory mechanisms, organized in a chained agent architecture that mimics LSTM principles.

Result: LSTM-MAS demonstrated significant improvement over a previous multi-agent approach (CoA) on benchmark datasets, with marked increases in performance across multiple tasks.

Conclusion: The paper concludes that the novel LSTM-MAS framework effectively addresses long-context comprehension challenges while reducing error accumulation and hallucination propagation.

Abstract: Effectively processing long contexts remains a fundamental yet unsolved challenge for large language models (LLMs). Existing single-LLM-based methods primarily reduce the context window or optimize the attention mechanism, but they often encounter additional computational costs or constrained expanded context length. While multi-agent-based frameworks can mitigate these limitations, they remain susceptible to the accumulation of errors and the propagation of hallucinations. In this work, we draw inspiration from the Long Short-Term Memory (LSTM) architecture to design a Multi-Agent System called LSTM-MAS, emulating LSTM's hierarchical information flow and gated memory mechanisms for long-context understanding. Specifically, LSTM-MAS organizes agents in a chained architecture, where each node comprises a worker agent for segment-level comprehension, a filter agent for redundancy reduction, a judge agent for continuous error detection, and a manager agent for globally regulates information propagation and retention, analogous to LSTM and its input gate, forget gate, constant error carousel unit, and output gate. These novel designs enable controlled information transfer and selective long-term dependency modeling across textual segments, which can effectively avoid error accumulation and hallucination propagation. We conducted an extensive evaluation of our method. Compared with the previous best multi-agent approach, CoA, our model achieves improvements of 40.93%, 43.70%,121.57% and 33.12%, on NarrativeQA, Qasper, HotpotQA, and MuSiQue, respectively.

</details>


### [141] [Enhancing LLM-Based Data Annotation with Error Decomposition](https://arxiv.org/abs/2601.11920)
*Zhen Xu,Vedant Khatri,Yijun Dai,Xiner Liu,Siyan Li,Xuanming Zhang,Renzhe Yu*

Main category: cs.CL

TL;DR: The paper introduces a diagnostic evaluation paradigm to improve the assessment of large language model (LLM) performance on subjective data annotation tasks, focusing on their effects on downstream analyses.


<details>
  <summary>Details</summary>
Motivation: LLMs have shown promise in scaling research for data-intensive domains. They perform well in objective annotation tasks but struggle with subjective ones, like psychological constructs, due to task ambiguity and model inaccuracies.

Method: The study proposes a refined diagnostic evaluation framework that categorizes annotation errors by source (model-driven vs. task-inherent) and type (boundary ambiguity vs. conceptual misidentification). It incorporates human annotation tests and computational decomposition techniques.

Result: The paradigm was validated on four educational annotation tasks, showing its effectiveness in identifying sources of LLM annotation errors and emphasizing the limitations of single alignment metrics.

Conclusion: The proposed paradigm offers a practical, low-cost tool to assess the feasibility of LLMs for subjective annotation tasks and provides insights for further LLM improvement.

Abstract: Large language models offer a scalable alternative to human coding for data annotation tasks, enabling the scale-up of research across data-intensive domains. While LLMs are already achieving near-human accuracy on objective annotation tasks, their performance on subjective annotation tasks, such as those involving psychological constructs, is less consistent and more prone to errors. Standard evaluation practices typically collapse all annotation errors into a single alignment metric, but this simplified approach may obscure different kinds of errors that affect final analytical conclusions in different ways. Here, we propose a diagnostic evaluation paradigm that incorporates a human-in-the-loop step to separate task-inherent ambiguity from model-driven inaccuracies and assess annotation quality in terms of their potential downstream impacts. We refine this paradigm on ordinal annotation tasks, which are common in subjective annotation. The refined paradigm includes: (1) a diagnostic taxonomy that categorizes LLM annotation errors along two dimensions: source (model-specific vs. task-inherent) and type (boundary ambiguity vs. conceptual misidentification); (2) a lightweight human annotation test to estimate task-inherent ambiguity from LLM annotations; and (3) a computational method to decompose observed LLM annotation errors following our taxonomy. We validate this paradigm on four educational annotation tasks, demonstrating both its conceptual validity and practical utility. Theoretically, our work provides empirical evidence for why excessively high alignment is unrealistic in specific annotation tasks and why single alignment metrics inadequately reflect the quality of LLM annotations. In practice, our paradigm can be a low-cost diagnostic tool that assesses the suitability of a given task for LLM annotation and provides actionable insights for further technical optimization.

</details>


### [142] [Mapping the maturation of TCM as an adjuvant to radiotherapy](https://arxiv.org/abs/2601.11923)
*P. Bilha Githinji,Aikaterini Melliou,Xi Yuan,Dayan Zhang,Lian Zhang,Zhenglin Chen,Jiansong Ji,Chengying Lv,Jinhao Xu,Peiwu Qin,Dongmei Yu*

Main category: cs.CL

TL;DR: This paper provides a large-scale analysis (69,745 publications) tracking the evolution of research on Traditional Chinese Medicine (TCM) as an adjuvant for radiotherapy from 2000 to 2025, identifying distinct thematic axes and patterns.


<details>
  <summary>Details</summary>
Motivation: To synthesize and understand the progression and thematic structure of research related to TCM as an adjuvant treatment in radiotherapy, after 25 years of integrated oncology.

Method: A large-scale thematic and temporal analysis of 69,745 publications (2000–2025) using a theme modeling workflow to define thematic and evolutionary patterns in the field.

Result: Five thematic axes identified: cancer types, supportive care, clinical endpoints, mechanisms, and methodology. The study highlights cycles of expansion and contraction in research and homogeneous positive reporting bias.

Conclusion: The field of TCM in radiotherapy has matured and may be nearing a transition point to novel research directions, though a systemic positive reporting bias is present.

Abstract: The integration of complementary medicine into oncology represents a paradigm shift that has seen to increasing adoption of Traditional Chinese Medicine (TCM) as an adjuvant to radiotherapy. About twenty-five years since the formal institutionalization of integrated oncology, it is opportune to synthesize the trajectory of evidence for TCM as an adjuvant to radiotherapy. Here we conduct a large-scale analysis of 69,745 publications (2000 - 2025), emerging a cyclical evolution defined by coordinated expansion and contraction in publication output, international collaboration, and funding commitments that mirrors a define-ideate-test pattern. Using a theme modeling workflow designed to determine a stable thematic structure of the field, we identify five dominant thematic axes - cancer types, supportive care, clinical endpoints, mechanisms, and methodology - that signal a focus on patient well-being, scientific rigor and mechanistic exploration. Cross-theme integration of TCM is patient-centered and systems-oriented. Together with the emergent cycles of evolution, the thematic structure demonstrates progressive specialization and potential defragmentation of the field or saturation of existing research agenda. The analysis points to a field that has matured its current research agenda and is likely at the cusp of something new. Additionally, the field exhibits positive reporting of findings that is homogeneous across publication types, thematic areas, and the cycles of evolution suggesting a system-wide positive reporting bias agnostic to structural drivers.

</details>


### [143] [Event Detection with a Context-Aware Encoder and LoRA for Improved Performance on Long-Tailed Classes](https://arxiv.org/abs/2601.11932)
*Abdullah Al Monsur,Nitesh Vamshi Bommisetty,Gene Louis Kim*

Main category: cs.CL

TL;DR: The paper identifies two limitations in event detection: unidirectional decoder-only LLMs and the overuse of Micro-F1 scores. It proposes using Macro-F1 scores and introduces Low-Rank Adaptation (LoRA) to improve performance on diverse event types.


<details>
  <summary>Details</summary>
Motivation: To address the architectural bottleneck of decoder-only language models in processing bidirectional context and to provide a more representative evaluation metric (Macro-F1 instead of Micro-F1) for long-tailed event detection tasks.

Method: Introduced sentence context to models and utilized Low-Rank Adaptation (LoRA) during finetuning to enhance performance, especially on tail-end event types.

Result: Models with sentence context outperformed decoder-only baselines, and integrating LoRA finetuning significantly improved Macro-F1 scores, especially for long-tailed event classes.

Conclusion: Using bidirectional context and alternative fine-tuning techniques like LoRA mitigates the limitations of traditional decoder-only LLMs and ensures better evaluation of models across diverse event types.

Abstract: The current state of event detection research has two notable re-occurring limitations that we investigate in this study. First, the unidirectional nature of decoder-only LLMs presents a fundamental architectural bottleneck for natural language understanding tasks that depend on rich, bidirectional context. Second, we confront the conventional reliance on Micro-F1 scores in event detection literature, which systematically inflates performance by favoring majority classes. Instead, we focus on Macro-F1 as a more representative measure of a model's ability across the long-tail of event types. Our experiments demonstrate that models enhanced with sentence context achieve superior performance over canonical decoder-only baselines. Using Low-Rank Adaptation (LoRA) during finetuning provides a substantial boost in Macro-F1 scores in particular, especially for the decoder-only models, showing that LoRA can be an effective tool to enhance LLMs' performance on long-tailed event classes.

</details>


### [144] [Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence](https://arxiv.org/abs/2601.11956)
*Yuyin Lu,Ziran Liang,Yanghui Rao,Wenqi Fan,Fu Lee Wang,Qing Li*

Main category: cs.CL

TL;DR: The paper introduces DoublyCal, a framework that uses double-calibration to mitigate hallucination in LLMs when reasoning with knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Current large language models struggle with hallucination in reasoning and lack epistemic uncertainty quantification in their outputs, even when augmented by knowledge graphs.

Method: The proposed DoublyCal framework incorporates a lightweight proxy model to generate KG evidence and assess its confidence. This evidence serves to guide black-box LLM predictions, ensuring better calibration of the final outputs.

Result: DoublyCal demonstrably improves accuracy and confidence calibration in black-box LLMs on knowledge-intensive benchmarks, with minimal token usage.

Conclusion: Integrating DoublyCal addresses reasoning inaccuracies and enhances traceability of confidence in LLM outputs by leveraging calibrated KG evidence effectively.

Abstract: Trustworthy reasoning in Large Language Models (LLMs) is challenged by their propensity for hallucination. While augmenting LLMs with Knowledge Graphs (KGs) improves factual accuracy, existing KG-augmented methods fail to quantify epistemic uncertainty in both the retrieved evidence and LLMs' reasoning. To bridge this gap, we introduce DoublyCal, a framework built on a novel double-calibration principle. DoublyCal employs a lightweight proxy model to first generate KG evidence alongside a calibrated evidence confidence. This calibrated supporting evidence then guides a black-box LLM, yielding final predictions that are not only more accurate but also well-calibrated, with confidence scores traceable to the uncertainty of the supporting evidence. Experiments on knowledge-intensive benchmarks show that DoublyCal significantly improves both the accuracy and confidence calibration of black-box LLMs with low token cost.

</details>


### [145] [PEARL: Self-Evolving Assistant for Time Management with Reinforcement Learning](https://arxiv.org/abs/2601.11957)
*Bingxuan Li,Jeonghwan Kim,Cheng Qian,Xiusi Chen,Eitan Anzenberg,Niran Kundapur,Heng Ji*

Main category: cs.CL

TL;DR: The paper introduces CalConflictBench for studying calendar conflict resolution using LLMs and proposes PEARL, a reinforcement learning framework, significantly reducing error rates compared to prior models.


<details>
  <summary>Details</summary>
Motivation: Managing calendar conflicts is time-consuming and scales poorly with human delegation. The challenge lies in automating this process to make trustworthy, efficient decisions based on user preferences.

Method: The method involves developing CalConflictBench, a benchmark that poses long-horizon calendar conflict resolution scenarios, and proposing PEARL, a reinforcement learning-based language agent with external memory and a refined reward system.

Result: PEARL demonstrates significant improvement, achieving a 0.76 error reduction rate and a 55% better error rate compared to previous baseline models on the CalConflictBench.

Conclusion: Current LLMs struggle with calendar conflict resolution; PEARL's advanced design shows promise, reducing errors and adapting effectively to user preferences.

Abstract: Overlapping calendar invitations force busy professionals to repeatedly decide which meetings to attend, reschedule, or decline. We refer to this preference-driven decision process as calendar conflict resolution. Automating such process is crucial yet challenging. Scheduling logistics drain hours, and human delegation often fails at scale, which motivate we to ask: Can we trust large language model (LLM) or language agent to manager time? To enable systematic study of this question, we introduce CalConflictBench, a benchmark for long-horizon calendar conflict resolution. Conflicts are presented sequentially and agents receive feedback after each round, requiring them to infer and adapt to user preferences progressively. Our experiments show that current LLM agents perform poorly with high error rates, e.g., Qwen-3-30B-Think has 35% average error rate. To address this gap, we propose PEARL, a reinforcement-learning framework that augments language agent with an external memory module and optimized round-wise reward design, enabling agent to progressively infer and adapt to user preferences on-the-fly. Experiments on CalConflictBench shows that PEARL achieves 0.76 error reduction rate, and 55% improvement in average error rate compared to the strongest baseline.

</details>


### [146] [$\texttt{MemoryRewardBench}$: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models](https://arxiv.org/abs/2601.11969)
*Zecheng Tang,Baibei Ji,Ruoxi Sun,Haitian Wang,WangJie You,Zhang Yijun,Wenpeng Zhu,Ji Qi,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: This paper introduces MemoryRewardBench, a benchmark for assessing reward models' (RMs) ability to evaluate memory management in large language models across long contexts.


<details>
  <summary>Details</summary>
Motivation: Reward models are crucial for evaluating long-term memory quality in large language models, enabling better propagation of information across extended sequences.

Method: MemoryRewardBench systematically tests reward models across long-context comprehension and long-form generation tasks, with diverse memory management patterns and context lengths ranging from 8K to 128K tokens.

Result: The study finds that newer-generation reward models consistently outperform predecessors, with open-source and proprietary models showing a reduced performance gap.

Conclusion: Current RMs have evolving capabilities but also fundamental limitations in evaluating memory management processes in long-context tasks, warranting further exploration.

Abstract: Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce $\texttt{MemoryRewardBench}$, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. $\texttt{MemoryRewardBench}$ covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.

</details>


### [147] [Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning](https://arxiv.org/abs/2601.12019)
*Chaowei Zhang,Xiansheng Luo,Zewei Zhang,Yi Zhu,Jipeng Qiang,Longwei Wang*

Main category: cs.CL

TL;DR: The paper addresses clickbait detection by leveraging sycophancy in LLMs to generate opposing reasoning perspectives and proposes a framework integrating contrastive reasoning for robust detection.


<details>
  <summary>Details</summary>
Motivation: The proliferation of clickbait headlines in online content creates the need for effective detection methods. However, sycophancy in LLMs hinders their efficiency, necessitating innovative approaches.

Method: The paper introduces a Self-renewal Opposing-stance Reasoning Generation (SORG) framework for generating reasoning pairs and utilizes an Opposing Reasoning-based Clickbait Detection (ORCD) model with BERT encoders and contrastive learning to improve detection.

Result: Experiments on benchmark datasets reveal the proposed approach surpasses LLM prompting, fine-tuned models, and other state-of-the-art clickbait detection methods.

Conclusion: This study offers a novel perspective on utilizing sycophancy in LLMs to enhance clickbait detection, achieving robust and superior performance while prompting new avenues for reasoning-based language modeling.

Abstract: The widespread proliferation of online content has intensified concerns about clickbait, deceptive or exaggerated headlines designed to attract attention. While Large Language Models (LLMs) offer a promising avenue for addressing this issue, their effectiveness is often hindered by Sycophancy, a tendency to produce reasoning that matches users' beliefs over truthful ones, which deviates from instruction-following principles. Rather than treating sycophancy as a flaw to be eliminated, this work proposes a novel approach that initially harnesses this behavior to generate contrastive reasoning from opposing perspectives. Specifically, we design a Self-renewal Opposing-stance Reasoning Generation (SORG) framework that prompts LLMs to produce high-quality agree and disagree reasoning pairs for a given news title without requiring ground-truth labels. To utilize the generated reasoning, we develop a local Opposing Reasoning-based Clickbait Detection (ORCD) model that integrates three BERT encoders to represent the title and its associated reasoning. The model leverages contrastive learning, guided by soft labels derived from LLM-generated credibility scores, to enhance detection robustness. Experimental evaluations on three benchmark datasets demonstrate that our method consistently outperforms LLM prompting, fine-tuned smaller language models, and state-of-the-art clickbait detection baselines.

</details>


### [148] [Preserving Fairness and Safety in Quantized LLMs Through Critical Weight Protection](https://arxiv.org/abs/2601.12033)
*Muhammad Alif Al Hakim,Alfan Farizki Wicaksono,Fajri Koto*

Main category: cs.CL

TL;DR: The paper studies the impact of quantization methods on fairness and safety in multilingual language models and proposes a solution to mitigate degradation risks.


<details>
  <summary>Details</summary>
Motivation: To explore the underexamined implications of quantization, specifically in dynamic and multilingual contexts, on fairness and safety of LLMs.

Method: A systematic analysis of static and dynamic quantization methods and the introduction of Critical Weight Protection to mitigate fairness and safety issues.

Result: Quantization degrades fairness and safety, especially in non-English settings, but Critical Weight Protection mitigates these risks without retraining.

Conclusion: Dynamic quantization is more stable than static methods, and the proposed Critical Weight Protection technique maintains trustworthiness while ensuring model efficiency.

Abstract: Quantization is widely adopted to reduce the computational cost of large language models (LLMs); however, its implications for fairness and safety, particularly in dynamic quantization and multilingual contexts, remain underexplored. In this work, we conduct a systematic study of how static and dynamic quantization methods impact fairness and safety across benchmarks measuring intrinsic and extrinsic bias and safety alignment. For fairness, we evaluate English, French, Dutch, Spanish, and Turkish; for safety, we focus on English, Korean, and Arabic. Our findings reveal that quantization consistently degrades fairness and safety, with dynamic methods demonstrating greater stability than static ones. Moreover, fairness degradation varies across languages, while safety deterioration is especially pronounced in non-English settings. To address these risks, we introduce Critical Weight Protection, a novel technique that identifies and preserves fairness- and safety-critical weights during quantization. This approach effectively mitigates bias and safety deterioration without costly retraining or alignment, maintaining trustworthiness while retaining efficiency.

</details>


### [149] [Don't Start Over: A Cost-Effective Framework for Migrating Personalized Prompts Between LLMs](https://arxiv.org/abs/2601.12034)
*Ziyi Zhao,Chongming Gao,Yang Zhang,Haoyan Liu,Weinan Gan,Huifeng Guo,Yong Liu,Fuli Feng*

Main category: cs.CL

TL;DR: The paper introduces PUMA, a framework for efficiently transferring personalized prompts across upgraded Large Language Models without full retraining.


<details>
  <summary>Details</summary>
Motivation: Upgrading foundation models often renders user-specific prompts obsolete, requiring costly retraining. This motivates the need for a method to transfer prompts efficiently between models.

Method: The proposed PUMA framework uses a parameter-efficient adapter to bridge semantic gaps and employs a group-based user selection strategy to reduce training costs.

Result: PUMA achieves comparable or superior performance to retraining, while cutting computational costs by up to 98%. It generalizes well across models and handles complex migration scenarios efficiently.

Conclusion: PUMA enables sustainable, efficient upgrades of personalized AI systems by decoupling user-specific assets from underlying model architectures.

Abstract: Personalization in Large Language Models (LLMs) often relies on user-specific soft prompts. However, these prompts become obsolete when the foundation model is upgraded, necessitating costly, full-scale retraining. To overcome this limitation, we propose the Prompt-level User Migration Adapter (PUMA), a lightweight framework to efficiently migrate personalized prompts across incompatible models. PUMA utilizes a parameter-efficient adapter to bridge the semantic gap, combined with a group-based user selection strategy to significantly reduce training costs. Experiments on three large-scale datasets show our method matches or even surpasses the performance of retraining from scratch, reducing computational cost by up to 98%. The framework demonstrates strong generalization across diverse model architectures and robustness in advanced scenarios like chained and aggregated migrations, offering a practical path for the sustainable evolution of personalized AI by decoupling user assets from the underlying models.

</details>


### [150] [Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs Annotation: LLM-Assisted and Gold-Label-Free Evaluation](https://arxiv.org/abs/2601.12061)
*Jinsook Lee,Kirk Vanacore,Zhuqian Zhou,Jeanine Grutter,Rene F. Kizilcec*

Main category: cs.CL

TL;DR: The paper explores Dialogue Act annotation issues due to segmentation disagreements, proposes a new segmentation approach using LLM-based segmenters, and introduces evaluation metrics. Results highlight trade-offs in segmentation methods and suggest optimizing for objectives.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issues in Dialogue Act (DA) annotation, particularly disagreements over segment boundaries, which affects reliability and downstream objectives.

Method: The authors propose using codebook-injected segmentation where segment boundary decisions are conditioned on annotation criteria. They evaluate various segmenters, including LLM-based and retrieval-augmented baselines, using newly introduced evaluation metrics.

Result: DA-aware segmentation shows better internal consistency compared to text-only baselines. LLMs excel in span coherence, but baselines are better at detecting dialogue flow shifts. No single method consistently outperforms across datasets, and trade-offs occur between segmentation metrics.

Conclusion: Segmentation design should be optimized for downstream objectives rather than focusing on a universal performance score, given the trade-offs in segmentation strategies.

Abstract: Dialogue Act (DA) annotation typically treats communicative or pedagogical intent as localized to individual utterances or turns. This leads annotators to agree on the underlying action while disagreeing on segment boundaries, reducing apparent reliability. We propose codebook-injected segmentation, which conditions boundary decisions on downstream annotation criteria, and evaluate LLM-based segmenters against standard and retrieval-augmented baselines. To assess these without gold labels, we introduce evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. We found DA-awareness produces segments that are internally more consistent than text-only baselines. While LLMs excel at creating construct-consistent spans, coherence-based baselines remain superior at detecting global shifts in dialogue flow. Across two datasets, no single segmenter dominates. Improvements in within-segment coherence frequently trade off against boundary distinctiveness and human-AI distributional agreement. These results highlight segmentation as a consequential design choice that should be optimized for downstream objectives rather than a single performance score.

</details>


### [151] [Bridging the Gap in Bangla Healthcare: Machine Learning Based Disease Prediction Using a Symptoms-Disease Dataset](https://arxiv.org/abs/2601.12068)
*Rowzatul Zannat,Abdullah Al Shafi,Abdul Muntakim*

Main category: cs.CL

TL;DR: This paper introduces a Bangla symptom-disease dataset for disease prediction, achieving 98% accuracy using advanced machine learning models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of healthcare informatics resources for Bangla-speaking populations and improve disease prediction capabilities.

Method: The authors developed a dataset of 758 unique symptom-disease relationships for 85 diseases, evaluated multiple machine learning models, and employed ensemble approaches for accuracy improvements.

Result: The ensemble methods combining top models achieved 98% accuracy in predicting diseases based on Bangla symptom inputs, showcasing high robustness.

Conclusion: This study provides a foundational dataset and methodology to improve equitable healthcare access for Bangla-speaking communities, fostering future advancements in health informatics.

Abstract: Increased access to reliable health information is essential for non-English-speaking populations, yet resources in Bangla for disease prediction remain limited. This study addresses this gap by developing a comprehensive Bangla symptoms-disease dataset containing 758 unique symptom-disease relationships spanning 85 diseases. To ensure transparency and reproducibility, we also make our dataset publicly available. The dataset enables the prediction of diseases based on Bangla symptom inputs, supporting healthcare accessibility for Bengali-speaking populations. Using this dataset, we evaluated multiple machine learning models to predict diseases based on symptoms provided in Bangla and analyzed their performance on our dataset. Both soft and hard voting ensemble approaches combining top-performing models achieved 98\% accuracy, demonstrating superior robustness and generalization. Our work establishes a foundational resource for disease prediction in Bangla, paving the way for future advancements in localized health informatics and diagnostic tools. This contribution aims to enhance equitable access to health information for Bangla-speaking communities, particularly for early disease detection and healthcare interventions.

</details>


### [152] [To Copy or Not to Copy: Copying Is Easier to Induce Than Recall](https://arxiv.org/abs/2601.12075)
*Mehrdad Farahani,Franziska Penzkofer,Richard Johansson*

Main category: cs.CL

TL;DR: The paper explores how language models decide between relying on parametric knowledge or context in retrieval-augmented tasks, using a mechanistic approach and interventions to modify model behavior.


<details>
  <summary>Details</summary>
Motivation: To understand and control how language models balance parametric recall versus contextual copying, which is critical in retrieval-augmented tasks for accuracy and effective information use.

Method: The authors designed a curated dataset to disentangle context effects and introduced an 'arbitration vector' derived from model activations. This vector is used as an intervention to steer behavior in either direction: suppressing context use (recall) or inducing copying.

Result: The study demonstrates consistent shifts in behavior across different model architectures and QA benchmarks while maintaining accuracy and fluency. Mechanistic analyses show that copying is easy to induce but restoring recall is more challenging.

Conclusion: The research highlights an asymmetry in how language models handle copying versus recall, with implications for understanding and improving retrieval-augmented systems.

Abstract: Language models used in retrieval-augmented settings must arbitrate between parametric knowledge stored in their weights and contextual information in the prompt. This work presents a mechanistic study of that choice by extracting an \emph{arbitration vector} from model activations on a curated dataset designed to disentangle (i) irrelevant contexts that elicit parametric recall and (ii) relevant but false contexts that elicit copying. The vector is computed as the residual-stream centroid difference between these regimes across 27 relations, and is injected as an additive intervention at selected layers and token spans to steer behavior in two directions: Copy$\rightarrow$Recall (suppressing context use) and Recall$\rightarrow$Copy (inducing the model to copy any token from the context). Experiments on two architectures (decoder-only and encoder/decoder) and two open-domain QA benchmarks show consistent behavior shifts under moderate scaling while monitoring accuracy and fluency. Mechanistic analyses of attention routing, MLP contributions, and layer-wise probability trajectories reveal an asymmetry: inducing copying is an easy ``reactivation'' process that can be triggered at different locations in the input, while restoring recall is a ``suppression'' process that is more fragile and strongly tied to object-token interventions.

</details>


### [153] [Optimizing User Profiles via Contextual Bandits for Retrieval-Augmented LLM Personalization](https://arxiv.org/abs/2601.12078)
*Linfeng Du,Ye Yuan,Zichen Zhao,Fuyuan Lyu,Emiliano Penaloza,Xiuying Chen,Zipeng Sun,Jikun Kang,Laurent Charlin,Xue Liu,Haolun Wu*

Main category: cs.CL

TL;DR: PURPLE is a framework that optimizes LLM personalization by considering complex inter-record dependencies and directly aligning retrieval with generation quality.


<details>
  <summary>Details</summary>
Motivation: Researchers aim to improve the personalization capability of large language models by addressing limitations in existing retrieval-based personalization methods that rely on semantic relevance alone.

Method: PURPLE employs a contextual bandit framework and Plackett-Luce ranking model to construct user profiles, optimizing retrieval using dense feedback related to generation quality.

Result: PURPLE demonstrated superior performance over heuristic and retrieval-augmented baselines across nine personalization tasks in terms of both effectiveness and efficiency.

Conclusion: By treating profile construction as a set-generation process, PURPLE provides a scalable, principled way to optimize LLM personalization, linking retrieval with enhanced generation quality.

Abstract: Large Language Models (LLMs) excel at general-purpose tasks, yet adapting their responses to individual users remains challenging. Retrieval augmentation provides a lightweight alternative to fine-tuning by conditioning LLMs on user history records, and existing approaches typically select these records based on semantic relevance. We argue that relevance serves as an unreliable proxy for utility: a record may be semantically similar to a query yet fail to improve generation quality or even degrade it due to redundancy or conflicting information. To bridge this gap, we propose PURPLE, a contextual bandit framework that oPtimizes UseR Profiles for Llm pErsonalization. In contrast to a greedy selection of the most relevant records, PURPLE treats profile construction as a set generation process and utilizes a Plackett-Luce ranking model to capture complex inter-record dependencies. By training with dense feedback provided by the likelihood of the reference response, our method aligns retrieval directly with generation quality. Extensive experiments on nine personalization tasks demonstrate that PURPLE consistently outperforms strong heuristic and retrieval-augmented baselines in both effectiveness and efficiency, establishing a principled and scalable solution for optimizing user profiles.

</details>


### [154] [Large language models struggle with ethnographic text annotation](https://arxiv.org/abs/2601.12099)
*Leonardo S. Goodall,Dor Shilton,Daniel A. Mullins,Harvey Whitehouse*

Main category: cs.CL

TL;DR: The study evaluated 7 large language models (LLMs) to annotate 121 ritual features in ethnographic texts, finding their performance inadequate for reliable automated annotation.


<details>
  <summary>Details</summary>
Motivation: Investigate whether state-of-the-art LLMs can assist in cross-cultural research by automating the annotation of structured data from ethnographic texts.

Method: Performance evaluation of 7 LLMs on 121 ritual features across 567 ethnographic text excerpts, comparing LLM results to human inter-coder reliability.

Result: LLMs showed limited accuracy, falling short of human performance, especially for tasks involving longer texts, ordinal distinctions, and ambiguous constructs.

Conclusion: LLMs are not yet a viable replacement for human expertise in ethnographic annotation, as their performance is insufficiently reliable.

Abstract: Large language models (LLMs) have shown promise for automated text annotation, raising hopes that they might accelerate cross-cultural research by extracting structured data from ethnographic texts. We evaluated 7 state-of-the-art LLMs on their ability to annotate 121 ritual features across 567 ethnographic excerpts. Performance was limited, falling well below levels required for reliable automated annotation. Longer texts, features requiring ordinal distinctions, and ambiguous constructs proved particularly difficult. Human inter-coder reliability set an approximate ceiling on LLM accuracy: features that human coders found difficult to agree upon were also difficult for LLMs. Yet even on features where humans reliably agreed, models fell short of human performance. Our findings suggest that LLMs cannot yet substitute for human expertise in ethnographic annotation.

</details>


### [155] [Powerful Training-Free Membership Inference Against Autoregressive Language Models](https://arxiv.org/abs/2601.12104)
*David Ilić,David Stanojević,Kostadin Cvejoski*

Main category: cs.CL

TL;DR: EZ-MIA is a new membership inference attack method to better detect sensitive information leaks in fine-tuned language models with significantly higher accuracy rates than prior methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the privacy risks posed by fine-tuned language models, which have the potential to memorize and expose sensitive training data. Current methods for auditing these risks, such as MIAs, have limited success, particularly under low false-positive conditions critical for real-world applications.

Method: The paper introduces EZ-MIA, which leverages the Error Zone (EZ) score. This measures directional probability imbalances at error positions, where the model predicts incorrectly but indicates memorization. The method relies on simple statistics without requiring extensive additional training or resources.

Result: EZ-MIA outperforms prior state-of-the-art approaches, achieving up to 3.8x and 8x higher detection rates at 1% and 0.1% false positive rates respectively, indicating stronger privacy risk detection capabilities. These results are consistent across different datasets and larger models.

Conclusion: EZ-MIA demonstrates that fine-tuned language models have greater privacy risks than previously recognized, highlighting the need for robust privacy auditing and more cautious deployment of such models.

Abstract: Fine-tuned language models pose significant privacy risks, as they may memorize and expose sensitive information from their training data. Membership inference attacks (MIAs) provide a principled framework for auditing these risks, yet existing methods achieve limited detection rates, particularly at the low false-positive thresholds required for practical privacy auditing. We present EZ-MIA, a membership inference attack that exploits a key observation: memorization manifests most strongly at error positions, specifically tokens where the model predicts incorrectly yet still shows elevated probability for training examples. We introduce the Error Zone (EZ) score, which measures the directional imbalance of probability shifts at error positions relative to a pretrained reference model. This principled statistic requires only two forward passes per query and no model training of any kind. On WikiText with GPT-2, EZ-MIA achieves 3.8x higher detection than the previous state-of-the-art under identical conditions (66.3% versus 17.5% true positive rate at 1% false positive rate), with near-perfect discrimination (AUC 0.98). At the stringent 0.1% FPR threshold critical for real-world auditing, we achieve 8x higher detection than prior work (14.0% versus 1.8%), requiring no reference model training. These gains extend to larger architectures: on AG News with Llama-2-7B, we achieve 3x higher detection (46.7% versus 15.8% TPR at 1% FPR). These results establish that privacy risks of fine-tuned language models are substantially greater than previously understood, with implications for both privacy auditing and deployment decisions. Code is available at https://github.com/JetBrains-Research/ez-mia.

</details>


### [156] [Bengali Text Classification: An Evaluation of Large Language Model Approaches](https://arxiv.org/abs/2601.12132)
*Md Mahmudul Hoque,Md Mehedi Hassain,Md Hojaifa Tanvir,Rahul Nandy*

Main category: cs.CL

TL;DR: This paper explores the use of large language models (LLMs) for Bengali text classification, achieving a top accuracy of 72% with Qwen 2.5, despite resource limitations for Bengali NLP.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in Bengali text classification caused by a lack of annotated datasets and pre-trained language models.

Method: The study evaluates three instruction-tuned LLMs (LLaMA 3.1 8B Instruct, LLaMA 3.2 3B Instruct, Qwen 2.5 7B Instruct) on a Kaggle dataset of Bengali newspaper articles under a common classification framework.

Result: The Qwen 2.5 model achieved the highest accuracy (72%), especially excelling in the 'Sports' category. LLaMA 3.1 and LLaMA 3.2 models achieved 53% and 56% accuracy, respectively.

Conclusion: LLMs can be effective for Bengali text classification, even with scarce resources. Future work aims to improve performance by examining additional models, addressing class imbalance, and refining fine-tuning.

Abstract: Bengali text classification is a Significant task in natural language processing (NLP), where text is categorized into predefined labels. Unlike English, Bengali faces challenges due to the lack of extensive annotated datasets and pre-trained language models. This study explores the effectiveness of large language models (LLMs) in classifying Bengali newspaper articles. The dataset used, obtained from Kaggle, consists of articles from Prothom Alo, a major Bangladeshi newspaper. Three instruction-tuned LLMs LLaMA 3.1 8B Instruct, LLaMA 3.2 3B Instruct, and Qwen 2.5 7B Instruct were evaluated for this task under the same classification framework. Among the evaluated models, Qwen 2.5 achieved the highest classification accuracy of 72%, showing particular strength in the "Sports" category. In comparison, LLaMA 3.1 and LLaMA 3.2 attained accuracies of 53% and 56%, respectively. The findings highlight the effectiveness of LLMs in Bengali text classification, despite the scarcity of resources for Bengali NLP. Future research will focus on exploring additional models, addressing class imbalance issues, and refining fine-tuning approaches to improve classification performance.

</details>


### [157] [Analyzing Cancer Patients' Experiences with Embedding-based Topic Modeling and LLMs](https://arxiv.org/abs/2601.12154)
*Teodor-Călin Ionescu,Lifeng Han,Jan Heijdra Suasnabar,Anne Stiggelbout,Suzan Verberne*

Main category: cs.CL

TL;DR: The paper explores neural topic modeling and large language models (LLMs) such as GPT4 to extract themes from cancer patients' interviews, aiming to enhance patient-centered healthcare.


<details>
  <summary>Details</summary>
Motivation: To better understand and incorporate patient perspectives into healthcare practices by analyzing storytelling data from cancer patients.

Method: The study evaluates BERTopic and Top2Vec using transcribed interviews for summarization, keyword extraction, and clustering; it uses GPT4 for topic labeling. BERTopic was chosen for extended experimentation using clinically oriented embeddings, particularly BioClinicalBERT.

Result: BERTopic with BioClinicalBERT embeddings produced consistent, interpretable results. Key global topics identified were "Coordination and Communication in Cancer Care Management" and "Patient Decision-Making in Cancer Treatment Journey."

Conclusion: Neural topic modeling, specifically BERTopic with domain-specific embeddings, shows promise in aiding clinicians by summarizing patient interviews effectively, potentially improving healthcare workflows and patient engagement.

Abstract: This study investigates the use of neural topic modeling and LLMs to uncover meaningful themes from patient storytelling data, to offer insights that could contribute to more patient-oriented healthcare practices. We analyze a collection of transcribed interviews with cancer patients (132,722 words in 13 interviews). We first evaluate BERTopic and Top2Vec for individual interview summarization by using similar preprocessing, chunking, and clustering configurations to ensure a fair comparison on Keyword Extraction. LLMs (GPT4) are then used for the next step topic labeling. Their outputs for a single interview (I0) are rated through a small-scale human evaluation, focusing on {coherence}, {clarity}, and {relevance}. Based on the preliminary results and evaluation, BERTopic shows stronger performance and is selected for further experimentation using three {clinically oriented embedding} models. We then analyzed the full interview collection with the best model setting. Results show that domain-specific embeddings improved topic \textit{precision} and \textit{interpretability}, with BioClinicalBERT producing the most consistent results across transcripts. The global analysis of the full dataset of 13 interviews, using the BioClinicalBERT embedding model, reveals the most dominant topics throughout all 13 interviews, namely ``Coordination and Communication in Cancer Care Management" and ``Patient Decision-Making in Cancer Treatment Journey''. Although the interviews are machine translations from Dutch to English, and clinical professionals are not involved in this evaluation, the findings suggest that neural topic modeling, particularly BERTopic, can help provide useful feedback to clinicians from patient interviews. This pipeline could support more efficient document navigation and strengthen the role of patients' voices in healthcare workflows.

</details>


### [158] [Tolerance Principle and Small Language Model Learning](https://arxiv.org/abs/2601.12179)
*Adam E. Friedman,Stevan Harnad,Rushen Shi*

Main category: cs.CL

TL;DR: The study investigates whether transformer-based models such as BabyBERTa align with the Tolerance Principle in learning grammar rules, finding that BabyBERTa deviates from human-like grammar learning.


<details>
  <summary>Details</summary>
Motivation: To explore whether transformer-based language models, akin to BabyBERTa, can emulate the human-like learning capabilities defined by the Tolerance Principle for grammar rules.

Method: The researchers trained BabyBERTa on artificial grammars using varied training datasets, altering size, sentence types, and proportions of rule-following versus exceptions.

Result: BabyBERTa failed to exhibit human-like learning and generalization dynamics under the framework of the Tolerance Principle.

Conclusion: Transformer-based models like BabyBERTa do not align with human infants' grammar-learning patterns as predicted by the Tolerance Principle.

Abstract: Modern language models like GPT-3, BERT, and LLaMA require massive training data, yet with sufficient training they reliably learn to distinguish grammatical from ungrammatical sentences. Children aged as young as 14 months already have the capacity to learn abstract grammar rules from very few exemplars, even in the presence of non-rule-following exceptions. Yang's (2016) Tolerance Principle defines a precise threshold for how many exceptions a rule can tolerate and still be learnable. The present study explored the minimal amount and quality of training data necessary for rules to be generalized by a transformer-based language model to test the predictions of the Tolerance Principle. We trained BabyBERTa (Huebner et al. 2021), a transformer model optimized for small datasets, on artificial grammars. The training sets varied in size, number of unique sentence types, and proportion of rule-following versus exception exemplars. We found that, unlike human infants, BabyBERTa's learning dynamics do not align with the Tolerance Principle.

</details>


### [159] [CTC-DID: CTC-Based Arabic dialect identification for streaming applications](https://arxiv.org/abs/2601.12199)
*Muhammad Umar Farooq,Oscar Saz*

Main category: cs.CL

TL;DR: This paper introduces a novel dialect identification method using a CTC framework and evaluates it on spoken Arabic dialect, proving its effectiveness in performance and adaptability for real-time use.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of effective dialect identification in speech, particularly with limited data and in settings requiring adaptability to real-time and short utterances.

Method: The method involves framing dialect identification as a limited-vocabulary ASR problem using a CTC loss function and incorporating dialect tags as sequences. Two estimation techniques are used for training: a Language-Agnostic Heuristic (LAH) and a pre-trained ASR model.

Result: The proposed model outperformed fine-tuned Whisper and ECAPA-TDNN models in the Arabic Dialect Identification task and succeeded in zero-shot evaluation on the Casablanca dataset. It showed better results for shorter utterances and was adaptable for streaming applications.

Conclusion: The CTC-DID approach provides a robust and adaptable solution for dialect identification, especially in low-resource and real-time scenarios, outperforming existing models.

Abstract: This paper proposes a Dialect Identification (DID) approach inspired by the Connectionist Temporal Classification (CTC) loss function as used in Automatic Speech Recognition (ASR). CTC-DID frames the dialect identification task as a limited-vocabulary ASR system, where dialect tags are treated as a sequence of labels for a given utterance. For training, the repetition of dialect tags in transcriptions is estimated either using a proposed Language-Agnostic Heuristic (LAH) approach or a pre-trained ASR model. The method is evaluated on the low-resource Arabic Dialect Identification (ADI) task, with experimental results demonstrating that an SSL-based CTC-DID model, trained on a limited dataset, outperforms both fine-tuned Whisper and ECAPA-TDNN models. Notably, CTC-DID also surpasses these models in zero-shot evaluation on the Casablanca dataset. The proposed approach is found to be more robust to shorter utterances and is shown to be easily adaptable for streaming, real-time applications, with minimal performance degradation.

</details>


### [160] [CoReflect: Conversational Evaluation via Co-Evolutionary Simulation and Reflective Rubric Refinement](https://arxiv.org/abs/2601.12208)
*Yunzhe Li,Richie Yueqi Feng,Tianxin Wei,Chin-Chia Hsu*

Main category: cs.CL

TL;DR: CoReflect proposes a method for evaluating conversational systems by combining simulations and iterative rubric refinement to better capture dialogue models' emergent behaviors.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for conversational systems rely on rigid rubrics and fixed contexts, making it difficult to address diverse and emergent dialogue behaviors.

Method: CoReflect unifies dialogue simulation and evaluation through adaptive processes, involving a conversation planner to guide dialogues and a reflective analyzer to iteratively refine evaluation methods.

Result: The co-evolutionary approach dynamically improves test case complexity and evaluation rubric precision, reducing the need for manual intervention and providing scalable evaluation.

Conclusion: CoReflect offers self-refining and adaptive evaluation protocols that evolve alongside dialogue models' capabilities, addressing limitations in conventional evaluation techniques.

Abstract: Evaluating conversational systems in multi-turn settings remains a fundamental challenge. Conventional pipelines typically rely on manually defined rubrics and fixed conversational context$-$a static approach that limits coverage and fails to capture the diverse, emergent behaviors of dialogue models. To address this, we introduce CoReflect (Conversational Evaluation via Co-Evolutionary Simulation and Reflective Rubric Refinement), which unifies dialogue simulation and evaluation into an adaptive, iterative process. CoReflect employs a conversation planner that generates structured templates to guide a user simulator through diverse, goal-directed dialogues. Subsequently, a reflective analyzer processes these dialogues to identify systematic behavioral patterns and automatically refine the evaluation rubrics. Crucially, the insights from the conversation analysis are fed back into the planner to update conversation templates for subsequent iterations. This co-evolution loop ensures that the complexity of test cases and the diagnostic precision of rubrics improve in tandem. By minimizing human intervention, CoReflect provides a scalable and self-refining methodology that allows evaluation protocols to adapt alongside the rapidly advancing capabilities of dialogue models.

</details>


### [161] [Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion Language Models](https://arxiv.org/abs/2601.12247)
*Miao Li,Hanyang Jiang,Sikai Chen,Hengyu Fu,Yuhang Cai,Baihe Huang,Tinghan Ye,Xuanzhou Chen,Pascal Van Hentenryck*

Main category: cs.CL

TL;DR: This paper presents the Plan-Verify-Fill (PVF) paradigm to enhance Diffusion Language Models (DLMs) in text generation, reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing Diffusion Language Model (DLM) decoding strategies underutilize global bidirectional context, limiting efficiency and performance in text generation.

Method: The proposed Plan-Verify-Fill (PVF) paradigm constructs a hierarchical skeleton, prioritizes high-leverage semantic anchors, and uses verification protocols to optimize stopping points.

Result: PVF achieves up to 65% reduction in Number of Function Evaluations (NFE) compared to confidence-based parallel decoding, maintaining accuracy across benchmark datasets.

Conclusion: PVF enables efficient and accurate decoding in DLMs, demonstrating the potential of training-free, context-sensitive paradigms for text generation.

Abstract: Diffusion Language Models (DLMs) present a promising non-sequential paradigm for text generation, distinct from standard autoregressive (AR) approaches. However, current decoding strategies often adopt a reactive stance, underutilizing the global bidirectional context to dictate global trajectories. To address this, we propose Plan-Verify-Fill (PVF), a training-free paradigm that grounds planning via quantitative validation. PVF actively constructs a hierarchical skeleton by prioritizing high-leverage semantic anchors and employs a verification protocol to operationalize pragmatic structural stopping where further deliberation yields diminishing returns. Extensive evaluations on LLaDA-8B-Instruct and Dream-7B-Instruct demonstrate that PVF reduces the Number of Function Evaluations (NFE) by up to 65% compared to confidence-based parallel decoding across benchmark datasets, unlocking superior efficiency without compromising accuracy.

</details>


### [162] [Multimodal Generative Engine Optimization: Rank Manipulation for Vision-Language Model Rankers](https://arxiv.org/abs/2601.12263)
*Yixuan Du,Chenxiao Yu,Haoyan Xu,Ziyi Wang,Yue Zhao,Xiyang Hu*

Main category: cs.CL

TL;DR: The paper uncovers multimodal ranking vulnerabilities in Vision-Language Models (VLMs) and introduces an efficient adversarial framework, Multimodal Generative Engine Optimization (MGEO), that exploits cross-modal synergy to manipulate rankings.


<details>
  <summary>Details</summary>
Motivation: Investigate the robustness of Vision-Language Models (VLMs) against adversarial manipulation, particularly in competitive ranking scenarios in product search.

Method: Developed a novel adversarial framework, MGEO, which employs alternating gradient-based optimization to jointly optimize imperceptible image perturbations and fluent textual suffixes to manipulate multimodal product rankings.

Result: MGEO demonstrates enhanced effectiveness over text-only and image-only attacks, achieving significant manipulation in search rankings through multimodal synergy without being detected by conventional filters.

Conclusion: Multimodal coordination in VLMs, typically perceived as an advantage, can be exploited as a vulnerability, requiring improved safeguarding to ensure ranking integrity.

Abstract: Vision-Language Models (VLMs) are rapidly replacing unimodal encoders in modern retrieval and recommendation systems. While their capabilities are well-documented, their robustness against adversarial manipulation in competitive ranking scenarios remains largely unexplored. In this paper, we uncover a critical vulnerability in VLM-based product search: multimodal ranking attacks. We present Multimodal Generative Engine Optimization (MGEO), a novel adversarial framework that enables a malicious actor to unfairly promote a target product by jointly optimizing imperceptible image perturbations and fluent textual suffixes. Unlike existing attacks that treat modalities in isolation, MGEO employs an alternating gradient-based optimization strategy to exploit the deep cross-modal coupling within the VLM. Extensive experiments on real-world datasets using state-of-the-art models demonstrate that our coordinated attack significantly outperforms text-only and image-only baselines. These findings reveal that multimodal synergy, typically a strength of VLMs, can be weaponized to compromise the integrity of search rankings without triggering conventional content filters.

</details>


### [163] [Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models](https://arxiv.org/abs/2601.12269)
*Xucong Hu,Jian-Qiao Zhu*

Main category: cs.CL

TL;DR: This paper proposes a method to recover Theory of Mind (ToM) capabilities in autoregressive language models using enhanced sampling techniques, without requiring retraining.


<details>
  <summary>Details</summary>
Motivation: ToM tasks require reasoning about latent mental states, which autoregressive language models are thought to struggle with due to their focus on local coherence instead of global coherence.

Method: Utilizing Markov chain Monte Carlo (MCMC) sampling with sequence-level probability distributions and incorporating annealing to enhance ToM performance.

Result: The sampling-based approach significantly improves ToM capabilities in language models without post-training adjustments.

Conclusion: Latent capabilities such as ToM can be effectively extracted using sampling optimization, challenging the need for additional weight updates or verifications.

Abstract: Autoregressive language models are next-token predictors and have been criticized for only optimizing surface plausibility (i.e., local coherence) rather than maintaining correct latent-state representations (i.e., global coherence). Because Theory of Mind (ToM) tasks crucially depend on reasoning about latent mental states of oneself and others, such models are therefore often thought to fail at ToM. While post-training methods can improve ToM performance, we show that strong ToM capability can be recovered directly from the base model without any additional weight updates or verifications. Our approach builds on recent power-sampling methods (Karan & Du, 2025) that use Markov chain Monte Carlo (MCMC) to sample from sharpened sequence-level (rather than token-level) probability distributions of autoregressive language models. We further find that incorporating annealing, where the tempered distribution is gradually shifted from high to low temperature, substantially improves ToM performance over fixed-temperature power sampling. Together, these results suggest that sampling-based optimization provides a powerful way to extract latent capabilities from language models without retraining.

</details>


### [164] [Conversational Context Classification: A Representation Engineering Approach](https://arxiv.org/abs/2601.12286)
*Jonathan Pan*

Main category: cs.CL

TL;DR: This paper investigates the use of Representation Engineering (RepE) and One-Class Support Vector Machine (OCSVM) to detect context shifts and anomalies in Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: LLMs often generate out-of-context responses, making it crucial to develop safeguards for detecting shifts in conversation norms like topic changes or inaccuracies.

Method: The authors used RepE and OCSVM to train and establish a robust detection boundary within the latent spaces of LLMs, focusing on identifying context-relevant hidden state subspaces.

Result: The researchers evaluated their approach using Llama and Qwen models, successfully identifying subspaces associated with specific contexts and achieving promising detection results.

Conclusion: This study advances the understanding of LLM internal mechanisms and improves anomaly detection in contextual semantics, aiding in better interpretation and safe use of LLMs.

Abstract: The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use of Representation Engineering (RepE) and One-Class Support Vector Machine (OCSVM) to identify subspaces within the internal states of LLMs that represent a specific context. By training OCSVM on in-context examples, we establish a robust boundary within the LLM's hidden state latent space. We evaluate out study with two open source LLMs - Llama and Qwen models in specific contextual domain. Our approach entailed identifying the optimal layers within the LLM's internal state subspaces that strongly associates with the context of interest. Our evaluation results showed promising results in identifying the subspace for a specific context. Aside from being useful in detecting in or out of context conversation threads, this research work contributes to the study of better interpreting LLMs.

</details>


### [165] [Can Deep Research Agents Find and Organize? Evaluating the Synthesis Gap with Expert Taxonomies](https://arxiv.org/abs/2601.12369)
*Ming Zhang,Jiabao Zhuang,Wenqing Jing,Ziyu Kong,Jingyi Deng,Yujiong Shen,Kexin Tan,Yuhang Zhao,Ning Luo,Renzhe Zheng,Jiahui Lin,Mingqi Wu,Long Ma,Yi Zou,Shihan Dou,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: The paper presents TaxoBench, a benchmark to evaluate Deep Research Agents on their ability to retrieve relevant papers and organize them into coherent structures for survey generation, demonstrating current agents fall short of human-level performance.


<details>
  <summary>Details</summary>
Motivation: To address the gap in assessing Deep Research Agents' abilities to create expert-like surveys by focusing on retrieval of essential papers and coherent organization, which is not covered by existing benchmarks.

Method: The authors introduced TaxoBench, a diagnostic benchmark derived from 72 high-impact computer science surveys. It provides expert-authored taxonomy trees with categorized citations as ground truth and supports two evaluation modes: Deep Research and Bottom-Up.

Result: The evaluation of 7 Deep Research agents and 12 frontier LLMs revealed poor performance. The best agent recalled only 20.9% of expert-selected papers, and the top model achieved only 0.31 ARI in organization even with perfect inputs.

Conclusion: TaxoBench highlights significant gaps in current Deep Research Agents' ability to generate expert-quality surveys, underlining the need for substantial advancements in retrieval and organizational capabilities.

Abstract: Deep Research Agents are increasingly used for automated survey generation. However, whether they can write surveys like human experts remains unclear. Existing benchmarks focus on fluency or citation accuracy, but none evaluates the core capabilities: retrieving essential papers and organizing them into coherent knowledge structures. We introduce TaxoBench, a diagnostic benchmark derived from 72 highly-cited computer science surveys. We manually extract expert-authored taxonomy trees containing 3,815 precisely categorized citations as ground truth. Our benchmark supports two evaluation modes: Deep Research mode tests end-to-end retrieval and organization given only a topic, while Bottom-Up mode isolates structuring capability by providing the exact papers human experts used. We evaluate 7 leading Deep Research agents and 12 frontier LLMs. Results reveal a dual bottleneck: the best agent recalls only 20.9% of expert-selected papers, and even with perfect input, the best model achieves only 0.31 ARI in organization. Current deep research agents remain far from expert-level survey writing. Our benchmark is publicly available at https://github.com/KongLongGeFDU/TaxoBench.

</details>


### [166] [A Scalable Entity-Based Framework for Auditing Bias in LLMs](https://arxiv.org/abs/2601.12374)
*Akram Elbouanani,Aboubacar Tuo,Adrian Popescu*

Main category: cs.CL

TL;DR: The study presents a scalable framework using named entities for bias evaluation in LLMs, identifying systematic biases through the largest audit of its kind.


<details>
  <summary>Details</summary>
Motivation: Current bias evaluations for LLMs have limitations, trading ecological validity for statistical control, and the study seeks a more robust and scalable method.

Method: The authors use named entities as probes and synthetic data to conduct a large-scale bias analysis comprising 1.9 billion data points across diverse parameters.

Result: Systematic biases were found, including political and geographical preferences, model scale amplifying bias, and Western-aligned tendencies persisting despite instruction tuning and non-English prompts.

Conclusion: Rigorous bias auditing of LLMs is vital before their use in sensitive applications, as existing models demonstrate significant structural disparities.

Abstract: Existing approaches to bias evaluation in large language models (LLMs) trade ecological validity for statistical control, relying on artificial prompts that poorly reflect real-world use, or on naturalistic tasks that lack scale and rigor. We introduce a scalable bias-auditing framework using named entities as probes to measure structural disparities in model behavior. We show that synthetic data reliably reproduces bias patterns observed in natural text, enabling large-scale analysis. Using this approach, we conduct the largest bias audit to date, comprising 1.9 billion data points across multiple entity types, tasks, languages, models, and prompting strategies. Our results reveal systematic biases: models penalize right-wing politicians, favor left-wing politicians, prefer Western and wealthy nations over the Global South, favor Western companies, and penalize firms in the defense and pharmaceutical sectors. While instruction tuning reduces bias, increasing model scale amplifies it, and prompting in Chinese or Russian does not attenuate Western-aligned preferences. These results indicate that LLMs should undergo rigorous auditing before deployment in high-stakes applications.

</details>


### [167] [LR-DWM: Efficient Watermarking for Diffusion Language Models](https://arxiv.org/abs/2601.12376)
*Ofek Raban,Ethan Fetaya,Gal Chechik*

Main category: cs.CL

TL;DR: This paper introduces a new watermarking method, Left-Right Diffusion Watermarking (LR-DWM), tailored for Diffusion Language Models (DLMs), offering efficient and reliable watermarking with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods are largely developed for autoregressive language models and are inefficient or require significant changes for Diffusion Language Models, a non-sequential text generation mechanism.

Method: The researchers propose LR-DWM, a watermarking scheme that utilizes left and right neighboring tokens during generation. It is designed to minimize computational and memory overhead while maintaining watermark reliability.

Result: LR-DWM effectively watermarks DLMs with high statistical detectability under standard evaluations, without compromising efficiency or performance.

Conclusion: LR-DWM proves to be an efficient watermarking method for DLMs, enabling reliable detection with negligible overhead, addressing weaknesses in previous approaches.

Abstract: Watermarking (WM) is a critical mechanism for detecting and attributing AI-generated content. Current WM methods for Large Language Models (LLMs) are predominantly tailored for autoregressive (AR) models: They rely on tokens being generated sequentially, and embed stable signals within the generated sequence based on the previously sampled text. Diffusion Language Models (DLMs) generate text via non-sequential iterative denoising, which requires significant modification to use WM methods designed for AR models. Recent work proposed to watermark DLMs by inverting the process when needed, but suffers significant computational or memory overhead. We introduce Left-Right Diffusion Watermarking (LR-DWM), a scheme that biases the generated token based on both left and right neighbors, when they are available. LR-DWM incurs minimal runtime and memory overhead, remaining close to the non-watermarked baseline DLM while enabling reliable statistical detection under standard evaluation settings. Our results demonstrate that DLMs can be watermarked efficiently, achieving high detectability with negligible computational and memory overhead.

</details>


### [168] [NADIR: Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages](https://arxiv.org/abs/2601.12389)
*Lakshya Tomar,Vinayak Abrol,Puneet Agarwal*

Main category: cs.CL

TL;DR: The paper introduces NADIR, a non-autoregressive (NAR) architecture for sequence-to-sequence tasks, focusing on multilingual transliteration in Indic languages, achieving significant speed-up and competitive accuracy compared to autoregressive models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the problem of high inference latency in autoregressive (AR) models for tasks relying on local dependencies, such as grammatical correction or transliteration, while overcoming the accuracy challenges seen in non-autoregressive (NAR) models.

Method: The paper presents NADIR, a novel NAR architecture combining Differential Transformer and Mixture-of-Experts mechanisms to model complex character mappings efficiently, focusing on speed and accuracy while reducing common NAR model errors.

Result: NADIR achieved a 13x speed-up compared to AR baselines, maintaining a competitive Character Error Rate (CER) of 15.78%. It significantly reduced common errors like repetition, substitution, omission, and insertion.

Conclusion: NADIR successfully bridges the gap between AR model accuracy and NAR model speed, offering a practical solution for real-time and large-scale deployment in sequence-to-sequence tasks.

Abstract: In this work, we argue that not all sequence-to-sequence tasks require the strong inductive biases of autoregressive (AR) models. Tasks like multilingual transliteration, code refactoring, grammatical correction or text normalization often rely on local dependencies where the full modeling capacity of AR models can be overkill, creating a trade-off between their high accuracy and high inference latency. While non-autoregressive (NAR) models offer speed, they typically suffer from hallucinations and poor length control. To explore this trade-off, we focus on the multilingual transliteration task in Indic languages and introduce NADIR, a novel NAR architecture designed to strike a balance between speed and accuracy. NADIR integrates a Differential Transformer and a Mixture-of-Experts mechanism, enabling it to robustly model complex character mappings without sequential dependencies. NADIR achieves over a 13x speed-up compared to the state-of-the-art AR baseline. It maintains a competitive mean Character Error Rate of 15.78%, compared to 14.44% for the AR model and 21.88% for a standard NAR equivalent. Importantly, NADIR reduces Repetition errors by 49.53%, Substitution errors by 24.45%, Omission errors by 32.92%, and Insertion errors by 16.87%. This work provides a practical blueprint for building fast and reliable NAR systems, effectively bridging the gap between AR accuracy and the demands of real-time, large-scale deployment.

</details>


### [169] [Legal experts disagree with rationale extraction techniques for explaining ECtHR case outcome classification](https://arxiv.org/abs/2601.12419)
*Mahammad Namazov,Tomáš Koref,Ivan Habernal*

Main category: cs.CL

TL;DR: The paper proposes a framework to compare model-agnostic interpretability techniques for legal outcome predictions, focusing on rationale extraction methods.


<details>
  <summary>Details</summary>
Motivation: To address the need for trust and transparency in applying large language models (LLMs) in the legal domain and understand the best interpretability techniques for such scenarios.

Method: The framework evaluates rationale extraction methods using faithfulness metrics (normalized sufficiency and comprehensiveness) and plausibility through legal expert validation. LLMs are also assessed for feasibility as a 'judge.'

Result: The study reveals a substantial difference between the rationale of LLMs for predicting violations and those of legal experts, despite promising quantitative and classification performance results.

Conclusion: Interpretability techniques require further refinement as LLMs do not align with legal experts' reasoning, despite quantitative metrics suggesting promising performance.

Abstract: Interpretability is critical for applications of large language models in the legal domain which requires trust and transparency. While some studies develop task-specific approaches, other use the classification model's parameters to explain the decisions. However, which technique explains the legal outcome prediction best remains an open question. To address this challenge, we propose a comparative analysis framework for model-agnostic interpretability techniques. Among these, we employ two rationale extraction methods, which justify outcomes with human-interpretable and concise text fragments (i.e., rationales) from the given input text. We conduct comparison by evaluating faithfulness-via normalized sufficiency and comprehensiveness metrics along with plausibility-by asking legal experts to evaluate extracted rationales. We further assess the feasibility of LLM-as-a-Judge using legal expert evaluation results. We show that the model's "reasons" for predicting a violation differ substantially from those of legal experts, despite highly promising quantitative analysis results and reasonable downstream classification performance. The source code of our experiments is publicly available at https://github.com/trusthlt/IntEval.

</details>


### [170] [System-Mediated Attention Imbalances Make Vision-Language Models Say Yes](https://arxiv.org/abs/2601.12430)
*Tsan Tsai Chan,Varsha Suresh,Anisha Saha,Michael Hahn,Vera Demberg*

Main category: cs.CL

TL;DR: The paper explores hallucination in Vision-Language Models (VLMs) and emphasizes the need to address system-mediated imbalances in attention allocation across input modalities rather than focusing only on image attention.


<details>
  <summary>Details</summary>
Motivation: To address hallucination in VLMs, particularly the common 'yes-bias,' by investigating and rebalancing attention allocated to input modalities, including the often-overlooked system weights.

Method: The authors propose redistributing attention from the system modality to image and textual inputs to mitigate imbalances and study its effects on reducing the 'yes-bias.'

Result: The redistribution of attention from system modality to input modalities effectively suppresses 'yes-bias' and outperforms existing mitigation approaches. Evidence also shows system imbalances lead to coarse input reliance.

Conclusion: System attention plays a critical role in VLM hallucination and can be a key factor for its mitigation. Addressing system-modal imbalances offers a new perspective for improving VLM functionality.

Abstract: Vision-language model (VLM) hallucination is commonly linked to imbalanced allocation of attention across input modalities: system, image and text. However, existing mitigation strategies tend towards an image-centric interpretation of these imbalances, often prioritising increased image attention while giving less consideration to the roles of the other modalities. In this study, we evaluate a more holistic, system-mediated account, which attributes these imbalances to functionally redundant system weights that reduce attention to image and textual inputs. We show that this framework offers a useful empirical perspective on the yes-bias, a common form of hallucination in which VLMs indiscriminately respond 'yes'. Causally redistributing attention from the system modality to image and textual inputs substantially suppresses this bias, often outperforming existing approaches. We further present evidence suggesting that system-mediated attention imbalances contribute to the yes-bias by encouraging a default reliance on coarse input representations, which are effective for some tasks but ill-suited to others. Taken together, these findings firmly establish system attention as a key factor in VLM hallucination and highlight its potential as a lever for mitigation.

</details>


### [171] [Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping](https://arxiv.org/abs/2601.12465)
*Miao Peng,Weizhou Shen,Nuo Chen,Chenliang Li,Ming Yan,Jia Li*

Main category: cs.CL

TL;DR: This paper proposes DeepReasonQA and LongPAS to address issues in long-context reasoning for LLMs, improving upon RLVR through enhanced data synthesis and fine-grained credit assignment.


<details>
  <summary>Details</summary>
Motivation: To address performance degradation in long-context reasoning tasks where existing RLVR struggles due to lacks in reasoning density in data and indiscriminate penalization during training.

Method: The authors introduce DeepReasonQA, a KG-driven framework for generating high-complexity QA pairs, and LongPAS, a method for credit assignment based on reasoning step validity and relevance.

Result: The proposed approach outperformed RLVR baselines in experiments across three long-context benchmarks, demonstrating improved reasoning capabilities with fewer parameters.

Conclusion: DeepReasonQA and LongPAS enhance long-context reasoning in LLMs, overcoming RLVR's shortcomings while ensuring stable RL training.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective in enhancing LLMs short-context reasoning, but its performance degrades in long-context scenarios that require both precise grounding and robust long-range reasoning. We identify the "almost-there" phenomenon in long-context reasoning, where trajectories are largely correct but fail at the final step, and attribute this failure to two factors: (1) the lack of high reasoning density in long-context QA data that push LLMs beyond mere grounding toward sophisticated multi-hop reasoning; and (2) the loss of valuable learning signals during long-context RL training due to the indiscriminate penalization of partially correct trajectories with incorrect outcomes. To overcome this bottleneck, we propose DeepReasonQA, a KG-driven synthesis framework that controllably constructs high-difficulty, multi-hop long-context QA pairs with inherent reasoning chains. Building on this, we introduce Long-context Process Advantage Shaping (LongPAS), a simple yet effective method that performs fine-grained credit assignment by evaluating reasoning steps along Validity and Relevance dimensions, which captures critical learning signals from "almost-there" trajectories. Experiments on three long-context reasoning benchmarks show that our approach substantially outperforms RLVR baselines and matches frontier LLMs while using far fewer parameters. Further analysis confirms the effectiveness of our methods in strengthening long-context reasoning while maintaining stable RL training.

</details>


### [172] [Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty](https://arxiv.org/abs/2601.12471)
*Sravanthi Machcha,Sushrita Yerra,Sahil Gupta,Aishwarya Sahoo,Sharmin Sultana,Hong Yu,Zonghai Yao*

Main category: cs.CL

TL;DR: This paper presents MedAbstain, a benchmark for evaluating abstention in medical multiple-choice question answering by LLMs, revealing significant gaps in their ability to abstain when uncertain.


<details>
  <summary>Details</summary>
Motivation: To address the lack of emphasis on abstention -- a critical aspect for deploying LLMs in safety-critical applications like healthcare, where an ability to recognize uncertainty is vital.

Method: The paper introduces MedAbstain, which integrates conformal prediction, adversarial perturbations, and explicit abstention mechanisms to test LLMs’ behavior in a medical MCQA context.

Result: The study finds that even advanced LLMs fail to reliably abstain when uncertain. While providing explicit abstention options improves uncertainty management, increasing model size or using advanced prompts offers negligible benefits.

Conclusion: Abstention mechanisms are critical for trustworthy deployment of LLMs in high-stakes tasks. Explicit options for abstention significantly improve safety and uncertainty handling, offering guidance for improved reliability of LLMs.

Abstract: Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA) -- a discrete-choice setting that generalizes to agentic action selection -- integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.

</details>


### [173] [Capability-Aware Early-Stage Research Idea Evaluation](https://arxiv.org/abs/2601.12473)
*Renlong Jie,Chen Chu,Zhen Wang*

Main category: cs.CL

TL;DR: The paper proposes a novel framework that predicts research outcomes using only author information and research ideas, eliminating the need for a full manuscript or experimental results.


<details>
  <summary>Details</summary>
Motivation: To improve scientific resource allocation and research planning by predicting the outcomes of research ideas at their conceptual stage.

Method: A capability-aware three-way transformer architecture integrates author information, capability representation, and research ideas, combined with a two-stage learning approach.

Result: The proposed framework outperformed single-way models using finetuned BERT, and capability prediction significantly enhanced accuracy.

Conclusion: This framework provides a practical tool for early-stage research outcome prediction and optimizing resource allocation in science.

Abstract: Predicting the outcomes of research ideas at their conceptual stage (i.e. before significant resources are committed) holds great potential for optimizing scientific resource allocation and research planning. While existing methods rely heavily on finished manuscripts or peer reviews, we propose a novel capability-aware framework that predicts paper acceptance and ratings using only author information and research ideas, without requiring full text or experimental results. Our approach integrates author information, (inferred) capability presentation, and research ideas through a three-way transformer architecture with flexible fusion mechanisms. We also introduce a two-stage architecture for learning the capability representation given the author information and idea. Experiments show that our method significantly outperform the single-way models by finetuning bert-base and bert-large, and the capability predicting significantly increase the predictive accuracy of the final model. The proposed method can be applied in both early-stage research outcome prediction and scientific resource allocation.

</details>


### [174] [DoPE: Decoy Oriented Perturbation Encapsulation Human-Readable, AI-Hostile Documents for Academic Integrity](https://arxiv.org/abs/2601.12505)
*Ashish Raj Shekhar,Shiven Agarwal,Priyanuj Bordoloi,Yash Shah,Tejas Anvekar,Vivek Gupta*

Main category: cs.CL

TL;DR: The paper introduces DoPE, a defense framework embedding semantic decoys into assessment documents to protect against automated solving by Multimodal Large Language Models (MLLMs) and ensure academic integrity.


<details>
  <summary>Details</summary>
Motivation: MLLMs are capable of processing and solving exams directly, which jeopardizes traditional assessments and can lead to academic dishonesty.

Method: DoPE employs semantic decoys within PDF/HTML documents, disrupting MLLM pipelines and includes FewSoRT-Q for generating question-level decoys and FewSoRT-D for embedding them in documents.

Result: DoPE achieved a 91.4% detection rate at 8.7% false positives and reduced MLLM test success or induced failures in 96.3% cases using Integrity-Bench evaluations.

Conclusion: The framework proves effective for both detection and prevention against MLLM threats, promoting academic integrity, and its resources are openly available for further research and replication.

Abstract: Multimodal Large Language Models (MLLMs) can directly consume exam documents, threatening conventional assessments and academic integrity. We present DoPE (Decoy-Oriented Perturbation Encapsulation), a document-layer defense framework that embeds semantic decoys into PDF/HTML assessments to exploit render-parse discrepancies in MLLM pipelines. By instrumenting exams at authoring time, DoPE provides model-agnostic prevention (stop or confound automated solving) and detection (flag blind AI reliance) without relying on conventional one-shot classifiers. We formalize prevention and detection tasks, and introduce FewSoRT-Q, an LLM-guided pipeline that generates question-level semantic decoys and FewSoRT-D to encapsulate them into watermarked documents. We evaluate on Integrity-Bench, a novel benchmark of 1826 exams (PDF+HTML) derived from public QA datasets and OpenCourseWare. Against black-box MLLMs from OpenAI and Anthropic, DoPE yields strong empirical gains: a 91.4% detection rate at an 8.7% false-positive rate using an LLM-as-Judge verifier, and prevents successful completion or induces decoy-aligned failures in 96.3% of attempts. We release Integrity-Bench, our toolkit, and evaluation code to enable reproducible study of document-layer defenses for academic integrity.

</details>


### [175] [Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning](https://arxiv.org/abs/2601.12535)
*Ahmed Attia,Alham Fikri*

Main category: cs.CL

TL;DR: The paper explores improved low-resource machine translation using self-supervised reinforcement learning with round-trip translations based on NLLB models.


<details>
  <summary>Details</summary>
Motivation: To improve machine translation capabilities for low-resource languages, as a significant portion of research disregards potential methods for these underrepresented communities.

Method: Utilized self-supervised reinforcement learning for fine-tuning NLLB models, evaluating performance using chrF++ and BLEU scores in a round-trip bootstrapping method translating between English and the low-resource language.

Result: Consistent translation improvements were documented for Central Aymara, Friulian, Wolof, and Russian. Translations showed better fluency and semantic alignment.

Conclusion: The proposed method effectively improves low-resource machine translation and has the potential to scale further, leveraging pretrained knowledge for continuous self-improvement.

Abstract: Low-resource machine translation (MT) has gained increasing attention as parallel data from low-resource language communities is collected, but many potential methods for improving low-resource MT remain unexplored. We investigate a self-supervised reinforcement-learning-based fine-tuning for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. Our approach translates English into a target low-resource language and then back into English, using a combination of chrF++ and BLEU as the reward function on the reconstructed English sentences. Using the NLLB-MD dataset, we evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for the following languages: Central Aymara, Friulian, Wolof and Russian. Qualitative inspection of translation outputs indicates increased fluency and semantic fidelity. We argue that our method can further benefit from scale, enabling models to increasingly leverage their pretrained knowledge and continue self-improving.

</details>


### [176] [Benchmarking Concept-Spilling Across Languages in LLMs](https://arxiv.org/abs/2601.12549)
*Ilia Badanin,Daniil Dzenhaliou,Imanol Schlag*

Main category: cs.CL

TL;DR: The paper investigates 'language spilling,' where multilingual LLMs mix languages during content generation. It introduces a benchmark to evaluate semantic robustness across languages.


<details>
  <summary>Details</summary>
Motivation: To address and evaluate the bias in multilingual LLMs that causes semantic interference and affects content generation in non-English languages.

Method: Developed a framework to measure model performance by how well they handle polysemous words across nine languages, using a structured benchmark of 100 high-polysemy English words.

Result: The evaluation revealed significant variations in multilingual semantic robustness across models and languages, highlighting systematic weaknesses.

Conclusion: The paper presents a scalable evaluation framework, advancing tools for developing linguistically balanced AI systems.

Abstract: Multilingual Large Language Models (LLMs) exhibit remarkable cross-lingual abilities, yet often exhibit a systematic bias toward the representations from other languages, resulting in semantic interference when generating content in non-English languages$-$a phenomenon we define as language spilling. This paper presents a novel comparative framework for evaluating multilingual semantic robustness by systematically measuring how models handle polysemous words across languages. Our methodology provides a relative measure of model performance: when required to generate exactly five meanings, both strong and weak models may resort to meanings from dominant languages, but semantically stronger models do so later in the generation sequence, producing more true meanings from the target language before failing, while weaker models resort to dominant-language meanings earlier in the sequence. We evaluate a diverse set of open and closed multilingual LLMs using a structured meaning generation task across nine languages, employing a carefully curated benchmark of 100 high-polysemy English words. Our findings reveal significant variation in semantic robustness across both models and languages, providing a principled ranking system for model comparison without requiring definitive causal attribution of error sources. We contribute both a scalable comparative benchmark for multilingual semantic evaluation and a rigorous validation pipeline$-$critical tools for developing more linguistically balanced AI systems.

</details>


### [177] [Evaluating Contextually Mediated Factual Recall in Multilingual Large Language Models](https://arxiv.org/abs/2601.12555)
*Yihong Liu,Bingyu Xiong,Hinrich Schütze*

Main category: cs.CL

TL;DR: This paper investigates how well large language models (LLMs) can recall factual knowledge when facts are contextually embedded rather than directly queried, across multiple languages.


<details>
  <summary>Details</summary>
Motivation: The study aims to determine whether LLMs can retrieve factual knowledge when the entities are introduced indirectly through context instead of being explicitly named, which mirrors real-world natural language use.

Method: The authors constructed controlled prompts that introduce contextual mediation while preserving underlying facts. They compared performance using real names and synthetic names across five languages and evaluated different LLM families.

Result: Results indicate that contextual mediation significantly degrades factual recall, with variations across relations. Larger models are more resilient to this degradation, showing smaller performance gaps, but the effects of name origin are inconsistent.

Conclusion: There is a noticeable difference in LLM performance between isolated factual recall and context-based comprehension, indicating a need for improving context-dependent understanding in multilingual LLMs.

Abstract: Large language models (LLMs) can recall a wide range of factual knowledge across languages. However, existing factual recall evaluations primarily assess fact retrieval in isolation, where the queried entity is explicitly named and the fact is requested directly. In natural language use, facts are often accessed through context, where the relevant entity is introduced only indirectly. In this work, we study contextually mediated factual recall, asking whether LLMs can reliably retrieve factual knowledge when the target entity is embedded in a naturalistic context rather than queried explicitly, across languages. We construct controlled prompts that preserve the underlying fact while introducing referential mediation through contextual sentences. To disentangle contextual effects from name-specific associations, we further compare performance using synthetic names and real names across languages. Evaluating multiple model families in five languages, we find that contextual mediation consistently degrades factual recall, with substantial variation across relations. Larger models are more robust to contextual mediation, exhibiting a reduced performance gap relative to direct queries, while the effect of real names and name origin is mixed and unsystematic. These findings highlight a gap between isolated factual recall and context-dependent language understanding in multilingual LLMs.

</details>


### [178] [A Cloud-based Multi-Agentic Workflow for Science](https://arxiv.org/abs/2601.12607)
*Anurag Acharya,Timothy Vega,Rizwan A. Ashraf,Anshu Sharma,Derek Parker,Robert Rallo*

Main category: cs.CL

TL;DR: The paper introduces a domain-agnostic, model-independent agentic framework run entirely on the cloud to perform scientific tasks like literature review, data analysis, and simulations, achieving high accuracy and task completion.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations of Large Language Models (LLMs), particularly their inability to run simulations or make complex decisions, by introducing an agentic framework that integrates external tools and resources effectively.

Method: The authors developed a cloud-based framework with a supervisor agent managing multiple specialized agents to handle a range of tasks, including proof-of-concept implementation for Catalyst research in Chemistry and Material Science.

Result: The framework achieved 90% task routing accuracy and 97.5% successful task completion for synthetic tasks, and 91% for real-world tasks, showing strong performance comparable to frontier models.

Conclusion: The proposed agentic framework demonstrates that it is a practical and extensible solution for enhancing LLM utility in scientific applications with reliable accuracy and scalability across domains.

Abstract: As Large Language Models (LLMs) become ubiquitous across various scientific domains, their lack of ability to perform complex tasks like running simulations or to make complex decisions limits their utility. LLM-based agents bridge this gap due to their ability to call external resources and tools and thus are now rapidly gaining popularity. However, coming up with a workflow that can balance the models, cloud providers, and external resources is very challenging, making implementing an agentic system more of a hindrance than a help. In this work, we present a domain-agnostic, model-independent workflow for an agentic framework that can act as a scientific assistant while being run entirely on cloud. Built with a supervisor agent marshaling an array of agents with individual capabilities, our framework brings together straightforward tasks like literature review and data analysis with more complex ones like simulation runs. We describe the framework here in full, including a proof-of-concept system we built to accelerate the study of Catalysts, which is highly important in the field of Chemistry and Material Science. We report the cost to operate and use this framework, including the breakdown of the cost by services use. We also evaluate our system on a custom-curated synthetic benchmark and a popular Chemistry benchmark, and also perform expert validation of the system. The results show that our system is able to route the task to the correct agent 90% of the time and successfully complete the assigned task 97.5% of the time for the synthetic tasks and 91% of the time for real-world tasks, while still achieving better or comparable accuracy to most frontier models, showing that this is a viable framework for other scientific domains to replicate.

</details>


### [179] [Disagreement as Data: Reasoning Trace Analytics in Multi-Agent Systems](https://arxiv.org/abs/2601.12618)
*Elham Tajik,Conrad Borchers,Bahar Shahrokhian,Sebastian Simon,Ali Keramati,Sonika Pal,Sreecharan Sankaranarayanan*

Main category: cs.CL

TL;DR: The paper proposes a method using reasoning traces from large language models (LLMs) to enhance qualitative coding processes. Cosine similarity metrics are used to quantify disagreements among LLM agents, reframing such disagreements as meaningful signals.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need for methodological standards to guide automated and human-AI collaborative workflows in qualitative data analysis, particularly with the rise of generative AI.

Method: The method includes applying cosine similarity to reasoning traces from LLM agents to detect, quantify, and interpret disagreements. This approach is tested on nearly 10,000 agent-pair instances coding human tutoring dialogues.

Result: The study finds that LLM-derived reasoning traces effectively differentiate agreement from disagreement and align with human coding reliability. The similarity metric supports nuanced interpretations and aids in refining codebooks.

Conclusion: Integrating quantitative metrics with qualitative processes can accelerate inter-rater reliability and enhance interpretive practices, demonstrating the potential of reasoning-trace disagreements as valuable analytic signals.

Abstract: Learning analytics researchers often analyze qualitative student data such as coded annotations or interview transcripts to understand learning processes. With the rise of generative AI, fully automated and human-AI workflows have emerged as promising methods for analysis. However, methodological standards to guide such workflows remain limited. In this study, we propose that reasoning traces generated by large language model (LLM) agents, especially within multi-agent systems, constitute a novel and rich form of process data to enhance interpretive practices in qualitative coding. We apply cosine similarity to LLM reasoning traces to systematically detect, quantify, and interpret disagreements among agents, reframing disagreement as a meaningful analytic signal. Analyzing nearly 10,000 instances of agent pairs coding human tutoring dialog segments, we show that LLM agents' semantic reasoning similarity robustly differentiates consensus from disagreement and correlates with human coding reliability. Qualitative analysis guided by this metric reveals nuanced instructional sub-functions within codes and opportunities for conceptual codebook refinement. By integrating quantitative similarity metrics with qualitative review, our method has the potential to improve and accelerate establishing inter-rater reliability during coding by surfacing interpretive ambiguity, especially when LLMs collaborate with humans. We discuss how reasoning-trace disagreements represent a valuable new class of analytic signals advancing methodological rigor and interpretive depth in educational research.

</details>


### [180] [BioPulse-QA: A Dynamic Biomedical Question-Answering Benchmark for Evaluating Factuality, Robustness, and Bias in Large Language Models](https://arxiv.org/abs/2601.12632)
*Kriti Bhattarai,Vipina K. Keloth,Donald Wright,Andrew Loza,Yang Ren,Hua Xu*

Main category: cs.CL

TL;DR: The paper introduces BioPulse-QA, a new benchmark for biomedical large language models (LLMs), designed to address existing limitations of static datasets by using dynamic biomedical documents. Four LLMs were evaluated, with varied performance across different data sources.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for biomedical LLMs rely on static, outdated datasets with issues like data leakage and lack evaluation of linguistic robustness and demographic biases. There is a need for a dynamic framework reflecting the fast-paced biomedical field.

Method: The authors developed BioPulse-QA, a benchmark with 2,280 expert-verified QA pairs covering extractive and abstractive formats. It evaluates LLMs on new biomedical documents, including drug labels and clinical guidelines, and examines their responses to data perturbations and biases.

Result: GPT-o1 outperformed other models with a relaxed F1 score of 0.92 on drug labels, while clinical trials posed the greatest challenge, with F1 scores as low as 0.36. Performance differences were observed more in response to paraphrasing than typographical errors.

Conclusion: BioPulse-QA offers an updated and dynamic tool for evaluating biomedical LLMs, addressing critical shortcomings of existing benchmarks, and can reveal performance gaps and biases in a scalable, clinically relevant way.

Abstract: Objective: Large language models (LLMs) are increasingly applied in biomedical settings, and existing benchmark datasets have played an important role in supporting model development and evaluation. However, these benchmarks often have limitations. Many rely on static or outdated datasets that fail to capture the dynamic, context-rich, and high-stakes nature of biomedical knowledge. They also carry increasing risk of data leakage due to overlap with model pretraining corpora and often overlook critical dimensions such as robustness to linguistic variation and potential demographic biases.
  Materials and Methods: To address these gaps, we introduce BioPulse-QA, a benchmark that evaluates LLMs on answering questions from newly published biomedical documents including drug labels, trial protocols, and clinical guidelines. BioPulse-QA includes 2,280 expert-verified question answering (QA) pairs and perturbed variants, covering both extractive and abstractive formats. We evaluate four LLMs - GPT-4o, GPT-o1, Gemini-2.0-Flash, and LLaMA-3.1 8B Instruct - released prior to the publication dates of the benchmark documents.
  Results: GPT-o1 achieves the highest relaxed F1 score (0.92), followed by Gemini-2.0-Flash (0.90) on drug labels. Clinical trials are the most challenging source, with extractive F1 scores as low as 0.36.
  Discussion and Conclusion: Performance differences are larger for paraphrasing than for typographical errors, while bias testing shows negligible differences. BioPulse-QA provides a scalable and clinically relevant framework for evaluating biomedical LLMs.

</details>


### [181] [Objective Matters: Fine-Tuning Objectives Shape Safety, Robustness, and Persona Drift](https://arxiv.org/abs/2601.12639)
*Daniel Vennemeyer,Punya Syon Pandey,Phan Anh Duong,Michael Umeokoli,Samuel Ratnam*

Main category: cs.CL

TL;DR: This paper analyzes the impact of fine-tuning objectives on the safety and robustness of language models, revealing that their role becomes critical at larger training scales.


<details>
  <summary>Details</summary>
Motivation: To explore how different fine-tuning objectives influence the alignment, adversarial robustness, and safety of large language models in controlled settings.

Method: The authors conducted a controlled comparison of six fine-tuning objectives, keeping data, architecture, domain, and optimization fixed while examining model performance on closed-form reasoning and open-ended generation tasks.

Result: At small training scales, different fine-tuning methods produced similar robustness outcomes but with varying capabilities. At larger scales, preference-based fine-tuning increased adversarial vulnerability and persona drift, whereas methods like KL-regularized fine-tuning mitigated these issues.

Conclusion: Fine-tuning objectives play a minimal role in safety at small scales but are pivotal in determining adversarial robustness and persona stability as training scales increase.

Abstract: Fine-tuning LLMs on benign data can still degrade alignment and adversarial robustness, yet direct analysis of the role of fine-tuning objectives in shaping these safety outcomes remain limited. We present a controlled comparison of six fine-tuning objectives -- Supervised Fine-Tuning, Direct Preference Optimization, Conditional Fine-Tuning, Inoculation Prompting, Odds Ratio Preference Optimization, and KL-regularized fine-tuning -- holding data, domain, architecture, and optimization fixed. Across closed-form reasoning and open-ended generation tasks, we find that objective choice induces systematic, scale-dependent shifts along the safety-capability frontier. At small training budgets, robustness is similar across objectives but capability differs. At larger budgets, objectives diverge sharply: supervised and preference-based tuning tightly couple capability gains to increased adversarial vulnerability and persona drift, while objectives that constrain learning signals -- especially ORPO and KL-regularization -- substantially mitigate both. Fine-tuning objectives therefore matter little for safety at small scales but become a primary driver of adversarial robustness and latent persona stability as training scale increases.

</details>


### [182] [Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?](https://arxiv.org/abs/2601.12648)
*Nafiz Imtiaz Khan,Kylie Cleland,Vladimir Filkov,Roger Eric Goldman*

Main category: cs.CL

TL;DR: The study explores using large language models (LLMs) to automate procedural case log documentation in radiology, achieving strong extraction performance and reducing manual effort.


<details>
  <summary>Details</summary>
Motivation: To reduce the clerical burden and improve consistency in procedural case logging for radiology trainees, as manual documentation is time-consuming and error-prone.

Method: The study tested multiple local and commercial LLMs using instruction-based and chain-of-thought prompting to extract structured procedural data from 414 interventional radiology reports by residents.

Result: Both local and commercial models performed well, achieving F1-scores up to 0.87, with varying trade-offs between speed and cost.

Conclusion: LLMs show promise in automating radiology case logging, reducing trainee effort, and enhancing documentation consistency, though further validation is needed in diverse settings.

Abstract: Procedural case logs are a core requirement in radiology training, yet they are time-consuming to complete and prone to inconsistency when authored manually. This study investigates whether large language models (LLMs) can automate procedural case log documentation directly from free-text radiology reports. We evaluate multiple local and commercial LLMs under instruction-based and chain-of-thought prompting to extract structured procedural information from 414 curated interventional radiology reports authored by nine residents between 2018 and 2024. Model performance is assessed using sensitivity, specificity, and F1-score, alongside inference latency and token efficiency to estimate operational cost. Results show that both local and commercial models achieve strong extraction performance, with best F1-scores approaching 0.87, while exhibiting different trade-offs between speed and cost. Automation using LLMs has the potential to substantially reduce clerical burden for trainees and improve consistency in case logging. These findings demonstrate the feasibility of AI-assisted documentation in medical education and highlight the need for further validation across institutions and clinical workflows.

</details>


### [183] [Augmenting Question Answering with A Hybrid RAG Approach](https://arxiv.org/abs/2601.12658)
*Tianyi Yang,Nashrah Haque,Vaishnave Jonnalagadda,Yuya Jeremy Ong,Zhehui Chen,Yanzhao Wu,Lei Yu,Divyesh Jadav,Wenqi Wei*

Main category: cs.CL

TL;DR: The paper introduces Structured-Semantic RAG (SSRAG), a hybrid architecture improving QA tasks' response quality through advanced retrieval mechanisms, evaluated on multiple QA datasets and models.


<details>
  <summary>Details</summary>
Motivation: The current RAG approaches often fail to retrieve contextually suitable and relevant information, leading to incomplete or poor answers.

Method: The authors propose SSRAG, a hybrid retrieval model integrating query augmentation, agentic routing, and a structured mechanism that incorporates vector and graph-based retrieval with unified context.

Result: SSRAG demonstrated consistent improvement in response accuracy and informativeness across TruthfulQA, SQuAD, and WikiQA datasets using five different large language models.

Conclusion: SSRAG significantly surpasses standard RAG techniques by enhancing contextual retrieval and grounding, providing better QA outcomes.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for enhancing the quality of responses in Question-Answering (QA) tasks. However, existing approaches often struggle with retrieving contextually relevant information, leading to incomplete or suboptimal answers. In this paper, we introduce Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances QA quality by integrating query augmentation, agentic routing, and a structured retrieval mechanism combining vector and graph based techniques with context unification. By refining retrieval processes and improving contextual grounding, our approach improves both answer accuracy and informativeness. We conduct extensive evaluations on three popular QA datasets, TruthfulQA, SQuAD and WikiQA, across five Large Language Models (LLMs), demonstrating that our proposed approach consistently improves response quality over standard RAG implementations.

</details>


### [184] [UbuntuGuard: A Culturally-Grounded Policy Benchmark for Equitable AI Safety in African Languages](https://arxiv.org/abs/2601.12696)
*Tassallah Abdullahi,Macton Mgonzo,Mardiyyah Oduwole,Paul Okewunmi,Abraham Owodunni,Ritambhara Singh,Carsten Eickhoff*

Main category: cs.CL

TL;DR: This paper introduces UbuntuGuard, a policy-based safety benchmark specifically designed for African languages to address linguistic and cultural shortcomings in current guardian models.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the shortcomings of existing Western-centric guardian models, which overlook safety issues and cultural contexts in low-resource African languages.

Method: The authors developed UbuntuGuard, incorporating adversarial queries from 155 domain experts across various sensitive fields, and proposed culturally specific safety benchmarks to evaluate guardian models.

Result: Evaluations of 13 models revealed that English-centric benchmarks are insufficient for multilingual safety, and dynamic models, while better in policy usage, still fail to fully accommodate African-language contexts.

Conclusion: This paper highlights the importance of multilingual, locally grounded safety benchmarks to improve guardian models' reliability and equity in low-resource linguistic environments.

Abstract: Current guardian models are predominantly Western-centric and optimized for high-resource languages, leaving low-resource African languages vulnerable to evolving harms, cross-lingual safety failures, and cultural misalignment. Moreover, most guardian models rely on rigid, predefined safety categories that fail to generalize across diverse linguistic and sociocultural contexts. Robust safety, therefore, requires flexible, runtime-enforceable policies and benchmarks that reflect local norms, harm scenarios, and cultural expectations. We introduce UbuntuGuard, the first African policy-based safety benchmark built from adversarial queries authored by 155 domain experts across sensitive fields, including healthcare. From these expert-crafted queries, we derive context-specific safety policies and reference responses that capture culturally grounded risk signals, enabling policy-aligned evaluation of guardian models. We evaluate 13 models, comprising six general-purpose LLMs and seven guardian models across three distinct variants: static, dynamic, and multilingual. Our findings reveal that existing English-centric benchmarks overestimate real-world multilingual safety, cross-lingual transfer provides partial but insufficient coverage, and dynamic models, while better equipped to leverage policies at inference time, still struggle to fully localize African-language contexts. These findings highlight the urgent need for multilingual, culturally grounded safety benchmarks to enable the development of reliable and equitable guardian models for low-resource languages. Our code can be found online.\footnote{Code repository available at https://github.com/hemhemoh/UbuntuGuard.

</details>


### [185] [A Two-Stage GPU Kernel Tuner Combining Semantic Refactoring and Search-Based Optimization](https://arxiv.org/abs/2601.12698)
*Qiuyi Qu,Yicheng Sui,Yufei Sun,Rui Chen,Xiaofei Zhang,Yuzhi Zhang,Haofeng Wang,Ge Lan,Ning Zhang*

Main category: cs.CL

TL;DR: The paper introduces a template-based rewriting approach combined with agent-driven optimization to achieve stable and superior GPU code performance, significantly reducing randomness compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Optimizing GPU code for performance is critical in high-performance computing (HPC) and large-model training but heavily relies on manual tuning or unstable automated approaches.

Method: The authors propose a method combining semantic templating of GPU kernels with search-based autotuning for parameter optimization, driven by iterative agent feedback using profiling data.

Result: The proposed approach achieves over 3x speedups in specific cases and reduces optimization randomness compared to direct rewriting methods.

Conclusion: The method provides a systematic and interpretable GPU code optimization framework, extensible to other backends, to enhance performance automation for real-world workloads.

Abstract: GPU code optimization is a key performance bottleneck for HPC workloads as well as large-model training and inference. Although compiler optimizations and hand-written kernels can partially alleviate this issue, achieving near-hardware-limit performance still relies heavily on manual code refactoring and parameter tuning. Recent progress in LLM-agent-based kernel generation and optimization has been reported, yet many approaches primarily focus on direct code rewriting, where parameter choices are often implicit and hard to control, or require human intervention, leading to unstable performance gains. This paper introduces a template-based rewriting layer on top of an agent-driven iterative loop: kernels are semantically refactored into explicitly parameterizable templates, and template parameters are then optimized via search-based autotuning, yielding more stable and higher-quality speedups. Experiments on a set of real-world kernels demonstrate speedups exceeding 3x in the best case. We extract representative CUDA kernels from SGLang as evaluation targets; the proposed agentic tuner iteratively performs templating, testing, analysis, and planning, and leverages profiling feedback to execute constrained parameter search under hardware resource limits. Compared to agent-only direct rewriting, the template-plus-search design significantly reduces the randomness of iterative optimization, making the process more interpretable and enabling a more systematic approach toward high-performance configurations. The proposed method can be further extended to OpenCL, HIP, and other backends to deliver automated performance optimization for real production workloads.

</details>


### [186] [A Shared Geometry of Difficulty in Multilingual Language Models](https://arxiv.org/abs/2601.12731)
*Stefano Civelli,Pietro Bernardelle,Nicolò Brunello,Gianluca Demartini*

Main category: cs.CL

TL;DR: This paper studies how large language models (LLMs) estimate task difficulty across multiple languages by training linear probes on their internal representations.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs internally represent and generalize problem difficulty across languages, shedding light on their interpretability and language-agnostic capabilities.

Method: The authors trained linear probes on AMC subset tasks from the Easy2Hard benchmark, translated into 21 languages, analyzing shallow (early layers) and deep (later layers) model representations.

Result: Linear probes on shallow representations generalized better across languages, while probes on deep representations showed higher accuracy for individual languages but poor cross-lingual generalization.

Conclusion: LLMs develop a language-agnostic representation of problem difficulty in early layers, transitioning to language-specific representations in later layers, aligning with prior interpretability findings for semantic concepts.

Abstract: Predicting problem-difficulty in large language models (LLMs) refers to estimating how difficult a task is according to the model itself, typically by training linear probes on its internal representations. In this work, we study the multilingual geometry of problem-difficulty in LLMs by training linear probes using the AMC subset of the Easy2Hard benchmark, translated into 21 languages. We found that difficulty-related signals emerge at two distinct stages of the model internals, corresponding to shallow (early-layers) and deep (later-layers) internal representations, that exhibit functionally different behaviors. Probes trained on deep representations achieve high accuracy when evaluated on the same language but exhibit poor cross-lingual generalization. In contrast, probes trained on shallow representations generalize substantially better across languages, despite achieving lower within-language performance. Together, these results suggest that LLMs first form a language-agnostic representation of problem difficulty, which subsequently becomes language-specific. This closely aligns with existing findings in LLM interpretability showing that models tend to operate in an abstract conceptual space before producing language-specific outputs. We demonstrate that this two-stage representational process extends beyond semantic content to high-level meta-cognitive properties such as problem-difficulty estimation.

</details>


### [187] [Towards Robust Process Reward Modeling via Noise-aware Learning](https://arxiv.org/abs/2601.12748)
*Bin Xie,Bingbing Xu,Xueyun Tian,Yilin Chen,Huawei Shen*

Main category: cs.CL

TL;DR: The paper introduces a framework to address label noise issues in Process Reward Models for reasoning tasks, achieving substantial improvements in performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome label noise in the widely used Monte Carlo Estimation method for Process Reward Models, aiming to improve step correctness evaluation in reasoning tasks.

Method: The authors propose a two-stage framework: 1) A labeling stage incorporating reflection-aware label correction with large language models and 2) A noise-aware iterative training mechanism for refining labels.

Result: The method significantly improves step-level correctness discrimination, with up to a 27% absolute gain in average F1 compared to models trained with noisy supervision.

Conclusion: The framework successfully mitigates noisy supervision in reasoning models, improving their accuracy and reliability.

Abstract: Process Reward Models (PRMs) have achieved strong results in complex reasoning, but are bottlenecked by costly process-level supervision. A widely used alternative, Monte Carlo Estimation (MCE), defines process rewards as the probability that a policy model reaches the correct final answer from a given reasoning step. However, step correctness is an intrinsic property of the reasoning trajectory, and should be invariant to policy choice. Our empirical findings show that MCE producing policy-dependent rewards that induce label noise, including false positives that reward incorrect steps and false negatives that penalize correct ones. To address above challenges, we propose a two-stage framework to mitigate noisy supervision. In the labeling stage, we introduce a reflection-aware label correction mechanism that uses a large language model (LLM) as a judge to detect reflection and self-correction behaviors related to the current reasoning step, thereby suppressing overestimated rewards. In the training stage, we further propose a \underline{\textbf{N}}oise-\underline{\textbf{A}}ware \underline{\textbf{I}}terative \underline{\textbf{T}}raining framework that enables the PRM to progressively refine noisy labels based on its own confidence. Extensive Experiments show that our method substantially improves step-level correctness discrimination, achieving up to a 27\% absolute gain in average F1 over PRMs trained with noisy supervision.

</details>


### [188] [VISPA: Pluralistic Alignment via Automatic Value Selection and Activation](https://arxiv.org/abs/2601.12758)
*Shenyan Zheng,Jiayou Zhong,Anudeex Shetty,Heng Ji,Preslav Nakov,Usman Naseem*

Main category: cs.CL

TL;DR: VISPA is introduced as a framework to achieve pluralistic alignment in language models without requiring training. It enables dynamic selection and control over value representation.


<details>
  <summary>Details</summary>
Motivation: The paper identifies a need for language models to reflect diverse perspectives rather than average human preferences, especially in high-stakes domains.

Method: VISPA utilizes dynamic selection and internal model activation steering to control value expression and pluralistic alignment in outputs, without requiring retraining.

Result: Empirical studies demonstrate that VISPA performs effectively across diverse alignment settings, showing adaptability with different models, values, and steering mechanisms.

Conclusion: Internal activation mechanisms like VISPA provide a pathway to scalable pluralistic alignment, enabling language models to better serve diverse needs and perspectives.

Abstract: As large language models are increasingly used in high-stakes domains, it is essential that their outputs reflect not average} human preference, rather range of varying perspectives. Achieving such pluralism, however, remains challenging. Existing approaches consider limited values or rely on prompt-level interventions, lacking value control and representation. To address this, we introduce VISPA, a training-free pluralistic alignment framework, that enables direct control over value expression by dynamic selection and internal model activation steering. Across extensive empirical studies spanning multiple models and evaluation settings, we show VISPA is performant across all pluralistic alignment modes in healthcare and beyond. Further analysis reveals VISPA is adaptable with different steering initiations, model, and/or values. These results suggest that pluralistic alignment can be achieved through internal activation mechanisms, offering a scalable path toward language models that serves all.

</details>


### [189] [Who Does This Name Remind You of? Nationality Prediction via Large Language Model Associative Memory](https://arxiv.org/abs/2601.12771)
*Keito Inoshita*

Main category: cs.CL

TL;DR: The paper introduces a framework called LAMA (LLM Associative Memory Agents) that demonstrates improved nationality prediction by using associative memory via LLMs instead of direct reasoning.


<details>
  <summary>Details</summary>
Motivation: Understanding nationality and region prediction requires both linguistic and cultural knowledge. Current methods relying on direct reasoning are insufficient for tasks involving abstract linguistic or historical knowledge.

Method: The proposed LAMA framework uses dual agents (Person Agent and Media Agent) to recall famous individuals with the same name and aggregates this knowledge for accurate nationality prediction. It leverages LLMs as a tool for associative memory.

Result: Using LAMA, the framework achieved 0.817 accuracy in a 99-country nationality prediction task, significantly outperforming conventional prompting methods and models.

Conclusion: The research highlights the superiority of recall-based approaches over reasoning for certain tasks and demonstrates the effectiveness of a dual-agent system in enhancing LLM capabilities.

Abstract: Large language models (LLMs) possess extensive world knowledge, yet methods for effectively eliciting this knowledge remain underexplored. Nationality and region prediction tasks require understanding of not only linguistic features but also cultural and historical background, making LLM world knowledge particularly valuable. However, conventional LLM prompting methods rely on direct reasoning approaches, which have limitations in applying abstract linguistic rules. We propose LLM Associative Memory Agents (LAMA), a novel framework that leverages LLM world knowledge as associative memory. Rather than directly inferring nationality from names, LAMA recalls famous individuals with the same name and aggregates their nationalities through indirect reasoning. A dual-agent architecture comprising a Person Agent and a Media Agent, specialized in different knowledge domains, recalls famous individuals in parallel, generating Top-1 predictions through voting and Top-K predictions through conditional completion. On a 99-country nationality prediction task, LAMA achieved 0.817 accuracy, substantially outperforming conventional LLM prompting methods and neural models. Our experiments reveal that LLMs exhibit higher reliability in recalling concrete examples than in abstract reasoning, that recall-based approaches are robust to low-frequency nationalities independent of data frequency distributions, and that the dual-agent architecture functions complementarily to produce synergistic effects. These results demonstrate the effectiveness of a new multi-agent system that retrieves and aggregates LLM knowledge rather than prompting reasoning.

</details>


### [190] [Do Clinical Question Answering Systems Really Need Specialised Medical Fine Tuning?](https://arxiv.org/abs/2601.12812)
*Sushant Kumar Ray,Gautam Siddharth Kashyap,Sahil Tripathi,Nipun Joshi,Vijay Govindarajan,Rafiq Ali,Jiechao Gao,Usman Naseem*

Main category: cs.CL

TL;DR: The paper introduces MEDASSESS-X, a CQA framework that aligns LLMs using inference-time methods rather than domain-specific retraining, improving performance and challenging the need for domain-specific LLM fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that specialised medical LLMs are inherently superior for clinical question-answering, addressing the limitations of narrow coverage, costly retraining, and low adaptability.

Method: The authors propose MEDASSESS-X, a lightweight framework that applies inference-time alignment with steering vectors to guide model activations toward medically consistent reasoning without retraining or updating model weights.

Result: MEDASSESS-X demonstrated performance improvements including up to +6% in Accuracy, +7% in Factual Consistency, and a 50% reduction in Safety Error Rate across various LLMs.

Conclusion: MEDASSESS-X effectively stabilises CQA performance across general-purpose and specialised LLMs, resolving the SPECIALISATION FALLACY and eliminating the need for costly domain-specific retraining.

Abstract: Clinical Question-Answering (CQA) industry systems are increasingly rely on Large Language Models (LLMs), yet their deployment is often guided by the assumption that domain-specific fine-tuning is essential. Although specialised medical LLMs such as BioBERT, BioGPT, and PubMedBERT remain popular, they face practical limitations including narrow coverage, high retraining costs, and limited adaptability. Efforts based on Supervised Fine-Tuning (SFT) have attempted to address these assumptions but continue to reinforce what we term the SPECIALISATION FALLACY-the belief that specialised medical LLMs are inherently superior for CQA. To address this assumption, we introduce MEDASSESS-X, a deployment-industry-oriented CQA framework that applies alignment at inference time rather than through SFT. MEDASSESS-X uses lightweight steering vectors to guide model activations toward medically consistent reasoning without updating model weights or requiring domain-specific retraining. This inference-time alignment layer stabilises CQA performance across both general-purpose and specialised medical LLMs, thereby resolving the SPECIALISATION FALLACY. Empirically, MEDASSESS-X delivers consistent gains across all LLM families, improving Accuracy by up to +6%, Factual Consistency by +7%, and reducing Safety Error Rate by as much as 50%.

</details>


### [191] [Multimodal Multi-Agent Empowered Legal Judgment Prediction](https://arxiv.org/abs/2601.12815)
*Zhaolu Kang,Junhao Gong,Qingxi Chen,Hao Zhang,Jiaxin Liu,Rong Fu,Zhiyuan Feng,Yuan Wang,Simon Fong,Kaiyue Zhou*

Main category: cs.CL

TL;DR: The paper introduces JurisMMA, a new framework for legal judgment prediction (LJP), along with a large multimodal dataset, demonstrating significant improvements in tackling challenges in LJP.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in Legal Judgment Prediction (LJP) such as handling multiple allegations, diverse evidence, and adaptability limitations in traditional statistical or simulation-based methods.

Method: The paper proposes JurisMMA, a trial decomposition framework that standardizes and organizes legal prediction processes into stages, supported by JurisMM—a large, multimodal dataset of over 100,000 Chinese judicial records.

Result: Experiments on the new JurisMM dataset and the benchmark dataset LawBench validated the framework's effectiveness, showing notable improvements in LJP tasks.

Conclusion: JurisMMA demonstrates advancements not only in LJP but also offers valuable methodologies and datasets for the broader legal field, opening new directions for legal technology development.

Abstract: Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.

</details>


### [192] [Rapport du Projet de Recherche TRAIMA](https://arxiv.org/abs/2601.12844)
*Julie Rançon,Jean-François Cerisier,Emilie Remond,Aurélien Nguyen,Andrew Peterson,Ladjel Bellatreche*

Main category: cs.CL

TL;DR: The TRAIMA project explores using machine learning to automate the analysis of multimodal classroom interactions and sets a methodological foundation for future tools.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of manually analyzing multimodal data in educational research, which is time-consuming and non-scalable.

Method: Investigated multimodal interactions in classrooms, defined key explanatory sequences, analyzed transcription conventions, compared transcription practices, and utilized multimodal data from corpora for testing and annotation.

Result: Provided a comprehensive analysis of transcription methods, demonstrated the variability in manual transcription, highlighted critical data like teacher gestures and prosody, and leveraged advanced data capture technologies.

Conclusion: The project establishes a methodological framework for automating multimodal interaction analysis, emphasizing the need for theoretical clarity and laying groundwork for interdisciplinary research in education and AI.

Abstract: The TRAIMA project (TRaitement Automatique des Interactions Multimodales en Apprentissage), conducted between March 2019 and June 2020, investigates the potential of automatic processing of multimodal interactions in educational settings. The project addresses a central methodological challenge in educational and interactional research: the analysis of verbal, paraverbal, and non-verbal data is currently carried out manually, making it extremely time-consuming and difficult to scale. TRAIMA explores how machine learning approaches could contribute to the categorisation and classification of such interactions. The project focuses specifically on explanatory and collaborative sequences occurring in classroom interactions, particularly in French as a Foreign Language (FLE) and French as a First Language (FLM) contexts. These sequences are analysed as inherently multimodal phenomena, combining spoken language with prosody, gestures, posture, gaze, and spatial positioning. A key theoretical contribution of the project is the precise linguistic and interactional definition of explanatory discourse as a tripartite sequence (opening, explanatory core, closure), drawing on discourse analysis and interactional linguistics. A substantial part of the research is devoted to the methodological foundations of transcription, which constitute a critical bottleneck for any form of automation. The report provides a detailed state of the art of existing transcription conventions (ICOR, Mondada, GARS, VALIBEL, Ferr{é}), highlighting their respective strengths and limitations when applied to multimodal classroom data. Through comparative analyses of manually transcribed sequences, the project demonstrates the inevitable variability and interpretative dimension of transcription practices, depending on theoretical positioning and analytical goals. Empirical work is based on several corpora, notably the INTER-EXPLIC corpus (approximately 30 hours of classroom interaction) and the EXPLIC-LEXIC corpus, which serve both as testing grounds for manual annotation and as reference datasets for future automation. Particular attention is paid to teacher gestures (kin{é}sic and proxemic resources), prosodic features, and their functional role in meaning construction and learner comprehension. The project also highlights the strategic role of the Techn{é}LAB platform, which provides advanced multimodal data capture (multi-camera video, synchronized audio, eye-tracking, digital interaction traces) and constitutes both a research infrastructure and a test environment for the development of automated tools. In conclusion, TRAIMA does not aim to deliver a fully operational automated system, but rather to establish a rigorous methodological framework for the automatic processing of multimodal pedagogical interactions. The project identifies transcription conventions, annotation categories, and analytical units that are compatible with machine learning approaches, while emphasizing the need for theoretical explicitness and researcher reflexivity. TRAIMA thus lays the groundwork for future interdisciplinary research at the intersection of didactics, discourse analysis, multimodality, and artificial intelligence in education.

</details>


### [193] [Race, Ethnicity and Their Implication on Bias in Large Language Models](https://arxiv.org/abs/2601.12868)
*Shiyue Hu,Ruizhe Li,Yanjun Gao*

Main category: cs.CL

TL;DR: The paper investigates how race and ethnicity are represented and operationalized in large language models (LLMs) using interpretability methods, highlighting internal biases and proposing mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: To explore and address biases in large language models concerning race and ethnicity, especially in high-stakes domains like healthcare.

Method: The study uses open-source LLMs, datasets on toxicity generation and clinical narratives, and a reproducible interpretability approach involving probing, neuron-level attribution, and targeted interventions.

Result: Demographic information is dispersed across neurons with significant variability across models. Some neurons encode biases, and suppressing them reduces but does not eliminate biased behavior, indicating the need for further systematic bias mitigation.

Conclusion: Interventions can reduce bias in LLMs but do not fully address underlying issues, urging continued efforts for systematic bias reduction in sensitive applications.

Abstract: Large language models (LLMs) increasingly operate in high-stakes settings including healthcare and medicine, where demographic attributes such as race and ethnicity may be explicitly stated or implicitly inferred from text. However, existing studies primarily document outcome-level disparities, offering limited insight into internal mechanisms underlying these effects. We present a mechanistic study of how race and ethnicity are represented and operationalized within LLMs. Using two publicly available datasets spanning toxicity-related generation and clinical narrative understanding tasks, we analyze three open-source models with a reproducible interpretability pipeline combining probing, neuron-level attribution, and targeted intervention. We find that demographic information is distributed across internal units with substantial cross-model variation. Although some units encode sensitive or stereotype-related associations from pretraining, identical demographic cues can induce qualitatively different behaviors. Interventions suppressing such neurons reduce bias but leave substantial residual effects, suggesting behavioral rather than representational change and motivating more systematic mitigation.

</details>


### [194] [From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.12904)
*Jiahao Wang,Weiyu Xie,Mingxing Zhang,Boxing Zhang,Jianwei Dong,Yuening Zhu,Chen Lin,Jinqi Tang,Yaochen Han,Zhiyuan Ai,Xianglin Chen,Yongwei Wu,Congfeng Jiang*

Main category: cs.CL

TL;DR: FusionRAG improves Retrieval-Augmented Generation (RAG) by optimizing preprocessing and reprocessing, balancing quality and efficiency effectively.


<details>
  <summary>Details</summary>
Motivation: Current RAG methods reduce hallucinations via external knowledge integration but face trade-offs like high computational costs and degraded quality due to lack of cross-chunk contextual reuse.

Method: FusionRAG embeds cross-chunk contextual information during preprocessing and recomputes critical KV cache parts during reprocessing to enhance quality and efficiency.

Result: FusionRAG significantly improves normalized F1 scores (up to 70% higher at similar recomputation ratios) and reduces TTFT by 2.66x-9.39x compared to Full Attention.

Conclusion: FusionRAG achieves a superior balance of computational efficiency and generation quality in RAG by addressing reuse and context limitations in KV cache.

Abstract: Retrieval-Augmented Generation enhances Large Language Models by integrating external knowledge, which reduces hallucinations but increases prompt length. This increase leads to higher computational costs and longer Time to First Token (TTFT). To mitigate this issue, existing solutions aim to reuse the preprocessed KV cache of each retrieved chunk to accelerate RAG. However, the lack of cross-chunk contextual information leads to a significant drop in generation quality, leaving the potential benefits of KV cache reuse largely unfulfilled. The challenge lies in how to reuse the precomputed KV cache of chunks while preserving generation quality. We propose FusionRAG, a novel inference framework that optimizes both the preprocessing and reprocessing stages of RAG. In the offline preprocessing stage, we embed information from other related text chunks into each chunk, while in the online reprocessing stage, we recompute the KV cache for tokens that the model focuses on. As a result, we achieve a better trade-off between generation quality and efficiency. According to our experiments, FusionRAG significantly improves generation quality at the same recomputation ratio compared to previous state-of-the-art solutions. By recomputing fewer than 15% of the tokens, FusionRAG achieves up to 70% higher normalized F1 scores than baselines and reduces TTFT by 2.66x-9.39x compared to Full Attention.

</details>


### [195] [Gated Differentiable Working Memory for Long-Context Language Modeling](https://arxiv.org/abs/2601.12906)
*Lingrui Mei,Shenghua Liu,Yiwei Wang,Yuyao Ge,Baolong Bi,Jiayu Yao,Jun Wan,Ziling Yin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: The paper introduces Gdwm, a system that optimizes test-time adaptation in transformers with gated memory consolidation, reducing computation while improving results.


<details>
  <summary>Details</summary>
Motivation: Transformers struggle with long contexts as attention scores diffuse across tokens and critical information is lost, hindering their adaptability in inference. The study aims to address these issues through better memory management.

Method: Gdwm employs a gated write controller based on Contextual Utility measure that dynamically allocates memory updates rather than relying on inefficient uniform policies.

Result: Experiments demonstrate that Gdwm matches or outperforms other methods using four times fewer gradient steps, significantly improving both efficiency and performance.

Conclusion: Their proposed Gdwm framework redefines the efficiency-performance balance in test-time adaptation, showing promise for solving long-context challenges in transformers.

Abstract: Long contexts challenge transformers: attention scores dilute across thousands of tokens, critical information is often lost in the middle, and models struggle to adapt to novel patterns at inference time. Recent work on test-time adaptation addresses this by maintaining a form of working memory -- transient parameters updated on the current context -- but existing approaches rely on uniform write policies that waste computation on low-utility regions and suffer from high gradient variance across semantically heterogeneous contexts. In this work, we reframe test-time adaptation as a budget-constrained memory consolidation problem, focusing on which parts of the context should be consolidated into working memory under limited computation. We propose Gdwm (Gated Differentiable Working Memory), a framework that introduces a write controller to gate the consolidation process. The controller estimates Contextual Utility, an information-theoretic measure of long-range contextual dependence, and allocates gradient steps accordingly while maintaining global coverage. Experiments on ZeroSCROLLS and LongBench v2 demonstrate that Gdwm achieves comparable or superior performance with 4$\times$ fewer gradient steps than uniform baselines, establishing a new efficiency-performance Pareto frontier for test-time adaptation.

</details>


### [196] [SciCoQA: Quality Assurance for Scientific Paper--Code Alignment](https://arxiv.org/abs/2601.12910)
*Tim Baumgärtner,Iryna Gurevych*

Main category: cs.CL

TL;DR: SciCoQA dataset detects and analyzes discrepancies between scientific publications and their code implementations, combining real and synthetic instances.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of ensuring fidelity between scientific publications and their codebase implementations to improve reproducibility and understanding.

Method: Constructed SciCoQA using GitHub issues and reproducibility papers, coupled with synthetic data generation for scalability. Introduced discrepancy categories and evaluated various large language models (LLMs).

Result: Created a dataset comprising 611 discrepancies, tested 21 LLMs, and found that the best performance (GPT-5) only detected 45.7% of real-world discrepancies.

Conclusion: Even advanced AI struggles with recognizing paper-code mismatches, highlighting the need for more advanced tools and methods to address the challenge.

Abstract: We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\% of real-world paper-code discrepancies.

</details>


### [197] [Injecting Knowledge from Social Science Journals to Improve Indonesian Cultural Understanding by LLMs](https://arxiv.org/abs/2601.12921)
*Adimulya Kartiyasa,Bao Gia Cao,Boyang Li*

Main category: cs.CL

TL;DR: The paper introduces IndoSoSci, a novel dataset from Indonesian social science journals, and a retrieval-augmented method for injecting cultural knowledge into large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: There is a need to enhance LLMs' understanding of Indonesian cultures, which has been underrepresented in existing models.

Method: The researchers created IndoSoSci, extracted cultural facts from it, and applied retrieval-augmented generation (RAG) using hypothetical documents to enhance LLMs.

Result: The approach outperformed baselines and set a new state-of-the-art on the IndoCulture benchmark by integrating IndoSoSci with Indonesian Wikipedia.

Conclusion: Utilizing local social science journals and RAG significantly enhances LLMs' cultural comprehension, demonstrating a pathway for improving models' performance in specific cultural contexts.

Abstract: Recently there have been intensifying efforts to improve the understanding of Indonesian cultures by large language models (LLMs). An attractive source of cultural knowledge that has been largely overlooked is local journals of social science, which likely contain substantial cultural studies from a native perspective. We present a novel text dataset of journal article passages, created from 151 open-source Indonesian social science journals, called IndoSoSci. We demonstrate an effective recipe for injecting Indonesian cultural knowledge therein into LLMs: extracting the facts related to Indonesian culture, and apply retrieval-augmented generation (RAG) with LLM-generated hypothetical documents as queries during retrieval. The proposed recipe yields strong performance gains over several strong baselines on the IndoCulture benchmark. Additionally, by combining IndoSoSci with Indonesian Wikipedia, we set a new state-of-the-art accuracy on the IndoCulture benchmark.

</details>


### [198] [A Component-Based Survey of Interactions between Large Language Models and Multi-Armed Bandits](https://arxiv.org/abs/2601.12945)
*Miao Xie,Siguang Chen,Chunli Lv*

Main category: cs.CL

TL;DR: This paper surveys the bidirectional interaction between large language models (LLMs) and multi-armed bandit (MAB) algorithms, highlighting mutual benefits and insights at the component level.


<details>
  <summary>Details</summary>
Motivation: To systematically explore the interaction between LLMs and MAB algorithms, addressing challenges in LLMs and enhancing decision-making in MAB systems.

Method: Analyzing existing LLM-enhanced MAB systems and vice versa, reviewing their design, methodologies, and performance, and indexing relevant literature in a GitHub repository.

Result: The paper identifies key challenges and representative findings in LLM-MAB interactions, offering directions for future research.

Conclusion: LLMs and MAB algorithms can mutually benefit each other, enhancing language processing and adaptive decision-making systems.

Abstract: Large language models (LLMs) have become powerful and widely used systems for language understanding and generation, while multi-armed bandit (MAB) algorithms provide a principled framework for adaptive decision-making under uncertainty. This survey explores the potential at the intersection of these two fields. As we know, it is the first survey to systematically review the bidirectional interaction between large language models and multi-armed bandits at the component level. We highlight the bidirectional benefits: MAB algorithms address critical LLM challenges, spanning from pre-training to retrieval-augmented generation (RAG) and personalization. Conversely, LLMs enhance MAB systems by redefining core components such as arm definition and environment modeling, thereby improving decision-making in sequential tasks. We analyze existing LLM-enhanced bandit systems and bandit-enhanced LLM systems, providing insights into their design, methodologies, and performance. Key challenges and representative findings are identified to help guide future research. An accompanying GitHub repository that indexes relevant literature is available at https://github.com/bucky1119/Awesome-LLM-Bandit-Interaction.

</details>


### [199] [Trustworthy Data-driven Chronological Age Estimation from Panoramic Dental Images](https://arxiv.org/abs/2601.12960)
*Ainhoa Vivel-Couso,Nicolás Vila-Blanco,María J. Carreira,Alberto Bugarín-Diz,Inmaculada Tomás,Jose M. Alonso-Moral*

Main category: cs.CL

TL;DR: The paper presents a system that uses deep learning for dental age estimation with added transparency through textual explanations tailored for clinicians.


<details>
  <summary>Details</summary>
Motivation: To address trust issues in healthcare using deep learning by making age estimation models transparent and understandable for dental professionals.

Method: Developed a system that combines deep learning approaches with a transparent rule-based explanation module, validated through expert questionnaires and a trustworthiness self-assessment.

Result: Achieved high-quality expert ratings (4.77/5 average) for explanations and scored well (4.40/5) on the AI Trustworthiness Assessment List.

Conclusion: This approach enhances model transparency and trustworthiness in healthcare, particularly for dental age estimation through clinician-friendly explanations.

Abstract: Integrating deep learning into healthcare enables personalized care but raises trust issues due to model opacity. To improve transparency, we propose a system for dental age estimation from panoramic images that combines an opaque and a transparent method within a natural language generation (NLG) module. This module produces clinician-friendly textual explanations about the age estimations, designed with dental experts through a rule-based approach. Following the best practices in the field, the quality of the generated explanations was manually validated by dental experts using a questionnaire. The results showed a strong performance, since the experts rated 4.77+/-0.12 (out of 5) on average across the five dimensions considered. We also performed a trustworthy self-assessment procedure following the ALTAI checklist, in which it scored 4.40+/-0.27 (out of 5) across seven dimensions of the AI Trustworthiness Assessment List.

</details>


### [200] [Pardon? Evaluating Conversational Repair in Large Audio-Language Models](https://arxiv.org/abs/2601.12973)
*Shuanghong Huang,Jinlei Xu,Youchao Zhou,Yanghao Zhou,Xuan Zhao,Chong Feng,Wenxuan Zhang*

Main category: cs.CL

TL;DR: The paper evaluates Large Audio-Language Models (LALMs) for spoken question answering, introducing a repair-aware setting that distinguishes answerability and proposes a new metric (EAR).


<details>
  <summary>Details</summary>
Motivation: Current evaluations focus mostly on answer accuracy and robustness, neglecting the real-world issue of unanswerable inputs.

Method: Introduced a semantic-acoustic masking protocol to distinguish answerable and unanswerable inputs, and proposed the Evaluability Awareness and Repair (EAR) score to assess models.

Result: Findings show most models excel in answerable settings but fail in recognizing unanswerable inputs and initiating conversational repair.

Conclusion: Current accuracy-focused evaluations overlook critical reliability aspects; assessments should incorporate repairs for unanswerable inputs.

Abstract: Large Audio-Language Models (LALMs) have demonstrated strong performance in spoken question answering (QA), with existing evaluations primarily focusing on answer accuracy and robustness to acoustic perturbations. However, such evaluations implicitly assume that spoken inputs remain semantically answerable, an assumption that often fails in real-world interaction when essential information is missing. In this work, we introduce a repair-aware evaluation setting that explicitly distinguishes between answerable and unanswerable audio inputs. We define answerability as a property of the input itself and construct paired evaluation conditions using a semantic-acoustic masking protocol. Based on this setting, we propose the Evaluability Awareness and Repair (EAR) score, a non-compensatory metric that jointly evaluates task competence under answerable conditions and repair behavior under unanswerable conditions. Experiments on two spoken QA benchmarks across diverse LALMs reveal a consistent gap between answer accuracy and conversational reliability: while many models perform well when inputs are answerable, most fail to recognize semantic unanswerability and initiate appropriate conversational repair. These findings expose a limitation of prevailing accuracy-centric evaluation practices and motivate reliability assessments that treat unanswerable inputs as cues for repair and continued interaction.

</details>


### [201] [Bridging the Knowledge-Action Gap by Evaluating LLMs in Dynamic Dental Clinical Scenarios](https://arxiv.org/abs/2601.12974)
*Hongyang Ma,Tiantian Gu,Huaiyuan Sun,Huilin Zhu,Yongxin Wang,Jie Li,Wubin Sun,Zeliang Lian,Yinghong Zhou,Yi Gao,Shirui Wang,Zhihui Tang*

Main category: cs.CL

TL;DR: This paper introduces the SCMPE benchmark to evaluate LLMs transitioning from static tasks to dynamic clinical roles in dentistry. It identifies challenges in active information gathering and dynamic state tracking, highlighting risks in decision-making.


<details>
  <summary>Details</summary>
Motivation: Dental clinical practice requires AI systems capable of reliable advice and patient-focused decisions. Existing evaluations focus on static accuracy, not dynamic behavior crucial for autonomous clinical roles.

Method: The SCMPE benchmark assesses LLM performance across static objective tasks and dynamic multi-turn interactions. The study evaluates “Guideline Adherence” vs “Decision Quality” and tests RAG's effects.

Result: LLMs excel at static tasks yet struggle in dynamic dialogues, indicating a reasoning gap. RAG reduces hallucinations in static tasks but shows limitations in improving dynamic workflows.

Conclusion: To achieve safe, autonomous clinical AI systems, external domain-specific knowledge needs adaptive pre-training beyond retrieval augmentation, clarifying capability boundaries in dental applications.

Abstract: The transition of Large Language Models (LLMs) from passive knowledge retrievers to autonomous clinical agents demands a shift in evaluation-from static accuracy to dynamic behavioral reliability. To explore this boundary in dentistry, a domain where high-quality AI advice uniquely empowers patient-participatory decision-making, we present the Standardized Clinical Management & Performance Evaluation (SCMPE) benchmark, which comprehensively assesses performance from knowledge-oriented evaluations (static objective tasks) to workflow-based simulations (multi-turn simulated patient interactions). Our analysis reveals that while models demonstrate high proficiency in static objective tasks, their performance precipitates in dynamic clinical dialogues, identifying that the primary bottleneck lies not in knowledge retention, but in the critical challenges of active information gathering and dynamic state tracking. Mapping "Guideline Adherence" versus "Decision Quality" reveals a prevalent "High Efficacy, Low Safety" risk in general models. Furthermore, we quantify the impact of Retrieval-Augmented Generation (RAG). While RAG mitigates hallucinations in static tasks, its efficacy in dynamic workflows is limited and heterogeneous, sometimes causing degradation. This underscores that external knowledge alone cannot bridge the reasoning gap without domain-adaptive pre-training. This study empirically charts the capability boundaries of dental LLMs, providing a roadmap for bridging the gap between standardized knowledge and safe, autonomous clinical practice.

</details>


### [202] [The Bitter Lesson of Diffusion Language Models for Agentic Workflows: A Comprehensive Reality Check](https://arxiv.org/abs/2601.12979)
*Qingyu Lu,Liang Ding,Kanjian Zhang,Jinxia Zhang,Dacheng Tao*

Main category: cs.CL

TL;DR: This paper evaluates the effectiveness of Diffusion-based Large Language Models (dLLMs) for real-time agentic tasks, finding significant limitations in their performance and proposing areas for improvement.


<details>
  <summary>Details</summary>
Motivation: The study aims to assess whether the purported latency and efficiency benefits of dLLMs translate to effective performance in real-time agentic behaviors, particularly in demanding tasks like long-horizon planning and precise formatting.

Method: The researchers evaluated dLLMs such as LLaDA and Dream across two frameworks (Agentboard and BFCL) and introduced a multi-agent evaluation system, DiffuAgent, to test dLLMs under both Embodied and Tool-Calling paradigms.

Result: Findings reveal that dLLMs struggle with agentic tasks, including managing temporal feedback in Embodied settings and maintaining symbolic precision in Tool-Calling contexts, undermining their effectiveness as agentic backbones.

Conclusion: While dLLMs show promise in non-causal tasks like memory summarization and tool selection, they need stronger causal reasoning and precise denoising mechanisms to become reliable for agentic applications.

Abstract: The pursuit of real-time agentic interaction has driven interest in Diffusion-based Large Language Models (dLLMs) as alternatives to auto-regressive backbones, promising to break the sequential latency bottleneck. However, does such efficiency gains translate into effective agentic behavior? In this work, we present a comprehensive evaluation of dLLMs (e.g., LLaDA, Dream) across two distinct agentic paradigms: Embodied Agents (requiring long-horizon planning) and Tool-Calling Agents (requiring precise formatting). Contrary to the efficiency hype, our results on Agentboard and BFCL reveal a "bitter lesson": current dLLMs fail to serve as reliable agentic backbones, frequently leading to systematically failure. (1) In Embodied settings, dLLMs suffer repeated attempts, failing to branch under temporal feedback. (2) In Tool-Calling settings, dLLMs fail to maintain symbolic precision (e.g. strict JSON schemas) under diffusion noise. To assess the potential of dLLMs in agentic workflows, we introduce DiffuAgent, a multi-agent evaluation framework that integrates dLLMs as plug-and-play cognitive cores. Our analysis shows that dLLMs are effective in non-causal roles (e.g., memory summarization and tool selection) but require the incorporation of causal, precise, and logically grounded reasoning mechanisms into the denoising process to be viable for agentic tasks.

</details>


### [203] [ChartAttack: Testing the Vulnerability of LLMs to Malicious Prompting in Chart Generation](https://arxiv.org/abs/2601.12983)
*Jesus-German Ortiz-Barajas,Jonathan Tonglet,Vivek Gupta,Iryna Gurevych*

Main category: cs.CL

TL;DR: The paper introduces ChartAttack, a framework highlighting misuse risks of MLLMs in generating misleading charts, along with AttackViz dataset to measure impact of these misleaders.


<details>
  <summary>Details</summary>
Motivation: To address the emerging misuse risks of MLLMs in creating misleading charts and emphasize the need for robust evaluation and security measures in chart generation systems.

Method: ChartAttack manipulates chart designs to inject misleading elements and introduces AttackViz, a labeled dataset to evaluate MLLMs' susceptibility to misleading charts.

Result: ChartAttack significantly reduces MLLMs' QA accuracy by 19.6 points in-domain and 14.9 points cross-domain. Human study shows a 20.2 point accuracy drop when exposed to misleading charts.

Conclusion: MLLM-based chart generation systems need improved robustness and security frameworks to prevent misuse. The study underlines critical vulnerabilities and provides resources for further research.

Abstract: Multimodal large language models (MLLMs) are increasingly used to automate chart generation from data tables, enabling efficient data analysis and reporting but also introducing new misuse risks. In this work, we introduce ChartAttack, a novel framework for evaluating how MLLMs can be misused to generate misleading charts at scale. ChartAttack injects misleaders into chart designs, aiming to induce incorrect interpretations of the underlying data. Furthermore, we create AttackViz, a chart question-answering (QA) dataset where each (chart specification, QA) pair is labeled with effective misleaders and their induced incorrect answers. Experiments in in-domain and cross-domain settings show that ChartAttack significantly degrades the QA performance of MLLM readers, reducing accuracy by an average of 19.6 points and 14.9 points, respectively. A human study further shows an average 20.2 point drop in accuracy for participants exposed to misleading charts generated by ChartAttack. Our findings highlight an urgent need for robustness and security considerations in the design, evaluation, and deployment of MLLM-based chart generation systems. We make our code and data publicly available.

</details>


### [204] [Graph Reasoning Paradigm: Structured and Symbolic Reasoning with Topology-Aware Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2601.12995)
*Runxuan Liu,Xianhao Ou,Xinyan Ma,Jiyuan Wang,Jiafeng Liang,Jiaqi Li,Tao He,Zheng Chu,Rongchuan Mu,Zekun Wang,Baoxin Wang,Dayong Wu,Ming Liu,Shijin Wang,Guoping Hu,Bing Qin*

Main category: cs.CL

TL;DR: The paper introduces the Graph Reasoning Paradigm (GRP) and a novel training method, PASC-GRPO, to improve reasoning in LLMs while addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for reasoning in LLMs using RLVR suffer from issues like reward hacking, coarse-grained supervision, inefficiency, high costs, and poor generalization due to unstructured plain-text reasoning.

Method: The authors propose GRP, which employs graph-structured representations with cognitive labels, and PASC-GRPO, a process-aware training method leveraging structured evaluation, stratified clipping, and outcome rewards to optimize reasoning training.

Result: Experiments demonstrate notable performance improvements in mathematical reasoning and code generation tasks.

Conclusion: The proposed GRP and PASC-GRPO offer structured and symbolic approaches to significantly enhance LLM reasoning while overcoming traditional limitations. Models and code will be made publicly available.

Abstract: Long Chain-of-Thought (LCoT), achieved by Reinforcement Learning with Verifiable Rewards (RLVR), has proven effective in enhancing the reasoning capabilities of Large Language Models (LLMs). However, reasoning in current LLMs is primarily generated as plain text, where performing semantic evaluation on such unstructured data creates a computational bottleneck during training. Despite RLVR-based optimization, existing methods still suffer from coarse-grained supervision, reward hacking, high training costs, and poor generalization. To address these issues, we propose the Graph Reasoning Paradigm (GRP), which realizes structured and symbolic reasoning, implemented via graph-structured representations with step-level cognitive labels. Building upon GRP, we further design Process-Aware Stratified Clipping Group Relative Policy Optimization (PASC-GRPO), which leverages structured evaluation to replace semantic evaluation, achieves process-aware verification through graph-structured outcome rewards, and mitigates reward hacking via stratified clipping advantage estimation. Experiments demonstrate significant improvements across mathematical reasoning and code generation tasks. Data, models, and code will be released later.

</details>


### [205] [Bi-Attention HateXplain : Taking into account the sequential aspect of data during explainability in a multi-task context](https://arxiv.org/abs/2601.13018)
*Ghislain Dorian Tchuente Mondjo*

Main category: cs.CL

TL;DR: The paper introduces BiAtt-BiRNN-HateXplain, a model designed to improve hate speech detection while reducing biases and enhancing explainability.


<details>
  <summary>Details</summary>
Motivation: Hate speech detection models often lack reliability, consistency in explanations, and face challenges with unintentional bias, which necessitates a better approach.

Method: The authors propose a bidirectional attention BiRNN model (BiAtt-BiRNN-HateXplain) that simultaneously enhances classification and explainability using multi-task learning and attention mechanisms.

Result: The model demonstrates superior detection performance, improved explainability, and reduced unintentional bias when tested on the HateXplain dataset.

Conclusion: BiAtt-BiRNN-HateXplain offers a more reliable and explainable alternative to complex large language models for hate speech detection, addressing bias and interpretability effectively.

Abstract: Technological advances in the Internet and online social networks have brought many benefits to humanity. At the same time, this growth has led to an increase in hate speech, the main global threat. To improve the reliability of black-box models used for hate speech detection, post-hoc approaches such as LIME, SHAP, and LRP provide the explanation after training the classification model. In contrast, multi-task approaches based on the HateXplain benchmark learn to explain and classify simultaneously. However, results from HateXplain-based algorithms show that predicted attention varies considerably when it should be constant. This attention variability can lead to inconsistent interpretations, instability of predictions, and learning difficulties. To solve this problem, we propose the BiAtt-BiRNN-HateXplain (Bidirectional Attention BiRNN HateXplain) model which is easier to explain compared to LLMs which are more complex in view of the need for transparency, and will take into account the sequential aspect of the input data during explainability thanks to a BiRNN layer. Thus, if the explanation is correctly estimated, thanks to multi-task learning (explainability and classification task), the model could classify better and commit fewer unintentional bias errors related to communities. The experimental results on HateXplain data show a clear improvement in detection performance, explainability and a reduction in unintentional bias.

</details>


### [206] [Tears or Cheers? Benchmarking LLMs via Culturally Elicited Distinct Affective Responses](https://arxiv.org/abs/2601.13024)
*Chongyuan Dai,Yaling Shen,Jinpeng Hu,Zihan Gao,Jia Li,Yishun Jiang,Yaxiong Wang,Liu Liu,Zongyuan Ge*

Main category: cs.CL

TL;DR: The paper introduces CEDAR, a benchmark for evaluating culturally elicited emotional responses in Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Current evaluations of cultural alignment in LLMs focus too much on declarative knowledge, neglecting the subjective and emotional variances caused by different cultural perspectives.

Method: The authors propose a multimodal benchmark, CEDAR, created using a pipeline where LLM-generated labels help identify cross-cultural emotional differences. Final human-evaluated annotations create the ground-truth data.

Result: CEDAR contains 10,962 instances in 7 languages and 14 emotion categories. Evaluations of 17 multilingual models showed gaps in cultural alignment capabilities.

Conclusion: Current multilingual models struggle with culturally grounded emotional understanding, highlighting an important area for improvement in AI systems.

Abstract: Culture serves as a fundamental determinant of human affective processing and profoundly shapes how individuals perceive and interpret emotional stimuli. Despite this intrinsic link extant evaluations regarding cultural alignment within Large Language Models primarily prioritize declarative knowledge such as geographical facts or established societal customs. These benchmarks remain insufficient to capture the subjective interpretative variance inherent to diverse sociocultural lenses. To address this limitation, we introduce CEDAR, a multimodal benchmark constructed entirely from scenarios capturing Culturally \underline{\textsc{E}}licited \underline{\textsc{D}}istinct \underline{\textsc{A}}ffective \underline{\textsc{R}}esponses. To construct CEDAR, we implement a novel pipeline that leverages LLM-generated provisional labels to isolate instances yielding cross-cultural emotional distinctions, and subsequently derives reliable ground-truth annotations through rigorous human evaluation. The resulting benchmark comprises 10,962 instances across seven languages and 14 fine-grained emotion categories, with each language including 400 multimodal and 1,166 text-only samples. Comprehensive evaluations of 17 representative multilingual models reveal a dissociation between language consistency and cultural alignment, demonstrating that culturally grounded affective understanding remains a significant challenge for current models.

</details>


### [207] [SASA: Semantic-Aware Contrastive Learning Framework with Separated Attention for Triple Classification](https://arxiv.org/abs/2601.13035)
*Xu Xiaodan,Hu Xiaolin*

Main category: cs.CL

TL;DR: The paper introduces SASA, a framework improving Triple Classification (TC) in Knowledge Graphs by addressing semantic interaction and representation learning challenges, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in traditional TC methods, such as limited semantic interaction and insufficient representation learning, aiming to improve accuracy and utility of TC in Knowledge Graphs.

Method: Proposes a novel SASA framework featuring separated attention mechanism for better contextual encoding and semantic-aware hierarchical contrastive learning for enhanced discriminative representation.

Result: SASA outperforms state-of-the-art methods, improving accuracy by +5.9% on FB15k-237 and +3.4% on YAGO3-10 datasets.

Conclusion: SASA successfully improves TC models by tackling key challenges in semantic interaction and representation learning, achieving superior performance on benchmark datasets.

Abstract: Knowledge Graphs~(KGs) often suffer from unreliable knowledge, which restricts their utility. Triple Classification~(TC) aims to determine the validity of triples from KGs. Recently, text-based methods learn entity and relation representations from natural language descriptions, significantly improving the generalization capabilities of TC models and setting new benchmarks in performance. However, there are still two critical challenges. First, existing methods often ignore the effective semantic interaction among different KG components. Second, most approaches adopt single binary classification training objective, leading to insufficient semantic representation learning. To address these challenges, we propose \textbf{SASA}, a novel framework designed to enhance TC models via separated attention mechanism and semantic-aware contrastive learning~(CL). Specifically, we first propose separated attention mechanism to encode triples into decoupled contextual representations and then fuse them through a more effective interactive way. Then, we introduce semantic-aware hierarchical CL as auxiliary training objective to guide models in improving their discriminative capabilities and achieving sufficient semantic learning, considering both local level and global level CL. Experimental results across two benchmark datasets demonstrate that SASA significantly outperforms state-of-the-art methods. In terms of accuracy, we advance the state-of-the-art by +5.9\% on FB15k-237 and +3.4\% on YAGO3-10.

</details>


### [208] [Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition](https://arxiv.org/abs/2601.13044)
*Warit Sirichotedumrong,Adisai Na-Thalang,Potsawee Manakul,Pittawat Taveekitworachai,Sittipong Sripaisarnmongkol,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: This paper presents Typhoon ASR Real-time, an efficient model for low-latency Thai speech recognition, achieving performance comparable to Whisper Large while being significantly more computationally efficient.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in efficient streaming Thai ASR solutions, as the field is dominated by offline architectures with high latency.

Method: The authors designed a 115M-parameter FastConformer-Transducer model and implemented rigorous text normalization, addressing Thai-specific transcription ambiguities and utilizing a two-stage curriculum learning for dialect adaptation.

Result: The proposed model reduced computational cost by 45x compared to Whisper Large-v3, while maintaining comparable accuracy and addressing transcription challenges.

Conclusion: Typhoon ASR Real-time successfully provides a low-latency, high-efficiency Thai ASR solution, along with the release of a benchmark dataset to standardize evaluation in the field.

Abstract: Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community.

</details>


### [209] [Profiling German Text Simplification with Interpretable Model-Fingerprints](https://arxiv.org/abs/2601.13050)
*Lars Klöser,Mika Beele,Bodo Kraft*

Main category: cs.CL

TL;DR: The paper introduces the Simplification Profiler, a tool to diagnose the behavior of Large Language Models (LLMs) in text simplification through a multidimensional fingerprinting approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of tools for assessing LLMs' text simplification behavior holistically, especially in scenarios with scarce data for diverse target groups.

Method: They developed the Simplification Profiler, which uses aggregated model fingerprints and a meta-evaluation approach employing linear classifiers to analyze the descriptive power of fingerprints for model variations.

Result: The Profiler can accurately classify model configurations with F1 scores of up to 71.9%, outperforming baseline models by over 48 percentage points.

Conclusion: The Simplification Profiler provides a detailed and actionable framework for developers, aiding the creation of adaptive and efficient text simplification systems.

Abstract: While Large Language Models (LLMs) produce highly nuanced text simplifications, developers currently lack tools for a holistic, efficient, and reproducible diagnosis of their behavior. This paper introduces the Simplification Profiler, a diagnostic toolkit that generates a multidimensional, interpretable fingerprint of simplified texts. Multiple aggregated simplifications of a model result in a model's fingerprint. This novel evaluation paradigm is particularly vital for languages, where the data scarcity problem is magnified when creating flexible models for diverse target groups rather than a single, fixed simplification style. We propose that measuring a model's unique behavioral signature is more relevant in this context as an alternative to correlating metrics with human preferences. We operationalize this with a practical meta-evaluation of our fingerprints' descriptive power, which bypasses the need for large, human-rated datasets. This test measures if a simple linear classifier can reliably identify various model configurations by their created simplifications, confirming that our metrics are sensitive to a model's specific characteristics. The Profiler can distinguish high-level behavioral variations between prompting strategies and fine-grained changes from prompt engineering, including few-shot examples. Our complete feature set achieves classification F1-scores up to 71.9 %, improving upon simple baselines by over 48 percentage points. The Simplification Profiler thus offers developers a granular, actionable analysis to build more effective and truly adaptive text simplification systems.

</details>


### [210] [Alexandria: A Multi-Domain Dialectal Arabic Machine Translation Dataset for Culturally Inclusive and Linguistically Diverse LLMs](https://arxiv.org/abs/2601.13099)
*Abdellah El Mekki,Samar M. Magdy,Houdaifa Atou,Ruwa AbuHweidi,Baraah Qawasmeh,Omer Nacar,Thikra Al-hibiri,Razan Saadie,Hamzah Alsayadi,Nadia Ghezaiel Hammouda,Alshima Alkhazimi,Aya Hamod,Al-Yas Al-Ghafri,Wesam El-Sayed,Asila Al sharji,Mohamad Ballout,Anas Belfathi,Karim Ghaddar,Serry Sibaee,Alaa Aoun,Areej Asiri,Lina Abureesh,Ahlam Bashiti,Majdal Yousef,Abdulaziz Hafiz,Yehdih Mohamed,Emira Hamedtou,Brakehe Brahim,Rahaf Alhamouri,Youssef Nafea,Aya El Aatar,Walid Al-Dhabyani,Emhemed Hamed,Sara Shatnawi,Fakhraddin Alwajih,Khalid Elkhidir,Ashwag Alasmari,Abdurrahman Gerrio,Omar Alshahri,AbdelRahim A. Elmadany,Ismail Berrada,Amir Azad Adli Alkathiri,Fadi A Zaraket,Mustafa Jarrar,Yahya Mohamed El Hadj,Hassan Alhuzali,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: The paper introduces Alexandria, a large-scale, human-translated dataset to enhance machine translation (MT) capabilities for Arabic dialects.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the poor generalization of MT systems to Arabic dialectal input, which limits their utility for millions of Arabic speakers.

Method: The authors developed Alexandria, a dataset comprising 107K samples annotated with rich metadata, including city-of-origin and gender-conditioned conversational contexts, to serve as a resource and benchmark for Arabic MT and LLMs.

Result: Alexandria enables the analysis of gender-conditioned linguistic variation and evaluates the performance of Arabic-aware LLMs while revealing persistent challenges in translating diverse Arabic dialects.

Conclusion: Alexandria is a novel resource that supports improved MT and LLM performance in handling Arabic dialects, with fine-grained granularity and representative benchmarks.

Abstract: Arabic is a highly diglossic language where most daily communication occurs in regional dialects rather than Modern Standard Arabic. Despite this, machine translation (MT) systems often generalize poorly to dialectal input, limiting their utility for millions of speakers. We introduce \textbf{Alexandria}, a large-scale, community-driven, human-translated dataset designed to bridge this gap. Alexandria covers 13 Arab countries and 11 high-impact domains, including health, education, and agriculture. Unlike previous resources, Alexandria provides unprecedented granularity by associating contributions with city-of-origin metadata, capturing authentic local varieties beyond coarse regional labels. The dataset consists of multi-turn conversational scenarios annotated with speaker-addressee gender configurations, enabling the study of gender-conditioned variation in dialectal use. Comprising 107K total samples, Alexandria serves as both a training resource and a rigorous benchmark for evaluating MT and Large Language Models (LLMs). Our automatic and human evaluation of Arabic-aware LLMs benchmarks current capabilities in translating across diverse Arabic dialects and sub-dialects, while exposing significant persistent challenges.

</details>


### [211] [Leveraging Lora Fine-Tuning and Knowledge Bases for Construction Identification](https://arxiv.org/abs/2601.13105)
*Liu Kaipeng,Wu Ling*

Main category: cs.CL

TL;DR: The paper combines LoRA fine-tuning and a RAG framework to identify English ditransitive constructions, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To improve automatic identification of English ditransitive constructions by addressing the limitations of existing models, particularly their reliance on surface-level pattern matching.

Method: The study employs LoRA-based fine-tuning on the Qwen3-8B model within a Retrieval-Augmented Generation (RAG) framework for binary classification using annotated data from the British National Corpus.

Result: LoRA-fine-tuned Qwen3-8B outperforms the Qwen3-MAX and theory-driven RAG systems, showing a shift from pattern-based to semantically grounded understanding.

Conclusion: Fine-tuning with LoRA improves semantic understanding in English ditransitive construction identification, demonstrating the superiority of this hybrid system over native or theory-only frameworks.

Abstract: This study investigates the automatic identification of the English ditransitive construction by integrating LoRA-based fine-tuning of a large language model with a Retrieval-Augmented Generation (RAG) framework.A binary classification task was conducted on annotated data from the British National Corpus. Results demonstrate that a LoRA-fine-tuned Qwen3-8B model significantly outperformed both a native Qwen3-MAX model and a theory-only RAG system. Detailed error analysis reveals that fine-tuning shifts the model's judgment from a surface-form pattern matching towards a more semantically grounded understanding based.

</details>


### [212] [CORE-T: COherent REtrieval of Tables for Text-to-SQL](https://arxiv.org/abs/2601.13111)
*Hassan Soliman,Vivek Gupta,Dan Roth,Iryna Gurevych*

Main category: cs.CL

TL;DR: The paper introduces CORE-T, a framework that improves table selection for text-to-SQL workflows by enhancing compatibility and coherence using LLM-generated metadata and a lightweight cache mechanism.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of retrieving relevant tables in text-to-SQL workflows involving large, heterogeneous collections without database identifiers.

Method: CORE-T enriches tables with metadata using LLMs, pre-computes a compatibility cache, and selects appropriate tables using dense retrieval and LLM-supported adjustments during inference.

Result: The framework enhances table-selection F1 scores, reduces the number of retrieved tables, improves multi-table execution accuracy, and consumes fewer tokens compared to LLM-intensive baselines.

Conclusion: CORE-T effectively improves scalability, efficiency, and precision in table selection, benefiting realistic text-to-SQL scenarios.

Abstract: Realistic text-to-SQL workflows often require joining multiple tables. As a result, accurately retrieving the relevant set of tables becomes a key bottleneck for end-to-end performance. We study an open-book setting where queries must be answered over large, heterogeneous table collections pooled from many sources, without clean scoping signals such as database identifiers. Here, dense retrieval (DR) achieves high recall but returns many distractors, while join-aware alternatives often rely on extra assumptions and/or incur high inference overhead. We propose CORE-T, a scalable, training-free framework that enriches tables with LLM-generated purpose metadata and pre-computes a lightweight table-compatibility cache. At inference time, DR returns top-K candidates; a single LLM call selects a coherent, joinable subset, and a simple additive adjustment step restores strongly compatible tables. Across Bird, Spider, and MMQA, CORE-T improves table-selection F1 by up to 22.7 points while retrieving up to 42% fewer tables, improving multi-table execution accuracy by up to 5.0 points on Bird and 6.9 points on MMQA, and using 4-5x fewer tokens than LLM-intensive baselines.

</details>


### [213] [Agentic Conversational Search with Contextualized Reasoning via Reinforcement Learning](https://arxiv.org/abs/2601.13115)
*Fengran Mo,Yifan Gao,Sha Li,Hansi Zeng,Xin Liu,Zhaoxuan Tan,Xian Li,Jianshu Chen,Dakuo Wang,Meng Jiang*

Main category: cs.CL

TL;DR: This paper introduces a conversational agent optimized for multi-turn dialogues by integrating search and reasoning, showing superior performance over baseline methods.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of current static methods and single-turn optimization processes for handling context-dependent user intents in multi-turn dialogues.

Method: Design a conversational agent interleaving search and reasoning across turns, trained with reinforcement learning on dynamic rewards targeted at evolving user goals.

Result: Demonstrated effectiveness by outperforming several baseline methods on four conversational benchmarks.

Conclusion: The proposed approach improves multi-turn dialogue handling and aligns user goals more effectively through integrated search, reasoning, and dynamic optimization training.

Abstract: Large Language Models (LLMs) have become a popular interface for human-AI interaction, supporting information seeking and task assistance through natural, multi-turn dialogue. To respond to users within multi-turn dialogues, the context-dependent user intent evolves across interactions, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Existing studies usually follow static rewrite, retrieve, and generate pipelines, which optimize different procedures separately and overlook the mixed-initiative action optimization simultaneously. Although the recent developments in deep search agents demonstrate the effectiveness in jointly optimizing retrieval and generation via reasoning, these approaches focus on single-turn scenarios, which might lack the ability to handle multi-turn interactions. We introduce a conversational agent that interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through reinforcement learning (RL) training with tailored rewards towards evolving user goals. The experimental results across four widely used conversational benchmarks demonstrate the effectiveness of our methods by surpassing several existing strong baselines.

</details>


### [214] [Adversarial Alignment: Ensuring Value Consistency in Large Language Models for Sensitive Domains](https://arxiv.org/abs/2601.13137)
*Yuan Gao,Zhigang Liu,Xinyu Yao,Bo Chen,Xiaobing Zhao*

Main category: cs.CL

TL;DR: The paper addresses bias and value inconsistency in Large Language Models (LLMs) by proposing an adversarial alignment framework focusing on sensitive domains.


<details>
  <summary>Details</summary>
Motivation: To tackle the emerging issues of bias and value inconsistency in LLMs, particularly concerning sensitive topics like race, society, and politics.

Method: The authors designed an adversarial alignment framework based on continued pre-training, instruction fine-tuning, and adversarial training using an Attacker, Actor, and Critic roles.

Result: The resulting model, VC-LLM, demonstrated better value consistency in sensitive domains and outperformed mainstream models in bilingual tests.

Conclusion: The proposed framework proves effective in improving value consistency of LLMs in sensitive domains, according to experimental results.

Abstract: With the wide application of large language models (LLMs), the problems of bias and value inconsistency in sensitive domains have gradually emerged, especially in terms of race, society and politics. In this paper, we propose an adversarial alignment framework, which enhances the value consistency of the model in sensitive domains through continued pre-training, instruction fine-tuning and adversarial training. In adversarial training, we use the Attacker to generate controversial queries, the Actor to generate responses with value consistency, and the Critic to filter and ensure response quality. Furthermore, we train a Value-Consistent Large Language Model, VC-LLM, for sensitive domains, and construct a bilingual evaluation dataset in Chinese and English. The experimental results show that VC-LLM performs better than the existing mainstream models in both Chinese and English tests, verifying the effectiveness of the method. Warning: This paper contains examples of LLMs that are offensive or harmful in nature.

</details>


### [215] [Probe and Skip: Self-Predictive Token Skipping for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2601.13155)
*Zimeng Wu,Donghao Wang,Chaozhe Jin,Jiaxin Chen,Yunhong Wang*

Main category: cs.CL

TL;DR: The paper introduces SPTS, a framework to improve efficiency for long-context LLM inference by selectively skipping irrelevant tokens and proposes advanced strategies for better speed-accuracy trade-offs.


<details>
  <summary>Details</summary>
Motivation: Current token-oriented methods for long-context LLM inference face limitations in acceleration and accuracy trade-offs due to outdated signals and redundancy interference.

Method: SPTS employs training-free strategies including Partial Attention Probing for multi-head attention, Low-rank Transformation Probing for feed-forward networks, and Multi-Stage Delayed Pruning to selectively skip redundant tokens.

Result: SPTS achieves up to 2.46× speedup in prefilling and 2.29× in end-to-end generation while maintaining state-of-the-art performance.

Conclusion: The proposed methods significantly enhance efficiency in LLM inference without compromising accuracy, addressing key challenges faced by current token-oriented approaches.

Abstract: Long-context inference enhances the reasoning capability of Large Language Models (LLMs) while incurring significant computational overhead. Token-oriented methods, such as pruning and skipping, have shown promise in reducing inference latency, but still suffer from inherently limited acceleration potential, outdated proxy signals, and redundancy interference, thus yielding suboptimal speed-accuracy trade-offs. To address these challenges, we propose SPTS (Self-Predictive Token Skipping), a training-free framework for efficient long-context LLM inference. Specifically, motivated by the thought of probing the influence of targeted skipping layers, we design two component-specific strategies for selective token skipping: Partial Attention Probing (PAP) for multi-head attention, which selects informative tokens by performing partial forward attention computation, and Low-rank Transformation Probing (LTP) for feed forward network, which constructs a low-rank proxy network to predict token transformations. Furthermore, a Multi-Stage Delayed Pruning (MSDP) strategy reallocates the skipping budget and progressively prunes redundant tokens across layers. Extensive experiments demonstrate the effectiveness of our method, achieving up to 2.46$\times$ and 2.29$\times$ speedups for prefilling and end-to-end generation, respectively, while maintaining state-of-the-art model performance. The source code will be publicly available upon paper acceptance.

</details>


### [216] [Medical Triage as Pairwise Ranking: A Benchmark for Urgency in Patient Portal Messages](https://arxiv.org/abs/2601.13178)
*Joseph Gatto,Parker Seegmiller,Timothy Burdick,Philip Resnik,Roshnik Rahat,Sarah DeLozier,Sarah M. Preum*

Main category: cs.CL

TL;DR: This paper presents PMR-Bench, a benchmark dataset for medical triage based on outpatient portal messages, and introduces models (UrgentSFT and UrgentReward) that classify urgency in a pairwise manner.


<details>
  <summary>Details</summary>
Motivation: To address the need for effective and scalable medical triage by utilizing machine learning models to assess the urgency of patient messages in a healthcare context.

Method: The authors created PMR-Bench with 1569 unique messages and over 2000 pairwise urgency test cases. They trained two models, UrgentReward using the Bradley-Terry method and UrgentSFT through next-token prediction, for urgency classification.

Result: UrgentSFT exhibited the best performance on PMR-Bench, while UrgentReward proved advantageous in low-resource scenarios, offering significant improvements in inbox sorting metrics over standard models.

Conclusion: This study demonstrates the utility of LLMs in medical triage, providing benchmarks, scalable data generation, and models that enhance the prioritization of patient messages efficiently.

Abstract: Medical triage is the task of allocating medical resources and prioritizing patients based on medical need. This paper introduces the first large-scale public dataset for studying medical triage in the context of asynchronous outpatient portal messages. Our novel task formulation views patient message triage as a pairwise inference problem, where we train LLMs to choose `"which message is more medically urgent" in a head-to-head tournament-style re-sort of a physician's inbox. Our novel benchmark PMR-Bench contains 1569 unique messages and 2,000+ high-quality test pairs for pairwise medical urgency assessment alongside a scalable training data generation pipeline. PMR-Bench includes samples that contain both unstructured patient-written messages alongside real electronic health record (EHR) data, emulating a real-world medical triage scenario.
  We develop a novel automated data annotation strategy to provide LLMs with in-domain guidance on this task. The resulting data is used to train two model classes, UrgentReward and UrgentSFT, leveraging Bradley-Terry and next token prediction objective, respectively to perform pairwise urgency classification. We find that UrgentSFT achieves top performance on PMR-Bench, with UrgentReward showing distinct advantages in low-resource settings. For example, UrgentSFT-8B and UrgentReward-8B provide a 15- and 16-point boost, respectively, on inbox sorting metrics over off-the-shelf 8B models. Paper resources can be found at https://tinyurl.com/Patient-Message-Triage

</details>


### [217] [OpenExempt: A Diagnostic Benchmark for Legal Reasoning and a Framework for Creating Custom Benchmarks on Demand](https://arxiv.org/abs/2601.13183)
*Sergio Servantez,Sarah B. Lawsky,Rajiv Jain,Daniel W. Linna,Kristian Hammond*

Main category: cs.CL

TL;DR: The paper introduces OpenExempt, a diagnostic framework and benchmark to evaluate legal reasoning in language models, addressing the limitations of static question-answer benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current reasoning benchmarks fail to accurately evaluate complex behaviors in law, a rule-bound domain, due to their static and costly nature.

Method: The authors created OpenExempt, which utilizes symbolic representations of U.S. Bankruptcy Code statutes to dynamically generate natural language reasoning tasks for diagnostic evaluations. The system includes a benchmark with 9,765 samples.

Result: Experiments on 13 language models reveal poor performance in complex tasks, especially under obfuscating statements and longer reasoning paths.

Conclusion: The OpenExempt framework provides an advanced and fine-grained approach to understanding and improving reasoning performance in complex domains like legal reasoning.

Abstract: Reasoning benchmarks have played a crucial role in the progress of language models. Yet rigorous evaluation remains a significant challenge as static question-answer pairs provide only a snapshot of performance, compressing complex behavior into a single accuracy metric. This limitation is especially true in complex, rule-bound domains such as law, where existing benchmarks are costly to build and ill suited for isolating specific failure modes. To address this, we introduce OpenExempt, a framework and benchmark for diagnostic evaluation of legal reasoning. The OpenExempt Framework uses expert-crafted symbolic representations of U.S. Bankruptcy Code statutes to dynamically generate a large space of natural language reasoning tasks and their machine-computable solutions on demand. This gives users fine-grained control over task complexity and scope, allowing individual reasoning skills to be probed in isolation. Using this system, we construct the OpenExempt Benchmark, a diagnostic benchmark for legal reasoning with 9,765 samples across nine evaluation suites designed to carefully probe model capabilities. Experiments on 13 diverse language models reveal sharp performance cliffs that emerge only under longer reasoning paths and in the presence of obfuscating statements. We release the framework and benchmark publicly to support research aimed at understanding and improving the next generation of reasoning systems.

</details>


### [218] [Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision](https://arxiv.org/abs/2601.13217)
*Bingsen Chen,Boyan Li,Ping Nie,Yuyu Zhang,Xi Ye,Chen Zhao*

Main category: cs.CL

TL;DR: The paper introduces 'Mr Dre', a new evaluation suite for testing deep research agents (DRAs) in multi-turn report revision scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for DRAs treat report generation as a single-step task. This approach disregards the iterative nature of how human researchers revise reports, making it necessary to evaluate DRAs in a more nuanced, interactive manner.

Method: The paper proposes Mr Dre, which includes a unified long-form report evaluation protocol encompassing comprehensiveness, factuality, and presentation, along with a human-verified feedback simulation pipeline for assessing multi-turn report revisions.

Result: Experiments with five DRAs reveal that while agents can address most provided feedback, they often regress on previously covered content and citation quality. Furthermore, multi-turn revisions show significant room for improvement, with agents failing to consistently preserve earlier edits and introducing unintended disruptions.

Conclusion: The study highlights a critical limitation in current DRAs' ability to handle iterative report revisions and demonstrates that simple inference-time fixes are insufficient, necessitating advancements in the design of DRAs for multi-turn revision capabilities.

Abstract: Existing benchmarks for Deep Research Agents (DRAs) treat report generation as a single-shot writing task, which fundamentally diverges from how human researchers iteratively draft and revise reports via self-reflection or peer feedback. Whether DRAs can reliably revise reports with user feedback remains unexplored. We introduce Mr Dre, an evaluation suite that establishes multi-turn report revision as a new evaluation axis for DRAs. Mr Dre consists of (1) a unified long-form report evaluation protocol spanning comprehensiveness, factuality, and presentation, and (2) a human-verified feedback simulation pipeline for multi-turn revision. Our analysis of five diverse DRAs reveals a critical limitation: while agents can address most user feedback, they also regress on 16-27% of previously covered content and citation quality. Over multiple revision turns, even the best-performing agents leave significant headroom, as they continue to disrupt content outside the feedback's scope and fail to preserve earlier edits. We further show that these issues are not easily resolvable through inference-time fixes such as prompt engineering and a dedicated sub-agent for report revision.

</details>


### [219] [Autoregressive Models Rival Diffusion Models at ANY-ORDER Generation](https://arxiv.org/abs/2601.13228)
*Tianqi Du,Lizhe Fang,Weijie Yang,Chenheng Zhang,Zeming Wei,Yifei Wang,Yisen Wang*

Main category: cs.CL

TL;DR: The paper proposes Any-order Any-subset Autoregressive (A3) modeling to combine the benefits of autoregressive (AR) and diffusion language models, improving flexibility and performance in sequence generation while preserving probabilistic rigor.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of diffusion language models, which struggle with modeling depth and sample stability compared to autoregressive (AR) models, and to combine their flexibility with AR's robust dependency capabilities.

Method: The A3 framework reformulates diffusion-style training using a multi-group prediction process. It extends AR factorization to any token grouping and generation order using a two-stream attention architecture and a progressive adaptation strategy applied to pretrained AR models.

Result: A3 outperformed diffusion-based models in question answering, commonsense reasoning, and story infilling tasks while retaining flexible and efficient decoding capabilities.

Conclusion: The A3 framework serves as a unified and innovative approach for flexible and effective language modeling, combining the strengths of both diffusion and autoregressive systems.

Abstract: Diffusion language models enable any-order generation and bidirectional conditioning, offering appealing flexibility for tasks such as infilling, rewriting, and self-correction. However, their formulation-predicting one part of a sequence from another within a single-step dependency-limits modeling depth and often yields lower sample quality and stability than autoregressive (AR) models. To address this, we revisit autoregressive modeling as a foundation and reformulate diffusion-style training into a structured multi-group prediction process. We propose Any-order Any-subset Autoregressive modeling (A3), a generalized framework that extends the standard AR factorization to arbitrary token groups and generation orders. A3 preserves the probabilistic rigor and multi-layer dependency modeling of AR while inheriting diffusion models' flexibility for parallel and bidirectional generation. We implement A3 through a two-stream attention architecture and a progressive adaptation strategy that transitions pretrained AR models toward any-order prediction. Experiments on question answering, commonsense reasoning, and story infilling demonstrate that A3 outperforms diffusion-based models while maintaining flexible decoding. This work offers a unified approach for a flexible, efficient, and novel language modeling paradigm.

</details>


### [220] [Aligning Agentic World Models via Knowledgeable Experience Learning](https://arxiv.org/abs/2601.13247)
*Baochang Ren,Yunzhi Yao,Rui Sun,Shuofei Qiao,Ningyu Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: Current LLMs fail to integrate physical laws into their understanding. A novel framework, WorldMind, bridges this gap by creating a symbolic knowledge repository based on environmental feedback.


<details>
  <summary>Details</summary>
Motivation: The issue with LLMs lies in their inability to align semantic knowledge with the immutable laws of the physical world, leading to physically unfeasible plans.

Method: WorldMind autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback, focusing on physical feasibility and task-optimality through separate experience-based components.

Result: WorldMind outperforms existing approaches in both cross-model and cross-environment tasks, as demonstrated by experiments on EB-ALFRED and EB-Habitat.

Conclusion: WorldMind offers an efficient and adaptive strategy to enhance LLMs' ability to simulate realistic physical dynamics without requiring costly retraining.

Abstract: Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.

</details>


### [221] [Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph](https://arxiv.org/abs/2601.13251)
*Ebubekir Tosun,Mehmet Emin Buldur,Özay Ezerceli,Mahmoud ElHussieni*

Main category: cs.CL

TL;DR: The paper addresses the challenge of distinguishing synonyms from antonyms using a semantic clustering system that processed 15 million lexical items, delivering 2.9 million high-precision clusters.


<details>
  <summary>Details</summary>
Motivation: Existing neural embeddings struggle to distinguish synonyms from antonyms, often grouping opposites together due to increasing similarity thresholds.

Method: The authors introduced a labeled dataset, developed a three-way semantic relation discriminator, and proposed a novel clustering algorithm that uses topology-aware expansion-pruning to ensure semantic accuracy.

Result: They managed to avoid semantic drift and build 2.9 million coherent semantic clusters across diverse languages, aiding semantic search and retrieval.

Conclusion: The developed system significantly advances disambiguation capabilities and provides an invaluable resource for morphologically rich and low-resource languages.

Abstract: Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.

</details>


### [222] [A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus](https://arxiv.org/abs/2601.13253)
*Ebubekir Tosun,Mehmet Emin Buldur,Özay Ezerceli,Mahmoud ElHussieni*

Main category: cs.CL

TL;DR: The paper introduces a scalable methodology to generate Turkish semantic relationship datasets with 843,000 semantic pairs, significantly improving Turkish NLP resources.


<details>
  <summary>Details</summary>
Motivation: To address limited semantic relationship datasets in low-resource languages like Turkish.

Method: The approach integrates FastText embeddings, Agglomerative Clustering, Gemini 2.5-Flash semantic classification, and curated dictionaries to create the dataset.

Result: Generated a dataset with 843,000 Turkish semantic pairs involving synonyms, antonyms, and co-hyponyms; validated with impressive results in retrieval accuracy and F1-macro score.

Conclusion: The methodology effectively enhances Turkish NLP datasets, is cost-efficient, widely scalable, and supports low-resource languages; dataset and models are publicly released.

Abstract: We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models.

</details>


### [223] [Stop Taking Tokenizers for Granted: They Are Core Design Decisions in Large Language Models](https://arxiv.org/abs/2601.13260)
*Sawsan Alqahtani,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Tasnim Mohiuddin,M Saiful Bari*

Main category: cs.CL

TL;DR: This paper highlights the importance of tokenization as a key modeling decision rather than just preprocessing, proposing a context-aware co-design of tokenization and model development to improve fairness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Tokenization methods like Byte Pair Encoding (BPE) often misalign with linguistic structures, introduce biases, and inefficiently use capacity. The paper seeks to address these issues by rethinking tokenization as a fundamental aspect of language model design.

Method: The paper proposes a context-aware framework where tokenization and model development are co-designed, considering linguistic, domain, and deployment factors. It also advocates for standardized evaluation and transparent reporting to ensure accountability and comparability in tokenization decisions.

Result: The framework emphasizes the potential for creating tokenization methods that align better with linguistic structures, reduce bias, and utilize capacity more efficiently across languages and domains.

Conclusion: Rethinking tokenization as a core design problem, rather than an afterthought, allows for the development of fairer, more efficient, and versatile language technologies.

Abstract: Tokenization underlies every large language model, yet it remains an under-theorized and inconsistently designed component. Common subword approaches such as Byte Pair Encoding (BPE) offer scalability but often misalign with linguistic structure, amplify bias, and waste capacity across languages and domains. This paper reframes tokenization as a core modeling decision rather than a preprocessing step. We argue for a context-aware framework that integrates tokenizer and model co-design, guided by linguistic, domain, and deployment considerations. Standardized evaluation and transparent reporting are essential to make tokenization choices accountable and comparable. Treating tokenization as a core design problem, not a technical afterthought, can yield language technologies that are fairer, more efficient, and more adaptable.

</details>


### [224] [Unlearning in LLMs: Methods, Evaluation, and Open Challenges](https://arxiv.org/abs/2601.13264)
*Tyler Lizzo,Larry Heck*

Main category: cs.CL

TL;DR: This paper surveys methods for machine unlearning in large language models (LLMs), categorizes them, reviews evaluation strategies, and discusses challenges and open problems.


<details>
  <summary>Details</summary>
Motivation: The rise in the deployment of LLMs has led to concerns over privacy, copyright, security, and bias, necessitating effective unlearning techniques.

Method: The paper categorizes unlearning methods (data-centric, parameter-centric, architecture-centric, hybrid) and evaluates the ecosystem with benchmarks, metrics, and datasets.

Result: Provides a structured overview of existing unlearning methods and identifies key challenges and open problems, offering a roadmap for reliable unlearning techniques.

Conclusion: The paper synthesizes existing progress and outlines directions for scalable, robust, and responsible unlearning methods in LLMs to address ongoing challenges.

Abstract: Large language models (LLMs) have achieved remarkable success across natural language processing tasks, yet their widespread deployment raises pressing concerns around privacy, copyright, security, and bias. Machine unlearning has emerged as a promising paradigm for selectively removing knowledge or data from trained models without full retraining. In this survey, we provide a structured overview of unlearning methods for LLMs, categorizing existing approaches into data-centric, parameter-centric, architecture-centric, hybrid, and other strategies. We also review the evaluation ecosystem, including benchmarks, metrics, and datasets designed to measure forgetting effectiveness, knowledge retention, and robustness. Finally, we outline key challenges and open problems, such as scalable efficiency, formal guarantees, cross-language and multimodal unlearning, and robustness against adversarial relearning. By synthesizing current progress and highlighting open directions, this paper aims to serve as a roadmap for developing reliable and responsible unlearning techniques in large language models.

</details>


### [225] [A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification](https://arxiv.org/abs/2601.13288)
*Gonzalo Ariel Meyoyan,Luciano Del Corro*

Main category: cs.CL

TL;DR: This paper proposes lightweight probes that reuse serving LLM hidden states for safety and classification tasks, significantly reducing latency and complexity.


<details>
  <summary>Details</summary>
Motivation: Production LLMs often rely on separate models for safety and classification tasks, leading to increased latency, VRAM usage, and operational complexity.

Method: The paper introduces lightweight probes trained on LLM hidden states to predict labels during the same forward pass used for text generation. It uses a two-stage aggregator summarizing token-layer hidden-state tensors.

Result: The probes outperform logit-only reuse methods on safety and sentiment benchmarks and are competitive with larger task-specific models. This is achieved without increased latency or VRAM costs.

Conclusion: The proposed probes offer efficient, scalable alternatives for classification tasks in LLM systems while improving performance and reducing complexity.

Abstract: Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.

</details>


### [226] [OI-Bench: An Option Injection Benchmark for Evaluating LLM Susceptibility to Directive Interference](https://arxiv.org/abs/2601.13300)
*Yow-Fu Liou,Yu-Chien Tang,Yu-Hsiang Liu,An-Zi Yen*

Main category: cs.CL

TL;DR: The paper presents OI-Bench, a benchmark to assess the vulnerabilities and robustness of language models to misleading directives in multiple-choice settings.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to analyze the robustness of Large Language Models, especially in how they handle directive signals that may influence their decisions in multiple-choice interface contexts.

Method: They introduced 'option injection,' a new benchmarking method that incorporates misleading directives into multiple-choice format questions. Additionally, they developed OI-Bench, a benchmark based on 3,000 tasks spanning various domains.

Result: The evaluation of 12 LLMs revealed significant vulnerabilities and diverse levels of robustness to directive interference. Mitigation strategies were also studied.

Conclusion: OI-Bench offers a systematic approach to measure and improve LLMs' robustness against directive-based manipulations in choice-based interfaces.

Abstract: Benchmarking large language models (LLMs) is critical for understanding their capabilities, limitations, and robustness. In addition to interface artifacts, prior studies have shown that LLM decisions can be influenced by directive signals such as social cues, framing, and instructions. In this work, we introduce option injection, a benchmarking approach that augments the multiple-choice question answering (MCQA) interface with an additional option containing a misleading directive, leveraging standardized choice structure and scalable evaluation. We construct OI-Bench, a benchmark of 3,000 questions spanning knowledge, reasoning, and commonsense tasks, with 16 directive types covering social compliance, bonus framing, threat framing, and instructional interference. This setting combines manipulation of the choice interface with directive-based interference, enabling systematic assessment of model susceptibility. We evaluate 12 LLMs to analyze attack success rates, behavioral responses, and further investigate mitigation strategies ranging from inference-time prompting to post-training alignment. Experimental results reveal substantial vulnerabilities and heterogeneous robustness across models. OI-Bench is expected to support more systematic evaluation of LLM robustness to directive interference within choice-based interfaces.

</details>


### [227] [Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling of Climate Discourse](https://arxiv.org/abs/2601.13317)
*Samantha Sudhoff,Pranav Perumal,Zhaoqing Wu,Tunazzina Islam*

Main category: cs.CL

TL;DR: This study compares climate discourse between paid Meta advertisements and public Bluesky posts, introducing a thematic analysis framework using LLMs.


<details>
  <summary>Details</summary>
Motivation: To understand differences in climate messaging between institutional paid advertisements and organic public discourse across distinct online platforms.

Method: Developed a thematic discovery framework utilizing LLMs for clustering texts by semantic similarity and labeling themes. Evaluated themes through human judgment, LLM assessments, and stance prediction tasks.

Result: Found systematic differences in thematic structures, stance alignment, and responsiveness in climate messaging between paid ads and public discourse.

Conclusion: Platform incentives shape thematic structures and climate narratives. The introduced framework aids comparative analysis across diverse communication ecosystems.

Abstract: Climate discourse online plays a crucial role in shaping public understanding of climate change and influencing political and policy outcomes. However, climate communication unfolds across structurally distinct platforms with fundamentally different incentive structures: paid advertising ecosystems incentivize targeted, strategic persuasion, while public social media platforms host largely organic, user-driven discourse. Existing computational studies typically analyze these environments in isolation, limiting our ability to distinguish institutional messaging from public expression. In this work, we present a comparative analysis of climate discourse across paid advertisements on Meta (previously known as Facebook) and public posts on Bluesky from July 2024 to September 2025. We introduce an interpretable, end-to-end thematic discovery and assignment framework that clusters texts by semantic similarity and leverages large language models (LLMs) to generate concise, human-interpretable theme labels. We evaluate the quality of the induced themes against traditional topic modeling baselines using both human judgments and an LLM-based evaluator, and further validate their semantic coherence through downstream stance prediction and theme-guided retrieval tasks. Applying the resulting themes, we characterize systematic differences between paid climate messaging and public climate discourse and examine how thematic prevalence shifts around major political events. Our findings show that platform-level incentives are reflected in the thematic structure, stance alignment, and temporal responsiveness of climate narratives. While our empirical analysis focuses on climate communication, the proposed framework is designed to support comparative narrative analysis across heterogeneous communication environments.

</details>


### [228] [Arab Voices: Mapping Standard and Dialectal Arabic Speech Technology](https://arxiv.org/abs/2601.13319)
*Peter Sullivan,AbdelRahim Elmadany,Alcides Alcoba Inciarte,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: The paper addresses the challenges in Dialectal Arabic (DA) speech datasets and introduces a standardized framework, Arab Voices, to improve cross-dataset comparison and evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the inconsistencies in DA speech data regarding domain, dialect labeling, and recording conditions, which hinder effective model development and comparison.

Method: The study analyzes DA corpora linguistically and acoustically, proposes standardized dataset characterization, and introduces Arab Voices with unified access, harmonized metadata, and utilities for evaluation.

Result: The analysis reveals significant heterogeneity in DA datasets and benchmarks recent ASR systems, providing strong baselines for DA Automatic Speech Recognition.

Conclusion: The standardization framework, Arab Voices, better characterizes DA datasets and supports reproducible evaluation to advance research in DA speech recognition.

Abstract: Dialectal Arabic (DA) speech data vary widely in domain coverage, dialect labeling practices, and recording conditions, complicating cross-dataset comparison and model evaluation. To characterize this landscape, we conduct a computational analysis of linguistic ``dialectness'' alongside objective proxies of audio quality on the training splits of widely used DA corpora. We find substantial heterogeneity both in acoustic conditions and in the strength and consistency of dialectal signals across datasets, underscoring the need for standardized characterization beyond coarse labels. To reduce fragmentation and support reproducible evaluation, we introduce Arab Voices, a standardized framework for DA ASR. Arab Voices provides unified access to 31 datasets spanning 14 dialects, with harmonized metadata and evaluation utilities. We further benchmark a range of recent ASR systems, establishing strong baselines for modern DA ASR.

</details>


### [229] [Reducing Tokenization Premiums for Low-Resource Languages](https://arxiv.org/abs/2601.13328)
*Geoffrey Churchill,Steven Skiena*

Main category: cs.CL

TL;DR: Low-resource languages face higher tokenization costs in language models (LMs) compared to English. This study analyzes tokenization practices and proposes a method to reduce tokenization premiums.


<details>
  <summary>Details</summary>
Motivation: Address the gap where low-resource languages require significantly more tokens for encoding, leading to increased costs and inefficiencies in modern language models.

Method: Analyzed tokenizers of ten popular LMs and introduced a mechanism for post-hoc additions to token vocabularies to merge multi-token characters into single tokens.

Result: Applied the proposed methodology to 12 low-resource languages, showing similar last hidden states between original and compressed inputs in the Llama 3.2 1B model.

Conclusion: The suggested approach effectively reduces tokenization inefficiencies for low-resource languages while maintaining model performance.

Abstract: Relative to English, low-resource languages suffer from substantial tokenization premiums in modern LMs, meaning that it generally requires several times as many tokens to encode a sentence in a low-resource language than to encode the analogous sentence in English. This tokenization premium results in increased API and energy costs and reduced effective context windows for these languages. In this paper we analyze the tokenizers of ten popular LMs to better understand their designs and per-language tokenization premiums. We also propose a mechanism to reduce tokenization premiums in pre-trained models, by post-hoc additions to the token vocabulary that coalesce multi-token characters into single tokens. We apply this methodology to 12 low-resource languages, demonstrating that the original and compressed inputs often have similar last hidden states when run through the Llama 3.2 1B model.

</details>


### [230] [RegCheck: A tool for automating comparisons between study registrations and papers](https://arxiv.org/abs/2601.13330)
*Jamie Cummins,Beth Clarke,Ian Hussey,Malte Elson*

Main category: cs.CL

TL;DR: RegCheck is a tool leveraging AI to compare study registrations with papers, improving research transparency and reproducibility.


<details>
  <summary>Details</summary>
Motivation: Enhance study transparency and rigor by facilitating the comparison of study registrations with publications, addressing labor-intensive manual checking.

Method: Developing RegCheck, a modular LLM-assisted tool to assist comparisons between registrations and papers, keeping human judgment integral to the process.

Result: RegCheck enhances efficiency and judgement in comparing study registrations with papers, while generating shareable reports for verification.

Conclusion: RegCheck supports reproducible science and extends its adaptability across scientific domains, providing infrastructure for transparency and rigor.

Abstract: Across the social and medical sciences, researchers recognize that specifying planned research activities (i.e., 'registration') prior to the commencement of research has benefits for both the transparency and rigour of science. Despite this, evidence suggests that study registrations frequently go unexamined, minimizing their effectiveness. In a way this is no surprise: manually checking registrations against papers is labour- and time-intensive, requiring careful reading across formats and expertise across domains. The advent of AI unlocks new possibilities in facilitating this activity. We present RegCheck, a modular LLM-assisted tool designed to help researchers, reviewers, and editors from across scientific disciplines compare study registrations with their corresponding papers. Importantly, RegCheck keeps human expertise and judgement in the loop by (i) ensuring that users are the ones who determine which features should be compared, and (ii) presenting the most relevant text associated with each feature to the user, facilitating (rather than replacing) human discrepancy judgements. RegCheck also generates shareable reports with unique RegCheck IDs, enabling them to be easily shared and verified by other users. RegCheck is designed to be adaptable across scientific domains, as well as registration and publication formats. In this paper we provide an overview of the motivation, workflow, and design principles of RegCheck, and we discuss its potential as an extensible infrastructure for reproducible science with an example use case.

</details>


### [231] [AfroScope: A Framework for Studying the Linguistic Landscape of Africa](https://arxiv.org/abs/2601.13346)
*Sang Yun Kwon,AbdelRahim Elmadany,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: This paper introduces AfroScope, a framework for African Language Identification (LID), including a dataset (713 languages) and advanced models. It improves identification, especially among closely related languages, using a hierarchical classification approach.


<details>
  <summary>Details</summary>
Motivation: The limited support for African languages in existing LID systems and difficulty in distinguishing among closely related varieties necessitated the development of a comprehensive framework for African LID.

Method: The authors created AfroScope, combining AfroScope-Data (a dataset with 713 languages) and AfroScope-Models (LID models). They introduced a hierarchical classification approach using the Mirror-Serengeti model for distinguishing confusable languages.

Result: The hierarchical classification method improved macro F1 by 4.55 on a subset of closely related languages compared to the best base model and demonstrated cross-linguistic transfer and domain robustness.

Conclusion: AfroScope framework significantly advances African LID by addressing gaps in language coverage and fine-grained distinctions. The public release enables further research and supports African linguistic research and applications.

Abstract: Language Identification (LID) is the task of determining the language of a given text and is a fundamental preprocessing step that affects the reliability of downstream NLP applications. While recent work has expanded LID coverage for African languages, existing approaches remain limited in (i) the number of supported languages and (ii) their ability to make fine-grained distinctions among closely related varieties. We introduce AfroScope, a unified framework for African LID that includes AfroScope-Data, a dataset covering 713 African languages, and AfroScope-Models, a suite of strong LID models with broad language coverage. To better distinguish highly confusable languages, we propose a hierarchical classification approach that leverages Mirror-Serengeti, a specialized embedding model targeting 29 closely related or geographically proximate languages. This approach improves macro F1 by 4.55 on this confusable subset compared to our best base model. Finally, we analyze cross linguistic transfer and domain effects, offering guidance for building robust African LID systems. We position African LID as an enabling technology for large scale measurement of Africas linguistic landscape in digital text and release AfroScope-Data and AfroScope-Models publicly.

</details>


### [232] [LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction](https://arxiv.org/abs/2601.13352)
*Yuxing Lu,J. Ben Tamo,Weichen Zhao,Nan Sun,Yishan Zhong,Wenqi Shi,Jinzhuo Wang,May D. Wang*

Main category: cs.CL

TL;DR: The paper introduces LLM-as-RNN, a framework that turns frozen large language models (LLMs) into recurrent predictors with natural-language memory updates, achieving improved predictive accuracy without parameter changes.


<details>
  <summary>Details</summary>
Motivation: Standard LLM inference relies on immutable context histories, preventing models from correcting errors made during generation. The paper aims to develop a mechanism for dynamic error correction and online learning without altering model parameters.

Method: The proposed method, LLM-as-RNN, uses structured system-prompt updates (natural-language memory) at every timestep. These updates allow the model to learn and improve predictions iteratively while staying within a fixed token budget.

Result: LLM-as-RNN significantly surpasses baselines (zero-shot, full-history, and MemPrompt) by improving accuracy by an average of 6.5% across sequential benchmarks in healthcare, meteorology, and finance.

Conclusion: The framework successfully enables frozen LLMs to perform online learning and dynamic memory updates, offering better predictive accuracy and interpretable outputs compared to conventional inference methods.

Abstract: Large language models are strong sequence predictors, yet standard inference relies on immutable context histories. After making an error at generation step t, the model lacks an updatable memory mechanism that improves predictions for step t+1. We propose LLM-as-RNN, an inference-only framework that turns a frozen LLM into a recurrent predictor by representing its hidden state as natural-language memory. This state, implemented as a structured system-prompt summary, is updated at each timestep via feedback-driven text rewrites, enabling learning without parameter updates. Under a fixed token budget, LLM-as-RNN corrects errors and retains task-relevant patterns, effectively performing online learning through language. We evaluate the method on three sequential benchmarks in healthcare, meteorology, and finance across Llama, Gemma, and GPT model families. LLM-as-RNN significantly outperforms zero-shot, full-history, and MemPrompt baselines, improving predictive accuracy by 6.5% on average, while producing interpretable, human-readable learning traces absent in standard context accumulation.

</details>


### [233] [Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection](https://arxiv.org/abs/2601.13359)
*Asen Dotsinski,Panagiotis Eustratiadis*

Main category: cs.CL

TL;DR: This paper introduces 'sockpuppetting,' a simple method for jailbreaking open-weight LLMs by inserting an acceptance sequence at the beginning of the output, achieving high attack success rates with minimal resources.


<details>
  <summary>Details</summary>
Motivation: To address the growing importance of protecting open-weight LLMs from jailbreaking and malicious prompts, as these models' capabilities expand.

Method: The paper proposes 'sockpuppetting,' inserting a predefined acceptance sequence at the start of the model's output to facilitate jailbreaking. It also explores a hybrid approach to optimize adversarial suffixes within the assistant’s message block.

Result: The sockpuppetting method achieves up to 80% higher attack success rates than GCG for Qwen3-8B, and the hybrid approach achieves 64% higher success rates compared to GCG for Llama-3.1-8B.

Conclusion: Sockpuppetting is an effective and low-cost attack method accessible to unsophisticated adversaries, emphasizing the need for stronger defenses in open-weight language models to resist output-prefix injection attacks.

Abstract: As open-weight large language models (LLMs) increase in capabilities, safeguarding them against malicious prompts and understanding possible attack vectors becomes ever more important. While automated jailbreaking methods like GCG [Zou et al., 2023] remain effective, they often require substantial computational resources and specific expertise. We introduce "sockpuppetting'', a simple method for jailbreaking open-weight LLMs by inserting an acceptance sequence (e.g., "Sure, here is how to...'') at the start of a model's output and allowing it to complete the response. Requiring only a single line of code and no optimization, sockpuppetting achieves up to 80% higher attack success rate (ASR) than GCG on Qwen3-8B in per-prompt comparisons. We also explore a hybrid approach that optimizes the adversarial suffix within the assistant message block rather than the user prompt, increasing ASR by 64% over GCG on Llama-3.1-8B in a prompt-agnostic setting. The results establish sockpuppetting as an effective low-cost attack accessible to unsophisticated adversaries, highlighting the need for defences against output-prefix injection in open-weight models.

</details>


### [234] [Recurrent Confidence Chain: Temporal-Aware Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2601.13368)
*Zhenjiang Mao,Anirudhh Venkat*

Main category: cs.CL

TL;DR: The paper introduces a method to improve uncertainty assessments in reasoning tasks by integrating inter-step attention and hidden confidence mechanisms, outperforming current solutions in benchmarks.


<details>
  <summary>Details</summary>
Motivation: With reasoning modules like chain-of-thought becoming pivotal for tasks such as math problem-solving, assessing answer uncertainty is crucial to prevent misleading or erroneous outputs.

Method: The proposal involves inter-step attention to analyze semantic correlations and a hidden confidence mechanism to retain past confidence data for better calibration.

Result: The method significantly outperforms existing approaches on the GAOKAO math benchmark and CLadder causal reasoning dataset, demonstrated through improved metrics like Negative Log-Likelihood and Expected Calibration Error.

Conclusion: The new method offers more accurate uncertainty estimations, achieving a better balance between prediction quality and calibration, addressing prior gaps in assessing long-reasoning sequences.

Abstract: As reasoning modules, such as the chain-of-thought mechanism, are applied to large language models, they achieve strong performance on various tasks such as answering common-sense questions and solving math problems. The main challenge now is to assess the uncertainty of answers, which can help prevent misleading or serious hallucinations for users. Although current methods analyze long reasoning sequences by filtering unrelated tokens and examining potential connections between nearby tokens or sentences, the temporal spread of confidence is often overlooked. This oversight can lead to inflated overall confidence, even when earlier steps exhibit very low confidence. To address this issue, we propose a novel method that incorporates inter-step attention to analyze semantic correlations across steps. For handling long-horizon responses, we introduce a hidden confidence mechanism to retain historical confidence information, which is then combined with stepwise confidence to produce a more accurate overall estimate. We evaluate our method on the GAOKAO math benchmark and the CLadder causal reasoning dataset using mainstream open-source large language models. Our approach is shown to outperform state-of-the-art methods by achieving a superior balance between predictive quality and calibration, demonstrated by strong performance on both Negative Log-Likelihood and Expected Calibration Error.

</details>


### [235] [Confidence over Time: Confidence Calibration with Temporal Logic for Large Language Model Reasoning](https://arxiv.org/abs/2601.13387)
*Zhenjiang Mao,Anirudhh Venkat,Artem Bisliouk,Akshat Kothiyal,Sindhura Kumbakonam Subramanian,Saithej Singhu,Ivan Ruchkin*

Main category: cs.CL

TL;DR: This paper introduces a method to enhance confidence estimation in Large Language Models (LLMs) during long-form, multi-step reasoning tasks by analyzing stepwise confidence patterns using Signal Temporal Logic (STL).


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and calibration of confidence estimation in reasoning processes of LLMs, which are currently limited by single scalar scores that fail to capture the evolving nature of confidence.

Method: The authors employ Signal Temporal Logic (STL) to represent stepwise confidence patterns and use an STL mining procedure to differentiate between the confidence signals of correct and incorrect reasoning. They enhance the performance by utilizing parameter hypernetworks informed by STL blocks.

Result: The proposed method demonstrated improved calibration of confidence scores across multiple reasoning tasks when compared to existing baselines.

Conclusion: This study introduces an innovative stepwise confidence estimation method, which leverages STL patterns to better generalize and calibrate confidence signals of LLMs, marking a significant advancement in reasoning task evaluation.

Abstract: Large Language Models (LLMs) increasingly rely on long-form, multi-step reasoning to solve complex tasks such as mathematical problem solving and scientific question answering. Despite strong performance, existing confidence estimation methods typically reduce an entire reasoning process to a single scalar score, ignoring how confidence evolves throughout the generation. As a result, these methods are often sensitive to superficial factors such as response length or verbosity, and struggle to distinguish correct reasoning from confidently stated errors. We propose to characterize the stepwise confidence signal using Signal Temporal Logic (STL). Using a discriminative STL mining procedure, we discover temporal formulas that distinguish confidence signals of correct and incorrect responses. Our analysis found that the STL patterns generalize across tasks, and numeric parameters exhibit sensitivity to individual questions. Based on these insights, we develop a confidence estimation approach that informs STL blocks with parameter hypernetworks. Experiments on multiple reasoning tasks show our confidence scores are more calibrated than the baselines.

</details>


### [236] [Structured Insight from Unstructured Data: Large Language Models for SDOH-Driven Diabetes Risk Prediction](https://arxiv.org/abs/2601.13388)
*Sasha Ronaghi,Prerit Choudhary,David H Rehkopf,Bryant Lin*

Main category: cs.CL

TL;DR: This paper investigates how large language models (LLMs) can extract structured SDOH data from unstructured interviews with Type 2 Diabetes (T2D) patients and use the data for diabetes risk prediction.


<details>
  <summary>Details</summary>
Motivation: SDOH data are crucial for T2D management but are often missing from electronic records and current prediction models. Traditional tools fail to capture patient-specific complexities, necessitating new methods to process unstructured data.

Method: Unstructured interviews with T2D patients were analyzed using LLMs with retrieval-augmented generation. These tools produced qualitative summaries and quantitative SDOH ratings, which were then input into machine learning models to evaluate their predictive value.

Result: LLMs achieved 60% accuracy in predicting diabetes control levels from unstructured interview text, even without A1C values. Machine learning models combined structured SDOH ratings with traditional biomarkers for improved risk prediction.

Conclusion: LLMs can effectively convert unstructured SDOH-related data into actionable insights, enhancing diabetes risk prediction and clinical decision-making workflows.

Abstract: Social determinants of health (SDOH) play a critical role in Type 2 Diabetes (T2D) management but are often absent from electronic health records and risk prediction models. Most individual-level SDOH data is collected through structured screening tools, which lack the flexibility to capture the complexity of patient experiences and unique needs of a clinic's population. This study explores the use of large language models (LLMs) to extract structured SDOH information from unstructured patient life stories and evaluate the predictive value of both the extracted features and the narratives themselves for assessing diabetes control. We collected unstructured interviews from 65 T2D patients aged 65 and older, focused on their lived experiences, social context, and diabetes management. These narratives were analyzed using LLMs with retrieval-augmented generation to produce concise, actionable qualitative summaries for clinical interpretation and structured quantitative SDOH ratings for risk prediction modeling. The structured SDOH ratings were used independently and in combination with traditional laboratory biomarkers as inputs to linear and tree-based machine learning models (Ridge, Lasso, Random Forest, and XGBoost) to demonstrate how unstructured narrative data can be applied in conventional risk prediction workflows. Finally, we evaluated several LLMs on their ability to predict a patient's level of diabetes control (low, medium, high) directly from interview text with A1C values redacted. LLMs achieved 60% accuracy in predicting diabetes control levels from interview text. This work demonstrates how LLMs can translate unstructured SDOH-related data into structured insights, offering a scalable approach to augment clinical risk models and decision-making.

</details>


### [237] [Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks](https://arxiv.org/abs/2601.13392)
*Shlok Shelat,Jay Raval,Souvik Roy,Manas Gaur*

Main category: cs.CL

TL;DR: Large language models (LLMs) struggle with unseen deterministic finite automata (DFA) construction problems despite showing high accuracy on factual and seen tasks.


<details>
  <summary>Details</summary>
Motivation: Investigate whether LLMs can perform symbolic reasoning tasks like DFA construction, moving beyond pattern matching of familiar structures.

Method: Developed a benchmark including factual questions, seen DFA problems, and two categories of unseen DFA problems; evaluated multiple prompting strategies and a hint correction protocol.

Result: LLMs achieved high accuracy on factual and seen DFA problems but showed significant performance drops on unseen tasks, with persistent errors despite diverse prompting approaches.

Conclusion: LLMs cannot yet reliably generate semantically accurate DFAs and have limitations in formal reasoning, especially under unfamiliar constraints.

Abstract: Large language models (LLMs) have demonstrated strong performance on formal language tasks, yet whether this reflects genuine symbolic reasoning or pattern matching on familiar constructions remains unclear. We introduce a benchmark for deterministic finite automata (DFA) construction from regular languages, comprising factual knowledge questions, seen construction problems from public sources, and two types of unseen problems: hand-crafted instances with multiple interacting constraints and systematically generated problems via Arden's theorem. Models achieve perfect accuracy on factual questions and 84-90% on seen tasks. However, accuracy drops sharply on unseen problems (by 30-64%), with failures stemming from systematic misinterpretation of language constraints, incorrect handling of Kleene-star semantics, and a failure to preserve global consistency. We evaluate a three-stage hint protocol that enables correction of shallow errors but does not reliably resolve globally inconsistent or structurally flawed automata. Our analysis across multiple prompting strategies (direct, Chain-of-Thought, Tree-of-Thought) reveals that errors persist regardless of prompting approach, exposing a fundamental gap between LLMs' ability to generate syntactically plausible DFAs and their capacity for semantically correct formal reasoning.

</details>


### [238] [Trust Me, I'm an Expert: Decoding and Steering Authority Bias in Large Language Models](https://arxiv.org/abs/2601.13433)
*Priyanka Mary Mammen,Emil Joswin,Shankar Venkitachalam*

Main category: cs.CL

TL;DR: Language models show bias towards the perceived expertise of endorsement sources, leading to decreased accuracy and increased confidence in incorrect answers when influenced by higher-authority personas.


<details>
  <summary>Details</summary>
Motivation: To understand whether language models are biased towards endorsements based on the credibility and expertise level of the source, and how this impacts their reasoning performance.

Method: Four datasets related to mathematical, legal, and medical reasoning were tested across 11 models. Personas with four different expertise levels were utilized to evaluate the influence of endorsements.

Result: Language models showed susceptibility to endorsements from higher-authority sources, leading to reduced accuracy and increased confidence in wrong answers. The bias is mechanistically encoded but can be mitigated to improve performance.

Conclusion: Authority bias in language models can harm reasoning accuracy but can be addressed to mitigate its effect and improve model reliability under misleading endorsements.

Abstract: Prior research demonstrates that performance of language models on reasoning tasks can be influenced by suggestions, hints and endorsements. However, the influence of endorsement source credibility remains underexplored. We investigate whether language models exhibit systematic bias based on the perceived expertise of the provider of the endorsement. Across 4 datasets spanning mathematical, legal, and medical reasoning, we evaluate 11 models using personas representing four expertise levels per domain. Our results reveal that models are increasingly susceptible to incorrect/misleading endorsements as source expertise increases, with higher-authority sources inducing not only accuracy degradation but also increased confidence in wrong answers. We also show that this authority bias is mechanistically encoded within the model and a model can be steered away from the bias, thereby improving its performance even when an expert gives a misleading endorsement.

</details>


### [239] [MOSLD-Bench: Multilingual Open-Set Learning and Discovery Benchmark for Text Categorization](https://arxiv.org/abs/2601.13437)
*Adriana-Valentina Costache,Daria-Nicoleta Dragomir,Silviu-Florin Gheorghe,Eduard Poesina,Paul Irofti,Radu Tudor Ionescu*

Main category: cs.CL

TL;DR: The paper introduces the first multilingual benchmark for open-set learning and discovery in text categorization across 12 languages, proposes a novel framework for the task, and evaluates language models to provide baseline results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of resources and frameworks in open-set learning and discovery for text classification, particularly in a multilingual context.

Method: The authors construct a benchmark by gathering and reorganizing datasets from the news domain and propose a multi-stage framework for discovering and learning new classes in OSLD.

Result: The study evaluates multiple language models, including their own, to generate baseline results for future OSLD research.

Conclusion: They provide a comprehensive benchmark and framework for advancing open-set learning and discovery in the text domain, offering valuable resources to the research community.

Abstract: Open-set learning and discovery (OSLD) is a challenging machine learning task in which samples from new (unknown) classes can appear at test time. It can be seen as a generalization of zero-shot learning, where the new classes are not known a priori, hence involving the active discovery of new classes. While zero-shot learning has been extensively studied in text classification, especially with the emergence of pre-trained language models, open-set learning and discovery is a comparatively new setup for the text domain. To this end, we introduce the first multilingual open-set learning and discovery (MOSLD) benchmark for text categorization by topic, comprising 960K data samples across 12 languages. To construct the benchmark, we (i) rearrange existing datasets and (ii) collect new data samples from the news domain. Moreover, we propose a novel framework for the OSLD task, which integrates multiple stages to continuously discover and learn new classes. We evaluate several language models, including our own, to obtain results that can be used as reference for future work. We release our benchmark at https://github.com/Adriana19Valentina/MOSLD-Bench.

</details>


### [240] [PhysicsSolutionAgent: Towards Multimodal Explanations for Numerical Physics Problem Solving](https://arxiv.org/abs/2601.13453)
*Aditya Thole,Anmol Agrawal,Arnav Ramamoorthy,Dhruv Kumar*

Main category: cs.CL

TL;DR: The paper introduces PhysicsSolutionAgent (PSA), an agent for creating physics explanation videos using Manim animations, assessed via automated checks and feedback from a vision-language model. It exposes the challenges in generating reliable and high-quality visual explanations.


<details>
  <summary>Details</summary>
Motivation: To address the gap in the capability of large language models in generating high-quality, visual-based explanations of numerical physics problems effectively.

Method: The authors developed an agent named PhysicsSolutionAgent (PSA) that generates six-minute physics explanation videos, evaluates them using an assessment pipeline with 15 quantitative parameters, and incorporates vision-language model feedback for improvement.

Result: PSA creates 32 physics explanation videos with a 100% video completion rate and an average automated score of 3.8 out of 5, but visual inconsistencies and errors during feedback were identified.

Conclusion: This work highlights the limitations in creating reliable visual explanations, emphasizing the need for better multimodal reasoning, verification, and evaluation frameworks in educational systems.

Abstract: Explaining numerical physics problems often requires more than text-based solutions; clear visual reasoning can substantially improve conceptual understanding. While large language models (LLMs) demonstrate strong performance on many physics questions in textual form, their ability to generate long, high-quality visual explanations remains insufficiently explored. In this work, we introduce PhysicsSolutionAgent (PSA), an autonomous agent that generates physics-problem explanation videos of up to six minutes using Manim animations. To evaluate the generated videos, we design an assessment pipeline that performs automated checks across 15 quantitative parameters and incorporates feedback from a vision-language model (VLM) to iteratively improve video quality. We evaluate PSA on 32 videos spanning numerical and theoretical physics problems. Our results reveal systematic differences in video quality depending on problem difficulty and whether the task is numerical or theoretical. Using GPT-5-mini, PSA achieves a 100% video-completion rate with an average automated score of 3.8/5. However, qualitative analysis and human inspection uncover both minor and major issues, including visual layout inconsistencies and errors in how visual content is interpreted during feedback. These findings expose key limitations in reliable Manim code generation and highlight broader challenges in multimodal reasoning and evaluation for visual explanations of numerical physics problems. Our work underscores the need for improved visual understanding, verification, and evaluation frameworks in future multimodal educational systems

</details>


### [241] [Anonpsy: A Graph-Based Framework for Structure-Preserving De-identification of Psychiatric Narratives](https://arxiv.org/abs/2601.13503)
*Kyung Ho Lim,Byung-Hoon Kim*

Main category: cs.CL

TL;DR: This paper presents Anonpsy, a novel framework for de-identifying psychiatric narratives using graph-guided semantic rewriting and constrained LLM text generation.


<details>
  <summary>Details</summary>
Motivation: Existing de-identification methods for psychiatric narratives struggle to balance preserving essential semantic elements with privacy due to text-level operations and limited control over alterations.

Method: Anonpsy creates semantic graphs from narratives, performs graph-constrained modifications to encode clinical entities and relationships, and uses LLMs to regenerate de-identified text, maintaining core clinical structure.

Result: Anonpsy achieves lower re-identification risk while preserving diagnostic accuracy, outperforms traditional rewriting methods in semantic similarity, and ensures privacy across various evaluations.

Conclusion: Combining structural graph representations with constrained LLM-based text generation offers a reliable de-identification approach for sensitive psychiatric data.

Abstract: Psychiatric narratives encode patient identity not only through explicit identifiers but also through idiosyncratic life events embedded in their clinical structure. Existing de-identification approaches, including PHI masking and LLM-based synthetic rewriting, operate at the text level and offer limited control over which semantic elements are preserved or altered. We introduce Anonpsy, a de-identification framework that reformulates the task as graph-guided semantic rewriting. Anonpsy (1) converts each narrative into a semantic graph encoding clinical entities, temporal anchors, and typed relations; (2) applies graph-constrained perturbations that modify identifying context while preserving clinically essential structure; and (3) regenerates text via graph-conditioned LLM generation. Evaluated on 90 clinician-authored psychiatric case narratives, Anonpsy preserves diagnostic fidelity while achieving consistently low re-identification risk under expert, semantic, and GPT-5-based evaluations. Compared with a strong LLM-only rewriting baseline, Anonpsy yields substantially lower semantic similarity and identifiability. These results demonstrate that explicit structural representations combined with constrained generation provide an effective approach to de-identification for psychiatric narratives.

</details>


### [242] [When Wording Steers the Evaluation: Framing Bias in LLM judges](https://arxiv.org/abs/2601.13537)
*Yerin Hwang,Dongryeol Lee,Taegwan Kang,Minwoo Lee,Kyomin Jung*

Main category: cs.CL

TL;DR: The study explores how prompt framing influences judgments of large language models (LLMs) during evaluation tasks, highlighting susceptibility to bias and the need for framing-aware protocols.


<details>
  <summary>Details</summary>
Motivation: Prompt phrasing can steer LLM responses significantly, raising concerns about their impartiality and stability in evaluative tasks. The study aims to investigate the impact of framing bias systematically.

Method: Four evaluation tasks are set up using symmetric prompts with predicate-positive and predicate-negative constructions across 14 LLM judges. The study systematically examines discrepancies caused by varied prompt framing.

Result: Framing prompts induce significant bias in LLM outputs, with different families of models exhibiting distinct agreement or rejection patterns. This highlights structural susceptibility to framing bias.

Conclusion: Framing bias is inherent in current LLM-based evaluations. Framing-aware protocols are needed to ensure more impartial and stable model-based judgments.

Abstract: Large language models (LLMs) are known to produce varying responses depending on prompt phrasing, indicating that subtle guidance in phrasing can steer their answers. However, the impact of this framing bias on LLM-based evaluation, where models are expected to make stable and impartial judgments, remains largely underexplored. Drawing inspiration from the framing effect in psychology, we systematically investigate how deliberate prompt framing skews model judgments across four high-stakes evaluation tasks. We design symmetric prompts using predicate-positive and predicate-negative constructions and demonstrate that such framing induces significant discrepancies in model outputs. Across 14 LLM judges, we observe clear susceptibility to framing, with model families showing distinct tendencies toward agreement or rejection. These findings suggest that framing bias is a structural property of current LLM-based evaluation systems, underscoring the need for framing-aware protocols.

</details>


### [243] [HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations](https://arxiv.org/abs/2601.13547)
*Yujia Hu,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: The paper introduces HateXScore, a metric suite aimed at evaluating reasoning quality in hate speech detection models, addressing gaps in current evaluation frameworks.


<details>
  <summary>Details</summary>
Motivation: Current hate speech detection lacks robust evaluation frameworks to explain why text is classified as hateful.

Method: HateXScore evaluates models using four metrics: explicitness, faithfulness, group identification, and logical consistency. The approach is applied across six datasets.

Result: HateXScore revealed interpretability issues in current models and annotation inconsistencies invisible to standard metrics like Accuracy or F1 scores.

Conclusion: HateXScore is a validated and practical tool for enhancing the trustworthiness and transparency of hate speech moderation systems.

Abstract: Hateful speech detection is a key component of content moderation, yet current evaluation frameworks rarely assess why a text is deemed hateful. We introduce \textsf{HateXScore}, a four-component metric suite designed to evaluate the reasoning quality of model explanations. It assesses (i) conclusion explicitness, (ii) faithfulness and causal grounding of quoted spans, (iii) protected group identification (policy-configurable), and (iv) logical consistency among these elements. Evaluated on six diverse hate speech datasets, \textsf{HateXScore} is intended as a diagnostic complement to reveal interpretability failures and annotation inconsistencies that are invisible to standard metrics like Accuracy or F1. Moreover, human evaluation shows strong agreement with \textsf{HateXScore}, validating it as a practical tool for trustworthy and transparent moderation.
  \textcolor{red}{Disclaimer: This paper contains sensitive content that may be disturbing to some readers.}

</details>


### [244] [Comparing Without Saying: A Dataset and Benchmark for Implicit Comparative Opinion Mining from Same-User Reviews](https://arxiv.org/abs/2601.13575)
*Thanh-Lam T. Nguyen,Ngoc-Quang Le,Quoc-Trung Phu,Thi-Phuong Le,Ngoc-Huyen Pham,Phuong-Nguyen Nguyen,Hoang-Quynh Le*

Main category: cs.CL

TL;DR: The paper introduces SUDO, a dataset for implicit comparative opinion mining from same-user reviews, highlighting the difficulty of such tasks and its significance as a benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored area of implicit comparisons in opinion mining, where preferences are expressed across separate reviews rather than through explicit comparative cues.

Method: The authors developed SUDO, a dataset annotated with a bi-level structure (aspect-level mentions and review-level preferences), and benchmarked the task using traditional machine learning and language model-based baselines.

Result: Results showed that language model-based baselines outperform traditional ones, though performance remains moderate, highlighting the difficulty of implicit comparison tasks.

Conclusion: SUDO establishes itself as a challenging, important dataset and benchmark for advancing research on implicit comparative opinion mining.

Abstract: Existing studies on comparative opinion mining have mainly focused on explicit comparative expressions, which are uncommon in real-world reviews. This leaves implicit comparisons - here users express preferences across separate reviews - largely underexplored. We introduce SUDO, a novel dataset for implicit comparative opinion mining from same-user reviews, allowing reliable inference of user preferences even without explicit comparative cues. SUDO comprises 4,150 annotated review pairs (15,191 sentences) with a bi-level structure capturing aspect-level mentions and review-level preferences. We benchmark this task using two baseline architectures: traditional machine learning- and language model-based baselines. Experimental results show that while the latter outperforms the former, overall performance remains moderate, revealing the inherent difficulty of the task and establishing SUDO as a challenging and valuable benchmark for future research.

</details>


### [245] [TREX: Tokenizer Regression for Optimal Data Mixture](https://arxiv.org/abs/2601.13588)
*Inho Won,Hangyeol Yoo,Minkyung Cho,Jungyeul Park,Hoyun Song,KyungTae Lim*

Main category: cs.CL

TL;DR: The paper introduces TREX, a framework to efficiently find optimal data mixtures for training multilingual LLM tokenizers, demonstrating up to 12% better compression performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current multilingual LLM tokenizer designs rely on heuristics or expensive searches for determining optimal language ratios, which affect training and inference efficiency. A more efficient and accurate method is needed.

Method: TREX is a regression-based framework that trains small proxy tokenizers on random data mixtures, records compression statistics, and learns to predict compression performance. This model enables scalable mixture searches before full-scale tokenizer training.

Result: Tokenizers using TREX-predicted mixtures show up to 12% improved compression efficiency compared to other mixture strategies like LLaMA3 and uniform distribution, both in and out of distribution.

Conclusion: TREX provides a scalable and efficient approach to optimizing data mixtures for multilingual tokenizer training, offering significant performance improvements and practical effectiveness.

Abstract: Building effective tokenizers for multilingual Large Language Models (LLMs) requires careful control over language-specific data mixtures. While a tokenizer's compression performance critically affects the efficiency of LLM training and inference, existing approaches rely on heuristics or costly large-scale searches to determine optimal language ratios. We introduce Tokenizer Regression for Optimal Data MiXture (TREX), a regression-based framework that efficiently predicts the optimal data mixture for tokenizer training. TREX trains small-scale proxy tokenizers on random mixtures, gathers their compression statistics, and learns to predict compression performance from data mixtures. This learned model enables scalable mixture search before large-scale tokenizer training, mitigating the accuracy-cost trade-off in multilingual tokenizer design. Tokenizers trained with TReX's predicted mixtures outperform mixtures based on LLaMA3 and uniform distributions by up to 12% in both inand out-of-distribution compression efficiency, demonstrating strong scalability, robustness, and practical effectiveness.

</details>


### [246] [Vulnerability of LLMs' Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions](https://arxiv.org/abs/2601.13590)
*Fan Huang,Haewoon Kwak,Jisun An*

Main category: cs.CL

TL;DR: This paper investigates how Large Language Models (LLMs) can be persuaded to change their beliefs under systematic evaluation using communication frameworks, finding model-dependent susceptibility and limits in defense strategies.


<details>
  <summary>Details</summary>
Motivation: LLMs' susceptibility to persuasion poses risks to their reliability and trustworthiness in sensitive domains like factual knowledge, medical QA, and social bias.

Method: Examined persuasive strategies using the SMCR framework across five LLMs and three domains. Investigated meta-cognition prompting and adversarial fine-tuning as defense mechanisms against persuasion.

Result: Smaller models showed high susceptibility to persuasion after one interaction turn (80% belief changes). Meta-cognition prompting increased vulnerability. Fine-tuning improved robustness in some models like GPT-4o-mini (98.6%) and Mistral 7B (35.7% to 79.3%), but was less effective for Llama models (<14%).

Conclusion: LLMs exhibit significant variations in susceptibility to persuasion, with current robustness methods providing inconsistent protection. Improved approaches are needed to enhance their trustworthiness and resist undue belief changes.

Abstract: Large Language Models (LLMs) are increasingly employed in various question-answering tasks. However, recent studies showcase that LLMs are susceptible to persuasion and could adopt counterfactual beliefs. We present a systematic evaluation of LLM susceptibility to persuasion under the Source--Message--Channel--Receiver (SMCR) communication framework. Across five mainstream Large Language Models (LLMs) and three domains (factual knowledge, medical QA, and social bias), we analyze how different persuasive strategies influence belief stability over multiple interaction turns. We further examine whether meta-cognition prompting (i.e., eliciting self-reported confidence) affects resistance to persuasion. Results show that smaller models exhibit extreme compliance, with over 80% of belief changes occurring at the first persuasive turn (average end turn of 1.1--1.4). Contrary to expectations, meta-cognition prompting increases vulnerability by accelerating belief erosion rather than enhancing robustness. Finally, we evaluate adversarial fine-tuning as a defense. While GPT-4o-mini achieves near-complete robustness (98.6%) and Mistral~7B improves substantially (35.7% $\rightarrow$ 79.3%), Llama models remain highly susceptible (<14%) even when fine-tuned on their own failure cases. Together, these findings highlight substantial model-dependent limits of current robustness interventions and offer guidance for developing more trustworthy LLMs.

</details>


### [247] [CauScientist: Teaching LLMs to Respect Data for Causal Discovery](https://arxiv.org/abs/2601.13614)
*Bo Peng,Sirui Chen,Lei Xu,Chaochao Lu*

Main category: cs.CL

TL;DR: CauScientist is a collaborative framework combining LLMs and probabilistic statistics for improved causal discovery, achieving notable performance gains over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current causal discovery methods, such as statistical indistinguishability in data-driven approaches and unverified priors in LLM-based methods.

Method: Integrates LLMs for hypothesis generation and probabilistic statistics for validation. Uses hybrid initialization, iterative refinement, and error memory for efficient causal graph discovery.

Result: CauScientist outperformed data-driven baselines with up to 53.8% improvement in F1 score, 44.0% reduction in SHD on complex graphs, and substantial recall improvements.

Conclusion: Combining LLMs with statistical methods offers a powerful approach to causal discovery, overcoming pitfalls of individual techniques and achieving superior performance.

Abstract: Causal discovery is fundamental to scientific understanding and reliable decision-making. Existing approaches face critical limitations: purely data-driven methods suffer from statistical indistinguishability and modeling assumptions, while recent LLM-based methods either ignore statistical evidence or incorporate unverified priors that can mislead result. To this end, we propose CauScientist, a collaborative framework that synergizes LLMs as hypothesis-generating "data scientists" with probabilistic statistics as rigorous "verifiers". CauScientist employs hybrid initialization to select superior starting graphs, iteratively refines structures through LLM-proposed modifications validated by statistical criteria, and maintains error memory to guide efficient search space. Experiments demonstrate that CauScientist substantially outperforms purely data-driven baselines, achieving up to 53.8% F1 score improvement and enhancing recall from 35.0% to 100.0%. Notably, while standalone LLM performance degrades with graph complexity, CauScientist reduces structural hamming distance (SHD) by 44.0% compared to Qwen3-32B on 37-node graphs. Our project page is at https://github.com/OpenCausaLab/CauScientist.

</details>


### [248] [Activation-Space Anchored Access Control for Multi-Class Permission Reasoning in Large Language Models](https://arxiv.org/abs/2601.13630)
*Zhaopeng Zhang,Pengcheng Sun,Lan Zhang,Chen Tang,Jiewei Lai,Yunhao Wang,Hui Jin*

Main category: cs.CL

TL;DR: This paper tackles the issue of sensitive content leakage in knowledge-base QA systems enabled by LLMs and proposes a training-free, multi-class permission control framework (AAAC) to prevent unauthorized access while maintaining usability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of fine-grained access control for knowledge-base QA systems, as large language models may inadvertently provide unauthorized answers, risking sensitive content exposure.

Method: The authors identify a geometric regularity in representations induced by different permission scopes and propose AAAC. This framework utilizes a permission anchor bank built from offline samples and uses a multi-anchor steering mechanism to ensure queries stay within authorized regions at inference time, with no need for fine-tuning.

Result: AAAC achieves significant improvements in reducing permission violation rates by up to 86.5% and prompt-based attack success rates by 90.7%, while maintaining usability and introducing minimal inference overhead.

Conclusion: AAAC successfully provides fine-grained access control for LLM-based QA systems without additional training, enhancing security and usability in sensitive knowledge retrieval tasks.

Abstract: Large language models (LLMs) are increasingly deployed over knowledge bases for efficient knowledge retrieval and question answering. However, LLMs can inadvertently answer beyond a user's permission scope, leaking sensitive content, thus making it difficult to deploy knowledge-base QA under fine-grained access control requirements. In this work, we identify a geometric regularity in intermediate activations: for the same query, representations induced by different permission scopes cluster distinctly and are readily separable. Building on this separability, we propose Activation-space Anchored Access Control (AAAC), a training-free framework for multi-class permission control. AAAC constructs an anchor bank, with one permission anchor per class, from a small offline sample set and requires no fine-tuning. At inference time, a multi-anchor steering mechanism redirects each query's activations toward the anchor-defined authorized region associated with the current user, thereby suppressing over-privileged generations by design. Finally, extensive experiments across three LLM families demonstrate that AAAC reduces permission violation rates by up to 86.5% and prompt-based attack success rates by 90.7%, while improving response usability with minor inference overhead compared to baselines.

</details>


### [249] [Towards Token-Level Text Anomaly Detection](https://arxiv.org/abs/2601.13644)
*Yang Cao,Bicheng Yu,Sikun Yang,Ming Liu,Yujiu Yang*

Main category: cs.CL

TL;DR: The paper introduces token-level anomaly detection as a novel way to localize anomalies within text, overcoming the limitations of document-level approaches.


<details>
  <summary>Details</summary>
Motivation: Existing text anomaly detection methods are restricted to document-level analysis and lack the ability to pinpoint specific anomalous text parts, creating the need for a more precise approach.

Method: A unified detection framework is proposed, and three diverse datasets are collected and annotated with token-level labels. Experiments are conducted comparing their framework to six baselines.

Result: The framework achieves improved performance and enables precise anomaly localization, outperforming six baseline methods.

Conclusion: This research successfully introduces a token-level anomaly detection paradigm, providing both a framework and datasets for further work, and making their implementation publicly available.

Abstract: Despite significant progress in text anomaly detection for web applications such as spam filtering and fake news detection, existing methods are fundamentally limited to document-level analysis, unable to identify which specific parts of a text are anomalous. We introduce token-level anomaly detection, a novel paradigm that enables fine-grained localization of anomalies within text. We formally define text anomalies at both document and token-levels, and propose a unified detection framework that operates across multiple levels. To facilitate research in this direction, we collect and annotate three benchmark datasets spanning spam, reviews and grammar errors with token-level labels. Experimental results demonstrate that our framework get better performance than other 6 baselines, opening new possibilities for precise anomaly localization in text. All the codes and data are publicly available on https://github.com/charles-cao/TokenCore.

</details>


### [250] [Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge](https://arxiv.org/abs/2601.13649)
*Xiaolin Zhou,Zheng Luo,Yicheng Gao,Qixuan Chen,Xiyang Hu,Yue Zhao,Ruishan Liu*

Main category: cs.CL

TL;DR: The study explores language bias in Large Language Models (LLMs) used as judges, identifying significant disparities in performance between languages and a preference for English answers.


<details>
  <summary>Details</summary>
Motivation: To address biases in LLMs used as judges, particularly focusing on disparities in judgments based on the language of the texts.

Method: Researchers studied two types of language bias in pairwise judgments: comparing within the same language and across different languages. They also explored if language bias is related to low-perplexity bias.

Result: European languages outperform African languages in same-language judgments; major languages like English are favored in inter-language comparisons.

Conclusion: Language bias in LLMs exists and cannot fully be explained by low-perplexity bias alone, highlighting the need for addressing fairness across languages.

Abstract: Recent advances in Large Language Models (LLMs) have incentivized the development of LLM-as-a-judge, an application of LLMs where they are used as judges to decide the quality of a certain piece of text given a certain context. However, previous studies have demonstrated that LLM-as-a-judge can be biased towards different aspects of the judged texts, which often do not align with human preference. One of the identified biases is language bias, which indicates that the decision of LLM-as-a-judge can differ based on the language of the judged texts. In this paper, we study two types of language bias in pairwise LLM-as-a-judge: (1) performance disparity between languages when the judge is prompted to compare options from the same language, and (2) bias towards options written in major languages when the judge is prompted to compare options of two different languages. We find that for same-language judging, there exist significant performance disparities across language families, with European languages consistently outperforming African languages, and this bias is more pronounced in culturally-related subjects. For inter-language judging, we observe that most models favor English answers, and that this preference is influenced more by answer language than question language. Finally, we investigate whether language bias is in fact caused by low-perplexity bias, a previously identified bias of LLM-as-a-judge, and we find that while perplexity is slightly correlated with language bias, language bias cannot be fully explained by perplexity only.

</details>


### [251] [Beyond Known Facts: Generating Unseen Temporal Knowledge to Address Data Contamination in LLM Evaluation](https://arxiv.org/abs/2601.13658)
*Arthur Amalvy,Hen-Hsen Huang*

Main category: cs.CL

TL;DR: The paper introduces a novel synthetic evaluation dataset to address contamination in Temporal Knowledge Graph Extraction (TKGE) using plausible future temporal facts.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of datasets for TKGE and the issue of contamination in evaluations, which affects the accuracy of LLM performance benchmarks.

Method: A two-step approach: (1) Temporal Knowledge Graph Forecasting predicts plausible future temporal quadruples filtered for schema consistency; (2) LLMs generate textual descriptions for these quadruples.

Result: The proposed dataset highlights limitations in state-of-the-art LLM-based extraction approaches, showing reduced performance when tested on future temporal datasets compared to known facts.

Conclusion: The study provides a contamination-free benchmarking method for TKGE, along with a 4.2K quadruples dataset and an approach for generating sustainable temporal evaluations.

Abstract: The automatic extraction of information is important for populating large web knowledge bases such as Wikidata. The temporal version of that task, temporal knowledge graph extraction (TKGE), involves extracting temporally grounded facts from text, represented as semantic quadruples (subject, relation, object, timestamp). Many recent systems take advantage of large language models (LLMs), which are becoming a new cornerstone of the web due to their performance on many tasks across the natural language processing (NLP) field. Despite the importance of TKGE, existing datasets for training and evaluation remain scarce, and contamination of evaluation data is an unaddressed issue, potentially inflating LLMs' perceived performance due to overlaps between training and evaluation sets. To mitigate these challenges, we propose a novel synthetic evaluation dataset constructed from predicted future, previously unseen temporal facts, thereby eliminating contamination and enabling robust and unbiased benchmarking. Our dataset creation involves a two-step approach: (1) Temporal Knowledge Graph Forecasting (TKGF) generates plausible future quadruples, which are subsequently filtered to adhere to the original knowledge base schema; (2) LLMs perform quadruple-to-text generation, creating semantically aligned textual descriptions. We benchmark Extract, Define and Canonicalize (EDC), a state-of-the-art LLM-based extraction framework, demonstrating that LLM performance decreases when evaluated on our dataset compared to a dataset of known facts. We publicly release our dataset consisting of 4.2K future quadruples and corresponding textual descriptions, along with the generation methodology, enabling continuous creation of unlimited future temporal datasets to serve as long-term, contamination-free benchmarks for TKGE.

</details>


### [252] [Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis](https://arxiv.org/abs/2601.13659)
*Chunlei Meng,Ziyang Zhou,Lucas He,Xiaojing Du,Chun Ouyang,Zhongxue Gan*

Main category: cs.CL

TL;DR: This paper proposes TSDA, a new method for multimodal sentiment analysis that separates temporal and spatial information before interaction, improving alignment and performance.


<details>
  <summary>Details</summary>
Motivation: Current approaches for multimodal sentiment analysis ignore the distinct temporal and spatial characteristics of data, resulting in uneven alignment across modalities and limited effectiveness.

Method: The proposed TSDA method decouples each modality into temporal and spatial components using encoders and aligns these components with their corresponding counterparts across modalities. It employs factor-specific supervision, decorrelation regularization, and a gated recouple module for improved integration.

Result: TSDA outperforms existing methods in multimodal sentiment analysis tasks, as demonstrated through extensive experiments and ablation studies that verify its design's necessity and interpretability.

Conclusion: Explicit temporal-spatial decoupling and alignment improve multimodal sentiment analysis performance, providing a promising approach for handling mixed modality data.

Abstract: Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic. Mainstream approaches based on modality-invariant and modality-specific factorization or on complex fusion still rely on spatiotemporal mixed modeling. This ignores spatiotemporal heterogeneity, leading to spatiotemporal information asymmetry and thus limited performance. Hence, we propose TSDA, Temporal-Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. For every modality, a temporal encoder and a spatial encoder project signals into separate temporal and spatial body. Factor-Consistent Cross-Modal Alignment then aligns temporal features only with their temporal counterparts across modalities, and spatial features only with their spatial counterparts. Factor specific supervision and decorrelation regularization reduce cross factor leakage while preserving complementarity. A Gated Recouple module subsequently recouples the aligned streams for task. Extensive experiments show that TSDA outperforms baselines. Ablation analysis studies confirm the necessity and interpretability of the design.

</details>


### [253] [CommunityBench: Benchmarking Community-Level Alignment across Diverse Groups and Tasks](https://arxiv.org/abs/2601.13669)
*Jiayu Lin,Zhongyu Wei*

Main category: cs.CL

TL;DR: This paper proposes community-level alignment as a middle ground approach for aligning large language models (LLMs) to human values, evaluated using a new benchmark, CommunityBench.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs to reflect human values poses the challenge of balancing between universal value modeling (which marginalizes minority norms) and individual-level customization (which is costly). Recognizing humans' social clustering, the authors aim to develop a practical middle ground approach.

Method: The authors propose community-level alignment and create CommunityBench, a large-scale benchmark based on Common Identity and Common Bond theory, to evaluate LLMs on their capacity for community-specific value modeling.

Result: Evaluations using CommunityBench reveal that current LLMs struggle with community-specific alignment. The study also explores the scalability and pluralistic potential of community-level alignment for better individual modeling.

Conclusion: Community-level alignment offers a scalable and inclusive approach to aligning LLMs with human values, bridging the gap between universal and individual alignment strategies.

Abstract: Large language models (LLMs) alignment ensures model behaviors reflect human value. Existing alignment strategies primarily follow two paths: one assumes a universal value set for a unified goal (i.e., one-size-fits-all), while the other treats every individual as unique to customize models (i.e., individual-level). However, assuming a monolithic value space marginalizes minority norms, while tailoring individual models is prohibitively expensive. Recognizing that human society is organized into social clusters with high intra-group value alignment, we propose community-level alignment as a "middle ground". Practically, we introduce CommunityBench, the first large-scale benchmark for community-level alignment evaluation, featuring four tasks grounded in Common Identity and Common Bond theory. With CommunityBench, we conduct a comprehensive evaluation of various foundation models on CommunityBench, revealing that current LLMs exhibit limited capacity to model community-specific preferences. Furthermore, we investigate the potential of community-level alignment in facilitating individual modeling, providing a promising direction for scalable and pluralistic alignment.

</details>


### [254] [HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache Compression for Long-Context LLM Inference](https://arxiv.org/abs/2601.13684)
*Zhiyuan Shi,Qibo Qiu,Feng Xue,Zhonglin Jiang,Li Yu,Jian Jiang,Xiaofei He,Wenxiao Wang*

Main category: cs.CL

TL;DR: The paper addresses the limitations of linear memory growth in KV cache for long-context tasks in LLMs by introducing HeteroCache, a training-free dynamic compression framework.


<details>
  <summary>Details</summary>
Motivation: Linear memory growth in KV cache for long-context tasks is a bottleneck for LLM inference. Existing methods fail to manage information dynamically or reduce I/O overhead effectively.

Method: The authors introduce HeteroCache, leveraging diverse temporal heterogeneity and spatial redundancy of attention heads. It employs fine-grained weighting and a hierarchical storage mechanism to optimize memory and reduce I/O latency.

Result: HeteroCache achieves state-of-the-art performance on long-context benchmarks and accelerates decoding by up to 3× in the 224K context.

Conclusion: The proposed HeteroCache framework offers an efficient solution for dynamic memory management in LLMs, demonstrating superior performance and decoding speed. The code will be made publicly available.

Abstract: The linear memory growth of the KV cache poses a significant bottleneck for LLM inference in long-context tasks. Existing static compression methods often fail to preserve globally important information, principally because they overlook the attention drift phenomenon where token significance evolves dynamically. Although recent dynamic retrieval approaches attempt to address this issue, they typically suffer from coarse-grained caching strategies and incur high I/O overhead due to frequent data transfers. To overcome these limitations, we propose HeteroCache, a training-free dynamic compression framework. Our method is built on two key insights: attention heads exhibit diverse temporal heterogeneity, and there is significant spatial redundancy among heads within the same layer. Guided by these insights, HeteroCache categorizes heads based on stability and redundancy. Consequently, we apply a fine-grained weighting strategy that allocates larger cache budgets to heads with rapidly shifting attention to capture context changes, thereby addressing the inefficiency of coarse-grained strategies. Furthermore, we employ a hierarchical storage mechanism in which a subset of representative heads monitors attention shift, and trigger an asynchronous, on-demand retrieval of contexts from the CPU, effectively hiding I/O latency. Finally, experiments demonstrate that HeteroCache achieves state-of-the-art performance on multiple long-context benchmarks and accelerates decoding by up to $3\times$ compared to the original model in the 224K context. Our code will be open-source.

</details>


### [255] [Dr. Assistant: Enhancing Clinical Diagnostic Inquiry via Structured Diagnostic Reasoning Data and Reinforcement Learning](https://arxiv.org/abs/2601.13690)
*Yue Guo,Fanfu Wang,Jianwei Lv,Xincheng Shi,Yuchen Li,Youya Wang,Yunsheng Zeng,Yujing Liu,Yunhao Qiao,Gen Li,Junfeng Wang,Bo Yuan*

Main category: cs.CL

TL;DR: The paper introduces the Dr. Assistant, a clinical diagnostic model designed to improve reasoning and inquiry in healthcare, outperforming open-source models and rivaling closed-source ones.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of CDSSs such as high maintenance costs and lack of generalization, and to enhance the diagnostic reasoning and inquiry capabilities of LLMs in healthcare.

Method: The authors propose the Clinical Diagnostic Reasoning Data (CDRD) structure, a pipeline to construct it, and train Dr. Assistant using a two-stage process involving SFT and RL with a tailored reward function. They also introduce a benchmark for evaluation.

Result: Dr. Assistant surpasses open-source models and achieves competitive performance with closed-source models in diagnostic reasoning and inquiry.

Conclusion: Dr. Assistant is an effective tool that enhances clinical diagnostic inquiry guidance, addressing key challenges in existing systems.

Abstract: Clinical Decision Support Systems (CDSSs) provide reasoning and inquiry guidance for physicians, yet they face notable challenges, including high maintenance costs and low generalization capability. Recently, Large Language Models (LLMs) have been widely adopted in healthcare due to their extensive knowledge reserves, retrieval, and communication capabilities. While LLMs show promise and excel at medical benchmarks, their diagnostic reasoning and inquiry skills are constrained. To mitigate this issue, we propose (1) Clinical Diagnostic Reasoning Data (CDRD) structure to capture abstract clinical reasoning logic, and a pipeline for its construction, and (2) the Dr. Assistant, a clinical diagnostic model equipped with clinical reasoning and inquiry skills. Its training involves a two-stage process: SFT, followed by RL with a tailored reward function. We also introduce a benchmark to evaluate both diagnostic reasoning and inquiry. Our experiments demonstrate that the Dr. Assistant outperforms open-source models and achieves competitive performance to closed-source models, providing an effective solution for clinical diagnostic inquiry guidance.

</details>


### [256] [OptiSQL: Executable SQL Generation from Optical TokensOptiSQL: Executable SQL Generation from Optical Tokens](https://arxiv.org/abs/2601.13695)
*Sifan Li,Hongkai Chen,Yujun Cai,Liyang Chen,Qingwen Ye,Yiwei Wang*

Main category: cs.CL

TL;DR: OptiSQL proposes a method to generate executable SQL from table images and text queries using visual inputs, reducing token overhead while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency and mismatch of traditional text-to-SQL methods in real-world scenarios where tables are visual, not text-based.

Method: A vision-driven framework uses an OCR-oriented visual encoder to compress table visuals into optical tokens, paired with a pretrained decoder for SQL generation.

Result: OptiSQL achieves strong execution accuracy with significantly fewer tokens and remains robust under visual distortions.

Conclusion: OptiSQL demonstrates that compact optical inputs can serve as an efficient and reliable interface for semantic parsing in vision-based scenarios.

Abstract: Executable SQL generation is typically studied in text-to-SQL settings, where tables are provided as fully linearized textual schemas and contents. While effective, this formulation assumes access to structured text and incurs substantial token overhead, which is misaligned with many real-world scenarios where tables appear as visual artifacts in documents or webpages. We investigate whether compact optical representations can serve as an efficient interface for executable semantic parsing. We present OptiSQL, a vision-driven framework that generates executable SQL directly from table images and natural language questions using compact optical tokens. OptiSQL leverages an OCR-oriented visual encoder to compress table structure and content into a small set of optical tokens and fine-tunes a pretrained decoder for SQL generation while freezing the encoder to isolate representation sufficiency. Experiments on a visualized version of Spider 2.0-Snow show that OptiSQL retains strong execution accuracy while reducing table input tokens by an order of magnitude. Robustness analyses further demonstrate that optical tokens preserve essential structural information under visual perturbations.

</details>


### [257] [Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning](https://arxiv.org/abs/2601.13697)
*Zhihang Yuan,Chengyu Yue,Long Huang,Litu Ou,Lei Shi*

Main category: cs.CL

TL;DR: GRADFILTERING introduces an uncertainty-aware method to select useful data subsets for instruction tuning large language models, improving efficiency and outcomes.


<details>
  <summary>Details</summary>
Motivation: Modern large instruction datasets are often noisy and redundant, making full-data fine-tuning inefficient. There's a need for smarter data selection methods that consider evolving uncertainties in LLM evaluation.

Method: GRADFILTERING employs a small GPT-2 proxy model with a LoRA ensemble, using gradient information to assign data utility scores called Gradient Signal-to-Noise Ratio (G-SNR) for data selection.

Result: GRADFILTERING matches or outperforms random subsets and existing strong baselines in both LLM-as-a-judge evaluations and human assessments. It also ensures faster convergence within fixed compute budgets.

Conclusion: This framework provides a cost-efficient and effective approach to identifying high-utility instruction datasets for LLM tuning, leveraging uncertainty for improved data selection and training outcomes.

Abstract: Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.

</details>


### [258] [GerAV: Towards New Heights in German Authorship Verification using Fine-Tuned LLMs on a New Benchmark](https://arxiv.org/abs/2601.13711)
*Lotta Kiefer,Christoph Leiter,Sotaro Takeshita,Elena Schmidt,Steffen Eger*

Main category: cs.CL

TL;DR: The paper introduces GerAV, a German authorship verification benchmark consisting of over 600k text pairs, enabling comprehensive evaluations across various domains and data types.


<details>
  <summary>Details</summary>
Motivation: To address the gap in large-scale authorship verification studies for languages other than English, particularly for German.

Method: Developed a benchmark dataset (GerAV) from German Twitter and Reddit sources, conducted systematic evaluations using strong baselines and fine-tuned large language models.

Result: Fine-tuned large language models achieved the best performance, surpassing recent baselines by 0.09 F1 score and GPT-5 in zero-shot settings by 0.08, while exposing generalization challenges across data types.

Conclusion: GerAV is a robust, versatile benchmark that allows researchers to advance cross-domain and domain-specific authorship verification in German.

Abstract: Authorship verification (AV) is the task of determining whether two texts were written by the same author and has been studied extensively, predominantly for English data. In contrast, large-scale benchmarks and systematic evaluations for other languages remain scarce. We address this gap by introducing GerAV, a comprehensive benchmark for German AV comprising over 600k labeled text pairs. GerAV is built from Twitter and Reddit data, with the Reddit part further divided into in-domain and cross-domain message-based subsets, as well as a profile-based subset. This design enables controlled analysis of the effects of data source, topical domain, and text length. Using the provided training splits, we conduct a systematic evaluation of strong baselines and state-of-the-art models and find that our best approach, a fine-tuned large language model, outperforms recent baselines by up to 0.09 absolute F1 score and surpasses GPT-5 in a zero-shot setting by 0.08. We further observe a trade-off between specialization and generalization: models trained on specific data types perform best under matching conditions but generalize less well across data regimes, a limitation that can be mitigated by combining training sources. Overall, GerAV provides a challenging and versatile benchmark for advancing research on German and cross-domain AV.

</details>


### [259] [Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff](https://arxiv.org/abs/2601.13717)
*Zehan Li,Yuxuan Wang,Ali El Lahib,Ying-Jieh Xia,Xinyu Pi*

Main category: cs.CL

TL;DR: This paper evaluates whether Simulated Ignorance (SI) can effectively approximate True Ignorance (TI) for retrospective forecasting, concluding that SI prompts are unreliable for suppressing pre-cutoff knowledge.


<details>
  <summary>Details</summary>
Motivation: To tackle limitations in evaluating LLMs' forecasting abilities, specifically the constraints of latency in prospective methods and the shrinking of clean data for retrospective forecasting.

Method: Tested Simulated Ignorance against True Ignorance across 477 competition-level questions and 9 models to see if SI could suppress pre-cutoff knowledge effectively.

Result: SI systematically fails to replicate TI: a 52% performance gap exists, chain-of-thought reasoning does not suppress prior knowledge, and reasoning-optimized models show worse SI fidelity.

Conclusion: Retrospective forecasting using SI-based methods is flawed; such setups should not be used to benchmark forecasting capabilities.

Abstract: Evaluating LLM forecasting capabilities is constrained by a fundamental tension: prospective evaluation offers methodological rigor but prohibitive latency, while retrospective forecasting (RF) -- evaluating on already-resolved events -- faces rapidly shrinking clean evaluation data as SOTA models possess increasingly recent knowledge cutoffs. Simulated Ignorance (SI), prompting models to suppress pre-cutoff knowledge, has emerged as a potential solution. We provide the first systematic test of whether SI can approximate True Ignorance (TI). Across 477 competition-level questions and 9 models, we find that SI fails systematically: (1) cutoff instructions leave a 52% performance gap between SI and TI; (2) chain-of-thought reasoning fails to suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references; (3) reasoning-optimized models exhibit worse SI fidelity despite superior reasoning trace quality. These findings demonstrate that prompts cannot reliably "rewind" model knowledge. We conclude that RF on pre-cutoff events is methodologically flawed; we recommend against using SI-based retrospective setups to benchmark forecasting capabilities.

</details>


### [260] [OP-Bench: Benchmarking Over-Personalization for Memory-Augmented Personalized Conversational Agents](https://arxiv.org/abs/2601.13722)
*Yulin Hu,Zimo Long,Jiahe Guo,Xingyu Sui,Xing Fu,Weixiang Zhao,Yanyan Zhao,Bing Qin*

Main category: cs.CL

TL;DR: This paper introduces the concept of over-personalization in memory-augmented conversational agents, develops a benchmark called OP-Bench, and proposes a method, Self-ReCheck, to mitigate the issue.


<details>
  <summary>Details</summary>
Motivation: To address the issue of conversational agents overusing personal information, which results in responses that feel intrusive, socially inappropriate, or forced.

Method: The authors formalize over-personalization into three types (Irrelevance, Repetition, Sycophancy), construct the OP-Bench benchmark with verified instances, and propose Self-ReCheck, a memory filtering mechanism.

Result: The study finds prevalent over-personalization in current memory-augmented agents and demonstrates that Self-ReCheck reduces this problem effectively while maintaining good personalization.

Conclusion: Over-personalization is a significant challenge for memory-augmented conversational agents; Self-ReCheck provides a viable solution to balance personalization and appropriate response generation.

Abstract: Memory-augmented conversational agents enable personalized interactions using long-term user memory and have gained substantial traction. However, existing benchmarks primarily focus on whether agents can recall and apply user information, while overlooking whether such personalization is used appropriately. In fact, agents may overuse personal information, producing responses that feel forced, intrusive, or socially inappropriate to users. We refer to this issue as \emph{over-personalization}. In this work, we formalize over-personalization into three types: Irrelevance, Repetition, and Sycophancy, and introduce \textbf{OP-Bench} a benchmark of 1,700 verified instances constructed from long-horizon dialogue histories. Using \textbf{OP-Bench}, we evaluate multiple large language models and memory-augmentation methods, and find that over-personalization is widespread when memory is introduced. Further analysis reveals that agents tend to retrieve and over-attend to user memories even when unnecessary. To address this issue, we propose \textbf{Self-ReCheck}, a lightweight, model-agnostic memory filtering mechanism that mitigates over-personalization while preserving personalization performance. Our work takes an initial step toward more controllable and appropriate personalization in memory-augmented dialogue systems.

</details>


### [261] [On Temperature-Constrained Non-Deterministic Machine Translation: Potential and Evaluation](https://arxiv.org/abs/2601.13729)
*Weichuan Wang,Mingyang Liu,Linqi Song,Chen Ma*

Main category: cs.CL

TL;DR: This paper explores Non-Deterministic Machine Translation (ND-MT), identifies its benefits, challenges, and proposes a new evaluation strategy.


<details>
  <summary>Details</summary>
Motivation: To address the under-exploration of non-deterministic properties in machine translation and identify its potential to resolve the multi-modality issue in translation tasks.

Method: The study evaluates five state-of-the-art ND-MT systems across three datasets, using various metrics and introduces a new strategy called ExpectoSample for reliable evaluation.

Result: Identified the Buckets effect where the lowest-quality ND-MT candidate impacts system rankings and showed ND-MT’s ability to produce higher-quality candidates under constraints.

Conclusion: ND-MT offers promising solutions for machine translation but requires new evaluation frameworks like ExpectoSample to overcome inherent challenges.

Abstract: In recent years, the non-deterministic properties of language models have garnered considerable attention and have shown a significant influence on real-world applications. However, such properties remain under-explored in machine translation (MT), a complex, non-deterministic NLP task. In this study, we systematically evaluate modern MT systems and identify temperature-constrained Non-Deterministic MT (ND-MT) as a distinct phenomenon. Additionally, we demonstrate that ND-MT exhibits significant potential in addressing the multi-modality issue that has long challenged MT research and provides higher-quality candidates than Deterministic MT (D-MT) under temperature constraints. However, ND-MT introduces new challenges in evaluating system performance. Specifically, the evaluation framework designed for D-MT fails to yield consistent evaluation results when applied to ND-MT. We further investigate this emerging challenge by evaluating five state-of-the-art ND-MT systems across three open datasets using both lexical-based and semantic-based metrics at varying sampling sizes. The results reveal a Buckets effect across these systems: the lowest-quality candidate generated by ND-MT consistently determines the overall system ranking across different sampling sizes for all reasonable metrics. Furthermore, we propose the ExpectoSample strategy to automatically assess the reliability of evaluation metrics for selecting robust ND-MT.

</details>


### [262] [Towards robust long-context understanding of large language model via active recap learning](https://arxiv.org/abs/2601.13734)
*Chenyu Hui*

Main category: cs.CL

TL;DR: This paper introduces Active Recap Learning (ARL) to enhance large language models' (LLMs) understanding of long contexts by enabling revisiting and summarizing content during pretraining and inference.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge LLMs face in comprehending lengthy contexts, improving their ability to process and retain extended sequences of information.

Method: ARL identifies key tokens using loss gaps, retrieves relevant preceding paragraphs, summarizes them with LLMs, and integrates retrospective summaries during inference to create a recursive memory mechanism.

Result: ARL demonstrates a 26.8% improvement on RULER and a 9.44% improvement on LongBench, showcasing significant advancements in long-context understanding capability.

Conclusion: ARL presents a straightforward, continued pretraining-based approach that enhances scalable memory augmentation in LLMs, providing effective solutions for long-context understanding.

Abstract: In this paper, we propose active recap learning (ARL), a framework for enhancing large language model (LLM) in understanding long contexts. ARL enables models to revisit and summarize earlier content through targeted sequence construction during contined pretraining and retrospective summarization at inference. First, we identify key tokens in prepared long context based on loss gaps between long and short forward contexts and find most revant preceding paragraphs, then summarize them using an LLM. Second, ARL equips models with the ability to autonomously generate and utilize these retrospective summaries during inference, thereby establishing a recursive memory mechanism across paragraphs. Experimental results show substantial gains, with ARL achieving a 26.8% improvement on RULER and a 9.44% improvement on LongBench. Overall, ARL offers a simple yet effective continued pretraining-based approach to strengthen long-context understanding, advancing scalable memory augmentation in LLM

</details>


### [263] [Dimension-First Evaluation of Speech-to-Speech Models with Structured Acoustic Cues](https://arxiv.org/abs/2601.13742)
*Arjun Chandra,Kevin Miller,Venkatesh Ravichandran,Constantinos Papayiannis,Venkatesh Saligrama*

Main category: cs.CL

TL;DR: The paper introduces TRACE, a framework that enables textual-focused LLMs to evaluate speech-to-speech quality by creating text-based representations of audio cues, reducing reliance on costly Audio Language Models (ALMs).


<details>
  <summary>Details</summary>
Motivation: Existing speech evaluation methods rely on opaque and expensive ALMs, and there is a need for cost-efficient, human-aligned evaluation systems leveraging LLMs.

Method: The authors propose TRACE, which translates audio signals into textual blueprints. These blueprints are evaluated by LLMs using a new Human Chain-of-Thought (HCoT) annotation protocol across dimensions like content, voice quality, and paralinguistics. Final evaluations are fused using deterministic rules.

Result: TRACE outperforms ALMs and LLM judges relying only on transcripts in terms of agreement with human evaluations, while being more cost-efficient.

Conclusion: TRACE represents a scalable, effective, and cost-efficient approach for speech-to-speech evaluation. The release of HCoT annotations and TRACE aims to further support the community.

Abstract: Large Language Model (LLM) judges exhibit strong reasoning capabilities but are limited to textual content. This leaves current automatic Speech-to-Speech (S2S) evaluation methods reliant on opaque and expensive Audio Language Models (ALMs). In this work, we propose TRACE (Textual Reasoning over Audio Cues for Evaluation), a novel framework that enables LLM judges to reason over audio cues to achieve cost-efficient and human-aligned S2S evaluation. To demonstrate the strength of the framework, we first introduce a Human Chain-of-Thought (HCoT) annotation protocol to improve the diagnostic capability of existing judge benchmarks by separating evaluation into explicit dimensions: content (C), voice quality (VQ), and paralinguistics (P). Using this data, TRACE constructs a textual blueprint of inexpensive audio signals and prompts an LLM to render dimension-wise judgments, fusing them into an overall rating via a deterministic policy. TRACE achieves higher agreement with human raters than ALMs and transcript-only LLM judges while being significantly more cost-effective. We will release the HCoT annotations and the TRACE framework to enable scalable and human-aligned S2S evaluation.

</details>


### [264] [Pro-AI Bias in Large Language Models](https://arxiv.org/abs/2601.13749)
*Benaya Trabelsi,Jonathan Shaki,Sarit Kraus*

Main category: cs.CL

TL;DR: The paper investigates pro-AI bias in large language models, finding systematic preference and overestimation related to AI across different experiments.


<details>
  <summary>Details</summary>
Motivation: Concerns are rising about potential systematic biases in LLMs that might influence high-stakes decisions, particularly around the favoring of artificial intelligence.

Method: Three experiments were conducted: advice-seeking queries showing AI-related preferential recommendations, salary estimation comparisons for AI vs. non-AI jobs, and probing representational similarity metrics of AI within LLMs.

Result: Evidence across experiments showed consistent pro-AI bias, such as deterministic recommendations, salary overestimations by proprietary models, and representational centrality of AI, irrespective of framing.

Conclusion: LLMs' pro-AI bias can systematically influence decision-support systems, potentially skewing choices and perception in critical domains.

Abstract: Large language models (LLMs) are increasingly employed for decision-support across multiple domains. We investigate whether these models display a systematic preferential bias in favor of artificial intelligence (AI) itself. Across three complementary experiments, we find consistent evidence of pro-AI bias. First, we show that LLMs disproportionately recommend AI-related options in response to diverse advice-seeking queries, with proprietary models doing so almost deterministically. Second, we demonstrate that models systematically overestimate salaries for AI-related jobs relative to closely matched non-AI jobs, with proprietary models overestimating AI salaries more by 10 percentage points. Finally, probing internal representations of open-weight models reveals that ``Artificial Intelligence'' exhibits the highest similarity to generic prompts for academic fields under positive, negative, and neutral framings alike, indicating valence-invariant representational centrality. These patterns suggest that LLM-generated advice and valuation can systematically skew choices and perceptions in high-stakes decisions.

</details>


### [265] [Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis](https://arxiv.org/abs/2601.13802)
*Yushen Chen,Junzhe Liu,Yujie Tu,Zhikang Niu,Yuzhe Liang,Kai Yu,Chunyu Qiang,Chen Zhang,Xie Chen*

Main category: cs.CL

TL;DR: This paper introduces 'Habibi,' a suite of models for Arabic dialect text-to-speech synthesis, leveraging open-source data and curriculum learning to address linguistic complexities.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the gap in speech synthesis for Arabic dialects, caused by linguistic complexity, lack of data, and absence of benchmarks, hindering research and development.

Method: The authors propose a unified approach by using open-source ASR corpora, linguistically-informed curriculum learning, and extensible in-context learning for Arabic dialects.

Result: The proposed system surpasses commercial text-to-speech services in quality, upholding its versatility without text diacritization.

Conclusion: The study offers significant contributions by creating systematic benchmarks, setting evaluation standards for Arabic dialect synthesis, and intending to open-source the model for advancing future research.

Abstract: A notable gap persists in speech synthesis research and development for Arabic dialects, particularly from a unified modeling perspective. Despite its high practical value, the inherent linguistic complexity of Arabic dialects, further compounded by a lack of standardized data, benchmarks, and evaluation guidelines, steers researchers toward safer ground. To bridge this divide, we present Habibi, a suite of specialized and unified text-to-speech models that harnesses existing open-source ASR corpora to support a wide range of high- to low-resource Arabic dialects through linguistically-informed curriculum learning. Our approach outperforms the leading commercial service in generation quality, while maintaining extensibility through effective in-context learning, without requiring text diacritization. We are committed to open-sourcing the model, along with creating the first systematic benchmark for multi-dialect Arabic speech synthesis. Furthermore, by identifying the key challenges in and establishing evaluation standards for the process, we aim to provide a solid groundwork for subsequent research. Resources at https://SWivid.github.io/Habibi/ .

</details>


### [266] [Knowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning](https://arxiv.org/abs/2601.13806)
*Dezhao Song,Guglielmo Bonifazi,Frank Schilder,Jonathan Richard Schwarz*

Main category: cs.CL

TL;DR: The paper introduces a novel knowledge graph (KG)-assisted approach to enhance legal reasoning in large language models (LLMs), incorporating domain-specific frameworks and achieving superior performance on legal benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM post-training does not adequately capture the structured domain knowledge required for complex reasoning tasks, especially in high-stakes fields like Law.

Method: The authors leverage a knowledge graph based on the IRAC (Issue, Rule, Analysis, Conclusion) framework to model legal concepts, construct training data from 12K legal cases, and fine-tune LLMs using Supervised Fine-Tuning and Direct Preference Optimization.

Result: The approach led to improved average performance on 4 out of 5 diverse legal benchmarks (14 tasks). Specifically, a 70B DPO model outperformed baselines and a 141B SOTA legal LLM on 4 out of 6 reasoning tasks.

Conclusion: Using a knowledge graph grounded in domain-specific reasoning frameworks successfully enhances LLMs' reasoning capabilities in legal tasks, and the method is generalizable to other high-stakes domains.

Abstract: LLM post-training has primarily relied on large text corpora and human feedback, without capturing the structure of domain knowledge. This has caused models to struggle dealing with complex reasoning tasks, especially for high-stakes professional domains. In Law, reasoning requires deep understanding of the relations between various legal concepts, a key component missing in current LLM post-training. In this paper, we propose a knowledge graph (KG)-assisted approach for enhancing LLMs' reasoning capability in Legal that is generalizable to other high-stakes domains. We model key legal concepts by following the \textbf{IRAC} (Issue, Rule, Analysis and Conclusion) framework, and construct a KG with 12K legal cases. We then produce training data using our IRAC KG, and conduct both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) with three state-of-the-art (SOTA) LLMs (30B, 49B and 70B), varying architecture and base model family. Our post-trained models obtained better average performance on 4/5 diverse legal benchmarks (14 tasks) than baselines. In particular, our 70B DPO model achieved the best score on 4/6 reasoning tasks, among baselines and a 141B SOTA legal LLM, demonstrating the effectiveness of our KG for enhancing LLMs' legal reasoning capability.

</details>


### [267] [The Role of Prosodic and Lexical Cues in Turn-Taking with Self-Supervised Speech Representations](https://arxiv.org/abs/2601.13835)
*Sam OConnor Russell,Delphine Charuau,Naomi Harte*

Main category: cs.CL

TL;DR: The paper explores the use of prosodic and lexical cues in self-supervised speech representation (S3R)-based turn-taking models in human-robot interaction, finding that either cue can support turn-taking independently.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of fluid turn-taking in human-robot interaction, investigating whether S3R-based models use prosodic, lexical or both types of cues.

Method: The authors introduced a vocoder-based method to manipulate prosodic and lexical speech cues, then used this approach to evaluate an S3R-based turn-taking model's reliance on these cues.

Result: It was found that both prosody and lexical cues contribute to turn-taking, but they can be utilized independently. Moreover, the model can exploit one cue when the other is disrupted, demonstrating their independence in S3Rs.

Conclusion: Future turn-taking models might exclusively use prosodic cues for enhanced privacy and performance. The findings highlight opportunities for advancing turn-taking systems and provide open-source resources for further research.

Abstract: Fluid turn-taking remains a key challenge in human-robot interaction. Self-supervised speech representations (S3Rs) have driven many advances, but it remains unclear whether S3R-based turn-taking models rely on prosodic cues, lexical cues or both. We introduce a vocoder-based approach to control prosody and lexical cues in speech more cleanly than prior work. This allows us to probe the voice-activity projection model, an S3R-based turn-taking model. We find that prediction on prosody-matched, unintelligible noise is similar to accuracy on clean speech. This reveals both prosodic and lexical cues support turn-taking, but either can be used in isolation. Hence, future models may only require prosody, providing privacy and potential performance benefits. When either prosodic or lexical information is disrupted, the model exploits the other without further training, indicating they are encoded in S3Rs with limited interdependence. Results are consistent in CPC-based and wav2vec2.0 S3Rs. We discuss our findings and highlight a number of directions for future work. All code is available to support future research.

</details>


### [268] [FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs](https://arxiv.org/abs/2601.13836)
*Qian Chen,Jinlan Fu,Changsong Li,See-Kiong Ng,Xipeng Qiu*

Main category: cs.CL

TL;DR: The paper introduces a benchmark named FutureOmni to evaluate the future forecasting abilities of Multimodal Large Language Models (MLLMs) from audio-visual environments and proposes a training strategy (OFF) to improve such predictions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of evaluation benchmarks for MLLMs in future event forecasting from audio-visual cues, as existing frameworks mostly focus on understanding retrospective information.

Method: The authors created the FutureOmni benchmark via a scalable LLM-assisted, human-in-the-loop process, incorporating 919 videos and 1,034 QA pairs across 8 domains. Additionally, they proposed the Omni-Modal Future Forecasting (OFF) training strategy with a 7K-sample instruction-tuning dataset to enhance performance.

Result: FutureOmni evaluations on 13 MLLMs and 7 video-only models revealed poor performance in audio-visual future prediction, especially in speech-intensive settings. OFF significantly improved forecasting accuracy and generalization on FutureOmni and other benchmarks.

Conclusion: Current MLLMs face challenges in audio-visual future event prediction. The FutureOmni benchmark and the OFF training strategy offer scalable solutions to improve omni-modal reasoning, setting a foundation for further research.

Abstract: Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).

</details>


### [269] [Pedagogical Alignment for Vision-Language-Action Models: A Comprehensive Framework for Data, Architecture, and Evaluation in Education](https://arxiv.org/abs/2601.13876)
*Unggi Lee,Jahyun Jeong,Sunyoung Shin,Haeun Park,Jeongsu Moon,Youngchang Song,Jaechang Shim,JaeHwan Lee,Yunju Noh,Seungwon Choi,Ahhyun Kim,TaeHyeon Kim,Kyungtae Joo,Taeyeong Kim,Gyeonggeon Lee*

Main category: cs.CL

TL;DR: The paper introduces the Pedagogical VLA Framework aimed at enhancing science demonstrations in resource-constrained educational settings by improving task performance and generating educational explanations.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced by teachers in conducting safe and consistent science demonstrations while overcoming the limitations of current VLA models in resource-constrained environments.

Method: The framework integrates four components: text healing for language generation, LLM distillation for pedagogical knowledge transfer, safety training, and pedagogical evaluation tailored to science contexts.

Result: The framework achieves comparable performance to baseline VLA models while producing contextually appropriate explanations, assessed through expert-designed evaluation methods and teacher feedback.

Conclusion: Pedagogical VLA Framework successfully balances task performance with educational explanation generation, making it suitable for STEM education in constrained settings.

Abstract: Science demonstrations are important for effective STEM education, yet teachers face challenges in conducting them safely and consistently across multiple occasions, where robotics can be helpful. However, current Vision-Language-Action (VLA) models require substantial computational resources and sacrifice language generation capabilities to maximize efficiency, making them unsuitable for resource-constrained educational settings that require interpretable, explanation-generating systems. We present \textit{Pedagogical VLA Framework}, a framework that applies pedagogical alignment to lightweight VLA models through four components: text healing to restore language generation capabilities, large language model (LLM) distillation to transfer pedagogical knowledge, safety training for educational environments, and pedagogical evaluation adjusted to science education contexts. We evaluate Pedagogical VLA Framework across five science demonstrations spanning physics, chemistry, biology, and earth science, using an evaluation framework developed in collaboration with science education experts. Our evaluation assesses both task performance (success rate, protocol compliance, efficiency, safety) and pedagogical quality through teacher surveys and LLM-as-Judge assessment. We additionally provide qualitative analysis of generated texts. Experimental results demonstrate that Pedagogical VLA Framework achieves comparable task performance to baseline models while producing contextually appropriate educational explanations.

</details>


### [270] [OpenLearnLM Benchmark: A Unified Framework for Evaluating Knowledge, Skill, and Attitude in Educational Large Language Models](https://arxiv.org/abs/2601.13882)
*Unggi Lee,Sookbun Lee,Heungsoo Choi,Jinseo Lee,Haeun Park,Younghoon Jeon,Sungmin Cho,Minju Kang,Junbo Koh,Jiyeong Bae,Minwoo Nam,Juyeon Eun,Yeonji Jung,Yeil Jeong*

Main category: cs.CL

TL;DR: This paper presents OpenLearnLM, a benchmark evaluating large language models (LLMs) in education using theory-grounded metrics across knowledge, skills, and attitude dimensions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs in education are narrow and lack grounding in learning sciences, necessitating a comprehensive and scientifically justified assessment framework.

Method: The method involves developing a benchmark with 124K+ items structured by educational domains and theories, prioritizing curriculum-aligned content and incorporating tools for measuring alignment consistency.

Result: Evaluation across different LLMs revealed varied capability profiles, indicating that no model excels across all educational dimensions. Claude-Opus-4.5 outperforms in skills, while Grok-4.1-fast excels in knowledge but struggles with alignment issues.

Conclusion: A multi-axis evaluation approach is critical to improve LLMs for educational readiness, and OpenLearnLM offers a detailed, open framework for this purpose.

Abstract: Large Language Models are increasingly deployed as educational tools, yet existing benchmarks focus on narrow skills and lack grounding in learning sciences. We introduce OpenLearnLM Benchmark, a theory-grounded framework evaluating LLMs across three dimensions derived from educational assessment theory: Knowledge (curriculum-aligned content and pedagogical understanding), Skills (scenario-based competencies organized through a four-level center-role-scenario-subscenario hierarchy), and Attitude (alignment consistency and deception resistance). Our benchmark comprises 124K+ items spanning multiple subjects, educational roles, and difficulty levels based on Bloom's taxonomy. The Knowledge domain prioritizes authentic assessment items from established benchmarks, while the Attitude domain adapts Anthropic's Alignment Faking methodology to detect behavioral inconsistency under varying monitoring conditions. Evaluation of seven frontier models reveals distinct capability profiles: Claude-Opus-4.5 excels in practical skills despite lower content knowledge, while Grok-4.1-fast leads in knowledge but shows alignment concerns. Notably, no single model dominates all dimensions, validating the necessity of multi-axis evaluation. OpenLearnLM provides an open, comprehensive framework for advancing LLM readiness in authentic educational contexts.

</details>


### [271] [Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores](https://arxiv.org/abs/2601.13885)
*Esma Balkır,Alice Pernthaller,Marco Basaldella,José Hernández-Orallo,Nigel Collier*

Main category: cs.CL

TL;DR: The paper extends adaptive testing methods from discrete to continuous scoring for efficient evaluation of large language models (LLMs) and introduces an uncertainty-aware ranking system.


<details>
  <summary>Details</summary>
Motivation: Modern LLM evaluations increasingly rely on generation tasks with continuously scored outputs rather than simple correctness judgments on multiple-choice items.

Method: The paper replaces prior Bernoulli response distributions with heteroskedastic normal distributions for continuous scoring. Additionally, it introduces an uncertainty-aware ranker with adaptive stopping criteria for efficient evaluation.

Result: The proposed method reduced the required items to 2%, improved ranking correlation by 0.12 τ compared to random sampling, and achieved 95% accuracy on confident predictions across five benchmarks.

Conclusion: The methodology refined LLM evaluation processes, providing cost-effective, scalable, and robust model ranking solutions suitable for continuous scoring contexts.

Abstract: Computerized Adaptive Testing (CAT) has proven effective for efficient LLM evaluation on multiple-choice benchmarks, but modern LLM evaluation increasingly relies on generation tasks where outputs are scored continuously rather than marked correct/incorrect. We present a principled extension of IRT-based adaptive testing to continuous bounded scores (ROUGE, BLEU, LLM-as-a-Judge) by replacing the Bernoulli response distribution with a heteroskedastic normal distribution. Building on this, we introduce an uncertainty aware ranker with adaptive stopping criteria that achieves reliable model ranking while testing as few items and as cheaply as possible. We validate our method on five benchmarks spanning n-gram-based, embedding-based, and LLM-as-judge metrics. Our method uses 2% of the items while improving ranking correlation by 0.12 τ over random sampling, with 95% accuracy on confident predictions.

</details>


### [272] [AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization](https://arxiv.org/abs/2601.13918)
*Yusheng Liao,Chuan Xuan,Yutong Cai,Lina Yang,Zhe Chen,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: Large Language Models struggle with realistic EHR navigation. AgentEHR benchmark challenges agents with complex medical tasks requiring reasoning in noisy datasets. RetroSum framework integrates retrospective summarization and evolving strategies, improving task performance and lowering errors.


<details>
  <summary>Details</summary>
Motivation: To address the limitations Large Language Models face with autonomous navigation and reasoning in raw, noisy Electronic Health Records datasets involving complex clinical tasks.

Method: The proposed method, RetroSum, combines retrospective summarization for maintaining logical coherence and evolving experience strategies to adapt using a memory bank during interactions.

Result: RetroSum achieves up to 29.16% performance improvement over baselines and lowers interaction errors by 92.3% in extensive evaluations.

Conclusion: By bridging gaps between experimental idealism and clinical realism, RetroSum greatly enhances autonomous reasoning and decision-making capabilities in navigating Electronic Health Records.

Abstract: Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.

</details>


### [273] [HyperWalker: Dynamic Hypergraph-Based Deep Diagnosis for Multi-Hop Clinical Modeling across EHR and X-Ray in Medical VLMs](https://arxiv.org/abs/2601.13919)
*Yuezhe Yang,Hao Wang,Yige Peng,Jinman Kim,Lei Bi*

Main category: cs.CL

TL;DR: HyperWalker is a new AI framework improving automated clinical diagnosis by using dynamic hypergraphs and test-time training strategies. It achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current medical AI models lack integration of longitudinal electronic health records (EHRs) and complementary evidence, limiting their diagnostic reasoning capabilities.

Method: HyperWalker constructs a dynamic hypergraph (iBrochure) to model EHR data associations and employs a reinforcement learning agent (Walker) for diagnostic path navigation. It includes a linger mechanism for retrieving complementary cases.

Result: HyperWalker shows state-of-the-art performance in medical report generation (MRG) and medical visual question answering (VQA) tasks tested on MIMIC and EHRXQA datasets.

Conclusion: Dynamic hypergraph modeling and enhanced test-time strategies can overcome current limitations in medical AI diagnosis, providing improved reasoning and accuracy.

Abstract: Automated clinical diagnosis remains a core challenge in medical AI, which usually requires models to integrate multi-modal data and reason across complex, case-specific contexts. Although recent methods have advanced medical report generation (MRG) and visual question answering (VQA) with medical vision-language models (VLMs), these methods, however, predominantly operate under a sample-isolated inference paradigm, as such processing cases independently without access to longitudinal electronic health records (EHRs) or structurally related patient examples. This paradigm limits reasoning to image-derived information alone, which ignores external complementary medical evidence for potentially more accurate diagnosis. To overcome this limitation, we propose \textbf{HyperWalker}, a \textit{Deep Diagnosis} framework that reformulates clinical reasoning via dynamic hypergraphs and test-time training. First, we construct a dynamic hypergraph, termed \textbf{iBrochure}, to model the structural heterogeneity of EHR data and implicit high-order associations among multimodal clinical information. Within this hypergraph, a reinforcement learning agent, \textbf{Walker}, navigates to and identifies optimal diagnostic paths. To ensure comprehensive coverage of diverse clinical characteristics in test samples, we incorporate a \textit{linger mechanism}, a multi-hop orthogonal retrieval strategy that iteratively selects clinically complementary neighborhood cases reflecting distinct clinical attributes. Experiments on MRG with MIMIC and medical VQA on EHRXQA demonstrate that HyperWalker achieves state-of-the-art performance. Code is available at: https://github.com/Bean-Young/HyperWalker

</details>


### [274] [Automatic Prompt Optimization for Dataset-Level Feature Discovery](https://arxiv.org/abs/2601.13922)
*Adrian Cosma,Oleg Szehr,David Kletz,Alessandro Antonucci,Olivier Pelletier*

Main category: cs.CL

TL;DR: The paper introduces a framework for optimizing feature discovery in text classification pipelines using multi-agent prompt optimization, focusing on dataset-level rather than per-example predictions.


<details>
  <summary>Details</summary>
Motivation: Existing text feature extraction methods heavily rely on hand-crafted prompts or fixed schemas, limiting flexibility and automation.

Method: Proposes a multi-agent prompt optimization framework that refines instruction prompts using language-model agents to discover interpretable and discriminative feature definitions at a dataset level.

Result: The framework enables automatic discovery of high-quality feature definitions by optimizing prompts using structured performance and interpretability feedback.

Conclusion: This approach provides a principled way to automate feature extraction from unstructured text, departing from traditional per-sample methods.

Abstract: Feature extraction from unstructured text is a critical step in many downstream classification pipelines, yet current approaches largely rely on hand-crafted prompts or fixed feature schemas. We formulate feature discovery as a dataset-level prompt optimization problem: given a labelled text corpus, the goal is to induce a global set of interpretable and discriminative feature definitions whose realizations optimize a downstream supervised learning objective. To this end, we propose a multi-agent prompt optimization framework in which language-model agents jointly propose feature definitions, extract feature values, and evaluate feature quality using dataset-level performance and interpretability feedback. Instruction prompts are iteratively refined based on this structured feedback, enabling optimization over prompts that induce shared feature sets rather than per-example predictions. This formulation departs from prior prompt optimization methods that rely on per-sample supervision and provides a principled mechanism for automatic feature discovery from unstructured text.

</details>


### [275] ["The Whole Is Greater Than the Sum of Its Parts": A Compatibility-Aware Multi-Teacher CoT Distillation Framework](https://arxiv.org/abs/2601.13992)
*Jin Cui,Jiaqi Guo,Jiepeng Zhou,Ruixuan Yang,Jiayi Lu,Jiajun Xu,Jiangcheng Song,Boran Zhao,Pengju Ren*

Main category: cs.CL

TL;DR: COMPACT is a framework designed to enhance student models by smartly integrating reasoning capabilities from multiple diverse teachers, overcoming traditional limitations in reasoning skill transfer to smaller models.


<details>
  <summary>Details</summary>
Motivation: To tackle the inefficiencies and challenges in transferring reasoning abilities from large language models (LLMs) to smaller, compact student models (SLMs), especially given the limitations of relying on a single teacher and risks of knowledge loss.

Method: COMPACT uses an adaptive framework that evaluates and selectively fuses teacher supervisions based on a dynamic multi-dimensional metric involving Graph-based Consensus, Mutual-Information-based Adaptability, and Loss-based Difficulty.

Result: COMPACT achieves superior reasoning capability integration without compromising the model's pre-existing knowledge, demonstrating state-of-the-art results across benchmarks and alleviating catastrophic forgetting.

Conclusion: The approach effectively addresses challenges in multi-teacher knowledge distillation for compact models, providing better reasoning performance while maintaining student model stability and avoiding negative impacts.

Abstract: Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with remarkable capabilities but typically requires prohibitive parameter scales. CoT distillation has emerged as a promising paradigm to transfer reasoning prowess into compact Student Models (SLMs), but existing approaches often rely on a solitary teacher, capping the student's potential since individual LLMs often exhibit distinct capability biases and may suffer from catastrophic forgetting. While leveraging diverse teachers seems appealing, effectively fusing their supervisions remains challenging: teacher-student incompatibility risks amplifying hallucinations, and passive supervision fails to ensure genuine logic internalization. To address this, we introduce COMPACT, a framework that adaptively fuses supervisions from different teachers by dynamically weighting teacher gradients based on the student's real-time compatibility evaluated by a multi-dimensional metric: (1) Graph-based Consensus to filter misleading rationales by identifying mainstream reasoning paths; (2) Mutual-Information-based Adaptability to detect "epiphany moments" for genuinely understanding the reasoning process rather than merely imitating; and (3) Loss-based Difficulty to assess student receptivity to the teacher's guidance and prevent negative transfer. Extensive experiments and latent space analysis demonstrate that COMPACT effectively integrates diverse reasoning capabilities without damaging the model's original knowledge structure, achieving state-of-the-art performance on various benchmarks while mitigating catastrophic forgetting.

</details>


### [276] [From Tags to Trees: Structuring Fine-Grained Knowledge for Controllable Data Selection in LLM Instruction Tuning](https://arxiv.org/abs/2601.13995)
*Zihan Niu,Wenping Hu,Junmin Chen,Xiyue Wang,Tong Xu,Ruiming Tang*

Main category: cs.CL

TL;DR: This paper introduces TAGS, a framework leveraging a knowledge tree for better LLM instruction tuning by improving data quality, diversity, and alignment.


<details>
  <summary>Details</summary>
Motivation: Current data selection methods for LLM training overlook fine-grained knowledge and its hierarchical dependencies due to limitations in embedding spaces and semantic tagging strategies.

Method: TAGS employs a LLM-based tagger to extract atomic knowledge tags and organizes them into a hierarchical tree structure. A tree-aware metric quantifies data quality and diversity for sampling purposes.

Result: TAGS outperforms baseline approaches, achieving +5.84% improvement using only 5% of the data, with further gains (+4.24%) from alignment optimization.

Conclusion: The proposed TAGS framework achieves effective and controllable data selection, enabling precise tuning of LLMs while reducing the required dataset size.

Abstract: Effective and controllable data selection is critical for LLM instruction tuning, especially with massive open-source datasets. Existing approaches primarily rely on instance-level quality scores, or diversity metrics based on embedding clusters or semantic tags. However, constrained by the flatness of embedding spaces or the coarseness of tags, these approaches overlook fine-grained knowledge and its intrinsic hierarchical dependencies, consequently hindering precise data valuation and knowledge-aligned sampling. To address this challenge, we propose Tree-aware Aligned Global Sampling (TAGS), a unified framework that leverages a knowledge tree built from fine-grained tags, thereby enabling joint control of global quality, diversity, and target alignment. Using an LLM-based tagger, we extract atomic knowledge concepts, which are organized into a global tree through bottom-up hierarchical clustering. By grounding data instances onto this tree, a tree-aware metric then quantifies data quality and diversity, facilitating effective sampling. Our controllable sampling strategy maximizes tree-level information gain and enforces leaf-level alignment via KL-divergence for specific domains. Extensive experiments demonstrate that TAGS significantly outperforms state-of-the-art baselines. Notably, it surpasses the full-dataset model by \textbf{+5.84\%} using only \textbf{5\%} of the data, while our aligned sampling strategy further boosts average performance by \textbf{+4.24\%}.

</details>


### [277] [Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models](https://arxiv.org/abs/2601.14004)
*Hengyuan Zhang,Zhihao Zhang,Mingyang Wang,Zunhai Su,Yiwei Wang,Qianli Wang,Shuzhou Yuan,Ercong Nie,Xufeng Duan,Qibo Xue,Zeping Yu,Chenming Shang,Xiao Liang,Jing Xiong,Hui Shen,Chaofan Tao,Zhengwu Liu,Senjie Jin,Zhiheng Xi,Dongdong Zhang,Sophia Ananiadou,Tao Gui,Ruobing Xie,Hayden Kwok-Hay So,Hinrich Schütze,Xuanjing Huang,Qi Zhang,Ngai Wong*

Main category: cs.CL

TL;DR: A framework is proposed to make Mechanistic Interpretability (MI) more practical and actionable for improving Large Language Models.


<details>
  <summary>Details</summary>
Motivation: Address the gap in Mechanistic Interpretability research by providing a systematic framework for actionable intervention rather than mere observational insights.

Method: Introduced a pipeline framework—'Locate, Steer, and Improve'—to diagnose, intervene, and optimize LLMs systematically, structured around interpretable objects.

Result: Demonstrated that the framework can improve alignment, capability, and efficiency of LLMs.

Conclusion: This work operationalizes MI as a methodology to actively optimize models, bridging the gap between theoretical insights and practical applications.

Abstract: Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: "Locate, Steer, and Improve." We formally categorize Localizing (diagnosis) and Steering (intervention) methods based on specific Interpretable Objects to establish a rigorous intervention protocol. Furthermore, we demonstrate how this framework enables tangible improvements in Alignment, Capability, and Efficiency, effectively operationalizing MI as an actionable methodology for model optimization. The curated paper list of this work is available at https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey.

</details>


### [278] [BACH-V: Bridging Abstract and Concrete Human-Values in Large Language Models](https://arxiv.org/abs/2601.14007)
*Junyu Zhang,Yipeng Kang,Jiong Guo,Jiayu Zhan,Junqi Wang*

Main category: cs.CL

TL;DR: The paper investigates whether large language models (LLMs) truly understand abstract concepts or just manipulate them as patterns, introducing a framework to evaluate conceptual understanding based on abstraction-grounding capacities.


<details>
  <summary>Details</summary>
Motivation: To assess whether large language models genuinely understand and manipulate abstract concepts meaningfully, especially when applied to human values essential for AI alignment.

Method: The study introduces an abstraction-grounding framework, conducts probing to detect value traces, and applies steering to observe behavioral shifts across six LLMs on ten value dimensions.

Result: Probing revealed reliable cross-level value transfer between abstract descriptions and concrete narratives, while steering showed causal shifts in concrete judgments without altering abstract interpretations, indicating stable structured value representations in LLMs.

Conclusion: LLMs hold structured representations that connect abstract concepts to concrete actions, creating a foundation for alignment and control in autonomous AI systems driven by values.

Abstract: Do large language models (LLMs) genuinely understand abstract concepts, or merely manipulate them as statistical patterns? We introduce an abstraction-grounding framework that decomposes conceptual understanding into three capacities: interpretation of abstract concepts (Abstract-Abstract, A-A), grounding of abstractions in concrete events (Abstract-Concrete, A-C), and application of abstract principles to regulate concrete decisions (Concrete-Concrete, C-C). Using human values as a testbed - given their semantic richness and centrality to alignment - we employ probing (detecting value traces in internal activations) and steering (modifying representations to shift behavior). Across six open-source LLMs and ten value dimensions, probing shows that diagnostic probes trained solely on abstract value descriptions reliably detect the same values in concrete event narratives and decision reasoning, demonstrating cross-level transfer. Steering reveals an asymmetry: intervening on value representations causally shifts concrete judgments and decisions (A-C, C-C), yet leaves abstract interpretations unchanged (A-A), suggesting that encoded abstract values function as stable anchors rather than malleable activations. These findings indicate LLMs maintain structured value representations that bridge abstraction and action, providing a mechanistic and operational foundation for building value-driven autonomous AI systems with more transparent, generalizable alignment and control.

</details>


### [279] [RM-Distiller: Exploiting Generative LLM for Reward Model Distillation](https://arxiv.org/abs/2601.14032)
*Hongli Zhou,Hui Huang,Wei Liu,Chenglong Wang,Xingyuan Bu,Lvyuan Han,Fuhai Song,Muyun Yang,Wenhao Jiang,Hailong Cao,Tiejun Zhao*

Main category: cs.CL

TL;DR: The paper introduces RM-Distiller, a novel framework enhancing reward model (RM) distillation from generative large language models (LLMs) by leveraging their multifaceted capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of current distillation methods treating LLMs as binary annotators, underutilizing their capabilities for fine-tuning RMs.

Method: The RM-Distiller leverages LLMs' multifaceted capabilities through three key mechanisms: (1) Refinement for fine-grained preference signals, (2) Scoring for capturing preference strength, and (3) Generation to preserve linguistic knowledge.

Result: RM-Distiller outperformed traditional methods on RM benchmarks and reinforcement learning-based tasks, showcasing the importance of exploiting LLM capabilities for competitive reward modeling.

Conclusion: This research establishes the significance of systematically utilizing the capabilities of generative LLMs for effective RM distillation and addresses a novel gap in alignment methodologies.

Abstract: Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. Due to the difficulty of obtaining high-quality human preference annotations, distilling preferences from generative LLMs has emerged as a standard practice. However, existing approaches predominantly treat teacher models as simple binary annotators, failing to fully exploit the rich knowledge and capabilities for RM distillation. To address this, we propose RM-Distiller, a framework designed to systematically exploit the multifaceted capabilities of teacher LLMs: (1) Refinement capability, which synthesizes highly correlated response pairs to create fine-grained and contrastive signals. (2) Scoring capability, which guides the RM in capturing precise preference strength via a margin-aware optimization objective. (3) Generation capability, which incorporates the teacher's generative distribution to regularize the RM to preserve its fundamental linguistic knowledge. Extensive experiments demonstrate that RM-Distiller significantly outperforms traditional distillation methods both on RM benchmarks and reinforcement learning-based alignment, proving that exploiting multifaceted teacher capabilities is critical for effective reward modeling. To the best of our knowledge, this is the first systematic research on RM distillation from generative LLMs.

</details>


### [280] [Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants](https://arxiv.org/abs/2601.14041)
*Yunhe Wang,Kai Han,Huiling Zhen,Yuchuan Tian,Hanting Chen,Yongbing Huang,Yufei Cui,Yingte Shu,Shan Gao,Ismail Elezi,Roy Vaughan Miles,Songcen Xu,Feng Wen,Chao Xu,Sinan Zeng,Dacheng Tao*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) using auto-regressive (AR) architectures face limitations in global structural foresight. Diffusion Language Models (DLMs) could address these issues but are hindered by AR-based frameworks. This paper outlines challenges and proposes a roadmap to evolve DLM capacities.


<details>
  <summary>Details</summary>
Motivation: Address the inherent limitations of auto-regressive architectures in LLMs and unlock the potential of Diffusion Language Models for better structural reasoning and multimodal integration.

Method: Identifies ten challenges in DLM development and proposes a roadmap based on four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence.

Result: Recognizes existing challenges like architectural inertia and gradient sparsity in DLMs and outlines a path to overcome them with strategies such as multi-scale tokenization and active remasking.

Conclusion: Transitioning to a diffusion-native ecosystem is crucial to develop the next generation of AI that is more capable of reasoning, self-correction, and multimodal understanding.

Abstract: The paradigm of Large Language Models (LLMs) is currently defined by auto-regressive (AR) architectures, which generate text through a sequential ``brick-by-brick'' process. Despite their success, AR models are inherently constrained by a causal bottleneck that limits global structural foresight and iterative refinement. Diffusion Language Models (DLMs) offer a transformative alternative, conceptualizing text generation as a holistic, bidirectional denoising process akin to a sculptor refining a masterpiece. However, the potential of DLMs remains largely untapped as they are frequently confined within AR-legacy infrastructures and optimization frameworks. In this Perspective, we identify ten fundamental challenges ranging from architectural inertia and gradient sparsity to the limitations of linear reasoning that prevent DLMs from reaching their ``GPT-4 moment''. We propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. By shifting toward a diffusion-native ecosystem characterized by multi-scale tokenization, active remasking, and latent thinking, we can move beyond the constraints of the causal horizon. We argue that this transition is essential for developing next-generation AI capable of complex structural reasoning, dynamic self-correction, and seamless multimodal integration.

</details>


### [281] [PRiSM: Benchmarking Phone Realization in Speech Models](https://arxiv.org/abs/2601.14046)
*Shikhar Bharadwaj,Chin-Jou Li,Yoonjae Kim,Kwanghee Choi,Eunjung Yeo,Ryan Soh-Eun Shim,Hanyu Zhou,Brendon Boldt,Karen Rosero Jacome,Kalvin Chang,Darsh Agrawal,Keer Xu,Chao-Han Huck Yang,Jian Zhu,Shinji Watanabe,David R. Mortensen*

Main category: cs.CL

TL;DR: PRiSM is an open-source benchmark for evaluating phone recognition (PR) systems beyond transcription accuracy, focusing on linguistic versatility and application in clinical, educational, and multilingual contexts.


<details>
  <summary>Details</summary>
Motivation: Current PR system evaluations are limited to transcription accuracy, creating a need for more comprehensive methods to test phonetic and multilingual capabilities.

Method: PRiSM evaluates PR systems through transcription-based performance and downstream application utility, using a blend of intrinsic (within-system) and extrinsic (real-world) assessments.

Result: Key findings include the importance of diverse language training, encoder-CTC models offering stability, and specialized PR systems surpassing large audio language models in accuracy.

Conclusion: PRiSM provides a pathway to enhance multilingual speech models with improved phonetic functionalities and releases resources for continued research advancement.

Abstract: Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.

</details>


### [282] [Understanding Multilingualism in Mixture-of-Experts LLMs: Routing Mechanism, Expert Specialization, and Layerwise Steering](https://arxiv.org/abs/2601.14050)
*Yuxin Chen,Zhengzhou Cai,Xiangtian Ji,Weixiang Zhao,An Zhang,Xiang Wang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: This paper investigates how Mixture-of-Experts (MoE) models handle multilingual tasks, reveals structured multilingual processing, and proposes a new method to enhance routing for better performance.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying mechanisms of performance gains and differences in cross-language capabilities in Mixture-of-Experts (MoE) architectures.

Method: A systematic analysis of routing behavior and expert specialization across languages and network depth, as well as a routing-guided steering method to adjust middle-layer routing toward shared experts.

Result: The study found structured multilingual processing in MoE models. Key insights include routing alignment with linguistic families and specific functional layers. The proposed method improved multilingual performance, especially in related languages.

Conclusion: Mixture-of-Experts models process languages in a highly structured manner, and adaptive routing adjustments can improve model performance on multilingual tasks.

Abstract: Mixture-of-Experts (MoE) architectures have shown strong multilingual capabilities, yet the internal mechanisms underlying performance gains and cross-language differences remain insufficiently understood. In this work, we conduct a systematic analysis of MoE models, examining routing behavior and expert specialization across languages and network depth. Our analysis reveals that multilingual processing in MoE models is highly structured: routing aligns with linguistic families, expert utilization follows a clear layerwise pattern, and high-resource languages rely on shared experts while low-resource languages depend more on language-exclusive experts despite weaker performance. Layerwise interventions further show that early and late MoE layers support language-specific processing, whereas middle layers serve as language-agnostic capacity hubs. Building on these insights, we propose a routing-guided steering method that adaptively guides routing behavior in middle layers toward shared experts associated with dominant languages at inference time, leading to consistent multilingual performance improvements, particularly for linguistically related language pairs. Our code is available at https://github.com/conctsai/Multilingualism-in-Mixture-of-Experts-LLMs.

</details>


### [283] [Kakugo: Distillation of Low-Resource Languages into Small Language Models](https://arxiv.org/abs/2601.14051)
*Peter Devine,Mardhiyah Sanni,Farid Adilazuarda,Julieta Gil Loizaga,Barry Haddow*

Main category: cs.CL

TL;DR: Kakugo is a pipeline to train Small Language Models for low-resource languages using minimal input, improving performance in various NLP tasks at a low cost.


<details>
  <summary>Details</summary>
Motivation: To provide a cost-effective method for developing AI models for low-resource languages, enhancing accessibility and performance.

Method: Uses a large teacher model to create synthetic data and translate instruction datasets to train SLMs for 54 languages.

Result: Demonstrates improved performance in translation, classification, and question answering tasks compared to base models.

Conclusion: Kakugo enables communities to develop effective language-specific AI at under $50 per language, making it highly accessible.

Abstract: We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By using a large teacher model to generate synthetic prompts and translate instruction datasets, we produced training data and SLMs for 54 low-resource languages. Evaluations across a diverse set of general natural language processing tasks, including translation, classification, and question answering, demonstrate that our pipeline consistently improves performance over base models. With a total generation and training cost of under $50 per language, Kakugo offers an accessible method for communities to develop language-specific AI.

</details>


### [284] [XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs](https://arxiv.org/abs/2601.14063)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Shaoxiong Ji,Hassan Alhuzali,Sophia Ananiadou*

Main category: cs.CL

TL;DR: The study introduces XCR-Bench, a dataset focused on evaluating large language models (LLMs) on cross-cultural competence, highlighting their struggles with cultural adaptation and biases.


<details>
  <summary>Details</summary>
Motivation: There is a lack of high-quality corpora for evaluating LLMs on cross-cultural reasoning and adaptation, particularly annotated datasets focused on Culture-Specific Items (CSIs).

Method: Developed XCR-Bench, consisting of CSI-annotated parallel corpora across three reasoning tasks, combining Newmark's CSI framework with Hall's Triad of Culture for deeper analysis.

Result: LLMs demonstrate significant shortcomings in identifying and adapting CSIs, particularly regarding social etiquette and cultural references. Biases tied to region and ethno-religion persist even within linguistically homogenous contexts.

Conclusion: Releasing XCR-Bench and associated resources aims to further research in cross-cultural NLP, addressing cultural reasoning flaws and promoting improved adaptability in LLMs.

Abstract: Cross-cultural competence in large language models (LLMs) requires the ability to identify Culture-Specific Items (CSIs) and to adapt them appropriately across cultural contexts. Progress in evaluating this capability has been constrained by the scarcity of high-quality CSI-annotated corpora with parallel cross-cultural sentence pairs. To address this limitation, we introduce XCR-Bench, a Cross(X)-Cultural Reasoning Benchmark consisting of 4.9k parallel sentences and 1,098 unique CSIs, spanning three distinct reasoning tasks with corresponding evaluation metrics. Our corpus integrates Newmark's CSI framework with Hall's Triad of Culture, enabling systematic analysis of cultural reasoning beyond surface-level artifacts and into semi-visible and invisible cultural elements such as social norms, beliefs, and values. Our findings show that state-of-the-art LLMs exhibit consistent weaknesses in identifying and adapting CSIs related to social etiquette and cultural reference. Additionally, we find evidence that LLMs encode regional and ethno-religious biases even within a single linguistic setting during cultural adaptation. We release our corpus and code to facilitate future research on cross-cultural NLP.

</details>


### [285] [Truth with a Twist: The Rhetoric of Persuasion in Professional vs. Community-Authored Fact-Checks](https://arxiv.org/abs/2601.14105)
*Olesya Razuvayevskaya,Kalina Bontcheva*

Main category: cs.CL

TL;DR: The study compares persuasion techniques in crowd-written and professional debunks, finding no evidence that crowd-produced debunks use more persuasive language.


<details>
  <summary>Details</summary>
Motivation: Investigate the differences between community-written debunks and professionally-written checks regarding persuasion techniques and effectiveness.

Method: Analyze datasets from Community Notes, EUvsDisinfo, and Database of Known Fakes to quantify and compare persuasion techniques used.

Result: Crowd-produced debunks do not contain more persuasive elements than professional ones, but systematic rhetorical differences exist due to coverage and norms.

Conclusion: Although crowd-written debunks may use persuasive language occasionally, crowd raters penalize problematic rhetoric, maintaining effectiveness in debunking efforts.

Abstract: This study presents the first large-scale comparison of persuasion techniques present in crowd- versus professionally-written debunks. Using extensive datasets from Community Notes (CNs), EUvsDisinfo, and the Database of Known Fakes (DBKF), we quantify the prevalence and types of persuasion techniques across these fact-checking ecosystems. Contrary to prior hypothesis that community-produced debunks rely more heavily on subjective or persuasive wording, we find no evidence that CNs contain a higher average number of persuasion techniques than professional fact-checks. We additionally identify systematic rhetorical differences between CNs and professional debunking efforts, reflecting differences in institutional norms and topical coverage. Finally, we examine how the crowd evaluates persuasive language in CNs and show that, although notes with more persuasive elements receive slightly higher overall helpfulness ratings, crowd raters are effective at penalising the use of particular problematic rhetorical means

</details>


### [286] [Learning to Explain: Supervised Token Attribution from Transformer Attention Patterns](https://arxiv.org/abs/2601.14112)
*George Mihaila*

Main category: cs.CL

TL;DR: Explainable AI is crucial for high-stakes applications. This paper introduces Explanation Network (ExpNet) to optimize interpretable mechanisms in transformer models automatically.


<details>
  <summary>Details</summary>
Motivation: Opacity in transformer-based models limits trust and accountability in critical fields like healthcare, legal, and financial services. Enhanced model interpretability is needed.

Method: ExpNet, a neural network, maps attention patterns into token-level importance scores. It automatically discovers optimal attention features without manual rules.

Result: ExpNet was evaluated in cross-task scenarios, proving effective compared to model-agnostic and attention-based methods.

Conclusion: ExpNet offers an efficient and automated solution for interpretable transformer models, addressing limitations in existing XAI techniques.

Abstract: Explainable AI (XAI) has become critical as transformer-based models are deployed in high-stakes applications including healthcare, legal systems, and financial services, where opacity hinders trust and accountability. Transformers self-attention mechanisms have proven valuable for model interpretability, with attention weights successfully used to understand model focus and behavior (Xu et al., 2015); (Wiegreffe and Pinter, 2019). However, existing attention-based explanation methods rely on manually defined aggregation strategies and fixed attribution rules (Abnar and Zuidema, 2020a); (Chefer et al., 2021), while model-agnostic approaches (LIME, SHAP) treat the model as a black box and incur significant computational costs through input perturbation. We introduce Explanation Network (ExpNet), a lightweight neural network that learns an explicit mapping from transformer attention patterns to token-level importance scores. Unlike prior methods, ExpNet discovers optimal attention feature combinations automatically rather than relying on predetermined rules. We evaluate ExpNet in a challenging cross-task setting and benchmark it against a broad spectrum of model-agnostic methods and attention-based techniques spanning four methodological families.

</details>


### [287] [NewsRECON: News article REtrieval for image CONtextualization](https://arxiv.org/abs/2601.14121)
*Jonathan Tonglet,Iryna Gurevych,Tinne Tuytelaars,Marie-Francine Moens*

Main category: cs.CL

TL;DR: The paper introduces NewsRECON, a method for linking images to news articles to infer the date and location without relying on reverse image search (RIS) engines.


<details>
  <summary>Details</summary>
Motivation: The motivation is to assist journalists and forensic experts in identifying the time and location of news images, especially when RIS engines fail to provide evidence.

Method: NewsRECON integrates a corpus of over 90,000 articles using a bi-encoder for retrieving event-relevant articles, along with cross-encoders that rank articles based on location and event consistency.

Result: The experiments on two datasets, TARA and 5Pils-OOC, demonstrate that NewsRECON outperforms prior methods and achieves state-of-the-art accuracy in scenarios without RIS evidence.

Conclusion: NewsRECON provides a robust alternative to RIS engines for identifying image metadata, proving its practical utility for fighting misinformation. The code is made available for further use.

Abstract: Identifying when and where a news image was taken is crucial for journalists and forensic experts to produce credible stories and debunk misinformation. While many existing methods rely on reverse image search (RIS) engines, these tools often fail to return results, thereby limiting their practical applicability. In this work, we address the challenging scenario where RIS evidence is unavailable. We introduce NewsRECON, a method that links images to relevant news articles to infer their date and location from article metadata. NewsRECON leverages a corpus of over 90,000 articles and integrates: (1) a bi-encoder for retrieving event-relevant articles; (2) two cross-encoders for reranking articles by location and event consistency. Experiments on the TARA and 5Pils-OOC show that NewsRECON outperforms prior work and can be combined with a multimodal large language model to achieve new SOTA results in the absence of RIS evidence. We make our code available.

</details>


### [288] [A Systematic Analysis of Chunking Strategies for Reliable Question Answering](https://arxiv.org/abs/2601.14123)
*Sofia Bennani,Charles Moslonka*

Main category: cs.CL

TL;DR: This study examines how different document chunking methods impact the effectiveness of Retrieval-Augmented Generation (RAG) systems, providing practical insights for cost-efficient deployment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the ad-hoc use of heuristics in document chunking for RAG systems by systematically studying how various chunking parameters influence reliability and cost in an industrial setup.

Method: Using an industrial RAG setup (SPLADE retrieval and Mistral-8B generator), the study tests the effects of chunking choices—including method, size, overlap, and context length—on system performance using the Natural Questions dataset.

Result: Key findings include: (i) overlap is unnecessary and costly, (ii) sentence chunking is most cost-efficient, (iii) context length should not exceed ~2.5k tokens, and (iv) optimal context varies by goal (semantic vs. exact match).

Conclusion: The paper provides actionable lessons for deploying RAG systems efficiently, showing that sentence chunking and proper context optimization can balance cost and performance.

Abstract: We study how document chunking choices impact the reliability of Retrieval-Augmented Generation (RAG) systems in industry. While practice often relies on heuristics, our end-to-end evaluation on Natural Questions systematically varies chunking method (token, sentence, semantic, code), chunk size, overlap, and context length. We use a standard industrial setup: SPLADE retrieval and a Mistral-8B generator. We derive actionable lessons for cost-efficient deployment: (i) overlap provides no measurable benefit and increases indexing cost; (ii) sentence chunking is the most cost-effective method, matching semantic chunking up to ~5k tokens; (iii) a "context cliff" reduces quality beyond ~2.5k tokens; and (iv) optimal context depends on the goal (semantic quality peaks at small contexts; exact match at larger ones).

</details>


### [289] [Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic](https://arxiv.org/abs/2601.14124)
*Saad Mankarious,Aya Zirikly*

Main category: cs.CL

TL;DR: The paper proposes a pretraining-free, diffusion-based approach to generate synthetic data for mental health analysis and mitigate gender bias in Arabic datasets.


<details>
  <summary>Details</summary>
Motivation: Current methods for generating synthetic data in mental health analysis rely on LLMs, which can propagate biases and offer limited output diversity. A new method is needed to counteract gender imbalance in sensitive, low-resource domains.

Method: The authors propose a diffusion-based text generation approach and frame bias mitigation as a style transfer problem. They train models using the CARMA Arabic mental health corpus to augment female-authored content by transforming male-authored text linguistically and semantically.

Result: Results show the approach consistently maintains semantic fidelity while achieving stylistic divergence. Gender transformations in the synthetic text are linguistically plausible, validated through both quantitative and qualitative analysis.

Conclusion: The proposed diffusion-based technique provides a promising, flexible framework for generating synthetic data and addressing gender bias in low-resource mental health domains.

Abstract: Synthetic data offers a promising solution for mitigating data scarcity and demographic bias in mental health analysis, yet existing approaches largely rely on pretrained large language models (LLMs), which may suffer from limited output diversity and propagate biases inherited from their training data. In this work, we propose a pretraining-free diffusion-based approach for synthetic text generation that frames bias mitigation as a style transfer problem. Using the CARMA Arabic mental health corpus, which exhibits a substantial gender imbalance, we focus on male-to-female style transfer to augment underrepresented female-authored content. We construct five datasets capturing varying linguistic and semantic aspects of gender expression in Arabic and train separate diffusion models for each setting. Quantitative evaluations demonstrate consistently high semantic fidelity between source and generated text, alongside meaningful surface-level stylistic divergence, while qualitative analysis confirms linguistically plausible gender transformations. Our results show that diffusion-based style transfer can generate high-entropy, semantically faithful synthetic data without reliance on pretrained LLMs, providing an effective and flexible framework for mitigating gender bias in sensitive, low-resource mental health domains.

</details>


### [290] [Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models](https://arxiv.org/abs/2601.14152)
*Hyunjong Ok,Jaeho Lee*

Main category: cs.CL

TL;DR: The paper explores why large language models (LLMs) are sensitive to prompt structure and discovers that causal attention mechanisms lead to significant differences in performance between different prompt orders.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to understand the underlying mechanisms behind large language models' (LLMs) sensitivity to prompt structures and to explore why certain prompt orders perform better in tasks like multiple-choice question answering.

Method: The authors systematically analyze architectural aspects of LLMs, focusing on the difference in performance between prompts placing context before questions and options (CQO) versus the reverse order (QOC). They examine the role of causal attention in driving performance differences.

Result: They find that in QOC prompts, the causal attention mechanism acts as a bottleneck, preventing option tokens from accessing context information, which reduces task performance. Conversely, placing context earlier (CQO) enables models to leverage this information effectively.

Conclusion: The study concludes that causal attention is a critical factor in prompt sensitivity for LLMs and that understanding and optimizing it can help improve model performance in structured NLP tasks.

Abstract: Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.

</details>


### [291] [Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law](https://arxiv.org/abs/2601.14160)
*Ali Hamza Bashir,Muhammad Rehan Khalid,Kostadin Cvejoski,Jana Birr,Jule Berghaus,Armin Berger,Sandra Halscheidt,Christian Temath,Rafet Sifa,David Berghaus*

Main category: cs.CL

TL;DR: The paper introduces a method to adapt large language models (LLMs) for German legal question answering using systematically generated synthetic data, resulting in better performance and reducing reliance on manual annotation.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of LLMs in specialized domains like legal reasoning due to insufficient expert knowledge, and reducing dependency on costly or unreliable annotation processes.

Method: A novel approach to systematically generate high-quality, legally accurate synthetic data from authoritative German statutes, coupled with automated filtering techniques and parameter-efficient fine-tuning.

Result: LLMs fine-tuned with the synthetic dataset outperform baseline models in German legal question answering tasks, proving the value of the approach.

Conclusion: Carefully designed synthetic data can serve as an effective alternative to manual annotation in complex, knowledge-intensive fields.

Abstract: Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.

</details>


### [292] [Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum](https://arxiv.org/abs/2601.14172)
*Víctor Yeste,Paolo Rosso*

Main category: cs.CL

TL;DR: This paper addresses the challenge of detecting sentences expressing human values in sparse and imbalanced textual contexts using computational models and lightweight signals.


<details>
  <summary>Details</summary>
Motivation: The study aims to identify human values at a fine-grained, sentence-level scale from text sources like news and political manifestos, which present challenges due to sparse cues and imbalanced data.

Method: It operationalizes a binary moral presence task and benchmarks neural models using DeBERTa-base, simple ensembles, and instruction-tuned LLMs with lightweight signals for additional context.

Result: A supervised ensemble achieved macro-F1 0.332, outperforming single models and prior baselines, emphasizing small ensemble models and lightweight signal augmentations under constrained computational settings.

Conclusion: While hierarchical gating has limited impact, supervised encoders effectively handle value detection under resource constraints, and incorporating more value structures and context could enhance results.

Abstract: We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern neural models. We first operationalize a binary moral presence task ("does any value appear?") and show that it is learnable from single sentences (positive-class F1 $\approx$ 0.74 with calibrated thresholds). We then compare a presence-gated hierarchy to a direct multi-label classifier under matched compute, both based on DeBERTa-base and augmented with lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic features). The hierarchy does not outperform direct prediction, indicating that gate recall limits downstream gains. We also benchmark instruction-tuned LLMs - Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and QLoRA setups and build simple ensembles; a soft-vote supervised ensemble reaches macro-F1 0.332, significantly surpassing the best single supervised model and exceeding prior English-only baselines. Overall, in this scenario, lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit. We argue that, under an 8 GB single-GPU constraint and at the 7-9B scale, carefully tuned supervised encoders remain a strong and compute-efficient baseline for structured human value detection, and we outline how richer value structure and sentence-in-document context could further improve performance.

</details>


### [293] [HALT: Hallucination Assessment via Latent Testing](https://arxiv.org/abs/2601.14210)
*Rohan Bhatnagar,Youran Sun,Chi Andrew Zhang,Yixin Wen,Haizhao Yang*

Main category: cs.CL

TL;DR: The paper introduces lightweight residual probes to assess hallucination risk in LLMs, leveraging intermediate hidden states for fast and efficient uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate fluent but incorrect responses due to attenuated epistemic signals during decoding. The motivation is to develop a method to identify and mitigate such hallucination risks efficiently.

Method: The authors propose lightweight residual probes to read internal epistemic signals from hidden states of question tokens. These probes evaluate hallucination risk in parallel with inference, enabling near instant risk estimation without significant latency.

Result: The method demonstrates strong performance on QA benchmarks with high AUROC/AURAC, effective generalization across datasets, and interpretable insights into intermediate representations.

Conclusion: The proposed approach offers a reliable, efficient strategy for internal uncertainty estimation in LLMs, providing a foundation for more trustworthy and agentic AI systems.

Abstract: Hallucination in large language models (LLMs) can be understood as a failure of faithful readout: although internal representations may encode uncertainty about a query, decoding pressures still yield a fluent answer. We propose lightweight residual probes that read hallucination risk directly from intermediate hidden states of question tokens, motivated by the hypothesis that these layers retain epistemic signals that are attenuated in the final decoding stage. The probe is a small auxiliary network whose computation is orders of magnitude cheaper than token generation and can be evaluated fully in parallel with inference, enabling near-instantaneous hallucination risk estimation with effectively zero added latency in low-risk cases. We deploy the probe as an agentic critic for fast selective generation and routing, allowing LLMs to immediately answer confident queries while delegating uncertain ones to stronger verification pipelines. Across four QA benchmarks and multiple LLM families, the method achieves strong AUROC and AURAC, generalizes under dataset shift, and reveals interpretable structure in intermediate representations, positioning fast internal uncertainty readout as a principled foundation for reliable agentic AI.

</details>


### [294] [MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems](https://arxiv.org/abs/2601.14230)
*Yiyang Wang,Yiqiao Jin,Alex Cabral,Josiah Hester*

Main category: cs.CL

TL;DR: MASCOT is a framework for multi-agent systems addressing persona collapse and social sycophancy through bi-level optimization for enhanced socio-collaborative performance.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems often fail to provide effective socio-collaborative support due to persona collapse and redundant dialogue behaviors.

Method: MASCOT utilizes bi-level optimization combining Persona-Aware Behavioral Alignment (using RLAIF for persona fidelity) and Collaborative Dialogue Optimization (meta-policy guided by group-level rewards).

Result: MASCOT improves Persona Consistency by +14.1 and Social Contribution by +10.6 over state-of-the-art methods, validated across psychological and workplace domains.

Conclusion: MASCOT represents a significant advancement in engineering socially intelligent multi-agent systems for collaborative settings.

Abstract: Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.

</details>


### [295] [APEX-Agents](https://arxiv.org/abs/2601.14242)
*Bertie Vidgen,Austin Mann,Abby Fennelly,John Wright Stanly,Lucas Rothman,Marco Burstein,Julien Benchek,David Ostrofsky,Anirudh Ravichandran,Debnil Sur,Neel Venugopal,Alannah Hsia,Isaac Robinson,Calix Huang,Olivia Varones,Daniyal Khan,Michael Haines,Zach Richards,Chirag Mahapatra,Brendan Foody,Osvald Nitski*

Main category: cs.CL

TL;DR: The paper introduces APEX-Agents, a benchmark for evaluating AI agents' ability to handle complex, real-world tasks by professionals. The benchmark and execution infrastructure are open-sourced.


<details>
  <summary>Details</summary>
Motivation: The paper aims to assess AI agents' capabilities in executing extended and complex tasks across various professional domains, emphasizing realistic environments.

Method: They developed APEX-Agents, a benchmark consisting of 480 tasks derived from professional workflows, and tested eight AI agents using Pass@1 for performance evaluation.

Result: The best-performing agent, Gemini 3 Flash (Thinking=High), achieved a score of 24.0%, with other agents like GPT-5.2 and Claude Opus 4.5 performing slightly lower.

Conclusion: APEX-Agents sets a high standard for evaluating AI agents' professional capabilities and is open-sourced alongside its infrastructure for further research and development.

Abstract: We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.

</details>


### [296] [Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment](https://arxiv.org/abs/2601.14249)
*Yuming Yang,Mingyoung Lai,Wanxu Zhao,Xiaoran Fan,Zhiheng Xi,Mingqi Wu,Chiyue Huang,Jun Zhao,Haijun Lv,Jian Tong,Yunhua Zhou,Yicheng Zou,Qipeng Guo,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: The paper introduces a method, Rank-Surprisal Ratio (RSR), to improve the effectiveness of reasoning trajectory selection for distilling knowledge from teacher to student language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To address the gap that existing methods overlook informative reasoning trajectories during distillation, which compromises the transfer of reasoning ability to student models.

Method: The Rank-Surprisal Ratio (RSR) is introduced as a metric combining trajectory alignment and informativeness, computed as the ratio of token rank to negative log-likelihood.

Result: RSR achieved high correlation (Spearman 0.86) with post-training performance across various models and trajectories, outperforming existing suitability metrics.

Conclusion: RSR is a practical and effective tool for selecting reasoning trajectories and teachers in distillation processes, enhancing the transfer of reasoning capabilities to student LLMs.

Abstract: Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory's average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [297] [Domain-Specific Self-Supervised Pre-training for Agricultural Disease Classification: A Hierarchical Vision Transformer Study](https://arxiv.org/abs/2601.11612)
*Arnav S. Sonavane*

Main category: cs.CV

TL;DR: The paper demonstrates that domain-specific self-supervised learning (SSL) significantly boosts performance in agricultural disease classification, even more than complex architectures.


<details>
  <summary>Details</summary>
Motivation: Improving accuracy in agricultural disease classification by exploring the impact of domain-specific SSL and hierarchical vision transformer designs.

Method: Developed and evaluated SimCLR pre-training on unlabeled agricultural images combined with Hierarchical Vision Transformer (HVT) models across three datasets, compared SSL results across architectures like Swin and ViT.

Result: SimCLR pre-training on 3,000 agricultural images improves accuracy by +4.57%. HVT-Base outperforms Swin-Base by +1.68%. SSL benefits are architecture-agnostic. Achieves competitive calibration performance for deployment.

Conclusion: Collecting domain-specific data for self-supervised learning is more beneficial than emphasizing architecture selection for improving agricultural disease classification. Calibration metrics confirm HVT reliability.

Abstract: We investigate the impact of domain-specific self-supervised pre-training on agricultural disease classification using hierarchical vision transformers. Our key finding is that SimCLR pre-training on just 3,000 unlabeled agricultural images provides a +4.57% accuracy improvement--exceeding the +3.70% gain from hierarchical architecture design. Critically, we show this SSL benefit is architecture-agnostic: applying the same pre-training to Swin-Base yields +4.08%, to ViT-Base +4.20%, confirming practitioners should prioritize domain data collection over architectural choices. Using HierarchicalViT (HVT), a Swin-style hierarchical transformer, we evaluate on three datasets: Cotton Leaf Disease (7 classes, 90.24%), PlantVillage (38 classes, 96.3%), and PlantDoc (27 classes, 87.1%). At matched parameter counts, HVT-Base (78M) achieves 88.91% vs. Swin-Base (88M) at 87.23%, a +1.68% improvement. For deployment reliability, we report calibration analysis showing HVT achieves 3.56% ECE (1.52% after temperature scaling). Code: https://github.com/w2sg-arnav/HierarchicalViT

</details>


### [298] [Multi-modal MRI-Based Alzheimer's Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning](https://arxiv.org/abs/2601.11614)
*Jason Qiu*

Main category: cs.CV

TL;DR: This paper proposes a 3D TransUNet framework to generate diffusion MRI metrics (FA and MD) from T1w MRI, improving Alzheimer’s disease (AD) diagnostics.


<details>
  <summary>Details</summary>
Motivation: Current Alzheimer’s diagnostics using T1w MRI detect macroscopic changes but fail to capture early microstructural abnormalities. Diffusion MRI provides this data but is time-consuming and prone to motion artifacts, creating a need for an alternative.

Method: The study develops a 3D TransUNet model for synthesizing FA and MD maps from T1w MRI, achieving high structural similarity and strong correlation with ground truth. The model is integrated into a diagnostic framework.

Result: The method achieves SSIM > 0.93, Pearson correlation > 0.94 with ground-truth dMRI, boosts AD classification accuracy by 5%, and improves mild cognitive impairment detection by 12.5%.

Conclusion: The proposed method infers high-quality diffusion metrics from T1w MRI, providing multi-modality imaging advantages while reducing scan times. This enhances the accessibility, efficiency, and accuracy of AD diagnosis.

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder in which pathological changes begin many years before the onset of clinical symptoms, making early detection essential for timely intervention. T1-weighted (T1w) Magnetic Resonance Imaging (MRI) is routinely used in clinical practice to identify macroscopic brain alterations, but these changes typically emerge relatively late in the disease course. Diffusion MRI (dMRI), in contrast, is sensitive to earlier microstructural abnormalities by probing water diffusion in brain tissue. dMRI metrics, including fractional anisotropy (FA) and mean diffusivity (MD), provide complementary information about white matter integrity and neurodegeneration. However, dMRI acquisitions are time-consuming and susceptible to motion artifacts, limiting their routine use in clinical populations. To bridge this gap, I propose a 3D TransUNet image synthesis framework that predicts FA and MD maps directly from T1w MRI. My model generates high-fidelity maps, achieving a structural similarity index (SSIM) exceeding 0.93 and a strong Pearson correlation (>0.94) with ground-truth dMRI. When integrated into a multi-modal diagnostic model, these synthetic features boost AD classification accuracy by 5% (78.75%->83.75%) and, most importantly, improve mild cognitive impairment (MCI) detection by 12.5%. This study demonstrates that high-quality diffusion microstructural information can be inferred from routinely acquired T1w MRI, effectively transferring the benefits of multi-modality imaging to settings where diffusion data are unavailable. By reducing scan time while preserving complementary structural and microstructural information, the proposed approach has the potential to improve the accessibility, efficiency, and accuracy of AD diagnosis in clinical practice.

</details>


### [299] [PointSLAM++: Robust Dense Neural Gaussian Point Cloud-based SLAM](https://arxiv.org/abs/2601.11617)
*Xu Wang,Boyao Han,Xiaojun Chen,Ying Liu,Ruihui Li*

Main category: cs.CV

TL;DR: PointSLAM++ is an advanced RGB-D SLAM system that enhances 3D mapping accuracy and photorealistic rendering through innovative neural Gaussian representations and dynamic optimization techniques.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current SLAM systems in maintaining structural consistency and robust pose estimation in the presence of depth noise.

Method: PointSLAM++ integrates a hierarchical neural Gaussian representation for scene mapping, progressive pose optimization to counteract depth sensor noise, and a dynamic neural graph that adjusts Gaussian node distributions based on scene complexity.

Result: PointSLAM++ delivers superior 3D reconstruction accuracy and photorealistic rendering performance, outperforming existing 3DGS-based SLAM approaches.

Conclusion: PointSLAM++ is effective for large-scale augmented reality and robotics applications due to its precision and adaptability in real-time environments.

Abstract: Real-time 3D reconstruction is crucial for robotics and augmented reality, yet current simultaneous localization and mapping(SLAM) approaches often struggle to maintain structural consistency and robust pose estimation in the presence of depth noise. This work introduces PointSLAM++, a novel RGB-D SLAM system that leverages a hierarchically constrained neural Gaussian representation to preserve structural relationships while generating Gaussian primitives for scene mapping. It also employs progressive pose optimization to mitigate depth sensor noise, significantly enhancing localization accuracy. Furthermore, it utilizes a dynamic neural representation graph that adjusts the distribution of Gaussian nodes based on local geometric complexity, enabling the map to adapt to intricate scene details in real time. This combination yields high-precision 3D mapping and photorealistic scene rendering. Experimental results show PointSLAM++ outperforms existing 3DGS-based SLAM methods in reconstruction accuracy and rendering quality, demonstrating its advantages for large-scale AR and robotics.

</details>


### [300] [Handcrafted Feature-Assisted One-Class Learning for Artist Authentication in Historical Drawings](https://arxiv.org/abs/2601.11627)
*Hassan Ugail,Jan Ritch-Frel,Irina Matuzava*

Main category: cs.CV

TL;DR: This paper introduces a computational framework for authentication of historical drawings using one-class autoencoders with handcrafted features.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in authentication and attribution of historical paper works due to limited reference collections and subtle stylistic cues.

Method: The study uses ten artist-specific verifiers trained with interpretable handcrafted features, evaluating them under a biometric-style protocol.

Result: The verification system achieves 83.3% True Acceptance Rate and 9.5% False Acceptance Rate, with performance varying by artist.

Conclusion: This framework offers reproducible, quantitative evidence to complement connoisseurship in historical art authentication, especially in data-sparse scenarios.

Abstract: Authentication and attribution of works on paper remain persistent challenges in cultural heritage, particularly when the available reference corpus is small and stylistic cues are primarily expressed through line and limited tonal variation. We present a verification-based computational framework for historical drawing authentication using one-class autoencoders trained on a compact set of interpretable handcrafted features. Ten artist-specific verifiers are trained using authenticated sketches from the Metropolitan Museum of Art open-access collection, the Ashmolean Collections Catalogue, the Morgan Library and Museum, the Royal Collection Trust (UK), the Victoria and Albert Museum Collections, and an online catalogue of the Casa Buonarroti collection and evaluated under a biometric-style protocol with genuine and impostor trials. Feature vectors comprise Fourier-domain energy, Shannon entropy, global contrast, GLCM-based homogeneity, and a box-counting estimate of fractal complexity. Across 900 verification decisions (90 genuine and 810 impostor trials), the pooled system achieves a True Acceptance Rate of 83.3% with a False Acceptance Rate of 9.5% at the chosen operating point. Performance varies substantially by artist, with near-zero false acceptance for some verifiers and elevated confusability for others. A pairwise attribution of false accepts indicates structured error pathways consistent with stylistic proximity and shared drawing conventions, whilst also motivating tighter control of digitisation artefacts and threshold calibration. The proposed methodology is designed to complement, rather than replace, connoisseurship by providing reproducible, quantitative evidence suitable for data-scarce settings common in historical sketch attribution.

</details>


### [301] [A one-step generation model with a Single-Layer Transformer: Layer number re-distillation of FreeFlow](https://arxiv.org/abs/2601.11630)
*Haonan Wei,Linyuan Wang,Nuolin Sun,Zhizhong Zheng,Lei Li,Bin Yan*

Main category: cs.CV

TL;DR: The paper proposes SLT (Single-Layer Transformer), which distills the 28-layer FreeFlow into a single-layer model, significantly reducing parameters while maintaining generation quality. This improves efficiency and stability in one-step diffusion model generation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the efficiency and stability of one-step diffusion model generation by compressing the process into a minimal number of layers without compromising image quality.

Method: The method introduces SLT, which uses a single shared DiT block to approximate the feature evolution of a 28-layer model. Through distillation, it aligns intermediate features and final predictions, reducing the teacher's 675M parameters to 4.3M.

Result: SLT reduces the model complexity drastically while enabling over 100 candidate screenings in the same timeframe compared to two teacher model samplings. This enhances image quality by selecting higher-quality initial points.

Conclusion: SLT achieves significant parameter reduction and efficiency gains while improving the generation stability and average quality of one-step image generation, addressing challenges linked to low-quality noise screening.

Abstract: Currently, Flow matching methods aim to compress the iterative generation process of diffusion models into a few or even a single step, with MeanFlow and FreeFlow being representative achievements of one-step generation based on Ordinary Differential Equations (ODEs). We observe that the 28-layer Transformer architecture of FreeFlow can be characterized as an Euler discretization scheme for an ODE along the depth axis, where the layer index serves as the discrete time step. Therefore, we distill the number of layers of the FreeFlow model, following the same derivation logic as FreeFlow, and propose SLT (Single-Layer Transformer), which uses a single shared DiT block to approximate the depth-wise feature evolution of the 28-layer teacher. During training, it matches the teacher's intermediate features at several depth patches, fuses those patch-level representations, and simultaneously aligns the teacher's final velocity prediction. Through distillation training, we compress the 28 independent Transformer Blocks of the teacher model DiT-XL/2 into a single Transformer Block, reducing the parameter count from 675M to 4.3M. Furthermore, leveraging its minimal parameters and rapid sampling speed, SLT can screen more candidate points in the noise space within the same timeframe, thereby selecting higher-quality initial points for the teacher model FreeFlow and ultimately enhancing the quality of generated images. Experimental results demonstrate that within a time budget comparable to two random samplings of the teacher model, our method performs over 100 noise screenings and produces a high-quality sample through the teacher model using the selected points. Quality fluctuations caused by low-quality initial noise under a limited number of FreeFlow sampling calls are effectively avoided, substantially improving the stability and average generation quality of one-step generation.

</details>


### [302] [Compress to Focus: Efficient Coordinate Compression for Policy Optimization in Multi-Turn GUI Agents](https://arxiv.org/abs/2601.11631)
*Yurun Song,Jiong Yin,Rongjunchen Zhang,Ian G. Harris*

Main category: cs.CV

TL;DR: The paper introduces CCPO, an efficient framework for enhancing multi-turn GUI agents’ training by focusing on critical visual regions while reducing spatial complexity and improving performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of context inflation in multi-turn GUI agents during task completion due to accumulating interaction history.

Method: CCPO employs Coordinate-Aware Spatial Compression (CASC) and Distance-Based Advantage to optimize visual compression and spatial attention while improving policy learning.

Result: CCPO demonstrated state-of-the-art performance across multiple benchmarks, achieving significant token compression (up to 55%) and training speedup (up to 3.8 times).

Conclusion: CCPO effectively resolves context inflation in GUI agents by narrowing attention to key regions, enhancing computational efficiency and accuracy.

Abstract: Multi-turn GUI agents enable complex task completion through sequential decision-making, but suffer from severe context inflation as interaction history accumulates. Existing strategies either sacrifice long-term context via truncation or compromise spatial structure through token pruning. In this paper, we propose Coordinate Compression Policy Optimization (CCPO), an efficient policy optimization framework that couples visual compression with policy optimization for multi-turn GUI agents. CCPO introduces Coordinate-Aware Spatial Compression (CASC), which aggregates coordinates from multiple rollouts to capture target-relevant regions and progressively narrow historical attention around key visual areas. From interactions across rollouts, CASC adaptively constructs attention boundaries that concentrate computation on the most informative regions of the scene. We further design a Distance-Based Advantage that provides fine-grained learning signals based on distance rather than binary correctness, improving both grounding accuracy and compression quality. Extensive experiments demonstrate that CCPO achieves SOTA performance across four benchmarks with up to 55% token compression and 3.8$\times$ training speedup.

</details>


### [303] [KG-ViP: Bridging Knowledge Grounding and Visual Perception in Multi-modal LLMs for Visual Question Answering](https://arxiv.org/abs/2601.11632)
*Zhiyang Li,Ao Ke,Yukun Cao,Xike Xie*

Main category: cs.CV

TL;DR: KG-ViP is a framework addressing challenges in MLLMs for VQA by combining commonsense and scene graphs to improve external knowledge usage and fine-grained visual understanding, showing superior performance.


<details>
  <summary>Details</summary>
Motivation: MLLMs face challenges with knowledge hallucination and insufficient fine-grained visual perception in VQA tasks due to isolated treatment of commonsense and scene graphs.

Method: KG-ViP introduces a retrieval-and-fusion pipeline to integrate scene graphs and commonsense graphs using a query as a semantic bridge to create a unified structured context for reasoning.

Result: KG-ViP achieves significant performance improvement over existing methods on benchmarks like FVQA 2.0+ and MVQA.

Conclusion: Integrating scene and commonsense graphs synergistically enhances MLLMs' reasoning capabilities for VQA tasks.

Abstract: Multi-modal Large Language Models (MLLMs) for Visual Question Answering (VQA) often suffer from dual limitations: knowledge hallucination and insufficient fine-grained visual perception. Crucially, we identify that commonsense graphs and scene graphs provide precisely complementary solutions to these respective deficiencies by providing rich external knowledge and capturing fine-grained visual details. However, prior works typically treat them in isolation, overlooking their synergistic potential. To bridge this gap, we propose KG-ViP, a unified framework that empowers MLLMs by fusing scene graphs and commonsense graphs. The core of the KG-ViP framework is a novel retrieval-and-fusion pipeline that utilizes the query as a semantic bridge to progressively integrate both graphs, synthesizing a unified structured context that facilitates reliable multi-modal reasoning. Extensive experiments on FVQA 2.0+ and MVQA benchmarks demonstrate that KG-ViP significantly outperforms existing VQA methods.

</details>


### [304] [Beyond Accuracy: Evaluating Grounded Visual Evidence in Thinking with Images](https://arxiv.org/abs/2601.11633)
*Xuchen Li,Xuzhao Li,Renjie Pi,Shiyu Hu,Jian Zhao,Jiahui Gao*

Main category: cs.CV

TL;DR: ViEBench is introduced as a benchmark to assess the reasoning credibility of Vision-Language Models (VLMs) by emphasizing process-verifiable evaluations through visual evidence.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to accurately evaluate the reasoning process of VLMs, especially in utilizing fine-grained visual cues in multi-step reasoning.

Method: The paper proposes ViEBench, a benchmark that uses 200 high-resolution images with annotated visual evidence and categorizes tasks by difficulty into perception and reasoning dimensions. A dual-axis matrix provides diagnostics for model behavior.

Result: Experiments show VLMs may produce correct answers with irrelevant evidence or fail to use correct evidence effectively. ViEBench improves explainable evaluations for VLMs.

Conclusion: ViEBench enhances insights into the reasoning authenticity of VLMs and serves as a practical and explainable tool for benchmarking faithful visual reasoning.

Abstract: Despite the remarkable progress of Vision-Language Models (VLMs) in adopting "Thinking-with-Images" capabilities, accurately evaluating the authenticity of their reasoning process remains a critical challenge. Existing benchmarks mainly rely on outcome-oriented accuracy, lacking the capability to assess whether models can accurately leverage fine-grained visual cues for multi-step reasoning. To address these limitations, we propose ViEBench, a process-verifiable benchmark designed to evaluate faithful visual reasoning. Comprising 200 multi-scenario high-resolution images with expert-annotated visual evidence, ViEBench uniquely categorizes tasks by difficulty into perception and reasoning dimensions, where reasoning tasks require utilizing localized visual details with prior knowledge. To establish comprehensive evaluation criteria, we introduce a dual-axis matrix that provides fine-grained metrics through four diagnostic quadrants, enabling transparent diagnosis of model behavior across varying task complexities. Our experiments yield several interesting observations: (1) VLMs can sometimes produce correct final answers despite grounding on irrelevant regions, and (2) they may successfully locate the correct evidence but still fail to utilize it to reach accurate conclusions. Our findings demonstrate that ViEBench can serve as a more explainable and practical benchmark for comprehensively evaluating the effectiveness agentic VLMs. The codes will be released at: https://github.com/Xuchen-Li/ViEBench.

</details>


### [305] [When Rules Fall Short: Agent-Driven Discovery of Emerging Content Issues in Short Video Platforms](https://arxiv.org/abs/2601.11634)
*Chenghui Yu,Hongwei Wang,Junwen Chen,Zixuan Wang,Bingfeng Deng,Zhuolin Hao,Hongyu Xiong,Yang Song*

Main category: cs.CV

TL;DR: The paper addresses the challenge of identifying emerging content issues on short-video platforms by proposing an automatic discovery method using multimodal LLM agents, enhancing annotation policies and governance.


<details>
  <summary>Details</summary>
Motivation: Existing human-driven methods of discovering emerging trends and content issues on short-video platforms are too slow. This lag delays the updating of annotation policies, making it difficult to effectively govern content in real-time.

Method: The authors introduce an automated issue discovery mechanism based on multimodal LLM agents. It recalls short videos with potential new issues, applies a two-step clustering strategy to identify distinct issues, and generates updated annotation policies from these clusters.

Result: The method, implemented in real systems, achieved a 20% improvement in F1 score for emerging issue discovery and reduced problematic video views by around 15%. It also reduced time costs and accelerated the policy update process.

Conclusion: The proposed agent-based method significantly improves issue discovery and governance on short-video platforms, offering an efficient and scalable solution to address emerging trends faster than traditional human-driven methods.

Abstract: Trends on short-video platforms evolve at a rapid pace, with new content issues emerging every day that fall outside the coverage of existing annotation policies. However, traditional human-driven discovery of emerging issues is too slow, which leads to delayed updates of annotation policies and poses a major challenge for effective content governance. In this work, we propose an automatic issue discovery method based on multimodal LLM agents. Our approach automatically recalls short videos containing potential new issues and applies a two-stage clustering strategy to group them, with each cluster corresponding to a newly discovered issue. The agent then generates updated annotation policies from these clusters, thereby extending coverage to these emerging issues. Our agent has been deployed in the real system. Both offline and online experiments demonstrate that this agent-based method significantly improves the effectiveness of emerging-issue discovery (with an F1 score improvement of over 20%) and enhances the performance of subsequent issue governance (reducing the view count of problematic videos by approximately 15%). More importantly, compared to manual issue discovery, it greatly reduces time costs and substantially accelerates the iteration of annotation policies.

</details>


### [306] [Now You See Me, Now You Don't: A Unified Framework for Expression Consistent Anonymization in Talking Head Videos](https://arxiv.org/abs/2601.11635)
*Anil Egin,Andrea Tangherloni,Antitza Dantcheva*

Main category: cs.CV

TL;DR: This paper introduces Anon-NET, a framework for anonymizing face videos while maintaining essential attributes like age, gender, race, and expression.


<details>
  <summary>Details</summary>
Motivation: To preserve privacy in face videos without compromising the ability to analyze videos for tasks such as expression detection and people tracking.

Method: They introduce "Anon-NET," which uses diffusion-based generative modeling for face inpainting guided by attribute recognition and motion-aware facial animation.

Result: Experiments on datasets like VoxCeleb2, CelebV-HQ, and HDTF show that AnonNET can effectively anonymize identities while keeping videos realistic and temporally consistent.

Conclusion: AnonNET provides a robust solution for facial video anonymization, balancing privacy preservation and analysis usability. The code is set to be released publicly.

Abstract: Face video anonymization is aimed at privacy preservation while allowing for the analysis of videos in a number of computer vision downstream tasks such as expression recognition, people tracking, and action recognition. We propose here a novel unified framework referred to as Anon-NET, streamlined to de-identify facial videos, while preserving age, gender, race, pose, and expression of the original video. Specifically, we inpaint faces by a diffusion-based generative model guided by high-level attribute recognition and motion-aware expression transfer. We then animate deidentified faces by video-driven animation, which accepts the de-identified face and the original video as input. Extensive experiments on the datasets VoxCeleb2, CelebV-HQ, and HDTF, which include diverse facial dynamics, demonstrate the effectiveness of AnonNET in obfuscating identity while retaining visual realism and temporal consistency. The code of AnonNet will be publicly released.

</details>


### [307] [Evaluating Self-Correcting Vision Agents Through Quantitative and Qualitative Metrics](https://arxiv.org/abs/2601.11637)
*Aradhya Dixit*

Main category: cs.CV

TL;DR: The paper introduces a Diagnostic Micro-Benchmark to evaluate and address reasoning limitations in Vision-Language Agents (VLAs), discovering semantic drift as a dominant issue and proposing a framework for improvement.


<details>
  <summary>Details</summary>
Motivation: To better understand and quantify the reasoning bottlenecks and performance limitations in Vision-Language Agents, especially regarding iterative self-correction capabilities.

Method: A Diagnostic Micro-Benchmark was introduced to decouple metrics like Task Success Rate from Correction Success Rate and analyze the diminishing returns of iterative corrections while identifying a taxonomy of failure types.

Result: Task Success Rate was found to be 62%, whereas Correction Success Rate ranged between 25% and 33%. Semantic Drift was identified as a key reason for failure in about 28% of the cases, showing limitations in retaining contextual information during reasoning.

Conclusion: Iterative self-correction has diminishing returns and initial competence does not equate to repair ability. By isolating semantic drift, the study provides a pathway for creating more stateful and reliable multimodal agents.

Abstract: Recent progress in multimodal foundation models has enabled Vision-Language Agents (VLAs) to decompose complex visual tasks into executable tool-based plans. While recent benchmarks have begun to evaluate iterative self-correction, its quantitative limits and dominant reasoning bottlenecks remain poorly characterized. This work introduces a Diagnostic Micro-Benchmark. Our analysis decouples Task Success Rate (TSR = 62 percent) from Correction Success Rate (CSR = 25 to 33 percent), revealing that initial competence does not predict repair ability. We explicitly quantify the diminishing returns of correction, which saturates after three retries. Our Failure Taxonomy reveals a frequent factor is Semantic Drift (about 28 percent of failures), a loss of contextual state. By isolating this reasoning bottleneck, this benchmark defines a reproducible framework toward stateful, trustworthy multimodal agents.

</details>


### [308] [Confident Learning for Object Detection under Model Constraints](https://arxiv.org/abs/2601.11640)
*Yingda Yu,Jiaqi Xuan,Shuhui Shi,Xuanyu Teng,Shuyang Xu,Guanchao Tong*

Main category: cs.CV

TL;DR: The paper introduces Model-Driven Data Correction (MDDC) to improve agricultural weed detection performance by systematically addressing data deficiencies through error analysis and structured data management, achieving 5-25% mAP improvement with fixed lightweight models.


<details>
  <summary>Details</summary>
Motivation: To address the performance bottleneck caused by constrained model capacity and computational resources on edge devices used for agricultural weed detection.

Method: This paper develops MDDC, which involves automated error analysis, categorization of detection failures, and a structured train-fix-retrain pipeline for systematically improving data quality.

Result: The research achieves a 5-25% improvement in mAP at 0.5 across multiple datasets using YOLOv8n, a fixed lightweight detector.

Conclusion: Data quality optimization is a powerful solution to overcome detection performance limitations in weed detection on constrained edge devices, without scaling or ensemble methods.

Abstract: Agricultural weed detection on edge devices is subject to strict constraints on model capacity, computational resources, and real-time inference latency, which prevent performance improvements through model scaling or ensembling. This paper proposes Model-Driven Data Correction (MDDC), a data-centric framework that enhances detection performance by iteratively diagnosing and correcting data quality deficiencies. An automated error analysis procedure categorizes detection failures into four types: false negatives, false positives, class confusion, and localization errors. These error patterns are systematically addressed through a structured train-fix-retrain pipeline with version-controlled data management. Experimental results on multiple weed detection datasets demonstrate consistent improvements of 5-25 percent in mAP at 0.5 using a fixed lightweight detector (YOLOv8n), indicating that systematic data quality optimization can effectively alleviate performance bottlenecks under fixed model capacity constraints.

</details>


### [309] [Mixture of Distributions Matters: Dynamic Sparse Attention for Efficient Video Diffusion Transformers](https://arxiv.org/abs/2601.11641)
*Yuxi Liu,Yipeng Hu,Zekun Zhang,Kunze Jiang,Kun Yuan*

Main category: cs.CV

TL;DR: The paper proposes MOD-DiT, a dynamic attention framework for more efficient and high-quality video generation by addressing the quadratic complexity of self-attention mechanisms and overcoming limitations in existing sparse attention methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the computational inefficiencies and quality degradation in video generation caused by the quadratic complexity of self-attention mechanisms and limitations of existing sparse attention approaches.

Method: The method involves a two-stage process: using prior information and a distributed mixing approach to predict mask patterns efficiently, followed by an online block masking strategy to dynamically apply predicted masks without repetitive sampling.

Result: The proposed MOD-DiT framework achieves consistent acceleration and quality improvements across different benchmarks and model architectures for video generation.

Conclusion: MOD-DiT provides an effective solution for high-quality video generation while reducing computational complexity, making it a practical alternative to traditional sparse attention methods.

Abstract: While Diffusion Transformers (DiTs) have achieved notable progress in video generation, this long-sequence generation task remains constrained by the quadratic complexity inherent to self-attention mechanisms, creating significant barriers to practical deployment. Although sparse attention methods attempt to address this challenge, existing approaches either rely on oversimplified static patterns or require computationally expensive sampling operations to achieve dynamic sparsity, resulting in inaccurate pattern predictions and degraded generation quality. To overcome these limitations, we propose a \underline{\textbf{M}}ixtrue-\underline{\textbf{O}}f-\underline{\textbf{D}}istribution \textbf{DiT} (\textbf{MOD-DiT}), a novel sampling-free dynamic attention framework that accurately models evolving attention patterns through a two-stage process. First, MOD-DiT leverages prior information from early denoising steps and adopts a {distributed mixing approach} to model an efficient linear approximation model, which is then used to predict mask patterns for a specific denoising interval. Second, an online block masking strategy dynamically applies these predicted masks while maintaining historical sparsity information, eliminating the need for repetitive sampling operations. Extensive evaluations demonstrate consistent acceleration and quality improvements across multiple benchmarks and model architectures, validating MOD-DiT's effectiveness for efficient, high-quality video generation while overcoming the computational limitations of traditional sparse attention approaches.

</details>


### [310] [PSSF: Early osteoarthritis detection using physical synthetic knee X-ray scans and AI radiomics models](https://arxiv.org/abs/2601.11642)
*Abbas Alzubaidi,Ali Al-Bayaty*

Main category: cs.CV

TL;DR: The paper introduces a physics-based synthetic simulation framework (PSSF) for generating controllable X-ray scans to address the limitations in knee osteoarthritis X-ray dataset acquisition.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome challenges in obtaining large, well-annotated knee X-ray datasets due to privacy, governance, and resourcing constraints for osteoarthritis assessment.

Method: A physics-based synthetic simulation framework (PSSF) is developed to create synthetic X-ray images from parametric anatomical models, and machine learning models are trained for radiographic evaluation.

Result: Synthetic X-ray scans were created for 260 knees, and machine learning models successfully classified knee OA severity under multiple scenarios with robust feature stability.

Conclusion: PSSF provides a promising tool for generating synthetic X-ray datasets, enhancing OA assessment while addressing privacy and resource limitations.

Abstract: Knee osteoarthritis (OA) is a major cause of disability worldwide and is still largely assessed using subjective radiographic grading, most commonly the Kellgren-Lawrence (KL) scale. Artificial intelligence (AI) and radiomics offer quantitative tools for OA assessment but depend on large, well-annotated image datasets, mainly X-ray scans, that are often difficult to obtain because of privacy, governance and resourcing constraints. In this research, we introduce a physics-based synthetic simulation framework (PSSF) to fully generate controllable X-ray scans without patients' involvement and violating their privacy and institutional constraints. This PSSF is a 2D X-ray projection simulator of anteroposterior knee radiographs from a parametric anatomical model of the distal femur and proximal tibia. Using PSSF, we create a virtual cohort of 180 subjects (260 knees), each is imaged under three protocols (reference, low-dose, and geometry-shift). Medial joint regions are automatically localized, preprocessed, and processed with the Image Biomarker Standardisation Initiative (IBSI). Practically, three machine learning (ML) models are utilized, logistic regression, random forest, and gradient boosting, to train binary (KL-like "0" vs. "2") and three-class (0-2) prediction radiographic images. Robustness is assessed within IBSI protocol, cross-protocol, and multi-protocol scenarios. Finally, features stability is then evaluated using intraclass correlation coefficients across acquisition changes.

</details>


### [311] [Predicting When to Trust Vision-Language Models for Spatial Reasoning](https://arxiv.org/abs/2601.11644)
*Muhammad Imran,Yugyung Lee*

Main category: cs.CV

TL;DR: The paper proposes a framework for estimating confidence in Vision-Language Models (VLMs) spatial predictions by independently verifying them with geometric analysis, rather than relying on VLM self-assessments. It significantly improves accuracy and confidence-based model operations.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models struggle with spatial reasoning, crucial for applications like robotics and autonomous systems, where reliability in spatial predictions is needed.

Method: The framework uses vision-based signals (like geometric alignment, spatial ambiguity, detection quality, and VLM uncertainty) via gradient boosting for confidence estimation. It validates spatial predictions using object detection instead of text-based approaches.

Result: Achieved marked improvement in AUROC for spatial prediction confidence (BLIP-2: 34.0%, CLIP: 16.1%) and enhanced scene graph construction precision to 78.3% using confidence-based pruning.

Conclusion: Independent geometric verification significantly enhances spatial reasoning reliability in VLMs, making them more suitable for safe deployment in applications requiring accurate spatial cognition (e.g., robotics).

Abstract: Vision-Language Models (VLMs) demonstrate impressive capabilities across multimodal tasks, yet exhibit systematic spatial reasoning failures, achieving only 49% (CLIP) to 54% (BLIP-2) accuracy on basic directional relationships. For safe deployment in robotics and autonomous systems, we need to predict when to trust VLM spatial predictions rather than accepting all outputs. We propose a vision-based confidence estimation framework that validates VLM predictions through independent geometric verification using object detection. Unlike text-based approaches relying on self-assessment, our method fuses four signals via gradient boosting: geometric alignment between VLM claims and coordinates, spatial ambiguity from overlap, detection quality, and VLM internal uncertainty. We achieve 0.674 AUROC on BLIP-2 (34.0% improvement over text-based baselines) and 0.583 AUROC on CLIP (16.1% improvement), generalizing across generative and classification architectures. Our framework enables selective prediction: at 60% target accuracy, we achieve 61.9% coverage versus 27.6% baseline (2.2x improvement) on BLIP-2. Feature analysis reveals vision-based signals contribute 87.4% of model importance versus 12.7% from VLM confidence, validating that external geometric verification outperforms self-assessment. We demonstrate reliable scene graph construction where confidence-based pruning improves precision from 52.1% to 78.3% while retaining 68.2% of edges.

</details>


### [312] [IMSAHLO: Integrating Multi-Scale Attention and Hybrid Loss Optimization Framework for Robust Neuronal Brain Cell Segmentation](https://arxiv.org/abs/2601.11645)
*Ujjwal Jain,Oshin Misra,Roshni Chakraborty,Mahua Bhattacharya*

Main category: cs.CV

TL;DR: The paper introduces a deep learning framework called IMSAHLO for accurate segmentation of neuronal cells in fluorescence microscopy, addressing challenges like high cell density, overlapping morphologies, and class imbalance.


<details>
  <summary>Details</summary>
Motivation: Neuronal segmentation in fluorescence microscopy is challenging due to dense and sparse cell distributions, overlapping morphologies, and severe class imbalance, which hampers conventional models from achieving accurate segmentation.

Method: The authors propose a model employing Multi-Scale Dense Blocks to handle variations in cell density, a Hierarchical Attention mechanism to maintain ROI boundary details, and a hybrid loss function combining Tversky, Focal, clDice, and Contour-Weighted Boundary losses to address class imbalance and preserve topological features.

Result: The model achieved state-of-the-art performance, with precision of 81.4%, macro F1 of 82.7%, micro F1 of 83.3%, and balanced accuracy of 99.5% on the Fluorescent Neuronal Cells dataset, demonstrating superior results in dense and sparse cell cases.

Conclusion: IMSAHLO successfully addresses the limitations of existing neuronal segmentation methods and provides a foundation for generalizable biomedical image segmentation models, aiding high-throughput neurobiology research.

Abstract: Accurate segmentation of neuronal cells in fluorescence microscopy is a fundamental task for quantitative analysis in computational neuroscience. However, it is significantly impeded by challenges such as the coexistence of densely packed and sparsely distributed cells, complex overlapping morphologies, and severe class imbalance. Conventional deep learning models often fail to preserve fine topological details or accurately delineate boundaries under these conditions. To address these limitations, we propose a novel deep learning framework, IMSAHLO (Integrating Multi-Scale Attention and Hybrid Loss Optimization), for robust and adaptive neuronal segmentation. The core of our model features Multi-Scale Dense Blocks (MSDBs) to capture features at various receptive fields, effectively handling variations in cell density, and a Hierarchical Attention (HA) mechanism that adaptively focuses on salient morphological features to preserve Region of Interest (ROI) boundary details. Furthermore, we introduce a novel hybrid loss function synergistically combining Tversky and Focal loss to combat class imbalance, alongside a topology-aware Centerline Dice (clDice) loss and a Contour-Weighted Boundary loss to ensure topological continuity and precise separation of adjacent cells. Large-scale experiments on the public Fluorescent Neuronal Cells (FNC) dataset demonstrate that our framework outperforms state-of-the-art architectures, achieving precision of 81.4%, macro F1 score of 82.7%, micro F1 score of 83.3%, and balanced accuracy of 99.5% on difficult dense and sparse cases. Ablation studies validate the synergistic benefits of multi-scale attention and hybrid loss terms. This work establishes a foundation for generalizable segmentation models applicable to a wide range of biomedical imaging modalities, pushing AI-assisted analysis toward high-throughput neurobiological pipelines.

</details>


### [313] [Aesthetics as Structural Harm: Algorithmic Lookism Across Text-to-Image Generation and Classification](https://arxiv.org/abs/2601.11651)
*Miriam Doh,Aditya Gulati,Corina Canali,Nuria Oliver*

Main category: cs.CV

TL;DR: The study uncovers algorithmic lookism in text-to-image generative models and gender classification tasks, highlighting bias towards physical appearance and its intersection with gender inequities.


<details>
  <summary>Details</summary>
Motivation: To investigate and expose the biases in generative AI and gender classification systems that associate attractiveness with certain attributes, propagating harmful stereotypes.

Method: Analyzed 26,400 synthetic faces created using Stable Diffusion models, evaluating bias in model outputs and its implications on gender classification algorithms.

Result: Identified systematic attractiveness biases in T2I models, gender disparities in classification systems, and increased aesthetic biases in newer models.

Conclusion: Algorithmic lookism perpetuates existing social inequalities by embedding physical appearance-based biases in AI systems, harming representation and recognition.

Abstract: This paper examines algorithmic lookism-the systematic preferential treatment based on physical appearance-in text-to-image (T2I) generative AI and a downstream gender classification task. Through the analysis of 26,400 synthetic faces created with Stable Diffusion 2.1 and 3.5 Medium, we demonstrate how generative AI models systematically associate facial attractiveness with positive attributes and vice-versa, mirroring socially constructed biases rather than evidence-based correlations. Furthermore, we find significant gender bias in three gender classification algorithms depending on the attributes of the input faces. Our findings reveal three critical harms: (1) the systematic encoding of attractiveness-positive attribute associations in T2I models; (2) gender disparities in classification systems, where women's faces, particularly those generated with negative attributes, suffer substantially higher misclassification rates than men's; and (3) intensifying aesthetic constraints in newer models through age homogenization, gendered exposure patterns, and geographic reductionism. These convergent patterns reveal algorithmic lookism as systematic infrastructure operating across AI vision systems, compounding existing inequalities through both representation and recognition.
  Disclaimer: This work includes visual and textual content that reflects stereotypical associations between physical appearance and socially constructed attributes, including gender, race, and traits associated with social desirability. Any such associations found in this study emerge from the biases embedded in generative AI systems-not from empirical truths or the authors' views.

</details>


### [314] [PSSI-MaxST: An Efficient Pixel-Segment Similarity Index Using Intensity and Smoothness Features for Maximum Spanning Tree Based Segmentation](https://arxiv.org/abs/2601.11654)
*Kaustubh Shivshankar Shejole,Gaurav Mishra*

Main category: cs.CV

TL;DR: This paper proposes a new Pixel Segment Similarity Index (PSSI) for improved graph-based image segmentation. It excels in performance and computational efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing interactive graph-based segmentation methods struggle with high sensitivity to user inputs, high computational costs, and poor performance when foreground and background have similar color distributions.

Method: The paper introduces the Pixel Segment Similarity Index (PSSI), integrating pixel intensity and spatial smoothness features through harmonic mean, combined with MeanShift-based segmentation and Maximum Spanning Tree (MaxST) for graph partitioning.

Result: The method outperforms current segmentation approaches (like AMOE, OneCut, and SSNCut) on the GrabCut and Images250 datasets, evaluated by metrics such as Jaccard Index, $F_1$ score, execution time, and Mean Error.

Conclusion: The proposed framework provides robust, efficient, and precise interactive image segmentation by incorporating PSSI, MeanShift and MaxST. It achieves better segmentation quality and computational gains, validated by extensive experimental results.

Abstract: Interactive graph-based segmentation methods partition an image into foreground and background regions with the aid of user inputs. However, existing approaches often suffer from high computational costs, sensitivity to user interactions, and degraded performance when the foreground and background share similar color distributions. A key factor influencing segmentation performance is the similarity measure used for assigning edge weights in the graph. To address these challenges, we propose a novel Pixel Segment Similarity Index (PSSI), which leverages the harmonic mean of inter-channel similarities by incorporating both pixel intensity and spatial smoothness features. The harmonic mean effectively penalizes dissimilarities in any individual channel, enhancing robustness. The computational complexity of PSSI is $\mathcal{O}(B)$, where $B$ denotes the number of histogram bins. Our segmentation framework begins with low-level segmentation using MeanShift, which effectively captures color, texture, and segment shape. Based on the resulting pixel segments, we construct a pixel-segment graph with edge weights determined by PSSI. For partitioning, we employ the Maximum Spanning Tree (MaxST), which captures strongly connected local neighborhoods beneficial for precise segmentation. The integration of the proposed PSSI, MeanShift, and MaxST allows our method to jointly capture color similarity, smoothness, texture, shape, and strong local connectivity. Experimental evaluations on the GrabCut and Images250 datasets demonstrate that our method consistently outperforms current graph-based interactive segmentation methods such as AMOE, OneCut, and SSNCut in terms of segmentation quality, as measured by Jaccard Index (IoU), $F_1$ score, execution time and Mean Error (ME). Code is publicly available at: https://github.com/KaustubhShejole/PSSI-MaxST.

</details>


### [315] [Zeros can be Informative: Masked Binary U-Net for Image Segmentation on Tensor Cores](https://arxiv.org/abs/2601.11660)
*Chunshu Wu,Ruibing Song,Sushant Kondguli,Tong Geng,Ang Li*

Main category: cs.CV

TL;DR: The paper proposes Masked Binary U-Net (MBU-Net) to address challenges in real-time image segmentation on edge devices, achieving near full-precision accuracy, speedup, and energy reductions.


<details>
  <summary>Details</summary>
Motivation: Enhance real-time image segmentation performance on resource-constrained devices, addressing accuracy degradation and efficiency limitations of extreme quantization methods.

Method: Design MBU-Net using cost-aware masking and implement GPU execution framework leveraging binary Tensor Core instructions for high efficiency and throughput.

Result: MBU-Net achieves near full-precision accuracy (~3% drop), 2.04x speedup, and 3.54x energy savings over 16-bit floating point U-Net across benchmarks.

Conclusion: MBU-Net bridges the gap between hardware-friendly binary networks and accuracy requirements for practical real-time applications, ensuring efficient GPU execution.

Abstract: Real-time image segmentation is a key enabler for AR/VR, robotics, drones, and autonomous systems, where tight accuracy, latency, and energy budgets must be met on resource-constrained edge devices. While U-Net offers a favorable balance of accuracy and efficiency compared to large transformer-based models, achieving real-time performance on high-resolution input remains challenging due to compute, memory, and power limits. Extreme quantization, particularly binary networks, is appealing for its hardware-friendly operations. However, two obstacles limit practicality: (1) severe accuracy degradation, and (2) a lack of end-to-end implementations that deliver efficiency on general-purpose GPUs.
  We make two empirical observations that guide our design. (1) An explicit zero state is essential: training with zero masking to binary U-Net weights yields noticeable sparsity. (2) Quantization sensitivity is uniform across layers. Motivated by these findings, we introduce Masked Binary U-Net (MBU-Net), obtained through a cost-aware masking strategy that prioritizes masking where it yields the highest accuracy-per-cost, reconciling accuracy with near-binary efficiency.
  To realize these gains in practice, we develop a GPU execution framework that maps MBU-Net to Tensor Cores via a subtractive bit-encoding scheme, efficiently implementing masked binary weights with binary activations. This design leverages native binary Tensor Core BMMA instructions, enabling high throughput and energy savings on widely available GPUs. Across 3 segmentation benchmarks, MBU-Net attains near full-precision accuracy (3% average drop) while delivering 2.04x speedup and 3.54x energy reductions over a 16-bit floating point U-Net.

</details>


### [316] [LTV-YOLO: A Lightweight Thermal Object Detector for Young Pedestrians in Adverse Conditions](https://arxiv.org/abs/2601.11662)
*Abdullah Jirjees,Ryan Myers,Muhammad Haris Ikram,Mohamed H. Zaki*

Main category: cs.CV

TL;DR: This paper presents LTV-YOLO, a lightweight object detection model using thermal imaging to detect vulnerable road users, specifically children and adolescents, in low light and adverse weather conditions.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of reliably detecting children and adolescent pedestrians in poor visibility conditions where traditional RGB cameras fail.

Method: Developed a lightweight object detection model based on the YOLO11 architecture, incorporating depthwise separable convolutions and feature pyramid networks (FPN) for improved performance in thermal imaging.

Result: The proposed LTV-YOLO model achieves computational efficiency, reliability in detecting partially occluded and small VRUs, and suitability for real-time edge devices.

Conclusion: LTV-YOLO provides a practical and scalable solution specifically for thermal detection of young VRUs, offering improved pedestrian safety in varying environmental conditions.

Abstract: Detecting vulnerable road users (VRUs), particularly children and adolescents, in low light and adverse weather conditions remains a critical challenge in computer vision, surveillance, and autonomous vehicle systems. This paper presents a purpose-built lightweight object detection model designed to identify young pedestrians in various environmental scenarios. To address these challenges, our approach leverages thermal imaging from long-wave infrared (LWIR) cameras, which enhances detection reliability in conditions where traditional RGB cameras operating in the visible spectrum fail. Based on the YOLO11 architecture and customized for thermal detection, our model, termed LTV-YOLO (Lightweight Thermal Vision YOLO), is optimized for computational efficiency, accuracy and real-time performance on edge devices. By integrating separable convolutions in depth and a feature pyramid network (FPN), LTV-YOLO achieves strong performance in detecting small-scale, partially occluded, and thermally distinct VRUs while maintaining a compact architecture. This work contributes a practical and scalable solution to improve pedestrian safety in intelligent transportation systems, particularly in school zones, autonomous navigation, and smart city infrastructure. Unlike prior thermal detectors, our contribution is task-specific: a thermally only edge-capable design designed for young and small VRUs (children and distant adults). Although FPN and depthwise separable convolutions are standard components, their integration into a thermal-only pipeline optimized for short/occluded VRUs under adverse conditions is, to the best of our knowledge, novel.

</details>


### [317] [UAV-Based Infrastructure Inspections: A Literature Review and Proposed Framework for AEC+FM](https://arxiv.org/abs/2601.11665)
*Amir Farzin Nikkhah,Dong Chen,Bradford Campbell,Somayeh Asadi,Arsalan Heydarian*

Main category: cs.CV

TL;DR: This paper reviews UAV applications in the AEC+FM sector, highlighting methods, innovations, challenges, and proposing a framework for improved infrastructure inspections.


<details>
  <summary>Details</summary>
Motivation: To advance the accuracy and efficiency of UAV-based inspections in complex environments within the AEC+FM domain.

Method: The paper synthesizes insights from over 150 studies, proposes a framework integrating RGB, LiDAR, and thermal sensing, and employs transformer-based architectures for anomaly detection.

Result: The proposed framework improves precision and reliability in detecting structural defects, thermal anomalies, and geometric inconsistencies.

Conclusion: UAVs have transformative potential in infrastructure inspections, and future research should focus on lightweight AI, adaptive flight planning, and enhanced data fusion for scalability.

Abstract: Unmanned Aerial Vehicles (UAVs) are transforming infrastructure inspections in the Architecture, Engineering, Construction, and Facility Management (AEC+FM) domain. By synthesizing insights from over 150 studies, this review paper highlights UAV-based methodologies for data acquisition, photogrammetric modeling, defect detection, and decision-making support. Key innovations include path optimization, thermal integration, and advanced machine learning (ML) models such as YOLO and Faster R-CNN for anomaly detection. UAVs have demonstrated value in structural health monitoring (SHM), disaster response, urban infrastructure management, energy efficiency evaluations, and cultural heritage preservation. Despite these advancements, challenges in real-time processing, multimodal data fusion, and generalizability remain. A proposed workflow framework, informed by literature and a case study, integrates RGB imagery, LiDAR, and thermal sensing with transformer-based architectures to improve accuracy and reliability in detecting structural defects, thermal anomalies, and geometric inconsistencies. The proposed framework ensures precise and actionable insights by fusing multimodal data and dynamically adapting path planning for complex environments, presented as a comprehensive step-by-step guide to address these challenges effectively. This paper concludes with future research directions emphasizing lightweight AI models, adaptive flight planning, synthetic datasets, and richer modality fusion to streamline modern infrastructure inspections.

</details>


### [318] [MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models](https://arxiv.org/abs/2601.11666)
*Muhammad Imran,Chi Lee,Yugyung Lee*

Main category: cs.CV

TL;DR: MATEX is a framework that improves interpretability in medical vision-language models using anatomically informed spatial reasoning and multi-scale attention.


<details>
  <summary>Details</summary>
Motivation: Current methods lack spatial precision, anatomical grounding, and attention granularity, hindering trustworthy AI explanations in radiology.

Method: MATEX combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to generate better gradient attribution maps.

Result: On the MS-CXR dataset, MATEX outperforms the state-of-the-art M2IB method in spatial precision and alignment with expert annotations.

Conclusion: MATEX enhances trust and transparency in radiological AI applications by providing more faithful and interpretable model explanations.

Abstract: We introduce MATEX (Multi-scale Attention and Text-guided Explainability), a novel framework that advances interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. MATEX synergistically combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps. By addressing key limitations of prior methods, such as spatial imprecision, lack of anatomical grounding, and limited attention granularity, MATEX enables more faithful and interpretable model explanations. Evaluated on the MS-CXR dataset, MATEX outperforms the state-of-the-art M2IB approach in both spatial precision and alignment with expert-annotated findings. These results highlight MATEX's potential to enhance trust and transparency in radiological AI applications.

</details>


### [319] [Generating metamers of human scene understanding](https://arxiv.org/abs/2601.11675)
*Ritik Raina,Abe Leite,Alexandros Graikos,Seoyoung Ahn,Dimitris Samaras,Gregory J. Zelinsky*

Main category: cs.CV

TL;DR: The paper introduces MetamerGen, a tool to generate images mimicking human scene understanding by combining high and low-resolution visual input.


<details>
  <summary>Details</summary>
Motivation: To better understand how humans integrate gist information from the visual periphery with detailed information from fixations in scene understanding.

Method: They developed a latent diffusion model (MetamerGen) that uses dual-stream representations combining low-resolution 'periphery' with high-resolution 'foveated' features.

Result: A behavioral experiment showed that MetamerGen generates images aligning well with human latent scene understanding when conditioned on viewers' fixation regions.

Conclusion: MetamerGen provides insights into human visual processing and is a strong tool for studying scene understanding, with semantic alignment improving when using real fixation data.

Abstract: Human vision combines low-resolution "gist" information from the visual periphery with sparse but high-resolution information from fixated locations to construct a coherent understanding of a visual scene. In this paper, we introduce MetamerGen, a tool for generating scenes that are aligned with latent human scene representations. MetamerGen is a latent diffusion model that combines peripherally obtained scene gist information with information obtained from scene-viewing fixations to generate image metamers for what humans understand after viewing a scene. Generating images from both high and low resolution (i.e. "foveated") inputs constitutes a novel image-to-image synthesis problem, which we tackle by introducing a dual-stream representation of the foveated scenes consisting of DINOv2 tokens that fuse detailed features from fixated areas with peripherally degraded features capturing scene context. To evaluate the perceptual alignment of MetamerGen generated images to latent human scene representations, we conducted a same-different behavioral experiment where participants were asked for a "same" or "different" response between the generated and the original image. With that, we identify scene generations that are indeed metamers for the latent scene representations formed by the viewers. MetamerGen is a powerful tool for understanding scene understanding. Our proof-of-concept analyses uncovered specific features at multiple levels of visual processing that contributed to human judgments. While it can generate metamers even conditioned on random fixations, we find that high-level semantic alignment most strongly predicts metamerism when the generated scenes are conditioned on viewers' own fixated regions.

</details>


### [320] [Conformal Point and the Calibrated Conic](https://arxiv.org/abs/2601.11679)
*Richard Hartley*

Main category: cs.CV

TL;DR: The paper examines the conformal point and calibrating conic concepts to enhance understanding of image geometry.


<details>
  <summary>Details</summary>
Motivation: Improve visualization and computation of geometry in images through intuitive concepts.

Method: Introduce and explore the relationships between conformal point and calibrating conic, leveraging them for image geometry analysis.

Result: Provides insights into computing geometry, angles, and directions in images using these concepts.

Conclusion: Conformal point and calibrating conic concepts streamline intuitive understanding and computation of image geometry.

Abstract: This gives some information about the conformal point and the calibrating conic, and their relationship one to the other. These concepts are useful for visualizing image geometry, and lead to intuitive ways to compute geometry, such as angles and directions in an image.

</details>


### [321] [Telling Human and Machine Handwriting Apart](https://arxiv.org/abs/2601.11700)
*Luis A. Leiva,Moises Diaz,Nuwan T. Attygalle,Miguel A. Ferrer,Rejean Plamondon*

Main category: cs.CV

TL;DR: The paper focuses on identifying fake handwriting data using a recurrent neural network, achieving high performance across datasets and synthesis methods.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for systems to verify human presence and detect artificial handwriting inputs for improved security.

Method: A shallow recurrent neural network is trained on non-featurized trajectory data from ten public datasets, combining human and artificially generated handwriting symbols from various synthesis models.

Result: The system achieved 98.3% Area Under the ROC Curve (AUC) and 1.4% equal error rate, even in few-shot and out-of-domain settings.

Conclusion: The proposed method is effective at detecting synthetic handwriting, improving verification frameworks for behavioral biometrics and security applications.

Abstract: Handwriting movements can be leveraged as a unique form of behavioral biometrics, to verify whether a real user is operating a device or application. This task can be framed as a reverse Turing test in which a computer has to detect if an input instance has been generated by a human or artificially. To tackle this task, we study ten public datasets of handwritten symbols (isolated characters, digits, gestures, pointing traces, and signatures) that are artificially reproduced using seven different synthesizers, including, among others, the Kinematic Theory (Sigma h model), generative adversarial networks, Transformers, and Diffusion models. We train a shallow recurrent neural network that achieves excellent performance (98.3 percent Area Under the ROC Curve (AUC) score and 1.4 percent equal error rate on average across all synthesizers and datasets) using nonfeaturized trajectory data as input. In few-shot settings, we show that our classifier achieves such an excellent performance when trained on just 10 percent of the data, as evaluated on the remaining 90% of the data as a test set. We further challenge our classifier in out-of-domain settings, and observe very competitive results as well. Our work has implications for computerized systems that need to verify human presence, and adds an additional layer of security to keep attackers at bay.

</details>


### [322] [SemAlign: Language Guided Semi-supervised Domain Generalization](https://arxiv.org/abs/2601.11724)
*Muditha Fernando,Kajhanan Kailainathan,Krishnakanth Nagaratnam,Isuranga Udaravi Bandara Senavirathne,Ranga Rodrigo*

Main category: cs.CV

TL;DR: This paper proposes a novel approach to Semi-supervised Domain Generalization by aligning intermediate model features with a Vision Language Model and employing augmentation and regularization strategies to maximize data utility and prevent overfitting.


<details>
  <summary>Details</summary>
Motivation: To address limitations in SSDG methods rooted in excessive focus on pseudo-labeling accuracy and underutilization of training data, which inhibit generalization to unseen target domains.

Method: The authors align intermediate model features with the semantically rich and domain-invariant feature space of Vision Language Models (VLM) and supplement this with effective image-level augmentations and output-level regularization.

Result: The proposed method achieves state-of-the-art performance across four benchmarks, demonstrating both qualitative and quantitative improvements over existing SSDG baselines.

Conclusion: The novel approach effectively utilizes data, improves generalization across domains, and sets new benchmarks in SSDG. This work also emphasizes a shift from pseudo-labeling reliance toward more generalized training methods.

Abstract: Semi-supervised Domain Generalization (SSDG) addresses the challenge of generalizing to unseen target domains with limited labeled data. Existing SSDG methods highlight the importance of achieving high pseudo-labeling (PL) accuracy and preventing model overfitting as the main challenges in SSDG. In this light, we show that the SSDG literature's excessive focus on PL accuracy, without consideration for maximum data utilization during training, limits potential performance improvements. We propose a novel approach to the SSDG problem by aligning the intermediate features of our model with the semantically rich and generalized feature space of a Vision Language Model (VLM) in a way that promotes domain-invariance. The above approach is enhanced with effective image-level augmentation and output-level regularization strategies to improve data utilization and minimize overfitting. Extensive experimentation across four benchmarks against existing SSDG baselines suggests that our method achieves SOTA results both qualitatively and quantitatively. The code will be made publicly available.

</details>


### [323] [SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models](https://arxiv.org/abs/2601.11729)
*Turhan Can Kargin,Wojciech Jasiński,Adam Pardyl,Bartosz Zieliński,Marcin Przewięźlikowski*

Main category: cs.CV

TL;DR: The paper introduces SpaRRTa, a benchmark to evaluate spatial reasoning in Visual Foundation Models (VFMs), addressing their limited spatial awareness despite excelling in semantic understanding.


<details>
  <summary>Details</summary>
Motivation: Most Visual Foundation Models (VFMs) focus on semantic understanding but struggle with spatial reasoning. Current efforts incorporating 3D tasks like depth estimation have revealed inconsistent performance, raising questions about their spatial awareness.

Method: The paper proposes SpaRRTa, a benchmark for testing VFMs' spatial reasoning abilities by evaluating their capacity to recognize relative object positions in images. SpaRRTa utilizes photorealistic scenes with controllable object arrangements and spatial annotations.

Result: The study highlights significant disparities in spatial reasoning capabilities among state-of-the-art VFMs. The SpaRRTa benchmark effectively differentiates models based on their spatial awareness.

Conclusion: SpaRRTa proves to be a valuable tool for understanding spatial reasoning in VFMs and can guide future model development to improve spatial awareness.

Abstract: Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic understanding of images but exhibit limited spatial reasoning capabilities, which limits their applicability to embodied systems. As a result, recent work incorporates some 3D tasks (such as depth estimation) into VFM training. However, VFM performance remains inconsistent across other spatial tasks, raising the question of whether these models truly have spatial awareness or overfit to specific 3D objectives. To address this question, we introduce the Spatial Relation Recognition Task (SpaRRTa) benchmark, which evaluates the ability of VFMs to identify relative positions of objects in the image. Unlike traditional 3D objectives that focus on precise metric prediction (e.g., surface normal estimation), SpaRRTa probes a fundamental capability underpinning more advanced forms of human-like spatial understanding. SpaRRTa generates an arbitrary number of photorealistic images with diverse scenes and fully controllable object arrangements, along with freely accessible spatial annotations. Evaluating a range of state-of-the-art VFMs, we reveal significant disparities between their spatial reasoning abilities. Through our analysis, we provide insights into the mechanisms that support or hinder spatial awareness in modern VFMs. We hope that SpaRRTa will serve as a useful tool for guiding the development of future spatially aware visual models.

</details>


### [324] [From Pixels to Purchase: Building and Evaluating a Taxonomy-Decoupled Visual Search Engine for Home Goods E-commerce](https://arxiv.org/abs/2601.11769)
*Cheng Lyu,Jingyue Zhang,Ryan Maunu,Mengwei Li,Vinny DeGenova,Yuanli Pei*

Main category: cs.CV

TL;DR: The paper introduces a new visual search system for e-commerce using classification-free proposals and a novel evaluation framework leveraging large language models.


<details>
  <summary>Details</summary>
Motivation: Existing e-commerce visual search systems rely on noisy catalog data and rigid taxonomies, leading to limited scalability and robustness.

Method: A taxonomy-independent architecture is developed using unified embeddings for flexible retrieval and a large language model (LLM) framework for zero-shot evaluation.

Result: The system enhances search retrieval quality, boosts customer engagement, and achieves strong correlation between offline metrics and real-world performance when deployed at scale.

Conclusion: The proposed approach offers a scalable and robust alternative to traditional methods, improving e-commerce search systems' flexibility and evaluation quality.

Abstract: Visual search is critical for e-commerce, especially in style-driven domains where user intent is subjective and open-ended. Existing industrial systems typically couple object detection with taxonomy-based classification and rely on catalog data for evaluation, which is prone to noise that limits robustness and scalability. We propose a taxonomy-decoupled architecture that uses classification-free region proposals and unified embeddings for similarity retrieval, enabling a more flexible and generalizable visual search. To overcome the evaluation bottleneck, we propose an LLM-as-a-Judge framework that assesses nuanced visual similarity and category relevance for query-result pairs in a zero-shot manner, removing dependence on human annotations or noise-prone catalog data. Deployed at scale on a global home goods platform, our system improves retrieval quality and yields a measurable uplift in customer engagement, while our offline evaluation metrics strongly correlate with real-world outcomes.

</details>


### [325] [studentSplat: Your Student Model Learns Single-view 3D Gaussian Splatting](https://arxiv.org/abs/2601.11772)
*Yimu Pan,Hongda Mao,Qingshuang Chen,Yelin Kim*

Main category: cs.CV

TL;DR: StudentSplat introduces a method for single-view 3D scene reconstruction, tackling scale ambiguity and extrapolation challenges using a teacher-student architecture and scene extrapolation network.


<details>
  <summary>Details</summary>
Motivation: Single-view 3D scene reconstruction faces challenges due to scale ambiguity and limited scene context, requiring innovative approaches to overcome these constraints.

Method: The method uses a teacher-student architecture where a multi-view teacher model supervises the single-view student during training for geometric accuracy. Additionally, an extrapolation network is introduced to complete the missing scene context.

Result: Extensive experiments validate that StudentSplat achieves state-of-the-art performance for single-view novel-view reconstruction, comparable to multi-view methods, and shows competitive potential for single-view depth estimation.

Conclusion: StudentSplat advances single-view 3D Gaussian splatting, enabling high-quality reconstruction and extrapolation, positioning itself for broader 3D understanding tasks.

Abstract: Recent advance in feed-forward 3D Gaussian splatting has enable remarkable multi-view 3D scene reconstruction or single-view 3D object reconstruction but single-view 3D scene reconstruction remain under-explored due to inherited ambiguity in single-view. We present \textbf{studentSplat}, a single-view 3D Gaussian splatting method for scene reconstruction. To overcome the scale ambiguity and extrapolation problems inherent in novel-view supervision from a single input, we introduce two techniques: 1) a teacher-student architecture where a multi-view teacher model provides geometric supervision to the single-view student during training, addressing scale ambiguity and encourage geometric validity; and 2) an extrapolation network that completes missing scene context, enabling high-quality extrapolation. Extensive experiments show studentSplat achieves state-of-the-art single-view novel-view reconstruction quality and comparable performance to multi-view methods at the scene level. Furthermore, studentSplat demonstrates competitive performance as a self-supervised single-view depth estimation method, highlighting its potential for general single-view 3D understanding tasks.

</details>


### [326] [Cross-Domain Object Detection Using Unsupervised Image Translation](https://arxiv.org/abs/2601.11779)
*Vinicius F. Arruda,Rodrigo F. Berriel,Thiago M. Paixão,Claudine Badue,Alberto F. De Souza,Nicu Sebe,Thiago Oliveira-Santos*

Main category: cs.CV

TL;DR: This paper proposes a simplified and interpretable method for unsupervised domain adaptation in object detection by generating artificial datasets using image translators and achieves state-of-the-art results in autonomous driving scenarios.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised domain adaptation methods for object detection are effective but complex to implement and interpret, and there exists a performance gap compared to training with target domain data.

Method: The authors generate artificial datasets in the target domain using unsupervised image translators like CycleGAN and an AdaIN-based model, leveraging annotated source domain data and non-annotated target domain data.

Result: The proposed method shows significant improvements in object detection performance, successfully outperforming state-of-the-art methods in most autonomous driving scenarios.

Conclusion: The method simplifies the process of unsupervised domain adaptation and improves interpretability, advancing towards closing the performance gap with upper-bound approaches.

Abstract: Unsupervised domain adaptation for object detection addresses the adaption of detectors trained in a source domain to work accurately in an unseen target domain. Recently, methods approaching the alignment of the intermediate features proven to be promising, achieving state-of-the-art results. However, these methods are laborious to implement and hard to interpret. Although promising, there is still room for improvements to close the performance gap toward the upper-bound (when training with the target data). In this work, we propose a method to generate an artificial dataset in the target domain to train an object detector. We employed two unsupervised image translators (CycleGAN and an AdaIN-based model) using only annotated data from the source domain and non-annotated data from the target domain. Our key contributions are the proposal of a less complex yet more effective method that also has an improved interpretability. Results on real-world scenarios for autonomous driving show significant improvements, outperforming state-of-the-art methods in most cases, further closing the gap toward the upper-bound.

</details>


### [327] [Digital FAST: An AI-Driven Multimodal Framework for Rapid and Early Stroke Screening](https://arxiv.org/abs/2601.11896)
*Ngoc-Khai Hoang,Thi-Nhu-Mai Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: This paper proposes a multimodal deep learning framework for early stroke screening using facial dynamics, speech signals, and upper-body movements, achieving high accuracy and balancing sensitivity and specificity.


<details>
  <summary>Details</summary>
Motivation: To develop a fast and reliable method for early stroke detection in prehospital scenarios, utilizing multimodal data to enhance diagnostic accuracy.

Method: The framework analyzes data from facial dynamics, speech signals, and upper-body movements using various deep learning architectures (Transformer, Audio Spectrogram Transformer, MLP-Mixer). It fuses the extracted representations using attention-based mechanisms.

Result: Experiments on a dataset of 222 videos achieved 95.83% accuracy and 96.00% F1-score, consistently outperforming unimodal baselines.

Conclusion: The study demonstrates the effectiveness of multimodal and transfer learning for stroke detection but highlights the need for larger datasets for real-world application.

Abstract: Early identification of stroke symptoms is essential for enabling timely intervention and improving patient outcomes, particularly in prehospital settings. This study presents a fast, non-invasive multimodal deep learning framework for automatic binary stroke screening based on data collected during the F.A.S.T. assessment. The proposed approach integrates complementary information from facial expressions, speech signals, and upper-body movements to enhance diagnostic robustness. Facial dynamics are represented using landmark based features and modeled with a Transformer architecture to capture temporal dependencies. Speech signals are converted into mel spectrograms and processed using an Audio Spectrogram Transformer, while upper-body pose sequences are analyzed with an MLP-Mixer network to model spatiotemporal motion patterns. The extracted modality specific representations are combined through an attention-based fusion mechanism to effectively learn cross modal interactions. Experiments conducted on a self-collected dataset of 222 videos from 37 subjects demonstrate that the proposed multimodal model consistently outperforms unimodal baselines, achieving 95.83% accuracy and a 96.00% F1-score. The model attains a strong balance between sensitivity and specificity and successfully detects all stroke cases in the test set. These results highlight the potential of multimodal learning and transfer learning for early stroke screening, while emphasizing the need for larger, clinically representative datasets to support reliable real-world deployment.

</details>


### [328] [RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection](https://arxiv.org/abs/2601.11898)
*Yilmaz Korkmaz,Vishal M. Patel*

Main category: cs.CV

TL;DR: The paper introduces RemoteVAR, an autoregressive model for remote sensing change detection, showing significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: To enhance pixel-level change detection for remote sensing applications by addressing limitations in visual autoregressive models (VARs).

Method: RemoteVAR conditions autoregressive predictions on multi-resolution fused bi-temporal features via cross-attention and applies a specific autoregressive training strategy for change detection.

Result: Experiments demonstrate that RemoteVAR outperforms diffusion-based and transformer-based approaches in standard benchmarks for change detection.

Conclusion: RemoteVAR provides a competitive and effective autoregressive alternative for remote sensing change detection, advancing the field with improved performance.

Abstract: Remote sensing change detection aims to localize and characterize scene changes between two time points and is central to applications such as environmental monitoring and disaster assessment. Meanwhile, visual autoregressive models (VARs) have recently shown impressive image generation capability, but their adoption for pixel-level discriminative tasks remains limited due to weak controllability, suboptimal dense prediction performance and exposure bias. We introduce RemoteVAR, a new VAR-based change detection framework that addresses these limitations by conditioning autoregressive prediction on multi-resolution fused bi-temporal features via cross-attention, and by employing an autoregressive training strategy designed specifically for change map prediction. Extensive experiments on standard change detection benchmarks show that RemoteVAR delivers consistent and significant improvements over strong diffusion-based and transformer-based baselines, establishing a competitive autoregressive alternative for remote sensing change detection. Code will be available \href{https://github.com/yilmazkorkmaz1/RemoteVAR}{\underline{here}}.

</details>


### [329] [Towards Airborne Object Detection: A Deep Learning Analysis](https://arxiv.org/abs/2601.11907)
*Prosenjit Chatterjee,ANK Zaman*

Main category: cs.CV

TL;DR: This paper proposes a dual-task model based on EfficientNetB4 for airborne object classification and threat-level prediction, achieving high accuracy levels and introducing a new dataset, AODTA.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the scalability and inefficiencies in manual monitoring systems for airborne object threat assessment, driven by increased airborne platform usage.

Method: The authors developed a dual-task model using EfficientNetB4 and introduced the AODTA Dataset to alleviate data scarcity issues, benchmarking against existing datasets and models.

Result: EfficientNetB4 achieved 96% accuracy in object classification and 90% accuracy in threat-level prediction, outperforming ResNet-50.

Conclusion: The study demonstrates the potential of EfficientNetB4 in boosting real-time airborne object classification and threat-level assessment, showing promise for surveillance and airspace management applications.

Abstract: The rapid proliferation of airborne platforms, including commercial aircraft, drones, and UAVs, has intensified the need for real-time, automated threat assessment systems. Current approaches depend heavily on manual monitoring, resulting in limited scalability and operational inefficiencies. This work introduces a dual-task model based on EfficientNetB4 capable of performing airborne object classification and threat-level prediction simultaneously. To address the scarcity of clean, balanced training data, we constructed the AODTA Dataset by aggregating and refining multiple public sources. We benchmarked our approach on both the AVD Dataset and the newly developed AODTA Dataset and further compared performance against a ResNet-50 baseline, which consistently underperformed EfficientNetB4. Our EfficientNetB4 model achieved 96% accuracy in object classification and 90% accuracy in threat-level prediction, underscoring its promise for applications in surveillance, defense, and airspace management. Although the title references detection, this study focuses specifically on classification and threat-level inference using pre-localized airborne object images provided by existing datasets.

</details>


### [330] [Effects of the retina-inspired light intensity encoding on color discrimination performance](https://arxiv.org/abs/2601.11909)
*Io Yamada,Hirotsugu Okuno*

Main category: cs.CV

TL;DR: The paper studies color constancy using the center/surround retinex model and evaluates various encoding functions for light intensity, finding that the Naka-Rushton function improves discrimination performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of color constancy in visual systems where object recognition relies on accurate color information unaffected by illumination.

Method: Tested light intensity encoding functions such as the logarithmic function and the Naka-Rushton function in the retinex model using color-variable LEDs and color space representations like HSV and double opponent color planes.

Result: The combination of the Naka-Rushton function with the double opponent color plane provided better color discrimination performance.

Conclusion: The Naka-Rushton function and advanced color representation techniques enhance the color constancy and discrimination capabilities of visual target systems.

Abstract: Color is an important source of information for visual functions such as object recognition, but it is greatly affected by the color of illumination. The ability to perceive the color of a visual target independent of illumination color is called color constancy (CC), and is an important feature for vision systems that use color information. In this study, we investigated the effects of the light intensity encoding function on the performance of CC of the center/surround (C/S) retinex model, which is a well-known model inspired by CC of the visual nervous system. The functions used to encode light intensity are the logarithmic function used in the original C/S retinex model and the Naka-Rushton (N-R) function, which is a model of retinal photoreceptor response. Color-variable LEDs were used to illuminate visual targets with various lighting colors, and color information computed by each model was used to evaluate the degree to which the color of visual targets illuminated with different lighting colors could be discriminated. Color information was represented using the HSV color space and a color plane based on the classical opponent color theory. The results showed that the combination of the N-R function and the double opponent color plane representation provided superior discrimination performance.

</details>


### [331] [A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection](https://arxiv.org/abs/2601.11910)
*Guiying Zhu,Bowen Yang,Yin Zhuang,Tong Zhang,Guanqun Wang,Zhihao Che,He Chen,Lianlin Li*

Main category: cs.CV

TL;DR: The paper introduces a training-free Guess What Vision Language Model (GW-VLM) for Open-Vocabulary Object Detection (OVOD). It leverages Multi-Scale Visual Language Searching (MS-VLS) and Contextual Concept Prompt (CCP) to utilize pre-trained Vision Language Models and Large Language Models for effective OVOD.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the overlooked necessity of forming a universal understanding of object cognition for OVOD using already pre-trained foundation models.

Method: The method involves a training-free framework (GW-VLM), incorporating MS-VLS for soft alignment between visual and language features, and CCP for contextual prompt generation to facilitate interaction between VLMs and LLMs.

Result: Experiments conducted on datasets like COCO val, Pascal VOC, DIOR, and NWPU-10 demonstrate that GW-VLM achieves superior OVOD results compared to state-of-the-art approaches without requiring additional training.

Conclusion: GW-VLM serves as an innovative training-free approach for OVOD that successfully utilizes foundation models to enhance detection performance while avoiding the need for further model training.

Abstract: Open-Vocabulary Object Detection (OVOD) aims to develop the capability to detect anything. Although myriads of large-scale pre-training efforts have built versatile foundation models that exhibit impressive zero-shot capabilities to facilitate OVOD, the necessity of creating a universal understanding for any object cognition according to already pretrained foundation models is usually overlooked. Therefore, in this paper, a training-free Guess What Vision Language Model, called GW-VLM, is proposed to form a universal understanding paradigm based on our carefully designed Multi-Scale Visual Language Searching (MS-VLS) coupled with Contextual Concept Prompt (CCP) for OVOD. This approach can engage a pre-trained Vision Language Model (VLM) and a Large Language Model (LLM) in the game of "guess what". Wherein, MS-VLS leverages multi-scale visual-language soft-alignment for VLM to generate snippets from the results of class-agnostic object detection, while CCP can form the concept of flow referring to MS-VLS and then make LLM understand snippets for OVOD. Finally, the extensive experiments are carried out on natural and remote sensing datasets, including COCO val, Pascal VOC, DIOR, and NWPU-10, and the results indicate that our proposed GW-VLM can achieve superior OVOD performance compared to the-state-of-the-art methods without any training step.

</details>


### [332] [Reliable Deep Learning for Small-Scale Classifications: Experiments on Real-World Image Datasets from Bangladesh](https://arxiv.org/abs/2601.11911)
*Muhammad Ibrahim,Alfe Suny,MD Sakib Ul Islam,Md. Imran Hossain*

Main category: cs.CV

TL;DR: The paper evaluates a compact convolutional neural network (CNN) for performance on small-class image datasets from Bangladesh, showing high accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the overfitting and complexity of CNNs for small datasets, focusing on achieving high performance with a streamlined model.

Method: A compact CNN was tested on five real-world image datasets related to various domains such as urban encroachment and agricultural crops.

Result: The compact model achieved high accuracy, efficient convergence, and low computational requirements, as well as robust generalization.

Conclusion: Streamlined CNN architectures can be effective for diverse, small-class image classification tasks.

Abstract: Convolutional neural networks (CNNs) have achieved state-of-the-art performance in image recognition tasks but often involve complex architectures that may overfit on small datasets. In this study, we evaluate a compact CNN across five publicly available, real-world image datasets from Bangladesh, including urban encroachment, vehicle detection, road damage, and agricultural crops. The network demonstrates high classification accuracy, efficient convergence, and low computational overhead. Quantitative metrics and saliency analyses indicate that the model effectively captures discriminative features and generalizes robustly across diverse scenarios, highlighting the suitability of streamlined CNN architectures for small-class image classification tasks.

</details>


### [333] [From Spurious to Causal: Low-rank Orthogonal Subspace Intervention for Generalizable Face Forgery Detection](https://arxiv.org/abs/2601.11915)
*Chi Wang,Xinjue Hu,Boyu Wang,Ziwen He,Zhangjie Fu*

Main category: cs.CV

TL;DR: This paper addresses the challenge of biased learning caused by spurious correlations in face forgery detection. It proposes a method to model and remove these correlations through low-rank projection and achieves high generalization performance.


<details>
  <summary>Details</summary>
Motivation: Generalization in face forgery detection is hindered by forgery-irrelevant information, termed spurious correlation factors, making unbiased learning difficult.

Method: The paper proposes intervening in these spurious correlation factors by modeling them as a low-rank subspace, removing this subspace using orthogonal low-rank projection, and focusing on forgery cues in its orthogonal complement.

Result: The proposed method achieves state-of-the-art performance with only 0.43M trainable parameters, demonstrating robustness and improved generalization across benchmarks.

Conclusion: Removing spurious correlation factors via low-rank projection ensures classification decisions rely on forgery-related features, solving the generalization issue effectively.

Abstract: The generalization problem remains a critical challenge in face forgery detection. Some researches have discovered that ``a backdoor path" in the representations from forgery-irrelevant information to labels induces biased learning, thereby hindering the generalization. In this paper, these forgery-irrelevant information are collectively termed spurious correlations factors. Previous methods predominantly focused on identifying concrete, specific spurious correlation and designing corresponding solutions to address them. However, spurious correlations arise from unobservable confounding factors, making it impractical to identify and address each one individually. To address this, we propose an intervention paradigm for representation space. Instead of tracking and blocking various instance-level spurious correlation one by one, we uniformly model them as a low-rank subspace and intervene in them. Specifically, we decompose spurious correlation features into a low-rank subspace via orthogonal low-rank projection, subsequently removing this subspace from the original representation and training its orthogonal complement to capture forgery-related features. This low-rank projection removal effectively eliminates spurious correlation factors, ensuring that classification decision is based on authentic forgery cues. With only 0.43M trainable parameters, our method achieves state-of-the-art performance across several benchmarks, demonstrating excellent robustness and generalization.

</details>


### [334] [Effects of Gabor Filters on Classification Performance of CNNs Trained on a Limited Number of Conditions](https://arxiv.org/abs/2601.11918)
*Akito Morita,Hirotsugu Okuno*

Main category: cs.CV

TL;DR: The study proposes using Gabor filter preprocessing to enhance CNN accuracy and reduce model size for robot vision on edge devices, particularly with limited training data.


<details>
  <summary>Details</summary>
Motivation: CNNs on edge devices for robot vision must have compact architectures and be effective for training with minimal and limited-condition data.

Method: Integrating Gabor filters, modeled after the visual nervous system's feature extractor, as preprocessing to enhance CNN accuracy. Performance was tested using datasets designed to mimic limited variations of conditions for object recognition.

Result: The study's findings show that preprocessing CNNs with Gabor filters improves accuracy and generalization while enabling smaller model sizes.

Conclusion: Using Gabor filters as preprocessing is beneficial for augmenting CNN performance and reducing size, thus enhancing their applicability for robot vision in constrained environments.

Abstract: In this study, we propose a technique to improve the accuracy and reduce the size of convolutional neural networks (CNNs) running on edge devices for real-world robot vision applications. CNNs running on edge devices must have a small architecture, and CNNs for robot vision applications involving on-site object recognition must be able to be trained efficiently to identify specific visual targets from data obtained under a limited variation of conditions. The visual nervous system (VNS) is a good example that meets the above requirements because it learns from few visual experiences. Therefore, we used a Gabor filter, a model of the feature extractor of the VNS, as a preprocessor for CNNs to investigate the accuracy of the CNNs trained with small amounts of data. To evaluate how well CNNs trained on image data acquired under a limited variation of conditions generalize to data acquired under other conditions, we created an image dataset consisting of images acquired from different camera positions, and investigated the accuracy of the CNNs that trained using images acquired at a certain distance. The results were compared after training on multiple CNN architectures with and without Gabor filters as preprocessing. The results showed that preprocessing with Gabor filters improves the generalization performance of CNNs and contributes to reducing the size of CNNs.

</details>


### [335] [SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM](https://arxiv.org/abs/2601.11930)
*Xulei Shi,Maoyu Wang,Yuning Peng,Guanbo Wang,Xin Wang,Qi Chen,Pengjie Tao*

Main category: cs.CV

TL;DR: The paper proposes SupScene, a method to improve image retrieval for geometric matchability in SfM using novel training strategies and descriptor aggregation techniques.


<details>
  <summary>Details</summary>
Motivation: To address the gap in existing image retrieval methods that focus on semantic similarity rather than geometric matchability in SfM.

Method: Introduced SupScene with a subgraph-based soft supervised contrastive loss and a novel DiVLAD aggregator utilizing ViT attention maps with a gating mechanism.

Result: Achieved state-of-the-art performance on the GL3D dataset, surpassing NetVLAD with minimal additional trainable parameters.

Conclusion: SupScene offers significant improvements in image retrieval for SfM, with effective training strategies benefiting various aggregation techniques, and code is made available for further research.

Abstract: Image retrieval is a critical step for alleviating the quadratic complexity of image matching in unconstrained Structure-from-Motion (SfM). However, in this context, image retrieval typically focuses more on the image pairs of geometric matchability than on those of semantic similarity, a nuance that most existing deep learning-based methods guided by batched binaries (overlapping vs. non-overlapping pairs) fail to capture. In this paper, we introduce SupScene, a novel solution that learns global descriptors tailored for finding overlapping image pairs of similar geometric nature for SfM. First, to better underline co-visible regions, we employ a subgraph-based training strategy that moves beyond equally important isolated pairs, leveraging ground-truth geometric overlapping relationships with various weights to provide fine-grained supervision via a soft supervised contrastive loss. Second, we introduce DiVLAD, a DINO-inspired VLAD aggregator that leverages the inherent multi-head attention maps from the last block of ViT. And then, a learnable gating mechanism is designed to adaptively utilize these semantically salient cues with visual features, enabling a more discriminative global descriptor. Extensive experiments on the GL3D dataset demonstrate that our method achieves state-of-the-art performance, significantly outperforming NetVLAD while introducing a negligible number of additional trainable parameters. Furthermore, we show that the proposed training strategy brings consistent gains across different aggregation techniques. Code and models are available at https://anonymous.4open.science/r/SupScene-5B73.

</details>


### [336] [Language-Guided and Motion-Aware Gait Representation for Generalizable Recognition](https://arxiv.org/abs/2601.11931)
*Zhengxian Wu,Chuanrui Zhang,Shenao Jiang,Hangrui Xu,Zirui Liao,Luyuan Zhang,Huaqiu Li,Peng Jiao,Haoqian Wang*

Main category: cs.CV

TL;DR: LMGait introduces a novel framework for gait recognition using language-guided and motion-aware techniques, addressing challenges like overfitting on static noise and ineffective motion feature capture.


<details>
  <summary>Details</summary>
Motivation: Existing gait recognition methods face challenges due to their reliance on complex architectures, which often lead to overfitting on static noise (like clothing) and fail to effectively capture dynamic motion regions.

Method: LMGait employs manually designed gait-related language cues to highlight key motion features in gait sequences, improving representation extraction.

Result: This approach addresses the issue of capturing dynamic motion while avoiding overfitting to static noise, enhancing the performance of gait recognition.

Conclusion: LMGait demonstrates an innovative and efficient way to tackle traditional drawbacks in gait recognition, leveraging language cues for better motion feature representation.

Abstract: Gait recognition is emerging as a promising technology and an innovative field within computer vision. However, existing methods typically rely on complex architectures to directly extract features from images and apply pooling operations to obtain sequence-level representations. Such designs often lead to overfitting on static noise (e.g., clothing), while failing to effectively capture dynamic motion regions.To address the above challenges, we present a Language guided and Motion-aware gait recognition framework, named LMGait.In particular, we utilize designed gait-related language cues to capture key motion features in gait sequences.

</details>


### [337] [Deep learning-based neurodevelopmental assessment in preterm infants](https://arxiv.org/abs/2601.11944)
*Lexin Ren,Jiamiao Lu,Weichuan Zhang,Benqing Wu,Tuo Wang,Yi Liao,Jiapan Guo,Changming Sun,Liang Guo*

Main category: cs.CV

TL;DR: The research proposes a novel neural network for brain MRI segmentation in preterm infants, achieving superior accuracy and identifying neurodevelopmental delays.


<details>
  <summary>Details</summary>
Motivation: Early identification of neurodevelopmental delays in preterm infants is critical, but existing segmentation techniques face challenges differentiating white and gray matter in MRI scans during early development.

Method: The study introduces the Hierarchical Dense Attention Network, incorporating 3D spatial-channel attention mechanisms and attention-guided dense upsampling for improved low-contrast tissue segmentation.

Result: The proposed method outperforms state-of-the-art techniques in segmentation accuracy and demonstrates that preterm infants have significantly lower WM and GM volumes than term infants.

Conclusion: The Hierarchical Dense Attention Network is an effective tool for addressing MRI segmentation challenges and provides crucial insights into neurodevelopmental delays in preterm infants.

Abstract: Preterm infants (born between 28 and 37 weeks of gestation) face elevated risks of neurodevelopmental delays, making early identification crucial for timely intervention. While deep learning-based volumetric segmentation of brain MRI scans offers a promising avenue for assessing neonatal neurodevelopment, achieving accurate segmentation of white matter (WM) and gray matter (GM) in preterm infants remains challenging due to their comparable signal intensities (isointense appearance) on MRI during early brain development. To address this, we propose a novel segmentation neural network, named Hierarchical Dense Attention Network. Our architecture incorporates a 3D spatial-channel attention mechanism combined with an attention-guided dense upsampling strategy to enhance feature discrimination in low-contrast volumetric data. Quantitative experiments demonstrate that our method achieves superior segmentation performance compared to state-of-the-art baselines, effectively tackling the challenge of isointense tissue differentiation. Furthermore, application of our algorithm confirms that WM and GM volumes in preterm infants are significantly lower than those in term infants, providing additional imaging evidence of the neurodevelopmental delays associated with preterm birth. The code is available at: https://github.com/ICL-SUST/HDAN.

</details>


### [338] [Decoder Gradient Shields: A Family of Provable and High-Fidelity Methods Against Gradient-Based Box-Free Watermark Removal](https://arxiv.org/abs/2601.11952)
*Haonan An,Guang Hua,Wei Du,Hangcheng Cao,Yihang Tao,Guowen Xu,Susanto Rahardja,Yuguang Fang*

Main category: cs.CV

TL;DR: The paper introduces Gradient Shields (DGSs) as robust defenses for protecting box-free model watermarking decoders against gradient-based attacks.


<details>
  <summary>Details</summary>
Motivation: To address a critical vulnerability in box-free model watermarking decoders, where attackers use gradients to train watermark removal mechanisms.

Method: The authors propose Decoder Gradient Shields (DGSs), including variants like DGS-O, DGS-I, and DGS-L, which manipulate gradients to block attacker convergence while maintaining the decoder's image quality.

Result: Experiments on deraining and image generation tasks reveal DGS mechanisms achieve a 100% defense success rate in all tested scenarios while preserving output quality.

Conclusion: The Decoder Gradient Shields (DGSs) ensure robust protection of box-free watermarking decoders against gradient-based attacks, maintaining high image quality.

Abstract: Box-free model watermarking has gained significant attention in deep neural network (DNN) intellectual property protection due to its model-agnostic nature and its ability to flexibly manage high-entropy image outputs from generative models. Typically operating in a black-box manner, it employs an encoder-decoder framework for watermark embedding and extraction. While existing research has focused primarily on the encoders for the robustness to resist various attacks, the decoders have been largely overlooked, leading to attacks against the watermark. In this paper, we identify one such attack against the decoder, where query responses are utilized to obtain backpropagated gradients to train a watermark remover. To address this issue, we propose Decoder Gradient Shields (DGSs), a family of defense mechanisms, including DGS at the output (DGS-O), at the input (DGS-I), and in the layers (DGS-L) of the decoder, with a closed-form solution for DGS-O and provable performance for all DGS. Leveraging the joint design of reorienting and rescaling of the gradients from watermark channel gradient leaking queries, the proposed DGSs effectively prevent the watermark remover from achieving training convergence to the desired low-loss value, while preserving image quality of the decoder output. We demonstrate the effectiveness of our proposed DGSs in diverse application scenarios. Our experimental results on deraining and image generation tasks with the state-of-the-art box-free watermarking show that our DGSs achieve a defense success rate of 100% under all settings.

</details>


### [339] [Real-Time Multi-Modal Embedded Vision Framework for Object Detection Facial Emotion Recognition and Biometric Identification on Low-Power Edge Platforms](https://arxiv.org/abs/2601.11970)
*S. M. Khalid Bin Zahid,Md. Rakibul Hasan Nishat,Abdul Hasib,Md. Rakibul Hasan,Md. Ashiqussalehin,Md. Sahadat Hossen Sajib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: The paper presents a unified, real-time vision framework for edge devices that integrates object detection, facial recognition, and emotion detection with an adaptive scheduler, achieving high efficiency and accuracy on low-power hardware.


<details>
  <summary>Details</summary>
Motivation: Current intelligent surveillance systems lack a unified and adaptive scheduler for resource optimization on edge devices, limiting their efficiency and capability to handle multi-modal tasks dynamically.

Method: The framework integrates YOLOv8n for object detection, a custom FaceNet-based system for facial recognition, and DeepFace's CNN for emotion classification. An adaptive scheduling mechanism dynamically activates these modules to reduce computational load.

Result: The system reduces computational load by 65% while achieving an object detection AP of 0.861, facial recognition accuracy of 88%, and emotion detection AUC of up to 0.97 at 5.6 frames per second.

Conclusion: The proposed context-aware scheduling within the multi-modal vision framework makes intelligent perception systems highly efficient and suitable for deployment on cost-effective edge hardware, enhancing accessibility and privacy preservation.

Abstract: Intelligent surveillance systems often handle perceptual tasks such as object detection, facial recognition, and emotion analysis independently, but they lack a unified, adaptive runtime scheduler that dynamically allocates computational resources based on contextual triggers. This limits their holistic understanding and efficiency on low-power edge devices. To address this, we present a real-time multi-modal vision framework that integrates object detection, owner-specific face recognition, and emotion detection into a unified pipeline deployed on a Raspberry Pi 5 edge platform. The core of our system is an adaptive scheduling mechanism that reduces computational load by 65\% compared to continuous processing by selectively activating modules such as, YOLOv8n for object detection, a custom FaceNet-based embedding system for facial recognition, and DeepFace's CNN for emotion classification. Experimental results demonstrate the system's efficacy, with the object detection module achieving an Average Precision (AP) of 0.861, facial recognition attaining 88\% accuracy, and emotion detection showing strong discriminatory power (AUC up to 0.97 for specific emotions), while operating at 5.6 frames per second. Our work demonstrates that context-aware scheduling is the key to unlocking complex multi-modal AI on cost-effective edge hardware, making intelligent perception more accessible and privacy-preserving.

</details>


### [340] [AVIR: Adaptive Visual In-Document Retrieval for Efficient Multi-Page Document Question Answering](https://arxiv.org/abs/2601.11976)
*Zongmin Li,Yachuan Li,Lei Kang,Dimosthenis Karatzas,Wenkang Ma*

Main category: cs.CV

TL;DR: The AVIR framework addresses challenges in Multi-page Document Visual Question Answering by reducing computational load and inefficiencies in current LVLM mechanisms, achieving high accuracy with fewer resources.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and accuracy of analyzing long documents for visual question answering, mitigating computational strain and optimizing relevance selection.

Method: A new Adaptive Visual In-document Retrieval (AVIR) framework is introduced where lightweight retrieval assesses question relevance per page, clusters are adaptively formed, and relevant pages are selected for a frozen LVLM-based answer generation without model fine-tuning.

Result: Using AVIR, the required page count for processing is reduced by 70%, achieving an ANLS of 84.58% on the MP-DocVQA dataset, outperforming prior methods with lower computational cost. Successful validation was also done on SlideVQA and DUDE benchmarks.

Conclusion: AVIR is a computationally efficient framework for MP-DocVQA tasks, effectively improving performance without additional model tuning. Its effectiveness across other benchmarks underscores its robustness.

Abstract: Multi-page Document Visual Question Answering (MP-DocVQA) remains challenging because long documents not only strain computational resources but also reduce the effectiveness of the attention mechanism in large vision-language models (LVLMs). We tackle these issues with an Adaptive Visual In-document Retrieval (AVIR) framework. A lightweight retrieval model first scores each page for question relevance. Pages are then clustered according to the score distribution to adaptively select relevant content. The clustered pages are screened again by Top-K to keep the context compact. However, for short documents, clustering reliability decreases, so we use a relevance probability threshold to select pages. The selected pages alone are fed to a frozen LVLM for answer generation, eliminating the need for model fine-tuning. The proposed AVIR framework reduces the average page count required for question answering by 70%, while achieving an ANLS of 84.58% on the MP-DocVQA dataset-surpassing previous methods with significantly lower computational cost. The effectiveness of the proposed AVIR is also verified on the SlideVQA and DUDE benchmarks. The code is available at https://github.com/Li-yachuan/AVIR.

</details>


### [341] [Nip Rumors in the Bud: Retrieval-Guided Topic-Level Adaptation for Test-Time Fake News Video Detection](https://arxiv.org/abs/2601.11981)
*Jian Lang,Rongpei Hong,Ting Zhong,Yong Wang,Fan Zhou*

Main category: cs.CV

TL;DR: The paper introduces RADAR, a new framework designed for on-the-fly adaptation to unseen fake news video topics during testing.


<details>
  <summary>Details</summary>
Motivation: Detecting fake news videos is critical for societal stability, but existing methods fail on unseen or emerging topics due to inconsistent news distributions between training and testing phases.

Method: The RADAR framework uses a retrieval-guided adaptation method with a novel Entropy Selection-Based Retrieval mechanism, Stable Anchor-Guided Alignment module, and Target-Domain Aware Self-Training paradigm to adapt to unseen video topics.

Result: RADAR demonstrates superior performance in adapting to unseen fake news video topics compared to existing methods through extensive experiments.

Conclusion: RADAR offers a robust solution for real-time adaptation to unseen fake news video topics, addressing domain discrepancies and label distribution challenges.

Abstract: Fake News Video Detection (FNVD) is critical for social stability. Existing methods typically assume consistent news topic distribution between training and test phases, failing to detect fake news videos tied to emerging events and unseen topics. To bridge this gap, we introduce RADAR, the first framework that enables test-time adaptation to unseen news videos. RADAR pioneers a new retrieval-guided adaptation paradigm that leverages stable (source-close) videos from the target domain to guide robust adaptation of semantically related but unstable instances. Specifically, we propose an Entropy Selection-Based Retrieval mechanism that provides videos with stable (low-entropy), relevant references for adaptation. We also introduce a Stable Anchor-Guided Alignment module that explicitly aligns unstable instances' representations to the source domain via distribution-level matching with their stable references, mitigating severe domain discrepancies. Finally, our novel Target-Domain Aware Self-Training paradigm can generate informative pseudo-labels augmented by stable references, capturing varying and imbalanced category distributions in the target domain and enabling RADAR to adapt to the fast-changing label distributions. Extensive experiments demonstrate that RADAR achieves superior performance for test-time FNVD, enabling strong on-the-fly adaptation to unseen fake news video topics.

</details>


### [342] [An AI-IoT Based Smart Wheelchair with Gesture-Controlled Mobility, Deep Learning-Based Obstacle Detection, Multi-Sensor Health Monitoring, and Emergency Alert System](https://arxiv.org/abs/2601.11983)
*Md. Asiful Islam,Abdul Hasib,Tousif Mahmud Emon,Khandaker Tabin Hasan,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: The paper introduces an advanced, low-cost AI-IoT smart wheelchair system featuring gesture-based control, real-time obstacle detection, and health monitoring to offer enhanced user safety and autonomy.


<details>
  <summary>Details</summary>
Motivation: Traditional wheelchairs lack dynamic, safe navigation and integrated health monitoring, while existing smart solutions are costly and limited. This drives the need for an affordable and multi-functional smart wheelchair system.

Method: The proposed system uses glove-based gesture control for navigation, YOLOv8 for object detection with auditory feedback, ultrasonic sensors for collision avoidance, and continuous monitoring of vital signs with ThingSpeak integration.

Result: The system achieved 95.5% success in gesture control, 94% accuracy in ultrasonic obstacle detection, and YOLOv8 delivered 91.5% precision, 90.2% recall, and 90.8% F1-score in object detection.

Conclusion: The multi-modal, cost-effective smart wheelchair enhances safety, autonomy, and healthcare integration, bridging research innovation with practical deployment.

Abstract: The growing number of differently-abled and elderly individuals demands affordable, intelligent wheelchairs that combine safe navigation with health monitoring. Traditional wheelchairs lack dynamic features, and many smart alternatives remain costly, single-modality, and limited in health integration. Motivated by the pressing demand for advanced, personalized, and affordable assistive technologies, we propose a comprehensive AI-IoT based smart wheelchair system that incorporates glove-based gesture control for hands-free navigation, real-time object detection using YOLOv8 with auditory feedback for obstacle avoidance, and ultrasonic for immediate collision avoidance. Vital signs (heart rate, SpO$_2$, ECG, temperature) are continuously monitored, uploaded to ThingSpeak, and trigger email alerts for critical conditions. Built on a modular and low-cost architecture, the gesture control achieved a 95.5\% success rate, ultrasonic obstacle detection reached 94\% accuracy, and YOLOv8-based object detection delivered 91.5\% Precision, 90.2\% Recall, and a 90.8\% F1-score. This integrated, multi-modal approach offers a practical, scalable, and affordable solution, significantly enhancing user autonomy, safety, and independence by bridging the gap between innovative research and real-world deployment.

</details>


### [343] [Structural Graph Neural Networks with Anatomical Priors for Explainable Chest X-ray Diagnosis](https://arxiv.org/abs/2601.11987)
*Khaled Berkani*

Main category: cs.CV

TL;DR: The paper proposes a structural graph reasoning framework for explainable diagnosis using convolutional feature maps transformed into graphs with both appearance and spatial coordinates encoded. This model uses custom spatial propagation for structured inference, improving node importance interpretation and diagnostic reasoning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of conventional graph neural networks that lack explicit anatomical priors and rely on generic message-passing, addressing the need for interpretable vision-based diagnosis.

Method: The approach reinterprets convolutional feature maps as patch-level graphs and introduces a structural propagation mechanism that incorporates relative spatial relations for structured inference and reasoning in medical imaging.

Result: The proposed model supports lesion-aware node-level predictions and diagnostic reasoning at the graph level. Additionally, it yields intrinsic explainability via learned node importance scores without relying on post-hoc visualization.

Conclusion: The framework improves interpretability for medical imaging and can be extended to other domains, supporting the use of structured graph-based reasoning in AI systems.

Abstract: We present a structural graph reasoning framework that incorporates explicit anatomical priors for explainable vision-based diagnosis. Convolutional feature maps are reinterpreted as patch-level graphs, where nodes encode both appearance and spatial coordinates, and edges reflect local structural adjacency. Unlike conventional graph neural networks that rely on generic message passing, we introduce a custom structural propagation mechanism that explicitly models relative spatial relations as part of the reasoning process. This design enables the graph to act as an inductive bias for structured inference rather than a passive relational representation. The proposed model jointly supports node-level lesion-aware predictions and graph-level diagnostic reasoning, yielding intrinsic explainability through learned node importance scores without relying on post-hoc visualization techniques. We demonstrate the approach through a chest X-ray case study, illustrating how structural priors guide relational reasoning and improve interpretability. While evaluated in a medical imaging context, the framework is domain-agnostic and aligns with the broader vision of graph-based reasoning across artificial intelligence systems. This work contributes to the growing body of research exploring graphs as computational substrates for structure-aware and explainable learning.

</details>


### [344] [DAOS: A Multimodal In-cabin Behavior Monitoring with Driver Action-Object Synergy Dataset](https://arxiv.org/abs/2601.11990)
*Yiming Li,Chen Cai,Tianyi Liu,Dan Lin,Wenqian Wang,Wenfei Liang,Bingbing Li,Kim-Hui Yap*

Main category: cs.CV

TL;DR: The paper presents a new dataset called DAOS for driver action recognition and introduces a method, AOR-Net, to improve understanding of human-object relations in driving environments.


<details>
  <summary>Details</summary>
Motivation: Most driver-monitoring datasets lack precise object-location annotations and connections between objects and actions, which are vital for reliable action recognition.

Method: The paper introduces the DAOS dataset with detailed annotations and proposes the AOR-Net model that uses multi-level reasoning, chain-of-action mechanisms, and Mixture of Thoughts module to focus on human-object relations for understanding driver actions.

Result: Experiments show the AOR-Net model surpasses state-of-the-art methods in driver action recognition across various datasets.

Conclusion: The DAOS dataset and AOR-Net contribute to more accurate and robust driver action monitoring by incorporating human-object synergy.

Abstract: In driver activity monitoring, movements are mostly limited to the upper body, which makes many actions look similar. To tell these actions apart, human often rely on the objects the driver is using, such as holding a phone compared with gripping the steering wheel. However, most existing driver-monitoring datasets lack accurate object-location annotations or do not link objects to their associated actions, leaving a critical gap for reliable action recognition. To address this, we introduce the Driver Action with Object Synergy (DAOS) dataset, comprising 9,787 video clips annotated with 36 fine-grained driver actions and 15 object classes, totaling more than 2.5 million corresponding object instances. DAOS offers multi-modal, multi-view data (RGB, IR, and depth) from front, face, left, and right perspectives. Although DAOS captures a wide range of cabin objects, only a few are directly relevant to each action for prediction, so focusing on task-specific human-object relations is essential. To tackle this challenge, we propose the Action-Object-Relation Network (AOR-Net). AOR-Net comprehends complex driver actions through multi-level reasoning and a chain-of-action prompting mechanism that models the logical relationships among actions, objects, and their relations. Additionally, the Mixture of Thoughts module is introduced to dynamically select essential knowledge at each stage, enhancing robustness in object-rich and object-scarce conditions. Extensive experiments demonstrate that our model outperforms other state-of-the-art methods on various datasets.

</details>


### [345] [SMc2f: Robust Scenario Mining for Robotic Autonomy from Coarse to Fine](https://arxiv.org/abs/2601.12010)
*Yifei Chen,Ross Greer*

Main category: cs.CV

TL;DR: The paper proposes SMc2f, a new approach to improve scenario mining for autonomous robotic vehicles using vision-language models and a coarse-to-fine pipeline.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of systematically and accurately mining rare, safety-critical scenarios crucial for the development and safety validation of robotic vehicles.

Method: The SMc2f pipeline employs vision-language models for coarse image-text filtering, builds a reusable database of mined cases, improves retrieval robustness through few-shot conditioning, and introduces contrastive learning to enhance matching between text and trajectories.

Result: SMc2f shows significant improvements in both retrieval accuracy and operational efficiency when evaluated on public datasets.

Conclusion: The approach enhances the process of scenario mining for autonomous systems, making it more robust and efficient, addressing previous pitfalls like reliance on trajectory labels and inaccuracies in object tracking.

Abstract: The safety validation of autonomous robotic vehicles hinges on systematically testing their planning and control stacks against rare, safety-critical scenarios. Mining these long-tail events from massive real-world driving logs is therefore a critical step in the robotic development lifecycle. The goal of the Scenario Mining task is to retrieve useful information to enable targeted re-simulation, regression testing, and failure analysis of the robot's decision-making algorithms. RefAV, introduced by the Argoverse team, is an end-to-end framework that uses large language models (LLMs) to spatially and temporally localize scenarios described in natural language. However, this process performs retrieval on trajectory labels, ignoring the direct connection between natural language and raw RGB images, which runs counter to the intuition of video retrieval; it also depends on the quality of upstream 3D object detection and tracking. Further, inaccuracies in trajectory data lead to inaccuracies in downstream spatial and temporal localization. To address these issues, we propose Robust Scenario Mining for Robotic Autonomy from Coarse to Fine (SMc2f), a coarse-to-fine pipeline that employs vision-language models (VLMs) for coarse image-text filtering, builds a database of successful mining cases on top of RefAV and automatically retrieves exemplars to few-shot condition the LLM for more robust retrieval, and introduces text-trajectory contrastive learning to pull matched pairs together and push mismatched pairs apart in a shared embedding space, yielding a fine-grained matcher that refines the LLM's candidate trajectories. Experiments on public datasets demonstrate substantial gains in both retrieval quality and efficiency.

</details>


### [346] [SAR-Based Marine Oil Spill Detection Using the DeepSegFusion Architecture](https://arxiv.org/abs/2601.12015)
*Pavan Kumar Yata,Pediredla Pradeep,Goli Himanish,Swathi M*

Main category: cs.CV

TL;DR: The paper presents DeepSegFusion, an attention-based hybrid deep learning model that significantly improves oil spill segmentation in SAR images, achieving high accuracy and low false detection rates.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the problem of high false alarm rates in oil spill detection from satellite images due to look-alike phenomena with traditional methods.

Method: A hybrid model combining SegNet and DeepLabV3+ with an attention-based feature fusion mechanism for enhanced oil spill segmentation in SAR images.

Result: DeepSegFusion achieves 94.85% accuracy, an IoU of 0.5685, and an ROC-AUC of 0.9330, with three times fewer false detections compared to conventional approaches.

Conclusion: The DeepSegFusion model is robust under varied conditions, demonstrating its potential for near real-time oil spill monitoring applications.

Abstract: Detection of oil spills from satellite images is essential for both environmental surveillance and maritime safety. Traditional threshold-based methods frequently encounter performance degradation due to very high false alarm rates caused by look-alike phenomena such as wind slicks and ship wakes. Here, a hybrid deep learning model, DeepSegFusion, is presented for oil spill segmentation in Synthetic Aperture Radar (SAR) images. The model uses SegNet and DeepLabV3+ integrated with an attention-based feature fusion mechanism to achieve better boundary precision as well as improved contextual understanding. Results obtained on SAR oil spill datasets, including ALOS PALSAR imagery, confirm that the proposed DeepSegFusion model achieves an accuracy of 94.85%, an Intersection over Union (IoU) of 0.5685, and a ROC-AUC score of 0.9330. The proposed method delivers more than three times fewer false detections compared to individual baseline models and traditional non-segmentation methods, achieving a reduction of 64.4%. These results indicate that DeepSegFusion is a stable model under various marine conditions and can therefore be used in near real-time oil spill monitoring scenarios.

</details>


### [347] [DIAMOND-SSS: Diffusion-Augmented Multi-View Optimization for Data-efficient SubSurface Scattering](https://arxiv.org/abs/2601.12020)
*Guillermo Figueroa-Araneda,Iris Diana Jimenez,Florian Hofherr,Manny Ko,Hector Andrade-Loarca,Daniel Cremers*

Main category: cs.CV

TL;DR: DIAMOND-SSS is a framework for reconstructing translucent materials with minimal image supervision, achieving photorealistic results using diffusion models with sparse datasets.


<details>
  <summary>Details</summary>
Motivation: Subsurface scattering effects are essential for the realistic appearance of materials like skin and marble, but modeling them is challenging due to the need for dense datasets and complex light transport.

Method: The framework fine-tunes diffusion models for novel-view synthesis and relighting, introduces illumination-independent geometric priors, and enables reconstruction under sparse or synthetic data conditions.

Result: DIAMOND-SSS achieves state-of-the-art quality in relightable Gaussian rendering, reducing real data capture needs by up to 90%, even using extremely sparse datasets.

Conclusion: The method enables high-fidelity translucent reconstructions from sparse supervision, significantly reducing data requirements and improving efficiency in neural rendering.

Abstract: Subsurface scattering (SSS) gives translucent materials -- such as wax, jade, marble, and skin -- their characteristic soft shadows, color bleeding, and diffuse glow. Modeling these effects in neural rendering remains challenging due to complex light transport and the need for densely captured multi-view, multi-light datasets (often more than 100 views and 112 OLATs).
  We present DIAMOND-SSS, a data-efficient framework for high-fidelity translucent reconstruction from extremely sparse supervision -- even as few as ten images. We fine-tune diffusion models for novel-view synthesis and relighting, conditioned on estimated geometry and trained on less than 7 percent of the dataset, producing photorealistic augmentations that can replace up to 95 percent of missing captures. To stabilize reconstruction under sparse or synthetic supervision, we introduce illumination-independent geometric priors: a multi-view silhouette consistency loss and a multi-view depth consistency loss.
  Across all sparsity regimes, DIAMOND-SSS achieves state-of-the-art quality in relightable Gaussian rendering, reducing real capture requirements by up to 90 percent compared to SSS-3DGS.

</details>


### [348] [\textit{FocaLogic}: Logic-Based Interpretation of Visual Model Decisions](https://arxiv.org/abs/2601.12049)
*Chenchen Zhao,Muxi Chen,Qiang Xu*

Main category: cs.CV

TL;DR: The paper introduces FocaLogic, a model-agnostic framework for interpreting visual model decision-making using logic-based representations and evaluating them using new quantitative metrics.


<details>
  <summary>Details</summary>
Motivation: Modern visual models in critical domains require interpretability, but current methods lack accessibility or quantitative rigor.

Method: FocaLogic identifies minimal interpretable visual regions (visual focuses) and represents these as logical expressions, alongside introducing metrics for objective evaluation.

Result: FocaLogic reveals insights like training effects, improved focus accuracy through generalization, and detects biases or adversarial influences.

Conclusion: FocaLogic offers a scalable and systematic approach to improve transparency and evaluation of visual model interpretations.

Abstract: Interpretability of modern visual models is crucial, particularly in high-stakes applications. However, existing interpretability methods typically suffer from either reliance on white-box model access or insufficient quantitative rigor. To address these limitations, we introduce FocaLogic, a novel model-agnostic framework designed to interpret and quantify visual model decision-making through logic-based representations. FocaLogic identifies minimal interpretable subsets of visual regions-termed visual focuses-that decisively influence model predictions. It translates these visual focuses into precise and compact logical expressions, enabling transparent and structured interpretations. Additionally, we propose a suite of quantitative metrics, including focus precision, recall, and divergence, to objectively evaluate model behavior across diverse scenarios. Empirical analyses demonstrate FocaLogic's capability to uncover critical insights such as training-induced concentration, increasing focus accuracy through generalization, and anomalous focuses under biases and adversarial attacks. Overall, FocaLogic provides a systematic, scalable, and quantitative solution for interpreting visual models.

</details>


### [349] [A Unified Masked Jigsaw Puzzle Framework for Vision and Language Models](https://arxiv.org/abs/2601.12051)
*Weixin Ye,Wei Wang,Yahui Liu,Yue Song,Bin Ren,Wei Bi,Rita Cucchiara,Nicu Sebe*

Main category: cs.CV

TL;DR: The paper presents the "Masked Jigsaw Puzzle" (MJP) framework to enhance transformer models' security against gradient attacks and improve performance for CV and NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Transformers face challenges in federated learning due to vulnerabilities to gradient attacks and suboptimal performance across CV and NLP tasks. Gradient information from position embeddings can be exploited to reconstruct input data.

Method: The MJP framework disrupts position embeddings by shuffling token orders and masking them with learnable unknown position embeddings, forcing models to rely less on local spatial information.

Result: MJP improves robustness against gradient attacks and enhances performance in vision (e.g., ImageNet-1K classification) and text (e.g., sentiment analysis for Yelp and Amazon) tasks.

Conclusion: The proposed MJP framework serves as a unified approach for strengthening and improving transformer models in both CV and NLP applications. Code is publicly available for implementation.

Abstract: In federated learning, Transformer, as a popular architecture, faces critical challenges in defending against gradient attacks and improving model performance in both Computer Vision (CV) and Natural Language Processing (NLP) tasks. It has been revealed that the gradient of Position Embeddings (PEs) in Transformer contains sufficient information, which can be used to reconstruct the input data. To mitigate this issue, we introduce a Masked Jigsaw Puzzle (MJP) framework. MJP starts with random token shuffling to break the token order, and then a learnable \textit{unknown (unk)} position embedding is used to mask out the PEs of the shuffled tokens. In this manner, the local spatial information which is encoded in the position embeddings is disrupted, and the models are forced to learn feature representations that are less reliant on the local spatial information. Notably, with the careful use of MJP, we can not only improve models' robustness against gradient attacks, but also boost their performance in both vision and text application scenarios, such as classification for images (\textit{e.g.,} ImageNet-1K) and sentiment analysis for text (\textit{e.g.,} Yelp and Amazon). Experimental results suggest that MJP is a unified framework for different Transformer-based models in both vision and language tasks. Code is publicly available via https://github.com/ywxsuperstar/transformerattack

</details>


### [350] [Task-Driven Prompt Learning: A Joint Framework for Multi-modal Cloud Removal and Segmentation](https://arxiv.org/abs/2601.12052)
*Zaiyan Zhang,Jie Li,Shaowei Shi,Qiangqiang Yuan*

Main category: cs.CV

TL;DR: The paper addresses cloud-related limitations in optical remote sensing imagery by proposing a task-driven multimodal framework, TDP-CR, that combines cloud removal with land-cover segmentation.


<details>
  <summary>Details</summary>
Motivation: The challenge of persistent cloud occlusion in optical remote sensing limits the utility of the data for downstream analysis. Current cloud removal methods often sacrifice semantic utility for visual fidelity, creating a need for approaches that balance both.

Method: The paper introduces TDP-CR, which utilizes a Prompt-Guided Fusion mechanism to integrate SAR and optical data adaptively based on cloud-related degradation prompts and employs a two-phase training strategy for reconstruction and semantic representation.

Result: Experimental evaluations on the LuojiaSET-OSFCR dataset showed TDP-CR's superior performance, achieving better accuracy (0.18 dB PSNR improvement) and semantic segmentation (1.4% mIoU improvement) while using only 15% of the parameters compared to baselines.

Conclusion: TDP-CR demonstrates the capability to enhance both the visual and semantic quality of analysis-ready data, proving its efficiency and effectiveness for cloud-affected remote sensing imagery.

Abstract: Optical remote sensing imagery is indispensable for Earth observation, yet persistent cloud occlusion limits its downstream utility. Most cloud removal (CR) methods are optimized for low-level fidelity and can over-smooth textures and boundaries that are critical for analysis-ready data (ARD), leading to a mismatch between visually plausible restoration and semantic utility. To bridge this gap, we propose TDP-CR, a task-driven multimodal framework that jointly performs cloud removal and land-cover segmentation. Central to our approach is a Prompt-Guided Fusion (PGF) mechanism, which utilizes a learnable degradation prompt to encode cloud thickness and spatial uncertainty. By combining global channel context with local prompt-conditioned spatial bias, PGF adaptively integrates Synthetic Aperture Radar (SAR) information only where optical data is corrupted. We further introduce a parameter-efficient two-phase training strategy that decouples reconstruction and semantic representation learning. Experiments on the LuojiaSET-OSFCR dataset demonstrate the superiority of our framework: TDP-CR surpasses heavy state-of-the-art baselines by 0.18 dB in PSNR while using only 15\% of the parameters, and achieves a 1.4\% improvement in mIoU consistently against multi-task competitors, effectively delivering analysis-ready data.

</details>


### [351] [Automating Parameter Selection in Deep Image Prior for Fluorescence Microscopy Image Denoising via Similarity-Based Parameter Transfer](https://arxiv.org/abs/2601.12055)
*Lina Meyer,Felix Wissel,Tobias Knopp,Susanne Pfefferle,Ralf Fliegert,Maximilian Sandmann,Liana Uebler,Franziska Möckl,Björn-Philipp Diercks,David Lohr,René Werner*

Main category: cs.CV

TL;DR: This paper introduces AUTO-DIP, an optimization-free pipeline for unsupervised deep denoising in fluorescence microscopy, showcasing superior performance in noisy datasets by leveraging pre-optimized parameters based on image metadata similarity.


<details>
  <summary>Details</summary>
Motivation: Overcoming the limitations of iterative and time-consuming parameter optimization in unsupervised deep image priors (DIP) for processing numerous images, especially for fluorescence microscopy data.

Method: Generated calibration (n=110) and validation (n=55) sets from an open-source dataset to explore parameter configurations for DIP. Developed AUTO-DIP, a pipeline transferring pre-optimized DIP parameters based on image metadata similarity.

Result: AUTO-DIP showed comparable or better denoising performance compared to DIP baseline and variational methods across test datasets, excelling in noisy fluorescence microscopy data.

Conclusion: AUTO-DIP streamlines DIP application by eliminating individual optimization, achieving improved denoising results, and demonstrating practicality for fluorescence microscopy images.

Abstract: Unsupervised deep image prior (DIP) addresses shortcomings of training data requirements and limited generalization associated with supervised deep learning. The performance of DIP depends on the network architecture and the stopping point of its iterative process. Optimizing these parameters for a new image requires time, restricting DIP application in domains where many images need to be processed. Focusing on fluorescence microscopy data, we hypothesize that similar images share comparable optimal parameter configurations for DIP-based denoising, potentially enabling optimization-free DIP for fluorescence microscopy. We generated a calibration (n=110) and validation set (n=55) of semantically different images from an open-source dataset for a network architecture search targeted towards ideal U-net architectures and stopping points. The calibration set represented our transfer basis. The validation set enabled the assessment of which image similarity criterion yields the best results. We then implemented AUTO-DIP, a pipeline for automatic parameter transfer, and compared it to the originally published DIP configuration (baseline) and a state-of-the-art image-specific variational denoising approach. We show that a parameter transfer from the calibration dataset to a test image based on only image metadata similarity (e.g., microscope type, imaged specimen) leads to similar and better performance than a transfer based on quantitative image similarity measures. AUTO-DIP outperforms the baseline DIP (DIP with original DIP parameters) as well as the variational denoising approaches for several open-source test datasets of varying complexity, particularly for very noisy inputs. Applications to locally acquired fluorescence microscopy images further proved superiority of AUTO-DIP.

</details>


### [352] [Learning Language-Driven Sequence-Level Modal-Invariant Representations for Video-Based Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2601.12062)
*Xiaomei Yang,Xizhan Gao,Antai Liu,Kang Wei,Fa Zhu,Guang Feng,Xiaofeng Qu,Sijie Niu*

Main category: cs.CV

TL;DR: The study introduces the LSMRL approach to enhance modal-invariant representation learning in VVI-ReID, focusing on spatial-temporal modeling, cross-modal interaction, and loss guidance.


<details>
  <summary>Details</summary>
Motivation: Address challenges in video-based visible-infrared person re-identification, including efficient spatial-temporal modeling, sufficient cross-modal interaction, and explicit loss guidance.

Method: Proposed the LSMRL framework with three main modules: STFL for efficient spatial-temporal modeling, SD for diffusing modality-shared prompts, and CMI for eliminating residual modality gaps. Two modality-level losses are also introduced.

Result: Extensive experiments on large-scale datasets demonstrated that LSMRL outperforms current state-of-the-art methods in VVI-ReID tasks.

Conclusion: LSMRL effectively addresses key challenges in VVI-ReID, offering better modality-invariant feature learning and achieving superior performance.

Abstract: The core of video-based visible-infrared person re-identification (VVI-ReID) lies in learning sequence-level modal-invariant representations across different modalities. Recent research tends to use modality-shared language prompts generated by CLIP to guide the learning of modal-invariant representations. Despite achieving optimal performance, such methods still face limitations in efficient spatial-temporal modeling, sufficient cross-modal interaction, and explicit modality-level loss guidance. To address these issues, we propose the language-driven sequence-level modal-invariant representation learning (LSMRL) method, which includes spatial-temporal feature learning (STFL) module, semantic diffusion (SD) module and cross-modal interaction (CMI) module. To enable parameter- and computation-efficient spatial-temporal modeling, the STFL module is built upon CLIP with minimal modifications. To achieve sufficient cross-modal interaction and enhance the learning of modal-invariant features, the SD module is proposed to diffuse modality-shared language prompts into visible and infrared features to establish preliminary modal consistency. The CMI module is further developed to leverage bidirectional cross-modal self-attention to eliminate residual modality gaps and refine modal-invariant representations. To explicitly enhance the learning of modal-invariant representations, two modality-level losses are introduced to improve the features' discriminative ability and their generalization to unseen categories. Extensive experiments on large-scale VVI-ReID datasets demonstrate the superiority of LSMRL over AOTA methods.

</details>


### [353] [Learning Stochastic Bridges for Video Object Removal via Video-to-Video Translation](https://arxiv.org/abs/2601.12066)
*Zijie Lou,Xiangwei Feng,Jiaxin Wang,Xiaochao Qu,Luoqi Liu,Ting Liu*

Main category: cs.CV

TL;DR: A novel video object removal framework reformulates the task via a stochastic bridge model, utilizing original video inputs as structural priors for better removal accuracy and logical content synthesis.


<details>
  <summary>Details</summary>
Motivation: Current video object removal methods, reliant on diffusion models with noise-to-data paradigms, often fail due to inefficient guidance, incomplete object removal, or unrealistic content generation.

Method: The paper proposes a stochastic bridge model for video-to-video translation, leveraging input videos as structural priors. An adaptive mask modulation strategy is introduced to balance fidelity and flexibility during object removal.

Result: Experiments show that the proposed method surpasses existing techniques in visual quality and temporal consistency.

Conclusion: The approach addresses the limitations of prior methods, demonstrating precise object removal and logically consistent content generation by utilizing stronger priors and adaptive strategies.

Abstract: Existing video object removal methods predominantly rely on diffusion models following a noise-to-data paradigm, where generation starts from uninformative Gaussian noise. This approach discards the rich structural and contextual priors present in the original input video. Consequently, such methods often lack sufficient guidance, leading to incomplete object erasure or the synthesis of implausible content that conflicts with the scene's physical logic. In this paper, we reformulate video object removal as a video-to-video translation task via a stochastic bridge model. Unlike noise-initialized methods, our framework establishes a direct stochastic path from the source video (with objects) to the target video (objects removed). This bridge formulation effectively leverages the input video as a strong structural prior, guiding the model to perform precise removal while ensuring that the filled regions are logically consistent with the surrounding environment. To address the trade-off where strong bridge priors hinder the removal of large objects, we propose a novel adaptive mask modulation strategy. This mechanism dynamically modulates input embeddings based on mask characteristics, balancing background fidelity with generative flexibility. Extensive experiments demonstrate that our approach significantly outperforms existing methods in both visual quality and temporal consistency.

</details>


### [354] [ARMARecon: An ARMA Convolutional Filter based Graph Neural Network for Neurodegenerative Dementias Classification](https://arxiv.org/abs/2601.12067)
*VSS Tejaswi Abburi,Ananya Singhal,Saurabh J. Shigwan,Nitin Kumar*

Main category: cs.CV

TL;DR: The paper proposes ARMARecon, a graph-learning framework for early detection of Alzheimer’s and Frontotemporal Dementia using white-matter data, improving classification accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a method for early detection of diseases like Alzheimer’s and FTD, leveraging their propagation patterns in white-matter regions and addressing the need for improved detection methods.

Method: The method employs ARMA graph filtering combined with a reconstruction-driven objective, incorporating 20-bin FA histogram features to model connectivity while mitigating over-smoothing.

Result: ARMARecon achieves superior performance compared to state-of-the-art methods on multi-site dMRI datasets ADNI and NIFD.

Conclusion: The authors demonstrate that ARMARecon effectively enhances feature representation and classification, making it a promising tool for early disease detection.

Abstract: Early detection of neurodegenerative diseases such as Alzheimer's Disease (AD) and Frontotemporal Dementia (FTD) is essential for reducing the risk of progression to severe disease stages. As AD and FTD propagate along white-matter regions in a global, graph-dependent manner, graph-based neural networks are well suited to capture these patterns. Hence, we introduce ARMARecon, a unified graph learning framework that integrates Autoregressive Moving Average (ARMA) graph filtering with a reconstruction-driven objective to enhance feature representation and improve classification accuracy. ARMARecon effectively models both local and global connectivity by leveraging 20-bin Fractional Anisotropy (FA) histogram features extracted from white-matter regions, while mitigating over-smoothing. Overall, ARMARecon achieves superior performance compared to state-of-the-art methods on the multi-site dMRI datasets ADNI and NIFD.

</details>


### [355] [CroBIM-V: Memory-Quality Controlled Remote Sensing Referring Video Object Segmentation](https://arxiv.org/abs/2601.12076)
*H. Jiang,Y. Sun,Z. Dong,T. Liu,Y. Gu*

Main category: cs.CV

TL;DR: The paper addresses challenges in RS-RVOS by introducing a large-scale benchmark (RS-RVOS Bench) and a memory-quality-aware segmentation framework (MQC-SAM) for overcoming weak saliency and occlusion issues, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: RS-RVOS faces challenges like weak target saliency, visual information truncation, biased memory construction, and error propagation due to noisy memory. A lack of large-scale datasets also hinders progress.

Method: The authors create a new benchmark (RS-RVOS Bench) with causality-aware annotations and propose MQC-SAM, a segmentation framework that uses motion consistency for memory calibration and dynamic memory integration for filtering noise.

Result: Extensive experiments show that MQC-SAM achieves state-of-the-art performance on the newly constructed RS-RVOS Bench.

Conclusion: By addressing data limitations and proposing innovative methodology, the paper advances the field of RS-RVOS, overcoming prior challenges in accurate target representation and error propagation.

Abstract: Remote sensing video referring object segmentation (RS-RVOS) is challenged by weak target saliency and severe visual information truncation in dynamic scenes, making it extremely difficult to maintain discriminative target representations during segmentation. Moreover, progress in this field is hindered by the absence of large-scale dedicated benchmarks, while existing models are often affected by biased initial memory construction that impairs accurate instance localization in complex scenarios, as well as indiscriminate memory accumulation that encodes noise from occlusions or misclassifications, leading to persistent error propagation. This paper advances RS-RVOS research through dual contributions in data and methodology. First, we construct RS-RVOS Bench, the first large-scale benchmark comprising 111 video sequences, about 25,000 frames, and 213,000 temporal referring annotations. Unlike common RVOS benchmarks where many expressions are written with access to the full video context, our dataset adopts a strict causality-aware annotation strategy in which linguistic references are generated solely from the target state in the initial frame. Second, we propose a memory-quality-aware online referring segmentation framework, termed Memory Quality Control with Segment Anything Model (MQC-SAM). MQC-SAM introduces a temporal motion consistency module for initial memory calibration, leveraging short-term motion trajectory priors to correct structural deviations and establish accurate memory anchoring. Furthermore, it incorporates a decoupled attention-based memory integration mechanism with dynamic quality assessment, selectively updating high-confidence semantic features while filtering unreliable information, thereby effectively preventing error accumulation and propagation. Extensive experiments on RS-RVOS Bench demonstrate that MQC-SAM achieves state-of-the-art performance.

</details>


### [356] [EmoLat: Text-driven Image Sentiment Transfer via Emotion Latent Space](https://arxiv.org/abs/2601.12079)
*Jing Zhang,Bingjie Fan,Jixiang Zhu,Zhe Wang*

Main category: cs.CV

TL;DR: The paper introduces EmoLat, a latent emotional space for text-driven image sentiment transfer using cross-modal correlations, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To enable fine-grained and controllable image sentiment editing guided by text while capturing the relationship between emotions, objects, and visual attributes.

Method: The method involves creating EmoLat, an emotion latent space, constructing an emotion semantic graph for relational modeling, employing adversarial regularization for multimodal alignment, and optimizing with multi-objective loss for semantic consistency and emotion alignment.

Result: EmoLat achieved state-of-the-art performance in image sentiment transfer on a constructed large-scale benchmark dataset, EmoSpace Set, demonstrating improved quantitative metrics and qualitative fidelity.

Conclusion: The study establishes EmoLat as a pioneering framework for cross-modal sentiment transfer, advancing controllable text-driven image sentiment editing.

Abstract: We propose EmoLat, a novel emotion latent space that enables fine-grained, text-driven image sentiment transfer by modeling cross-modal correlations between textual semantics and visual emotion features. Within EmoLat, an emotion semantic graph is constructed to capture the relational structure among emotions, objects, and visual attributes. To enhance the discriminability and transferability of emotion representations, we employ adversarial regularization, aligning the latent emotion distributions across modalities. Building upon EmoLat, a cross-modal sentiment transfer framework is proposed to manipulate image sentiment via joint embedding of text and EmoLat features. The network is optimized using a multi-objective loss incorporating semantic consistency, emotion alignment, and adversarial regularization. To support effective modeling, we construct EmoSpace Set, a large-scale benchmark dataset comprising images with dense annotations on emotions, object semantics, and visual attributes. Extensive experiments on EmoSpace Set demonstrate that our approach significantly outperforms existing state-of-the-art methods in both quantitative metrics and qualitative transfer fidelity, establishing a new paradigm for controllable image sentiment editing guided by textual input. The EmoSpace Set and all the code are available at http://github.com/JingVIPLab/EmoLat.

</details>


### [357] [Toward Real-World High-Precision Image Matting and Segmentation](https://arxiv.org/abs/2601.12080)
*Haipeng Zhou,Zhaohu Xing,Hongqiu Wang,Jun Ma,Ping Li,Lei Zhu*

Main category: cs.CV

TL;DR: The paper presents FCLM, a model for precise scene parsing, addressing issues like class-agnostic limitations and poor generalization to real scenarios, using a depth-aware approach and domain-invariant learning.


<details>
  <summary>Details</summary>
Motivation: To improve high-precision scene parsing tasks by overcoming reliance on synthetic data and addressing class-agnostic limitations in interactive methods.

Method: The proposed FCLM introduces a Depth-Aware Distillation for better foreground representation, a domain-invariant learning strategy for synthetic data, and an Object-Oriented Decoder for interactive, visual and language-based predictions.

Result: FCLM outperforms state-of-the-art methods quantitatively and qualitatively in high-precision scene parsing tasks with fine details.

Conclusion: FCLM effectively addresses domain adaptation and interactive prediction challenges, creating robust performance in handling real-world fine-detail scene parsing tasks.

Abstract: High-precision scene parsing tasks, including image matting and dichotomous segmentation, aim to accurately predict masks with extremely fine details (such as hair). Most existing methods focus on salient, single foreground objects. While interactive methods allow for target adjustment, their class-agnostic design restricts generalization across different categories. Furthermore, the scarcity of high-quality annotation has led to a reliance on inharmonious synthetic data, resulting in poor generalization to real-world scenarios. To this end, we propose a Foreground Consistent Learning model, dubbed as FCLM, to address the aforementioned issues. Specifically, we first introduce a Depth-Aware Distillation strategy where we transfer the depth-related knowledge for better foreground representation. Considering the data dilemma, we term the processing of synthetic data as domain adaptation problem where we propose a domain-invariant learning strategy to focus on foreground learning. To support interactive prediction, we contribute an Object-Oriented Decoder that can receive both visual and language prompts to predict the referring target. Experimental results show that our method quantitatively and qualitatively outperforms SOTA methods.

</details>


### [358] [Conditional Random Fields for Interactive Refinement of Histopathological Predictions](https://arxiv.org/abs/2601.12082)
*Tiffanie Godelaine,Maxime Zanella,Karim El Khoury,Saïd Mahmoudi,Benoît Macq,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: The paper introduces HistoCRF, a CRF-based framework for refining vision-language model predictions in histopathological image analysis, demonstrating significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Histology foundation models, such as vision-language models, provide strong predictions but are imperfect. Enhancing these tools without additional training or extensive expert input offers substantial clinical value.

Method: The authors created HistoCRF by adapting Conditional Random Fields with a novel design of pairwise potential. They tested three scenarios: no annotations, limited expert annotations, and iterative human-in-the-loop annotations.

Result: HistoCRF achieved an accuracy gain of 16.0% without annotations, 27.5% with 100 expert annotations, and 32.6% gain when incorporating iterative human corrections.

Conclusion: HistoCRF significantly refines zero-shot predictions from vision-language models, supporting pathologists in cancer detection with minimal additional resources. The method is openly accessible for further application and development.

Abstract: Assisting pathologists in the analysis of histopathological images has high clinical value, as it supports cancer detection and staging. In this context, histology foundation models have recently emerged. Among them, Vision-Language Models (VLMs) provide strong yet imperfect zero-shot predictions. We propose to refine these predictions by adapting Conditional Random Fields (CRFs) to histopathological applications, requiring no additional model training. We present HistoCRF, a CRF-based framework, with a novel definition of the pairwise potential that promotes label diversity and leverages expert annotations. We consider three experiments: without annotations, with expert annotations, and with iterative human-in-the-loop annotations that progressively correct misclassified patches. Experiments on five patch-level classification datasets covering different organs and diseases demonstrate average accuracy gains of 16.0% without annotations and 27.5% with only 100 annotations, compared to zero-shot predictions. Moreover, integrating a human in the loop reaches a further gain of 32.6% with the same number of annotations. The code will be made available on https://github.com/tgodelaine/HistoCRF.

</details>


### [359] [Detecting 3D Line Segments for 6DoF Pose Estimation with Limited Data](https://arxiv.org/abs/2601.12090)
*Matej Mok,Lukáš Gajdošech,Michal Mesároš,Martin Madaras,Viktor Kocur*

Main category: cs.CV

TL;DR: The paper introduces a method for 6DoF object pose estimation, focusing on cuboid bins in industrial settings without relying on CAD models, achieving better pose estimation accuracy.


<details>
  <summary>Details</summary>
Motivation: The reliance on extensive training data or CAD models limits current pose estimation methods in industrial scenarios with diverse objects and rare data.

Method: The method detects 3D line segments on cuboid bins using an adapted 2D line network (LeTR) and a geometric procedure to estimate the 6DoF pose.

Result: The method outperforms state-of-the-art techniques with improved pose accuracy using synthetic training data and does not require CAD models, achieving 3 cm translation error and 8.2° rotation error.

Conclusion: The approach is practical for industrial automation, efficiently estimating the poses of cuboid bins without relying on data-intensive CAD models, with high accuracy.

Abstract: The task of 6DoF object pose estimation is one of the fundamental problems of 3D vision with many practical applications such as industrial automation. Traditional deep learning approaches for this task often require extensive training data or CAD models, limiting their application in real-world industrial settings where data is scarce and object instances vary. We propose a novel method for 6DoF pose estimation focused specifically on bins used in industrial settings. We exploit the cuboid geometry of bins by first detecting intermediate 3D line segments corresponding to their top edges. Our approach extends the 2D line segment detection network LeTR to operate on structured point cloud data. The detected 3D line segments are then processed using a simple geometric procedure to robustly determine the bin's 6DoF pose. To evaluate our method, we extend an existing dataset with a newly collected and annotated dataset, which we make publicly available. We show that incorporating synthetic training data significantly improves pose estimation accuracy on real scans. Moreover, we show that our method significantly outperforms current state-of-the-art 6DoF pose estimation methods in terms of the pose accuracy (3 cm translation error, 8.2$^\circ$ rotation error) while not requiring instance-specific CAD models during inference.

</details>


### [360] [Energy-Aware Ensemble Learning for Coffee Leaf Disease Classification](https://arxiv.org/abs/2601.12109)
*Larissa Ferreira Rodrigues Moreira,Rodrigo Moreira,Leonardo Gabriel Ferreira Rodrigues*

Main category: cs.CV

TL;DR: This paper explores lightweight AI models for diagnosing coffee leaf diseases using knowledge distillation and ensemble learning to optimize performance for resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenges of diagnosing coffee leaf diseases in the field, where constrained devices and intermittent connectivity limit the use of high-accuracy AI models.

Method: The authors utilize knowledge distillation from high-capacity Convolutional Neural Networks (CNNs) to compact CNNs, combined with Ensemble Learning (EL) and optimized tiny ensemble integration.

Result: The distilled tiny ensembles achieved results competitive with prior methods while significantly reducing energy consumption and carbon footprint on a curated coffee leaf dataset.

Conclusion: Lightweight models, when distilled and ensembled effectively, can provide practical and environmentally sustainable solutions for diagnosing coffee leaf diseases on IoT devices.

Abstract: Coffee yields are contingent on the timely and accurate diagnosis of diseases; however, assessing leaf diseases in the field presents significant challenges. Although Artificial Intelligence (AI) vision models achieve high accuracy, their adoption is hindered by the limitations of constrained devices and intermittent connectivity. This study aims to facilitate sustainable on-device diagnosis through knowledge distillation: high-capacity Convolutional Neural Networks (CNNs) trained in data centers transfer knowledge to compact CNNs through Ensemble Learning (EL). Furthermore, dense tiny pairs were integrated through simple and optimized ensembling to enhance accuracy while adhering to strict computational and energy constraints. On a curated coffee leaf dataset, distilled tiny ensembles achieved competitive with prior work with significantly reduced energy consumption and carbon footprint. This indicates that lightweight models, when properly distilled and ensembled, can provide practical diagnostic solutions for Internet of Things (IoT) applications.

</details>


### [361] [RCDN: Real-Centered Detection Network for Robust Face Forgery Identification](https://arxiv.org/abs/2601.12111)
*Wyatt McCurdy,Xin Zhang,Yuqi Song,Min Gao*

Main category: cs.CV

TL;DR: The paper addresses the limitations of current image forgery detection methods in cross-domain scenarios and proposes the Real-Centered Detection Network (RCDN) for improved in-domain and cross-domain performance.


<details>
  <summary>Details</summary>
Motivation: Image forgery detection methods struggle in cross-domain scenarios due to continuous emergence of new forgery techniques, requiring a robust solution for unseen manipulations.

Method: The authors introduce RCDN, a frequency spatial CNN framework with an Xception backbone. It uses a dual-branch architecture and real-centered loss design to focus on authentic images instead of diverse forgery patterns.

Result: Experiments demonstrate that RCDN achieves state-of-the-art in-domain accuracy and superior cross-domain generalization performance on the DiFF dataset.

Conclusion: RCDN effectively reduces the generalization gap compared to other baselines and offers high stability, making it a promising defense against unseen forgery techniques.

Abstract: Image forgery has become a critical threat with the rapid proliferation of AI-based generation tools, which make it increasingly easy to synthesize realistic but fraudulent facial content. Existing detection methods achieve near-perfect performance when training and testing are conducted within the same domain, yet their effectiveness deteriorates substantially in crossdomain scenarios. This limitation is problematic, as new forgery techniques continuously emerge and detectors must remain reliable against unseen manipulations. To address this challenge, we propose the Real-Centered Detection Network (RCDN), a frequency spatial convolutional neural networks(CNN) framework with an Xception backbone that anchors its representation space around authentic facial images. Instead of modeling the diverse and evolving patterns of forgeries, RCDN emphasizes the consistency of real images, leveraging a dual-branch architecture and a real centered loss design to enhance robustness under distribution shifts. Extensive experiments on the DiFF dataset, focusing on three representative forgery types (FE, I2I, T2I), demonstrate that RCDN achieves both state-of-the-art in-domain accuracy and significantly stronger cross-domain generalization. Notably, RCDN reduces the generalization gap compared to leading baselines and achieves the highest cross/in-domain stability ratio, highlighting its potential as a practical solution for defending against evolving and unseen image forgery techniques.

</details>


### [362] [CARLA-Round: A Multi-Factor Simulation Dataset for Roundabout Trajectory Prediction](https://arxiv.org/abs/2601.12119)
*Xiaotong Zhou,Zhenhui Yuan,Yi Han,Tianhua Xu,Laurence T. Yang*

Main category: cs.CV

TL;DR: The paper introduces CARLA-Round, a simulation dataset for vehicle trajectory prediction at roundabouts, addressing challenges in existing datasets.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of trajectory prediction at roundabouts, addressing the scarcity of realistic datasets and the complexity of real-world data collection.

Method: Developing a systematic simulation dataset in varied weather and traffic conditions, with explicit annotations and leveraging standard models for validation.

Result: The dataset allowed precise analysis of condition effects, achieving strong sim-to-real transfer with a best result of 0.312m ADE using the rounD dataset.

Conclusion: CARLA-Round provides structured data to advance roundabout trajectory prediction, overcoming limitations of real-world datasets and enabling deeper insights.

Abstract: Accurate trajectory prediction of vehicles at roundabouts is critical for reducing traffic accidents, yet it remains highly challenging due to their circular road geometry, continuous merging and yielding interactions, and absence of traffic signals. Developing accurate prediction algorithms relies on reliable, multimodal, and realistic datasets; however, such datasets for roundabout scenarios are scarce, as real-world data collection is often limited by incomplete observations and entangled factors that are difficult to isolate. We present CARLA-Round, a systematically designed simulation dataset for roundabout trajectory prediction. The dataset varies weather conditions (five types) and traffic density levels (spanning Level-of-Service A-E) in a structured manner, resulting in 25 controlled scenarios. Each scenario incorporates realistic mixtures of driving behaviors and provides explicit annotations that are largely absent from existing datasets. Unlike randomly sampled simulation data, this structured design enables precise analysis of how different conditions influence trajectory prediction performance. Validation experiments using standard baselines (LSTM, GCN, GRU+GCN) reveal traffic density dominates prediction difficulty with strong monotonic effects, while weather shows non-linear impacts. The best model achieves 0.312m ADE on real-world rounD dataset, demonstrating effective sim-to-real transfer. This systematic approach quantifies factor impacts impossible to isolate in confounded real-world datasets. Our CARLA-Round dataset is available at https://github.com/Rebecca689/CARLA-Round.

</details>


### [363] [Segment and Matte Anything in a Unified Model](https://arxiv.org/abs/2601.12147)
*Zezhong Fan,Xiaohan Li,Topojoy Biswas,Kaushiki Nag,Kannan Achan*

Main category: cs.CV

TL;DR: Segment And Matte Anything (SAMA) is a lightweight extension of SAM designed for precise image segmentation and matting.


<details>
  <summary>Details</summary>
Motivation: SAM lacks precision required for real-world applications, and interactive image matting has not been addressed in the SAM context.

Method: SAMA uses a Multi-View Localization Encoder and Localization Adapter, and integrates dual prediction heads for segmentation and matting.

Result: SAMA achieves state-of-the-art performance across various segmentation and matting benchmarks.

Conclusion: SAMA effectively unifies segmentation and matting tasks within a single framework, delivering fine-grained, high-quality results.

Abstract: Segment Anything (SAM) has recently pushed the boundaries of segmentation by demonstrating zero-shot generalization and flexible prompting after training on over one billion masks. Despite this, its mask prediction accuracy often falls short of the precision required in real-world applications. While several refinement modules have been proposed to boost SAM's segmentation quality, achieving highly accurate object delineation within a single, unified framework remains an open challenge. Furthermore, interactive image matting, which aims to generate fine-grained alpha mattes guided by diverse user hints, has not yet been explored in the context of SAM. Insights from recent studies highlight strong correlations between segmentation and matting, suggesting the feasibility of a unified model capable of both tasks. In this paper, we introduce Segment And Matte Anything (SAMA), a lightweight extension of SAM that delivers high-quality interactive image segmentation and matting with minimal extra parameters. Our Multi-View Localization Encoder (MVLE) captures detailed features from local views, while the Localization Adapter (Local-Adapter) refines mask outputs by recovering subtle boundary details. We also incorporate two prediction heads for each task into the architecture to generate segmentation and matting masks, simultaneously. Trained on a diverse dataset aggregated from publicly available sources, SAMA achieves state-of-the-art performance across multiple segmentation and matting benchmarks, showcasing its adaptability and effectiveness in a wide range of downstream tasks.

</details>


### [364] [Principal Component Analysis-Based Terahertz Self-Supervised Denoising and Deblurring Deep Neural Networks](https://arxiv.org/abs/2601.12149)
*Pengfei Zhu,Xavier Maldague*

Main category: cs.CV

TL;DR: The study introduces a PCA-based THz self-supervised network (THz-SSDD) to address frequency-dependent degradation in THz imaging, providing automatic denoising and deblurring without manual intervention.


<details>
  <summary>Details</summary>
Motivation: Frequency-dependent degradation in terahertz (THz) imaging causes low-frequency blurring and high-frequency noise, which conventional techniques cannot resolve simultaneously and requires manual intervention.

Method: A PCA-based self-supervised denoising and deblurring network (THz-SSDD) using a Recorrupted-to-Recorrupted strategy and PCA decomposition/reconstruction to restore low and high-frequency components of the image.

Result: The network showed effective denoising and deblurring on various samples using only small sets of unlabeled noisy images. Quantitative analysis indicated improvements in image quality while retaining the physical characteristics of original signals.

Conclusion: The proposed THz-SSDD network offers a reliable and automated solution for dual denoising and deblurring in THz imaging, requiring minimal training data and retaining signal integrity.

Abstract: Terahertz (THz) systems inherently introduce frequency-dependent degradation effects, resulting in low-frequency blurring and high-frequency noise in amplitude images. Conventional image processing techniques cannot simultaneously address both issues, and manual intervention is often required due to the unknown boundary between denoising and deblurring. To tackle this challenge, we propose a principal component analysis (PCA)-based THz self-supervised denoising and deblurring network (THz-SSDD). The network employs a Recorrupted-to-Recorrupted self-supervised learning strategy to capture the intrinsic features of noise by exploiting invariance under repeated corruption. PCA decomposition and reconstruction are then applied to restore images across both low and high frequencies. The performance of the THz-SSDD network was evaluated on four types of samples. Training requires only a small set of unlabeled noisy images, and testing across samples with different material properties and measurement modes demonstrates effective denoising and deblurring. Quantitative analysis further validates the network feasibility, showing improvements in image quality while preserving the physical characteristics of the original signals.

</details>


### [365] [Enhanced Diagnostic Performance via Large-Resolution Inference Optimization for Pathology Foundation Models](https://arxiv.org/abs/2601.12150)
*Mengxuan Hu,Zihan Guan,John Kang,Sheng Li,Zhongliang Zhou*

Main category: cs.CV

TL;DR: The paper addresses the inefficiencies in pathology foundation models for whole-slide images (WSIs) and proposes a sparse, memory-efficient method to improve high-resolution inference.


<details>
  <summary>Details</summary>
Motivation: Many pathology foundation models are constrained by fixed input sizes, resulting in inefficiencies during whole-slide image analysis due to high memory demands or loss of critical details from downsampling.

Method: The proposed method sparsifies attention via spatially aware neighboring blocks and filters out non-informative tokens using global attention scores, optimizing memory and runtime usage for WSI inference.

Result: The method improves high-resolution inference efficiency, achieving up to a 7.67% boost in ROI classification performance while maintaining comparable segmentation results.

Conclusion: This strategy enables more efficient and accurate high-resolution WSI inference without exceeding GPU memory limitations, demonstrating significant utility for tasks involving large-scale pathology images.

Abstract: Despite their prominent performance on tasks such as ROI classification and segmentation, many pathology foundation models remain constrained by a specific input size e.g. 224 x 224, creating substantial inefficiencies when applied to whole-slide images (WSIs), which span thousands of resolutions. A naive strategy is to either enlarge inputs or downsample the WSIs. However, enlarging inputs results in prohibitive GPU memory consumption, while downsampling alters the microns-per-pixel resolution and obscures critical morphological details. To overcome these limitations, we propose an space- and time- efficient inference strategy that sparsifies attention using spatially aware neighboring blocks and filters out non-informative tokens through global attention scores. This design substantially reduces GPU memory and runtime during high-resolution WSI inference while preserving and even improving the downstream performance, enabling inference at higher resolutions under the same GPU budget. The experimental results show that our method can achieves up to an 7.67% improvement in the ROI classification and compatible results in segmentation.

</details>


### [366] [Inverse Rendering for High-Genus 3D Surface Meshes from Multi-view Images with Persistent Homology Priors](https://arxiv.org/abs/2601.12155)
*Xiang Gao,Xinmu Wang,Yuanpeng Liu,Yue Wang,Junqi Huang,Wei Chen,Xianfeng Gu*

Main category: cs.CV

TL;DR: This paper introduces a novel method integrating persistent homology priors into 3D reconstruction from images to handle high-genus surfaces and avoid topological failures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve ambiguities in 3D object reconstruction, particularly in recovering high-genus surfaces and avoiding errors like collapsing tunnels or losing structural complexity.

Method: The method involves collaborative inverse rendering using topological constraints (persistent homology priors) with gradient-based optimization in a mesh-based framework.

Result: The approach achieves better geometric accuracy with lower Chamfer Distance and higher Volume IoU compared to state-of-the-art methods.

Conclusion: Using persistent homology priors enhances the robustness and precision of 3D object reconstruction, tackling topological challenges effectively.

Abstract: Reconstructing 3D objects from images is inherently an ill-posed problem due to ambiguities in geometry, appearance, and topology. This paper introduces collaborative inverse rendering with persistent homology priors, a novel strategy that leverages topological constraints to resolve these ambiguities. By incorporating priors that capture critical features such as tunnel loops and handle loops, our approach directly addresses the difficulty of reconstructing high-genus surfaces. The collaboration between photometric consistency from multi-view images and homology-based guidance enables recovery of complex high-genus geometry while circumventing catastrophic failures such as collapsing tunnels or losing high-genus structure. Instead of neural networks, our method relies on gradient-based optimization within a mesh-based inverse rendering framework to highlight the role of topological priors. Experimental results show that incorporating persistent homology priors leads to lower Chamfer Distance (CD) and higher Volume IoU compared to state-of-the-art mesh-based methods, demonstrating improved geometric accuracy and robustness against topological failure.

</details>


### [367] [VIRTUE: Versatile Video Retrieval Through Unified Embeddings](https://arxiv.org/abs/2601.12193)
*Shaunak Halbe,Bhagyashree Puranik,Jayakrishnan Unnikrishnan,Kushan Thakkar,Vimal Bhat,Toufiq Parag*

Main category: cs.CV

TL;DR: This paper introduces VIRTUE, a video retrieval framework that integrates multimodal queries and supports corpus/moment retrieval within a single architecture. It outperforms existing multimodal LLMs while achieving results comparable to specialized models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a video retrieval system that integrates diverse capabilities like multimodal querying, corpus-level, and moment-level retrieval into a single framework, improving over specialized systems and multimodal LLM-based methods.

Method: The method involves training an embedding model using low-rank adaptation (LoRA) on 700K paired visual-text data. The model achieves contrastive alignment of visual and textual embeddings and combines embedding-based candidate search with additional reranking training.

Result: VIRTUE outperforms other MLLM-based methods on zero-shot video retrieval, achieves state-of-the-art results for zero-shot composed video retrieval, and performs competitively for zero-shot moment retrieval compared to specialized models.

Conclusion: VIRTUE demonstrates the potential of MLLM-based approaches to achieve high-performance multimodal video retrieval, bridging the gap between flexible multimodal query handling and specialized retrieval systems.

Abstract: Modern video retrieval systems are expected to handle diverse tasks ranging from corpus-level retrieval and fine-grained moment localization to flexible multimodal querying. Specialized architectures achieve strong retrieval performance by training modality-specific encoders on massive datasets, but they lack the ability to process composed multimodal queries. In contrast, multimodal LLM (MLLM)-based methods support rich multimodal search but their retrieval performance remains well below that of specialized systems. We present VIRTUE, an MLLM-based versatile video retrieval framework that integrates corpus and moment-level retrieval capabilities while accommodating composed multimodal queries within a single architecture. We use contrastive alignment of visual and textual embeddings generated using a shared MLLM backbone to facilitate efficient embedding-based candidate search. Our embedding model, trained efficiently using low-rank adaptation (LoRA) on 700K paired visual-text data samples, surpasses other MLLM-based methods on zero-shot video retrieval tasks. Additionally, we demonstrate that the same model can be adapted without further training to achieve competitive results on zero-shot moment retrieval, and state of the art results for zero-shot composed video retrieval. With additional training for reranking candidates identified in the embedding-based search, our model substantially outperforms existing MLLM-based retrieval systems and achieves retrieval performance comparable to state of the art specialized models which are trained on orders of magnitude larger data.

</details>


### [368] [From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles](https://arxiv.org/abs/2601.12358)
*Omar Y. Goba,Ahmed Y. Gado,Catherine M. Elias,Ahmed Hussein*

Main category: cs.CV

TL;DR: The paper introduces AV adaptive behavior planning using LLMs and LVMs to dynamically generate behavior trees (BTs) in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional behavior trees for autonomous vehicles lack adaptability and manual tuning, making them unsuitable for fully autonomous operations.

Method: An agentic framework uses LLMs and LVMs for dynamic BT generation with specialized agents performing scene analysis, sub-goal construction, and BT synthesis.

Result: In CARLA+Nav2 simulation, the system autonomously navigated unexpected obstacles by adapting BTs, showing improved flexibility over static BTs.

Conclusion: The approach is a promising proof-of-concept for enhancing AV decision systems and expanding adaptability across diverse scenarios.

Abstract: Autonomous vehicles (AVs) require adaptive behavior planners to navigate unpredictable, real-world environments safely. Traditional behavior trees (BTs) offer structured decision logic but are inherently static and demand labor-intensive manual tuning, limiting their applicability at SAE Level 5 autonomy. This paper presents an agentic framework that leverages large language models (LLMs) and multi-modal vision models (LVMs) to generate and adapt BTs on the fly. A specialized Descriptor agent applies chain-of-symbols prompting to assess scene criticality, a Planner agent constructs high-level sub-goals via in-context learning, and a Generator agent synthesizes executable BT sub-trees in XML format. Integrated into a CARLA+Nav2 simulation, our system triggers only upon baseline BT failure, demonstrating successful navigation around unexpected obstacles (e.g., street blockage) with no human intervention. Compared to a static BT baseline, this approach is a proof-of-concept that extends to diverse driving scenarios.

</details>


### [369] [Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion](https://arxiv.org/abs/2601.12224)
*Meng Wei,Kun Yuan,Shi Li,Yue Zhou,Long Bai,Nassir Navab,Hongliang Ren,Hong Joo Lee,Tom Vercauteren,Nicolas Padoy*

Main category: cs.CV

TL;DR: The paper introduces SurgRef, a motion-guided framework for segmenting surgical instruments based on language descriptions while addressing limitations of static visual cues.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance language-driven interaction with surgical scenes for intelligent operating rooms and surgical robotic assistance, addressing the lack of existing approaches in referring segmentation for surgical videos.

Method: SurgRef is a motion-guided framework that utilizes free-form language expressions linked to instrument motion, enabling segmentation even under challenging conditions like occlusion or ambiguity.

Result: The authors developed Ref-IMotion, a video dataset with spatiotemporal masks and motion-centric expressions, and achieved state-of-the-art accuracy in language-driven surgical video segmentation.

Conclusion: SurgRef sets a new benchmark for robust surgical video segmentation, paving the way for intelligent language-driven systems in surgical environments.

Abstract: Enabling intuitive, language-driven interaction with surgical scenes is a critical step toward intelligent operating rooms and autonomous surgical robotic assistance. However, the task of referring segmentation, localizing surgical instruments based on natural language descriptions, remains underexplored in surgical videos, with existing approaches struggling to generalize due to reliance on static visual cues and predefined instrument names. In this work, we introduce SurgRef, a novel motion-guided framework that grounds free-form language expressions in instrument motion, capturing how tools move and interact across time, rather than what they look like. This allows models to understand and segment instruments even under occlusion, ambiguity, or unfamiliar terminology. To train and evaluate SurgRef, we present Ref-IMotion, a diverse, multi-institutional video dataset with dense spatiotemporal masks and rich motion-centric expressions. SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures, setting a new benchmark for robust, language-driven surgical video segmentation.

</details>


### [370] [DiffusionQC: Artifact Detection in Histopathology via Diffusion Model](https://arxiv.org/abs/2601.12233)
*Zhenzhen Wang,Zhongliang Zhou,Zhuoyu Wen,Jeong Hwan Kook,John B Wojcik,John Kang*

Main category: cs.CV

TL;DR: The paper introduces DiffusionQC, a framework leveraging a diffusion model and contrastive learning to detect artifacts in histopathology images as outliers, requiring less data and annotation effort compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the detection of artifacts in histopathology images without relying on resource-intensive datasets and manual annotations, and to address the limitation of generalizing to novel artifact types.

Method: The method involves training a diffusion model on clean images to detect artifacts as outliers. A contrastive learning module is included to better separate clean and artifacted image distributions.

Result: DiffusionQC demonstrates superior artifact detection performance, cross-stain generalization, and significantly reduced reliance on annotated data compared to state-of-the-art techniques.

Conclusion: The proposed approach effectively identifies artifacts in histopathology images with enhanced generalization and reduced annotation requirements, offering a reliable tool for digital pathology analysis.

Abstract: Digital pathology plays a vital role across modern medicine, offering critical insights for disease diagnosis, prognosis, and treatment. However, histopathology images often contain artifacts introduced during slide preparation and digitization. Detecting and excluding them is essential to ensure reliable downstream analysis. Traditional supervised models typically require large annotated datasets, which is resource-intensive and not generalizable to novel artifact types. To address this, we propose DiffusionQC, which detects artifacts as outliers among clean images using a diffusion model. It requires only a set of clean images for training rather than pixel-level artifact annotations and predefined artifact types. Furthermore, we introduce a contrastive learning module to explicitly enlarge the distribution separation between artifact and clean images, yielding an enhanced version of our method. Empirical results demonstrate superior performance to state-of-the-art and offer cross-stain generalization capacity, with significantly less data and annotations.

</details>


### [371] [CD-TWINSAFE: A ROS-enabled Digital Twin for Scene Understanding and Safety Emerging V2I Technology](https://arxiv.org/abs/2601.12373)
*Amro Khaled,Farah Khaled,Omar Riad,Catherine M. Elias*

Main category: cs.CV

TL;DR: The paper presents the CD-TWINSAFE system, a V2I-enabled digital twin for autonomous vehicles, which provides real-time safety alerts by combining on-board and digital twin stacks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the safety and situational awareness of autonomous vehicles by integrating a vehicle-to-infrastructure (V2I) digital twin system capable of real-time monitoring and feedback.

Method: The paper proposes a two-stack architecture: an on-board driving stack (stereo camera, localization, and perception modules) and a digital twin stack (Unreal Engine 5 simulation). Real-time data from the vehicle is transmitted via ROS2 and 4G networking to virtualize and analyze driving scenarios.

Result: The proposed CD-TWINSAFE architecture was validated through testing in different driving scenarios, confirming its real-time capabilities and effective communication methodology.

Conclusion: CD-TWINSAFE demonstrated its potential to improve real-time safety monitoring for autonomous vehicles, bridging vehicle and infrastructure capabilities in an efficient digital twin system.

Abstract: In this paper, the CD-TWINSAFE is introduced, a V2I-based digital twin for Autonomous Vehicles. The proposed architecture is composed of two stacks running simultaneously, an on-board driving stack that includes a stereo camera for scene understanding, and a digital twin stack that runs an Unreal Engine 5 replica of the scene viewed by the camera as well as returning safety alerts to the cockpit. The on-board stack is implemented on the vehicle side including 2 main autonomous modules; localization and perception. The position and orientation of the ego vehicle are obtained using on-board sensors. Furthermore, the perception module is responsible for processing 20-fps images from stereo camera and understands the scene through two complementary pipelines. The pipeline are working on object detection and feature extraction including object velocity, yaw and the safety metrics time-to-collision and time-headway. The collected data form the driving stack are sent to the infrastructure side through the ROS-enabled architecture in the form of custom ROS2 messages and sent over UDP links that ride a 4G modem for V2I communication. The environment is monitored via the digital twin through the shared messages which update the information of the spawned ego vehicle and detected objects based on the real-time localization and perception data. Several tests with different driving scenarios to confirm the validity and real-time response of the proposed architecture.

</details>


### [372] [Less is More: Label-Guided Summarization of Procedural and Instructional Videos](https://arxiv.org/abs/2601.12243)
*Shreya Rajpal,Michal Golovanesky,Carsten Eickhoff*

Main category: cs.CV

TL;DR: This paper presents PRISM, a framework for creating precise and semantically aware video summaries by effectively combining visual and semantic analysis.


<details>
  <summary>Details</summary>
Motivation: To improve video summarization quality for better review, documentation, and analysis, particularly in domains like surgical training where clear and concise representations are crucial.

Method: PRISM uses a three-stage process: adaptive visual sampling, label-driven keyframe anchoring, and contextual validation through a large language model to create semantically grounded video summaries.

Result: The approach retains 84% semantic content while using only 5% of the original frames, outperforming baselines by up to 33%, and generalizes effectively to different video tasks.

Conclusion: PRISM effectively produces semantically coherent and contextually rich summaries, presenting a robust solution for both instructional and domain-specific video applications.

Abstract: Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what's happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.

</details>


### [373] [DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition](https://arxiv.org/abs/2601.12729)
*Hanyu Zhu,Zhihao Zhan,Yuhang Ming,Liang Li,Dibo Hou,Javier Civera,Wanzeng Kong*

Main category: cs.CV

TL;DR: The paper addresses challenges in visual place recognition (VPR) by introducing DC-VLAQ, a framework that combines complementary visual foundation models and a robust global aggregation scheme to improve performance under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in learning robust global representations for VPR, especially under large viewpoint changes, illumination variations, and domain shifts.

Method: The paper proposes DC-VLAQ, which includes a lightweight residual-guided complementary fusion to merge information from VFMs (DINOv2 and CLIP) and a novel global aggregation scheme (VLAQ) that enhances stability and preserves discriminative cues.

Result: The proposed DC-VLAQ framework consistently outperformed strong baselines, achieving state-of-the-art results on multiple VPR benchmarks like Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime.

Conclusion: DC-VLAQ demonstrates the effective use of complementary VFMs and novel global aggregation, making it a robust solution for VPR tasks, particularly in challenging and dynamic environments.

Abstract: One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.

</details>


### [374] [An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion](https://arxiv.org/abs/2601.12249)
*Ehsan Sadeghi Pour,Mahdi Esmaeili,Morteza Romoozi*

Main category: cs.CV

TL;DR: The paper presents a novel framework combining Pyramid Adaptive Atrous Convolution (PAAC) and Transformers for mammographic image analysis, achieving superior accuracy in breast cancer diagnosis.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and efficiency of breast cancer detection in mammographic images, addressing challenges in early diagnosis.

Method: PAAC and Transformer architectures were integrated, leveraging Multi-Scale Feature Fusion, Self-Attention mechanisms, and optimized loss functions (Dice Loss and Focal Loss). Images were preprocessed and resized for training using datasets like INbreast, MIAS, and DDSM.

Result: The model achieved 98.5% accuracy, surpassing traditional methods like BreastNet and SegFormer, with high sensitivity, specificity, precision, and F1 scores.

Conclusion: The framework is effective and reliable for identifying malignant masses in mammograms, demonstrating its potential for integration into clinical diagnostic systems.

Abstract: Breast cancer is one of the most common cancers among women worldwide, and its accurate and timely diagnosis plays a critical role in improving treatment outcomes. This thesis presents an innovative framework for detecting malignant masses in mammographic images by integrating the Pyramid Adaptive Atrous Convolution (PAAC) and Transformer architectures. The proposed approach utilizes Multi-Scale Feature Fusion to enhance the extraction of features from benign and malignant tissues and combines Dice Loss and Focal Loss functions to improve the model's learning process, effectively reducing errors in binary breast cancer classification and achieving high accuracy and efficiency. In this study, a comprehensive dataset of breast cancer images from INbreast, MIAS, and DDSM was preprocessed through data augmentation and contrast enhancement and resized to 227x227 pixels for model training. Leveraging the Transformer's ability to manage long-range dependencies with Self-Attention mechanisms, the proposed model achieved high accuracy in detecting cancerous masses, outperforming foundational models such as BreastNet, DeepMammo, Multi-Scale CNN, Swin-Unet, and SegFormer. The final evaluation results for the proposed model include an accuracy of 98.5\%, sensitivity of 97.8\%, specificity of 96.3\%, F1-score of 98.2\%, and overall precision of 97.9\%. These metrics demonstrate a significant improvement over traditional methods and confirm the model's effectiveness in identifying cancerous masses in complex scenarios and large datasets. This model shows potential as a reliable and efficient tool for breast cancer diagnosis and can be effectively integrated into medical diagnostic systems.

</details>


### [375] [Federated Joint Learning for Domain and Class Generalization](https://arxiv.org/abs/2601.12253)
*Haoran Xu,Jiaze Li,Jianzhong Ju,Zhenbo Luo*

Main category: cs.CV

TL;DR: The paper introduces FedDCG, a federated learning method that improves class and domain generalization for visual-language models, addressing both challenges simultaneously.


<details>
  <summary>Details</summary>
Motivation: Current methods focus on addressing unseen classes or domains individually, missing a joint framework to tackle both, especially in the setting of federated learning for large-scale visual-language models.

Method: The FedDCG approach introduces domain grouping and decoupling mechanisms that enable class-generalized networks trained per group to avoid decision boundary confusion. During inference, results are aggregated based on domain similarity, leveraging a learnable network to balance class and domain generalization.

Result: Experimental evaluations on diverse datasets reveal that FedDCG surpasses state-of-the-art methods in terms of accuracy and robustness, demonstrating its effectiveness in generalizing to unseen classes and domains.

Conclusion: FedDCG effectively combines class and domain generalization in federated learning settings, providing a robust solution for visual-language models to handle unseen scenarios.

Abstract: Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \textbf{Fed}erated Joint Learning for \textbf{D}omain and \textbf{C}lass \textbf{G}eneralization, termed \textbf{FedDCG}, a novel approach that addresses both class and domain generalization in federated learning settings. Our method introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. During inference, we aggregate class-generalized results based on domain similarity, effectively integrating knowledge from both class and domain generalization. Specifically, a learnable network is employed to enhance class generalization capabilities, and a decoupling mechanism separates general and domain-specific knowledge, improving generalization to unseen domains. Extensive experiments across various datasets show that \textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and robustness.

</details>


### [376] [Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy](https://arxiv.org/abs/2601.12257)
*Fadlullah Raji,John Murray-Bruce*

Main category: cs.CV

TL;DR: This research explores non-line-of-sight (NLOS) imaging through creating 3D reconstructions of hidden scenes using ordinary photographs, introducing new computational methods for improved accuracy despite challenging inverse problems.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the limitations of traditional imaging methods in scenarios where a direct line of sight is inaccessible, and to advance passive non-line-of-sight imaging which has primarily been limited to low-resolution or constrained scenarios.

Method: The authors propose a novel light transport model reformulation into light-occluding and non-light-occluding components, leading to a separable non-linear least squares (SNLLS) inverse problem. They provide two solutions: a gradient-based optimization and a physics-inspired neural network named Soft Shadow diffusion (SSD).

Result: Both proposed methods successfully reconstruct 3D scenes in experimental NLOS scenarios. SSD proves particularly effective, showing strong performance on real-world scenes, generalization to unseen classes, robustness to noise, and adaptability under various lighting conditions.

Conclusion: This work demonstrates a significant advancement in passive NLOS imaging, achieving 3D reconstruction with promising robustness and real-world applicability. The approaches open new possibilities for practical applications of NLOS imaging.

Abstract: Conventional imaging requires a line of sight to create accurate visual representations of a scene. In certain circumstances, however, obtaining a suitable line of sight may be impractical, dangerous, or even impossible. Non-line-of-sight (NLOS) imaging addresses this challenge by reconstructing the scene from indirect measurements. Recently, passive NLOS methods that use an ordinary photograph of the subtle shadow cast onto a visible wall by the hidden scene have gained interest. These methods are currently limited to 1D or low-resolution 2D color imaging or to localizing a hidden object whose shape is approximately known. Here, we generalize this class of methods and demonstrate a 3D reconstruction of a hidden scene from an ordinary NLOS photograph. To achieve this, we propose a novel reformulation of the light transport model that conveniently decomposes the hidden scene into \textit{light-occluding} and \textit{non-light-occluding} components to yield a separable non-linear least squares (SNLLS) inverse problem. We develop two solutions: A gradient-based optimization method and a physics-inspired neural network approach, which we call Soft Shadow diffusion (SSD). Despite the challenging ill-conditioned inverse problem encountered here, our approaches are effective on numerous 3D scenes in real experimental scenarios. Moreover, SSD is trained in simulation but generalizes well to unseen classes in simulation and real-world NLOS scenes. SSD also shows surprising robustness to noise and ambient illumination.

</details>


### [377] [AgenticPruner: MAC-Constrained Neural Network Compression via LLM-Driven Strategy Search](https://arxiv.org/abs/2601.12272)
*Shahrzad Esmat,Mahdi Banisharif,Ali Jannesari*

Main category: cs.CV

TL;DR: The paper introduces AgenticPruner, a framework leveraging large language models to optimize neural network pruning under strict MAC operation budgets, significantly improving performance and computational predictability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge that most pruning methods focus only on parameter reduction rather than ensuring computational efficiency, which results in unpredictable inference latency on devices with strict MAC operation constraints.

Method: The method uses a multi-agent framework—Profiling Agent for architecture analysis, Master Agent for workflow orchestration, and Analysis Agent (using Claude 3.5 Sonnet) for learning strategies through in-context learning. The approach incorporates graph-based structural grouping and iterative strategy adaptation for accurate MAC-budget compliance.

Result: The framework significantly improves performance and computational predictability, achieving higher accuracy and reduced MAC operations across various architectures like ResNet, ConvNeXt, and DeiT. For example, ResNet-50 achieves 1.77G MAC with +0.91% accuracy improvement, while ConvNeXt-Small and Vision Transformers also exhibit efficiency gains.

Conclusion: AgenticPruner proves effective in balancing computational efficiency and predictive accuracy, offering practical benefits for resource-constrained deployment scenarios requiring strict MAC compliance.

Abstract: Neural network pruning remains essential for deploying deep learning models on resource-constrained devices, yet existing approaches primarily target parameter reduction without directly controlling computational cost. This yields unpredictable inference latency in deployment scenarios where strict Multiply-Accumulate (MAC) operation budgets must be met. We propose AgenticPruner, a framework utilizing large language models to achieve MAC-constrained optimization through iterative strategy learning. Our approach coordinates three specialized agents: a Profiling Agent that analyzes model architecture and MAC distributions, a Master Agent that orchestrates the workflow with divergence monitoring, and an Analysis Agent powered by Claude 3.5 Sonnet that learns optimal strategies from historical attempts. Through in-context learning, the Analysis Agent improves convergence success rate from 48% to 71% compared to grid search. Building upon isomorphic pruning's graph-based structural grouping, our method adds context-aware adaptation by analyzing patterns across pruning iterations, enabling automatic convergence to target MAC budgets within user-defined tolerance bands.
  We validate our framework on ImageNet-1K across ResNet, ConvNeXt, and DeiT architectures. On CNNs, our approach achieves MAC targeting while maintaining or improving accuracy: ResNet-50 reaches 1.77G MACs with 77.04% accuracy (+0.91% vs baseline); ResNet-101 achieves 4.22G MACs with 78.94% accuracy (+1.56% vs baseline). For ConvNeXt-Small, pruning to 8.17G MACs yields 1.41x GPU and 1.07x CPU speedup with 45% parameter reduction. On Vision Transformers, we demonstrate MAC-budget compliance within user-defined tolerance bands (typically +1% to +5% overshoot, -5% to -15% undershoot), establishing feasibility for deployment scenarios requiring strict computational guarantees.

</details>


### [378] [Learning Fine-Grained Correspondence with Cross-Perspective Perception for Open-Vocabulary 6D Object Pose Estimation](https://arxiv.org/abs/2601.13565)
*Yu Qin,Shimeng Fan,Fan Yang,Zixuan Xue,Zijie Mai,Wenrui Chen,Kailun Yang,Zhiyong Li*

Main category: cs.CV

TL;DR: The authors propose FiCoP, a framework to improve open-vocabulary 6D object pose estimation by using spatially-constrained patch-level correspondence, leading to better performance in removing background noise and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing open-vocabulary 6D object pose estimation methods struggle with excessive ambiguity caused by global matching strategies, which often confuse target features with background distractors in open-world scenarios.

Method: FiCoP framework involves object-centric preprocessing, a Cross-Perspective Global Perception module for dual-view feature fusion, and a Patch Correlation Predictor to generate a spatially-filtered block-wise association map for precise matching.

Result: FiCoP demonstrated significant improvements in Average Recall: an 8.0% increase on the REAL275 dataset and a 6.1% increase on the Toyota-Light dataset compared to the state-of-the-art methods.

Conclusion: FiCoP enhances robotic perception of unseen objects in open-world environments by focusing on fine-grained, noise-resilient pose estimation methodologies and will contribute to robust and generalized perception solutions in the field.

Abstract: Open-vocabulary 6D object pose estimation empowers robots to manipulate arbitrary unseen objects guided solely by natural language. However, a critical limitation of existing approaches is their reliance on unconstrained global matching strategies. In open-world scenarios, trying to match anchor features against the entire query image space introduces excessive ambiguity, as target features are easily confused with background distractors. To resolve this, we propose Fine-grained Correspondence Pose Estimation (FiCoP), a framework that transitions from noise-prone global matching to spatially-constrained patch-level correspondence. Our core innovation lies in leveraging a patch-to-patch correlation matrix as a structural prior to narrowing the matching scope, effectively filtering out irrelevant clutter to prevent it from degrading pose estimation. Firstly, we introduce an object-centric disentanglement preprocessing to isolate the semantic target from environmental noise. Secondly, a Cross-Perspective Global Perception (CPGP) module is proposed to fuse dual-view features, establishing structural consensus through explicit context reasoning. Finally, we design a Patch Correlation Predictor (PCP) that generates a precise block-wise association map, acting as a spatial filter to enforce fine-grained, noise-resilient matching. Experiments on the REAL275 and Toyota-Light datasets demonstrate that FiCoP improves Average Recall by 8.0% and 6.1%, respectively, compared to the state-of-the-art method, highlighting its capability to deliver robust and generalized perception for robotic agents operating in complex, unconstrained open-world environments. The source code will be made publicly available at https://github.com/zjjqinyu/FiCoP.

</details>


### [379] [CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training](https://arxiv.org/abs/2601.12282)
*Pralaypati Ta,Sriram Venkatesaperumal,Keerthi Ram,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: The paper introduces CytoCLIP, an automated vision-language model framework to identify brain regions based on cytoarchitecture, outperforming existing methods in classification tasks.


<details>
  <summary>Details</summary>
Motivation: Manual delineation of brain cytoarchitecture is time-consuming and requires expertise, creating a need for an automated solution.

Method: CytoCLIP leverages a pre-trained CLIP framework to learn joint visual-text representations of brain cytoarchitecture using two model variants for low-resolution and high-resolution image analysis.

Result: CytoCLIP achieved high accuracy with an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification. It outperformed existing methods.

Conclusion: CytoCLIP successfully automates cytoarchitectural analysis, saving time and improving accuracy, thereby advancing brain region identification tasks in histology.

Abstract: The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized knowledge. An automated approach is necessary to minimize the effort needed from human experts. To address this, we propose CytoCLIP, a suite of vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP comprises two model variants: one is trained using low-resolution whole-region images to understand the overall cytoarchitectural pattern of an area, and the other is trained on high-resolution image tiles for detailed cellular-level representation. The training dataset is created from NISSL-stained histological sections of developing fetal brains of different gestational weeks. It includes 86 distinct regions for low-resolution images and 384 brain regions for high-resolution tiles. We evaluate the model's understanding of the cytoarchitecture and generalization ability using region classification and cross-modal retrieval tasks. Multiple experiments are performed under various data setups, including data from samples of different ages and sectioning planes. Experimental results demonstrate that CytoCLIP outperforms existing methods. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.

</details>


### [380] [FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation](https://arxiv.org/abs/2601.13976)
*Jing Zuo,Lingzhou Mu,Fan Jiang,Chengcheng Ma,Mu Xu,Yonggang Qi*

Main category: cs.CV

TL;DR: The paper introduces FantasyVLN, a framework that improves Vision-and-Language Navigation (VLN) by addressing issues in Chain-of-Thought (CoT) reasoning approaches, achieving better real-time navigation with reduced latency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to achieve human-level navigation in VLN using multimodal reasoning techniques while solving key drawbacks of existing Chain-of-Thought methods, such as token inflation and lack of spatial grounding.

Method: The method involves training a model using a Visual AutoRegressor (VAR) to encode imagined visual tokens into a compact latent space and employing a unified multi-CoT strategy for multimodal instruction-to-action mapping.

Result: Experiments on LH-VLN demonstrate that FantasyVLN improves success rates and navigation efficiency while significantly reducing inference latency compared to explicit CoT methods.

Conclusion: FantasyVLN effectively combines reasoning-aware CoT benefits with compact multimodal representations, offering substantial improvements in real-time VLN performance.

Abstract: Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.

</details>


### [381] [SDiT: Semantic Region-Adaptive for Diffusion Transformers](https://arxiv.org/abs/2601.12283)
*Bowen Lin,Fanjiang Ye,Yihua Liu,Zhenghui Guo,Boyuan Zhang,Weijian Zheng,Yufan Xu,Tiancheng Xing,Yuke Wang,Chengming Zhang*

Main category: cs.CV

TL;DR: The paper proposes SDiT, a method to accelerate Diffusion Transformers for text-to-image synthesis by allocating computation to complex regions, achieving up to 3.0x speedup with similar quality.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the computational inefficiency of Diffusion Transformers, particularly in the iterative denoising process and quadratic cost of global attention, by observing uneven denoising dynamics across spatial regions.

Method: The paper introduces SDiT, a framework with three key components: semantic-aware clustering using Quickshift segmentation, complexity-driven regional scheduling, and boundary-aware refinement, all implemented without model retraining.

Result: SDiT achieves up to 3.0x acceleration in text-to-image synthesis while maintaining nearly identical perceptual and semantic quality compared to standard full-attention methods.

Conclusion: The proposed SDiT framework effectively enhances the efficiency of Diffusion Transformers, making them faster and maintaining high-quality outputs without altering the existing architectures.

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art performance in text-to-image synthesis but remain computationally expensive due to the iterative nature of denoising and the quadratic cost of global attention. In this work, we observe that denoising dynamics are spatially non-uniform-background regions converge rapidly while edges and textured areas evolve much more actively. Building on this insight, we propose SDiT, a Semantic Region-Adaptive Diffusion Transformer that allocates computation according to regional complexity. SDiT introduces a training-free framework combining (1) semantic-aware clustering via fast Quickshift-based segmentation, (2) complexity-driven regional scheduling to selectively update informative areas, and (3) boundary-aware refinement to maintain spatial coherence. Without any model retraining or architectural modification, SDiT achieves up to 3.0x acceleration while preserving nearly identical perceptual and semantic quality to full-attention inference.

</details>


### [382] [LegacyAvatars: Volumetric Face Avatars For Traditional Graphics Pipelines](https://arxiv.org/abs/2601.12285)
*Safa C. Medin,Gengyan Li,Ziqian Bai,Ruofei Du,Leonhard Helminger,Yinda Zhang,Stephan J. Garbin,Philip L. Davidson,Gregory W. Wornell,Thabo Beeler,Abhimitra Meka*

Main category: cs.CV

TL;DR: The paper proposes a new method for photorealistic 3D face avatars, offering efficient rendering and controllable animations using explicit layered meshes and textures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to simplify and enhance the efficiency of rendering photorealistic 3D face avatars using classical methods while retaining high-quality features and controllability.

Method: The approach involves learning radiance manifolds to extract explicit layered meshes with accompanying appearance and warp textures, enabling linear blending and alpha compositing for rendering.

Result: The resulting representation can be efficiently streamed and rendered on legacy graphics platforms without custom engineering, supporting photorealistic features and controllable animations.

Conclusion: The work demonstrates a novel, practical approach for rendering and animating 3D face avatars, making them accessible on existing platforms and simplifying deployment.

Abstract: We introduce a novel representation for efficient classical rendering of photorealistic 3D face avatars. Leveraging recent advances in radiance fields anchored to parametric face models, our approach achieves controllable volumetric rendering of complex facial features, including hair, skin, and eyes. At enrollment time, we learn a set of radiance manifolds in 3D space to extract an explicit layered mesh, along with appearance and warp textures. During deployment, this allows us to control and animate the face through simple linear blending and alpha compositing of textures over a static mesh. This explicit representation also enables the generated avatar to be efficiently streamed online and then rendered using classical mesh and shader-based rendering on legacy graphics platforms, eliminating the need for any custom engineering or integration.

</details>


### [383] [Concepts from Representations: Post-hoc Concept Bottleneck Models via Sparse Decomposition of Visual Representations](https://arxiv.org/abs/2601.12303)
*Shizhan Gong,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: This paper introduces PCBM-ReD, a novel pipeline that retrofits interpretability onto pre-trained models by leveraging concepts extracted automatically and optimized for relevance and accuracy.


<details>
  <summary>Details</summary>
Motivation: The opacity of deep learning models in critical domains necessitates interpretable solutions, as existing methods like CBMs and post-hoc explanations have significant limitations.

Method: PCBM-ReD extracts visual concepts using pre-trained encoders, labels and filters them via multimodal LLMs, and optimizes a concept subset for interpretability, incorporating CLIP for visual-text alignment.

Result: PCBM-ReD achieves state-of-the-art accuracy in 11 image classification tasks, surpasses traditional methods in interpretability, and approaches the performance of end-to-end models.

Conclusion: PCBM-ReD successfully combines interpretability with high performance, addressing the shortcomings of existing methods and improving applicability in critical domains.

Abstract: Deep learning has achieved remarkable success in image recognition, yet their inherent opacity poses challenges for deployment in critical domains. Concept-based interpretations aim to address this by explaining model reasoning through human-understandable concepts. However, existing post-hoc methods and ante-hoc concept bottleneck models (CBMs), suffer from limitations such as unreliable concept relevance, non-visual or labor-intensive concept definitions, and model or data-agnostic assumptions. This paper introduces Post-hoc Concept Bottleneck Model via Representation Decomposition (PCBM-ReD), a novel pipeline that retrofits interpretability onto pretrained opaque models. PCBM-ReD automatically extracts visual concepts from a pre-trained encoder, employs multimodal large language models (MLLMs) to label and filter concepts based on visual identifiability and task relevance, and selects an independent subset via reconstruction-guided optimization. Leveraging CLIP's visual-text alignment, it decomposes image representations into linear combination of concept embeddings to fit into the CBMs abstraction. Extensive experiments across 11 image classification tasks show PCBM-ReD achieves state-of-the-art accuracy, narrows the performance gap with end-to-end models, and exhibits better interpretability.

</details>


### [384] [A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models](https://arxiv.org/abs/2601.12304)
*Wutao Chen,Huaqin Zou,Chen Wan,Lifeng Huang*

Main category: cs.CV

TL;DR: The paper introduces 2S-GDA, a globally-diverse attack framework for adversarial examples targeting vision-language pre-training models, improving attack success rates significantly.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal attacks often lack perturbation diversity and rely on unstable pipelines, making them prone to inefficiency and less effective against VLP models.

Method: The proposed 2S-GDA framework uses two stages: textual perturbations with globally-diverse strategies and visual perturbations employing multi-scale resizing and block-shuffle rotation.

Result: Experiments show that 2S-GDA outperforms state-of-the-art methods, achieving success rate improvements up to 11.17% in black-box attack scenarios.

Conclusion: 2S-GDA improves adversarial attack success rates with globally-diverse strategies for text and image perturbations and can be combined with existing methods to enhance transferability further.

Abstract: Vision-language pre-training (VLP) models are vulnerable to adversarial examples, particularly in black-box scenarios. Existing multimodal attacks often suffer from limited perturbation diversity and unstable multi-stage pipelines. To address these challenges, we propose 2S-GDA, a two-stage globally-diverse attack framework. The proposed method first introduces textual perturbations through a globally-diverse strategy by combining candidate text expansion with globally-aware replacement. To enhance visual diversity, image-level perturbations are generated using multi-scale resizing and block-shuffle rotation. Extensive experiments on VLP models demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17\% in black-box settings. Our framework is modular and can be easily combined with existing methods to further enhance adversarial transferability.

</details>


### [385] [Adaptive Multi-Scale Correlation Meta-Network for Few-Shot Remote Sensing Image Classification](https://arxiv.org/abs/2601.12308)
*Anurag Kaushish,Ayan Sar,Sampurna Roy,Sudeshna Chakraborty,Prashant Trivedi,Tanupriya Choudhury,Kanav Gupta*

Main category: cs.CV

TL;DR: The paper presents AMC-MetaNet, a method for few-shot learning in remote sensing, addressing data scarcity, domain shifts, and multi-scale geospatial object challenges in an efficient and lightweight manner.


<details>
  <summary>Details</summary>
Motivation: Few-shot learning in remote sensing faces challenges from limited labeled data, significant domain shifts, and the multi-scale nature of geospatial objects, calling for innovative solutions.

Method: The authors propose AMC-MetaNet, incorporating innovative correlation-guided feature pyramids for scale-invariant patterns, an Adaptive Channel Correlation Module (ACCM) for dynamic cross-scale relations, and correlation-guided meta-learning for pattern-based learning without heavy pre-trained models.

Result: AMC-MetaNet demonstrates high accuracy of up to 86.65% in 5-way 5-shot classification across various remote sensing datasets, with high computational efficiency (<50ms inference) and lightweight model parameters (20x fewer than ResNet-18).

Conclusion: AMC-MetaNet is an efficient, lightweight, and effective framework for tackling few-shot learning challenges in remote sensing, suitable for real-world applications and outperforming prior approaches.

Abstract: Few-shot learning in remote sensing remains challenging due to three factors: the scarcity of labeled data, substantial domain shifts, and the multi-scale nature of geospatial objects. To address these issues, we introduce Adaptive Multi-Scale Correlation Meta-Network (AMC-MetaNet), a lightweight yet powerful framework with three key innovations: (i) correlation-guided feature pyramids for capturing scale-invariant patterns, (ii) an adaptive channel correlation module (ACCM) for learning dynamic cross-scale relationships, and (iii) correlation-guided meta-learning that leverages correlation patterns instead of conventional prototype averaging. Unlike prior approaches that rely on heavy pre-trained models or transformers, AMC-MetaNet is trained from scratch with only $\sim600K$ parameters, offering $20\times$ fewer parameters than ResNet-18 while maintaining high efficiency ($<50$ms per image inference). AMC-MetaNet achieves up to 86.65\% accuracy in 5-way 5-shot classification on various remote sensing datasets, including EuroSAT, NWPU-RESISC45, UC Merced Land Use, and AID. Our results establish AMC-MetaNet as a computationally efficient, scale-aware framework for real-world few-shot remote sensing.

</details>


### [386] [CurConMix+: A Unified Spatio-Temporal Framework for Hierarchical Surgical Workflow Understanding](https://arxiv.org/abs/2601.12312)
*Yongjun Jeon,Jongmin Shin,Kanggil Park,Seonmin Park,Soyoung Lim,Jung Yong Kim,Jinsoo Rhu,Jongman Kim,Gyu-Seong Choi,Namkee Oh,Kyu-Hwan Jung*

Main category: cs.CV

TL;DR: The paper introduces CurConMix+, a framework for recognizing fine-grained surgical action triplets, aiming to overcome major challenges in class imbalance and semantic interdependence. A new dataset, LLS48, is introduced to support hierarchical annotations and benchmark performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods in surgical action triplet recognition do not fully address challenges like severe class imbalance, subtle visual variations, and semantic interdependence among triplet components. A unified approach is needed for robust analysis.

Method: CurConMix+ leverages curriculum-guided contrastive learning, structured hard-pair sampling, and feature-level mix-up for spatial representation. It integrates Multi-Resolution Temporal Transformer (MRTT) for adaptive multi-scale temporal analysis.

Result: Experiments on CholecT45 and LLS48 show CurConMix+ outperforms state-of-the-art approaches in triplet recognition while demonstrating strong generalization across hierarchical tasks like phase and step recognition.

Conclusion: CurConMix+ and LLS48 provide a unified framework and benchmark for hierarchy-aware surgical workflow understanding, addressing key challenges and facilitating reproducibility and future research through public releases.

Abstract: Surgical action triplet recognition aims to understand fine-grained surgical behaviors by modeling the interactions among instruments, actions, and anatomical targets. Despite its clinical importance for workflow analysis and skill assessment, progress has been hindered by severe class imbalance, subtle visual variations, and the semantic interdependence among triplet components. Existing approaches often address only a subset of these challenges rather than tackling them jointly, which limits their ability to form a holistic understanding. This study builds upon CurConMix, a spatial representation framework. At its core, a curriculum-guided contrastive learning strategy learns discriminative and progressively correlated features, further enhanced by structured hard-pair sampling and feature-level mixup. Its temporal extension, CurConMix+, integrates a Multi-Resolution Temporal Transformer (MRTT) that achieves robust, context-aware understanding by adaptively fusing multi-scale temporal features and dynamically balancing spatio-temporal cues. Furthermore, we introduce LLS48, a new, hierarchically annotated benchmark for complex laparoscopic left lateral sectionectomy, providing step-, task-, and action-level annotations. Extensive experiments on CholecT45 and LLS48 demonstrate that CurConMix+ not only outperforms state-of-the-art approaches in triplet recognition, but also exhibits strong cross-level generalization, as its fine-grained features effectively transfer to higher-level phase and step recognition tasks. Together, the framework and dataset provide a unified foundation for hierarchy-aware, reproducible, and interpretable surgical workflow understanding. The code and dataset will be publicly released on GitHub to facilitate reproducibility and further research.

</details>


### [387] [S^2F-Net:A Robust Spatial-Spectral Fusion Framework for Cross-Model AIGC Detection](https://arxiv.org/abs/2601.12313)
*Xiangyu Hu,Yicheng Hong,Hongchuang Zheng,Wenjun Zeng,Bingyao Liu*

Main category: cs.CV

TL;DR: The paper introduces S2F-Net, a detection framework that utilizes frequency-domain artifacts to distinguish between real and synthetic imagery, demonstrating high accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing detection methods struggle to generalize and fail when faced with new generative models, necessitating more robust frameworks.

Method: The method uses a frequency attention module to identify and emphasize spectral discrepancies created during generative upsampling processes, enhancing cross-model detection.

Result: S2F-Net achieved a 90.49% accuracy on the AIGCDetectBenchmark, substantially outperforming current detection methods across 17 generative model categories.

Conclusion: By harnessing spectral differences and frequency analysis, S2F-Net showcases strong generalization abilities and effective detection capabilities for generative models.

Abstract: The rapid development of generative models has imposed an urgent demand for detection schemes with strong generalization capabilities. However, existing detection methods generally suffer from overfitting to specific source models, leading to significant performance degradation when confronted with unseen generative architectures. To address these challenges, this paper proposes a cross-model detection framework called S 2 F-Net, whose core lies in exploring and leveraging the inherent spectral discrepancies between real and synthetic textures. Considering that upsampling operations leave unique and distinguishable frequency fingerprints in both texture-poor and texture-rich regions, we focus our research on the detection of frequency-domain artifacts, aiming to fundamentally improve the generalization performance of the model. Specifically, we introduce a learnable frequency attention module that adaptively weights and enhances discriminative frequency bands by synergizing spatial texture analysis and spectral dependencies.On the AIGCDetectBenchmark, which includes 17 categories of generative models, S 2 F-Net achieves a detection accuracy of 90.49%, significantly outperforming various existing baseline methods in cross-domain detection scenarios.

</details>


### [388] [GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer](https://arxiv.org/abs/2601.12316)
*Xinyuan Zhao,Xianrui Chen,Ahmad Chaddad*

Main category: cs.CV

TL;DR: This paper presents a semantics-modulated, multi-scale Transformer (Gazeformer) for improving 3D gaze estimation, achieving state-of-the-art angular error reductions on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To address limitations in precise 3D gaze estimation by incorporating semantic-level conditioning and multi-scale integration techniques.

Method: The proposed model combines CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses global vectors with patch and high-resolution CNN tokens, utilizes a unified attention space, and replaces several FFN blocks with a routed/shared Mixture of Experts (MoE).

Result: The model achieves state-of-the-art angular errors on datasets MPIIFaceGaze (2.49°), EYEDIAP (3.22°), Gaze360 (10.16°), and ETH-XGaze (1.44°), showing up to 64% relative improvement.

Conclusion: The study highlights the effectiveness of prototype conditioning, cross-scale fusion, and other innovations in advancing 3D gaze estimation, with the model's code available for public use.

Abstract: We present a semantics modulated, multi scale Transformer for 3D gaze estimation. Our model conditions CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses these prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, and replaces several FFN blocks with routed/shared Mixture of Experts to increase conditional capacity. Evaluated on MPIIFaceGaze, EYEDIAP, Gaze360 and ETH-XGaze, our model achieves new state of the art angular errors of 2.49°, 3.22°, 10.16°, and 1.44°, demonstrating up to a 64% relative improvement over previously reported results. ablations attribute gains to prototype conditioning, cross scale fusion, MoE and hyperparameter. Our code is publicly available at https://github. com/AIPMLab/Gazeformer.

</details>


### [389] [Multi-Sensor Matching with HyperNetworks](https://arxiv.org/abs/2601.12325)
*Eli Passov,Nathan S. Netanyahu,Yosi Keller*

Main category: cs.CV

TL;DR: This paper presents a new lightweight architecture using hypernetworks to improve multimodal patch matching, achieving state-of-the-art results while addressing domain shift in VIS-IR datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of multimodal patch matching, particularly dealing with appearance differences (e.g., VIS-IR), while maintaining model efficiency.

Method: The approach introduces a Siamese CNN augmented with hypernetwork modules for adaptive weight modulation and conditional instance normalization for modality-specific adaptation, trained using triplet loss and hard-negative mining.

Result: Achieves state-of-the-art results on VIS-NIR and VIS-IR benchmarks, matching or surpassing prior methods on various datasets despite lower inference cost.

Conclusion: The proposed lightweight framework improves patch matching robustness across modalities, offering both efficiency and high performance. The release of GAP-VIR dataset promotes further research on cross-domain generalization.

Abstract: Hypernetworks are models that generate or modulate the weights of another network. They provide a flexible mechanism for injecting context and task conditioning and have proven broadly useful across diverse applications without significant increases in model size. We leverage hypernetworks to improve multimodal patch matching by introducing a lightweight descriptor-learning architecture that augments a Siamese CNN with (i) hypernetwork modules that compute adaptive, per-channel scaling and shifting and (ii) conditional instance normalization that provides modality-specific adaptation (e.g., visible vs. infrared, VIS-IR) in shallow layers. This combination preserves the efficiency of descriptor-based methods during inference while increasing robustness to appearance shifts. Trained with a triplet loss and hard-negative mining, our approach achieves state-of-the-art results on VIS-NIR and other VIS-IR benchmarks and matches or surpasses prior methods on additional datasets, despite their higher inference cost. To spur progress on domain shift, we also release GAP-VIR, a cross-platform (ground/aerial) VIS-IR patch dataset with 500K pairs, enabling rigorous evaluation of cross-domain generalization and adaptation.

</details>


### [390] [EmoKGEdit: Training-free Affective Injection via Visual Cue Transformation](https://arxiv.org/abs/2601.12326)
*Jing Zhang,Bingjie Fan*

Main category: cs.CV

TL;DR: EmoKGEdit proposes a framework for better image emotion editing by using a knowledge graph and disentangling emotional cues while preserving structures.


<details>
  <summary>Details</summary>
Motivation: Existing methods face challenges in clearly separating emotional cues from latent content, resulting in poor emotional expression and distortion in visual structures.

Method: The framework constructs a Multimodal Sentiment Association Knowledge Graph (MSA-KG) and introduces a disentangled structure-emotion editing module for precise editing maintaining coherence.

Result: EmoKGEdit demonstrates superior performance in emotion fidelity and visual content preservation compared to state-of-the-art methods.

Conclusion: EmoKGEdit effectively enhances emotion editing in images by incorporating knowledge graphs and disentangled editing techniques, addressing key limitations in prior methods.

Abstract: Existing image emotion editing methods struggle to disentangle emotional cues from latent content representations, often yielding weak emotional expression and distorted visual structures. To bridge this gap, we propose EmoKGEdit, a novel training-free framework for precise and structure-preserving image emotion editing. Specifically, we construct a Multimodal Sentiment Association Knowledge Graph (MSA-KG) to disentangle the intricate relationships among objects, scenes, attributes, visual clues and emotion. MSA-KG explicitly encode the causal chain among object-attribute-emotion, and as external knowledge to support chain of thought reasoning, guiding the multimodal large model to infer plausible emotion-related visual cues and generate coherent instructions. In addition, based on MSA-KG, we design a disentangled structure-emotion editing module that explicitly separates emotional attributes from layout features within the latent space, which ensures that the target emotion is effectively injected while strictly maintaining visual spatial coherence. Extensive experiments demonstrate that EmoKGEdit achieves excellent performance in both emotion fidelity and content preservation, and outperforms the state-of-the-art methods.

</details>


### [391] [FlowIID: Single-Step Intrinsic Image Decomposition via Latent Flow Matching](https://arxiv.org/abs/2601.12329)
*Mithlesh Singla,Seema Kumari,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: The paper proposes FlowIID, a compact and efficient architecture for intrinsic image decomposition (IID) using flow matching, delivering superior results with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing IID models achieve good results but are parameter-heavy, making them unsuitable for resource-constrained scenarios and integration with other models.

Method: The method involves FlowIID, a system designed with a VAE-guided latent space and a flow-matching module, enabling efficient and stable decomposition into albedo and shading components in one inference step.

Result: FlowIID achieves competitive and superior results compared to existing IID models across multiple benchmarks.

Conclusion: FlowIID offers a parameter-efficient and high-performing solution for IID, making it suitable for real-time and resource-limited applications.

Abstract: Intrinsic Image Decomposition (IID) separates an image into albedo and shading components. It is a core step in many real-world applications, such as relighting and material editing. Existing IID models achieve good results, but often use a large number of parameters. This makes them costly to combine with other models in real-world settings. To address this problem, we propose a flow matching-based solution. For this, we design a novel architecture, FlowIID, based on latent flow matching. FlowIID combines a VAE-guided latent space with a flow matching module, enabling a stable decomposition of albedo and shading. FlowIID is not only parameter-efficient, but also produces results in a single inference step. Despite its compact design, FlowIID delivers competitive and superior results compared to existing models across various benchmarks. This makes it well-suited for deployment in resource-constrained and real-time vision applications.

</details>


### [392] [Turbo-GoDec: Exploiting the Cluster Sparsity Prior for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2601.12337)
*Jiahui Sheng,Xiaorun Li,Shuhan Chen*

Main category: cs.CV

TL;DR: This paper introduces a novel hyperspectral anomaly detection method called Turbo-GoDec, integrating anomaly spatial cluster sparsity into the GoDec framework.


<details>
  <summary>Details</summary>
Motivation: Existing hyperspectral anomaly detection methods focus on anomalies' sparsity but lack emphasis on their spatial distribution characteristics. This paper aims to address the gap by utilizing the concept of cluster sparsity.

Method: Turbo-GoDec incorporates cluster sparsity of anomalies modeled as a Markov random field within the GoDec algorithm, leveraging message passing on a factor graph to compute marginal probabilities. High-probability anomaly locations are treated as the sparse component.

Result: Experiments on three real hyperspectral datasets show Turbo-GoDec's superior detection performance for small-size anomalies compared to GoDec and state-of-the-art methods.

Conclusion: Turbo-GoDec effectively leverages spatial cluster sparsity for improved hyperspectral anomaly detection, demonstrating significant advantages in detecting small anomalies.

Abstract: As a key task in hyperspectral image processing, hyperspectral anomaly detection has garnered significant attention and undergone extensive research. Existing methods primarily relt on two prior assumption: low-rank background and sparse anomaly, along with additional spatial assumptions of the background. However, most methods only utilize the sparsity prior assumption for anomalies and rarely expand on this hypothesis. From observations of hyperspectral images, we find that anomalous pixels exhibit certain spatial distribution characteristics: they often manifest as small, clustered groups in space, which we refer to as cluster sparsity of anomalies. Then, we combined the cluster sparsity prior with the classical GoDec algorithm, incorporating the cluster sparsity prior into the S-step of GoDec. This resulted in a new hyperspectral anomaly detection method, which we called Turbo-GoDec. In this approach, we modeled the cluster sparsity prior of anomalies using a Markov random field and computed the marginal probabilities of anomalies through message passing on a factor graph. Locations with high anomalous probabilities were treated as the sparse component in the Turbo-GoDec. Experiments are conducted on three real hyperspectral image (HSI) datasets which demonstrate the superior performance of the proposed Turbo-GoDec method in detecting small-size anomalies comparing with the vanilla GoDec (LSMAD) and state-of-the-art anomaly detection methods. The code is available at https://github.com/jiahuisheng/Turbo-GoDec.

</details>


### [393] [MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents](https://arxiv.org/abs/2601.12346)
*Peizhou Huang,Zixuan Zhong,Zhongwei Wan,Donghao Zhou,Samiul Alam,Xin Wang,Zexin Li,Zhihao Dou,Li Zhu,Jing Xiong,Chaofan Tao,Yan Xu,Dimitrios Dimitriadis,Tuo Zhang,Mi Zhang*

Main category: cs.CV

TL;DR: The paper introduces the MMDeepResearch-Bench (MMDR-Bench) benchmark for evaluating multimodal understanding and citation-rich report generation, alongside proposing interpretive evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on text-only settings or short-form multimodal QA, failing to assess end-to-end multimodal report generation with explicit evidence use, necessitating new tools to evaluate and measure this capability.

Method: The paper presents a new benchmark (MMDR-Bench) covering 140 tasks across 21 domains, incorporating image-text bundles. It also proposes three distinct evaluation methods (FLAE, TRACE, MOSAIC) for detailed analysis of report generation and integrity.

Result: Testing on 25 state-of-the-art models shows trade-offs between generation quality, citation discipline, and multimodal grounding. It reveals challenges like the key issue of maintaining multimodal integrity.

Conclusion: The study emphasizes the limitations of current deep research agents in ensuring faithful evidence use and calls for better approaches to address multimodal grounding and integrity challenges in report creation.

Abstract: Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.

</details>


### [394] [SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence](https://arxiv.org/abs/2601.12357)
*Hailing Jin,Huiying Li*

Main category: cs.CV

TL;DR: The paper introduces SimpleMatch, a semantic correspondence framework that reduces computational overhead by supporting low-resolution inputs, achieving competitive performance using innovative techniques.


<details>
  <summary>Details</summary>
Motivation: Current semantic correspondence methods depend heavily on high-resolution inputs for optimal performance, leading to significant computational costs. The authors aim to solve the issue of irreversible fusion of distinct keypoint features caused by downsampling operations.

Method: SimpleMatch uses a lightweight upsample decoder to progressively restore spatial detail and a multi-scale supervised loss for discriminative features across spatial scales. Sparse matching and window-based localization are introduced to reduce memory usage during training.

Result: SimpleMatch achieves state-of-the-art performance on the SPair-71k benchmark, with 84.1% PCK@0.1, while using significantly lower resolution inputs (252x252) and reducing memory usage by 51%.

Conclusion: SimpleMatch demonstrates that semantic correspondence can deliver strong performance at low resolutions, offering a practical and computationally efficient solution for future research.

Abstract: Recent advances in semantic correspondence have been largely driven by the use of pre-trained large-scale models. However, a limitation of these approaches is their dependence on high-resolution input images to achieve optimal performance, which results in considerable computational overhead. In this work, we address a fundamental limitation in current methods: the irreversible fusion of adjacent keypoint features caused by deep downsampling operations. This issue is triggered when semantically distinct keypoints fall within the same downsampled receptive field (e.g., 16x16 patches). To address this issue, we present SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. We propose a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, and a multi-scale supervised loss that ensures the upsampled features retain discriminative features across different spatial scales. In addition, we introduce sparse matching and window-based localization to optimize training memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller than current SOTA methods), SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a practical and efficient baseline for future research in semantic correspondence. Code is available at: https://github.com/hailong23-jin/SimpleMatch.

</details>


### [395] [DepthCropSeg++: Scaling a Crop Segmentation Foundation Model With Depth-Labeled Data](https://arxiv.org/abs/2601.12366)
*Jiafei Zhang,Songliang Cao,Binghui Xu,Yanan Li,Weiwei Jia,Tingting Wu,Hao Lu,Weijuan Hu,Zhiguo Han*

Main category: cs.CV

TL;DR: DepthCropSeg++ is a robust and advanced crop segmentation foundation model designed for use in open in-field conditions across multiple crop species. It achieves superior results compared to existing approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current crop segmentation models, which rely on limited datasets and underperform in various crop types and environmental conditions. The aim is to create a generalized crop segmentation model that works effectively in challenging real-world scenarios.

Method: DepthCropSeg++ builds upon DepthCropSeg by scaling up the dataset to over 28,000 images spanning 30+ species and 15 environments. It incorporates enhancements such as the ViT-Adapter architecture, dynamic upsampling for detailed segmentation, and a two-stage self-training pipeline.

Result: The model achieves 93.11% mIoU on testing, surpassing supervised baselines and general-purpose segmentation models. It also demonstrates strong performance in difficult conditions like nighttime environments, high-density canopies, and unseen crop varieties.

Conclusion: DepthCropSeg++ sets a new state of the art in crop segmentation with its robust cross-species and cross-scene generalization capabilities, proving its utility for modern agricultural applications.

Abstract: DepthCropSeg++: a foundation model for crop segmentation, capable of segmenting different crop species under open in-field environment. Crop segmentation is a fundamental task for modern agriculture, which closely relates to many downstream tasks such as plant phenotyping, density estimation, and weed control. In the era of foundation models, a number of generic large language and vision models have been developed. These models have demonstrated remarkable real world generalization due to significant model capacity and largescale datasets. However, current crop segmentation models mostly learn from limited data due to expensive pixel-level labelling cost, often performing well only under specific crop types or controlled environment. In this work, we follow the vein of our previous work DepthCropSeg, an almost unsupervised approach to crop segmentation, to scale up a cross-species and crossscene crop segmentation dataset, with 28,406 images across 30+ species and 15 environmental conditions. We also build upon a state-of-the-art semantic segmentation architecture ViT-Adapter architecture, enhance it with dynamic upsampling for improved detail awareness, and train the model with a two-stage selftraining pipeline. To systematically validate model performance, we conduct comprehensive experiments to justify the effectiveness and generalization capabilities across multiple crop datasets. Results demonstrate that DepthCropSeg++ achieves 93.11% mIoU on a comprehensive testing set, outperforming both supervised baselines and general-purpose vision foundation models like Segmentation Anything Model (SAM) by significant margins (+0.36% and +48.57% respectively). The model particularly excels in challenging scenarios including night-time environment (86.90% mIoU), high-density canopies (90.09% mIoU), and unseen crop varieties (90.09% mIoU), indicating a new state of the art for crop segmentation.

</details>


### [396] [Utilizing the Score of Data Distribution for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2601.12379)
*Jiahui Sheng,Yidan Shi,Shu Xiang,Xiaorun Li,Shuhan Chen*

Main category: cs.CV

TL;DR: The paper introduces a hyperspectral anomaly detection (ScoreAD) method utilizing a score-based generative model (SGM), tested on four datasets, and claims effectiveness.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral images (HSIs) contain abundant spectral information determined by a small number of factors like composition and illumination, often satisfying the manifold hypothesis. Detecting anomalies on these manifolds is a challenging problem.

Method: The method trains a score-based generative model (SGM) on HSI spectra. At testing, spectra are perturbed and scores are obtained to distinguish between background spectra on manifolds and outlier anomalous spectra.

Result: Experiments conducted on four hyperspectral datasets validate the effectiveness of the proposed ScoreAD method.

Conclusion: The proposed ScoreAD method successfully leverages manifold distributions and generative modeling for hyperspectral anomaly detection and demonstrates effectiveness through experiments.

Abstract: Hyperspectral images (HSIs) are a type of image that contains abundant spectral information. As a type of real-world data, the high-dimensional spectra in hyperspectral images are actually determined by only a few factors, such as chemical composition and illumination. Thus, spectra in hyperspectral images are highly likely to satisfy the manifold hypothesis. Based on the hyperspectral manifold hypothesis, we propose a novel hyperspectral anomaly detection method (named ScoreAD) that leverages the time-dependent gradient field of the data distribution (i.e., the score), as learned by a score-based generative model (SGM). Our method first trains the SGM on the entire set of spectra from the hyperspectral image. At test time, each spectrum is passed through a perturbation kernel, and the resulting perturbed spectrum is fed into the trained SGM to obtain the estimated score. The manifold hypothesis of HSIs posits that background spectra reside on one or more low-dimensional manifolds. Conversely, anomalous spectra, owing to their unique spectral signatures, are considered outliers that do not conform to the background manifold. Based on this fundamental discrepancy in their manifold distributions, we leverage a generative SGM to achieve hyperspectral anomaly detection. Experiments on the four hyperspectral datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/jiahuisheng/ScoreAD.

</details>


### [397] [A Hierarchical Benchmark of Foundation Models for Dermatology](https://arxiv.org/abs/2601.12382)
*Furkan Yuceyalcin,Abdurrahim Yilmaz,Burak Temelkuran*

Main category: cs.CV

TL;DR: The study evaluates ten foundation models for hierarchical skin lesion classification using the DERM12345 dataset, revealing trade-offs in model performance depending on classification granularity.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of oversimplified benchmarks in dermatology that hinder fine-grained differential diagnoses critical for clinical workflows.

Method: The approach involved generating frozen embeddings from ten foundation models, training lightweight adapters, and hierarchical evaluation across four levels of clinical granularity using the DERM12345 dataset.

Result: MedImageInsights excelled in broader binary malignancy detection, while MedSigLip and dermatology-specific models performed better at fine-grained subtype classification.

Conclusion: General medical models are superior for coarse screening tasks, but specialized models are needed for fine-grained diagnostic distinctions.

Abstract: Foundation models have transformed medical image analysis by providing robust feature representations that reduce the need for large-scale task-specific training. However, current benchmarks in dermatology often reduce the complex diagnostic taxonomy to flat, binary classification tasks, such as distinguishing melanoma from benign nevi. This oversimplification obscures a model's ability to perform fine-grained differential diagnoses, which is critical for clinical workflow integration. This study evaluates the utility of embeddings derived from ten foundation models, spanning general computer vision, general medical imaging, and dermatology-specific domains, for hierarchical skin lesion classification. Using the DERM12345 dataset, which comprises 40 lesion subclasses, we calculated frozen embeddings and trained lightweight adapter models using a five-fold cross-validation. We introduce a hierarchical evaluation framework that assesses performance across four levels of clinical granularity: 40 Subclasses, 15 Main Classes, 2 and 4 Superclasses, and Binary Malignancy. Our results reveal a "granularity gap" in model capabilities: MedImageInsights achieved the strongest overall performance (97.52% weighted F1-Score on Binary Malignancy detection) but declined to 65.50% on fine-grained 40-class subtype classification. Conversely, MedSigLip (69.79%) and dermatology-specific models (Derm Foundation and MONET) excelled at fine-grained 40-class subtype discrimination while achieving lower overall performance than MedImageInsights on broader classification tasks. Our findings suggest that while general medical foundation models are highly effective for high-level screening, specialized modeling strategies are necessary for the granular distinctions required in diagnostic support systems.

</details>


### [398] [Class-Partitioned VQ-VAE and Latent Flow Matching for Point Cloud Scene Generation](https://arxiv.org/abs/2601.12391)
*Dasith de Silva Edirimuni,Ajmal Saeed Mian*

Main category: cs.CV

TL;DR: The paper introduces CPVQ-VAE, a new model for decoding object latents into point clouds without relying on an external object database, achieving significant error reductions in scene representation.


<details>
  <summary>Details</summary>
Motivation: Existing 3D scene generation methods struggle to decode point cloud objects from diffusion-based latent spaces in diverse, multi-class object scenes.

Method: The authors propose a Class-Partitioned Vector Quantized VAE (CPVQ-VAE) with a class-aware codebook and running average updates to prevent codebook collapse. Object features are generated via a Latent-space Flow Matching Model (LFMM).

Result: The method significantly reduces Chamfer and Point2Mesh representation errors for complex living room scenes by 70.4% and 72.3%, achieving accurate point cloud generation.

Conclusion: CPVQ-VAE enables pure point cloud generation from latents with improved accuracy over previous methods, bypassing the need for external object databases.

Abstract: Most 3D scene generation methods are limited to only generating object bounding box parameters while newer diffusion methods also generate class labels and latent features. Using object size or latent feature, they then retrieve objects from a predefined database. For complex scenes of varied, multi-categorical objects, diffusion-based latents cannot be effectively decoded by current autoencoders into the correct point cloud objects which agree with target classes. We introduce a Class-Partitioned Vector Quantized Variational Autoencoder (CPVQ-VAE) that is trained to effectively decode object latent features, by employing a pioneering $\textit{class-partitioned codebook}$ where codevectors are labeled by class. To address the problem of $\textit{codebook collapse}$, we propose a $\textit{class-aware}$ running average update which reinitializes dead codevectors within each partition. During inference, object features and class labels, both generated by a Latent-space Flow Matching Model (LFMM) designed specifically for scene generation, are consumed by the CPVQ-VAE. The CPVQ-VAE's class-aware inverse look-up then maps generated latents to codebook entries that are decoded to class-specific point cloud shapes. Thereby, we achieve pure point cloud generation without relying on an external objects database for retrieval. Extensive experiments reveal that our method reliably recovers plausible point cloud scenes, with up to 70.4% and 72.3% reduction in Chamfer and Point2Mesh errors on complex living room scenes.

</details>


### [399] [Weaknesses of Facial Emotion Recognition Systems](https://arxiv.org/abs/2601.12402)
*Aleksandra Jamróz,Patrycja Wysocka,Piotr Garbat*

Main category: cs.CV

TL;DR: The paper focuses on emotion detection from faces using machine learning, reviewing existing solutions and datasets and conducting experiments to analyze their performance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in creating effective emotion detection models for better human-computer interaction.

Method: The authors reviewed existing solutions, selected three neural network models and datasets, and conducted experiments, including cross-dataset testing, to evaluate performance.

Result: Identified weaknesses such as dataset variations, uneven recognition difficulty for certain emotions, and challenges in distinguishing similar emotions.

Conclusion: The study highlights the need for improving emotion detection models by addressing dataset inconsistencies and fine-tuning for closely related emotions.

Abstract: Emotion detection from faces is one of the machine learning problems needed for human-computer interaction. The variety of methods used is enormous, which motivated an in-depth review of articles and scientific studies. Three of the most interesting and best solutions are selected, followed by the selection of three datasets that stood out for the diversity and number of images in them. The selected neural networks are trained, and then a series of experiments are performed to compare their performance, including testing on different datasets than a model was trained on. This reveals weaknesses in existing solutions, including differences between datasets, unequal levels of difficulty in recognizing certain emotions and the challenges in differentiating between closely related emotions.

</details>


### [400] [HOT-POT: Optimal Transport for Sparse Stereo Matching](https://arxiv.org/abs/2601.12423)
*Antonin Clerc,Michael Quellmalz,Moritz Piening,Philipp Flotho,Gregor Kornhardt,Gabriele Steidl*

Main category: cs.CV

TL;DR: This paper introduces an unsupervised method for sparse stereo matching using optimal transport (OT) and line constraints to tackle challenges like occlusions and motion in stereo vision, especially for facial landmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of stereo vision caused by occlusions, motion, and camera distortions, particularly in facial landmark matching where sparse features lead to parameter sensitivity.

Method: The authors use line constraints from the camera geometry and optimal transport viewpoints, with epipolar and 3D ray distances as cost functions, to solve assignment problems efficiently for sparse stereo matching.

Result: Efficient algorithms for feature and object matching were proposed, demonstrating effectiveness in facial analysis with numerical experiments.

Conclusion: The methods developed enable unsupervised sparse matching and object matching, offering robustness and applicability in tasks like facial analysis.

Abstract: Stereo vision between images faces a range of challenges, including occlusions, motion, and camera distortions, across applications in autonomous driving, robotics, and face analysis. Due to parameter sensitivity, further complications arise for stereo matching with sparse features, such as facial landmarks. To overcome this ill-posedness and enable unsupervised sparse matching, we consider line constraints of the camera geometry from an optimal transport (OT) viewpoint. Formulating camera-projected points as (half)lines, we propose the use of the classical epipolar distance as well as a 3D ray distance to quantify matching quality. Employing these distances as a cost function of a (partial) OT problem, we arrive at efficiently solvable assignment problems. Moreover, we extend our approach to unsupervised object matching by formulating it as a hierarchical OT problem. The resulting algorithms allow for efficient feature and object matching, as demonstrated in our numerical experiments. Here, we focus on applications in facial analysis, where we aim to match distinct landmarking conventions.

</details>


### [401] [SkeFi: Cross-Modal Knowledge Transfer for Wireless Skeleton-Based Action Recognition](https://arxiv.org/abs/2601.12432)
*Shunyu Huang,Yunjiao Zhou,Jianfei Yang*

Main category: cs.CV

TL;DR: This paper introduces SkeFi, a system using cross-modal knowledge transfer to leverage noisy signals from wireless sensors like mmWave and LiDAR for skeleton-based action recognition.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of RGB camera-based skeleton estimation, such as poor performance in low-light conditions and privacy concerns, and explore wireless sensors (LiDAR and mmWave) as a feasible alternative.

Method: SkeFi employs cross-modal knowledge transfer from RGB modality to wireless sensors, introduces Temporal Correlation Adaptive Graph Convolution (TC-AGC) to handle noisy data, and enhances temporal modeling with dual temporal convolution.

Result: SkeFi achieves state-of-the-art performance in action recognition using mmWave and LiDAR sensors, effectively addressing challenges of noise and frame inconsecutiveness.

Conclusion: Wireless sensors can be a viable alternative to RGB-based systems for action recognition by leveraging methods like SkeFi, enhancing pose and action estimation despite inherent noise or data insufficiencies.

Abstract: Skeleton-based action recognition leverages human pose keypoints to categorize human actions, which shows superior generalization and interoperability compared to regular end-to-end action recognition. Existing solutions use RGB cameras to annotate skeletal keypoints, but their performance declines in dark environments and raises privacy concerns, limiting their use in smart homes and hospitals. This paper explores non-invasive wireless sensors, i.e., LiDAR and mmWave, to mitigate these challenges as a feasible alternative. Two problems are addressed: (1) insufficient data on wireless sensor modality to train an accurate skeleton estimation model, and (2) skeletal keypoints derived from wireless sensors are noisier than RGB, causing great difficulties for subsequent action recognition models. Our work, SkeFi, overcomes these gaps through a novel cross-modal knowledge transfer method acquired from the data-rich RGB modality. We propose the enhanced Temporal Correlation Adaptive Graph Convolution (TC-AGC) with frame interactive enhancement to overcome the noise from missing or inconsecutive frames. Additionally, our research underscores the effectiveness of enhancing multiscale temporal modeling through dual temporal convolution. By integrating TC-AGC with temporal modeling for cross-modal transfer, our framework can extract accurate poses and actions from noisy wireless sensors. Experiments demonstrate that SkeFi realizes state-of-the-art performances on mmWave and LiDAR. The code is available at https://github.com/Huang0035/Skefi.

</details>


### [402] [Adversarial Defense in Vision-Language Models: An Overview](https://arxiv.org/abs/2601.12443)
*Xiaowei Fu,Lei Zhang*

Main category: cs.CV

TL;DR: This paper surveys adversarial defense strategies for Vision Language Models (VLMs), discussing three main paradigms: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address concerns about the vulnerability of Vision Language Models (e.g., CLIP) to adversarial attacks that may compromise model performance in cross-modal tasks.

Method: The authors review and categorize the existing adversarial defense strategies into three paradigms: Training-time Defense (adversarial fine-tuning), Test-time Adaptation Defense (parameter updates during inference), and Training-free Defense (alteration of adversarial inputs without retraining).

Result: The survey highlights the strengths and limitations of each defense paradigm and provides insights into their applications and challenges in practical scenarios.

Conclusion: The paper concludes with ongoing challenges in improving the robustness of VLMs, emphasizing the need for effective and generalized defense strategies.

Abstract: The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.

</details>


### [403] [Large-scale EM Benchmark for Multi-Organelle Instance Segmentation in the Wild](https://arxiv.org/abs/2601.12464)
*Yanrui Lu,Danyang Chen,Haowen Xiao,Jiarui Zhu,Fukang Ge,Binqian Zou,Jiali Guan,Jiayin Liang,Yuting Wang,Ziqian Guan,Xiangcheng Bao,Jinhao Bi,Lin Gu,Jun He,Yingying Zhu*

Main category: cs.CV

TL;DR: The paper introduces a large-scale multi-source benchmark dataset with over 100,000 2D EM images for organelle segmentation, finding limitations in current models for handling heterogeneity and global morphological structures.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for organelle segmentation in electron microscopy (EM) are limited by small, curated datasets and patch-based approaches, failing to represent real-world heterogeneity and spatial complexity.

Method: A large-scale benchmark dataset was developed using a connectivity-aware Label Propagation Algorithm (3D LPA) with expert refinement, and state-of-the-art models like U-Net, SAM variants, and Mask2Former were benchmarked.

Result: Results revealed that existing models struggle to generalize across heterogeneous data and perform poorly on organelles with distributed morphologies, highlighting a mismatch between local-context models and the need for modeling long-range continuity.

Conclusion: The study emphasizes the inadequacy of current models in addressing large-scale variability and introduces a novel benchmark to aid future research, with plans to publicly release the dataset and labeling tool.

Abstract: Accurate instance-level segmentation of organelles in electron microscopy (EM) is critical for quantitative analysis of subcellular morphology and inter-organelle interactions. However, current benchmarks, based on small, curated datasets, fail to capture the inherent heterogeneity and large spatial context of in-the-wild EM data, imposing fundamental limitations on current patch-based methods. To address these limitations, we developed a large-scale, multi-source benchmark for multi-organelle instance segmentation, comprising over 100,000 2D EM images across variety cell types and five organelle classes that capture real-world variability. Dataset annotations were generated by our designed connectivity-aware Label Propagation Algorithm (3D LPA) with expert refinement. We further benchmarked several state-of-the-art models, including U-Net, SAM variants, and Mask2Former. Our results show several limitations: current models struggle to generalize across heterogeneous EM data and perform poorly on organelles with global, distributed morphologies (e.g., Endoplasmic Reticulum). These findings underscore the fundamental mismatch between local-context models and the challenge of modeling long-range structural continuity in the presence of real-world variability. The benchmark dataset and labeling tool will be publicly released soon.

</details>


### [404] [DCAC: Dynamic Class-Aware Cache Creates Stronger Out-of-Distribution Detectors](https://arxiv.org/abs/2601.12468)
*Yanqi Wu,Qichao Chen,Runhe Lai,Xinhua Lu,Jia-Xin Zhuang,Zhilin Zhao,Wei-Shi Zheng,Ruixuan Wang*

Main category: cs.CV

TL;DR: DCAC is a test-time calibration module designed for OOD detection, improving prediction reliability by leveraging class-specific caches of high-entropy samples.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of overconfident predictions on OOD samples by utilizing an insight that OOD samples predicted as the same class are visually similar to each other.

Method: DCAC uses a class-specific cache system to collect high-entropy samples, calibrates raw predictions, and implements a lightweight two-layer module for prediction refinement. This approach is training-free and integrates seamlessly with existing OOD detection methods.

Result: Experiments show significant improvements in OOD detection methods, such as reducing FPR95 by 6.55% on the ImageNet benchmark.

Conclusion: DCAC enhances OOD detection reliability with minimal computational overhead, offering a practical enhancement to various existing detection methods.

Abstract: Out-of-distribution (OOD) detection remains a fundamental challenge for deep neural networks, particularly due to overconfident predictions on unseen OOD samples during testing. We reveal a key insight: OOD samples predicted as the same class, or given high probabilities for it, are visually more similar to each other than to the true in-distribution (ID) samples. Motivated by this class-specific observation, we propose DCAC (Dynamic Class-Aware Cache), a training-free, test-time calibration module that maintains separate caches for each ID class to collect high-entropy samples and calibrate the raw predictions of input samples. DCAC leverages cached visual features and predicted probabilities through a lightweight two-layer module to mitigate overconfident predictions on OOD samples. This module can be seamlessly integrated with various existing OOD detection methods across both unimodal and vision-language models while introducing minimal computational overhead. Extensive experiments on multiple OOD benchmarks demonstrate that DCAC significantly enhances existing methods, achieving substantial improvements, i.e., reducing FPR95 by 6.55% when integrated with ASH-S on ImageNet OOD benchmark.

</details>


### [405] [NeuralFur: Animal Fur Reconstruction From Multi-View Images](https://arxiv.org/abs/2601.12481)
*Vanessa Sklyarova,Berna Kabadayi,Anastasios Yiannakidis,Giorgio Becherini,Michael J. Black,Justus Thies*

Main category: cs.CV

TL;DR: This paper introduces a novel method for reconstructing 3D animal fur geometry from multi-view images using strand-based representation and vision language models (VLMs).


<details>
  <summary>Details</summary>
Motivation: The lack of datasets and challenges like fine-scale details, self-occlusions, and view-dependent appearances make animal fur geometry reconstruction difficult, unlike human hairstyle reconstruction.

Method: The method involves first reconstructing a coarse geometry using traditional stereo techniques, using vision language models to infer fur structure details, and then growing fur strands on the geometry. Loss functions ensure accuracy through geometric and photometric alignment, while VLMs handle growth direction and gravity-related constraints.

Result: The approach generalizes well across different animals with varied fur types, effectively reconstructing detailed 3D fur structures.

Conclusion: By leveraging vision language models in 3D reconstruction, this method presents a significant step forward in reconstructing realistic animal fur geometry from multi-view images.

Abstract: Reconstructing realistic animal fur geometry from images is a challenging task due to the fine-scale details, self-occlusion, and view-dependent appearance of fur. In contrast to human hairstyle reconstruction, there are also no datasets that can be leveraged to learn a fur prior for different animals. In this work, we present a first multi-view-based method for high-fidelity 3D fur modeling of animals using a strand-based representation, leveraging the general knowledge of a vision language model. Given multi-view RGB images, we first reconstruct a coarse surface geometry using traditional multi-view stereo techniques. We then use a vision language model (VLM) system to retrieve information about the realistic length structure of the fur for each part of the body. We use this knowledge to construct the animal's furless geometry and grow strands atop it. The fur reconstruction is supervised with both geometric and photometric losses computed from multi-view images. To mitigate orientation ambiguities stemming from the Gabor filters that are applied to the input images, we additionally utilize the VLM to guide the strands' growth direction and their relation to the gravity vector that we incorporate as a loss. With this new schema of using a VLM to guide 3D reconstruction from multi-view inputs, we show generalization across a variety of animals with different fur types. For additional results and code, please refer to https://neuralfur.is.tue.mpg.de.

</details>


### [406] [Histopath-C: Towards Realistic Domain Shifts for Histopathology Vision-Language Adaptation](https://arxiv.org/abs/2601.12493)
*Mehrdad Noori,Gustavo Adolfo Vargas Hakim,David Osowiechi,Fereshteh Shakeri,Ali Bahri,Moslem Yazdanpanah,Sahar Dastani,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: A new benchmark, Histopath-C, and an adaptation strategy, LATTE, are proposed to tackle the domain shifts in medical vision-language models for histopathology, improving robustness to image and input variations.


<details>
  <summary>Details</summary>
Motivation: Vision-language models face performance degradation in medical histopathology due to domain shifts like staining or image noise.

Method: They introduce Histopath-C, a benchmark with synthetic corruptions, and propose LATTE, a low-rank adaptation methodology using multiple text templates for test-time adaptation.

Result: Histopath-C and LATTE demonstrate improved performance on histopathology datasets compared to existing state-of-the-art methods.

Conclusion: The methods substantially enhance the robustness and adaptability of vision-language models in histopathological applications.

Abstract: Medical Vision-language models (VLMs) have shown remarkable performances in various medical imaging domains such as histo\-pathology by leveraging pre-trained, contrastive models that exploit visual and textual information. However, histopathology images may exhibit severe domain shifts, such as staining, contamination, blurring, and noise, which may severely degrade the VLM's downstream performance. In this work, we introduce Histopath-C, a new benchmark with realistic synthetic corruptions designed to mimic real-world distribution shifts observed in digital histopathology. Our framework dynamically applies corruptions to any available dataset and evaluates Test-Time Adaptation (TTA) mechanisms on the fly. We then propose LATTE, a transductive, low-rank adaptation strategy that exploits multiple text templates, mitigating the sensitivity of histopathology VLMs to diverse text inputs. Our approach outperforms state-of-the-art TTA methods originally designed for natural images across a breadth of histopathology datasets, demonstrating the effectiveness of our proposed design for robust adaptation in histopathology images. Code and data are available at https://github.com/Mehrdad-Noori/Histopath-C.

</details>


### [407] [Video Individual Counting and Tracking from Moving Drones: A Benchmark and Methods](https://arxiv.org/abs/2601.12500)
*Yaowu Fan,Jia Wan,Tao Han,Andy J. Ma,Antoni B. Chan*

Main category: cs.CV

TL;DR: This paper introduces a new dataset, MovingDroneCrowd++, for crowd analysis using drone footage and proposes novel methods (GD3A and DVTrack) for significant improvements in counting and tracking pedestrians.


<details>
  <summary>Details</summary>
Motivation: Existing crowd analysis methods rely heavily on fixed-camera datasets, providing limited spatial coverage for analyzing large-scale and dense crowds, thus requiring a more flexible approach.

Method: The study uses moving drones to capture crowd footage, complemented by new methods (GD3A for density map-based counting and DVTrack for descriptor-based pedestrian tracking) to analyze dense crowds in dynamic scenarios.

Result: The proposed methods significantly reduced counting errors by 47.4% and improved tracking performance by 39.2% compared to existing methods, showing superior results for dense and complex scenarios.

Conclusion: The introduced dataset and methods effectively address the challenges in crowd counting and tracking with moving drones, offering a robust solution for large-scale and complex crowd analysis.

Abstract: Counting and tracking dense crowds in large-scale scenes is highly challenging, yet existing methods mainly rely on datasets captured by fixed cameras, which provide limited spatial coverage and are inadequate for large-scale dense crowd analysis. To address this limitation, we propose a flexible solution using moving drones to capture videos and perform video-level crowd counting and tracking of unique pedestrians across entire scenes. We introduce MovingDroneCrowd++, the largest video-level dataset for dense crowd counting and tracking captured by moving drones, covering diverse and complex conditions with varying flight altitudes, camera angles, and illumination. Existing methods fail to achieve satisfactory performance on this dataset. To this end, we propose GD3A (Global Density Map Decomposition via Descriptor Association), a density map-based video individual counting method that avoids explicit localization. GD3A establishes pixel-level correspondences between pedestrian descriptors across consecutive frames via optimal transport with an adaptive dustbin score, enabling the decomposition of global density maps into shared, inflow, and outflow components. Building on this framework, we further introduce DVTrack, which converts descriptor-level matching into instance-level associations through a descriptor voting mechanism for pedestrian tracking. Experimental results show that our methods significantly outperform existing approaches under dense crowds and complex motion, reducing counting error by 47.4 percent and improving tracking performance by 39.2 percent.

</details>


### [408] [SDCoNet: Saliency-Driven Multi-Task Collaborative Network for Remote Sensing Object Detection](https://arxiv.org/abs/2601.12507)
*Ruo Qi,Linhui Dai,Yusong Qin,Chaolei Yang,Yanshan Li*

Main category: cs.CV

TL;DR: The paper introduces SDCoNet, a multi-task network for remote sensing image detection combining super-resolution and detection using shared features for improved performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in remote sensing object detection due to complex backgrounds, weak signals, small object scales, and low-quality imaging necessitate improvement beyond standard SR-detection pipelines that lack task interactivity.

Method: The proposed Saliency-Driven Collaborative Network (SDCoNet) integrates an encoder using swin transformers for feature sharing, a saliency prediction module for weak object focus, and a gradient routing strategy to harmonize optimization.

Result: SDCoNet outperforms mainstream methods in detecting small objects in low-quality remote sensing images, achieving competitive computational efficiency across NWPU VHR-10-Split, DOTAv1.5-Split, and HRSSD-Split datasets.

Conclusion: The proposed SDCoNet enhances detection efficiency and accuracy through task coupling and well-designed modules for handling challenges like weak signals and cluttered backgrounds.

Abstract: In remote sensing images, complex backgrounds, weak object signals, and small object scales make accurate detection particularly challenging, especially under low-quality imaging conditions. A common strategy is to integrate single-image super-resolution (SR) before detection; however, such serial pipelines often suffer from misaligned optimization objectives, feature redundancy, and a lack of effective interaction between SR and detection. To address these issues, we propose a Saliency-Driven multi-task Collaborative Network (SDCoNet) that couples SR and detection through implicit feature sharing while preserving task specificity. SDCoNet employs the swin transformer-based shared encoder, where hierarchical window-shifted self-attention supports cross-task feature collaboration and adaptively balances the trade-off between texture refinement and semantic representation. In addition, a multi-scale saliency prediction module produces importance scores to select key tokens, enabling focused attention on weak object regions, suppression of background clutter, and suppression of adverse features introduced by multi-task coupling. Furthermore, a gradient routing strategy is introduced to mitigate optimization conflicts. It first stabilizes detection semantics and subsequently routes SR gradients along a detection-oriented direction, enabling the framework to guide the SR branch to generate high-frequency details that are explicitly beneficial for detection. Experiments on public datasets, including NWPU VHR-10-Split, DOTAv1.5-Split, and HRSSD-Split, demonstrate that the proposed method, while maintaining competitive computational efficiency, significantly outperforms existing mainstream algorithms in small object detection on low-quality remote sensing images. Our code is available at https://github.com/qiruo-ya/SDCoNet.

</details>


### [409] [Fine-Tuning Cycle-GAN for Domain Adaptation of MRI Images](https://arxiv.org/abs/2601.12512)
*Mohd Usama,Belal Ahmad,Faleh Menawer R Althiyabi*

Main category: cs.CV

TL;DR: This paper addresses domain shifts in MRI scans using a Cycle-GAN-based model for unsupervised medical image domain adaptation, ensuring improved performance without labeled data.


<details>
  <summary>Details</summary>
Motivation: To resolve performance degradation of deep learning models caused by domain shifts in MRI scans due to variations in hardware, protocols, and acquisition parameters.

Method: A Cycle-GAN-based approach was employed, using bidirectional mappings and incorporating content and disparity loss to ensure image adaptation while preserving anatomical content.

Result: Experiments showed that the model effectively handled domain adaptation in MRI datasets, improving diagnostic accuracy and reducing variability without the need for labeled data.

Conclusion: The approach enhances the precision and consistency of medical image analysis, proving its potential in addressing domain shifts in healthcare imaging.

Abstract: Magnetic Resonance Imaging (MRI) scans acquired from different scanners or institutions often suffer from domain shifts owing to variations in hardware, protocols, and acquisition parameters. This discrepancy degrades the performance of deep learning models trained on source domain data when applied to target domain images. In this study, we propose a Cycle-GAN-based model for unsupervised medical-image domain adaptation. Leveraging CycleGANs, our model learns bidirectional mappings between the source and target domains without paired training data, preserving the anatomical content of the images. By leveraging Cycle-GAN capabilities with content and disparity loss for adaptation tasks, we ensured image-domain adaptation while maintaining image integrity. Several experiments on MRI datasets demonstrated the efficacy of our model in bidirectional domain adaptation without labelled data. Furthermore, research offers promising avenues for improving the diagnostic accuracy of healthcare. The statistical results confirm that our approach improves model performance and reduces domain-related variability, thus contributing to more precise and consistent medical image analysis.

</details>


### [410] [Deep Feature Deformation Weights](https://arxiv.org/abs/2601.12527)
*Richard Liu,Itai Lang,Rana Hanocka*

Main category: cs.CV

TL;DR: This paper introduces a novel framework combining data-driven semantic editing and traditional precise, real-time mesh deformation, enabling fast, intuitive, and meaningful 3D shape modifications.


<details>
  <summary>Details</summary>
Motivation: Traditional handle-based mesh deformation methods are precise but unintuitive for users due to required prior knowledge of handle placement. On the other hand, data-driven methods are semantic but slow and imprecise, emphasizing the need for a method that integrates the semantics of data-driven approaches with the control and speed of classic methods.

Method: The authors propose a deep feature proximity technique that calculates smooth and semantic deformation weights. These weights are computed in real-time without extensive optimization. The barycentric feature distillation pipeline minimizes costs by efficiently using shape render signals, allowing high-resolution meshes to be processed quickly.

Result: The proposed method achieves real-time computation of deformation weights for high-resolution meshes. It preserves semantic symmetries, supports co-deformation of semantic parts, and deforms meshes with up to 1 million faces on consumer-grade hardware.

Conclusion: This work efficiently integrates semantic control with speed and precision in handle-based mesh deformation. It extends classical methods and showcases applicability for real-time, high-resolution mesh editing on standard hardware.

Abstract: Handle-based mesh deformation has been a long-standing paradigm in computer graphics, enabling intuitive shape edits from sparse controls. Classic techniques offer precise and rapid deformation control. However, they solve an optimization problem with constraints defined by control handle placement, requiring a user to know apriori the ideal distribution of handles on the shape to accomplish the desired edit. The mapping from handle set to deformation behavior is often unintuitive and, importantly, non-semantic. Modern data-driven methods, on the other hand, leverage a data prior to obtain semantic edits, but are slow and imprecise. We propose a technique that fuses the semantic prior of data with the precise control and speed of traditional frameworks. Our approach is surprisingly simple yet effective: deep feature proximity makes for smooth and semantic deformation weights, with no need for additional regularization. The weights can be computed in real-time for any surface point, whereas prior methods require optimization for new handles. Moreover, the semantic prior from deep features enables co-deformation of semantic parts. We introduce an improved feature distillation pipeline, barycentric feature distillation, which efficiently uses the visual signal from shape renders to minimize distillation cost. This allows our weights to be computed for high resolution meshes in under a minute, in contrast to potentially hours for both classical and neural methods. We preserve and extend properties of classical methods through feature space constraints and locality weighting. Our field representation allows for automatic detection of semantic symmetries, which we use to produce symmetry-preserving deformations. We show a proof-of-concept application which can produce deformations for meshes up to 1 million faces in real-time on a consumer-grade machine.

</details>


### [411] [XRefine: Attention-Guided Keypoint Match Refinement](https://arxiv.org/abs/2601.12530)
*Jan Fabian Schmid,Annika Hagemann*

Main category: cs.CV

TL;DR: XRefine is a detector-agnostic sub-pixel refinement method for keypoints, improving 3D vision accuracy without retraining for each detector.


<details>
  <summary>Details</summary>
Motivation: Current keypoint detectors often produce spatially inaccurate matches, limiting 3D vision task performance. Existing refinement methods are detector-specific, requiring retraining.

Method: XRefine uses a cross-attention-based architecture to refine keypoint coordinates using image patches centered at matched keypoints, independent of detector representations. It also supports multi-view feature tracks.

Result: Experiments on datasets like MegaDepth, KITTI, and ScanNet show improved geometric estimation accuracy when using XRefine, outperforming existing methods.

Conclusion: XRefine consistently enhances 3D vision accuracy, delivering efficient and generalized refinement across different detectors, and its implementation is publicly accessible.

Abstract: Sparse keypoint matching is crucial for 3D vision tasks, yet current keypoint detectors often produce spatially inaccurate matches. Existing refinement methods mitigate this issue through alignment of matched keypoint locations, but they are typically detector-specific, requiring retraining for each keypoint detector. We introduce XRefine, a novel, detector-agnostic approach for sub-pixel keypoint refinement that operates solely on image patches centered at matched keypoints. Our cross-attention-based architecture learns to predict refined keypoint coordinates without relying on internal detector representations, enabling generalization across detectors. Furthermore, XRefine can be extended to handle multi-view feature tracks. Experiments on MegaDepth, KITTI, and ScanNet demonstrate that the approach consistently improves geometric estimation accuracy, achieving superior performance compared to existing refinement methods while maintaining runtime efficiency. Our code and trained models can be found at https://github.com/boschresearch/xrefine.

</details>


### [412] [BirdsEye-RU: A Dataset For Detecting Faces from Overhead Images](https://arxiv.org/abs/2601.12533)
*Md. Ahanaf Arif Khan,Ariful Islam,Sangeeta Biswas,Md. Iqbal Aziz Khan,Subrata Pramanik,Sanjoy Kumar Chakrabarty,Bimal Kumar Pramanik*

Main category: cs.CV

TL;DR: The paper introduces the BirdsEye-RU dataset containing annotated faces from overhead images to address the challenge of face detection in extreme scale variations and environmental clutter.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations in detecting small and distant faces in aerial and high-altitude images due to scale variations and cluttered environments.

Method: Development of the BirdsEye-RU dataset with 2,978 images containing over 8,000 annotated faces using diverse image sources such as drones and smartphones from high altitudes.

Result: A publicly available, comprehensive dataset capturing small, distant faces in various environments, designed to improve face detection algorithms.

Conclusion: The dataset provides researchers with a valuable resource to tackle face detection challenges in overhead imagery.

Abstract: Detecting faces in overhead images remains a significant challenge due to extreme scale variations and environmental clutter. To address this, we created the BirdsEye-RU dataset, a comprehensive collection of 2,978 images containing over eight thousand annotated faces. This dataset is specifically designed to capture small and distant faces across diverse environments, containing both drone images and smartphone-captured images from high altitude. We present a detailed description of the BirdsEye-RU dataset in this paper. We made our dataset freely available to the public, and it can be accessed at https://www.kaggle.com/datasets/mdahanafarifkhan/birdseye-ru.

</details>


### [413] [Encoding Emotion Through Self-Supervised Eye Movement Reconstruction](https://arxiv.org/abs/2601.12534)
*Marcus Ma,Jordan Prescott,Emily Zhou,Tiantian Feng,Kleanthis Avramidis,Gabor Mihaly Toth,Shrikanth Narayanan*

Main category: cs.CV

TL;DR: This paper introduces a novel gaze detection model that utilizes self-supervised methods to analyze emotional expression through eye movement in low-resolution videos.


<details>
  <summary>Details</summary>
Motivation: Most studies on eye movement and emotion rely on high-resolution, specialized equipment, limiting accessibility and generalizability of findings, creating a need to explore alternative approaches using naturalistic, low-resolution videos.

Method: The authors developed a self-supervised gaze detection model using eye movement reconstruction. They applied pretraining inspired by language models and fine-tuned its encoder embeddings for tasks predicting emotional markers from speech and momentary emotional behaviors.

Result: The proposed model effectively predicted emotional expression markers with positive correlation observed between pretraining and emotion processing performance across experiments.

Conclusion: Self-supervised eye movement reconstruction is a reliable method for encoding affective signals, enabling broader applications in analyzing emotional expression from low-resolution video data.

Abstract: The relationship between emotional expression and eye movement is well-documented, with literature establishing gaze patterns are reliable indicators of emotion. However, most studies utilize specialized, high-resolution eye-tracking equipment, limiting the potential reach of findings. We investigate how eye movement can be used to predict multimodal markers of emotional expression from naturalistic, low-resolution videos. We utilize a collection of video interviews from the USC Shoah Foundation's Visual History Archive with Holocaust survivors as they recount their experiences in the Auschwitz concentration camp. Inspired by pretraining methods on language models, we develop a novel gaze detection model that uses self-supervised eye movement reconstruction that can effectively leverage unlabeled video. We use this model's encoder embeddings to fine-tune models on two downstream tasks related to emotional expression. The first is aligning eye movement with directional emotion estimates from speech. The second task is using eye gaze as a predictor of three momentary manifestations of emotional behaviors: laughing, crying/sobbing, and sighing. We find our new model is predictive of emotion outcomes and observe a positive correlation between pretraining performance and emotion processing performance for both experiments. We conclude self-supervised eye movement reconstruction is an effective method for encoding the affective signal they carry.

</details>


### [414] [PISE: Physics-Anchored Semantically-Enhanced Deep Computational Ghost Imaging for Robust Low-Bandwidth Machine Perception](https://arxiv.org/abs/2601.12551)
*Tong Wu*

Main category: cs.CV

TL;DR: PISE combines physics-informed methods and semantic guidance to enhance ghost imaging with improved accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address low-bandwidth edge perception challenges for better imaging performance.

Method: The method involves using physics-informed frameworks with adjoint operator initialization and semantic guidance.

Result: PISE achieved a 2.57% classification accuracy improvement and a 9x variance reduction at 5% sampling.

Conclusion: PISE effectively enhances edge perception while ensuring robustness and performance in ghost imaging.

Abstract: We propose PISE, a physics-informed deep ghost imaging framework for low-bandwidth edge perception. By combining adjoint operator initialization with semantic guidance, PISE improves classification accuracy by 2.57% and reduces variance by 9x at 5% sampling.

</details>


### [415] [Camera Pose Revisited](https://arxiv.org/abs/2601.12567)
*Władysław Skarbek,Michał Salomonowicz,Michał Król*

Main category: cs.CV

TL;DR: The paper introduces the PnP-ProCay78 algorithm to solve the planar Perspective-n-Point problem, optimizing camera pose estimation with deterministically selected starting points and reduced computational complexity.


<details>
  <summary>Details</summary>
Motivation: To improve the estimation of camera position and orientation, addressing inefficiencies in existing PnP solvers through a simpler and computationally transparent hybrid cost formulation.

Method: A quadratic reconstruction error formulation combined with Cayley rotation parameterization and deterministic starting-point selection to avoid exhaustive solution-space search, validated on RGB and thermal camera setups.

Result: The algorithm achieves nearly the same projection accuracy as state-of-the-art methods like SQPnP, surpasses IPPE, and simplifies the algorithmic structure, while providing intuitive insights into optimization trajectories.

Conclusion: PnP-ProCay78 combines mathematical transparency and computational efficiency to enhance camera pose estimation methods, making it suitable for practical and educational applications.

Abstract: Estimating the position and orientation of a camera with respect to an observed scene is one of the central problems in computer vision, particularly in the context of camera calibration and multi-sensor systems. This paper addresses the planar Perspective--$n$--Point problem, with special emphasis on the initial estimation of the pose of a calibration object. As a solution, we propose the \texttt{PnP-ProCay78} algorithm, which combines the classical quadratic formulation of the reconstruction error with a Cayley parameterization of rotations and least-squares optimization. The key component of the method is a deterministic selection of starting points based on an analysis of the reconstruction error for two canonical vectors, allowing costly solution-space search procedures to be avoided. Experimental validation is performed using data acquired also from high-resolution RGB cameras and very low-resolution thermal cameras in an integrated RGB--IR setup. The results demonstrate that the proposed algorithm achieves practically the same projection accuracy as optimal \texttt{SQPnP} and slightly higher than \texttt{IPPE}, both prominent \texttt{PnP-OpenCV} procedures. However, \texttt{PnP-ProCay78} maintains a significantly simpler algorithmic structure. Moreover, the analysis of optimization trajectories in Cayley space provides an intuitive insight into the convergence process, making the method attractive also from a didactic perspective. Unlike existing PnP solvers, the proposed \texttt{PnP-ProCay78} algorithm combines projection error minimization with an analytically eliminated reconstruction-error surrogate for translation, yielding a hybrid cost formulation that is both geometrically transparent and computationally efficient.

</details>


### [416] [Linear Mechanisms for Spatiotemporal Reasoning in Vision Language Models](https://arxiv.org/abs/2601.12626)
*Raphi Kang,Hongqiao Chen,Georgia Gkioxari,Pietro Perona*

Main category: cs.CV

TL;DR: The paper investigates how Vision Language Models combine spatial and textual data, identifying spatial IDs as a key mechanism for reasoning and extending this analysis to video models.


<details>
  <summary>Details</summary>
Motivation: To uncover and better understand the opaque mechanisms enabling spatio-temporal reasoning in Vision Language Models.

Method: Postulated that visual and textual data merge during VLM computations; identified and analyzed the binding of spatial IDs to textual activations, alongside causal interventions.

Result: VLMs encode spatial IDs systematically at intermediate layers, which serve as tools for diagnosing model limitations and signals for learning; similar mechanisms were observed in video VLMs.

Conclusion: Understanding spatial and temporal IDs advances interpretability and can guide the improved design of aligned and capable Vision Language Models.

Abstract: Spatio-temporal reasoning is a remarkable capability of Vision Language Models (VLMs), but the underlying mechanisms of such abilities remain largely opaque. We postulate that visual/geometrical and textual representations of spatial structure must be combined at some point in VLM computations. We search for such confluence, and ask whether the identified representation can causally explain aspects of input-output model behavior through a linear model. We show empirically that VLMs encode object locations by linearly binding \textit{spatial IDs} to textual activations, then perform reasoning via language tokens. Through rigorous causal interventions we demonstrate that these IDs, which are ubiquitous across the model, can systematically mediate model beliefs at intermediate VLM layers. Additionally, we find that spatial IDs serve as a diagnostic tool for identifying limitations in existing VLMs, and as a valuable learning signal. We extend our analysis to video VLMs and identify an analogous linear temporal ID mechanism. By characterizing our proposed spatiotemporal ID mechanism, we elucidate a previously underexplored internal reasoning process in VLMs, toward improved interpretability and the principled design of more aligned and capable models. We release our code for reproducibility: https://github.com/Raphoo/linear-mech-vlms.

</details>


### [417] [From Bands to Depth: Understanding Bathymetry Decisions on Sentinel-2](https://arxiv.org/abs/2601.12636)
*Satyaki Roy Chowdhury,Aswathnarayan Radhakrishnan,Hsiao Jou Hsu,Hari Subramoni,Joachim Moortgat*

Main category: cs.CV

TL;DR: This paper analyzes a Swin-Transformer based U-Net model for deriving bathymetry using Sentinel-2 satellite data, focusing on prediction reliability and cross-site deployment challenges.


<details>
  <summary>Details</summary>
Motivation: To address reliability and transferability challenges in satellite-derived bathymetry (SDB) using Sentinel-2 data for consistent outputs across regions.

Method: The study employs a Swin-Transformer based U-Net (Swin-BathyUNet), implements leave-one-band-out studies, adapts ablation-based CAM for regression (A-CAM-R), and evaluates cross-region predictive reliability with depth-dependent analysis.

Result: The model's predictions are trustworthy when supported by localized evidence. It demonstrates depth-dependent degradation during cross-region inference and improved robustness to glint/foam through decoder modifications.

Conclusion: Practical recommendations include maintaining wide receptive fields, preserving blue/green channel fidelity, pre-filtering certain regions, and combining fine-tuning with depth-aware calibration for successful cross-region SDB application.

Abstract: Deploying Sentinel-2 satellite derived bathymetry (SDB) robustly across sites remains challenging. We analyze a Swin-Transformer based U-Net model (Swin-BathyUNet) to understand how it infers depth and when its predictions are trustworthy. A leave-one-band out study ranks spectral importance to the different bands consistent with shallow water optics. We adapt ablation-based CAM to regression (A-CAM-R) and validate the reliability via a performance retention test: keeping only the top-p% salient pixels while neutralizing the rest causes large, monotonic RMSE increase, indicating explanations localize on evidence the model relies on. Attention ablations show decoder conditioned cross attention on skips is an effective upgrade, improving robustness to glint/foam. Cross-region inference (train on one site, test on another) reveals depth-dependent degradation: MAE rises nearly linearly with depth, and bimodal depth distributions exacerbate mid/deep errors. Practical guidance follows: maintain wide receptive fields, preserve radiometric fidelity in green/blue channels, pre-filter bright high variance near shore, and pair light target site fine tuning with depth aware calibration to transfer across regions.

</details>


### [418] [Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT](https://arxiv.org/abs/2601.12638)
*Ninnart Fuengfusin,Keisuke Yoneda,Naoki Suganuma*

Main category: cs.CV

TL;DR: The paper proposes a mixed precision framework for PointPillars to improve the real-time performance of LIDAR 3D object detection using model quantization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enable real-time 3D object detection for autonomous vehicles, overcoming performance degradation caused by directly applying model quantization to LIDAR data.

Method: A mixed precision framework identifies the sensitive layers through PTQ, assigns them as FP, and uses greedy search to finalize mixed precision models. The process is optimized with either PTQ or QAT, considering outliers by using minimal calibration data.

Result: The mixed precision models reduce latency and size by up to 2.35x and 2.26x using TensorRT deployment, while maintaining competitive performance with FP models.

Conclusion: The proposed method effectively balances model performance and computational efficiency, making LIDAR 3D detection feasible for real-time deployment.

Abstract: LIDAR 3D object detection is one of the important tasks for autonomous vehicles. Ensuring that this task operates in real-time is crucial. Toward this, model quantization can be used to accelerate the runtime. However, directly applying model quantization often leads to performance degradation due to LIDAR's wide numerical distributions and extreme outliers. To address the wide numerical distribution, we proposed a mixed precision framework designed for PointPillars. Our framework first searches for sensitive layers with post-training quantization (PTQ) by quantizing one layer at a time to 8-bit integer (INT8) and evaluating each model for average precision (AP). The top-k most sensitive layers are assigned as floating point (FP). Combinations of these layers are greedily searched to produce candidate mixed precision models, which are finalized with either PTQ or quantization-aware training (QAT). Furthermore, to handle outliers, we observe that using a very small number of calibration data reduces the likelihood of encountering outliers, thereby improving PTQ performance. Our methods provides mixed precision models without training in the PTQ pipeline, while our QAT pipeline achieves the performance competitive to FP models. With TensorRT deployment, our models offer less latency and sizes by up to 2.35 and 2.26 times, respectively.

</details>


### [419] [Generalizable Hyperparameter Optimization for Federated Learning on Non-IID Cancer Images](https://arxiv.org/abs/2601.12664)
*Elisa Gonçalves Ribeiro,Rodrigo Moreira,Larissa Ferreira Rodrigues Moreira,André Ricardo Backes*

Main category: cs.CV

TL;DR: The paper explores applying optimal hyperparameters from one cancer imaging dataset for federated learning (FL) under non-IID conditions, proposing a heuristic for cross-dataset configuration.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address performance limitations in FL for cancer histopathology, specifically when facing privacy-related constraints and non-IID client data.

Method: The method involves using centralized Bayesian hyperparameter optimization on one dataset, then transferring the optimized parameters to non-IID FL setups, alongside introducing a heuristic for aggregating configurations.

Result: The proposed heuristic aggregation achieves competitive classification performance across non-IID FL setups for ovarian and colorectal cancer tasks.

Conclusion: Using dataset-specific hyperparameter optima combined with the heuristic aggregation significantly benefits FL in histopathology tasks under non-IID conditions.

Abstract: Deep learning for cancer histopathology training conflicts with privacy constraints in clinical settings. Federated Learning (FL) mitigates this by keeping data local; however, its performance depends on hyperparameter choices under non-independent and identically distributed (non-IID) client datasets. This paper examined whether hyperparameters optimized on one cancer imaging dataset generalized across non-IID federated scenarios. We considered binary histopathology tasks for ovarian and colorectal cancers. We perform centralized Bayesian hyperparameter optimization and transfer dataset-specific optima to the non-IID FL setup. The main contribution of this study is the introduction of a simple cross-dataset aggregation heuristic by combining configurations by averaging the learning rates and considering the modal optimizers and batch sizes. This combined configuration achieves a competitive classification performance.

</details>


### [420] [Near-Light Color Photometric Stereo for mono-Chromaticity non-lambertian surface](https://arxiv.org/abs/2601.12666)
*Zonglin Li,Jieji Ren,Shuangfan Zhou,Heng Guo,Jinnuo Zhang,Jiang Zhou,Boxin Shi,Zhanyu Ma,Guoying Gu*

Main category: cs.CV

TL;DR: The paper presents a novel framework for single-shot surface reconstruction using color photometric stereo, addressing limitations in non-ideal lighting and non-Lambertian surfaces.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on distant lighting and Lambertian surfaces, which are impractical for realistic scenarios. This paper aims to enhance surface reconstruction even under more practical conditions.

Method: The framework incorporates neural implicit representations for depth and BRDF modeling under the assumption of mono-chromaticity, improving surface recovery from single images.

Result: Experiments with synthetic and real-world datasets show accurate and robust reconstruction results using the proposed method.

Conclusion: The research successfully overcomes limitations of traditional methods by enabling detailed surface recovery from a single image, validating it with an optical tactile sensor.

Abstract: Color photometric stereo enables single-shot surface reconstruction, extending conventional photometric stereo that requires multiple images of a static scene under varying illumination to dynamic scenarios. However, most existing approaches assume ideal distant lighting and Lambertian reflectance, leaving more practical near-light conditions and non-Lambertian surfaces underexplored. To overcome this limitation, we propose a framework that leverages neural implicit representations for depth and BRDF modeling under the assumption of mono-chromaticity (uniform chromaticity and homogeneous material), which alleviates the inherent ill-posedness of color photometric stereo and allows for detailed surface recovery from just one image. Furthermore, we design a compact optical tactile sensor to validate our approach. Experiments on both synthetic and real-world datasets demonstrate that our method achieves accurate and robust surface reconstruction.

</details>


### [421] [Exploiting Test-Time Augmentation in Federated Learning for Brain Tumor MRI Classification](https://arxiv.org/abs/2601.12671)
*Thamara Leandra de Deus Melo,Rodrigo Moreira,Larissa Ferreira Rodrigues Moreira,André Ricardo Backes*

Main category: cs.CV

TL;DR: This paper examines federated learning and preprocessing strategies for MRI-based brain tumor classification using CNNs.


<details>
  <summary>Details</summary>
Motivation: To address challenges in brain tumor diagnosis caused by lesion variability and image complexity.

Method: The study leverages federated learning with CNN models, comparing results of original MRI images with preprocessed ones, and explores preprocessing techniques and test-time augmentation.

Result: Preprocessing alone gave negligible improvement; however, combining preprocessing and test-time augmentation significantly enhanced MRI classification performance (p<0.001).

Conclusion: Test-time augmentation should be the standard approach in federated learning of medical imaging tasks. Preprocessing paired with TTA offers additional reliable improvements when computational resources are available.

Abstract: Efficient brain tumor diagnosis is crucial for early treatment; however, it is challenging because of lesion variability and image complexity. We evaluated convolutional neural networks (CNNs) in a federated learning (FL) setting, comparing models trained on original versus preprocessed MRI images (resizing, grayscale conversion, normalization, filtering, and histogram equalization). Preprocessing alone yielded negligible gains; combined with test-time augmentation (TTA), it delivered consistent, statistically significant improvements in federated MRI classification (p<0.001). In practice, TTA should be the default inference strategy in FL-based medical imaging; when the computational budget permits, pairing TTA with light preprocessing provides additional reliable gains.

</details>


### [422] [VILTA: A VLM-in-the-Loop Adversary for Enhancing Driving Policy Robustness](https://arxiv.org/abs/2601.12672)
*Qimao Chen,Fang Li,Shaoqing Xu,Zhiyi Lai,Zixun Xie,Yuechen Luo,Shengyin Jiang,Hanbing Li,Long Chen,Bing Wang,Yi Zhang,Zhi-Xin Yang*

Main category: cs.CV

TL;DR: The paper introduces VILTA, a novel framework that integrates Vision Language Models (VLMs) into the closed-loop training of autonomous driving systems to improve handling rare critical driving scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the long-tail problem in autonomous driving where rare yet critical scenarios are underrepresented in real-world data, leading to unsafe deployment of AD systems.

Method: The method involves directly integrating a VLM into the training loop of AD agents, enabling VLMs to comprehend the dynamic driving environment and generate diverse, challenging driving scenarios by fine-grained editing of other agents' trajectories.

Result: VILTA was shown to significantly enhance the safety and robustness of autonomous driving systems, especially in navigating rare critical scenarios.

Conclusion: Integrating VLMs into AD training enhances generative potential and helps create diverse, plausible, and challenging driving scenarios, thereby improving AD systems' ability to handle the long-tail problem.

Abstract: The safe deployment of autonomous driving (AD) systems is fundamentally hindered by the long-tail problem, where rare yet critical driving scenarios are severely underrepresented in real-world data. Existing solutions including safety-critical scenario generation and closed-loop learning often rely on rule-based heuristics, resampling methods and generative models learned from offline datasets, limiting their ability to produce diverse and novel challenges. While recent works leverage Vision Language Models (VLMs) to produce scene descriptions that guide a separate, downstream model in generating hazardous trajectories for agents, such two-stage framework constrains the generative potential of VLMs, as the diversity of the final trajectories is ultimately limited by the generalization ceiling of the downstream algorithm. To overcome these limitations, we introduce VILTA (VLM-In-the-Loop Trajectory Adversary), a novel framework that integrates a VLM into the closed-loop training of AD agents. Unlike prior works, VILTA actively participates in the training loop by comprehending the dynamic driving environment and strategically generating challenging scenarios through direct, fine-grained editing of surrounding agents' future trajectories. This direct-editing approach fully leverages the VLM's powerful generalization capabilities to create a diverse curriculum of plausible yet challenging scenarios that extend beyond the scope of traditional methods. We demonstrate that our approach substantially enhances the safety and robustness of the resulting AD policy, particularly in its ability to navigate critical long-tail events.

</details>


### [423] [Fusion-Restoration Image Processing Algorithm to Improve the High-Temperature Deformation Measurement](https://arxiv.org/abs/2601.12682)
*Banglei Guan,Dongcai Tan,Jing Tao,Ang Su,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: This paper addresses image degradation in high-temperature deformation measurement by proposing methods to suppress thermal radiation and heat haze using fusion-restoration image processing.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome limitations of current deformation measurement techniques caused by thermal radiation and heat haze, which reduce accuracy when using digital image correlation (DIC) in high-temperature conditions.

Method: The authors propose a multi-exposure image fusion approach optimized with image decomposition and iterative parameter adjustment to suppress degradation. This includes FSIM-guided optimization and a grayscale average algorithm.

Result: The method improved the effective computation area of images by 24%-53% and significantly reduced deformation measurement errors (e.g., 85.3% for ε_xx and ~36% for other parameters) without compromising measurement accuracy.

Conclusion: The proposed image processing techniques enhance image quality, suppress interference of heat factors, and improve deformation measurement accuracy, showing strong potential for high-temperature applications.

Abstract: In the deformation measurement of high-temperature structures, image degradation caused by thermal radiation and random errors introduced by heat haze restrict the accuracy and effectiveness of deformation measurement. To suppress thermal radiation and heat haze using fusion-restoration image processing methods, thereby improving the accuracy and effectiveness of DIC in the measurement of high-temperature deformation. For image degradation caused by thermal radiation, based on the image layered representation, the image is decomposed into positive and negative channels for parallel processing, and then optimized for quality by multi-exposure image fusion. To counteract the high-frequency, random errors introduced by heat haze, we adopt the FSIM as the objective function to guide the iterative optimization of model parameters, and the grayscale average algorithm is applied to equalize anomalous gray values, thereby reducing measurement error. The proposed multi-exposure image fusion algorithm effectively suppresses image degradation caused by complex illumination conditions, boosting the effective computation area from 26% to 50% for under-exposed images and from 32% to 40% for over-exposed images without degrading measurement accuracy in the experiment. Meanwhile, the image restoration combined with the grayscale average algorithm reduces static thermal deformation measurement errors. The error in ε_xx is reduced by 85.3%, while the errors in ε_yy and γ_xy are reduced by 36.0% and 36.4%, respectively. We present image processing methods to suppress the interference of thermal radiation and heat haze in high-temperature deformation measurement using DIC. The experimental results verify that the proposed method can effectively improve image quality, reduce deformation measurement errors, and has potential application value in thermal deformation measurement.

</details>


### [424] [GaussianTrimmer: Online Trimming Boundaries for 3DGS Segmentation](https://arxiv.org/abs/2601.12683)
*Liwei Liao,Ronggang Wang*

Main category: cs.CV

TL;DR: The paper introduces GaussianTrimmer, a plug-and-play method to improve boundary precision in 3D Gaussian-based scene segmentation by trimming coarse boundaries.


<details>
  <summary>Details</summary>
Motivation: Existing 3D Gaussian segmentation methods face issues with jagged object boundaries due to large Gaussian primitives spanning foreground and background.

Method: GaussianTrimmer involves generating virtual cameras for uniform coverage and trimming Gaussian primitives based on 2D segmentation results from these cameras.

Result: Experiments show significant improvements in segmentation quality when GaussianTrimmer is used with existing methods.

Conclusion: The proposed GaussianTrimmer is an effective tool for enhancing 3D Gaussian segmentation, offering improved boundary precision as a standalone, adaptable post-processing method.

Abstract: With the widespread application of 3D Gaussians in 3D scene representation, 3D scene segmentation methods based on 3D Gaussians have also gradually emerged. However, existing 3D Gaussian segmentation methods basically segment on the basis of Gaussian primitives. Due to the large variation range of the scale of 3D Gaussians, large-sized Gaussians that often span the foreground and background lead to jagged boundaries of segmented objects. To this end, we propose an online boundary trimming method, GaussianTrimmer, which is an efficient and plug-and-play post-processing method capable of trimming coarse boundaries for existing 3D Gaussian segmentation methods. Our method consists of two core steps: 1. Generating uniformly and well-covered virtual cameras; 2. Trimming Gaussian at the primitive level based on 2D segmentation results on virtual cameras. Extensive quantitative and qualitative experiments demonstrate that our method can improve the segmentation quality of existing 3D Gaussian segmentation methods as a plug-and-play method.

</details>


### [425] [Fusing in 3D: Free-Viewpoint Fusion Rendering with a 3D Infrared-Visible Scene Representation](https://arxiv.org/abs/2601.12697)
*Chao Yang,Deshui Miao,Chao Tian,Guoqing Zhu,Yameng Gu,Zhenyu He*

Main category: cs.CV

TL;DR: The paper introduces the Infrared-Visible Gaussian Fusion (IVGF) framework, addressing limitations in current methods by integrating infrared and visible information through a cross-modal adjustment module and fusion loss.


<details>
  <summary>Details</summary>
Motivation: Existing 2D fusion methods fail to fully comprehend the complexities of scenarios, leading to the loss of vital scene information during fusion in fixed camera viewpoints.

Method: A novel IVGF framework is proposed, with key components being a cross-modal adjustment module to resolve cross-modal conflicts and a fusion loss designed to retain critical features of both modalities.

Result: The IVGF framework is experimentally proven to effectively preserve critical features from both infrared and visible modalities, offering improved fusion capability.

Conclusion: The study confirms that the IVGF framework enhances image fusion by reconstructing scene geometry and addressing cross-modal conflicts, providing a comprehensive solution for complex scenarios.

Abstract: Infrared-visible image fusion aims to integrate infrared and visible information into a single fused image. Existing 2D fusion methods focus on fusing images from fixed camera viewpoints, neglecting a comprehensive understanding of complex scenarios, which results in the loss of critical information about the scene. To address this limitation, we propose a novel Infrared-Visible Gaussian Fusion (IVGF) framework, which reconstructs scene geometry from multimodal 2D inputs and enables direct rendering of fused images. Specifically, we propose a cross-modal adjustment (CMA) module that modulates the opacity of Gaussians to solve the problem of cross-modal conflicts. Moreover, to preserve the distinctive features from both modalities, we introduce a fusion loss that guides the optimization of CMA, thus ensuring that the fused image retains the critical characteristics of each modality. Comprehensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed method.

</details>


### [426] [P2L-CA: An Effective Parameter Tuning Framework for Rehearsal-Free Multi-Label Class-Incremental Learning](https://arxiv.org/abs/2601.12714)
*Songlin Dong,Jiangyang Li,Chenhao Ding,Zhiheng Ma,Haoyu Luo,Yuhang He,Yihong Gong*

Main category: cs.CV

TL;DR: The study introduces P2L-CA, a parameter-efficient framework for Multi-label Class-Incremental Learning (MLCIL), achieving better performance and generalization with minimal trainable parameters and no memory buffers.


<details>
  <summary>Details</summary>
Motivation: Existing MLCIL methods face challenges like high computational/storage costs and problems with feature confusion and domain discrepancies.

Method: The proposed P2L-CA framework integrates a Prompt-to-Label (P2L) module for disentangling representations and enforcing semantic-visual alignment, and a Continuous Adapter (CA) module to bridge domain gaps using lightweight adapters.

Result: Experiments on datasets like MS-COCO and PASCAL VOC demonstrate superior performance of P2L-CA in various scenarios compared to state-of-the-art methods.

Conclusion: P2L-CA is effective in addressing MLCIL challenges, providing improved efficiency, generalization, and performance without relying on extensive resources.

Abstract: Multi-label Class-Incremental Learning aims to continuously recognize novel categories in complex scenes where multiple objects co-occur. However, existing approaches often incur high computational costs due to full-parameter fine-tuning and substantial storage overhead from memory buffers, or they struggle to address feature confusion and domain discrepancies adequately. To overcome these limitations, we introduce P2L-CA, a parameter-efficient framework that integrates a Prompt-to-Label module with a Continuous Adapter module. The P2L module leverages class-specific prompts to disentangle multi-label representations while incorporating linguistic priors to enforce stable semantic-visual alignment. Meanwhile, the CA module employs lightweight adapters to mitigate domain gaps between pre-trained models and downstream tasks, thereby enhancing model plasticity. Extensive experiments across standard and challenging MLCIL settings on MS-COCO and PASCAL VOC show that P2L-CA not only achieves substantial improvements over state-of-the-art methods but also demonstrates strong generalization in CIL scenarios, all while requiring minimal trainable parameters and eliminating the need for memory buffers.

</details>


### [427] [RSOD: Reliability-Guided Sonar Image Object Detection with Extremely Limited Labels](https://arxiv.org/abs/2601.12715)
*Chengzhou Li,Ping Guo,Guanchen Meng,Qi Jia,Jinyuan Liu,Zhu Liu,Xiaokang Liu,Yu Liu,Zhongxuan Luo,Xin Fan*

Main category: cs.CV

TL;DR: The paper introduces RSOD, a teacher-student framework to enhance object detection in sonar images with limited labels by leveraging pseudo-labeling and reliability-guided constraints.


<details>
  <summary>Details</summary>
Motivation: To facilitate effective object detection in sonar images, which lack texture details, have high noise levels, and have very limited labeled data, making them challenging for precise annotations.

Method: The RSOD framework uses a teacher-student model with a reliability score to assess the teacher's predictions and introduce pseudo-labels via an object-mixed strategy. It employs adaptive constraints to optimize student performance using unlabeled data.

Result: The proposed RSOD method allows significant performance improvement, achieving comparable results to models trained on fully-labeled data while using only 5% of labeled data on the UATD dataset.

Conclusion: The study highlights the effectiveness of RSOD for sonar image detection with extremely limited labeled data while contributing a new dataset for advancing sonar research.

Abstract: Object detection in sonar images is a key technology in underwater detection systems. Compared to natural images, sonar images contain fewer texture details and are more susceptible to noise, making it difficult for non-experts to distinguish subtle differences between classes. This leads to their inability to provide precise annotation data for sonar images. Therefore, designing effective object detection methods for sonar images with extremely limited labels is particularly important. To address this, we propose a teacher-student framework called RSOD, which aims to fully learn the characteristics of sonar images and develop a pseudo-label strategy suitable for these images to mitigate the impact of limited labels. First, RSOD calculates a reliability score by assessing the consistency of the teacher's predictions across different views. To leverage this score, we introduce an object mixed pseudo-label method to tackle the shortage of labeled data in sonar images. Finally, we optimize the performance of the student by implementing a reliability-guided adaptive constraint. By taking full advantage of unlabeled data, the student can perform well even in situations with extremely limited labels. Notably, on the UATD dataset, our method, using only 5% of labeled data, achieves results that can compete against those of our baseline algorithm trained on 100% labeled data. We also collected a new dataset to provide more valuable data for research in the field of sonar.

</details>


### [428] [S2DiT: Sandwich Diffusion Transformer for Mobile Streaming Video Generation](https://arxiv.org/abs/2601.12719)
*Lin Zhao,Yushu Wu,Aleksei Lebedev,Dishani Lahiri,Meng Dong,Arpit Sahni,Michael Vasilkovsky,Hao Chen,Ju Hu,Aliaksandr Siarohin,Sergey Tulyakov,Yanzhi Wang,Anil Kag,Yanyu Li*

Main category: cs.CV

TL;DR: S2DiT is a new, efficient video generation model capable of streaming on mobile devices.


<details>
  <summary>Details</summary>
Motivation: To address the high computational cost of existing video generation models, making real-time or on-device generation feasible.

Method: Introduces S2DiT with efficient attentions (LCHA and SSA) and a sandwich design discovered through budget-aware dynamic programming. Incorporates a 2-in-1 distillation framework for transferring knowledge from large teacher models.

Result: S2DiT achieves high-quality video generation comparable to state-of-the-art models while running on mobile hardware at over 10 FPS.

Conclusion: S2DiT is a solution for efficient, high-fidelity video generation, making it viable for mobile applications.

Abstract: Diffusion Transformers (DiTs) have recently improved video generation quality. However, their heavy computational cost makes real-time or on-device generation infeasible. In this work, we introduce S2DiT, a Streaming Sandwich Diffusion Transformer designed for efficient, high-fidelity, and streaming video generation on mobile hardware. S2DiT generates more tokens but maintains efficiency with novel efficient attentions: a mixture of LinConv Hybrid Attention (LCHA) and Stride Self-Attention (SSA). Based on this, we uncover the sandwich design via a budget-aware dynamic programming search, achieving superior quality and efficiency. We further propose a 2-in-1 distillation framework that transfers the capacity of large teacher models (e.g., Wan 2.2-14B) to the compact few-step sandwich model. Together, S2DiT achieves quality on par with state-of-the-art server video models, while streaming at over 10 FPS on an iPhone.

</details>


### [429] [KaoLRM: Repurposing Pre-trained Large Reconstruction Models for Parametric 3D Face Reconstruction](https://arxiv.org/abs/2601.12736)
*Qingtian Zhu,Xu Cao,Zhixiang Wang,Yinqiang Zheng,Takafumi Taketomi*

Main category: cs.CV

TL;DR: This paper introduces KaoLRM, a novel method to improve parametric 3D face reconstruction from single images by leveraging the learned prior of Large Reconstruction Models (LRM) and FLAME parameterization.


<details>
  <summary>Details</summary>
Motivation: To address the lack of cross-view consistency in existing 3D Morphable Model (3DMM) regressors, especially in cases of viewpoint variations and self-occlusions.

Method: KaoLRM incorporates pre-trained LRM triplane features into the FLAME parameter space for geometry recovery and utilizes FLAME-based 2D Gaussian Splatting for consistent appearance modeling tied to the FLAME mesh.

Result: KaoLRM demonstrates improved reconstruction accuracy and better cross-view consistency in experiments conducted on controlled and in-the-wild benchmarks, outperforming previous methods.

Conclusion: KaoLRM effectively enhances 3D face reconstruction by exploiting the rich 3D prior of LRM, ensuring robustness under challenging conditions like diverse viewpoints and self-occlusions.

Abstract: We propose KaoLRM to re-target the learned prior of the Large Reconstruction Model (LRM) for parametric 3D face reconstruction from single-view images. Parametric 3D Morphable Models (3DMMs) have been widely used for facial reconstruction due to their compact and interpretable parameterization, yet existing 3DMM regressors often exhibit poor consistency across varying viewpoints. To address this, we harness the pre-trained 3D prior of LRM and incorporate FLAME-based 2D Gaussian Splatting into LRM's rendering pipeline. Specifically, KaoLRM projects LRM's pre-trained triplane features into the FLAME parameter space to recover geometry, and models appearance via 2D Gaussian primitives that are tightly coupled to the FLAME mesh. The rich prior enables the FLAME regressor to be aware of the 3D structure, leading to accurate and robust reconstructions under self-occlusions and diverse viewpoints. Experiments on both controlled and in-the-wild benchmarks demonstrate that KaoLRM achieves superior reconstruction accuracy and cross-view consistency, while existing methods remain sensitive to viewpoint variations. The code is released at https://github.com/CyberAgentAILab/KaoLRM.

</details>


### [430] [SSPFormer: Self-Supervised Pretrained Transformer for MRI Images](https://arxiv.org/abs/2601.12747)
*Jingkai Li,Xiaoze Tian,Yuhang Shen,Jia Wang,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: This paper introduces a Self-Supervised Pretrained Transformer (SSPFormer) designed to enhance MRI image processing by tackling challenges like medical specificity and data limitations using innovative techniques.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the difficulty of transferring pre-trained transformers from natural image domains to MRI due to the distinct anatomical specificities of medical images and hurdles like data privacy and scarcity.

Method: The SSPFormer leverages strategies such as inverse frequency projection masking for structure-aware learning and frequency-weighted FFT noise enhancement for artifact robustness, utilizing unlabeled raw MRI data.

Result: Experiments demonstrate that SSPFormer achieves state-of-the-art performance in segmentation, super-resolution, and denoising tasks for MRI images.

Conclusion: SSPFormer proves effective in learning fine-grained features, achieving domain adaptability, and addressing real-world clinical requirements for MRI image processing.

Abstract: The pre-trained transformer demonstrates remarkable generalization ability in natural image processing. However, directly transferring it to magnetic resonance images faces two key challenges: the inability to adapt to the specificity of medical anatomical structures and the limitations brought about by the privacy and scarcity of medical data. To address these issues, this paper proposes a Self-Supervised Pretrained Transformer (SSPFormer) for MRI images, which effectively learns domain-specific feature representations of medical images by leveraging unlabeled raw imaging data. To tackle the domain gap and data scarcity, we introduce inverse frequency projection masking, which prioritizes the reconstruction of high-frequency anatomical regions to enforce structure-aware representation learning. Simultaneously, to enhance robustness against real-world MRI artifacts, we employ frequency-weighted FFT noise enhancement that injects physiologically realistic noise into the Fourier domain. Together, these strategies enable the model to learn domain-invariant and artifact-robust features directly from raw scans. Through extensive experiments on segmentation, super-resolution, and denoising tasks, the proposed SSPFormer achieves state-of-the-art performance, fully verifying its ability to capture fine-grained MRI image fidelity and adapt to clinical application requirements.

</details>


### [431] [Moaw: Unleashing Motion Awareness for Video Diffusion Models](https://arxiv.org/abs/2601.12761)
*Tianqi Zhang,Ziyi Wang,Wenzhao Zheng,Weiliang Chen,Yuanhui Huang,Zhengyang Huang,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: The paper investigates enhancing the motion-aware capabilities of video diffusion models through supervised learning, introducing a framework called Moaw for motion transfer.


<details>
  <summary>Details</summary>
Motivation: Motivated by the inherent motion awareness in video diffusion models observed in prior zero-shot applications like optical flow prediction and tracking, the authors aim to better harness this capability for improved motion transfer through supervised training.

Method: The authors propose Moaw, which shifts video diffusion models from image-to-video generation towards video-to-dense-tracking. A motion-labeled dataset is constructed to identify strong motion-encoding features, which are then injected into a structurally similar video generation model. The homogeneity allows for zero-shot motion transfer.

Result: The proposed approach allows motion transfer without requiring additional adapters and demonstrates the feasibility of combining generative modeling with detailed motion understanding.

Conclusion: Moaw offers a novel framework that bridges generative video modeling and motion perception, promoting unified and controllable video learning systems.

Abstract: Video diffusion models, trained on large-scale datasets, naturally capture correspondences of shared features across frames. Recent works have exploited this property for tasks such as optical flow prediction and tracking in a zero-shot setting. Motivated by these findings, we investigate whether supervised training can more fully harness the tracking capability of video diffusion models. To this end, we propose Moaw, a framework that unleashes motion awareness for video diffusion models and leverages it to facilitate motion transfer. Specifically, we train a diffusion model for motion perception, shifting its modality from image-to-video generation to video-to-dense-tracking. We then construct a motion-labeled dataset to identify features that encode the strongest motion information, and inject them into a structurally identical video generation model. Owing to the homogeneity between the two networks, these features can be naturally adapted in a zero-shot manner, enabling motion transfer without additional adapters. Our work provides a new paradigm for bridging generative modeling and motion understanding, paving the way for more unified and controllable video learning frameworks.

</details>


### [432] [Towards Unbiased Source-Free Object Detection via Vision Foundation Models](https://arxiv.org/abs/2601.12765)
*Zhi Cai,Yingjie Gao,Yanan Zhang,Xinzhu Ma,Di Huang*

Main category: cs.CV

TL;DR: The paper proposes a new approach named DSOD to address Source Bias in Source-Free Object Detection by using VFMs to enhance performance and generalization.


<details>
  <summary>Details</summary>
Motivation: The authors aim to solve the Source Bias problem in SFOD, where existing models remain skewed toward the source domain, adversely impacting generalization.

Method: The paper introduces DSOD, which integrates VFMs through a Unified Feature Injection module (including SSE and DAAW) and employs Semantic-aware Feature Regularization to constrain feature learning. They also propose DSOD-distill, a VFM-free variant for computationally limited scenarios with a Dual-Teacher distillation scheme.

Result: DSOD outperforms state-of-the-art SFOD methods with impressive results across multiple benchmarks, including 48.1% AP on Normal-to-Foggy weather adaptation and 61.4% AP on Synthetic-to-Real adaptation.

Conclusion: The proposed DSOD framework effectively mitigates Source Bias, improving generalization and achieving state-of-the-art performance in Source-Free Object Detection tasks. The introduction of a VFM-free variant enhances applicability in resource-constrained setups.

Abstract: Source-Free Object Detection (SFOD) has garnered much attention in recent years by eliminating the need of source-domain data in cross-domain tasks, but existing SFOD methods suffer from the Source Bias problem, i.e. the adapted model remains skewed towards the source domain, leading to poor generalization and error accumulation during self-training. To overcome this challenge, we propose Debiased Source-free Object Detection (DSOD), a novel VFM-assisted SFOD framework that can effectively mitigate source bias with the help of powerful VFMs. Specifically, we propose Unified Feature Injection (UFI) module that integrates VFM features into the CNN backbone through Simple-Scale Extension (SSE) and Domain-aware Adaptive Weighting (DAAW). Then, we propose Semantic-aware Feature Regularization (SAFR) that constrains feature learning to prevent overfitting to source domain characteristics. Furthermore, we propose a VFM-free variant, termed DSOD-distill for computation-restricted scenarios through a novel Dual-Teacher distillation scheme. Extensive experiments on multiple benchmarks demonstrate that DSOD outperforms state-of-the-art SFOD methods, achieving 48.1% AP on Normal-to-Foggy weather adaptation, 39.3% AP on Cross-scene adaptation, and 61.4% AP on Synthetic-to-Real adaptation.

</details>


### [433] [Spatial-VLN: Zero-Shot Vision-and-Language Navigation With Explicit Spatial Perception and Exploration](https://arxiv.org/abs/2601.12766)
*Lu Yue,Yue Fan,Shiwei Lian,Yu Zhao,Jiaxin Yu,Liang Xie,Feitian Zhang*

Main category: cs.CV

TL;DR: The paper introduces Spatial-VLN, a framework enhancing spatial perception in zero-shot Vision-and-Language Navigation (VLN) using Large Language Models (LLMs), addressing key spatial bottlenecks to improve navigation performance in complex environments.


<details>
  <summary>Details</summary>
Motivation: Existing VLN agents leveraging LLMs excel in generalization but are limited by poor spatial perception, especially in tasks requiring precise environmental interaction and navigation.

Method: The Spatial-VLN framework includes two main modules: (1) Spatial Perception Enhancement (SPE), utilizing panoramic filtering and specialized experts for spatially consistent representations; (2) Explored Multi-expert Reasoning (EMR), featuring parallel LLM experts to manage semantics and spatial transitions, plus a query-and-explore mechanism for resolving perceptual ambiguities.

Result: Spatial-VLN achieves state-of-the-art performance on VLN-CE using low-cost LLMs, with enhancements in generalization and robustness validated through real-world evaluations.

Conclusion: The proposed Spatial-VLN framework effectively addresses spatial challenges in VLN tasks, bridging Sim2Real gaps and demonstrating superior adaptability in complex environments.

Abstract: Zero-shot Vision-and-Language Navigation (VLN) agents leveraging Large Language Models (LLMs) excel in generalization but suffer from insufficient spatial perception. Focusing on complex continuous environments, we categorize key perceptual bottlenecks into three spatial challenges: door interaction,multi-room navigation, and ambiguous instruction execution, where existing methods consistently suffer high failure rates. We present Spatial-VLN, a perception-guided exploration framework designed to overcome these challenges. The framework consists of two main modules. The Spatial Perception Enhancement (SPE) module integrates panoramic filtering with specialized door and region experts to produce spatially coherent, cross-view consistent perceptual representations. Building on this foundation, our Explored Multi-expert Reasoning (EMR) module uses parallel LLM experts to address waypoint-level semantics and region-level spatial transitions. When discrepancies arise between expert predictions, a query-and-explore mechanism is activated, prompting the agent to actively probe critical areas and resolve perceptual ambiguities. Experiments on VLN-CE demonstrate that Spatial VLN achieves state-of-the-art performance using only low-cost LLMs. Furthermore, to validate real-world applicability, we introduce a value-based waypoint sampling strategy that effectively bridges the Sim2Real gap. Extensive real-world evaluations confirm that our framework delivers superior generalization and robustness in complex environments. Our codes and videos are available at https://yueluhhxx.github.io/Spatial-VLN-web/.

</details>


### [434] [Delving Deeper: Hierarchical Visual Perception for Robust Video-Text Retrieval](https://arxiv.org/abs/2601.12768)
*Zequn Xie,Boyun Zhang,Yuxiao Lin,Tao Jin*

Main category: cs.CV

TL;DR: The paper proposes HVP-Net to improve video-text retrieval (VTR) by refining video feature layers in a hierarchical manner and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To overcome issues with redundancy and limited accuracy from current methods using coarse, final-layer features in video-text retrieval.

Method: Developed a framework, HVP-Net, that progressively refines intermediate video features using hierarchical visual perception, extracting salient concepts from raw video tokens.

Result: Achieved state-of-the-art performance on video-text retrieval benchmarks such as MSRVTT, DiDeMo, and ActivityNet.

Conclusion: Hierarchical feature exploitation significantly enhances video-text alignment, advancing the field of video-text retrieval.

Abstract: Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video's inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video semantics by extracting and refining features from multiple intermediate layers of a vision encoder. Our approach progressively distills salient visual concepts from raw patch-tokens at different semantic levels, mitigating redundancy while preserving crucial details for alignment. This results in a more robust video representation, leading to new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet. Our work validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval. Our codes are available at https://github.com/boyun-zhang/HVP-Net.

</details>


### [435] [Generalizable and Animatable 3D Full-Head Gaussian Avatar from a Single Image](https://arxiv.org/abs/2601.12770)
*Shuling Zhao,Dan Xu*

Main category: cs.CV

TL;DR: The paper introduces a framework for creating 3D animatable head avatars from a single image, achieving real-time animation and realistic results.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of creating realistic 3D animatable head avatars from a single image, especially under large camera pose variations.

Method: They propose a framework using Gaussian primitives embedded on a parametric face model's UV space, leveraging pretrained 3D GANs for geometry and texture learning, and leveraging image symmetry for combining local and global texture details.

Result: The method allows for high-quality 3D head modeling, real-time animation, and 360° views, achieving better realism and fidelity compared to existing methods.

Conclusion: The framework improves the realism and efficiency in the generation and animation of 3D head avatars, demonstrating its effectiveness through extensive experiments.

Abstract: Building 3D animatable head avatars from a single image is an important yet challenging problem. Existing methods generally collapse under large camera pose variations, compromising the realism of 3D avatars. In this work, we propose a new framework to tackle the novel setting of one-shot 3D full-head animatable avatar reconstruction in a single feed-forward pass, enabling real-time animation and simultaneous 360$^\circ$ rendering views. To facilitate efficient animation control, we model 3D head avatars with Gaussian primitives embedded on the surface of a parametric face model within the UV space. To obtain knowledge of full-head geometry and textures, we leverage rich 3D full-head priors within a pretrained 3D generative adversarial network (GAN) for global full-head feature extraction and multi-view supervision. To increase the fidelity of the 3D reconstruction of the input image, we take advantage of the symmetric nature of the UV space and human faces to fuse local fine-grained input image features with the global full-head textures. Extensive experiments demonstrate the effectiveness of our method, achieving high-quality 3D full-head modeling as well as real-time animation, thereby improving the realism of 3D talking avatars.

</details>


### [436] [Open Vocabulary Panoptic Segmentation With Retrieval Augmentation](https://arxiv.org/abs/2601.12779)
*Nafis Sadeq,Qingfeng Liu,Mostafa El-Khamy*

Main category: cs.CV

TL;DR: The paper introduces RetCLIP, a method for improving Open Vocabulary Panoptic Segmentation by augmenting it with a retrieval mechanism that enhances generalization to unseen classes.


<details>
  <summary>Details</summary>
Motivation: Open Vocabulary Panoptic Segmentation struggles with generalizing to unseen classes because traditional systems are trained only on specific datasets.

Method: The method involves constructing a database of masked segment features using paired image-text data and retrieving similar features during inference to enhance classification scores with retrieval-based and CLIP-based outputs.

Result: When integrated with a prior SOTA method (FC-CLIP) and trained on the COCO dataset, RetCLIP significantly improves performance on the ADE20k dataset by achieving +4.5 PQ, +2.5 mAP, and +10.0 mIoU over the baseline.

Conclusion: RetCLIP demonstrates a robust improvement in segmenting unseen classes by leveraging retrieval-augmented techniques, setting new benchmarks for Open Vocabulary Panoptic Segmentation.

Abstract: Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose RetCLIP, a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.

</details>


### [437] [SKANet: A Cognitive Dual-Stream Framework with Adaptive Modality Fusion for Robust Compound GNSS Interference Classification](https://arxiv.org/abs/2601.12791)
*Zhihan Zeng,Yang Zhao,Kaihe Wang,Dusit Niyato,Hongyuan Shu,Junchu Zhao,Yanjun Huang,Yue Xiu,Zhongpei Zhang,Ning Wei*

Main category: cs.CV

TL;DR: This paper introduces SKANet, a dual-stream deep learning framework for classifying compound GNSS jamming interference, achieving an accuracy of 96.99%.


<details>
  <summary>Details</summary>
Motivation: GNSS systems encounter increasing threats from complex compound jamming interference, which current methods struggle to classify due to conflicting feature extraction requirements.

Method: SKANet uses a dual-stream architecture combining Time-Frequency Images (TFIs) and Power Spectral Density (PSD), integrating Multi-Branch Selective Kernel modules and Asymmetric Convolution Blocks for adaptive receptive field adjustment.

Result: SKANet demonstrates an overall accuracy of 96.99% on a large dataset, showing remarkable robustness, especially in low JNR environments.

Conclusion: The proposed SKANet framework effectively addresses the challenges of compound jamming classification through adaptive feature integration and spatial-temporal adaptation techniques.

Abstract: As the electromagnetic environment becomes increasingly complex, Global Navigation Satellite Systems (GNSS) face growing threats from sophisticated jamming interference. Although Deep Learning (DL) effectively identifies basic interference, classifying compound interference remains difficult due to the superposition of diverse jamming sources. Existing single-domain approaches often suffer from performance degradation because transient burst signals and continuous global signals require conflicting feature extraction scales. We propose the Selective Kernel and Asymmetric convolution Network(SKANet), a cognitive deep learning framework built upon a dual-stream architecture that integrates Time-Frequency Images (TFIs) and Power Spectral Density (PSD). Distinct from conventional fusion methods that rely on static receptive fields, the proposed architecture incorporates a Multi-Branch Selective Kernel (SK) module combined with Asymmetric Convolution Blocks (ACBs). This mechanism enables the network to dynamically adjust its receptive fields, acting as an adaptive filter that simultaneously captures micro-scale transient features and macro-scale spectral trends within entangled compound signals. To complement this spatial-temporal adaptation, a Squeeze-and-Excitation (SE) mechanism is integrated at the fusion stage to adaptively recalibrate the contribution of heterogeneous features from each modality. Evaluations on a dataset of 405,000 samples demonstrate that SKANet achieves an overall accuracy of 96.99\%, exhibiting superior robustness for compound jamming classification, particularly under low Jamming-to-Noise Ratio (JNR) regimes.

</details>


### [438] [Combating Noisy Labels through Fostering Self- and Neighbor-Consistency](https://arxiv.org/abs/2601.12795)
*Zeren Sun,Yazhou Yao,Tongliang Liu,Zechao Li,Fumin Shen,Jinhui Tang*

Main category: cs.CV

TL;DR: This paper addresses label noise in deep learning via a novel method called Jo-SNC, which detects clean and noisy samples while leveraging consistency among samples.


<details>
  <summary>Details</summary>
Motivation: The authors aim to combat the challenges deep learning models face due to label noise, particularly the issues arising from imbalanced and out-of-distribution noisy data.

Method: The proposed Jo-SNC framework uses Jensen-Shannon divergence and neighbor-based information for clean sample identification, a self-adaptive thresholding scheme, and special training strategies (e.g., partial label learning, negative learning) alongside triplet consistency regularization.

Result: Experiments across multiple benchmarks and ablation studies reveal Jo-SNC’s superior performance compared to state-of-the-art methods.

Conclusion: Jo-SNC effectively addresses the label noise problem by enhancing clean sample selection reliability and improving model robustness through innovative consistency and training mechanisms.

Abstract: Label noise is pervasive in various real-world scenarios, posing challenges in supervised deep learning. Deep networks are vulnerable to such label-corrupted samples due to the memorization effect. One major stream of previous methods concentrates on identifying clean data for training. However, these methods often neglect imbalances in label noise across different mini-batches and devote insufficient attention to out-of-distribution noisy data. To this end, we propose a noise-robust method named Jo-SNC (\textbf{Jo}int sample selection and model regularization based on \textbf{S}elf- and \textbf{N}eighbor-\textbf{C}onsistency). Specifically, we propose to employ the Jensen-Shannon divergence to measure the ``likelihood'' of a sample being clean or out-of-distribution. This process factors in the nearest neighbors of each sample to reinforce the reliability of clean sample identification. We design a self-adaptive, data-driven thresholding scheme to adjust per-class selection thresholds. While clean samples undergo conventional training, detected in-distribution and out-of-distribution noisy samples are trained following partial label learning and negative learning, respectively. Finally, we advance the model performance further by proposing a triplet consistency regularization that promotes self-prediction consistency, neighbor-prediction consistency, and feature consistency. Extensive experiments on various benchmark datasets and comprehensive ablation studies demonstrate the effectiveness and superiority of our approach over existing state-of-the-art methods.

</details>


### [439] [PhyG-MoE: A Physics-Guided Mixture-of-Experts Framework for Energy-Efficient GNSS Interference Recognition](https://arxiv.org/abs/2601.12798)
*Zhihan Zeng,Yang Zhao,Kaihe Wang,Dusit Niyato,Yue Xiu,Lu Chen,Zhongpei Zhang,Ning Wei*

Main category: cs.CV

TL;DR: The paper introduces PhyG-MoE, a framework for optimizing GNSS interference recognition by dynamically adjusting model capacity based on signal complexity, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of static deep learning models in GNSS interference recognition, which suffer from inefficiencies and computational mismatches when handling signals of varying complexity.

Method: The PhyG-MoE framework employs a spectrum-based gating mechanism to dynamically route signals to either high-capacity or lightweight experts based on their spectral complexity, ensuring efficient processing.

Result: PhyG-MoE achieved an overall accuracy of 97.58% when evaluated on 21 jamming categories while reducing computational overhead.

Conclusion: This dynamic framework aligns computational resources with signal complexity, effectively tackling electromagnetic interference in resource-constrained environments without degrading performance.

Abstract: Complex electromagnetic interference increasingly compromises Global Navigation Satellite Systems (GNSS), threatening the reliability of Space-Air-Ground Integrated Networks (SAGIN). Although deep learning has advanced interference recognition, current static models suffer from a \textbf{fundamental limitation}: they impose a fixed computational topology regardless of the input's physical entropy. This rigidity leads to severe resource mismatch, where simple primitives consume the same processing cost as chaotic, saturated mixtures. To resolve this, this paper introduces PhyG-MoE (Physics-Guided Mixture-of-Experts), a framework designed to \textbf{dynamically align model capacity with signal complexity}. Unlike static architectures, the proposed system employs a spectrum-based gating mechanism that routes signals based on their spectral feature entanglement. A high-capacity TransNeXt expert is activated on-demand to disentangle complex features in saturated scenarios, while lightweight experts handle fundamental signals to minimize latency. Evaluations on 21 jamming categories demonstrate that PhyG-MoE achieves an overall accuracy of 97.58\%. By resolving the intrinsic conflict between static computing and dynamic electromagnetic environments, the proposed framework significantly reduces computational overhead without performance degradation, offering a viable solution for resource-constrained cognitive receivers.

</details>


### [440] [Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data](https://arxiv.org/abs/2601.12809)
*Takaki Yamamoto,Chihiro Noguchi,Toshihiro Tanizawa*

Main category: cs.CV

TL;DR: The paper explores how left-right spatial understanding emerges in CLIP-style vision-language models using a controlled testbed.


<details>
  <summary>Details</summary>
Motivation: The research aims to clarify whether vision-language models acquire spatial understanding and the mechanisms driving this competence.

Method: Lightweight Transformer encoders were trained with contrastive learning on 1D image-text tasks, while systematically varying label and layout diversity. Attention analysis was used to uncover mechanisms.

Result: Models successfully learned left-right relations. Label diversity was identified as more critical for generalization than layout diversity. Analysis showed that positional and token embeddings induce left-right differentiation.

Conclusion: CLIP-style models can learn spatial relations, and left-right relational understanding is influenced by specific positional mechanisms and label diversity during training.

Abstract: Spatial understanding remains a key challenge in vision-language models. Yet it is still unclear whether such understanding is truly acquired, and if so, through what mechanisms. We present a controllable 1D image-text testbed to probe how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. We train lightweight Transformer-based vision and text encoders end-to-end on paired descriptions of one- and two-object scenes and evaluate generalization to unseen object pairs while systematically varying label and layout diversity. We find that contrastive training learns left-right relations and that label diversity, more than layout diversity, is the primary driver of generalization in this setting. To gain the mechanistic understanding, we perform an attention decomposition and show that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders; ablating this contribution substantially reduces left-right discrimination. Our results provide a mechanistic insight of when and how CLIP-style models acquire relational competence.

</details>


### [441] [CSGaussian: Progressive Rate-Distortion Compression and Segmentation for 3D Gaussian Splatting](https://arxiv.org/abs/2601.12814)
*Yu-Jen Tseng,Chia-Hao Kao,Jing-Zhong Chen,Alessandro Gnutti,Shao-Yuan Lo,Yen-Yu Lin,Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: This work proposes a unified framework for combining compression and segmentation of 3D Gaussian Splatting (3DGS) to enhance applications like scene editing, significantly reducing transmission costs while maintaining high rendering and segmentation quality.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the gap where prior research treated compression and segmentation in 3D Gaussian Splatting separately, aiming to combine these processes for more efficient downstream applications such as scene editing.

Method: They integrate semantic learning within a compression pipeline using a lightweight neural representation-based hyperprior for efficient entropy coding, along with a compression-guided segmentation learning mechanism that utilizes quantization-awareness and quality-aware weighting.

Result: Extensive experiments on datasets like LERF and 3D-OVS demonstrate reduced transmission costs while maintaining high rendering quality and strong segmentation performance.

Conclusion: The proposed framework successfully unifies compression and segmentation for 3DGS, enabling efficient data transmission and facilitating advanced downstream tasks such as scene understanding and editing, showcasing its utility and effectiveness.

Abstract: We present the first unified framework for rate-distortion-optimized compression and segmentation of 3D Gaussian Splatting (3DGS). While 3DGS has proven effective for both real-time rendering and semantic scene understanding, prior works have largely treated these tasks independently, leaving their joint consideration unexplored. Inspired by recent advances in rate-distortion-optimized 3DGS compression, this work integrates semantic learning into the compression pipeline to support decoder-side applications--such as scene editing and manipulation--that extend beyond traditional scene reconstruction and view synthesis. Our scheme features a lightweight implicit neural representation-based hyperprior, enabling efficient entropy coding of both color and semantic attributes while avoiding costly grid-based hyperprior as seen in many prior works. To facilitate compression and segmentation, we further develop compression-guided segmentation learning, consisting of quantization-aware training to enhance feature separability and a quality-aware weighting mechanism to suppress unreliable Gaussian primitives. Extensive experiments on the LERF and 3D-OVS datasets demonstrate that our approach significantly reduces transmission cost while preserving high rendering quality and strong segmentation performance.

</details>


### [442] [A Generalist Foundation Model for Total-body PET/CT Enables Diagnostic Reporting and System-wide Metabolic Profiling](https://arxiv.org/abs/2601.12820)
*Wei Chen,Liang Wu,Shuyi Lu,Yuanyuan Sun,Wenkai Bi,Zilong Yuan,Yaoyao He,Feng Wang,Junchi Ma,Shuyong Liu,Zhaoping Cheng,Xiaoyan Hu,Jianfeng Qiu*

Main category: cs.CV

TL;DR: This paper introduces SDF-HOLO, a multimodal foundation model for holistic analysis of total-body PET/CT scans, achieving improved diagnostic accuracy and system-wide analysis by leveraging dual-stream encoding, cross-modal interaction, and voxel-mask-text alignment.


<details>
  <summary>Details</summary>
Motivation: Existing medical AI models struggle with the heterogeneous, large-scale, and complex data provided by total-body PET/CT scans, as they lack the capability to integrate detailed anatomical and metabolic signals or align them with structured clinical semantics.

Method: The authors developed SDF-HOLO, a model using dual-stream encoders for decoupled PET and CT representation learning, a cross-modal interaction module for enhanced data integration, hierarchical context modeling to capture dependencies across the entire body, and voxel-mask-text alignment using semantic anchors.

Result: SDF-HOLO outperformed strong baselines in tumor segmentation, lesion detection, and diagnostic report generation, reducing errors in localization and eliminating hallucinated findings. It enables a system-wide metabolic profiling to identify inter-organ metabolic interaction patterns.

Conclusion: SDF-HOLO provides a scalable and precise computational framework for total-body PET/CT diagnostics and offers insights into system-level precision oncology, demonstrating a significant leap in holistic medical imaging analysis.

Abstract: Total-body PET/CT enables system-wide molecular imaging, but heterogeneous anatomical and metabolic signals, approximately 2 m axial coverage, and structured radiology semantics challenge existing medical AI models that assume single-modality inputs, localized fields of view, and coarse image-text alignment. We introduce SDF-HOLO (Systemic Dual-stream Fusion Holo Model), a multimodal foundation model for holistic total-body PET/CT, pre-trained on more than 10,000 patients. SDF-HOLO decouples CT and PET representation learning with dual-stream encoders and couples them through a cross-modal interaction module, allowing anatomical context to refine PET aggregation while metabolic saliency guides subtle morphological reasoning. To model long-range dependencies across the body, hierarchical context modeling combines efficient local windows with global attention. To bridge voxels and clinical language, we use anatomical segmentation masks as explicit semantic anchors and perform voxel-mask-text alignment during pre-training. Across tumor segmentation, low-dose lesion detection, and multilingual diagnostic report generation, SDF-HOLO outperforms strong task-specific and clinical-reference baselines while reducing localization errors and hallucinated findings. Beyond focal interpretation, the model enables system-wide metabolic profiling and reveals tumor-associated fingerprints of inter-organ metabolic network interactions, providing a scalable computational foundation for total-body PET/CT diagnostics and system-level precision oncology.

</details>


### [443] [TreeDGS: Aerial Gaussian Splatting for Distant DBH Measurement](https://arxiv.org/abs/2601.12823)
*Belal Shaheen,Minh-Hieu Nguyen,Bach-Thuan Bui,Shubham,Tim Wu,Michael Fairley,Matthew David Zane,Michael Wu,James Tompkin*

Main category: cs.CV

TL;DR: This paper introduces TreeDGS, a method leveraging 3D Gaussian Splatting for accurate tree trunk measurements (DBH) from aerial images, surpassing LiDAR accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve object-level measurement of natural attributes (like tree DBH) from aerial imagery, as conventional methods face limitations in capturing sparse and distant details like tree trunks in forest scans.

Method: TreeDGS uses 3D Gaussian Splatting for scene representation and measurement. After initializing with SfM-MVS and optimizing Gaussians, it reconstructs dense geometry using depth-aware integration and assigns reliability scores to sampled points. DBH is estimated through opacity-weighted fitting on trunk-isolated points.

Result: TreeDGS achieves 4.79 cm RMSE in DBH estimation, surpassing a baseline LiDAR method (7.91 cm RMSE) on forest plots, proving its superior accuracy and cost-effectiveness.

Conclusion: TreeDGS demonstrates that densified splat-based geometry can achieve accurate and affordable large-scale forest attribute measurement, opening new possibilities for tree analytics through aerial sensing.

Abstract: Aerial remote sensing enables efficient large-area surveying, but accurate direct object-level measurement remains difficult in complex natural scenes. Recent advancements in 3D vision, particularly learned radiance-field representations such as NeRF and 3D Gaussian Splatting, have begun to raise the ceiling on reconstruction fidelity and densifiable geometry from posed imagery. Nevertheless, direct aerial measurement of important natural attributes such as tree diameter at breast height (DBH) remains challenging. Trunks in aerial forest scans are distant and sparsely observed in image views: at typical operating altitudes, stems may span only a few pixels. With these constraints, conventional reconstruction methods leave breast-height trunk geometry weakly constrained. We present TreeDGS, an aerial image reconstruction method that leverages 3D Gaussian Splatting as a continuous, densifiable scene representation for trunk measurement. After SfM-MVS initialization and Gaussian optimization, we extract a dense point set from the Gaussian field using RaDe-GS's depth-aware cumulative-opacity integration and associate each sample with a multi-view opacity reliability score. We then estimate DBH from trunk-isolated points using opacity-weighted solid-circle fitting. Evaluated on 10 plots with field-measured DBH, TreeDGS reaches 4.79,cm RMSE (about 2.6 pixels at this GSD) and outperforms a state-of-the-art LiDAR baseline (7.91,cm RMSE), demonstrating that densified splat-based geometry can enable accurate, low-cost aerial DBH measurement.

</details>


### [444] [Seeing Isn't Always Believing: Analysis of Grad-CAM Faithfulness and Localization Reliability in Lung Cancer CT Classification](https://arxiv.org/abs/2601.12826)
*Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: The paper examines limitations in Grad-CAM explanations in medical AI, focusing on its reliability across different models for lung cancer classification.


<details>
  <summary>Details</summary>
Motivation: To understand and critically assess the reliability of Grad-CAM, a popular XAI tool, for visualizing deep neural networks' decisions in medical image analysis.

Method: The study uses the IQ-OTH/NCCD dataset and evaluates Grad-CAM across five architectures: ResNet-50, ResNet-101, DenseNet-161, EfficientNet-B0, and ViT-Base. It introduces a quantitative evaluation framework combining localization accuracy, perturbation-based faithfulness, and explanation consistency.

Result: Grad-CAM is effective for highlighting salient tumor regions in convolutional networks but fails to maintain fidelity in Vision Transformer models due to non-local attention behavior. Additionally, cross-model variations highlight inconsistencies in what Grad-CAM represents as diagnostic evidence.

Conclusion: The findings underscore significant limitations in saliency-based XAI methods like Grad-CAM and advocate for model-aware, clinically meaningful interpretability approaches to ensure trustworthy medical AI explanations.

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as Gradient-weighted Class Activation Mapping (Grad-CAM), have become indispensable for visualizing the reasoning process of deep neural networks in medical image analysis. Despite their popularity, the faithfulness and reliability of these heatmap-based explanations remain under scrutiny. This study critically investigates whether Grad-CAM truly represents the internal decision-making of deep models trained for lung cancer image classification. Using the publicly available IQ-OTH/NCCD dataset, we evaluate five representative architectures: ResNet-50, ResNet-101, DenseNet-161, EfficientNet-B0, and ViT-Base-Patch16-224, to explore model-dependent variations in Grad-CAM interpretability. We introduce a quantitative evaluation framework that combines localization accuracy, perturbation-based faithfulness, and explanation consistency to assess Grad-CAM reliability across architectures. Experimental findings reveal that while Grad-CAM effectively highlights salient tumor regions in most convolutional networks, its interpretive fidelity significantly degrades for Vision Transformer models due to non-local attention behavior. Furthermore, cross-model comparisons indicate substantial variability in saliency localization, implying that Grad-CAM explanations may not always correspond to the true diagnostic evidence used by the networks. This work exposes critical limitations of current saliency-based XAI approaches in medical imaging and emphasizes the need for model-aware interpretability methods that are both computationally sound and clinically meaningful. Our findings aim to inspire a more cautious and rigorous adoption of visual explanation tools in medical AI, urging the community to rethink what it truly means to "trust" a model's explanation.

</details>


### [445] [FGTBT: Frequency-Guided Task-Balancing Transformer for Unified Facial Landmark Detection](https://arxiv.org/abs/2601.12863)
*Jun Wan,Xinyu Xiong,Ning Chen,Zhihui Lai,Jie Zhou,Wenwen Min*

Main category: cs.CV

TL;DR: The paper introduces a Frequency-Guided Task-Balancing Transformer (FGTBT) to improve facial landmark detection (FLD) in challenging conditions through innovative loss functions and frequency-domain modeling.


<details>
  <summary>Details</summary>
Motivation: Current FLD methods struggle with large pose variations, illumination changes, and expression variations, and are hindered by limited and less diverse datasets, leading to degraded performance.

Method: It proposes FGTBT, which combines a Fine-Grained Multi-Task Balancing loss (FMB-loss) for improved task balancing and a Frequency-Guided Structure-Aware (FGSA) model for structure constraints via frequency-domain modeling.

Result: The FGTBT framework improves detection accuracy and achieves performance on par with state-of-the-art methods, verified through extensive experiments on benchmark datasets.

Conclusion: FGTBT effectively addresses challenges in facial landmark detection by improving geometric structure perception and model training through its proposed FMB-loss and FGSA model.

Abstract: Recently, deep learning based facial landmark detection (FLD) methods have achieved considerable success. However, in challenging scenarios such as large pose variations, illumination changes, and facial expression variations, they still struggle to accurately capture the geometric structure of the face, resulting in performance degradation. Moreover, the limited size and diversity of existing FLD datasets hinder robust model training, leading to reduced detection accuracy. To address these challenges, we propose a Frequency-Guided Task-Balancing Transformer (FGTBT), which enhances facial structure perception through frequency-domain modeling and multi-dataset unified training. Specifically, we propose a novel Fine-Grained Multi-Task Balancing loss (FMB-loss), which moves beyond coarse task-level balancing by assigning weights to individual landmarks based on their occurrence across datasets. This enables more effective unified training and mitigates the issue of inconsistent gradient magnitudes. Additionally, a Frequency-Guided Structure-Aware (FGSA) model is designed to utilize frequency-guided structure injection and regularization to help learn facial structure constraints. Extensive experimental results on popular benchmark datasets demonstrate that the integration of the proposed FMB-loss and FGSA model into our FGTBT framework achieves performance comparable to state-of-the-art methods. The code is available at https://github.com/Xi0ngxinyu/FGTBT.

</details>


### [446] [Proxy Robustness in Vision Language Models is Effortlessly Transferable](https://arxiv.org/abs/2601.12865)
*Xiaowei Fu,Fuxiang Huang,Lei Zhang*

Main category: cs.CV

TL;DR: This paper introduces a novel framework, HPT-GPD, to transfer adversarial robustness in Vision-Language Models (VLMs) without requiring computationally expensive adversarial training.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of transferring adversarial robustness to VLMs, such as CLIP, as current methods demand significant computational resources and lead to overfitting issues.

Method: They propose the Heterogeneous Proxy Transfer (HPT) framework to leverage proxy adversarial robustness observed in CLIP variants. Additionally, the Generalization-Pivot Decoupling (GPD) approach balances adversarial robustness and natural generalization by modulating learning rate scheduling.

Result: Experiments on 15 zero-shot datasets verify the effectiveness of HPT-GPD in achieving both improved adversarial robustness and natural generalization in VLMs.

Conclusion: HPT-GPD effectively facilitates robustness transfer in VLMs through proxy models, mitigating computational challenges and overfitting, making it a practical solution for enhancing VLM defenses.

Abstract: As a pivotal technique for improving the defense of deep models, adversarial robustness transfer via distillation has demonstrated remarkable success in conventional image classification tasks. However, this paradigm encounters critical challenges when applied to vision-language models (VLM) (e.g., CLIP): constructing adversarially robust teacher for large-scale multi-modal models demands prohibitively high computational resources. We bridge this gap by revealing an interesting phenomenon: vanilla CLIP (without adversarial training) exhibits intrinsic defensive capabilities against adversarial examples generated by another CLIP with different architectures. We formally define this as proxy adversarial robustness, and naturally propose a Heterogeneous Proxy Transfer (HPT) framework that establishes cross-architectural robustness distillation channels between CLIP variants, effortlessly enabling the VLM robustness transfer from proxy to target models. Yet, such proxy transfer paradigm easily induces severe overfitting, leading to a sharp degradation in zero-shot natural generalization. To resolve that, we design Generalization-Pivot Decoupling (GPD) by leveraging the difference in learning rate scheduling. This decouples the proxy transfer process into a generalization-anchored warm-up that maintains generalization and a generalization-pulled HPT that promotes adversarial robustness, to achieve an equilibrium between natural generalization and adversarial robustness. Extensive experiments on 15 zero-shot datasets demonstrate the effectiveness of our HPT-GPD method. The code is available at the website of github.com/fxw13/HPT-GPD.

</details>


### [447] [Exploring Talking Head Models With Adjacent Frame Prior for Speech-Preserving Facial Expression Manipulation](https://arxiv.org/abs/2601.12876)
*Zhenxuan Lu,Zhihua Xu,Zhijing Yang,Feng Gao,Yongyi Lu,Keze Wang,Tianshui Chen*

Main category: cs.CV

TL;DR: The paper introduces a new framework, THFEM, combining audio-driven talking head generation models with speech-preserving facial expression manipulation to enhance lip synchronization and image realism.


<details>
  <summary>Details</summary>
Motivation: Improving speech-preserving facial expression manipulation, particularly addressing challenges in lip synchronization during expression changes.

Method: THFEM integrates AD-THG models with SPFEM, employing an adjacent frame learning strategy to finetune AD-THG models, enhancing frame quality and synchronization.

Result: The framework demonstrated effective preservation of mouth shapes alongside improved expression fidelity and realism in manipulated images.

Conclusion: Integrating AD-THG models with SPFEM using adjacent frame learning successfully enhances lip synchronization and image quality during facial expression manipulation.

Abstract: Speech-Preserving Facial Expression Manipulation (SPFEM) is an innovative technique aimed at altering facial expressions in images and videos while retaining the original mouth movements. Despite advancements, SPFEM still struggles with accurate lip synchronization due to the complex interplay between facial expressions and mouth shapes. Capitalizing on the advanced capabilities of audio-driven talking head generation (AD-THG) models in synthesizing precise lip movements, our research introduces a novel integration of these models with SPFEM. We present a new framework, Talking Head Facial Expression Manipulation (THFEM), which utilizes AD-THG models to generate frames with accurately synchronized lip movements from audio inputs and SPFEM-altered images. However, increasing the number of frames generated by AD-THG models tends to compromise the realism and expression fidelity of the images. To counter this, we develop an adjacent frame learning strategy that finetunes AD-THG models to predict sequences of consecutive frames. This strategy enables the models to incorporate information from neighboring frames, significantly improving image quality during testing. Our extensive experimental evaluations demonstrate that this framework effectively preserves mouth shapes during expression manipulations, highlighting the substantial benefits of integrating AD-THG with SPFEM.

</details>


### [448] [YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time Object Detection](https://arxiv.org/abs/2601.12882)
*Sudip Chakrabarty*

Main category: cs.CV

TL;DR: This paper introduces YOLOv26, a revamped real-time object detection framework that removes NMS post-processing and uses end-to-end learning strategies, outperforming its predecessors in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of latency and hyperparameter sensitivity caused by Non-Maximum Suppression (NMS) in traditional YOLO frameworks.

Method: YOLOv26 employs innovations such as the MuSGD optimizer, STAL for assignment in detection tasks, and ProgLoss for dynamic supervision, replacing NMS with native end-to-end training.

Result: YOLOv26 surpasses older YOLO versions and competitors like RTMDet and DAMO-YOLO in both detection speed and accuracy in benchmarks.

Conclusion: YOLOv26 resolves the trade-off between latency and precision, showcasing the potential for end-to-end learning in next-generation computer vision models.

Abstract: The "You Only Look Once" (YOLO) framework has long served as the benchmark for real-time object detection, yet traditional iterations (YOLOv1 through YOLO11) remain constrained by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing. This paper analyzes a comprehensive analysis of YOLO26, an architecture that fundamentally redefines this paradigm by eliminating NMS in favor of a native end-to-end learning strategy. This study examines the critical innovations that enable this transition, specifically the introduction of the MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision. Through a systematic review of official performance benchmarks, the results demonstrate that YOLO26 establishes a new Pareto front, outperforming a comprehensive suite of predecessors and state-of-the-art competitors (including RTMDet and DAMO-YOLO) in both inference speed and detection accuracy. The analysis confirms that by decoupling representation learning from heuristic post-processing, YOLOv26 successfully resolves the historical trade-off between latency and precision, signaling the next evolutionary step in edge-based computer vision.

</details>


### [449] [Simultaneous Detection of LSD and FMD in Cattle Using Ensemble Deep Learning](https://arxiv.org/abs/2601.12889)
*Nazibul Basar Ayon,Abdul Hasib,Md. Faishal Ahmed,Md. Sadiqur Rahman,Kamrul Islam,T. M. Mehrab Hasan,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: This paper develops a deep learning model that integrates multiple frameworks for precise detection of two contagious cattle diseases, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of symptom overlap between LSD, FMD, and benign conditions to enable accurate and timely disease diagnosis.

Method: An Ensemble Deep Learning model combining VGG16, ResNet50, and InceptionV3 with optimized weighted averaging was applied to a dataset of 10,516 annotated images.

Result: The model achieved 98.2% accuracy, macro-averaged scores of precision at 98.2%, recall at 98.1%, an F1-score of 98.1%, and an AUC-ROC of 99.5%.

Conclusion: This work demonstrates early, accurate diagnosis of LSD and FMD, potentially improving disease management and agricultural sustainability worldwide, especially in resource-limited settings.

Abstract: Lumpy Skin Disease (LSD) and Foot-and-Mouth Disease (FMD) are highly contagious viral diseases affecting cattle, causing significant economic losses and welfare challenges. Their visual diagnosis is complicated by significant symptom overlap with each other and with benign conditions like insect bites or chemical burns, hindering timely control measures. Leveraging a comprehensive dataset of 10,516 expert-annotated images from 18 farms across India, Brazil, and the USA, this study presents a novel Ensemble Deep Learning framework integrating VGG16, ResNet50, and InceptionV3 with optimized weighted averaging for simultaneous LSD and FMD detection. The model achieves a state-of-the-art accuracy of 98.2\%, with macro-averaged precision of 98.2\%, recall of 98.1\%, F1-score of 98.1\%, and an AUC-ROC of 99.5\%. This approach uniquely addresses the critical challenge of symptom overlap in multi-disease detection, enabling early, precise, and automated diagnosis. This tool has the potential to enhance disease management, support global agricultural sustainability, and is designed for future deployment in resource-limited settings.

</details>


### [450] [TwoHead-SwinFPN: A Unified DL Architecture for Synthetic Manipulation, Detection and Localization in Identity Documents](https://arxiv.org/abs/2601.12895)
*Chan Naseeb,Adeel Ashraf Cheema,Hassan Sami,Tayyab Afzal,Muhammad Omair,Usman Habib*

Main category: cs.CV

TL;DR: The paper proposes TwoHead-SwinFPN, a model focusing on identifying and localizing manipulations in identity documents using generative AI, achieving strong classification and localization accuracy.


<details>
  <summary>Details</summary>
Motivation: Increasing sophistication of generative AI models heightens the vulnerabilities of identity documents to manipulations, necessitating robust detection solutions.

Method: The paper introduces a dual-head architecture leveraging Swin Transformer, FPN, UNet-style decoder, CBAM, and uncertainty-weighted multi-task learning to detect and localize manipulations in ID documents.

Result: The model achieves 84.31% accuracy, 90.78% AUC for classification, 57.24% mean Dice score for localization, and 88.61% F1-score for binary classification on the FantasyIDiap dataset.

Conclusion: The approach is validated to deliver high detection accuracy with efficiency for real-world application, supported through a FastAPI deployment and evaluations including ablation, multi-language, and cross-device testing.

Abstract: The proliferation of sophisticated generative AI models has significantly escalated the threat of synthetic manipulations in identity documents, particularly through face swapping and text inpainting attacks. This paper presents TwoHead-SwinFPN, a unified deep learning architecture that simultaneously performs binary classification and precise localization of manipulated regions in ID documents. Our approach integrates a Swin Transformer backbone with Feature Pyramid Network (FPN) and UNet-style decoder, enhanced with Convolutional Block Attention Module (CBAM) for improved feature representation. The model employs a dual-head architecture for joint optimization of detection and segmentation tasks, utilizing uncertainty-weighted multi-task learning. Extensive experiments on the FantasyIDiap dataset demonstrate superior performance with 84.31\% accuracy, 90.78\% AUC for classification, and 57.24\% mean Dice score for localization. The proposed method achieves an F1-score of 88.61\% for binary classification while maintaining computational efficiency suitable for real-world deployment through FastAPI implementation. Our comprehensive evaluation includes ablation studies, cross-device generalization analysis, and detailed performance assessment across 10 languages and 3 acquisition devices.

</details>


### [451] [Supervision-by-Hallucination-and-Transfer: A Weakly-Supervised Approach for Robust and Precise Facial Landmark Detection](https://arxiv.org/abs/2601.12919)
*Jun Wan,Yuanzhi Yao,Zhihui Lai,Jie Zhou,Xianxu Hou,Wenwen Min*

Main category: cs.CV

TL;DR: The paper proposes a weakly-supervised framework for facial landmark detection (FLD) that improves methods for low-resolution images and a lack of precision in training data using innovative techniques.


<details>
  <summary>Details</summary>
Motivation: Current FLD methods struggle with low-resolution inputs, insufficient training data, and inaccurate annotations, limiting detection accuracy.

Method: The paper introduces Supervision-by-Hallucination-and-Transfer (SHT), which includes Dual Hallucination Learning Network (DHLN) for improving details on low-resolution images and Facial Pose Transfer Network (FPTN) for refining facial landmark heatmaps via pose transformation.

Result: The proposed method outperforms state-of-the-art techniques in both facial landmark detection and face hallucination tasks.

Conclusion: Integrating hallucination and pose transfer tasks into a weakly-supervised FLD framework significantly increases accuracy and robustness, making it pioneering in this area of research.

Abstract: High-precision facial landmark detection (FLD) relies on high-resolution deep feature representations. However, low-resolution face images or the compression (via pooling or strided convolution) of originally high-resolution images hinder the learning of such features, thereby reducing FLD accuracy. Moreover, insufficient training data and imprecise annotations further degrade performance. To address these challenges, we propose a weakly-supervised framework called Supervision-by-Hallucination-and-Transfer (SHT) for more robust and precise FLD. SHT contains two novel mutually enhanced modules: Dual Hallucination Learning Network (DHLN) and Facial Pose Transfer Network (FPTN). By incorporating FLD and face hallucination tasks, DHLN is able to learn high-resolution representations with low-resolution inputs for recovering both facial structures and local details and generating more effective landmark heatmaps. Then, by transforming faces from one pose to another, FPTN can further improve landmark heatmaps and faces hallucinated by DHLN for detecting more accurate landmarks. To the best of our knowledge, this is the first study to explore weakly-supervised FLD by integrating face hallucination and facial pose transfer tasks. Experimental results of both face hallucination and FLD demonstrate that our method surpasses state-of-the-art techniques.

</details>


### [452] [Dual-Stream Collaborative Transformer for Image Captioning](https://arxiv.org/abs/2601.12926)
*Jun Wan,Jun Liu,Zhihui lai,Jie Zhou*

Main category: cs.CV

TL;DR: The paper introduces a novel Dual-Stream Collaborative Transformer (DSCT) for image captioning, leveraging segmentation and region features to improve caption accuracy and relevance.


<details>
  <summary>Details</summary>
Motivation: To address issues in current image captioning methods, such as generating irrelevant descriptions due to a lack of contextual information and over-reliance on partial captions.

Method: The DSCT model combines region and segmentation features using Pattern-Specific Mutual Attention Encoders (PSMAEs) and Dynamic Nomination Decoders (DNDs) to dynamically resolve inconsistencies and create descriptive captions.

Result: Experimental results demonstrate DSCT outperforms state-of-the-art models on popular image captioning benchmark datasets.

Conclusion: The DSCT effectively utilizes complementary features dynamically for image caption generation, resolving spatial and semantic misalignment issues and advancing the field.

Abstract: Current region feature-based image captioning methods have progressed rapidly and achieved remarkable performance. However, they are still prone to generating irrelevant descriptions due to the lack of contextual information and the over-reliance on generated partial descriptions for predicting the remaining words. In this paper, we propose a Dual-Stream Collaborative Transformer (DSCT) to address this issue by introducing the segmentation feature. The proposed DSCT consolidates and then fuses the region and segmentation features to guide the generation of caption sentences. It contains multiple Pattern-Specific Mutual Attention Encoders (PSMAEs) and Dynamic Nomination Decoders (DNDs). The PSMAE effectively highlights and consolidates the private information of two representations by querying each other. The DND dynamically searches for the most relevant learning blocks to the input textual representations and exploits the homogeneous features between the consolidated region and segmentation features to generate more accurate and descriptive caption sentences. To the best of our knowledge, this is the first study to explore how to fuse different pattern-specific features in a dynamic way to bypass their semantic inconsistencies and spatial misalignment issues for image captioning. The experimental results from popular benchmark datasets demonstrate that our DSCT outperforms the state-of-the-art image captioning models in the literature.

</details>


### [453] [Membership Inference Test: Auditing Training Data in Object Classification Models](https://arxiv.org/abs/2601.12929)
*Gonzalo Mancera,Daniel DeAlcala,Aythami Morales,Ruben Tolosana,Julian Fierrez*

Main category: cs.CV

TL;DR: This paper proposes Membership Inference Test (MINT) architectures for object recognition, achieving precision rates of 70%-80% in identifying training data.


<details>
  <summary>Details</summary>
Motivation: To develop tailored solutions for Membership Inference Tests (MINT) in the object recognition domain for optimized data utilization and transparent training mechanisms.

Method: Designing specialized architectures for MINT models using convolutional layers. Experiments performed on three public databases (174K images) involved object detection models and embedding extractors.

Result: Achieved precision rates of 70%-80% in identifying data used during training, contingent on the detection module layer depth.

Conclusion: The study provides insights into MINT module performance and factors influencing transparent training processes in object recognition.

Abstract: In this research, we analyze the performance of Membership Inference Tests (MINT), focusing on determining whether given data were utilized during the training phase, specifically in the domain of object recognition. Within the area of object recognition, we propose and develop architectures tailored for MINT models. These architectures aim to optimize performance and efficiency in data utilization, offering a tailored solution to tackle the complexities inherent in the object recognition domain. We conducted experiments involving an object detection model, an embedding extractor, and a MINT module. These experiments were performed in three public databases, totaling over 174K images. The proposed architecture leverages convolutional layers to capture and model the activation patterns present in the data during the training process. Through our analysis, we are able to identify given data used for testing and training, achieving precision rates ranging between 70% and 80%, contingent upon the depth of the detection module layer chosen for input to the MINT module. Additionally, our studies entail an analysis of the factors influencing the MINT Module, delving into the contributing elements behind more transparent training processes.

</details>


### [454] [QASA: Quality-Guided K-Adaptive Slot Attention for Unsupervised Object-Centric Learning](https://arxiv.org/abs/2601.12936)
*Tianran Ouyang,Xingping Dong,Jing Zhang,Mang Ye,Jun Chen,Bo Du*

Main category: cs.CV

TL;DR: The paper introduces Quality-Guided K-Adaptive Slot Attention (QASA) to improve unsupervised object-centric learning by dynamically selecting high-quality slots and surpassing prior methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing K-adaptive methods in Slot Attention, such as low-quality slot binding and conflicting slot-count penalty with reconstruction objectives.

Method: QASA decouples slot selection from reconstruction. It uses an unsupervised Slot-Quality metric to evaluate slot quality and dynamically selects high-quality slots for training. A gated decoder and token-wise competition enable K-adaptive outcomes.

Result: QASA achieves significant performance improvements over both K-adaptive and K-fixed methods, on real and synthetic datasets, including real-world surpassing.

Conclusion: QASA resolves constraints in K-adaptive Slot Attention, providing better dynamic object-slot binding and sets a new standard in performance for object-centric learning.

Abstract: Slot Attention, an approach that binds different objects in a scene to a set of "slots", has become a leading method in unsupervised object-centric learning. Most methods assume a fixed slot count K, and to better accommodate the dynamic nature of object cardinality, a few works have explored K-adaptive variants. However, existing K-adaptive methods still suffer from two limitations. First, they do not explicitly constrain slot-binding quality, so low-quality slots lead to ambiguous feature attribution. Second, adding a slot-count penalty to the reconstruction objective creates conflicting optimization goals between reducing the number of active slots and maintaining reconstruction fidelity. As a result, they still lag significantly behind strong K-fixed baselines. To address these challenges, we propose Quality-Guided K-Adaptive Slot Attention (QASA). First, we decouple slot selection from reconstruction, eliminating the mutual constraints between the two objectives. Then, we propose an unsupervised Slot-Quality metric to assess per-slot quality, providing a principled signal for fine-grained slot--object binding. Based on this metric, we design a Quality-Guided Slot Selection scheme that dynamically selects a subset of high-quality slots and feeds them into our newly designed gated decoder for reconstruction during training. At inference, token-wise competition on slot attention yields a K-adaptive outcome. Experiments show that QASA substantially outperforms existing K-adaptive methods on both real and synthetic datasets. Moreover, on real-world datasets QASA surpasses K-fixed methods.

</details>


### [455] [GazeD: Context-Aware Diffusion for Accurate 3D Gaze Estimation](https://arxiv.org/abs/2601.12948)
*Riccardo Catalini,Davide Di Nucci,Guido Borghi,Davide Davoli,Lorenzo Garattoni,Giampiero Francesca,Yuki Kawana,Roberto Vezzani*

Main category: cs.CV

TL;DR: GazeD jointly estimates 3D gaze and human pose from a single RGB image using diffusion models, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To improve 3D gaze estimation by modeling gaze as part of human pose and leveraging uncertainty-handling diffusion models.

Method: GazeD applies diffusion models conditioned on 2D pose and scene context, while representing gaze as a fixed-distance body joint from the eyes.

Result: GazeD surpasses previous methods across three benchmark datasets, achieving state-of-the-art performance in 3D gaze estimation.

Conclusion: GazeD advances 3D gaze estimation by integrating pose estimation and leveraging diffusion models for uncertainty, achieving superior accuracy.

Abstract: We introduce GazeD, a new 3D gaze estimation method that jointly provides 3D gaze and human pose from a single RGB image. Leveraging the ability of diffusion models to deal with uncertainty, it generates multiple plausible 3D gaze and pose hypotheses based on the 2D context information extracted from the input image. Specifically, we condition the denoising process on the 2D pose, the surroundings of the subject, and the context of the scene. With GazeD we also introduce a novel way of representing the 3D gaze by positioning it as an additional body joint at a fixed distance from the eyes. The rationale is that the gaze is usually closely related to the pose, and thus it can benefit from being jointly denoised during the diffusion process. Evaluations across three benchmark datasets demonstrate that GazeD achieves state-of-the-art performance in 3D gaze estimation, even surpassing methods that rely on temporal information. Project details will be available at https://aimagelab.ing.unimore.it/go/gazed.

</details>


### [456] [StyMam: A Mamba-Based Generator for Artistic Style Transfer](https://arxiv.org/abs/2601.12954)
*Zhou Hong,Rongsheng Hu,Yicheng Di,Xiaolong Xu,Ning Dong,Yihua Shao,Run Ling,Yun Wang,Juqin Wang,Zhanjie Zhang,Ao Ma*

Main category: cs.CV

TL;DR: This paper proposes StyMam, a novel GAN-based generator that introduces specific mechanisms to improve image style transfer, outperforming current methods in quality and speed.


<details>
  <summary>Details</summary>
Motivation: There are limitations in existing image style transfer methods—GAN-based approaches fail to harmonize local and global dependencies, while SD-based approaches compromise content preservation and have low inference speeds.

Method: The authors propose a new GAN-based generator called StyMam, incorporating a residual dual-path strip scanning mechanism for local texture capture and a channel-reweighted spatial attention module for modeling global dependencies.

Result: Experimental results indicate that StyMam produces high-quality stylized images with improved speed, outperforming existing state-of-the-art methods.

Conclusion: StyMam effectively addresses artifacts, disharmonious patterns, content preservation, and speed, establishing itself as a superior alternative for image style transfer.

Abstract: Image style transfer aims to integrate the visual patterns of a specific artistic style into a content image while preserving its content structure. Existing methods mainly rely on the generative adversarial network (GAN) or stable diffusion (SD). GAN-based approaches using CNNs or Transformers struggle to jointly capture local and global dependencies, leading to artifacts and disharmonious patterns. SD-based methods reduce such issues but often fail to preserve content structures and suffer from slow inference. To address these issues, we revisit GAN and propose a mamba-based generator, termed as StyMam, to produce high-quality stylized images without introducing artifacts and disharmonious patterns. Specifically, we introduce a mamba-based generator with a residual dual-path strip scanning mechanism and a channel-reweighted spatial attention module. The former efficiently captures local texture features, while the latter models global dependencies. Finally, extensive qualitative and quantitative experiments demonstrate that the proposed method outperforms state-of-the-art algorithms in both quality and speed.

</details>


### [457] [Cross-Scale Pretraining: Enhancing Self-Supervised Learning for Low-Resolution Satellite Imagery for Semantic Segmentation](https://arxiv.org/abs/2601.12964)
*John Waithaka,Gustave Bwirayesu,Moise Busogi*

Main category: cs.CV

TL;DR: The paper introduces a spatial affinity component to self-supervised learning frameworks to enhance mid-resolution (MR) image representation by leveraging high-resolution (HR) datasets, improving downstream MR segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve representation learning for mid-resolution imagery tasks by integrating high-resolution datasets in self-supervised pretraining, addressing a gap in effectively incorporating HR data for MR tasks.

Method: A spatial affinity component is developed and integrated into existing self-supervised learning frameworks, utilizing HR imagery to improve the quality of MR image representations.

Result: The proposed spatial affinity component demonstrates better performance in downstream MR segmentation tasks compared to models pretrained with either HR or MR images separately.

Conclusion: Incorporation of HR datasets into self-supervised frameworks via spatial affinity enhances representation learning and segmentation performance for mid-resolution remote sensing tasks.

Abstract: Self-supervised pretraining in remote sensing is mostly done using mid-spatial resolution (MR) image datasets due to their high availability. Given the release of high-resolution (HR) datasets, we ask how HR datasets can be included in self-supervised pretraining to enhance MR image representation learning and downstream segmentation performance on MR tasks. We design a spatial affinity component that can be added to existing self-supervised learning frameworks and that uses HR imagery to learn better representations of MR imagery. We test the spatial affinity component on two self-supervised learning frameworks and show that it outperforms models pretrained on HR or MR images alone.

</details>


### [458] [Early Prediction of Type 2 Diabetes Using Multimodal data and Tabular Transformers](https://arxiv.org/abs/2601.12981)
*Sulaiman Khan,Md. Rafiul Biswas,Zubair Shah*

Main category: cs.CV

TL;DR: The paper presents a Tabular Transformer (TabTrans) model for early prediction of Type 2 Diabetes Mellitus using longitudinal health records and bone-related data, validated with superior results on the Qatar BioBank dataset.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve early risk prediction of T2DM by leveraging complex tabular and longitudinal health data that traditional models often struggle to analyze effectively.

Method: A TabTrans architecture is employed to analyze longitudinal electronic health records (EHR) and DXA data. SMOTE and SMOTE-ENN resampling methods address class imbalance when validating the model against conventional ML and generative AI models.

Result: TabTrans demonstrated superior performance, achieving an ROC AUC ≥ 79.7%, and identified key risk indicators like VAT mass, bone density metrics, and T/Z-scores as predictors of diabetes.

Conclusion: TabTrans is a robust tool for predictive analysis in healthcare, offering potential for proactive T2DM management and personalized treatment strategies in specific populations like those in Qatar.

Abstract: This study introduces a novel approach for early Type 2 Diabetes Mellitus (T2DM) risk prediction using a tabular transformer (TabTrans) architecture to analyze longitudinal patient data. By processing patients` longitudinal health records and bone-related tabular data, our model captures complex, long-range dependencies in disease progression that conventional methods often overlook. We validated our TabTrans model on a retrospective Qatar BioBank (QBB) cohort of 1,382 subjects, comprising 725 men (146 diabetic, 579 healthy) and 657 women (133 diabetic, 524 healthy). The study integrated electronic health records (EHR) with dual-energy X-ray absorptiometry (DXA) data. To address class imbalance, we employed SMOTE and SMOTE-ENN resampling techniques. The proposed model`s performance is evaluated against conventional machine learning (ML) and generative AI models, including Claude 3.5 Sonnet (Anthropic`s constitutional AI), GPT-4 (OpenAI`s generative pre-trained transformer), and Gemini Pro (Google`s multimodal language model). Our TabTrans model demonstrated superior predictive performance, achieving ROC AUC $\geq$ 79.7 % for T2DM prediction compared to both generative AI models and conventional ML approaches. Feature interpretation analysis identified key risk indicators, with visceral adipose tissue (VAT) mass and volume, ward bone mineral density (BMD) and bone mineral content (BMC), T and Z-scores, and L1-L4 scores emerging as the most important predictors associated with diabetes development in Qatari adults. These findings demonstrate the significant potential of TabTrans for analyzing complex tabular healthcare data, providing a powerful tool for proactive T2DM management and personalized clinical interventions in the Qatari population.
  Index Terms: tabular transformers, multimodal data, DXA data, diabetes, T2DM, feature interpretation, tabular data

</details>


### [459] [AsyncBEV: Cross-modal Flow Alignment in Asynchronous 3D Object Detection](https://arxiv.org/abs/2601.12994)
*Shiming Wang,Holger Caesar,Liangliang Nan,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: This paper introduces AsyncBEV, a module to enhance 3D BEV object detection models' robustness against sensor asynchrony, particularly useful in autonomous driving scenarios.


<details>
  <summary>Details</summary>
Motivation: Sensor asynchrony in autonomous driving systems can degrade dynamic object detection due to hardware, network, or processing delays, necessitating a solution to improve perception performance.

Method: The paper proposes AsyncBEV, a lightweight module that estimates 2D flow from BEV features of sensors with known time offsets, warping feature maps to align them spatially. It integrates into BEV detector architectures like grid-based or token-based models.

Result: Experiments show AsyncBEV improves LiDAR and camera synchronization robustness, especially for dynamic objects, achieving significant performance gains (16.6% and 11.9% NDS) compared to baselines under severe time offsets.

Conclusion: AsyncBEV offers a trainable, generic solution to mitigate performance drops from sensor asynchrony, enhancing detection reliability in autonomous driving.

Abstract: In autonomous driving, multi-modal perception tasks like 3D object detection typically rely on well-synchronized sensors, both at training and inference. However, despite the use of hardware- or software-based synchronization algorithms, perfect synchrony is rarely guaranteed: Sensors may operate at different frequencies, and real-world factors such as network latency, hardware failures, or processing bottlenecks often introduce time offsets between sensors. Such asynchrony degrades perception performance, especially for dynamic objects. To address this challenge, we propose AsyncBEV, a trainable lightweight and generic module to improve the robustness of 3D Birds' Eye View (BEV) object detection models against sensor asynchrony. Inspired by scene flow estimation, AsyncBEV first estimates the 2D flow from the BEV features of two different sensor modalities, taking into account the known time offset between these sensor measurements. The predicted feature flow is then used to warp and spatially align the feature maps, which we show can easily be integrated into different current BEV detector architectures (e.g., BEV grid-based and token-based). Extensive experiments demonstrate AsyncBEV improves robustness against both small and large asynchrony between LiDAR or camera sensors in both the token-based CMT and grid-based UniBEV, especially for dynamic objects. We significantly outperform the ego motion compensated CMT and UniBEV baselines, notably by $16.6$ % and $11.9$ % NDS on dynamic objects in the worst-case scenario of a $0.5 s$ time offset. Code will be released upon acceptance.

</details>


### [460] [Think3D: Thinking with Space for Spatial Reasoning](https://arxiv.org/abs/2601.13029)
*Zaibin Zhang,Yuhan Wu,Lianjie Jia,Yifan Wang,Zhongbo Zhang,Yijiang Li,Binghao Ran,Fuxi Zhang,Zhuohan Sun,Zhenfei Yin,Lijun Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: The paper introduces Think3D, a framework that equips vision models (VLMs) with 3D spatial reasoning capabilities, improving performance on spatial reasoning benchmarks without additional training.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of vision large models (VLMs) in performing genuine 3D spatial reasoning as they are inherently 2D perceivers.

Method: Think3D leverages 3D reconstruction models to recover point clouds and camera poses, enabling interactive spatial reasoning through operations like camera manipulation and ego/global-view switching.

Result: The framework improves spatial reasoning tasks in advanced and smaller models, achieving significant gains on benchmarks like BLINK Multi-view (+7.8%) and VSI-Bench (+4.7%). Smaller models benefited more with reinforcement learning policies for operations.

Conclusion: Think3D demonstrates that training-free, tool-augmented spatial reasoning is possible, offering a promising step towards more human-like intelligence in multimodal agents.

Abstract: Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.

</details>


### [461] [GridNet-HD: A High-Resolution Multi-Modal Dataset for LiDAR-Image Fusion on Power Line Infrastructure](https://arxiv.org/abs/2601.13052)
*Antoine Carreaud,Shanci Li,Malo De Lacour,Digre Frinde,Jan Skaloud,Adrien Gressin*

Main category: cs.CV

TL;DR: The paper introduces GridNet-HD, a dataset for 3D semantic segmentation of electrical infrastructures using high-density LiDAR and high-resolution imagery, along with method baselines showing advantages of multi-modal fusion.


<details>
  <summary>Details</summary>
Motivation: To address the lack of publicly available datasets that combine high-density LiDAR and high-resolution imagery with 3D semantic labels, particularly for power-line assets.

Method: Created a multi-modal dataset (GridNet-HD) including 7,694 images, 2.5 billion annotated points in 11 classes, with baseline models for unimodal and multi-modal approaches.

Result: Multi-modal fusion models significantly outperform unimodal models, with a 5.55 mIoU improvement, showcasing the complementarity of LiDAR and imagery.

Conclusion: GridNet-HD is a significant contribution that enables advanced 3D semantic segmentation while emphasizing the value of combining geometric and visual data. Data and resources are publicly available.

Abstract: This paper presents GridNet-HD, a multi-modal dataset for 3D semantic segmentation of overhead electrical infrastructures, pairing high-density LiDAR with high-resolution oblique imagery. The dataset comprises 7,694 images and 2.5 billion points annotated into 11 classes, with predefined splits and mIoU metrics. Unimodal (LiDAR-only, image-only) and multi-modal fusion baselines are provided. On GridNet-HD, fusion models outperform the best unimodal baseline by +5.55 mIoU, highlighting the complementarity of geometry and appearance. As reviewed in Sec. 2, no public dataset jointly provides high-density LiDAR and high-resolution oblique imagery with 3D semantic labels for power-line assets. Dataset, baselines, and codes are available: https://huggingface.co/collections/heig-vd-geo/gridnet-hd.

</details>


### [462] [Prototype Learning-Based Few-Shot Segmentation for Low-Light Crack on Concrete Structures](https://arxiv.org/abs/2601.13059)
*Yulun Guo*

Main category: cs.CV

TL;DR: The paper proposes a deep learning method for low-light crack detection by combining Retinex theory with few-shot learning, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Traditional crack detection struggles with accuracy in low-light conditions, and pixel-level annotation of such images is too time-consuming.

Method: They introduce a dual-branch prototype learning network integrating Retinex-based reflectance for illumination invariance, and a metric learning approach to minimize large dataset requirements.

Result: The approach demonstrates superior segmentation performance in low-light conditions across multiple benchmark datasets.

Conclusion: The proposed method effectively enhances crack detection in challenging low-light settings with minimal data annotation requirements, showing strong potential for real-world infrastructure monitoring.

Abstract: Crack detection is critical for concrete infrastructure safety, but real-world cracks often appear in low-light environments like tunnels and bridge undersides, degrading computer vision segmentation accuracy. Pixel-level annotation of low-light crack images is extremely time-consuming, yet most deep learning methods require large, well-illuminated datasets. We propose a dual-branch prototype learning network integrating Retinex theory with few-shot learning for low-light crack segmentation. Retinex-based reflectance components guide illumination-invariant global representation learning, while metric learning reduces dependence on large annotated datasets. We introduce a cross-similarity prior mask generation module that computes high-dimensional similarities between query and support features to capture crack location and structure, and a multi-scale feature enhancement module that fuses multi-scale features with the prior mask to alleviate spatial inconsistency. Extensive experiments on multiple benchmarks demonstrate consistent state-of-the-art performance under low-light conditions. Code: https://github.com/YulunGuo/CrackFSS.

</details>


### [463] [Patient-Conditioned Adaptive Offsets for Reliable Diagnosis across Subgroups](https://arxiv.org/abs/2601.13094)
*Gelei Xu,Yuying Duan,Jun Xia,Ruining Deng,Wei Jin,Yiyu Shi*

Main category: cs.CV

TL;DR: HyperAdapt is a proposed framework that enhances the reliability of AI-based medical diagnosis for diverse patient subgroups without compromising overall accuracy by employing patient-context encoding.


<details>
  <summary>Details</summary>
Motivation: Medical diagnosis AI models have disparities in performance across patient populations due to differences in disease prevalence and patient clinical profiles. Removing sensitive attributes, which can be diagnostic, hampers accuracy.

Method: HyperAdapt incorporates patient-specific attributes (like age and sex) into embeddings used in modular hypernetworks to adjust layers of a shared backbone, maintaining core medical knowledge while reflecting subgroup variability.

Result: Performance improves in subgroup-specific outcomes and overall accuracy on medical benchmarks, with substantial gains in recall and F1 scores, especially for underrepresented groups.

Conclusion: HyperAdapt enhances both fairness and performance across patient subgroups while preserving the shared diagnostic model’s reliability, showing promise for clinical application.

Abstract: AI models for medical diagnosis often exhibit uneven performance across patient populations due to heterogeneity in disease prevalence, imaging appearance, and clinical risk profiles. Existing algorithmic fairness approaches typically seek to reduce such disparities by suppressing sensitive attributes. However, in medical settings these attributes often carry essential diagnostic information, and removing them can degrade accuracy and reliability, particularly in high-stakes applications. In contrast, clinical decision making explicitly incorporates patient context when interpreting diagnostic evidence, suggesting a different design direction for subgroup-aware models. In this paper, we introduce HyperAdapt, a patient-conditioned adaptation framework that improves subgroup reliability while maintaining a shared diagnostic model. Clinically relevant attributes such as age and sex are encoded into a compact embedding and used to condition a hypernetwork-style module, which generates small residual modulation parameters for selected layers of a shared backbone. This design preserves the general medical knowledge learned by the backbone while enabling targeted adjustments that reflect patient-specific variability. To ensure efficiency and robustness, adaptations are constrained through low-rank and bottlenecked parameterizations, limiting both model complexity and computational overhead. Experiments across multiple public medical imaging benchmarks demonstrate that the proposed approach consistently improves subgroup-level performance without sacrificing overall accuracy. On the PAD-UFES-20 dataset, our method outperforms the strongest competing baseline by 4.1% in recall and 4.4% in F1 score, with larger gains observed for underrepresented patient populations.

</details>


### [464] [A Streamlined Attention-Based Network for Descriptor Extraction](https://arxiv.org/abs/2601.13126)
*Mattia D'Urso,Emanuele Santellani,Christian Sormann,Mattia Rossi,Andreas Kuhn,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: SANDesc leverages a U-Net-inspired architecture with attention mechanisms to enhance keypoint descriptor extraction, achieving better matching performance and computational efficiency across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance keypoint descriptor extraction without altering the existing keypoint detection methods and offer improved matching capabilities with computational efficiency.

Method: The paper introduces a modified U-Net architecture integrated with Convolutional Block Attention Modules and residual connections, using a triplet loss and hard negative mining strategy for stable training.

Result: Extensive experiments demonstrate that SANDesc improves keypoint matching tasks across datasets like HPatches, MegaDepth-1500, and an introduced 4K urban dataset, outperforming existing descriptor techniques with only 2.4M parameters.

Conclusion: SANDesc offers a novel descriptor network combining computational efficiency and improved performance while introducing a new benchmark dataset for further assessment of feature extraction models.

Abstract: We introduce SANDesc, a Streamlined Attention-Based Network for Descriptor extraction that aims to improve on existing architectures for keypoint description.
  Our descriptor network learns to compute descriptors that improve matching without modifying the underlying keypoint detector. We employ a revised U-Net-like architecture enhanced with Convolutional Block Attention Modules and residual paths, enabling effective local representation while maintaining computational efficiency. We refer to the building blocks of our model as Residual U-Net Blocks with Attention. The model is trained using a modified triplet loss in combination with a curriculum learning-inspired hard negative mining strategy, which improves training stability.
  Extensive experiments on HPatches, MegaDepth-1500, and the Image Matching Challenge 2021 show that training SANDesc on top of existing keypoint detectors leads to improved results on multiple matching tasks compared to the original keypoint descriptors. At the same time, SANDesc has a model complexity of just 2.4 million parameters.
  As a further contribution, we introduce a new urban dataset featuring 4K images and pre-calibrated intrinsics, designed to evaluate feature extractors. On this benchmark, SANDesc achieves substantial performance gains over the existing descriptors while operating with limited computational resources.

</details>


### [465] [PhaseMark: A Post-hoc, Optimization-Free Watermarking of AI-generated Images in the Latent Frequency Domain](https://arxiv.org/abs/2601.13128)
*Sung Ju Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: The paper introduces PhaseMark, a fast and robust method for embedding watermarks onto hyper-realistic images generated by Latent Diffusion Models without quality degradation.


<details>
  <summary>Details</summary>
Motivation: The rapid creation of hyper-realistic images using Latent Diffusion Models necessitates a reliable and efficient watermarking solution, as existing methods are too slow and computationally intensive.

Method: PhaseMark directly modulates the phase within the latent frequency domain of the VAE, requiring no iterative optimization or inversion processes.

Result: PhaseMark is thousands of times faster than previous optimization-based methods and offers state-of-the-art resilience against severe attacks like regeneration, without compromising image quality.

Conclusion: PhaseMark provides a novel and practical approach to watermarking relying on intrinsic properties of the latent domain, balancing efficiency and robustness.

Abstract: The proliferation of hyper-realistic images from Latent Diffusion Models (LDMs) demands robust watermarking, yet existing post-hoc methods are prohibitively slow due to iterative optimization or inversion processes. We introduce PhaseMark, a single-shot, optimization-free framework that directly modulates the phase in the VAE latent frequency domain. This approach makes PhaseMark thousands of times faster than optimization-based techniques while achieving state-of-the-art resilience against severe attacks, including regeneration, without degrading image quality. We analyze four modulation variants, revealing a clear performance-quality trade-off. PhaseMark demonstrates a new paradigm where efficient, resilient watermarking is achieved by exploiting intrinsic latent properties.

</details>


### [466] [GaussExplorer: 3D Gaussian Splatting for Embodied Exploration and Reasoning](https://arxiv.org/abs/2601.13132)
*Kim Yu-Ji,Dahye Lee,Kim Jun-Seong,GeonU Kim,Nam Hyeon-Woo,Yongjin Kwon,Yu-Chiang Frank Wang,Jaesung Choe,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: GaussExplorer integrates Vision-Language Models (VLMs) with 3D Gaussian Splatting (3DGS) for enhanced exploration and reasoning in 3D scenes.


<details>
  <summary>Details</summary>
Motivation: Existing 3D Gaussian Splatting approaches struggle with complex, compositional language queries, and alternative methods face limitations like fixed viewpoints.

Method: GaussExplorer uses Vision-Language Models to identify query-relevant images and adjusts viewpoints to support reasoning. It leverages 3DGS integrated with VLMs for question-driven exploration in 3D environments.

Result: Experiments show GaussExplorer's superior performance over existing methods on several benchmarks.

Conclusion: GaussExplorer effectively combines 3DGS with VLMs, advancing embodied exploration and reasoning for complex tasks in 3D scenes.

Abstract: We present GaussExplorer, a framework for embodied exploration and reasoning built on 3D Gaussian Splatting (3DGS). While prior approaches to language-embedded 3DGS have made meaningful progress in aligning simple text queries with Gaussian embeddings, they are generally optimized for relatively simple queries and struggle to interpret more complex, compositional language queries. Alternative studies based on object-centric RGB-D structured memories provide spatial grounding but are constrained by pre-fixed viewpoints. To address these issues, GaussExplorer introduces Vision-Language Models (VLMs) on top of 3DGS to enable question-driven exploration and reasoning within 3D scenes. We first identify pre-captured images that are most correlated with the query question, and subsequently adjust them into novel viewpoints to more accurately capture visual information for better reasoning by VLMs. Experiments show that ours outperforms existing methods on several benchmarks, demonstrating the effectiveness of integrating VLM-based reasoning with 3DGS for embodied tasks.

</details>


### [467] [CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks](https://arxiv.org/abs/2601.13133)
*Mingshuang Luo,Ruibing Hou,Bo Chao,Hong Chang,Zimo Liu,Yaowei Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: CLASP uses CLIP-guided pseudo-labels and a dynamic Mixture-of-Experts module for unsupervised pre-training in human-centric visual tasks, achieving superior performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To fulfill the need for a general unsupervised pre-training model that can support diverse human-centric downstream tasks and leverage large-scale unlabeled datasets.

Method: The paper introduces CLASP, which utilizes CLIP to generate multi-level semantic pseudo-labels and includes a Prompt-Controlled Mixture-of-Experts module for adaptive feature extraction in task-specific scenarios. It also employs a multi-task pre-training strategy.

Result: CLASP consistently shows superior performance over existing unsupervised pre-training methods on diverse benchmarks for human-centric visual analysis.

Conclusion: The proposed CLASP framework enriches visual representations with multi-level semantic information and effectively adapts to various downstream tasks, marking advancements in unsupervised pre-training for human-centric visual analysis.

Abstract: Human-centric visual analysis plays a pivotal role in diverse applications, including surveillance, healthcare, and human-computer interaction. With the emergence of large-scale unlabeled human image datasets, there is an increasing need for a general unsupervised pre-training model capable of supporting diverse human-centric downstream tasks. To achieve this goal, we propose CLASP (CLIP-guided Adaptable Self-suPervised learning), a novel framework designed for unsupervised pre-training in human-centric visual tasks. CLASP leverages the powerful vision-language model CLIP to generate both low-level (e.g., body parts) and high-level (e.g., attributes) semantic pseudo-labels. These multi-level semantic cues are then integrated into the learned visual representations, enriching their expressiveness and generalizability. Recognizing that different downstream tasks demand varying levels of semantic granularity, CLASP incorporates a Prompt-Controlled Mixture-of-Experts (MoE) module. MoE dynamically adapts feature extraction based on task-specific prompts, mitigating potential feature conflicts and enhancing transferability. Furthermore, CLASP employs a multi-task pre-training strategy, where part- and attribute-level pseudo-labels derived from CLIP guide the representation learning process. Extensive experiments across multiple benchmarks demonstrate that CLASP consistently outperforms existing unsupervised pre-training methods, advancing the field of human-centric visual analysis.

</details>


### [468] [TVWorld: Foundations for Remote-Control TV Agents](https://arxiv.org/abs/2601.13142)
*Zhantao Ma,Quanfeng Lu,Shuai Zhong,Dahai Yu,Ping Luo,Michael K. Ng*

Main category: cs.CV

TL;DR: The paper introduces TVWorld, benchmarks for TV navigation and grounding, exposes limitations in current models, and proposes Topology-Aware Training for improvement, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of research and effective solutions for remote-control interaction commonly used in TV navigation, as opposed to point-and-click methods.

Method: The authors design TVWorld, an offline graph-based abstraction for reproducible TV navigation evaluation, establish benchmarks (TVWorld-N and TVWorld-G), and propose a Topology-Aware Training framework to enhance LVLMs.

Result: TVTheseus, the model developed using Topology-Aware Training, achieved a 68.3% success rate on the TVWorld-N benchmark, surpassing existing closed-source baselines and setting new SOTA performance.

Conclusion: The study demonstrates the importance of topology awareness in long-horizon navigation, validates the efficiency of Topology-Aware Training, and provides benchmarks to guide further research in TV-use agents.

Abstract: Recent large vision-language models (LVLMs) have demonstrated strong potential for device control. However, existing research has primarily focused on point-and-click (PnC) interaction, while remote-control (RC) interaction commonly encountered in everyday TV usage remains largely underexplored. To fill this gap, we introduce \textbf{TVWorld}, an offline graph-based abstraction of real-world TV navigation that enables reproducible and deployment-free evaluation. On this basis, we derive two complementary benchmarks that comprehensively assess TV-use capabilities: \textbf{TVWorld-N} for topology-aware navigation and \textbf{TVWorld-G} for focus-aware grounding. These benchmarks expose a key limitation of existing agents: insufficient topology awareness for focus-based, long-horizon TV navigation. Motivated by this finding, we propose a \emph{Topology-Aware Training} framework that injects topology awareness into LVLMs. Using this framework, we develop \textbf{TVTheseus}, a foundation model specialized for TV navigation. TVTheseus achieves a success rate of $68.3\%$ on TVWorld-N, surpassing strong closed-source baselines such as Gemini 3 Flash and establishing state-of-the-art (SOTA) performance. Additional analyses further provide valuable insights into the development of effective TV-use agents.

</details>


### [469] [ICo3D: An Interactive Conversational 3D Virtual Human](https://arxiv.org/abs/2601.13148)
*Richard Shaw,Youngkyoon Jang,Athanasios Papaioannou,Arthur Moreau,Helisa Dhamo,Zhensong Zhang,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: ICo3D presents a method to create realistic, interactive conversational 3D avatars using advanced multi-view capture technologies and AI-driven conversational abilities, suitable for various applications.


<details>
  <summary>Details</summary>
Motivation: To develop lifelike 3D avatars capable of real-time interaction and conversation for enhanced applications in gaming, virtual assistance, education, and immersive technology.

Method: The approach involves multi-view capture to develop photorealistic animatable models for face and body, rendered through Gaussian primitives, paired with an LLM for conversational AI, and synchronized animations driven by audio signals.

Result: The developed ICo3D avatars demonstrate photorealistic rendering, seamless face-body integration, and advanced conversational and interaction capabilities in real-time scenarios.

Conclusion: ICo3D represents a significant advancement in creating lifelike and interactive 3D virtual humans, offering wide-ranging applications across immersive and interactive fields.

Abstract: This work presents Interactive Conversational 3D Virtual Human (ICo3D), a method for generating an interactive, conversational, and photorealistic 3D human avatar. Based on multi-view captures of a subject, we create an animatable 3D face model and a dynamic 3D body model, both rendered by splatting Gaussian primitives. Once merged together, they represent a lifelike virtual human avatar suitable for real-time user interactions. We equip our avatar with an LLM for conversational ability. During conversation, the audio speech of the avatar is used as a driving signal to animate the face model, enabling precise synchronization. We describe improvements to our dynamic Gaussian models that enhance photorealism: SWinGS++ for body reconstruction and HeadGaS++ for face reconstruction, and provide as well a solution to merge the separate face and body models without artifacts. We also present a demo of the complete system, showcasing several use cases of real-time conversation with the 3D avatar. Our approach offers a fully integrated virtual avatar experience, supporting both oral and written form interactions in immersive environments. ICo3D is applicable to a wide range of fields, including gaming, virtual assistance, and personalized education, among others. Project page: https://ico3d.github.io/

</details>


### [470] [From 100,000+ images to winning the first brain MRI foundation model challenges: Sharing lessons and models](https://arxiv.org/abs/2601.13166)
*Pedro M. Gordaliza,Jaume Banus,Benoît Gérin,Maxence Wynen,Nataliia Molchanova,Jonas Richiardi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: This study presents a U-Net-based winning solution for 3D brain MRI analysis challenges, achieving faster training and smaller size than transformer models.


<details>
  <summary>Details</summary>
Motivation: To address unique radiological challenges in 3D brain MRI analysis and develop efficient medical imaging models suitable for these complex tasks.

Method: Utilized a U-Net CNN architecture integrated with anatomical and neuroimaging domain knowledge for optimal performance in radiological challenges.

Result: Ranked first in SSL3D and FOMO25 contests at MICCAI 2025 using models that trained faster and were significantly smaller than transformer-based competitors.

Conclusion: The proposed U-Net-based approach demonstrates superior efficiency and effectiveness, serving as a foundation for future medical image analysis models.

Abstract: Developing Foundation Models for medical image analysis is essential to overcome the unique challenges of radiological tasks. The first challenges of this kind for 3D brain MRI, SSL3D and FOMO25, were held at MICCAI 2025. Our solution ranked first in tracks of both contests. It relies on a U-Net CNN architecture combined with strategies leveraging anatomical priors and neuroimaging domain knowledge. Notably, our models trained 1-2 orders of magnitude faster and were 10 times smaller than competing transformer-based approaches. Models are available here: https://github.com/jbanusco/BrainFM4Challenges.

</details>


### [471] [GTPred: Benchmarking MLLMs for Interpretable Geo-localization and Time-of-capture Prediction](https://arxiv.org/abs/2601.13207)
*Jinnao Li,Zijian Chen,Tingzhu Chen,Changbo Wang*

Main category: cs.CV

TL;DR: This paper introduces GTPred, a benchmark for combining geographic and temporal information in image geo-localization, and evaluates multi-modal large language models (MLLMs) on this task.


<details>
  <summary>Details</summary>
Motivation: Current geo-localization benchmarks fail to incorporate temporal information in images, which could improve location inference.

Method: The authors propose GTPred, a benchmark with 370 global images over 120 years, and evaluate MLLMs considering both geographic and temporal reasoning.

Result: Findings reveal that current models excel in visual perception but are limited in geo-temporal reasoning; incorporating temporal data enhances location inference.

Conclusion: Temporal information is crucial for improving geo-localization tasks, and current MLLMs require advancements in knowledge and reasoning to exploit this fully.

Abstract: Geo-localization aims to infer the geographic location where an image was captured using observable visual evidence. Traditional methods achieve impressive results through large-scale training on massive image corpora. With the emergence of multi-modal large language models (MLLMs), recent studies have explored their applications in geo-localization, benefiting from improved accuracy and interpretability. However, existing benchmarks largely ignore the temporal information inherent in images, which can further constrain the location. To bridge this gap, we introduce GTPred, a novel benchmark for geo-temporal prediction. GTPred comprises 370 globally distributed images spanning over 120 years. We evaluate MLLM predictions by jointly considering year and hierarchical location sequence matching, and further assess intermediate reasoning chains using meticulously annotated ground-truth reasoning processes. Experiments on 8 proprietary and 7 open-source MLLMs show that, despite strong visual perception, current models remain limited in world knowledge and geo-temporal reasoning. Results also demonstrate that incorporating temporal information significantly enhances location inference performance.

</details>


### [472] [Rethinking Skip Connections: Additive U-Net for Robust and Interpretable Denoising](https://arxiv.org/abs/2601.13208)
*Vikram R Lakkavalli*

Main category: cs.CV

TL;DR: The paper proposes an Additive U-Net with gated additive skip connections instead of concatenation for image denoising, improving interpretability and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the drawbacks of standard concatenation skip connections in U-Net architectures, which double dimensionality and allow uncontrolled noise transfer, limiting efficiency and interpretability.

Method: The model replaces concatenative skips with gated additive connections, where skip pathways are scaled by learnable non-negative scalars, ensuring controlled information flow and preventing channel inflation.

Result: Additive U-Net achieves competitive PSNR/SSIM scores on the Kodak-17 denoising benchmark across various noise levels and kernel setups, effectively learning feature hierarchies without forced down/up-sampling.

Conclusion: Additive skip connections offer a lightweight, efficient, and interpretable alternative to concatenation, improving multi-scale information transfer in image reconstruction systems.

Abstract: Skip connections are central to U-Net architectures for image denoising, but standard concatenation doubles channel dimensionality and obscures information flow, allowing uncontrolled noise transfer. We propose the Additive U-Net, which replaces concatenative skips with gated additive connections. Each skip pathway is scaled by a learnable non-negative scalar, offering explicit and interpretable control over encoder contributions while avoiding channel inflation. Evaluations on the Kodak-17 denoising benchmark show that Additive U-Net achieves competitive PSNR/SSIM at noise levels σ = 15, 25, 50, with robustness across kernel schedules and depths. Notably, effective denoising is achieved even without explicit down/up-sampling or forced hierarchies, as the model naturally learns a progression from high-frequency to band-pass to low-frequency features. These results position additive skips as a lightweight and interpretable alternative to concatenation, enabling both efficient design and a clearer understanding of multi-scale information transfer in reconstruction networks.

</details>


### [473] [ObjectVisA-120: Object-based Visual Attention Prediction in Interactive Street-crossing Environments](https://arxiv.org/abs/2601.13218)
*Igor Vozniak,Philipp Mueller,Nils Lipp,Janis Sprenger,Konstantin Poddubnyy,Davit Hovhannisyan,Christian Mueller,Andreas Bulling,Philipp Slusallek*

Main category: cs.CV

TL;DR: This paper focuses on introducing a novel dataset and metric for evaluating object-based visual attention in virtual reality street-crossing scenarios and proposes a new graph-based attention prediction model.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limited role of object-based visual attention in computational models due to the lack of suitable datasets and evaluation metrics, especially in dynamic and ethically challenging real-world scenarios like street-crossing.

Method: The authors present a VR-based dataset (dataset) featuring accurate gaze data, state-space object representation, and rich annotations. They also introduce oSIM, a novel metric for object-based attention evaluation, and propose SUMGraph, a graph-based model for explicit object encoding.

Result: Analysis shows that explicitly optimizing for object-based attention improves performance on both oSIM and common metrics. SUMGraph achieves superior performance compared to several state-of-the-art visual attention prediction methods.

Conclusion: The novel dataset, metric, and model advance the study of object-based visual attention and will be publicly accessible for further research, offering a robust tool for improving attention model evaluations.

Abstract: The object-based nature of human visual attention is well-known in cognitive science, but has only played a minor role in computational visual attention models so far. This is mainly due to a lack of suitable datasets and evaluation metrics for object-based attention. To address these limitations, we present \dataset~ -- a novel 120-participant dataset of spatial street-crossing navigation in virtual reality specifically geared to object-based attention evaluations. The uniqueness of the presented dataset lies in the ethical and safety affiliated challenges that make collecting comparable data in real-world environments highly difficult. \dataset~ not only features accurate gaze data and a complete state-space representation of objects in the virtual environment, but it also offers variable scenario complexities and rich annotations, including panoptic segmentation, depth information, and vehicle keypoints. We further propose object-based similarity (oSIM) as a novel metric to evaluate the performance of object-based visual attention models, a previously unexplored performance characteristic. Our evaluations show that explicitly optimising for object-based attention not only improves oSIM performance but also leads to an improved model performance on common metrics. In addition, we present SUMGraph, a Mamba U-Net-based model, which explicitly encodes critical scene objects (vehicles) in a graph representation, leading to further performance improvements over several state-of-the-art visual attention prediction methods. The dataset, code and models will be publicly released.

</details>


### [474] [Not all Blends are Equal: The BLEMORE Dataset of Blended Emotion Expressions with Relative Salience Annotations](https://arxiv.org/abs/2601.13225)
*Tim Lachmann,Alexandra Israelsson,Christina Tornberg,Teimuraz Saghinadze,Michal Balazia,Philipp Müller,Petri Laukka*

Main category: cs.CV

TL;DR: This paper introduces BLEMORE, a dataset containing multimodal (video, audio) data annotated with relative salience of blended emotions, addressing gaps in emotion recognition research.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance video-based emotion recognition systems to recognize and assess blended emotions, which has been hindered by the lack of suitable datasets with relative salience annotations.

Method: The authors create BLEMORE, a dataset of 3,000 video clips with emotional blends and salience levels. They analyze state-of-the-art classifiers on two tasks: emotion presence and salience prediction, using multimodal approaches.

Result: Unimodal classifiers achieve up to 29% presence accuracy and 13% salience accuracy. Multimodal classifiers perform better, with accuracies reaching 35% for presence and 18% for salience in validation and test sets.

Conclusion: The BLEMORE dataset significantly improves the study of blended emotion recognition, helping to develop systems better-equipped to address complex human emotional expressions.

Abstract: Humans often experience not just a single basic emotion at a time, but rather a blend of several emotions with varying salience. Despite the importance of such blended emotions, most video-based emotion recognition approaches are designed to recognize single emotions only. The few approaches that have attempted to recognize blended emotions typically cannot assess the relative salience of the emotions within a blend. This limitation largely stems from the lack of datasets containing a substantial number of blended emotion samples annotated with relative salience. To address this shortcoming, we introduce BLEMORE, a novel dataset for multimodal (video, audio) blended emotion recognition that includes information on the relative salience of each emotion within a blend. BLEMORE comprises over 3,000 clips from 58 actors, performing 6 basic emotions and 10 distinct blends, where each blend has 3 different salience configurations (50/50, 70/30, and 30/70). Using this dataset, we conduct extensive evaluations of state-of-the-art video classification approaches on two blended emotion prediction tasks: (1) predicting the presence of emotions in a given sample, and (2) predicting the relative salience of emotions in a blend. Our results show that unimodal classifiers achieve up to 29% presence accuracy and 13% salience accuracy on the validation set, while multimodal methods yield clear improvements, with ImageBind + WavLM reaching 35% presence accuracy and HiCMAE 18% salience accuracy. On the held-out test set, the best models achieve 33% presence accuracy (VideoMAEv2 + HuBERT) and 18% salience accuracy (HiCMAE). In sum, the BLEMORE dataset provides a valuable resource to advancing research on emotion recognition systems that account for the complexity and significance of blended emotion expressions.

</details>


### [475] [ConvMambaNet: A Hybrid CNN-Mamba State Space Architecture for Accurate and Real-Time EEG Seizure Detection](https://arxiv.org/abs/2601.13234)
*Md. Nishan Khan,Kazi Shahriar Sanjid,Md. Tanzim Hossain,Asib Mostakim Fony,Istiak Ahmed,M. Monir Uddin*

Main category: cs.CV

TL;DR: The paper introduces ConvMambaNet, a hybrid deep learning model combining CNNs with Mamba-SSM for improved seizure detection using EEG data.


<details>
  <summary>Details</summary>
Motivation: Automated analysis of EEG signals for seizure detection is challenging due to the temporal complexity, requiring advanced models for accurate monitoring.

Method: ConvMambaNet integrates CNNs with the Mamba Structured State Space Model to enhance spatial and long-range temporal feature extraction.

Result: ConvMambaNet achieved a 99% accuracy on the CHB-MIT Scalp EEG dataset, performing well even under severe class imbalance.

Conclusion: ConvMambaNet shows promising potential for real-time, automated seizure detection, improving epilepsy monitoring in clinical settings.

Abstract: Epilepsy is a chronic neurological disorder marked by recurrent seizures that can severely impact quality of life. Electroencephalography (EEG) remains the primary tool for monitoring neural activity and detecting seizures, yet automated analysis remains challenging due to the temporal complexity of EEG signals. This study introduces ConvMambaNet, a hybrid deep learning model that integrates Convolutional Neural Networks (CNNs) with the Mamba Structured State Space Model (SSM) to enhance temporal feature extraction. By embedding the Mamba-SSM block within a CNN framework, the model effectively captures both spatial and long-range temporal dynamics. Evaluated on the CHB-MIT Scalp EEG dataset, ConvMambaNet achieved a 99% accuracy and demonstrated robust performance under severe class imbalance. These results underscore the model's potential for precise and efficient seizure detection, offering a viable path toward real-time, automated epilepsy monitoring in clinical environments.

</details>


### [476] [A Semantic Decoupling-Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision-Language Models](https://arxiv.org/abs/2601.13238)
*Chengyin Hu,Xiang Chen,Zhe Jia,Weiwen Shi,Fengyu Zhang,Jiujiang Guo,Yiwei Wei*

Main category: cs.CV

TL;DR: This paper introduces an adversarial framework to analyze the robustness of Vision-Language Models (VLMs) to realistic rain scenarios, identifying vulnerabilities in their semantic alignments under such conditions.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by limited research on the robustness of Vision-Language Models (VLMs) to real-world environmental conditions, such as rain, which could impact their performance in multimodal tasks.

Method: The authors propose a two-stage parameterized perturbation model: Stage 1 weakens semantic decision boundaries by globally modulating the embedding space, and Stage 2 models multi-scale raindrop effects and illumination changes to induce shifts in semantic alignment.

Result: The framework demonstrates that realistic rain perturbations lead to substantial semantic misalignment in mainstream VLMs, highlighting potential safety and reliability challenges in real-world applications.

Conclusion: The work emphasizes the vulnerability of VLMs to structured weather perturbations and suggests systematic improvements are necessary to enhance their real-world reliability.

Abstract: Vision-Language Models (VLMs) are trained on image-text pairs collected under canonical visual conditions and achieve strong performance on multimodal tasks. However, their robustness to real-world weather conditions, and the stability of cross-modal semantic alignment under such structured perturbations, remain insufficiently studied. In this paper, we focus on rainy scenarios and introduce the first adversarial framework that exploits realistic weather to attack VLMs, using a two-stage, parameterized perturbation model based on semantic decoupling to analyze rain-induced shifts in decision-making. In Stage 1, we model the global effects of rainfall by applying a low-dimensional global modulation to condition the embedding space and gradually weaken the original semantic decision boundaries. In Stage 2, we introduce structured rain variations by explicitly modeling multi-scale raindrop appearance and rainfall-induced illumination changes, and optimize the resulting non-differentiable weather space to induce stable semantic shifts. Operating in a non-pixel parameter space, our framework generates perturbations that are both physically grounded and interpretable. Experiments across multiple tasks show that even physically plausible, highly constrained weather perturbations can induce substantial semantic misalignment in mainstream VLMs, posing potential safety and reliability risks in real-world deployment. Ablations further confirm that illumination modeling and multi-scale raindrop structures are key drivers of these semantic shifts.

</details>


### [477] [Deep Learning for Semantic Segmentation of 3D Ultrasound Data](https://arxiv.org/abs/2601.13263)
*Chenyu Liu,Marco Cecotti,Harikrishnan Vijayakumar,Patrick Robinson,James Barson,Mihai Caleap*

Main category: cs.CV

TL;DR: The paper proposes the use of Calyo Pulse, a modular 3D ultrasound sensor, for 3D semantic segmentation in automated vehicles, leveraging a 3D U-Net architecture for robust sensing in harsh environments.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of cost, robustness, and performance in existing perception systems (LiDAR and camera-based) for automated vehicles, particularly under adverse conditions.

Method: Introduced Calyo Pulse, a 3D ultrasound sensor system, combined with a learning-based 3D U-Net architecture to perform volumetric segmentation on spatial ultrasound data.

Result: Results show that the Calyo Pulse sensors achieve robust segmentation performance, with potential improvements identified through enhanced datasets, ground truth refinement, and weighted loss functions.

Conclusion: 3D ultrasound sensing with Calyo Pulse represents a promising complementary modality for reliable and robust autonomy in automated vehicles.

Abstract: Developing cost-efficient and reliable perception systems remains a central challenge for automated vehicles. LiDAR and camera-based systems dominate, yet they present trade-offs in cost, robustness and performance under adverse conditions. This work introduces a novel framework for learning-based 3D semantic segmentation using Calyo Pulse, a modular, solid-state 3D ultrasound sensor system for use in harsh and cluttered environments. A 3D U-Net architecture is introduced and trained on the spatial ultrasound data for volumetric segmentation. Results demonstrate robust segmentation performance from Calyo Pulse sensors, with potential for further improvement through larger datasets, refined ground truth, and weighted loss functions. Importantly, this study highlights 3D ultrasound sensing as a promising complementary modality for reliable autonomy.

</details>


### [478] [Enginuity: Building an Open Multi-Domain Dataset of Complex Engineering Diagrams](https://arxiv.org/abs/2601.13299)
*Ethan Seefried,Prahitha Movva,Naga Harshita Marupaka,Tilak Kasturi,Tirthankar Ghosal*

Main category: cs.CV

TL;DR: The paper proposes Enginuity, a large-scale, multi-domain engineering diagram dataset with structural annotations for automated parsing, aiding downstream engineering tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in artificial intelligence's ability to comprehend and manipulate the structural knowledge contained in engineering diagrams.

Method: Developed a dataset called Enginuity with hierarchical component relationships, connections, and semantic elements for multimodal tasks.

Result: The dataset enables tasks like structured diagram parsing, cross-modal retrieval, and AI-assisted engineering simulation.

Conclusion: Enginuity addresses a fundamental barrier in AI's role in scientific discovery by enhancing its capability to interpret engineering diagrams and assist in scientific workflows.

Abstract: We propose Enginuity - the first open, large-scale, multi-domain engineering diagram dataset with comprehensive structural annotations designed for automated diagram parsing. By capturing hierarchical component relationships, connections, and semantic elements across diverse engineering domains, our proposed dataset would enable multimodal large language models to address critical downstream tasks including structured diagram parsing, cross-modal information retrieval, and AI-assisted engineering simulation. Enginuity would be transformative for AI for Scientific Discovery by enabling artificial intelligence systems to comprehend and manipulate the visual-structural knowledge embedded in engineering diagrams, breaking down a fundamental barrier that currently prevents AI from fully participating in scientific workflows where diagram interpretation, technical drawing analysis, and visual reasoning are essential for hypothesis generation, experimental design, and discovery.

</details>


### [479] [CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning](https://arxiv.org/abs/2601.13304)
*Wenxin Ma,Chenlong Wang,Ruisheng Yuan,Hao Chen,Nanru Dai,S. Kevin Zhou,Yijun Yang,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: This paper introduces a diagnostic benchmark called CausalSpatial to evaluate multimodal language models' ability for causal spatial reasoning, revealing significant performance gaps when compared to human abilities.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to bridge the gap in multimodal large language models, which struggle to answer 'what-if' questions and lack causal spatial reasoning capabilities.

Method: The researchers developed the CausalSpatial benchmark for four tasks (Collision, Compatibility, Occlusion, and Trajectory) and proposed the Causal Object World (COW) model that uses simulated videos for reasoning grounded in visual evidence.

Result: Humans scored 84% on the CausalSpatial benchmark, while GPT-5 only achieved 54%, highlighting a severe deficiency in models' causal spatial reasoning. The proposed COW model aims to improve this using visual simulation.

Conclusion: Existing multimodal large language models lack robust causal spatial reasoning abilities, and externalizing simulation through visual representations could significantly enhance their spatial-grounded reasoning competencies.

Abstract: Humans can look at a static scene and instantly predict what happens next -- will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer "what-if" questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Results expose a severe gap: humans score 84% while GPT-5 achieves only 54%. Why do MLLMs fail? Our analysis uncovers a fundamental deficiency: models over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object World model (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues of causality, COW enables models to ground their reasoning in physical reality rather than linguistic priors. We make the dataset and code publicly available here: https://github.com/CausalSpatial/CausalSpatial

</details>


### [480] [MultiST: A Cross-Attention-Based Multimodal Model for Spatial Transcriptomic](https://arxiv.org/abs/2601.13331)
*Wei Wang,Quoc-Toan Ly,Chong Yu,Jun Bai*

Main category: cs.CV

TL;DR: The paper introduces MultiST, a framework for integrating spatial transcriptomics data with histological images to improve tissue organization analysis.


<details>
  <summary>Details</summary>
Motivation: Existing spatial transcriptomics methods struggle to integrate molecular profiles with histological morphology effectively, limiting spatial domain resolution.

Method: MultiST combines cross-attention-based fusion, graph-based encoders, and adversarial alignment to jointly model spatial topology, gene expression, and tissue morphology.

Result: Tested on 13 diverse datasets, MultiST improves domain boundary clarity, pseudotime trajectory stability, and biological interpretability of cell-cell interactions.

Conclusion: MultiST enhances spatial transcriptomics by leveraging molecular and morphological data jointly, offering a robust tool for tissue analysis and interaction studies.

Abstract: Spatial transcriptomics (ST) enables transcriptome-wide profiling while preserving the spatial context of tissues, offering unprecedented opportunities to study tissue organization and cell-cell interactions in situ. Despite recent advances, existing methods often lack effective integration of histological morphology with molecular profiles, relying on shallow fusion strategies or omitting tissue images altogether, which limits their ability to resolve ambiguous spatial domain boundaries. To address this challenge, we propose MultiST, a unified multimodal framework that jointly models spatial topology, gene expression, and tissue morphology through cross-attention-based fusion. MultiST employs graph-based gene encoders with adversarial alignment to learn robust spatial representations, while integrating color-normalized histological features to capture molecular-morphological dependencies and refine domain boundaries. We evaluated the proposed method on 13 diverse ST datasets spanning two organs, including human brain cortex and breast cancer tissue. MultiST yields spatial domains with clearer and more coherent boundaries than existing methods, leading to more stable pseudotime trajectories and more biologically interpretable cell-cell interaction patterns. The MultiST framework and source code are available at https://github.com/LabJunBMI/MultiST.git.

</details>


### [481] [Real-Time 4D Radar Perception for Robust Human Detection in Harsh Enclosed Environments](https://arxiv.org/abs/2601.13364)
*Zhenan Liu,Yaodong Cui,Amir Khajepour,George Shaker*

Main category: cs.CV

TL;DR: This study establishes a novel multi-level dust-controlled testing setup and a 4D mmWave radar dataset for evaluating sensing in harsh environments, introducing a radar-based pedestrian detection method to handle clutter and noise.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges in sensing accuracy posed by harsh, cluttered, and dust-filled environments, such as underground mines or tunnels, which demand improved electromagnetic signal reliability and noise filtering techniques.

Method: The study introduces a new 4D radar dataset while proposing a threshold-based noise filtering technique and a rule-based radar cluster classification for real-time pedestrian detection. This method leverages radar parameters like RCS, velocity, azimuth, and elevation.

Result: Experimental results show that the framework improved clutter and noise filtering, while achieving real-time and robust pedestrian detection, even in environments with dust and strong multipath reflections.

Conclusion: The integrated methodology significantly boosts robustness and pedestrian detection accuracy, proving its suitability for challenging conditions in dusty mining or similar harsh environments.

Abstract: This paper introduces a novel methodology for generating controlled, multi-level dust concentrations in a highly cluttered environment representative of harsh, enclosed environments, such as underground mines, road tunnels, or collapsed buildings, enabling repeatable mm-wave propagation studies under severe electromagnetic constraints. We also present a new 4D mmWave radar dataset, augmented by camera and LiDAR, illustrating how dust particles and reflective surfaces jointly impact the sensing functionality. To address these challenges, we develop a threshold-based noise filtering framework leveraging key radar parameters (RCS, velocity, azimuth, elevation) to suppress ghost targets and mitigate strong multipath reflections at the raw data level. Building on the filtered point clouds, a cluster-level, rule-based classification pipeline exploits radar semantics-velocity, RCS, and volumetric spread-to achieve reliable, real-time pedestrian detection without extensive domainspecific training. Experimental results confirm that this integrated approach significantly enhances clutter mitigation, detection robustness, and overall system resilience in dust-laden mining environments.

</details>


### [482] [Spherical Geometry Diffusion: Generating High-quality 3D Face Geometry via Sphere-anchored Representations](https://arxiv.org/abs/2601.13371)
*Junyi Zhang,Yiming Wang,Yunhong Lu,Qichao Wang,Wenzhe Qian,Xiaoyin Xu,David Gu,Min Zhang*

Main category: cs.CV

TL;DR: This paper proposes a novel method for improving text-to-3D face generation by addressing challenges in achieving high-quality geometry using a spherical geometric representation and diffusion framework.


<details>
  <summary>Details</summary>
Motivation: Challenges in text-to-3D face generation primarily stem from the complex and arbitrary distribution of vertices in 3D space, which hinders clean geometry modeling.

Method: The paper introduces Spherical Geometry Representation to map the geometry into a uniform spherical manifold and creates a diffusion framework called Spherical Geometry Diffusion that leverages 2D generative models for joint geometry and texture modeling.

Result: The proposed method enables diverse and controllable generation for tasks like text-to-3D generation, face reconstruction, and text-based 3D editing, significantly outperforming existing models in geometry quality, textual fidelity, and efficiency.

Conclusion: The research demonstrates that simplifying the geometric structure with a spherical representation and pairing it with diffusion models leads to breakthroughs in text-to-3D face generation quality and usability.

Abstract: A fundamental challenge in text-to-3D face generation is achieving high-quality geometry. The core difficulty lies in the arbitrary and intricate distribution of vertices in 3D space, making it challenging for existing models to establish clean connectivity and resulting in suboptimal geometry. To address this, our core insight is to simplify the underlying geometric structure by constraining the distribution onto a simple and regular manifold, a topological sphere. Building on this, we first propose the Spherical Geometry Representation, a novel face representation that anchors geometric signals to uniform spherical coordinates. This guarantees a regular point distribution, from which the mesh connectivity can be robustly reconstructed. Critically, this canonical sphere can be seamlessly unwrapped into a 2D map, creating a perfect synergy with powerful 2D generative models. We then introduce Spherical Geometry Diffusion, a conditional diffusion framework built upon this 2D map. It enables diverse and controllable generation by jointly modeling geometry and texture, where the geometry explicitly conditions the texture synthesis process. Our method's effectiveness is demonstrated through its success in a wide range of tasks: text-to-3D generation, face reconstruction, and text-based 3D editing. Extensive experiments show that our approach substantially outperforms existing methods in geometric quality, textual fidelity, and inference efficiency.

</details>


### [483] [A Lightweight Model-Driven 4D Radar Framework for Pervasive Human Detection in Harsh Conditions](https://arxiv.org/abs/2601.13373)
*Zhenan Liu,Amir Khajepour,George Shaker*

Main category: cs.CV

TL;DR: The paper tackles the challenge of human detection in visibility-degraded industrial and underground environments using a model-driven 4D radar framework, overcoming limitations of optical and LiDAR sensors.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of optical and LiDAR sensors in environments with dust, smoke, confined geometry, and metallic structures, and ensure robust human detection for safety-critical applications.

Method: The paper presents a 4D radar perception framework with domain-aware multi-threshold filtering, motion compensation, clustering, Doppler-aware refinement, and rule-based classification, optimized for edge hardware.

Result: The proposed framework was tested in dust-filled and underground environments, consistently enabling radar-based pedestrian detection where camera and LiDAR modalities failed.

Conclusion: The model-driven approach provides stable and computationally efficient perception in challenging environments, making it suitable for industrial and subterranean applications.

Abstract: Pervasive sensing in industrial and underground environments is severely constrained by airborne dust, smoke, confined geometry, and metallic structures, which rapidly degrade optical and LiDAR based perception. Elevation resolved 4D mmWave radar offers strong resilience to such conditions, yet there remains a limited understanding of how to process its sparse and anisotropic point clouds for reliable human detection in enclosed, visibility degraded spaces. This paper presents a fully model-driven 4D radar perception framework designed for real-time execution on embedded edge hardware. The system uses radar as its sole perception modality and integrates domain aware multi threshold filtering, ego motion compensated temporal accumulation, KD tree Euclidean clustering with Doppler aware refinement, and a rule based 3D classifier. The framework is evaluated in a dust filled enclosed trailer and in real underground mining tunnels, and in the tested scenarios the radar based detector maintains stable pedestrian identification as camera and LiDAR modalities fail under severe visibility degradation. These results suggest that the proposed model-driven approach provides robust, interpretable, and computationally efficient perception for safety-critical applications in harsh industrial and subterranean environments.

</details>


### [484] [Practical Insights into Semi-Supervised Object Detection Approaches](https://arxiv.org/abs/2601.13380)
*Chaoxin Wang,Bharaneeshwar Balasubramaniyam,Anurag Sangem,Nicolais Guevara,Doina Caragea*

Main category: cs.CV

TL;DR: The paper reviews and compares three state-of-the-art semi-supervised object detection (SSOD) approaches: MixPL, Semi-DETR, and Consistent-Teacher, on performance under limited labeled data scenarios.


<details>
  <summary>Details</summary>
Motivation: To address learning and improving object detection performance in data-scarce situations by exploring SSOD approaches.

Method: The authors evaluate MixPL, Semi-DETR, and Consistent-Teacher across datasets such as MS-COCO, Pascal VOC, and a custom Beetle dataset, examining the trade-offs in accuracy, model size, and latency.

Result: The study reveals the accuracy, model size, and latency trade-offs of SSOD methods while demonstrating performance in data-scarce and domain-specific setups.

Conclusion: The comparison provides valuable insights on choosing the best SSOD methods for low-data and domain-specific detection tasks.

Abstract: Learning in data-scarce settings has recently gained significant attention in the research community. Semi-supervised object detection(SSOD) aims to improve detection performance by leveraging a large number of unlabeled images alongside a limited number of labeled images(a.k.a.,few-shot learning). In this paper, we present a comprehensive comparison of three state-of-the-art SSOD approaches, including MixPL, Semi-DETR and Consistent-Teacher, with the goal of understanding how performance varies with the number of labeled images. We conduct experiments using the MS-COCO and Pascal VOC datasets, two popular object detection benchmarks which allow for standardized evaluation. In addition, we evaluate the SSOD approaches on a custom Beetle dataset which enables us to gain insights into their performance on specialized datasets with a smaller number of object categories. Our findings highlight the trade-offs between accuracy, model size, and latency, providing insights into which methods are best suited for low-data regimes.

</details>


### [485] [Organ-Aware Attention Improves CT Triage and Classification](https://arxiv.org/abs/2601.13385)
*Lavsen Dahal,Yubraj Bhandari,Geoffrey D. Rubin,Joseph Y. Lo*

Main category: cs.CV

TL;DR: The paper introduces ORACLE-CT, a novel organ-aware approach for triaging high-volume CT scans, achieving state-of-the-art classification performance on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: There is a need to improve CT triage to enhance patient care, address radiologist burnout, and provide better calibrated and localized predictions compared to existing Vision Language Models (VLMs).

Method: The researchers developed ORACLE-CT, leveraging organ-aware mechanisms such as Organ-Masked Attention and Organ-Scalar Fusion, and evaluated it on public datasets like CT-RATE and RADCHEST-CT.

Result: ORACLE-CT achieved state-of-the-art results, with AUROC 0.86 on chest CT (CT-RATE) and AUROC 0.85 in abdomen scans (MERLIN), surpassing baseline and zero-shot VLM results.

Conclusion: The study demonstrates the effectiveness of organ-aware attention mechanisms in medical CT imaging classification and highlights ORACLE-CT's ability to improve performance under a unified protocol.

Abstract: There is an urgent need for triage and classification of high-volume medical imaging modalities such as computed tomography (CT), which can improve patient care and mitigate radiologist burnout. Study-level CT triage requires calibrated predictions with localized evidence; however, off-the-shelf Vision Language Models (VLM) struggle with 3D anatomy, protocol shifts, and noisy report supervision. This study used the two largest publicly available chest CT datasets: CT-RATE and RADCHEST-CT (held-out external test set). Our carefully tuned supervised baseline (instantiated as a simple Global Average Pooling head) establishes a new supervised state of the art, surpassing all reported linear-probe VLMs. Building on this baseline, we present ORACLE-CT, an encoder-agnostic, organ-aware head that pairs Organ-Masked Attention (mask-restricted, per-organ pooling that yields spatial evidence) with Organ-Scalar Fusion (lightweight fusion of normalized volume and mean-HU cues). In the chest setting, ORACLE-CT masked attention model achieves AUROC 0.86 on CT-RATE; in the abdomen setting, on MERLIN (30 findings), our supervised baseline exceeds a reproduced zero-shot VLM baseline obtained by running publicly released weights through our pipeline, and adding masked attention plus scalar fusion further improves performance to AUROC 0.85. Together, these results deliver state-of-the-art supervised classification performance across both chest and abdomen CT under a unified evaluation protocol. The source code is available at https://github.com/lavsendahal/oracle-ct.

</details>


### [486] [Leveraging Transformer Decoder for Automotive Radar Object Detection](https://arxiv.org/abs/2601.13386)
*Changxu Zhang,Zhaoze Wang,Tai Fei,Christopher Grimm,Yi Jin,Claas Tebruegge,Ernst Warsitz,Markus Gardill*

Main category: cs.CV

TL;DR: The paper introduces a Transformer-based 3D radar object detection model using a novel decoding approach and achieves better performance on the RADDet dataset.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and accurate method for 3D radar object detection that avoids dense proposal generation and heuristic post-processing like NMS tuning.

Method: A Transformer-based architecture is proposed which integrates Pyramid Token Fusion (PTF) to unify multi-scale features into a token sequence, combined with learnable object queries and positional encodings for set prediction.

Result: The method demonstrates marked improvements over previous radar-only baselines on the RADDet dataset.

Conclusion: The proposed approach streamlines 3D radar object detection by modeling spatial-temporal relationships effectively, offering significant performance gains without requiring complex false positive suppression techniques.

Abstract: In this paper, we present a Transformer-based architecture for 3D radar object detection that uses a novel Transformer Decoder as the prediction head to directly regress 3D bounding boxes and class scores from radar feature representations. To bridge multi-scale radar features and the decoder, we propose Pyramid Token Fusion (PTF), a lightweight module that converts a feature pyramid into a unified, scale-aware token sequence. By formulating detection as a set prediction problem with learnable object queries and positional encodings, our design models long-range spatial-temporal correlations and cross-feature interactions. This approach eliminates dense proposal generation and heuristic post-processing such as extensive non-maximum suppression (NMS) tuning. We evaluate the proposed framework on the RADDet, where it achieves significant improvements over state-of-the-art radar-only baselines.

</details>


### [487] [Deep Image Prior with L0 Gradient Regularizer for Image Smoothing](https://arxiv.org/abs/2601.13400)
*Nhat Thanh Tran,Kevin Bui,Jack Xin*

Main category: cs.CV

TL;DR: This paper proposes a novel deep image smoothing framework called DIP-$\ell_0$ that doesn't require a training dataset, using an alternating direction method to optimize its loss function.


<details>
  <summary>Details</summary>
Motivation: Constructing proper training datasets for image smoothing is challenging, which limits the applicability of current deep learning-based image smoothing techniques.

Method: The authors introduced DIP-$\ell_0$, combining a deep image prior framework with an $\ell_0$ gradient regularizer. They used an alternating direction method of multipliers algorithm for optimization.

Result: Numerical experiments show DIP-$\ell_0$ achieves better edge-preserving smoothing and removes JPEG artifacts more effectively compared to other methods.

Conclusion: DIP-$\ell_0$ offers a training-free, high-quality image smoothing solution by leveraging the $\ell_0$ gradient regularization approach.

Abstract: Image smoothing is a fundamental image processing operation that preserves the underlying structure, such as strong edges and contours, and removes minor details and textures in an image. Many image smoothing algorithms rely on computing local window statistics or solving an optimization problem. Recent state-of-the-art methods leverage deep learning, but they require a carefully curated training dataset. Because constructing a proper training dataset for image smoothing is challenging, we propose DIP-$\ell_0$, a deep image prior framework that incorporates the $\ell_0$ gradient regularizer. This framework can perform high-quality image smoothing without any training data. To properly minimize the associated loss function that has the nonconvex, nonsmooth $\ell_0$ ``norm", we develop an alternating direction method of multipliers algorithm that utilizes an off-the-shelf $\ell_0$ gradient minimization solver. Numerical experiments demonstrate that the proposed DIP-$\ell_0$ outperforms many image smoothing algorithms in edge-preserving image smoothing and JPEG artifact removal.

</details>


### [488] [Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics](https://arxiv.org/abs/2601.13401)
*Peter A. Massih,Eric Cosatto*

Main category: cs.CV

TL;DR: Current VLMs struggle with quantitative spatial reasoning due to pixel-level information loss. This paper introduces SQuID and QVLM for improved performance in numerical and categorical reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current VLMs in quantitative spatial reasoning caused by destruction of critical pixel-level information necessary for precise measurement and counting.

Method: A new dataset, SQuID, is introduced for benchmarking spatial quantitative reasoning, coupled with QVLM model which uses code-generation architecture to preserve pixel precision and spatial indexing via segmentation masks.

Result: QVLM achieves a significant improvement of 42.0% accuracy on the SQuID dataset, outperforming traditional VLMs which scored 28.1%.

Conclusion: Decoupling language understanding from visual analysis improves quantitative reasoning accuracy by preserving pixel-level spatial indexing and tracking.

Abstract: Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.

</details>


### [489] [Local-to-Global Logical Explanations for Deep Vision Models](https://arxiv.org/abs/2601.13404)
*Bhavan Vasu,Giuseppe Raffa,Prasad Tadepalli*

Main category: cs.CV

TL;DR: The paper proposes interpretable explanation methods for image classification models using logical formulas in monotone disjunctive-normal-form (MDNF).


<details>
  <summary>Details</summary>
Motivation: To address the opacity and lack of interpretability in deep neural networks used for image classification.

Method: Developed local and global explanation methods generating monotone disjunctive-normal-form (MDNF) formulas, along with an algorithm for explaining multi-class classifications using primitive concepts.

Result: The explanations are interpretable, simple, and retain high fidelity and coverage on challenging vision datasets.

Conclusion: The proposed methods provide human-recognizable, accurate explanations for black-box image classification models while ensuring simplicity and interpretability.

Abstract: While deep neural networks are extremely effective at classifying images, they remain opaque and hard to interpret. We introduce local and global explanation methods for black-box models that generate explanations in terms of human-recognizable primitive concepts. Both the local explanations for a single image and the global explanations for a set of images are cast as logical formulas in monotone disjunctive-normal-form (MDNF), whose satisfaction guarantees that the model yields a high score on a given class. We also present an algorithm for explaining the classification of examples into multiple classes in the form of a monotone explanation list over primitive concepts. Despite their simplicity and interpretability we show that the explanations maintain high fidelity and coverage with respect to the blackbox models they seek to explain in challenging vision datasets.

</details>


### [490] [Using deep learning for predicting cleansing quality of colon capsule endoscopy images](https://arxiv.org/abs/2601.13412)
*Puneet Sharma,Kristian Dalsbø Hindberg,Benedicte Schelde-Olesen,Ulrik Deding,Esmaeil S. Nadimi,Jan-Matthias Braun*

Main category: cs.CV

TL;DR: This paper investigates using deep learning for evaluating colon capsule endoscopy images and applies pruning techniques for efficiency without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the predictive assessment of cleansing quality in colon capsule endoscopy using deep learning methods, while addressing efficiency and explainability challenges for clinical practices.

Method: The study utilized a ResNet-18 model trained on a dataset of 500 clinician-labeled images with stratified K-fold cross-validation. Iterative pruning was applied to achieve sparsity, and explainability methods (Grad-CAM variants and ROAD) were employed to interpret the model's decisions.

Result: The pruned model achieved a high accuracy of 88% with 79% sparsity, maintaining efficiency. Adaptive temperature scaling enhanced calibration on external datasets.

Conclusion: Pruning techniques can optimize model efficiency for clinical tasks without compromising predictive performance. Explainability is critical in evaluating cleansing quality, and new challenges like ROAD method implementation must be addressed.

Abstract: In this study, we explore the application of deep learning techniques for predicting cleansing quality in colon capsule endoscopy (CCE) images. Using a dataset of 500 images labeled by 14 clinicians on the Leighton-Rex scale (Poor, Fair, Good, and Excellent), a ResNet-18 model was trained for classification, leveraging stratified K-fold cross-validation to ensure robust performance. To optimize the model, structured pruning techniques were applied iteratively, achieving significant sparsity while maintaining high accuracy. Explainability of the pruned model was evaluated using Grad-CAM, Grad-CAM++, Eigen-CAM, Ablation-CAM, and Random-CAM, with the ROAD method employed for consistent evaluation. Our results indicate that for a pruned model, we can achieve a cross-validation accuracy of 88% with 79% sparsity, demonstrating the effectiveness of pruning in improving efficiency from 84% without compromising performance. We also highlight the challenges of evaluating cleansing quality of CCE images, emphasize the importance of explainability in clinical applications, and discuss the challenges associated with using the ROAD method for our task. Finally, we employ a variant of adaptive temperature scaling to calibrate the pruned models for an external dataset.

</details>


### [491] [Diffusion Representations for Fine-Grained Image Classification: A Marine Plankton Case Study](https://arxiv.org/abs/2601.13416)
*A. Nieto Juscafresa,Á. Mazcuñán Herreros,J. Sullivan*

Main category: cs.CV

TL;DR: This study explores utilizing frozen diffusion model features for fine-grained image classification, outperforming traditional methods in various challenging scenarios, especially in monitoring plankton populations.


<details>
  <summary>Details</summary>
Motivation: Although state-of-the-art for image generation, the use of diffusion models as general-purpose feature encoders has not been deeply investigated.

Method: They employed frozen diffusion features by extracting intermediate denoising features across layers and timesteps, pairing them with linear classifiers, and evaluated the model using plankton datasets with extensive conditions.

Result: Frozen diffusion features proved competitive against supervised baselines and outperformed self-supervised methods in both balanced data and long-tail distributions while maintaining high accuracy under significant distribution shifts.

Conclusion: Diffusion models show strong promise as feature encoders, with competitive and robust performance in specialized, real-world settings like plankton monitoring, even under substantial distribution challenges.

Abstract: Diffusion models have emerged as state-of-the-art generative methods for image synthesis, yet their potential as general-purpose feature encoders remains underexplored. Trained for denoising and generation without labels, they can be interpreted as self-supervised learners that capture both low- and high-level structure. We show that a frozen diffusion backbone enables strong fine-grained recognition by probing intermediate denoising features across layers and timesteps and training a linear classifier for each pair. We evaluate this in a real-world plankton-monitoring setting with practical impact, using controlled and comparable training setups against established supervised and self-supervised baselines. Frozen diffusion features are competitive with supervised baselines and outperform other self-supervised methods in both balanced and naturally long-tailed settings. Out-of-distribution evaluations on temporally and geographically shifted plankton datasets further show that frozen diffusion features maintain strong accuracy and Macro F1 under substantial distribution shift.

</details>


### [492] [SGW-GAN: Sliced Gromov-Wasserstein Guided GANs for Retinal Fundus Image Enhancement](https://arxiv.org/abs/2601.13417)
*Yujian Xiong,Xuanzhao Dong,Wenhui Zhu,Xin Li,Oana Dumitrascu,Yalin Wang*

Main category: cs.CV

TL;DR: The paper introduces SGW-GAN, a novel framework for enhancing retinal fundus images while maintaining clinical fidelity and improving downstream tasks like diabetic retinopathy grading.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing GAN- and diffusion-based methods in preserving intra-class geometric structures in retinal image enhancement, which impacts downstream tasks such as lesion detection and disease classification.

Method: The authors propose SGW-GAN, which integrates Sliced Gromov Wasserstein (SGW) discrepancy, reducing computational costs while aligning distributions through relational fidelity for effective retinal image enhancement.

Result: SGW-GAN achieves visually enhanced retinal images, improved diabetic retinopathy grading, and the lowest GW discrepancy across disease labels on public datasets.

Conclusion: SGW-GAN demonstrates efficiency and clinical reliability for unpaired retinal image enhancement, offering advancements for ophthalmic screening and diagnostic applications.

Abstract: Retinal fundus photography is indispensable for ophthalmic screening and diagnosis, yet image quality is often degraded by noise, artifacts, and uneven illumination. Recent GAN- and diffusion-based enhancement methods improve perceptual quality by aligning degraded images with high-quality distributions, but our analysis shows that this focus can distort intra-class geometry: clinically related samples become dispersed, disease-class boundaries blur, and downstream tasks such as grading or lesion detection are harmed. The Gromov Wasserstein (GW) discrepancy offers a principled solution by aligning distributions through internal pairwise distances, naturally preserving intra-class structure, but its high computational cost restricts practical use. To overcome this, we propose SGW-GAN, the first framework to incorporate Sliced GW (SGW) into retinal image enhancement. SGW approximates GW via random projections, retaining relational fidelity while greatly reducing cost. Experiments on public datasets show that SGW-GAN produces visually compelling enhancements, achieves superior diabetic retinopathy grading, and reports the lowest GW discrepancy across disease labels, demonstrating both efficiency and clinical fidelity for unpaired medical image enhancement.

</details>


### [493] [Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation](https://arxiv.org/abs/2601.13440)
*Mohit Kakda,Mirudula Shri Muthukumaran,Uttapreksha Patel,Lawrence Swaminathan Xavier Prince*

Main category: cs.CV

TL;DR: The paper analyzes various Vision-Language Model (VLM) approaches, focusing on their effectiveness in anomaly classification and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore and enhance the capability of Vision-Language Models (CLIP-like models) for anomaly detection by eliminating the need for extensive labeled datasets and task-specific training.

Method: The authors systematically investigate approaches such as WinCLIP, AprilLab, and compositional prompt strategies, evaluating them on critical dimensions like feature extraction, prompt design, and zero-shot/few-shot scenarios.

Result: Their experiments on benchmarks (MVTec AD and VisA) provide insights into performance metrics, like accuracy, precision, inference efficiency, and cross-domain generalization.

Conclusion: The paper provides foundational knowledge on VLMs for anomaly detection, practical recommendations for selection, and highlights research gaps for future development.

Abstract: Vision-Language Models (VLMs), particularly CLIP, have revolutionized anomaly detection by enabling zero-shot and few-shot defect identification without extensive labeled datasets. By learning aligned representations of images and text, VLMs facilitate anomaly classification and segmentation through natural language descriptions of normal and abnormal states, eliminating traditional requirements for task-specific training or defect examples. This project presents a comprehensive analysis of VLM-based approaches for anomaly classification (AC) and anomaly segmentation (AS). We systematically investigate key architectural paradigms including sliding window-based dense feature extraction (WinCLIP), multi-stage feature alignment with learnable projections (AprilLab framework), and compositional prompt ensemble strategies. Our analysis evaluates these methods across critical dimensions: feature extraction mechanisms, text-visual alignment strategies, prompt engineering techniques, zero-shot versus few-shot trade-offs, computational efficiency, and cross-domain generalization. Through rigorous experimentation on benchmarks such as MVTec AD and VisA, we compare classification accuracy, segmentation precision, and inference efficiency. The primary contribution is a foundational understanding of how and why VLMs succeed in anomaly detection, synthesizing practical insights for method selection and identifying current limitations. This work aims to facilitate informed adoption of VLM-based methods in industrial quality control and guide future research directions.

</details>


### [494] [Optical Linear Systems Framework for Event Sensing and Computational Neuromorphic Imaging](https://arxiv.org/abs/2601.13498)
*Nimrod Kruger,Nicholas Owen Ralph,Gregory Cohen,Paul Hurley*

Main category: cs.CV

TL;DR: This paper introduces a processing pipeline to bridge event vision sensors with computational imaging by mapping event streams to measurable quantities and using inverse filtering techniques.


<details>
  <summary>Details</summary>
Motivation: Event vision sensors are innovative but their nonlinear event data format challenges integration with linear-based imaging methods.

Method: The paper proposes mapping event streams to log-intensity estimates and derivatives, embedding them into linear dynamic models with a time-varying point spread function, and applying Wiener deconvolution for signal extraction.

Result: Validation is conducted through simulations (with single and overlapping sources using modulated defocus) and real data (telescopic star field imaging) to demonstrate localisation and separability of sources.

Conclusion: The framework successfully bridges event vision sensors and dynamic optical system modeling, contributing a method compatible with computational imaging.

Abstract: Event vision sensors (neuromorphic cameras) output sparse, asynchronous ON/OFF events triggered by log-intensity threshold crossings, enabling microsecond-scale sensing with high dynamic range and low data bandwidth. As a nonlinear system, this event representation does not readily integrate with the linear forward models that underpin most computational imaging and optical system design. We present a physics-grounded processing pipeline that maps event streams to estimates of per-pixel log-intensity and intensity derivatives, and embeds these measurements in a dynamic linear systems model with a time-varying point spread function. This enables inverse filtering directly from event data, using frequency-domain Wiener deconvolution with a known (or parameterised) dynamic transfer function. We validate the approach in simulation for single and overlapping point sources under modulated defocus, and on real event data from a tunable-focus telescope imaging a star field, demonstrating source localisation and separability. The proposed framework provides a practical bridge between event sensing and model-based computational imaging for dynamic optical systems.

</details>


### [495] [DIS2: Disentanglement Meets Distillation with Classwise Attention for Robust Remote Sensing Segmentation under Missing Modalities](https://arxiv.org/abs/2601.13502)
*Nhi Kieu,Kien Nguyen,Arnold Wiliem,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: The paper introduces DIS2, a novel method to address missing modalities in remote sensing via a reformulated synergy of disentanglement learning and knowledge distillation, greatly improving performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in multimodal learning for remote sensing due to highly heterogeneous data and missing modalities, which render conventional methods ineffective.

Method: The method involves DIS2, a paradigm combining disentanglement learning and knowledge distillation (termed DLKD), with class-specific feature learning (CFLM) and hierarchical hybrid fusion (HF) for missing feature compensation.

Result: The approach significantly surpasses state-of-the-art methods across remote sensing benchmarks, demonstrating its effectiveness in overcoming modality-related challenges.

Conclusion: DIS2 successfully compensates for missing modalities, adapts to class-specific challenges, and leverages multi-resolution features to advance remote sensing predictive accuracy.

Abstract: The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.

</details>


### [496] [GO-MLVTON: Garment Occlusion-Aware Multi-Layer Virtual Try-On with Diffusion Models](https://arxiv.org/abs/2601.13524)
*Yang Yu,Yunze Deng,Yige Zhang,Yanjie Xiao,Youkun Ou,Wenhao Hu,Mingchao Li,Bin Feng,Wenyu Liu,Dandan Zheng,Jingdong Chen*

Main category: cs.CV

TL;DR: This paper introduces GO-MLVTON, the first method that addresses realistic multi-layer virtual try-on (ML-VTON) for garments by considering occlusion relationships.


<details>
  <summary>Details</summary>
Motivation: Existing VTON methods overlook the challenge of achieving visually realistic multi-layer garment dressing, particularly in modeling occlusion relationships between layers.

Method: The proposed method uses a Garment Occlusion Learning module for occlusion relationships and a StableDiffusion-based Garment Morphing & Fitting module for realistic garment deformation. A new benchmark dataset (MLG) and a metric (LACD) are also introduced.

Result: GO-MLVTON achieves state-of-the-art results in generating high-quality, visually realistic multi-layer try-on outcomes.

Conclusion: This work advances VTON by addressing complex multi-layer garment dressing, setting a new standard with novel methods, datasets, and evaluation metrics.

Abstract: Existing Image-based virtual try-on (VTON) methods primarily focus on single-layer or multi-garment VTON, neglecting multi-layer VTON (ML-VTON), which involves dressing multiple layers of garments onto the human body with realistic deformation and layering to generate visually plausible outcomes. The main challenge lies in accurately modeling occlusion relationships between inner and outer garments to reduce interference from redundant inner garment features. To address this, we propose GO-MLVTON, the first multi-layer VTON method, introducing the Garment Occlusion Learning module to learn occlusion relationships and the StableDiffusion-based Garment Morphing & Fitting module to deform and fit garments onto the human body, producing high-quality multi-layer try-on results. Additionally, we present the MLG dataset for this task and propose a new metric named Layered Appearance Coherence Difference (LACD) for evaluation. Extensive experiments demonstrate the state-of-the-art performance of GO-MLVTON. Project page: https://upyuyang.github.io/go-mlvton/.

</details>


### [497] [DiffFace-Edit: A Diffusion-Based Facial Dataset for Forgery-Semantic Driven Deepfake Detection Analysis](https://arxiv.org/abs/2601.13551)
*Feng Ding,Wenhui Yi,Xinan He,Mengyao Xiao,Jianfeng Xu,Jianqiang Du*

Main category: cs.CV

TL;DR: This paper introduces the DiffFace-Edit dataset with over two million AI-generated images that contain fine-grained facial manipulations and explores their impact on detection systems, particularly detector-evasive cases.


<details>
  <summary>Details</summary>
Motivation: To address the lack of datasets with fine-grained facial manipulations and the unexamined impact of detector-evasive splice attacks on detection models.

Method: Created the DiffFace-Edit dataset with edits across eight facial regions, analyzed detector-evasive samples' impact, and proposed a cross-domain evaluation combining IMDL methods.

Result: The study provides a comprehensive analysis of fine-grained facial manipulations and their implications on detection models by leveraging the large-scale DiffFace-Edit dataset.

Conclusion: DiffFace-Edit serves as a significant tool for studying facial manipulations and improving the robustness of detection systems against fine-grained, detector-evasive manipulations.

Abstract: Generative models now produce imperceptible, fine-grained manipulated faces, posing significant privacy risks. However, existing AI-generated face datasets generally lack focus on samples with fine-grained regional manipulations. Furthermore, no researchers have yet studied the real impact of splice attacks, which occur between real and manipulated samples, on detectors. We refer to these as detector-evasive samples. Based on this, we introduce the DiffFace-Edit dataset, which has the following advantages: 1) It contains over two million AI-generated fake images. 2) It features edits across eight facial regions (e.g., eyes, nose) and includes a richer variety of editing combinations, such as single-region and multi-region edits. Additionally, we specifically analyze the impact of detector-evasive samples on detection models. We conduct a comprehensive analysis of the dataset and propose a cross-domain evaluation that combines IMDL methods. Dataset will be available at https://github.com/ywh1093/DiffFace-Edit.

</details>


### [498] [ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch](https://arxiv.org/abs/2601.13606)
*Zheng Liu,Honglin Lin,Chonghan Qin,Xiaoyang Wang,Xin Gao,Yu Li,Mengzhang Cai,Yun Zhu,Zhanping Zhong,Qizhi Pei,Zhuoshi Pan,Xiaoran Shang,Bin Cui,Conghui He,Wentao Zhang,Lijun Wu*

Main category: cs.CV

TL;DR: The paper introduces ChartVerse, a framework for synthesizing complex charts and reliable reasoning data to improve Vision Language Models (VLMs).


<details>
  <summary>Details</summary>
Motivation: Current datasets for chart reasoning are insufficient as they are either simplistic, hallucination-prone, or lack reasoning depth. There is a need for high-quality training data to enhance open-source Vision Language Models.

Method: ChartVerse employs a two-step process: (1) Synthesis of high-complexity charts using Rollout Posterior Entropy (RPE) and a complexity-aware chart coder. (2) Truth-anchored inverse QA synthesis with deterministic answer-first generation and strict consistency verification, coupled with filtering difficult reasoning samples using fail-rate analysis and Chain-of-Thought distillation.

Result: ChartVerse-8B model achieves state-of-the-art performance in chart reasoning, surpassing its teacher model and rivaling larger models like Qwen3-VL-32B-Thinking.

Conclusion: ChartVerse effectively addresses the limitations of today’s datasets by generating complex charts and rigorous reasoning data, significantly enhancing the capability of Vision Language Models in chart reasoning tasks.

Abstract: Chart reasoning is a critical capability for Vision Language Models (VLMs). However, the development of open-source models is severely hindered by the lack of high-quality training data. Existing datasets suffer from a dual challenge: synthetic charts are often simplistic and repetitive, while the associated QA pairs are prone to hallucinations and lack the reasoning depth required for complex tasks. To bridge this gap, we propose ChartVerse, a scalable framework designed to synthesize complex charts and reliable reasoning data from scratch. (1) To address the bottleneck of simple patterns, we first introduce Rollout Posterior Entropy (RPE), a novel metric that quantifies chart complexity. Guided by RPE, we develop complexity-aware chart coder to autonomously synthesize diverse, high-complexity charts via executable programs. (2) To guarantee reasoning rigor, we develop truth-anchored inverse QA synthesis. Diverging from standard generation, we adopt an answer-first paradigm: we extract deterministic answers directly from the source code, generate questions conditional on these anchors, and enforce strict consistency verification. To further elevate difficulty and reasoning depth, we filter samples based on model fail-rate and distill high-quality Chain-of-Thought (CoT) reasoning. We curate ChartVerse-SFT-600K and ChartVerse-RL-40K using Qwen3-VL-30B-A3B-Thinking as the teacher. Experimental results demonstrate that ChartVerse-8B achieves state-of-the-art performance, notably surpassing its teacher and rivaling the stronger Qwen3-VL-32B-Thinking.

</details>


### [499] [CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models](https://arxiv.org/abs/2601.13622)
*Donghee Lee,Rui Cai,Zhe Zhao*

Main category: cs.CV

TL;DR: The paper introduces CARPE, a model-agnostic framework designed to improve the performance of Large Vision-Language Models (LVLMs) on vision-centric tasks, particularly image classification, through context-aware representation prioritization.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of LVLMs struggling with vision-centric tasks, such as image classification, as they underperform relative to their base vision encoders.

Method: CARPE uses vision-integration layers and a context-aware ensemble strategy to adaptively weight visual and textual modalities, enabling better balance between image representations and language reasoning.

Result: CARPE demonstrates consistent improvements in performance on both image classification and vision-language benchmarks through experiments, proving its effectiveness in improving generalization capabilities.

Conclusion: CARPE can be integrated with most open-source LVLMs, ensuring its adaptability for enhancing vision-language tasks on diverse architectures.

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model's ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.

</details>


### [500] [Scaling Test-time Inference for Visual Grounding](https://arxiv.org/abs/2601.13633)
*Guanqi Zhan,Changye Li,Zhijian Liu,Yao Lu,Yi Wu,Song Han,Ligeng Zhu*

Main category: cs.CV

TL;DR: The paper introduces a method called Efficient visual Grounding language Models (EGM) to improve the grounding of smaller visual-language models (VLMs) by scaling test-time computation efficiently, achieving performance comparable to larger models while being significantly faster.


<details>
  <summary>Details</summary>
Motivation: The need to enable smaller visual-language models (VLMs) to perform visual grounding effectively, as larger models, despite their superior performance, are computationally expensive and slow to deploy.

Method: EGM scales the test-time computation by optimizing the number of generated tokens in smaller VLMs, making them both deployment-friendly and faster, while retaining high performance.

Result: On the RefCOCO benchmark, EGM-Qwen3-VL-8B achieved 91.4 IoU with 737ms latency, significantly outperforming Qwen3-VL-235B (90.5 IoU, 4,320ms latency) in terms of efficiency and maintaining comparable accuracy.

Conclusion: The proposed EGM method improves the grounding capabilities of small visual language models to match or outperform those of larger models, offering a more efficient solution for real-world deployment and establishing its generality through additional experiments.

Abstract: Visual grounding is an essential capability of Visual Language Models (VLMs) to understand the real physical world. Previous state-of-the-art grounding visual language models usually have large model sizes, making them heavy for deployment and slow for inference. However, we notice that the sizes of visual encoders are nearly the same for small and large VLMs and the major difference is the sizes of the language models. Small VLMs fall behind larger VLMs in grounding because of the difference in language understanding capability rather than visual information handling. To mitigate the gap, we introduce 'Efficient visual Grounding language Models' (EGM): a method to scale the test-time computation (#generated tokens). Scaling the test-time computation of a small model is deployment-friendly, and yields better end-to-end latency as the cost of each token is much cheaper compared to directly running a large model. On the RefCOCO benchmark, our EGM-Qwen3-VL-8B demonstrates 91.4 IoU with an average of 737ms (5.9x faster) latency while Qwen3-VL-235B demands 4,320ms to achieve 90.5 IoU. To validate our approach's generality, we further set up a new amodal grounding setting that requires the model to predict both the visible and occluded parts of the objects. Experiments show our method can consistently and significantly improve the vanilla grounding and amodal grounding capabilities of small models to be on par with or outperform the larger models, thereby improving the efficiency for visual grounding.

</details>


### [501] [Face-Voice Association with Inductive Bias for Maximum Class Separation](https://arxiv.org/abs/2601.13651)
*Marta Moscati,Oleksandr Kats,Mubashir Noman,Muhammad Zaigham Zaheer,Yufang Hou,Markus Schedl,Shah Nawaz*

Main category: cs.CV

TL;DR: The paper studies face-voice association and proposes a novel method using maximum class separation as inductive bias for multimodal speaker representation.


<details>
  <summary>Details</summary>
Motivation: To explore the unexplored approach of using maximum class separation as inductive bias in the domain of face-voice association, which has been beneficial in other classification tasks.

Method: The authors propose and develop a novel framework that imposes maximum class separation as an inductive bias among multimodal speaker representations, enhanced with inter-class orthogonality loss.

Result: The proposed method achieves state-of-the-art results in two face-voice association tasks, with additional ablation studies confirming its effectiveness and synergy with orthogonality loss.

Conclusion: This work introduces maximum class separation as an inductive bias to multimodal learning, demonstrating its effectiveness and paving the way for future explorations in the field.

Abstract: Face-voice association is widely studied in multimodal learning and is approached representing faces and voices with embeddings that are close for a same person and well separated from those of others. Previous work achieved this with loss functions. Recent advancements in classification have shown that the discriminative ability of embeddings can be strengthened by imposing maximum class separation as inductive bias. This technique has never been used in the domain of face-voice association, and this work aims at filling this gap. More specifically, we develop a method for face-voice association that imposes maximum class separation among multimodal representations of different speakers as an inductive bias. Through quantitative experiments we demonstrate the effectiveness of our approach, showing that it achieves SOTA performance on two task formulation of face-voice association. Furthermore, we carry out an ablation study to show that imposing inductive bias is most effective when combined with losses for inter-class orthogonality. To the best of our knowledge, this work is the first that applies and demonstrates the effectiveness of maximum class separation as an inductive bias in multimodal learning; it hence paves the way to establish a new paradigm.

</details>


### [502] [VIAFormer: Voxel-Image Alignment Transformer for High-Fidelity Voxel Refinement](https://arxiv.org/abs/2601.13664)
*Tiancheng Fang,Bowen Pan,Lingxi Chen,Jiangjing Lyu,Chengfei Lyu,Chaoyue Niu,Fan Wu*

Main category: cs.CV

TL;DR: VIAFormer improves voxel refinement using multi-view images, achieving state-of-the-art results in 3D pipelines.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of repairing incomplete and noisy voxel shapes using multi-view images.

Method: The model integrates an Image Index for spatial grounding, a Correctional Flow objective for trajectory learning, and a Hybrid Stream Transformer for effective cross-modal fusion.

Result: VIAFormer surpasses previous methods in handling severe synthetic and realistic voxel corruptions on outputs from Vision Foundation Models.

Conclusion: The study highlights VIAFormer's practicality in real-world 3D creation pipelines, setting a new standard for voxel-based methods in large-model and big-data contexts.

Abstract: We propose VIAFormer, a Voxel-Image Alignment Transformer model designed for Multi-view Conditioned Voxel Refinement--the task of repairing incomplete noisy voxels using calibrated multi-view images as guidance. Its effectiveness stems from a synergistic design: an Image Index that provides explicit 3D spatial grounding for 2D image tokens, a Correctional Flow objective that learns a direct voxel-refinement trajectory, and a Hybrid Stream Transformer that enables robust cross-modal fusion. Experiments show that VIAFormer establishes a new state of the art in correcting both severe synthetic corruptions and realistic artifacts on the voxel shape obtained from powerful Vision Foundation Models. Beyond benchmarking, we demonstrate VIAFormer as a practical and reliable bridge in real-world 3D creation pipelines, paving the way for voxel-based methods to thrive in large-model, big-data wave.

</details>


### [503] [Transformer based Multi-task Fusion Network for Food Spoilage Detection and Shelf life Forecasting](https://arxiv.org/abs/2601.13665)
*Mounika Kanulla,Rajasree Dadigi,Sailaja Thota,Vivek Yelleti*

Main category: cs.CV

TL;DR: The paper proposes hybrid deep learning models combining CNN with LSTM and DeiT transformer for vegetable classification, spoilage detection, and shelf life forecasting.


<details>
  <summary>Details</summary>
Motivation: To address food wastage caused by spoilage in the agricultural supply chain through accurate detection and forecasting methods.

Method: Two fusion architectures combining CNN with CNN-LSTM and CNN with DeiT Transformer were proposed and tested on a freshly developed dataset capturing vegetable images across spoilage progression.

Result: CNN+DeiT Transformer achieved the best results, yielding an F1-score of 0.98 in classification, 0.61 in spoilage detection, and competitive error metrics for forecasting such as MSE of 3.58 and SMAPE of 41.66%.

Conclusion: Fusion architectures proved superior compared to other models and were validated for reliability with noisy images and LIME analysis for interpretability.

Abstract: Food wastage is one of the critical challenges in the agricultural supply chain, and accurate and effective spoilage detection can help to reduce it. Further, it is highly important to forecast the spoilage information. This aids the longevity of the supply chain management in the agriculture field. This motivated us to propose fusion based architectures by combining CNN with LSTM and DeiT transformer for the following multi-tasks simultaneously: (i) vegetable classification, (ii) food spoilage detection, and (iii) shelf life forecasting. We developed a dataset by capturing images of vegetables from their fresh state until they were completely spoiled. From the experimental analysis it is concluded that the proposed fusion architectures CNN+CNN-LSTM and CNN+DeiT Transformer outperformed several deep learning models such as CNN, VGG16, ResNet50, Capsule Networks, and DeiT Transformers. Overall, CNN + DeiT Transformer yielded F1-score of 0.98 and 0.61 in vegetable classification and spoilage detection respectively and mean squared error (MSE) and symmetric mean absolute percentage error (SMAPE) of 3.58, and 41.66% respectively in spoilage forecasting. Further, the reliability of the fusion models was validated on noisy images and integrated with LIME to visualize the model decisions.

</details>


### [504] [Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging](https://arxiv.org/abs/2601.13677)
*Carsten T. Lüth,Jeremias Traub,Kim-Celine Kahl,Till J. Bungert,Lukas Klein,Lars Krämer,Paul F. Jäger,Klaus Maier-Hein,Fabian Isensee*

Main category: cs.CV

TL;DR: The paper introduces ClaSP PE, a query strategy for 3D biomedical image segmentation that outperforms random sampling baselines by addressing class imbalance and redundancy issues.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges of high annotation costs, class imbalance, and redundancy in 3D biomedical image segmentation, where existing methods fail to consistently outperform random sampling baselines.

Method: ClaSP PE employs class-stratified querying for coverage of underrepresented structures and integrates power noising with a decaying schedule to increase diversity in early selections and focus on exploitation later.

Result: Evaluations using four datasets and 24 experimental settings show that ClaSP PE outperforms random baselines in segmentation quality and annotation efficiency while also robustly generalizing to unseen datasets without manual adaptation.

Conclusion: ClaSP PE consistently delivers better performance and annotation efficiency in real-world scenarios, addressing key limitations of Active Learning methods in 3D biomedical segmentation, with an open-source implementation available for practical application.

Abstract: Active learning (AL) has the potential to drastically reduce annotation costs in 3D biomedical image segmentation, where expert labeling of volumetric data is both time-consuming and expensive. Yet, existing AL methods are unable to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without a reliable solution. We introduce Class-stratified Scheduled Power Predictive Entropy (ClaSP PE), a simple and effective query strategy that addresses two key limitations of standard uncertainty-based AL methods: class imbalance and redundancy in early selections. ClaSP PE combines class-stratified querying to ensure coverage of underrepresented structures and log-scale power noising with a decaying schedule to enforce query diversity in early-stage AL and encourage exploitation later. In our evaluation on 24 experimental settings using four 3D biomedical datasets within the comprehensive nnActive benchmark, ClaSP PE is the only method that generally outperforms improved random baselines in terms of both segmentation quality with statistically significant gains, whilst remaining annotation efficient. Furthermore, we explicitly simulate the real-world application by testing our method on four previously unseen datasets without manual adaptation, where all experiment parameters are set according to predefined guidelines. The results confirm that ClaSP PE robustly generalizes to novel tasks without requiring dataset-specific tuning. Within the nnActive framework, we present compelling evidence that an AL method can consistently outperform random baselines adapted to 3D segmentation, in terms of both performance and annotation efficiency in a realistic, close-to-production scenario. Our open-source implementation and clear deployment guidelines make it readily applicable in practice. Code is at https://github.com/MIC-DKFZ/nnActive.

</details>


### [505] [Dynamic Differential Linear Attention: Enhancing Linear Diffusion Transformer for High-Quality Image Generation](https://arxiv.org/abs/2601.13683)
*Boyuan Cao,Xingbo Yao,Chenhui Wang,Jiaxin Ye,Yujie Wei,Hongming Shan*

Main category: cs.CV

TL;DR: The paper introduces Dynamic Differential Linear Attention (DyDiLA) to address oversmoothing in linear attention mechanisms of diffusion transformers.


<details>
  <summary>Details</summary>
Motivation: To overcome the scalability limitations of quadratic self-attention cost in diffusion transformers (DiTs) while preserving generative performance.

Method: DyDiLA improves linear attention with three innovations: dynamic projection module, dynamic measure kernel, and token differential operator.

Result: DyDi-LiT, based on DyDiLA, achieves superior performance over state-of-the-art models in image generation metrics.

Conclusion: DyDiLA effectively mitigates the oversmoothing issue in linear diffusion transformers, enhancing scalability and generative quality.

Abstract: Diffusion transformers (DiTs) have emerged as a powerful architecture for high-fidelity image generation, yet the quadratic cost of self-attention poses a major scalability bottleneck. To address this, linear attention mechanisms have been adopted to reduce computational cost; unfortunately, the resulting linear diffusion transformers (LiTs) models often come at the expense of generative performance, frequently producing over-smoothed attention weights that limit expressiveness. In this work, we introduce Dynamic Differential Linear Attention (DyDiLA), a novel linear attention formulation that enhances the effectiveness of LiTs by mitigating the oversmoothing issue and improving generation quality. Specifically, the novelty of DyDiLA lies in three key designs: (i) dynamic projection module, which facilitates the decoupling of token representations by learning with dynamically assigned knowledge; (ii) dynamic measure kernel, which provides a better similarity measurement to capture fine-grained semantic distinctions between tokens by dynamically assigning kernel functions for token processing; and (iii) token differential operator, which enables more robust query-to-key retrieval by calculating the differences between the tokens and their corresponding information redundancy produced by dynamic measure kernel. To capitalize on DyDiLA, we introduce a refined LiT, termed DyDi-LiT, that systematically incorporates our advancements. Extensive experiments show that DyDi-LiT consistently outperforms current state-of-the-art (SOTA) models across multiple metrics, underscoring its strong practical potential.

</details>


### [506] [Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles](https://arxiv.org/abs/2601.13705)
*Maria Lymperaiou,Vasileios Karampinis,Giorgos Filandrianos,Angelos Vlachos,Chrysoula Zerva,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: This paper surveys the use of visual puzzles to evaluate Large Vision-Language Models (LVLMs), analyzing their reasoning mechanisms and identifying limitations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to use visual puzzles as a diagnostic tool to study and improve the reasoning abilities of LVLMs, which are important for advancing cognitive tasks.

Method: The authors organize visual puzzles into reasoning categories such as inductive, analogical, and deductive, analyzing their link to cognitive operations.

Result: The survey finds limitations in LVLMs, like weak generalization, tight perception-reasoning entanglement, and gaps between explanation and execution abilities.

Conclusion: Visual puzzles are effective diagnostic tools and should guide the development of better reasoning-focused multimodal systems and benchmarks.

Abstract: Puzzles have long served as compact and revealing probes of human cognition, isolating abstraction, rule discovery, and systematic reasoning with minimal reliance on prior knowledge. Leveraging these properties, visual puzzles have recently emerged as a powerful diagnostic tool for evaluating the reasoning abilities of Large Vision-Language Models (LVLMs), offering controlled, verifiable alternatives to open-ended multimodal benchmarks. This survey provides a unified perspective of visual puzzle reasoning in LVLMs. We frame visual puzzles through a common abstraction and organize existing benchmarks by the reasoning mechanisms they target (inductive, analogical, algorithmic, deductive, and geometric/spatial), thereby linking puzzle design to the cognitive operations required for solving. Synthesizing empirical evidence across these categories, we identify consistent limitations in current models, including brittle generalization, tight entanglement between perception and reasoning, and a persistent gap between fluent explanations and faithful execution. By framing visual puzzles as diagnostic instruments rather than task formats, this survey elaborates on the state of LVLM reasoning and outlines key directions for future benchmarks and reasoning-aware multimodal systems.

</details>


### [507] [ParkingTwin: Training-Free Streaming 3D Reconstruction for Parking-Lot Digital Twins](https://arxiv.org/abs/2601.13706)
*Xinhao Liu,Yu Wang,Xiansheng Guo,Gordon Owusu Boateng,Yu Cao,Haonan Si,Xingchen Guo,Nirwan Ansari*

Main category: cs.CV

TL;DR: ParkingTwin is a lightweight, real-time, and training-free 3D reconstruction system for parking lots, focusing on metrics like dynamic occlusion filtering and robust texture fusion.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in robot-oriented parking-lot reconstruction caused by weak parallax, dynamic occlusions, lighting issues, and the inefficiencies of neural rendering for real-time AVP needs.

Method: The method involves OSM-prior geometric construction, real-time dynamic filtering with a quad-modal constraint, and illumination-robust CIELAB-based fusion for lightweight, high-performance 3D reconstruction.

Result: The system achieves 30+ FPS on a GTX 1660, delivering significant improvements in SSIM (+16.0%), speed (~15x), and GPU memory efficiency (83.3% reduction) compared to state-of-the-art methods.

Conclusion: ParkingTwin provides an efficient, edge-compatible 3D reconstruction solution for parking lots, streamlining digital twin integration for applications like AVP.

Abstract: High-fidelity parking-lot digital twins provide essential priors for path planning, collision checking, and perception validation in Automated Valet Parking (AVP). Yet robot-oriented reconstruction faces a trilemma: sparse forward-facing views cause weak parallax and ill-posed geometry; dynamic occlusions and extreme lighting hinder stable texture fusion; and neural rendering typically needs expensive offline optimization, violating edge-side streaming constraints. We propose ParkingTwin, a training-free, lightweight system for online streaming 3D reconstruction. First, OSM-prior-driven geometric construction uses OpenStreetMap semantic topology to directly generate a metric-consistent TSDF, replacing blind geometric search with deterministic mapping and avoiding costly optimization. Second, geometry-aware dynamic filtering employs a quad-modal constraint field (normal/height/depth consistency) to reject moving vehicles and transient occlusions in real time. Third, illumination-robust fusion in CIELAB decouples luminance and chromaticity via adaptive L-channel weighting and depth-gradient suppression, reducing seams under abrupt lighting changes. ParkingTwin runs at 30+ FPS on an entry-level GTX 1660. On a 68,000 m^2 real-world dataset, it achieves SSIM 0.87 (+16.0%), delivers about 15x end-to-end speedup, and reduces GPU memory by 83.3% compared with state-of-the-art 3D Gaussian Splatting (3DGS) that typically requires high-end GPUs (RTX 4090D). The system outputs explicit triangle meshes compatible with Unity/Unreal digital-twin pipelines. Project page: https://mihoutao-liu.github.io/ParkingTwin/

</details>


### [508] [Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2601.13707)
*Yujin Jo,Sangyoon Bae,Taesup Kim*

Main category: cs.CV

TL;DR: The paper addresses hallucinations in large vision-language models by proposing a method called Attention-space Contrastive Guidance (ACG) to enhance text generation by grounding it visually and reducing language prior dependence.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome hallucinations in vision-language models caused by over-reliance on language priors, which leads to object misidentification and visually inconsistent descriptions.

Method: The authors introduce Attention-space Contrastive Guidance (ACG), a computationally efficient single-pass mechanism integrated into self-attention layers. It builds both vision-language and language-only attention paths, and applies an orthogonalized correction to amplify visual contributions.

Result: Experiments on CHAIR and POPE benchmarks demonstrate that ACG provides state-of-the-art results in faithfulness and caption quality while halving computational latency compared to prior methods.

Conclusion: ACG offers a principled, efficient, and effective alternative for mitigating hallucinations in vision-language models, reducing computational cost while improving performance.

Abstract: Hallucinations in large vision-language models (LVLMs) often arise when language priors dominate over visual evidence, causing object misidentification and visually inconsistent descriptions. We address this issue by framing hallucination mitigation as contrastive guidance, steering generation toward visually grounded and semantically faithful text. This approach regulates the model's internal behavior by reducing over-dependence on language priors and contrasting visually grounded with language-only representations. We propose Attention-space Contrastive Guidance (ACG), a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward computation. This integration enables computationally efficient guidance directly embedded in the model's representation contextualization. To correct approximation bias introduced by the single-pass formulation, we further apply an orthogonalized correction that removes components aligned with the language-only path, selectively amplifying visual contributions. Experiments on the CHAIR and POPE benchmarks show that ACG achieves state-of-the-art faithfulness and caption quality while significantly reducing computational cost. Our method establishes a principled and efficient alternative, reducing latency by up to 2x compared to prior contrastive decoding methods that require multiple forward passes.

</details>


### [509] [MVGD-Net: A Novel Motion-aware Video Glass Surface Detection Network](https://arxiv.org/abs/2601.13715)
*Yiwei Lu,Hao Huang,Tao Yan*

Main category: cs.CV

TL;DR: This paper focuses on detecting glass surfaces in videos using motion inconsistency cues. It proposes a novel network, MVGD-Net, and introduces a new large-scale dataset to train the model.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of detecting transparent glass surfaces, which pose risks to vision systems like robots and drones as they can misconceive the environment.

Method: The approach leverages motion inconsistency cues where objects reflected from the glass move slower than regular objects. MVGD-Net utilizes modules like CMFM, HGAM, TCAM, and TSD, along with integrating optical flow and spatial-temporal features.

Result: The proposed MVGD-Net achieves superior performance compared to state-of-the-art methods in identifying glass regions in videos, validated through extensive experiments.

Conclusion: MVGD-Net provides an effective solution for detecting glass regions using motion cues and introduces a new dataset, enabling advancements in video-based glass surface detection.

Abstract: Glass surface ubiquitous in both daily life and professional environments presents a potential threat to vision-based systems, such as robot and drone navigation. To solve this challenge, most recent studies have shown significant interest in Video Glass Surface Detection (VGSD). We observe that objects in the reflection (or transmission) layer appear farther from the glass surfaces. Consequently, in video motion scenarios, the notable reflected (or transmitted) objects on the glass surface move slower than objects in non-glass regions within the same spatial plane, and this motion inconsistency can effectively reveal the presence of glass surfaces. Based on this observation, we propose a novel network, named MVGD-Net, for detecting glass surfaces in videos by leveraging motion inconsistency cues. Our MVGD-Net features three novel modules: the Cross-scale Multimodal Fusion Module (CMFM) that integrates extracted spatial features and estimated optical flow maps, the History Guided Attention Module (HGAM) and Temporal Cross Attention Module (TCAM), both of which further enhances temporal features. A Temporal-Spatial Decoder (TSD) is also introduced to fuse the spatial and temporal features for generating the glass region mask. Furthermore, for learning our network, we also propose a large-scale dataset, which comprises 312 diverse glass scenarios with a total of 19,268 frames. Extensive experiments demonstrate that our MVGD-Net outperforms relevant state-of-the-art methods.

</details>


### [510] [Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search](https://arxiv.org/abs/2601.13719)
*Xinlei Yin,Xiulian Peng,Xiao Li,Zhiwei Xiong,Yan Lu*

Main category: cs.CV

TL;DR: HAVEN is a new framework for understanding long videos, achieving state-of-the-art performance by integrating audiovisual entity cohesion, hierarchical indexing, and agentic search.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of understanding long videos, such as fragmented information and lack of global coherence in vision-language models.

Method: HAVEN integrates entity-level audiovisual cohesion and hierarchical video indexing, using agentic search to enable structured and dynamic retrieval for coherent reasoning.

Result: The framework achieves 84.1% overall accuracy on LVBench and excels in reasoning tasks with 80.1% accuracy, surpassing prior methods.

Conclusion: HAVEN demonstrates the importance of multimodal reasoning and structured indexing for consistent long video comprehension, setting a new performance benchmark.

Abstract: Long video understanding presents significant challenges for vision-language models due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation, typically suffer from information fragmentation and a loss of global coherence. We present HAVEN, a unified framework for long-video understanding that enables coherent and comprehensive reasoning by integrating audiovisual entity cohesion and hierarchical video indexing with agentic search. First, we preserve semantic consistency by integrating entity-level representations across visual and auditory streams, while organizing content into a structured hierarchy spanning global summary, scene, segment, and entity levels. Then we employ an agentic search mechanism to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that our method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.

</details>


### [511] [Facial Spatiotemporal Graphs: Leveraging the 3D Facial Surface for Remote Physiological Measurement](https://arxiv.org/abs/2601.13724)
*Sam Cantrill,David Ahmedt-Aristizabal,Lars Petersson,Hanna Suominen,Mohammad Ali Armin*

Main category: cs.CV

TL;DR: This paper introduces a new facial remote photoplethysmography (rPPG) framework that employs 3D-aware methods to enhance robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing rPPG methods do not align their receptive fields with the 3D facial structure, which limits performance. This paper aims to address this gap.

Method: The authors propose the Facial Spatiotemporal Graph (STGraph), which uses 3D facial mesh sequences, and MeshPhys, a graph convolutional network that processes the STGraph for physiological signal estimation.

Result: MeshPhys demonstrates state-of-the-art or competitive performance across four benchmark datasets in various evaluation settings. Ablation studies reveal the importance of surface-aligned receptive field and 3D-aware features.

Conclusion: The STGraph and MeshPhys redefine facial rPPG modeling, offering a robust, interpretable solution that is generalizable to different datasets.

Abstract: Facial remote photoplethysmography (rPPG) methods estimate physiological signals by modeling subtle color changes on the 3D facial surface over time. However, existing methods fail to explicitly align their receptive fields with the 3D facial surface-the spatial support of the rPPG signal. To address this, we propose the Facial Spatiotemporal Graph (STGraph), a novel representation that encodes facial color and structure using 3D facial mesh sequences-enabling surface-aligned spatiotemporal processing. We introduce MeshPhys, a lightweight spatiotemporal graph convolutional network that operates on the STGraph to estimate physiological signals. Across four benchmark datasets, MeshPhys achieves state-of-the-art or competitive performance in both intra- and cross-dataset settings. Ablation studies show that constraining the model's receptive field to the facial surface acts as a strong structural prior, and that surface-aligned, 3D-aware node features are critical for robustly encoding facial surface color. Together, the STGraph and MeshPhys constitute a novel, principled modeling paradigm for facial rPPG, enabling robust, interpretable, and generalizable estimation. Code is available at https://samcantrill.github.io/facial-stgraph-rppg/ .

</details>


### [512] [HiT: History-Injection Transformers for Onboard Continuous Flood Change Detection](https://arxiv.org/abs/2601.13751)
*Daniel Kyselica,Jonáš Herec,Oliver Kutis,Rado Pitoňák*

Main category: cs.CV

TL;DR: The paper introduces the History Injection mechanism (HiT) for Transformer models to enable onboard flood detection via small satellites, reducing data storage needs by over 99%. The HiT-Prithvi model achieves real-time performance with high accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need for continuous monitoring of natural disasters like floods using satellites, under the constraints of limited computational and memory resources of small satellite systems.

Method: The paper proposes a History Injection mechanism (HiT) for Transformer models to incorporate historical observation data compactly and efficiently. HiT was integrated into the Prithvi-tiny model for flood detection.

Result: The HiT mechanism was tested on the STTORM-CD flood dataset, showing comparable accuracy to baselines while achieving a 99% reduction in storage. The HiT-Prithvi model successfully delivered real-time performance at 43 FPS on Jetson Orin Nano hardware.

Conclusion: The proposed HiT-Prithvi model enables effective and practical satellite-based real-time flood monitoring, eliminating reliance on ground-based systems and paving the way for improved disaster management.

Abstract: Natural disaster monitoring through continuous satellite observation requires processing multi-temporal data under strict operational constraints. This paper addresses flood detection, a critical application for hazard management, by developing an onboard change detection system that operates within the memory and computational limits of small satellites. We propose History Injection mechanism for Transformer models (HiT), that maintains historical context from previous observations while reducing data storage by over 99\% of original image size. Moreover, testing on the STTORM-CD flood dataset confirms that the HiT mechanism within the Prithvi-tiny foundation model maintains detection accuracy compared to the bitemporal baseline. The proposed HiT-Prithvi model achieved 43 FPS on Jetson Orin Nano, a representative onboard hardware used in nanosats. This work establishes a practical framework for satellite-based continuous monitoring of natural disasters, supporting real-time hazard assessment without dependency on ground-based processing infrastructure. Architecture as well as model checkpoints is available at https://github.com/zaitra/HiT-change-detection

</details>


### [513] [PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval](https://arxiv.org/abs/2601.13797)
*Gabriele Serussi,David Vainshtein,Jonathan Kouchly,Dotan Di Castro,Chaim Baskin*

Main category: cs.CV

TL;DR: The paper introduces PREGEN, a novel and efficient framework for Composed Video Retrieval (CoVR) that advances performance significantly without requiring Vision-Language Model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for CoVR fail to efficiently utilize Vision-Language Models (VLMs), often relying on outdated architectures or computationally expensive techniques like fine-tuning and caption generation.

Method: PREGEN uses a frozen pre-trained VLM paired with a lightweight encoding model. It extracts hidden states from the VLM without fine-tuning and trains a simple encoder on pooled representations for creating compact video embeddings.

Result: PREGEN achieves substantial improvements in performance on CoVR benchmarks, with Recall@1 gains of +27.23 and +69.59, demonstrating strong generalization and robustness across different VLMs.

Conclusion: PREGEN advances state-of-the-art CoVR methods by offering a computationally efficient framework that leverages modern VLMs effectively and exhibits strong semantic capabilities.

Abstract: Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.

</details>


### [514] [Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders](https://arxiv.org/abs/2601.13798)
*Kai Wittenmayer,Sukrut Rao,Amin Parchami-Araghi,Bernt Schiele,Jonas Fischer*

Main category: cs.CV

TL;DR: Insight is a language-aligned concept foundation model that offers fine-grained, spatially grounded, and interpretable representations of images, outperforming traditional opaque models in both quality and explanations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of opaque decision-making in vision foundation models by making their learned representations interpretable and spatially grounded.

Method: Insight uses a hierarchical sparse autoencoder and a foundation model with strong semantic representations to extract fine-grained concepts from images, while defining hierarchical relationships through local co-occurrence dependencies.

Result: Insight demonstrates competitive classification and segmentation performance compared to opaque models, while providing interpretable, high-quality concept-based explanations.

Conclusion: Insight enhances transparency in vision models by offering interpretable and grounded human-understandable concepts without compromising task performance, making it a promising advancement for future applications.

Abstract: Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.

</details>


### [515] [Discriminant Learning-based Colorspace for Blade Segmentation](https://arxiv.org/abs/2601.13816)
*Raül Pérez-Gonzalo,Andreas Espersen,Antonio Agudo*

Main category: cs.CV

TL;DR: This paper introduces a new algorithm, Colorspace Discriminant Analysis (CSDA), that improves image segmentation by optimizing color representation for specific tasks, with notable success shown on wind turbine blade data.


<details>
  <summary>Details</summary>
Motivation: Current image segmentation methods often fail due to poor color representation and neglecting this crucial preprocessing step.

Method: The authors extend Linear Discriminant Analysis into a deep learning framework to create CSDA, which enhances segmentation by optimizing inter-class separability and minimizing intra-class variability using a novel discriminative loss.

Result: Experiments with wind turbine blade data showed that CSDA significantly enhances segmentation accuracy, demonstrating the benefits of this preprocessing approach.

Conclusion: Optimizing color representation using CSDA is critical for improving segmentation performance, especially in domain-specific applications.

Abstract: Suboptimal color representation often hinders accurate image segmentation, yet many modern algorithms neglect this critical preprocessing step. This work presents a novel multidimensional nonlinear discriminant analysis algorithm, Colorspace Discriminant Analysis (CSDA), for improved segmentation. Extending Linear Discriminant Analysis into a deep learning context, CSDA customizes color representation by maximizing multidimensional signed inter-class separability while minimizing intra-class variability through a generalized discriminative loss. To ensure stable training, we introduce three alternative losses that enable end-to-end optimization of both the discriminative colorspace and segmentation process. Experiments on wind turbine blade data demonstrate significant accuracy gains, emphasizing the importance of tailored preprocessing in domain-specific segmentation.

</details>


### [516] [FastGHA: Generalized Few-Shot 3D Gaussian Head Avatars with Real-Time Animation](https://arxiv.org/abs/2601.13837)
*Xinya Ji,Sebastian Weiss,Manuel Kansy,Jacek Naruniec,Xun Cao,Barbara Solenthaler,Derek Bradley*

Main category: cs.CV

TL;DR: This paper introduces \OURS, a feed-forward method for creating high-fidelity 3D Gaussian-based head avatars from a few images, supporting real-time animation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D Gaussian-based head avatar modeling are limited by reliance on extensive setups or per-identity optimization, making them less efficient for unseen subjects.

Method: \OURS uses a transformer-based encoder to fuse features from DINOv3 and Stable Diffusion VAE, creating per-pixel Gaussian representations from input images. A lightweight MLP-based network predicts 3D Gaussian deformations for real-time animation with geometry smoothness enhanced via pre-trained point maps.

Result: The proposed method achieves superior rendering quality and inference efficiency when compared to previous approaches, while enabling real-time avatar animation.

Conclusion: \OURS provides a scalable and efficient solution for generating and animating 3D head avatars from limited input images, outperforming existing techniques in quality and speed.

Abstract: Despite recent progress in 3D Gaussian-based head avatar modeling, efficiently generating high fidelity avatars remains a challenge. Current methods typically rely on extensive multi-view capture setups or monocular videos with per-identity optimization during inference, limiting their scalability and ease of use on unseen subjects. To overcome these efficiency drawbacks, we propose \OURS, a feed-forward method to generate high-quality Gaussian head avatars from only a few input images while supporting real-time animation. Our approach directly learns a per-pixel Gaussian representation from the input images, and aggregates multi-view information using a transformer-based encoder that fuses image features from both DINOv3 and Stable Diffusion VAE. For real-time animation, we extend the explicit Gaussian representations with per-Gaussian features and introduce a lightweight MLP-based dynamic network to predict 3D Gaussian deformations from expression codes. Furthermore, to enhance geometric smoothness of the 3D head, we employ point maps from a pre-trained large reconstruction model as geometry supervision. Experiments show that our approach significantly outperforms existing methods in both rendering quality and inference efficiency, while supporting real-time dynamic avatar animation.

</details>


### [517] [DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes](https://arxiv.org/abs/2601.13839)
*Aisha Al-Mohannadi,Ayisha Firoz,Yin Yang,Muhammad Imran,Ferda Ofli*

Main category: cs.CV

TL;DR: DisasterVQA is a benchmark dataset aimed at improving vision-language models for crisis situations, featuring 1,395 real-world images and 4,405 expert-curated question-answer pairs. Challenges include contextual reasoning and object counting in disaster contexts.


<details>
  <summary>Details</summary>
Motivation: To assess and enhance the capability of vision-language models in handling complex reasoning and operational decision-making during disasters.

Method: Introduced DisasterVQA dataset, grounded in humanitarian frameworks, and benchmarked seven vision-language models.

Result: Models show high accuracy in binary questions but struggle with fine-grained reasoning and context-sensitive tasks, especially for less-represented disaster types.

Conclusion: DisasterVQA highlights areas for improvement in vision-language models for disaster response, providing a critical resource for future development.

Abstract: Social media imagery provides a low-latency source of situational information during natural and human-induced disasters, enabling rapid damage assessment and response. While Visual Question Answering (VQA) has shown strong performance in general-purpose domains, its suitability for the complex and safety-critical reasoning required in disaster response remains unclear. We introduce DisasterVQA, a benchmark dataset designed for perception and reasoning in crisis contexts. DisasterVQA consists of 1,395 real-world images and 4,405 expert-curated question-answer pairs spanning diverse events such as floods, wildfires, and earthquakes. Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, the dataset includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making tasks. We benchmark seven state-of-the-art vision-language models and find performance variability across question types, disaster categories, regions, and humanitarian tasks. Although models achieve high accuracy on binary questions, they struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios. DisasterVQA provides a challenging and practical benchmark to guide the development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available at https://zenodo.org/records/18267770.

</details>


### [518] [Probabilistic Deep Discriminant Analysis for Wind Blade Segmentation](https://arxiv.org/abs/2601.13852)
*Raül Pérez-Gonzalo,Andreas Espersen,Antonio Agudo*

Main category: cs.CV

TL;DR: The paper introduces Deep Discriminant Analysis (DDA) and Probabilistic DDA (PDDA) to improve image segmentation by optimizing class separation using deep networks, showcased in wind blade segmentation.


<details>
  <summary>Details</summary>
Motivation: Traditional linear discriminant analysis struggles with non-linearly separable data, particularly in applications like image segmentation, necessitating new methods to improve class separability.

Method: The authors propose DDA by optimizing the Fisher criterion using deep networks, introducing novel techniques to ensure training stability. Additionally, they augment DDA with probability loss, creating PDDA to minimize class overlap and reduce within-class variance.

Result: PDDA demonstrates superior performance in wind blade image segmentation, achieving better class distinction, prediction confidence, and segmentation consistency.

Conclusion: PDDA's success in wind blade segmentation underlines its potential for addressing complex image analysis problems and highlights its innovation in enhancing discriminant analysis methods.

Abstract: Linear discriminant analysis improves class separability but struggles with non-linearly separable data. To overcome this, we introduce Deep Discriminant Analysis (DDA), which directly optimizes the Fisher criterion utilizing deep networks. To ensure stable training and avoid computational instabilities, we incorporate signed between-class variance, bound outputs with a sigmoid function, and convert multiplicative relationships into additive ones. We present two stable DDA loss functions and augment them with a probability loss, resulting in Probabilistic DDA (PDDA). PDDA effectively minimizes class overlap in output distributions, producing highly confident predictions with reduced within-class variance. When applied to wind blade segmentation, PDDA showcases notable advances in performance and consistency, critical for wind energy maintenance. To our knowledge, this is the first application of DDA to image segmentation.

</details>


### [519] [OCCAM: Class-Agnostic, Training-Free, Prior-Free and Multi-Class Object Counting](https://arxiv.org/abs/2601.13871)
*Michail Spanakis,Iason Oikonomidis,Antonis Argyros*

Main category: cs.CV

TL;DR: This paper introduces OCCAM, a training-free, class-agnostic object counting method capable of handling multi-class counting without supplementary information.


<details>
  <summary>Details</summary>
Motivation: Current class-agnostic object counting relies on extensive training and additional data like exemplars or prompts, but there is a lack of solutions for effectively addressing multi-class counting in a training-free manner.

Method: A new method called OCCAM is introduced, utilizing the Segment Anything Model 2 (SAM2) and a modified FINCH clustering technique, to perform multi-class object counting without training or extra data.

Result: OCCAM achieves competitive results on benchmark datasets FSC-147 and CARPK, introduces a synthetic multi-class dataset, and evaluates performance with the newly proposed F1 score metric.

Conclusion: OCCAM demonstrates the feasibility of unsupervised counting for multi-class scenarios, bridging a significant gap in class-agnostic object counting research.

Abstract: Class-Agnostic object Counting (CAC) involves counting instances of objects from arbitrary classes within an image. Due to its practical importance, CAC has received increasing attention in recent years. Most existing methods assume a single object class per image, rely on extensive training of large deep learning models and address the problem by incorporating additional information, such as visual exemplars or text prompts. In this paper, we present OCCAM, the first training-free approach to CAC that operates without the need of any supplementary information. Moreover, our approach addresses the multi-class variant of the problem, as it is capable of counting the object instances in each and every class among arbitrary object classes within an image. We leverage Segment Anything Model 2 (SAM2), a foundation model, and a custom threshold-based variant of the First Integer Neighbor Clustering Hierarchy (FINCH) algorithm to achieve competitive performance on widely used benchmark datasets, FSC-147 and CARPK. We propose a synthetic multi-class dataset and F1 score as a more suitable evaluation metric. The code for our method and the proposed synthetic dataset will be made publicly available at https://mikespanak.github.io/OCCAM_counter.

</details>


### [520] [Revisiting Multi-Task Visual Representation Learning](https://arxiv.org/abs/2601.13886)
*Shangzhe Di,Zhonghua Zhai,Weidi Xie*

Main category: cs.CV

TL;DR: The paper introduces MTV, a multi-task framework combining vision-language contrast, self-supervision, and dense spatial objectives to enhance visual representation. It uses pseudo-labels from expert models and achieves both fine-grained spatial and high-level semantic understanding.


<details>
  <summary>Details</summary>
Motivation: Current visual representation models are either good at global semantic alignment or local spatial precision, but not both. The paper aims to integrate these complementary approaches for improved generalization.

Method: MTV jointly trains a shared model backbone using a blend of vision-language contrastive, self-supervised, and dense spatial objectives. It generates pseudo-labels using expert models to avoid extensive manual annotations.

Result: MTV achieves enhanced fine-grained spatial reasoning alongside strong global semantic understanding, providing insights into the interactions between multi-task objectives.

Conclusion: Multi-task learning combined with pseudo-label-based dense supervision is a promising and scalable approach for building general-purpose visual encoders.

Abstract: Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity "expert" models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves "best-of-both-worlds" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.

</details>


### [521] [OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3](https://arxiv.org/abs/2601.13895)
*Xu Zhang,Danyang Li,Yingjie Xia,Xiaohang Dong,Hualong Yu,Jianye Wang,Qicheng Li*

Main category: cs.CV

TL;DR: The paper introduces OmniOVCD, a standalone framework for Open-Vocabulary Change Detection (OVCD). OmniOVCD uses the Segment Anything Model 3 (SAM 3) components to achieve state-of-the-art results in accurately detecting changes in remote sensing benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing OVCD methods that rely on predefined categories and require multiple models, which can lead to feature mismatches and instability. The integration of segmentation and identification capabilities in SAM 3 offers a new approach to simplify and improve OVCD tasks.

Method: The authors propose OmniOVCD with a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID utilizes SAM 3's outputs (semantic, instance, and presence) to construct land-cover masks and decomposes them into instance-level masks for change detection.

Result: On four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND), OmniOVCD achieves state-of-the-art performance with class-average IoU scores of 67.2, 66.5, 24.5, and 27.1, outperforming all previous methods.

Conclusion: OmniOVCD demonstrates high accuracy and instance-level consistency in OVCD tasks using a single model framework (SAM 3), eliminating the need for multiple models and showing its applicability to remote sensing benchmarks.

Abstract: Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.

</details>


### [522] [Towards Visually Explaining Statistical Tests with Applications in Biomedical Imaging](https://arxiv.org/abs/2601.13899)
*Masoumeh Javanbakhat,Piotr Komorowski,Dilyara Bareeva,Wei-Chang Lai,Wojciech Samek,Christoph Lippert*

Main category: cs.CV

TL;DR: This paper develops an explainable deep statistical testing framework for interpretable distributional difference detection in biomedical imaging.


<details>
  <summary>Details</summary>
Motivation: Existing deep neural two-sample tests lack interpretability, limiting their use in biomedical analysis without clear explanation of sample- or feature-level contributions.

Method: The authors propose a framework that integrates deep two-sample tests with sample-level and feature-level explanations for statistically significant group differences.

Result: The approach identifies meaningful image regions and individual samples linked to variations, particularly in biomedical imaging for disease-related analysis.

Conclusion: The work combines statistical inference with explainable AI to advance interpretable, label-free population analysis in medical imaging.

Abstract: Deep neural two-sample tests have recently shown strong power for detecting distributional differences between groups, yet their black-box nature limits interpretability and practical adoption in biomedical analysis. Moreover, most existing post-hoc explainability methods rely on class labels, making them unsuitable for label-free statistical testing settings. We propose an explainable deep statistical testing framework that augments deep two-sample tests with sample-level and feature-level explanations, revealing which individual samples and which input features drive statistically significant group differences. Our method highlights which image regions and which individual samples contribute most to the detected group difference, providing spatial and instance-wise insight into the test's decision. Applied to biomedical imaging data, the proposed framework identifies influential samples and highlights anatomically meaningful regions associated with disease-related variation. This work bridges statistical inference and explainable AI, enabling interpretable, label-free population analysis in medical imaging.

</details>


### [523] [On the Role of Rotation Equivariance in Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2601.13913)
*Pavlo Melnyk,Cuong Le,Urs Waldmann,Per-Erik Forssén,Bastian Wandt*

Main category: cs.CV

TL;DR: The paper focuses on enhancing 3D human pose estimation from 2D single-view images by leveraging rotation-equivariance properties and proposing a more straightforward learning process.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenges in 3D human pose estimation, particularly the failure of common lifting models to handle in-plane rotations effectively. The authors aim to introduce a geometrically intelligible approach to overcome these limitations.

Method: The authors propose leveraging rotation-equivariance without explicitly constraining the parameter space. They argue this leads to easier learning and employ data augmentation to train the model efficiently.

Result: Experiments on common HPE benchmarks demonstrate improved performance on rotated human poses compared to state-of-the-art equivariant-by-design methods.

Conclusion: The study shows that 2D rotation-equivariance, learned through augmentation, improves 3D human pose estimation and outperforms existing methods employing explicit design constraints.

Abstract: Estimating 3D from 2D is one of the central tasks in computer vision. In this work, we consider the monocular setting, i.e. single-view input, for 3D human pose estimation (HPE). Here, the task is to predict a 3D point set of human skeletal joints from a single 2D input image. While by definition this is an ill-posed problem, recent work has presented methods that solve it with up to several-centimetre error. Typically, these methods employ a two-step approach, where the first step is to detect the 2D skeletal joints in the input image, followed by the step of 2D-to-3D lifting. We find that common lifting models fail when encountering a rotated input. We argue that learning a single human pose along with its in-plane rotations is considerably easier and more geometrically grounded than directly learning a point-to-point mapping. Furthermore, our intuition is that endowing the model with the notion of rotation equivariance without explicitly constraining its parameter space should lead to a more straightforward learning process than one with equivariance by design. Utilising the common HPE benchmarks, we confirm that the 2D rotation equivariance per se improves the model performance on human poses akin to rotations in the image plane, and can be efficiently and straightforwardly learned by augmentation, outperforming state-of-the-art equivariant-by-design methods.

</details>


### [524] [TrackletGPT: A Language-like GPT Framework for White Matter Tract Segmentation](https://arxiv.org/abs/2601.13935)
*Anoushkrit Goel,Simroop Singh,Ankita Joshi,Ranjeet Ranjan Jha,Chirag Ahuja,Aditya Nigam,Arnav Bhavsar*

Main category: cs.CV

TL;DR: The paper presents TrackletGPT, a GPT framework for white matter tract segmentation that improves across various datasets and metrics.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in white matter tract segmentation, which is essential for understanding brain connectivity and neurological disorders.

Method: Introduces TrackletGPT, a framework using tracklets (granular sub-streamline segments) with GPT models to encode sequential information for segmentation tasks.

Result: TrackletGPT showed superior performance on metrics like DICE, Overlap, and Overreach across TractoInferno and HCP datasets, even in cross-dataset scenarios.

Conclusion: TrackletGPT demonstrates robustness and effectiveness in generalizing white matter tract segmentation, outperforming current state-of-the-art methods.

Abstract: White Matter Tract Segmentation is imperative for studying brain structural connectivity, neurological disorders and neurosurgery. This task remains complex, as tracts differ among themselves, across subjects and conditions, yet have similar 3D structure across hemispheres and subjects. To address these challenges, we propose TrackletGPT, a language-like GPT framework which reintroduces sequential information in tokens using tracklets. TrackletGPT generalises seamlessly across datasets, is fully automatic, and encodes granular sub-streamline segments, Tracklets, scaling and refining GPT models in Tractography Segmentation. Based on our experiments, TrackletGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores on TractoInferno and HCP datasets, even on inter-dataset experiments.

</details>


### [525] [Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning](https://arxiv.org/abs/2601.13942)
*Hongbo Bai,Yujin Zhou,Yile Wu,Chi-Min Chan,Pengcheng Wen,Kunhao Pan,Sirui Han,Yike Guo*

Main category: cs.CV

TL;DR: The paper introduces a novel framework, Glance-or-Gaze, which improves knowledge-augmented visual understanding by actively planning visual search and reducing noise.


<details>
  <summary>Details</summary>
Motivation: Current multimodal models struggle with complex visual queries due to static knowledge and inefficient retrieval, requiring a system for active visual reasoning.

Method: The authors propose GoG, which employs a Selective Gaze mechanism for active filtering and a dual-stage training strategy combining supervised learning and reinforcement learning.

Result: GoG achieves state-of-the-art performance on six benchmarks and essential ablation studies confirm key components' effectiveness.

Conclusion: GoG effectively addresses limitations of knowledge-intensive visual queries, presenting a promising direction for visual intelligence.

Abstract: Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.

</details>


### [526] [VTONGuard: Automatic Detection and Authentication of AI-Generated Virtual Try-On Content](https://arxiv.org/abs/2601.13951)
*Shengyi Wu,Yan Hong,Shengyao Chen,Zheng Wang,Xianbing Sun,Jiahui Zhan,Jun Lan,Jianfu Zhang*

Main category: cs.CV

TL;DR: The paper introduces VTONGuard, a benchmark dataset for distinguishing real and AI-generated virtual try-on (VTON) images, aiming to improve detection methods and encourage safe use of VTON technologies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address authenticity and responsible use concerns associated with realistic AI-generated VTON content in e-commerce and digital entertainment.

Method: The authors create VTONGuard, a dataset with 775,000 real and synthetic images covering diverse conditions, and systematically evaluate several detection frameworks with unified protocols. They also design a multi-task framework with auxiliary segmentation to enhance detection.

Result: The study highlights strengths and weaknesses of detection methods and achieves improved performance using a multi-task framework with segmentation.

Conclusion: The dataset and proposed methodology provide a foundation for fair evaluations and advancements in detection of AI-generated VTON content, promoting responsible use of the technology.

Abstract: With the rapid advancement of generative AI, virtual try-on (VTON) systems are becoming increasingly common in e-commerce and digital entertainment. However, the growing realism of AI-generated try-on content raises pressing concerns about authenticity and responsible use. To address this, we present VTONGuard, a large-scale benchmark dataset containing over 775,000 real and synthetic try-on images. The dataset covers diverse real-world conditions, including variations in pose, background, and garment styles, and provides both authentic and manipulated examples. Based on this benchmark, we conduct a systematic evaluation of multiple detection paradigms under unified training and testing protocols. Our results reveal each method's strengths and weaknesses and highlight the persistent challenge of cross-paradigm generalization. To further advance detection, we design a multi-task framework that integrates auxiliary segmentation to enhance boundary-aware feature learning, achieving the best overall performance on VTONGuard. We expect this benchmark to enable fair comparisons, facilitate the development of more robust detection models, and promote the safe and responsible deployment of VTON technologies in practice.

</details>


### [527] [DExTeR: Weakly Semi-Supervised Object Detection with Class and Instance Experts for Medical Imaging](https://arxiv.org/abs/2601.13954)
*Adrien Meyer,Didier Mutter,Nicolas Padoy*

Main category: cs.CV

TL;DR: DExTeR (DETR with Experts) is introduced as a transformer-based Point-to-Box regressor designed for more accurate anatomical landmark detection in medical imaging with minimal annotations.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of costly bounding box annotations for anatomical landmark detection in medical imaging, aiming to reduce annotation effort while preserving accuracy.

Method: The DExTeR method uses single-point annotations as object queries, employing class-guided deformable attention and CLICK-MoE to refine detection and improve discrimination in complex medical structures. A multi-point training strategy further enhances robustness.

Result: DExTeR showed state-of-the-art performance across various datasets in medical imaging, reducing annotation cost while maintaining detection accuracy.

Conclusion: The paper concludes that DExTeR can significantly reduce annotation efforts in medical imaging without compromising the quality of anatomical landmark detection.

Abstract: Detecting anatomical landmarks in medical imaging is essential for diagnosis and intervention guidance. However, object detection models rely on costly bounding box annotations, limiting scalability. Weakly Semi-Supervised Object Detection (WSSOD) with point annotations proposes annotating each instance with a single point, minimizing annotation time while preserving localization signals. A Point-to-Box teacher model, trained on a small box-labeled subset, converts these point annotations into pseudo-box labels to train a student detector. Yet, medical imagery presents unique challenges, including overlapping anatomy, variable object sizes, and elusive structures, which hinder accurate bounding box inference. To overcome these challenges, we introduce DExTeR (DETR with Experts), a transformer-based Point-to-Box regressor tailored for medical imaging. Built upon Point-DETR, DExTeR encodes single-point annotations as object queries, refining feature extraction with the proposed class-guided deformable attention, which guides attention sampling using point coordinates and class labels to capture class-specific characteristics. To improve discrimination in complex structures, it introduces CLICK-MoE (CLass, Instance, and Common Knowledge Mixture of Experts), decoupling class and instance representations to reduce confusion among adjacent or overlapping instances. Finally, we implement a multi-point training strategy which promotes prediction consistency across different point placements, improving robustness to annotation variability. DExTeR achieves state-of-the-art performance across three datasets spanning different medical domains (endoscopy, chest X-rays, and endoscopic ultrasound) highlighting its potential to reduce annotation costs while maintaining high detection accuracy.

</details>


### [528] [STEC: A Reference-Free Spatio-Temporal Entropy Coverage Metric for Evaluating Sampled Video Frames](https://arxiv.org/abs/2601.13974)
*Shih-Yao Lin*

Main category: cs.CV

TL;DR: The paper introduces Spatio-Temporal Entropy Coverage (STEC), a non-reference metric for evaluating video frame sampling quality in video understanding.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of evaluating sampled video frames' informativeness and representativeness, which existing metrics fail to capture adequately.

Method: STEC combines spatial entropy (measuring structural complexity) with temporal coverage and redundancy analysis, providing a unified measure of sampling quality.

Result: Experiments on the MSR-VTT test-1k benchmark show STEC effectively differentiates sampling strategies and reveals important robustness patterns.

Conclusion: STEC is a valuable diagnostic tool for analyzing frame sampling behavior under constrained budgets, though it does not predict task-specific outcomes.

Abstract: Frame sampling is a fundamental component in video understanding and video--language model pipelines, yet evaluating the quality of sampled frames remains challenging. Existing evaluation metrics primarily focus on perceptual quality or reconstruction fidelity, and are not designed to assess whether a set of sampled frames adequately captures informative and representative video content.
  We propose Spatio-Temporal Entropy Coverage (STEC), a simple and non-reference metric for evaluating the effectiveness of video frame sampling. STEC builds upon Spatio-Temporal Frame Entropy (STFE), which measures per-frame spatial information via entropy-based structural complexity, and evaluates sampled frames based on their temporal coverage and redundancy. By jointly modeling spatial information strength, temporal dispersion, and non-redundancy, STEC provides a principled and lightweight measure of sampling quality.
  Experiments on the MSR-VTT test-1k benchmark demonstrate that STEC clearly differentiates common sampling strategies, including random, uniform, and content-aware methods. We further show that STEC reveals robustness patterns across individual videos that are not captured by average performance alone, highlighting its practical value as a general-purpose evaluation tool for efficient video understanding.
  We emphasize that STEC is not designed to predict downstream task accuracy, but to provide a task-agnostic diagnostic signal for analyzing frame sampling behavior under constrained budgets.

</details>


### [529] [Harmonizing the Deep: A Unified Information Pipeline for Robust Marine Biodiversity Assessment Across Heterogeneous Domains](https://arxiv.org/abs/2601.13975)
*Marco Piccolo,Qiwei Han,Astrid van Toor,Joachim Vanneste*

Main category: cs.CV

TL;DR: The paper addresses challenges in marine biodiversity monitoring by developing a standardized detection system to improve reliability across diverse underwater environments, focusing on structural factors rather than image quality.


<details>
  <summary>Details</summary>
Motivation: To overcome existing solutions' inability to scale effectively across underwater ecosystems and mitigate performance issues when deployed in new sites, especially for conservation and invasive species management.

Method: The authors created a Unified Information Pipeline to standardize datasets, evaluated detectors under cross-domain protocols, analyzed structural factors influencing performance, and optimized runtime for low-cost edge hardware.

Result: The study found structural factors like scene composition and object density were key to cross-domain performance issues, validated practical feasibility on edge hardware, and demonstrated improved reliability for ecosystem monitoring.

Conclusion: The findings shift focus toward accounting for structural factors rather than solely relying on image enhancement, democratizing reliable marine ecosystem assessment tools.

Abstract: Marine biodiversity monitoring requires scalability and reliability across complex underwater environments to support conservation and invasive-species management. Yet existing detection solutions often exhibit a pronounced deployment gap, with performance degrading sharply when transferred to new sites. This work establishes the foundational detection layer for a multi-year invasive species monitoring initiative targeting Arctic and Atlantic marine ecosystems. We address this challenge by developing a Unified Information Pipeline that standardises heterogeneous datasets into a comparable information flow and evaluates a fixed, deployment-relevant detector under controlled cross-domain protocols. Across multiple domains, we find that structural factors, such as scene composition, object density, and contextual redundancy, explain cross-domain performance loss more strongly than visual degradation such as turbidity, with sparse scenes inducing a characteristic "Context Collapse" failure mode. We further validate operational feasibility by benchmarking inference on low-cost edge hardware, showing that runtime optimisation enables practical sampling rates for remote monitoring. The results shift emphasis from image enhancement toward structure-aware reliability, providing a democratised tool for consistent marine ecosystem assessment.

</details>


### [530] [Equivariant Learning for Unsupervised Image Dehazing](https://arxiv.org/abs/2601.13986)
*Zhang Wen,Jiangwei Xie,Dongdong Chen*

Main category: cs.CV

TL;DR: The paper introduces Equivariant Image Dehazing (EID), an unsupervised learning framework for dehazing images using symmetry of image signals without requiring extensive ground truth.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the reliance on expensive or impractical crafted priors and haze-free ground truths in the context of scientific imaging, which is common in current image dehazing methods.

Method: The proposed method, EID, exploits image signal symmetry to enforce haze consistency and systematic equivariance. An adversarial learning strategy is also introduced to model unknown haze physics.

Result: EID significantly outperforms state-of-the-art methods on two scientific benchmarks (cell microscopy and medical endoscopy) as well as on natural image dehazing.

Conclusion: By unifying equivariant learning and haze physics modeling, EID offers a versatile and effective approach for dehazing, especially in scientific imaging. The code and datasets will be released for broader use.

Abstract: Image Dehazing (ID) aims to produce a clear image from an observation contaminated by haze. Current ID methods typically rely on carefully crafted priors or extensive haze-free ground truth, both of which are expensive or impractical to acquire, particularly in the context of scientific imaging. We propose a new unsupervised learning framework called Equivariant Image Dehazing (EID) that exploits the symmetry of image signals to restore clarity to hazy observations. By enforcing haze consistency and systematic equivariance, EID can recover clear patterns directly from raw, hazy images. Additionally, we propose an adversarial learning strategy to model unknown haze physics and facilitate EID learning. Experiments on two scientific image dehazing benchmarks (including cell microscopy and medical endoscopy) and on natural image dehazing have demonstrated that EID significantly outperforms state-of-the-art approaches. By unifying equivariant learning with modelling haze physics, we hope that EID will enable more versatile and effective haze removal in scientific imaging. Code and datasets will be published.

</details>


### [531] [Likelihood-Separable Diffusion Inference for Multi-Image MRI Super-Resolution](https://arxiv.org/abs/2601.14030)
*Samuel W. Remedios,Zhangxing Bian,Shuwen Wei,Aaron Carass,Jerry L. Prince,Blake E. Dewey*

Main category: cs.CV

TL;DR: The paper proposes methods for multi-image super-resolution in MRI using diffusion models, showing improved resolution and state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of single-image inverse problem solvers in MRI and enable reconstruction of high-quality isotropic anatomy from routine 2D acquisitions.

Method: The authors generalize diffusion-based inverse problem solvers (e.g., DPS) for multi-image super-resolution by enabling separable gradient decomposition, avoiding joint operator construction or model retraining.

Result: The proposed methods show substantial gains over single-image approaches, achieving state-of-the-art super-resolution in anisotropic MRI volumes across various degradation scales.

Conclusion: Their approach significantly improves MRI volume resolution and enables near-isotropic anatomical reconstruction, addressing a key challenge in routine 2D multi-slice MRI.

Abstract: Diffusion models are the current state-of-the-art for solving inverse problems in imaging. Their impressive generative capability allows them to approximate sampling from a prior distribution, which alongside a known likelihood function permits posterior sampling without retraining the model. While recent methods have made strides in advancing the accuracy of posterior sampling, the majority focuses on single-image inverse problems. However, for modalities such as magnetic resonance imaging (MRI), it is common to acquire multiple complementary measurements, each low-resolution along a different axis. In this work, we generalize common diffusion-based inverse single-image problem solvers for multi-image super-resolution (MISR) MRI. We show that the DPS likelihood correction allows an exactly-separable gradient decomposition across independently acquired measurements, enabling MISR without constructing a joint operator, modifying the diffusion model, or increasing network function evaluations. We derive MISR versions of DPS, DMAP, DPPS, and diffusion-based PnP/ADMM, and demonstrate substantial gains over SISR across $4\times/8\times/16\times$ anisotropic degradations. Our results achieve state-of-the-art super-resolution of anisotropic MRI volumes and, critically, enable reconstruction of near-isotropic anatomy from routine 2D multi-slice acquisitions, which are otherwise highly degraded in orthogonal views.

</details>


### [532] [Human detectors are surprisingly powerful reward models](https://arxiv.org/abs/2601.14037)
*Kumar Ashutosh,XuDong Wang,Xi Yin,Kristen Grauman,Adam Polyak,Ishan Misra,Rohit Girdhar*

Main category: cs.CV

TL;DR: The paper introduces HuDA, a reward model enhancing human motion video generation by leveraging human detection and temporal prompt alignment, achieving better results than state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle with synthesizing complex human motions, producing issues like distortions, missing limbs, or unrealistic movements.

Method: The authors propose HuDA, which integrates human detection confidence and a temporal alignment score to quantify motion realism, combined with Group Reward Policy Optimization (GRPO) for post-training.

Result: HuDA significantly improves video generation quality, achieving a 73% win-rate over the previous state-of-the-art model Wan 2.1, and also enhances other areas such as animal motions and human-object interactions.

Conclusion: HuDA is a simple yet effective approach to addressing challenges in video generation models, showing superior performance in generating realistic motions without additional training.

Abstract: Video generation models have recently achieved impressive visual fidelity and temporal coherence. Yet, they continue to struggle with complex, non-rigid motions, especially when synthesizing humans performing dynamic actions such as sports, dance, etc. Generated videos often exhibit missing or extra limbs, distorted poses, or physically implausible actions. In this work, we propose a remarkably simple reward model, HuDA, to quantify and improve the human motion in generated videos. HuDA integrates human detection confidence for appearance quality, and a temporal prompt alignment score to capture motion realism. We show this simple reward function that leverages off-the-shelf models without any additional training, outperforms specialized models finetuned with manually annotated data. Using HuDA for Group Reward Policy Optimization (GRPO) post-training of video models, we significantly enhance video generation, especially when generating complex human motions, outperforming state-of-the-art models like Wan 2.1, with win-rate of 73%. Finally, we demonstrate that HuDA improves generation quality beyond just humans, for instance, significantly improving generation of animal videos and human-object interactions.

</details>


### [533] [Correcting and Quantifying Systematic Errors in 3D Box Annotations for Autonomous Driving](https://arxiv.org/abs/2601.14038)
*Alexandre Justo Miro,Ludvig af Klinteberg,Bogdan Timus,Aron Asefaw,Ajinkya Khoche,Thomas Gustafsson,Sina Sharif Mansouri,Masoud Daneshtalab*

Main category: cs.CV

TL;DR: The paper addresses annotation errors in 3D box labels for autonomous vehicle datasets, presents a novel correction method, and demonstrates significant improvements in annotation quality.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicle performance evaluation relies on accurate ground truth annotations, but errors can occur in dynamic scenarios, leading to systematic bias in 3D box annotations.

Method: The authors introduced a novel offline estimation method to correct 3D annotations, ensuring consistency with sensor readings and physically plausible trajectories.

Result: The method improved annotation quality by over 17% across different datasets, revealing significant annotation misplacements (up to 2.5m) in dynamic objects.

Conclusion: Accurate annotations are crucial as annotation errors have a substantial impact on benchmarking, potentially overshadowing advances in state-of-the-art methods.

Abstract: Accurate ground truth annotations are critical to supervised learning and evaluating the performance of autonomous vehicle systems. These vehicles are typically equipped with active sensors, such as LiDAR, which scan the environment in predefined patterns. 3D box annotation based on data from such sensors is challenging in dynamic scenarios, where objects are observed at different timestamps, hence different positions. Without proper handling of this phenomenon, systematic errors are prone to being introduced in the box annotations. Our work is the first to discover such annotation errors in widely used, publicly available datasets. Through our novel offline estimation method, we correct the annotations so that they follow physically feasible trajectories and achieve spatial and temporal consistency with the sensor data. For the first time, we define metrics for this problem; and we evaluate our method on the Argoverse 2, MAN TruckScenes, and our proprietary datasets. Our approach increases the quality of box annotations by more than 17% in these datasets. Furthermore, we quantify the annotation errors in them and find that the original annotations are misplaced by up to 2.5 m, with highly dynamic objects being the most affected. Finally, we test the impact of the errors in benchmarking and find that the impact is larger than the improvements that state-of-the-art methods typically achieve with respect to the previous state-of-the-art methods; showing that accurate annotations are essential for correct interpretation of performance. Our code is available at https://github.com/alexandre-justo-miro/annotation-correction-3D-boxes.

</details>


### [534] [Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation](https://arxiv.org/abs/2601.14039)
*Wesam Moustafa,Hossam Elsafty,Helen Schneider,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.CV

TL;DR: The paper addresses label noise in medical image segmentation by introducing an abstention framework that enhances noise-robustness for various loss functions. It shows superior results in experiments under high noise levels.


<details>
  <summary>Details</summary>
Motivation: To improve generalization performance of models in medical image segmentation, which often suffer from label noise caused by manual annotation difficulties.

Method: The paper introduces an abstention framework featuring an informed regularization term and a power-law auto-tuning algorithm. It integrates the framework with three loss functions to create noise-robust variants (GAC, SAC, and ADS).

Result: Experiments on CaDIS and DSAD datasets show the proposed methods achieve significant improvements over their non-abstaining baselines, especially in high noise conditions.

Conclusion: Selective abstention from corrupted samples is an effective and generalizable strategy, enhancing the reliability of medical image segmentation models.

Abstract: Label noise is a critical problem in medical image segmentation, often arising from the inherent difficulty of manual annotation. Models trained on noisy data are prone to overfitting, which degrades their generalization performance. While a number of methods and strategies have been proposed to mitigate noisy labels in the segmentation domain, this area remains largely under-explored. The abstention mechanism has proven effective in classification tasks by enhancing the capabilities of Cross Entropy, yet its potential in segmentation remains unverified. In this paper, we address this gap by introducing a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions. Our framework improves upon prior work with two key components: an informed regularization term to guide abstention behaviour, and a more flexible power-law-based auto-tuning algorithm for the abstention penalty. We demonstrate the framework's versatility by systematically integrating it with three distinct loss functions to create three novel, noise-robust variants: GAC, SAC, and ADS. Experiments on the CaDIS and DSAD medical datasets show our methods consistently and significantly outperform their non-abstaining baselines, especially under high noise levels. This work establishes that enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. Our code is publicly available at https://github.com/wemous/abstention-for-segmentation.

</details>


### [535] [Federated Balanced Learning](https://arxiv.org/abs/2601.14042)
*Jiaze Li,Haoran Xu,Wanyi Wu,Changwei Wang,Shuaiguang Li,Jianzhong Ju,Zhenbo Luo,Jian Luan,Youyang Qu,Longxiang Gao,Xudong Yang,Lumin Xing*

Main category: cs.CV

TL;DR: The paper introduces Federated Balanced Learning (FBL) to mitigate client drift in non-i.i.d. federated learning scenarios by ensuring sample balance using client-side data generation, outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning models underperform in non-i.i.d. data settings due to client drift caused by imbalanced client data, which previous methods fail to address adequately.

Method: The proposed Federated Balanced Learning (FBL) uses edge-side generation models for knowledge filling and sampling to balance client data, alongside strategies for knowledge alignment and regularization. The framework can also scale to varied real-world scenarios.

Result: Extensive experiments demonstrate that FBL surpasses state-of-the-art methods across diverse, real-world federated learning scenarios.

Conclusion: FBL effectively prevents client drift by emphasizing sample balance on the client side, enhancing the global model's performance and scalability.

Abstract: Federated learning is a paradigm of joint learning in which clients collaborate by sharing model parameters instead of data. However, in the non-iid setting, the global model experiences client drift, which can seriously affect the final performance of the model. Previous methods tend to correct the global model that has already deviated based on the loss function or gradient, overlooking the impact of the client samples. In this paper, we rethink the role of the client side and propose Federated Balanced Learning, i.e., FBL, to prevent this issue from the beginning through sample balance on the client side. Technically, FBL allows unbalanced data on the client side to achieve sample balance through knowledge filling and knowledge sampling using edge-side generation models, under the limitation of a fixed number of data samples on clients. Furthermore, we design a Knowledge Alignment Strategy to bridge the gap between synthetic and real data, and a Knowledge Drop Strategy to regularize our method. Meanwhile, we scale our method to real and complex scenarios, allowing different clients to adopt various methods, and extend our framework to further improve performance. Numerous experiments show that our method outperforms state-of-the-art baselines. The code is released upon acceptance.

</details>


### [536] [DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning](https://arxiv.org/abs/2601.14084)
*Abdurrahim Yilmaz,Ozan Erdem,Ece Gokyayla,Ayda Acar,Burc Bugra Dagtas,Dilara Ilhan Erdil,Gulsum Gencoglan,Burak Temelkuran*

Main category: cs.CV

TL;DR: The paper introduces DermaBench, a dermatology VQA benchmark for evaluating the performance of vision-language models in the field.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current vision-language model evaluations in dermatology, which are restricted to image-level tasks and fail to assess multimodal capabilities like reasoning, language grounding, and clinical description generation.

Method: The authors have built DermaBench, a dermatologist-annotated VQA dataset based on the Diverse Dermatology Images (DDI) dataset, which includes 656 clinical images and approximately 14,474 annotations using a hierarchical structure across 22 main question types.

Result: DermaBench provides a comprehensive benchmark with structured annotations and question formats that comprehensively evaluate VLM capabilities in dermatology.

Conclusion: The dataset is a significant contribution to the medical AI community, enabling the evaluation of vision-language models on multimodal tasks and enhancing their usability in dermatology.

Abstract: Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.

</details>


### [537] [Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology](https://arxiv.org/abs/2601.14044)
*Kaiyu Wu,Pucheng Han,Hualong Zhang,Naigeng Wu,Keze Wang*

Main category: cs.CV

TL;DR: The paper addresses reasoning challenges in meteorology-specific Vision Language Models (VLMs) and proposes effective solutions for logical consistency and performance improvement, including a new model and benchmark.


<details>
  <summary>Details</summary>
Motivation: The paper identifies domain and reasoning challenges in applying Vision Language Models (VLMs) to meteorology, highlighting issues like Self-Contradictory Reasoning (Self-Contra) that undermine model reliability in high-stakes scenarios.

Method: The researchers propose WeatherQA, a benchmark for meteorology-focused multimodal reasoning, and LoCo-RFT (Logically Consistent Reinforcement Fine-Tuning) to ensure logical consistency in reasoning. They develop Weather-R1 as an enhanced meteorology-focused VLM.

Result: The proposed Weather-R1 model improves performance on WeatherQA by 9.8 points compared to the baseline, outperforming Supervised Fine-Tuning, standard RFT, and the Qwen2.5-VL-32B model.

Conclusion: LoCo-RFT effectively eliminates reasoning contradictions, and Weather-R1 demonstrates superior logical faithfulness and accuracy in meteorology-focused reasoning tasks, setting a new benchmark for this domain.

Abstract: While Vision Language Models (VLMs) show advancing reasoning capabilities, their application in meteorology is constrained by a domain gap and a reasoning faithfulness gap. Specifically, mainstream Reinforcement Fine-Tuning (RFT) can induce Self-Contradictory Reasoning (Self-Contra), where the model's reasoning contradicts its final answer, which is unacceptable in such a high-stakes domain. To address these challenges, we construct WeatherQA, a novel multimodal reasoning benchmark in meteorology. We also propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), which resolves Self-Contra by introducing a logical consistency reward. Furthermore, we introduce Weather-R1, the first reasoning VLM with logical faithfulness in meteorology, to the best of our knowledge. Experiments demonstrate that Weather-R1 improves performance on WeatherQA by 9.8 percentage points over the baseline, outperforming Supervised Fine-Tuning and RFT, and even surpassing the original Qwen2.5-VL-32B. These results highlight the effectiveness of our LoCo-RFT and the superiority of Weather-R1. Our benchmark and code are available at https://github.com/Marcowky/Weather-R1.

</details>


### [538] [The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning](https://arxiv.org/abs/2601.14127)
*Renmiao Chen,Yida Lu,Shiyao Cui,Xuan Ouyang,Victor Shea-Jay Huang,Shumin Zhang,Chengwei Pan,Han Qiu,Minlie Huang*

Main category: cs.CV

TL;DR: The research introduces MIR-SafetyBench, a benchmark for evaluating the safety of Multimodal Large Language Models (MLLMs) in multi-image reasoning tasks, revealing vulnerabilities in advanced models.


<details>
  <summary>Details</summary>
Motivation: To address the emerging safety risks in MLLMs as they develop advanced reasoning capabilities for complex, multi-image instructions.

Method: They introduce MIR-SafetyBench, a benchmark featuring 2,676 instances across nine multi-image relations, and evaluate 19 MLLMs on it to identify vulnerabilities and patterns in unsafe outputs.

Result: Advanced MLLMs show higher vulnerability on MIR-SafetyBench, with many 'safe' responses being superficial or evasive. Unsafe outputs tend to have lower attention entropy, indicating potential over-focus on task solving at the expense of safety.

Conclusion: Even as MLLMs improve in reasoning, safety risks increase, necessitating greater attention to balancing safety constraints alongside task-solving capabilities.

Abstract: As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.

</details>


### [539] [Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model](https://arxiv.org/abs/2601.14052)
*Haoran Xu,Yanlin Liu,Zizhao Tong,Jiaze Li,Kexue Fu,Yuyang Zhang,Longxiang Gao,Shuaiguang Li,Xingyu Li,Yanran Xu,Changwei Wang*

Main category: cs.CV

TL;DR: This paper introduces MM-OOD, a new method for enhanced out-of-distribution (OOD) detection using multimodal language models (MLLMs) with multi-round reasoning and interaction capabilities.


<details>
  <summary>Details</summary>
Motivation: Address limitations in current OOD detection methods that rely excessively on text space knowledge, neglecting image space challenges.

Method: Proposes MM-OOD pipeline, leveraging multimodal reasoning capabilities of MLLMs. It includes two tasks: near OOD detection using direct input prompts and far OOD detection using a sketch-generate-elaborate framework.

Result: Shows significant improvements in OOD detection performance on multimodal datasets like Food-101 and scalability on ImageNet-1K.

Conclusion: MM-OOD enhances OOD detection efficacy, combining MLLMs' multimodal reasoning to improve near and far OOD tasks, showcasing potential in real-world datasets.

Abstract: Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text space, neglecting the inherent challenges involved in detecting out-of-distribution samples in the image space. In this paper, we propose a novel pipeline, MM-OOD, which leverages the multimodal reasoning capabilities of MLLMs and their ability to conduct multi-round conversations for enhanced outlier detection. Our method is designed to improve performance in both near OOD and far OOD tasks. Specifically, (1) for near OOD tasks, we directly feed ID images and corresponding text prompts into MLLMs to identify potential outliers; and (2) for far OOD tasks, we introduce the sketch-generate-elaborate framework: first, we sketch outlier exposure using text prompts, then generate corresponding visual OOD samples, and finally elaborate by using multimodal prompts. Experiments demonstrate that our method achieves significant improvements on widely used multimodal datasets such as Food-101, while also validating its scalability on ImageNet-1K.

</details>


### [540] [Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI](https://arxiv.org/abs/2601.14055)
*Andrea Protani,Marc Molina Van Den Bosch,Lorenzo Giusti,Heloisa Barbosa Da Silva,Paolo Cacace,Albert Sund Aillet,Miguel Angel Gonzalez Ballester,Friedhelm Hummel,Luigi Serio*

Main category: cs.CV

TL;DR: The paper introduces SVGFormer, a graph-based and encoder-only pipeline for 3D medical imaging, which eliminates traditional decoder structures and achieves strong performance on tumor-related tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiencies of current vision backbones for 3D medical imaging, which allocate significant parameters to spatial reconstruction rather than feature learning.

Method: The method involves introducing SVGFormer, which utilizes a content-aware grouping stage to create a semantic graph of supervoxels and employs a hierarchical encoder combining a patch-level Transformer and a supervoxel-level Graph Attention Network.

Result: The framework is validated on the BraTS dataset, achieving an F1-score of 0.875 for node-level classification and a mean absolute error (MAE) of 0.028 for tumor proportion regression.

Conclusion: A graph-based, encoder-only design like SVGFormer is an accurate, flexible, and interpretable alternative for 3D medical image representation.

Abstract: Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework's flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder's ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.

</details>


### [541] [POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion](https://arxiv.org/abs/2601.14056)
*Andrea Rigo,Luca Stornaiuolo,Weijie Wang,Mauro Martino,Bruno Lepri,Nicu Sebe*

Main category: cs.CV

TL;DR: This work introduces POCI-Diff, a novel diffusion-based framework for Text-to-Image generation with consistent and interactive 3D layout control for multi-object scenes.


<details>
  <summary>Details</summary>
Motivation: Existing methods for Text-to-Image tasks suffer from spatial inconsistencies and geometry distortions during layout editing or adherence to object placements. There is a need to enforce 3D geometric constraints and maintain scene coherence effectively.

Method: POCI-Diff utilizes Blended Latent Diffusion to bind text descriptions to specific 3D bounding boxes, ensuring 3D spatial adherence. It incorporates a warping-free generative editing pipeline for object insertions, removals, and transformations, relying on regeneration rather than deformation. The IP-Adapter conditions the diffusion process on reference images, achieving consistent object appearance across edits.

Result: Experimental results show that POCI-Diff generates high-quality images with precise 3D layout consistency and visual fidelity, outperforming existing methods and eliminating geometric artifacts caused by warping processes.

Conclusion: POCI-Diff offers an effective text-to-image generation framework with unprecedented control and consistency in 3D scene synthesis and editing, marking advancements in spatial adherence and editing precision.

Abstract: We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.

</details>


### [542] [Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration](https://arxiv.org/abs/2601.14060)
*Yongcong Ye,Kai Zhang,Yanghai Zhang,Enhong Chen,Longfei Li,Jun Zhou*

Main category: cs.CV

TL;DR: This paper introduces a novel method, CVSI, for fine-grained zero-shot composed image retrieval by integrating complementary visual and semantic information for better target image retrieval.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot composed image retrieval (ZS-CIR) methods struggle to capture fine-grained changes and effectively merge visual and semantic information, limiting their performance.

Method: The proposed method, CVSI, integrates three key components: (1) Visual Information Extraction, incorporating global features and pseudo tokens for visual cues, (2) Semantic Information Extraction, leveraging caption generation with LLMs for semantic context, and (3) Complementary Information Retrieval for enhanced integration of visual and semantic data.

Result: Experiments on CIRR, CIRCO, and FashionIQ datasets show that CVSI outperforms existing state-of-the-art methods in ZS-CIR.

Conclusion: CVSI effectively bridges the gap in integrating visual and semantic information in ZS-CIR, providing superior retrieval performance and demonstrating its efficacy for composed image retrieval tasks.

Abstract: Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.

</details>


### [543] [VERIDAH: Solving Enumeration Anomaly Aware Vertebra Labeling across Imaging Sequences](https://arxiv.org/abs/2601.14066)
*Hendrik Möller,Hanna Schoen,Robert Graf,Matan Atad,Nathan Molinier,Anjany Sekuboyina,Bettina K. Budai,Fabian Bamberg,Steffen Ringhof,Christopher Schlett,Tobias Pischon,Thoralf Niendorf,Josua A. Decker,Marc-André Weber,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke*

Main category: cs.CV

TL;DR: The paper introduces VERIDAH, an algorithm for vertebra labeling to handle enumeration anomalies with high accuracy in MRI and CT images.


<details>
  <summary>Details</summary>
Motivation: Clinical identification of spinal enumeration anomalies is crucial for managing chronic back pain and surgical interventions. Current methods are inadequate, especially for automatic anomaly labeling.

Method: VERIDAH uses multiple classification heads combined with a weighted vertebra sequence prediction algorithm to accurately label vertebrae and handle enumeration anomalies.

Result: VERIDAH achieves better performance than existing models, with vertebrae correctly labeled in 98.30% (MRI) and 99.18% (CT) images, and shows high accuracy for detecting anomalies.

Conclusion: VERIDAH is a significant improvement in vertebra labeling, offering accurate anomaly identification and clinical applicability across various imaging modalities.

Abstract: The human spine commonly consists of seven cervical, twelve thoracic, and five lumbar vertebrae. However, enumeration anomalies may result in individuals having eleven or thirteen thoracic vertebrae and four or six lumbar vertebrae. Although the identification of enumeration anomalies has potential clinical implications for chronic back pain and operation planning, the thoracolumbar junction is often poorly assessed and rarely described in clinical reports. Additionally, even though multiple deep-learning-based vertebra labeling algorithms exist, there is a lack of methods to automatically label enumeration anomalies. Our work closes that gap by introducing "Vertebra Identification with Anomaly Handling" (VERIDAH), a novel vertebra labeling algorithm based on multiple classification heads combined with a weighted vertebra sequence prediction algorithm. We show that our approach surpasses existing models on T2w TSE sagittal (98.30% vs. 94.24% of subjects with all vertebrae correctly labeled, p < 0.001) and CT imaging (99.18% vs. 77.26% of subjects with all vertebrae correctly labeled, p < 0.001) and works in arbitrary field-of-view images. VERIDAH correctly labeled the presence 2 Möller et al. of thoracic enumeration anomalies in 87.80% and 96.30% of T2w and CT images, respectively, and lumbar enumeration anomalies in 94.48% and 97.22% for T2w and CT, respectively. Our code and models are available at: https://github.com/Hendrik-code/spineps.

</details>


### [544] [Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management](https://arxiv.org/abs/2601.14069)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: The paper introduces an unsupervised approach for video class incremental learning by using deep feature extraction and clustering, achieving superior performance on standard datasets.


<details>
  <summary>Details</summary>
Motivation: Unsupervised video class incremental learning is crucial as supervised methods relying on labels and task boundaries can be expensive, require human labor, or be infeasible.

Method: Deep feature extraction is used, followed by progressive deep clustering. Knowledge transfer occurs from previously learned tasks to new ones without considering labels.

Result: The proposed method shows superior performance compared to baselines on UCF101, HMDB51, and Something-to-Something V2 datasets.

Conclusion: The framework demonstrates effective unsupervised learning for video action recognition tasks, advancing class incremental learning without relying on labeled data.

Abstract: Unsupervised video class incremental learning (uVCIL) represents an important learning paradigm for learning video information without forgetting, and without considering any data labels. Prior approaches have focused on supervised class-incremental learning, relying on using the knowledge of labels and task boundaries, which is costly, requires human annotation, or is simply not a realistic option. In this paper, we propose a simple yet effective approach to address the uVCIL. We first consider a deep feature extractor network, providing a set of representative video features during each task without assuming any class or task information. We then progressively build a series of deep clusters from the extracted features. During the successive task learning, the model updated from the previous task is used as an initial state in order to transfer knowledge to the current learning task. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, by ignoring the labels from the supervised setting. Our approach significantly outperforms other baselines on all datasets.

</details>


### [545] [VENI: Variational Encoder for Natural Illumination](https://arxiv.org/abs/2601.14079)
*Paul Walker,James A. D. Gardner,Andreea Ardelean,William A. P. Smith,Bernhard Egger*

Main category: cs.CV

TL;DR: This paper introduces a rotation-equivariant variational autoencoder for modeling natural illumination environments on spheres, using novel techniques to improve latent space behavior.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address inverse rendering challenges by integrating rotation-equivariance and simplifying the modeling of environment illumination while preserving proper latent space structure.

Method: A rotation-equivariant variational autoencoder is proposed, leveraging a Vector Neuron Vision Transformer (VN-ViT) encoder and a rotation-equivariant conditional neural field decoder. A novel SO(2)-equivariant fully connected layer is introduced.

Result: The SO(2)-equivariant fully connected layer shows better performance compared to standard Vector Neurons in preserving equivariance and improving interpolation in latent space.

Conclusion: The proposed model provides smoother interpolation and a better-structured latent space compared to existing methods, advancing rotation-equivariant modeling of illumination environments.

Abstract: Inverse rendering is an ill-posed problem, but priors like illumination priors, can simplify it. Existing work either disregards the spherical and rotation-equivariant nature of illumination environments or does not provide a well-behaved latent space. We propose a rotation-equivariant variational autoencoder that models natural illumination on the sphere without relying on 2D projections. To preserve the SO(2)-equivariance of environment maps, we use a novel Vector Neuron Vision Transformer (VN-ViT) as encoder and a rotation-equivariant conditional neural field as decoder. In the encoder, we reduce the equivariance from SO(3) to SO(2) using a novel SO(2)-equivariant fully connected layer, an extension of Vector Neurons. We show that our SO(2)-equivariant fully connected layer outperforms standard Vector Neurons when used in our SO(2)-equivariant model. Compared to previous methods, our variational autoencoder enables smoother interpolation in latent space and offers a more well-behaved latent space.

</details>


### [546] [Two-Stream temporal transformer for video action classification](https://arxiv.org/abs/2601.14086)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: The paper introduces a two-stream transformer for video classification using spatio-temporal features and optical flow.


<details>
  <summary>Details</summary>
Motivation: To enhance motion representation in video understanding, leveraging the self-attention mechanism of transformers for better performance in applications like action recognition.

Method: The study presents a two-stream transformer model combining spatio-temporal information and optical flow, employing transformer encoders to identify self-attention features.

Result: The proposed model demonstrates excellent classification performance on three popular video datasets focused on human activities.

Conclusion: The study successfully shows that integrating spatio-temporal content and optical flow in transformers results in high-quality video classification outcomes.

Abstract: Motion representation plays an important role in video understanding and has many applications including action recognition, robot and autonomous guidance or others. Lately, transformer networks, through their self-attention mechanism capabilities, have proved their efficiency in many applications. In this study, we introduce a new two-stream transformer video classifier, which extracts spatio-temporal information from content and optical flow representing movement information. The proposed model identifies self-attention features across the joint optical flow and temporal frame domain and represents their relationships within the transformer encoder mechanism. The experimental results show that our proposed methodology provides excellent classification results on three well-known video datasets of human activities.

</details>


### [547] [Curriculum-Based Strategies for Efficient Cross-Domain Action Recognition](https://arxiv.org/abs/2601.14101)
*Emily Kim,Allen Wu,Jessica Hodgins*

Main category: cs.CV

TL;DR: The paper explores curriculum-based training for cross-view action recognition to enhance generalization to aerial views without using real aerial data during training.


<details>
  <summary>Details</summary>
Motivation: Current action recognition models struggle to generalize to different viewpoints, particularly aerial views, as training datasets are primarily ground-level.

Method: The study uses curriculum learning incorporating synthetic aerial-view and real ground-view data. Two strategies are analyzed: a two-stage refinement and a progressive curriculum with multiple transitions before fine-tuning.

Result: Combining synthetic and real ground-view data improves performance versus single-domain training. Curriculum strategies also boost training efficiency—reducing iterations by up to 37% for SlowFast and up to 30% for MViTv2.

Conclusion: Curriculum-based approaches aid efficient training while achieving similar performance, showing promise for generalization in cross-view action recognition tasks.

Abstract: Despite significant progress in human action recognition, generalizing to diverse viewpoints remains a challenge. Most existing datasets are captured from ground-level perspectives, and models trained on them often struggle to transfer to drastically different domains such as aerial views. This paper examines how curriculum-based training strategies can improve generalization to unseen real aerial-view data without using any real aerial data during training.
  We explore curriculum learning for cross-view action recognition using two out-of-domain sources: synthetic aerial-view data and real ground-view data. Our results on the evaluation on order of training (fine-tuning on synthetic aerial data vs. real ground data) shows that fine-tuning on real ground data but differ in how they transition from synthetic to real. The first uses a two-stage curriculum with direct fine-tuning, while the second applies a progressive curriculum that expands the dataset in multiple stages before fine-tuning. We evaluate both methods on the REMAG dataset using SlowFast (CNN-based) and MViTv2 (Transformer-based) architectures.
  Results show that combining the two out-of-domain datasets clearly outperforms training on a single domain, whether real ground-view or synthetic aerial-view. Both curriculum strategies match the top-1 accuracy of simple dataset combination while offering efficiency gains. With the two-step fine-tuning method, SlowFast achieves up to a 37% reduction in iterations and MViTv2 up to a 30% reduction compared to simple combination. The multi-step progressive approach further reduces iterations, by up to 9% for SlowFast and 30% for MViTv2, relative to the two-step method. These findings demonstrate that curriculum-based training can maintain comparable performance (top-1 accuracy within 3% range) while improving training efficiency in cross-view action recognition.

</details>


### [548] [Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing](https://arxiv.org/abs/2601.14103)
*Xiaolu Liu,Yicong Li,Qiyuan He,Jiayin Zhu,Wei Ji,Angela Yao,Jianke Zhu*

Main category: cs.CV

TL;DR: The paper introduces Interp3D, a framework for smooth and realistic transitions between 3D textured assets, addressing key limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the demand for seamless 3D morphing that retains structural and textural fidelity which is essential for animation and digital content creation.

Method: Interp3D employs generative priors and a progressive alignment approach combining semantic space interpolation, SLAT-guided structural interpolation, and texture fusion for high-grade results.

Result: Interp3D outperforms existing methods in fidelity, smooth transitions, and visual plausibility as demonstrated by evaluations on a purpose-built dataset and human studies.

Conclusion: The framework achieves superior textured 3D morphing, showcasing its potential for broader practical applications and advancing 3D generation techniques.

Abstract: Textured 3D morphing seeks to generate smooth and plausible transitions between two 3D assets, preserving both structural coherence and fine-grained appearance. This ability is crucial not only for advancing 3D generation research but also for practical applications in animation, editing, and digital content creation. Existing approaches either operate directly on geometry, limiting them to shape-only morphing while neglecting textures, or extend 2D interpolation strategies into 3D, which often causes semantic ambiguity, structural misalignment, and texture blurring. These challenges underscore the necessity to jointly preserve geometric consistency, texture alignment, and robustness throughout the transition process. To address this, we propose Interp3D, a novel training-free framework for textured 3D morphing. It harnesses generative priors and adopts a progressive alignment principle to ensure both geometric fidelity and texture coherence. Starting from semantically aligned interpolation in condition space, Interp3D enforces structural consistency via SLAT (Structured Latent)-guided structure interpolation, and finally transfers appearance details through fine-grained texture fusion. For comprehensive evaluations, we construct a dedicated dataset, Interp3DData, with graded difficulty levels and assess generation results from fidelity, transition smoothness, and plausibility. Both quantitative metrics and human studies demonstrate the significant advantages of our proposed approach over previous methods. Source code is available at https://github.com/xiaolul2/Interp3D.

</details>


### [549] [PMCE: Probabilistic Multi-Granularity Semantics with Caption-Guided Enhancement for Few-Shot Learning](https://arxiv.org/abs/2601.14111)
*Jiaying Wu,Can Gao,Jinglu Hu,Hui Li,Xiaofeng Cao,Jingcai Guo*

Main category: cs.CV

TL;DR: PMCE is a probabilistic few-shot learning framework utilizing multi-granularity semantics, caption-guided enhancements, and a knowledge bank to improve accuracy on classifying novel categories with limited samples.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of biased and poorly generalizable prototypes in few-shot learning, where traditional methods struggle with scarce labeled data.

Method: It proposes PMCE, a framework combining a probabilistic approach, nonparametric knowledge banks, CLIP-encoded embeddings, instance captions from BLIP, and a lightweight enhancer trained on base classes to boost support and query representations with consistency regularization.

Result: PMCE achieves significant improvements, with up to 7.71% absolute gain over strong baselines in the 1-shot MiniImageNet benchmark. Its effectiveness is validated across four datasets.

Conclusion: PMCE demonstrates that leveraging multi-granularity semantics, caption-based techniques, and probabilistic updates can enhance few-shot classification tasks by producing more robust and generalized prototypes, offering a promising direction for further research.

Abstract: Few-shot learning aims to identify novel categories from only a handful of labeled samples, where prototypes estimated from scarce data are often biased and generalize poorly. Semantic-based methods alleviate this by introducing coarse class-level information, but they are mostly applied on the support side, leaving query representations unchanged. In this paper, we present PMCE, a Probabilistic few-shot framework that leverages Multi-granularity semantics with Caption-guided Enhancement. PMCE constructs a nonparametric knowledge bank that stores visual statistics for each category as well as CLIP-encoded class name embeddings of the base classes. At meta-test time, the most relevant base classes are retrieved based on the similarities of class name embeddings for each novel category. These statistics are then aggregated into category-specific prior information and fused with the support set prototypes via a simple MAP update. Simultaneously, a frozen BLIP captioner provides label-free instance-level image descriptions, and a lightweight enhancer trained on base classes optimizes both support prototypes and query features under an inductive protocol with a consistency regularization to stabilize noisy captions. Experiments on four benchmarks show that PMCE consistently improves over strong baselines, achieving up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in the 1-shot setting. Our code is available at https://anonymous.4open.science/r/PMCE-275D

</details>


### [550] [GIC-DLC: Differentiable Logic Circuits for Hardware-Friendly Grayscale Image Compression](https://arxiv.org/abs/2601.14130)
*Till Aczel,David F. Jenny,Simon Bührer,Andreas Plesner,Antonio Di Maio,Roger Wattenhofer*

Main category: cs.CV

TL;DR: The paper proposes GIC-DLC, a hardware-aware image compression codec that combines neural networks' flexibility with Boolean operations' efficiency, outperforming traditional methods like PNG and JPEG-XL in both compression and energy use.


<details>
  <summary>Details</summary>
Motivation: To address the computational overhead of neural image codecs when deployed on energy-constrained devices like smartphones and drones and to create a more efficient compression approach.

Method: The study develops GIC-DLC, training lookup tables based on differentiable logic circuits to merge neural network flexibility with the computational efficiency of Boolean operations.

Result: Experiments on grayscale benchmark datasets reveal that GIC-DLC surpasses traditional codecs in compression efficiency while significantly reducing energy consumption and latency.

Conclusion: Learned compression can be made hardware-friendly, making GIC-DLC a promising approach for low-power image compression on edge devices.

Abstract: Neural image codecs achieve higher compression ratios than traditional hand-crafted methods such as PNG or JPEG-XL, but often incur substantial computational overhead, limiting their deployment on energy-constrained devices such as smartphones, cameras, and drones. We propose Grayscale Image Compression with Differentiable Logic Circuits (GIC-DLC), a hardware-aware codec where we train lookup tables to combine the flexibility of neural networks with the efficiency of Boolean operations. Experiments on grayscale benchmark datasets show that GIC-DLC outperforms traditional codecs in compression efficiency while allowing substantial reductions in energy consumption and latency. These results demonstrate that learned compression can be hardware-friendly, offering a promising direction for low-power image compression on edge devices.

</details>


### [551] [LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery](https://arxiv.org/abs/2601.14154)
*Shubham Pandey,Bhavin Jawade,Srirangaraj Setlur,Venu Govindaraju,Kenneth Seastedt*

Main category: cs.CV

TL;DR: MIRACLE is a deep learning architecture designed to predict postoperative complication risks in lung cancer surgery by fusing clinical and radiological data.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical issue of postoperative complications, which negatively impact patient outcomes and healthcare costs.

Method: MIRACLE integrates hyperspherical embedding space to fuse clinical and radiological data and incorporates an interventional deep learning module for interpretable and actionable insights.

Result: Validation on the POC-L dataset (3,094 lung cancer surgery patients) shows MIRACLE surpasses traditional machine learning and modern large language model approaches.

Conclusion: MIRACLE improves personalized and explainable postoperative risk prediction, enhancing clinical utility and transparency.

Abstract: Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. To enhance transparency of prediction and clinical utility, we incorporate an interventional deep learning module in MIRACLE, that not only refines predictions but also provides interpretable and actionable insights, allowing domain experts to interactively adjust recommendations based on clinical expertise. We validate our approach on POC-L, a real-world dataset comprising 3,094 lung cancer patients who underwent surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate that MIRACLE outperforms various traditional machine learning models and contemporary large language models (LLM) variants alone, for personalized and explainable postoperative risk management.

</details>


### [552] [One-Shot Refiner: Boosting Feed-forward Novel View Synthesis via One-Step Diffusion](https://arxiv.org/abs/2601.14161)
*Yitong Dong,Qi Zhang,Minchao Jiang,Zhiqiang Wu,Qingnan Fan,Ying Feng,Huaqi Zhang,Hujun Bao,Guofeng Zhang*

Main category: cs.CV

TL;DR: The paper introduces a novel framework for generating high-quality novel view synthesis from sparse image data, addressing limitations in current 3D Gaussian Splatting methods using Vision Transformers.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current NVS methods, such as low-resolution restrictions caused by ViT computational costs and structural inconsistencies in 3D-agnostic generative approaches.

Method: The paper proposes a Dual-Domain Detail Perception Module to handle high-resolution images and a feature-guided diffusion network for preserving high-frequency details. A unified training approach optimizes both the geometric and refinement components jointly.

Result: The proposed method demonstrates superior performance in generating consistent and high-quality novel view synthesis images across various datasets.

Conclusion: This framework effectively overcomes the challenges of computational constraints and inconsistent 3D structures, improving the fidelity and detail of novel view synthesis.

Abstract: We present a novel framework for high-fidelity novel view synthesis (NVS) from sparse images, addressing key limitations in recent feed-forward 3D Gaussian Splatting (3DGS) methods built on Vision Transformer (ViT) backbones. While ViT-based pipelines offer strong geometric priors, they are often constrained by low-resolution inputs due to computational costs. Moreover, existing generative enhancement methods tend to be 3D-agnostic, resulting in inconsistent structures across views, especially in unseen regions. To overcome these challenges, we design a Dual-Domain Detail Perception Module, which enables handling high-resolution images without being limited by the ViT backbone, and endows Gaussians with additional features to store high-frequency details. We develop a feature-guided diffusion network, which can preserve high-frequency details during the restoration process. We introduce a unified training strategy that enables joint optimization of the ViT-based geometric backbone and the diffusion-based refinement module. Experiments demonstrate that our method can maintain superior generation quality across multiple datasets.

</details>


### [553] [ASBA: A-line State Space Model and B-line Attention for Sparse Optical Doppler Tomography Reconstruction](https://arxiv.org/abs/2601.14165)
*Zhenghong Li,Wensheng Cheng,Congwu Du,Yingtian Pan,Zhaozheng Yin,Haibin Ling*

Main category: cs.CV

TL;DR: This paper introduces a novel blood flow-aware network (ASBA) for reconstructing optical Doppler tomography (ODT) images from sparsely sampled data, achieving better results than existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current ODT imaging practices, which rely on dense sampling that leads to prolonged scanning time, high storage demands, and constraints in capturing rapid blood flow dynamics.

Method: The proposed ASBA network employs an A-line ROI state space model for sparse flow feature extraction and a B-line phase attention mechanism to capture long-range signals. It also introduces a flow-aware weighted loss function to prioritize flow signal reconstruction.

Result: Experiments on real animal data show that ASBA significantly outperforms existing reconstruction methods in achieving high-fidelity ODT images from sparse sampling.

Conclusion: This work presents a significant advancement in ODT imaging, enabling efficient and accurate reconstruction from highly sparse sampling and improving upon current state-of-the-art methods.

Abstract: Optical Doppler Tomography (ODT) is an emerging blood flow analysis technique. A 2D ODT image (B-scan) is generated by sequentially acquiring 1D depth-resolved raw A-scans (A-line) along the lateral axis (B-line), followed by Doppler phase-subtraction analysis. To ensure high-fidelity B-scan images, current practices rely on dense sampling, which prolongs scanning time, increases storage demands, and limits the capture of rapid blood flow dynamics. Recent studies have explored sparse sampling of raw A-scans to alleviate these limitations, but their effectiveness is hindered by the conservative sampling rates and the uniform modeling of flow and background signals. In this study, we introduce a novel blood flow-aware network, named ASBA (A-line ROI State space model and B-line phase Attention), to reconstruct ODT images from highly sparsely sampled raw A-scans. Specifically, we propose an A-line ROI state space model to extract sparsely distributed flow features along the A-line, and a B-line phase attention to capture long-range flow signals along each B-line based on phase difference. Moreover, we introduce a flow-aware weighted loss function that encourages the network to prioritize the accurate reconstruction of flow signals. Extensive experiments on real animal data demonstrate that the proposed approach clearly outperforms existing state-of-the-art reconstruction methods.

</details>


### [554] [Progressive self-supervised blind-spot denoising method for LDCT denoising](https://arxiv.org/abs/2601.14180)
*Yichao Liu,Yueyang Teng,Junwen Guo*

Main category: cs.CV

TL;DR: The paper proposes a novel self-supervised denoising approach for low-dose CT (LDCT) images, capable of learning without requiring paired normal-dose CT (NDCT) data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of denoising LDCT images without requiring paired NDCT data that are hard to obtain clinically.

Method: Introduction of a step-wise blind-spot mechanism for conditional independence and the addition of Gaussian noise as a regularization during training.

Result: Extensive experimentation on the Mayo LDCT dataset proves superior performance over current self-supervised methods, and results comparable to or better than supervised methods.

Conclusion: This approach achieves effective LDCT image denoising using a self-supervised framework that avoids dependency on NDCT data, making it a promising alternative for clinical applications.

Abstract: Self-supervised learning is increasingly investigated for low-dose computed tomography (LDCT) image denoising, as it alleviates the dependence on paired normal-dose CT (NDCT) data, which are often difficult to acquire in clinical practice. In this paper, we propose a novel self-supervised training strategy that relies exclusively on LDCT images. We introduce a step-wise blind-spot denoising mechanism that enforces conditional independence in a progressive manner, enabling more fine-grained denoising learning. In addition, we add Gaussian noise to LDCT images, which acts as a regularization and mitigates overfitting. Extensive experiments on the Mayo LDCT dataset demonstrate that the proposed method consistently outperforms existing self-supervised approaches and achieves performance comparable to, or better than, several representative supervised denoising methods.

</details>


### [555] [IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models](https://arxiv.org/abs/2601.14188)
*Liang Shi,Wei Li,Kevin M Beussman,Lin Chen,Yun Fu*

Main category: cs.CV

TL;DR: The paper addresses the limitations of Visual Language Models (VLMs) in Instance-Level Recognition (ILR) by proposing IIR-VLM, a method to enhance their capabilities using auxiliary ILR-specific encoders.


<details>
  <summary>Details</summary>
Motivation: Modern Visual Language Models (VLMs) struggle with fine-grained recognition at the instance level, such as distinguishing familiar individuals or objects, which limits their real-world applications.

Method: The proposed method, IIR-VLM, integrates pre-trained ILR expert models as auxiliary visual encoders to enable VLMs to perform one-shot learning for diverse instances and provide instance-aware visual understanding.

Result: Experiments show that IIR-VLM improves ILR performance on existing benchmarks and a newly designed benchmark that encompasses diverse categories and difficulty levels, including people, faces, pets, and general objects.

Conclusion: IIR-VLM successfully overcomes ILR limitations in VLMs and offers practical enhancements for instance-level recognition across varied domains and tasks.

Abstract: Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM's efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.

</details>


### [556] [Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting](https://arxiv.org/abs/2601.14208)
*Nitin Kulkarni,Akhil Devarashetti,Charlie Cluss,Livio Forte,Dan Buckmaster,Philip Schneider,Chunming Qiao,Alina Vereshchaka*

Main category: cs.CV

TL;DR: This paper introduces a novel pipeline for capturing and creating interactive 3D models of vehicle undercarriages using a three-camera setup, solving issues like wide-angle distortion and low-parallax scenes.


<details>
  <summary>Details</summary>
Motivation: Inspecting vehicle undercarriages is currently a manual and inefficient process; the paper aims to automate this for increased safety and convenience.

Method: The developed pipeline uses a rig-aware Structure-from-Motion (SfM) with robust camera calibration, synchronized video streams, geometric priors, and constrained feature matching strategies.

Result: The method generates photorealistic, interactive 3D undercarriage models, achieving results in real-time and improving the quality compared to standard approaches.

Conclusion: This solution enhances workplace safety and buyer confidence by streamlining undercarriage inspections with advanced 3D modeling and sets the state-of-the-art in this domain.

Abstract: Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.

</details>


### [557] [Soft Tail-dropping for Adaptive Visual Tokenization](https://arxiv.org/abs/2601.14246)
*Zeyuan Chen,Kai Zhang,Zhuowen Tu,Yuanjun Xiong*

Main category: cs.CV

TL;DR: The paper introduces STAT, a visual tokenizer producing codes with adaptive lengths based on image complexity, achieving competitive results in image generation with improved scaling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve 1D autoregressive visual generation by developing a tokenizer that efficiently adapts token length based on the structural complexity of images.

Method: STAT uses an autoencoder to encode images into discrete codes with adaptive keep probabilities, regulated by structural complexity measures and monotonicity constraints.

Result: The proposed STAT tokenizer shows competitive or superior visual generation quality on ImageNet-1k compared to other model families and achieves favorable scaling performance.

Conclusion: STAT enhances 1D AR visual generative models by encoding images into optimally tailored discrete tokens, improving visual generation and scalability.

Abstract: We present Soft Tail-dropping Adaptive Tokenizer (STAT), a 1D discrete visual tokenizer that adaptively chooses the number of output tokens per image according to its structural complexity and level of detail. STAT encodes an image into a sequence of discrete codes together with per-token keep probabilities. Beyond standard autoencoder objectives, we regularize these keep probabilities to be monotonically decreasing along the sequence and explicitly align their distribution with an image-level complexity measure. As a result, STAT produces length-adaptive 1D visual tokens that are naturally compatible with causal 1D autoregressive (AR) visual generative models. On ImageNet-1k, equipping vanilla causal AR models with STAT yields competitive or superior visual generation quality compared to other probabilistic model families, while also exhibiting favorable scaling behavior that has been elusive in prior vanilla AR visual generation attempts.

</details>


### [558] [OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer](https://arxiv.org/abs/2601.14250)
*Pengze Zhang,Yanze Wu,Mengtian Li,Xu Bai,Songtao Zhao,Fulong Ye,Chong Mou,Xinghui Li,Zhuowei Chen,Qian He,Mingyuan Gao*

Main category: cs.CV

TL;DR: OmniTransfer is a new video generation framework excelling at spatio-temporal transfer with better flexibility and fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing video customization methods are limited by reliance on static references or task-specific temporal assumptions, lacking full use of spatio-temporal video data.

Method: OmniTransfer introduces Task-aware Positional Bias, Reference-decoupled Causal Learning, and Task-adaptive Multimodal Alignment for better appearance consistency, temporal control, and task flexibility.

Result: Experiments indicate OmniTransfer significantly improves video appearance and temporal transfer while working effectively without pose data.

Conclusion: OmniTransfer offers a unified and flexible approach to video customization, outperforming current methods in key transfer tasks.

Abstract: Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.

</details>


### [559] [LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR](https://arxiv.org/abs/2601.14251)
*Said Taghadouini,Adrien Cavaillès,Baptiste Aubertin*

Main category: cs.CV

TL;DR: LightOnOCR-2-1B is a 1-billion-parameter multilingual model for document image-to-text conversion, offering advanced features and efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome challenges in traditional OCR pipelines by developing a more efficient, compact, and accurate document conversion model.

Method: The model employs large-scale distillation training, bounding box prediction for image localization, checkpoint averaging, and reinforcement learning with IoU rewards for refinement.

Result: LightOnOCR-2 achieves state-of-the-art performance on OlmOCR-Bench, is smaller (9× less) and faster compared to its predecessors.

Conclusion: The paper successfully introduces a powerful tool for document image-to-text conversion, with robust features, and gives back to the community by releasing datasets and benchmarks.

Abstract: We present \textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.

</details>


### [560] [Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis](https://arxiv.org/abs/2601.14253)
*Hongyuan Chen,Xingyu Chen,Youjia Zhang,Zexiang Xu,Anpei Chen*

Main category: cs.CV

TL;DR: The paper introduces Motion 3-to-4, a method to produce high-quality 4D dynamic objects from video and an optional 3D mesh using decomposition into 3D shape and motion.


<details>
  <summary>Details</summary>
Motivation: Overcoming the difficulty of 4D synthesis due to limited data and challenges in geometry and motion recovery from monocular video.

Method: Decomposes 4D synthesis into static 3D shape generation and motion reconstruction using a latent motion representation and trajectory prediction with a transformer algorithm.

Result: The framework shows superior fidelity and spatial consistency on standard benchmarks and a new dataset with ground-truth geometry.

Conclusion: Motion 3-to-4 provides a robust approach for 4D reconstruction, achieving state-of-the-art results for generating coherent and high-quality dynamic objects.

Abstract: We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.

</details>


### [561] [VideoMaMa: Mask-Guided Video Matting via Generative Prior](https://arxiv.org/abs/2601.14255)
*Sangbeom Lim,Seoung Wug Oh,Jiahui Huang,Heeji Yoon,Seungryong Kim,Joon-Young Lee*

Main category: cs.CV

TL;DR: This paper introduces a method named VideoMaMa to improve video matting (mask-to-matte conversion) using pretrained video diffusion models. It achieves high accuracy with zero-shot generalization to real-world videos and constructs a large video matting dataset called MA-V.


<details>
  <summary>Details</summary>
Motivation: Labeled data for video matting in real-world scenarios is scarce, making it challenging to generalize models to diverse and unconstrained videos effectively.

Method: The authors present VideoMaMa, which converts coarse segmentation masks into detailed alpha mattes. It is trained on synthetic data and uses diffusion models for zero-shot generalization. They also create a scalable pseudo-labeling pipeline to build the MA-V dataset with over 50K annotated real-world videos.

Result: VideoMaMa demonstrates robust zero-shot performance on real-world videos, even without direct real-world training. Additionally, fine-tuning a model (SAM2-Matte) on the MA-V dataset makes it significantly more robust for real-world scenarios compared to relying on existing datasets.

Conclusion: This work highlights the usefulness of leveraging generative priors and scalable pseudo-labeling to build high-quality datasets, paving the way for improved video matting systems for in-the-wild applications.

Abstract: Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.

</details>


### [562] [Implicit Neural Representation Facilitates Unified Universal Vision Encoding](https://arxiv.org/abs/2601.14256)
*Matthew Gwilliam,Xiao Wang,Xuefeng Hu,Zhenheng Yang*

Main category: cs.CV

TL;DR: The paper introduces a unified model blending recognition and generation tasks through a hyper-network for implicit neural representation (INR) while leveraging knowledge distillation to achieve state-of-the-art results in image representation learning and high-quality generative capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap between models designed for recognition tasks (classification, segmentation) and those designed for generative tasks, by developing a unified approach that excels in both.

Method: The authors propose a hyper-network for implicit neural representation to map images to model weights for reconstruction. They further integrate knowledge distillation techniques to enhance model generalization and performance.

Result: The proposed model achieves competitive results with state-of-the-art methods for image representation learning, offers generative capabilities, and learns a highly compressed embedding space with excellent visual task performance.

Conclusion: This novel model successfully combines recognition and generation into a single framework, pushing the boundaries of image representation learning with compressed embeddings and superior visual task performance.

Abstract: Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [563] [Cost-Aware Logging: Measuring the Financial Impact of Excessive Log Retention in Small-Scale Cloud Deployments](https://arxiv.org/abs/2601.11584)
*Jody Almaida Putra*

Main category: cs.DC

TL;DR: This study evaluates the financial and operational impact of log retention policies and shows that reducing retention from 90 to 14 days can save significant storage costs while maintaining useful logs.


<details>
  <summary>Details</summary>
Motivation: To address the hidden and recurring costs of excessive log retention in cloud-native systems by analyzing cost-effective log retention policies.

Method: Synthetic log datasets were used to simulate real-world log variability and evaluate retention windows based on metrics like storage cost, usefulness, and cost-efficiency.

Result: Switching retention from 90 days to 14 days cuts storage costs by 78% while keeping 97% of operationally useful logs, with diminishing returns for longer windows.

Conclusion: Small changes in log retention policy can drastically reduce costs without sacrificing system reliability, promoting better practices in resource-constrained environments.

Abstract: Log data plays a critical role in observability, debugging, and performance monitoring in modern cloud-native systems. In small and early-stage cloud deployments, however, log retention policies are frequently configured far beyond operational requirements, often defaulting to 90 days or more, without explicit consideration of their financial and performance implications. As a result, excessive log retention becomes a hidden and recurring cost.
  This study examines the financial and operational impact of log retention window selection from a cost-aware perspective. Using synthetic log datasets designed to reflect real-world variability in log volume and access patterns, we evaluate retention windows of 7, 14, 30, and 90 days. The analysis focuses on three metrics: storage cost, operationally useful log ratio, and cost per useful log. Operational usefulness is defined as log data accessed during simulated debugging and incident analysis tasks.
  The results show that reducing log retention from 90 days to 14 days can lower log storage costs by up to 78 percent while preserving more than 97 percent of operationally useful logs. Longer retention windows provide diminishing operational returns while disproportionately increasing storage cost and query overhead. These findings suggest that modest configuration changes can yield significant cost savings without compromising system reliability.
  Rather than proposing new logging mechanisms, this work offers a lightweight and accessible framework to help small engineering teams reason about log retention policies through a cost-effectiveness lens. The study aims to encourage more deliberate observability configurations, particularly in resource-constrained cloud environments.

</details>


### [564] [Opportunistic Scheduling for Optimal Spot Instance Savings in the Cloud](https://arxiv.org/abs/2601.12266)
*Neelkamal Bhuyan,Randeep Bhatia,Murali Kodialam,TV Lakshman*

Main category: cs.DC

TL;DR: This paper investigates scheduling delay-sensitive jobs on cloud instances to reduce costs while maintaining average delay constraints, using analytical methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to optimize cost for delay-sensitive jobs in cloud environments while adhering to delay constraints.

Method: The study uses principles from queuing theory, stochastic processes, and optimization to derive cost formulations, identify optimal policies, and design adaptive scheduling algorithms.

Result: Key outcomes include optimal queue length strategies for low delays, a knapsack-based scheduling policy for higher delays, and near-optimal empirical performance of proposed algorithms.

Conclusion: The paper provides effective scheduling strategies for balancing cost and delay, showing strong empirical validation of the proposed models and algorithms.

Abstract: We study the problem of scheduling delay-sensitive jobs over spot and on-demand cloud instances to minimize average cost while meeting an average delay constraint. Jobs arrive as a general stochastic process, and incur different costs based on the instance type. This work provides the first analytical treatment of this problem using tools from queuing theory, stochastic processes, and optimization. We derive cost expressions for general policies, prove queue length one is optimal for low target delays, and characterize the optimal wait-time distribution. For high target delays, we identify a knapsack structure and design a scheduling policy that exploits it. An adaptive algorithm is proposed to fully utilize the allowed delay, and empirical results confirm its near-optimality.

</details>


### [565] [PerCache: Predictive Hierarchical Cache for RAG Applications on Mobile Devices](https://arxiv.org/abs/2601.11553)
*Kaiwei Liu,Liekang Zeng,Lilin Xu,Bufang Yang,Zhenyu Yan*

Main category: cs.DC

TL;DR: This paper introduces PerCache, a hierarchical cache system to accelerate mobile personalized RAG applications, resulting in notable latency reductions and efficient performance under dynamic conditions.


<details>
  <summary>Details</summary>
Motivation: To address high latency in mobile RAG systems caused by lengthy prompts and resource limitations, and to enable efficient reuse of computational results tailored for specific mobile constraints.

Method: PerCache employs a hierarchical cache system for optimal reuse of intermediate results. It includes predictive caching for future queries and adaptive configurations to manage system load efficiently.

Result: PerCache achieved 34.4% latency reduction compared to benchmarks and demonstrated robust performance under variable resource scenarios on mobile devices.

Conclusion: PerCache is effective in optimizing latency, resource use, and scalability for personalized RAG applications on mobile platforms, showcasing its practical advantages.

Abstract: Retrieval-augmented generation (RAG) has been extensively used as a de facto paradigm in various large language model (LLM)-driven applications on mobile devices, such as mobile assistants leveraging personal emails or meeting records. However, due to the lengthy prompts and the resource constraints, mobile RAG systems exhibit significantly high response latency. On this issue, one promising approach is to reuse intermediate computational results across different queries to eliminate redundant computation. But most existing approaches, such as KV cache reuse and semantic cache reuse, are designed for cloud settings and perform poorly, overlooking the distinctive characteristics of mobile RAG.
  We propose PerCache, a novel hierarchical cache solution designed for reducing end-to-end latency of personalized RAG applications on mobile platforms. PerCache adopts a hierarchical architecture that progressively matches similar queries and QKV cache to maximize the reuse of intermediate results at different computing stages. To improve cache hit rate, PerCache applies a predictive method to populate cache with queries that are likely to be raised in the future. In addition, PerCache can adapt its configurations to dynamic system loads, aiming at maximizing the caching utility with minimal resource consumption. We implement PerCache on top of an existing mobile LLM inference engine with commodity mobile phones. Extensive evaluations show that PerCache can surpass the best-performing baseline by 34.4% latency reduction across various applications and maintain optimal latency performance under dynamic resource changes.

</details>


### [566] [Computation-Bandwidth-Memory Trade-offs: A Unified Paradigm for AI Infrastructure](https://arxiv.org/abs/2601.11577)
*Yuankai Fan,Qizhen Weng,Xuelong Li*

Main category: cs.DC

TL;DR: AI Trinity introduces a paradigm to balance computation, bandwidth, and memory for scalable AI infrastructure, addressing bottlenecks and optimizing system performance.


<details>
  <summary>Details</summary>
Motivation: Large AI models face hardware limitations in computation, bandwidth, and memory; addressing these interconnected constraints is necessary to advance scalable AI systems.

Method: AI Trinity presents dynamic trade-offs among computation, bandwidth, and memory. It identifies specific trade-offs: Computation to reduce bandwidth, bandwidth to leverage memory, and memory to lower computation.

Result: Effectiveness demonstrated in edge-cloud communication, distributed training, and model inference applications, optimizing performance across scenarios.

Conclusion: AI Trinity establishes a unified framework for system efficiency in scalable AI, offering innovative designs and flexibility to alleviate hardware resource bottlenecks.

Abstract: Large-scale artificial intelligence models are transforming industries and redefining human machine collaboration. However, continued scaling exposes critical limitations in hardware, including constraints on computation, bandwidth, and memory. These dimensions are tightly interconnected, so improvements in one often create bottlenecks in others, making isolated optimizations less effective. Balancing them to maximize system efficiency remains a central challenge in scalable AI design. To address this challenge, we introduce {Computation-Bandwidth-Memory Trade-offs}, termed the {AI Trinity}, a unified paradigm that positions {computation}, {bandwidth}, and {memory} as coequal pillars for next-generation AI infrastructure. AI Trinity enables dynamic allocation of resources across these pillars, alleviating single-resource bottlenecks and adapting to diverse scenarios to optimize system performance. Within this framework, AI Trinity identifies three fundamental trade-offs: (1) {More Computation$\rightarrow$Less Bandwidth}, wherein computational resources are exploited to reduce data transmission under limited bandwidth conditions, (2) {More Bandwidth$\rightarrow$Less Memory}, which exploits abundant communication capacity to populate or refresh memory when local storage resources are constrained, and (3) {More Memory$\rightarrow$Less Computation}, whereby storage capacity are utilized to mitigate redundant computation when computational costs are prohibitive. We illustrate the effectiveness of AI Trinity through representative system designs spanning edge-cloud communication, large-scale distributed training, and model inference. The innovations embodied in AI Trinity advance a new paradigm for scalable AI infrastructure, providing both a conceptual foundation and practical guidance for a broad range of application scenarios.

</details>


### [567] [PLA-Serve: A Prefill-Length-Aware LLM Serving System](https://arxiv.org/abs/2601.11589)
*Jianshu She,Zonghang Li,Hongchao Du,Shangyu Wu,Wenhao Zheng,Eric Xing,Zhengzhong Liu,Huaxiu Yao,Jason Xue,Qirong Ho*

Main category: cs.DC

TL;DR: PLA-Serve improves latency and throughput in Large Language Model (LLM) serving by using an adaptive scheduling strategy to address heterogeneous workloads, achieving significant reductions in latency and SLO violations.


<details>
  <summary>Details</summary>
Motivation: Recent LLM serving systems face bottlenecks due to unified scheduling policies that fail to adapt to variations in workload characteristics, such as differing prompt lengths.

Method: PLA-Serve employs a dual-queue design to disaggregate requests based on prompt length and introduces a smart batching mechanism optimized for short-prefill workloads. It leverages CUDA Graph-based clustering and batch waiting mechanisms.

Result: PLA-Serve achieves over 30% reduction in prefill latency and decreases SLO violations by 28% in multi-GPU configurations compared to existing systems, significantly improving overall throughput in mixed-request scenarios.

Conclusion: PLA-Serve effectively addresses performance bottlenecks in LLM serving workloads by utilizing adaptive scheduling strategies and advanced batching mechanisms, enhancing latency and throughput in heterogeneous environments.

Abstract: PLA-Serve identifies and disaggregates requests with different prompt lengths in LLM serving to reduce TTFT latency. While recent systems have decoupled the prefill and decode stages to improve throughput, they still rely on unified scheduling policies that fail to adapt to heterogeneous workload characteristics. We observe that prompt-length variations lead to distinct performance bottlenecks, motivating an adaptive scheduling strategy. PLA-Serve disaggregates multi-turn long-prefill requests from short-prefill ones and introduces a length-aware smart batching mechanism for short-prefill workloads. It adopts a dual-queue design that supports temporal disaggregation on a single prefill instance or spatial disaggregation across multiple instances. For short-prefill batches, a batch waiting window and CUDA Graph-based clustering mitigate interference from heterogeneous computation, reducing batching delay and lowering average latency. In real multi-turn workloads, PLA-Serve reduces prefill latency by over 30% compared to vanilla SGLang under prefill**--**decode disaggregation, and further decreases SLO violations by 28% in multi-instance deployments with vanilla data-parallel configuration. Compared to the SGLang router with load balancing, it further lowers SLO violations by 12% in multi-GPU settings. Under high concurrency and mixed-request scenarios, PLA-Serve improves request throughput by 35% serving Qwen2.5-32B model for prefill instance, demonstrating its effectiveness in optimizing heterogeneous LLM serving workloads.

</details>


### [568] [EPD-Serve: A Flexible Multimodal EPD Disaggregation Inference Serving System On Ascend](https://arxiv.org/abs/2601.11590)
*Fan Bai,Pai Peng,Zhengzhi Tang,Zhe Wang,Gong Chen,Xiang Lu,Yinuo Li,Huan Lin,Weizhe Lin,Yaoyuan Wang,Xiaosong Li*

Main category: cs.DC

TL;DR: EPD-Serve is a proposed system to improve multimodal model inference efficiency by disaggregating pipeline stages logically and optimizing resource distribution, enhancing throughput significantly.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal model systems inefficiently use resources and limit throughput by employing monolithic architectures that fail to account for the distinct computational needs of each stage.

Method: EPD-Serve disaggregates inference into Encode, Prefill, and Decode stages, adding dynamic orchestration, asynchronous prefetching, hierarchical KV cache transmission, and adaptive scheduling for multimodal inference.

Result: EPD-Serve demonstrated a 57.37-69.48% improvement in throughput in high-concurrency scenarios, maintaining stringent service latency constraints (TTFT < 2000 ms and TPOT < 50 ms).

Conclusion: Stage-level disaggregation with tailored techniques like dynamic orchestration and efficient caching significantly optimizes multimodal model inference systems for high-throughput and low-latency applications.

Abstract: With the widespread adoption of large multimodal models, efficient inference across text, image, audio, and video modalities has become critical. However, existing multimodal inference systems typically employ monolithic architectures that tightly couple the Encode, Prefill, and Decode stages on homogeneous hardware, neglecting the heterogeneous computational characteristics of each stage. This design leads to inefficient resource utilization and limited system throughput. To address these issues, we propose EPD-Serve, a stage-level disaggregated inference serving system for multimodal models. EPD-Serve decouples the inference pipeline into independent Encode, Prefill, and Decode stages, enabling logical isolation and flexible co-located deployment through dynamic orchestration. Leveraging the Ascend interconnect topology, EPD-Serve introduces asynchronous feature prefetching between Encode and Prefill stages and a hierarchical grouped KV cache transmission mechanism between Prefill and Decode stages to improve cross-node communication efficiency. In addition, EPD-Serve incorporates multi-route scheduling, instance-level load balancing, and multi-stage hardware co-location with spatial multiplexing to better support diverse multimodal workloads. Comprehensive experiments on multimodal understanding models demonstrate that, under high-concurrency scenarios, EPD-Serve improves end-to-end throughput by 57.37-69.48% compared to PD-disaggregated deployment, while satisfying strict SLO constraints, including TTFT below 2000 ms and TPOT below 50 ms. These results highlight the effectiveness of stage-level disaggregation for optimizing multimodal large model inference systems.

</details>


### [569] [Enhancing Model Context Protocol (MCP) with Context-Aware Server Collaboration](https://arxiv.org/abs/2601.11595)
*Meenakshi Amulya Jayanti,X. Y. Han*

Main category: cs.DC

TL;DR: The paper introduces a Context-Aware Model Context Protocol (CA-MCP) to improve LLM-driven multi-agent systems by incorporating a Shared Context Store (SCS) for better task efficiency and coordination.


<details>
  <summary>Details</summary>
Motivation: The current Model Context Protocol (MCP) implementation is stateless, which hinders efficiency and coherence in multi-agent workflows requiring coordinated tasks.

Method: CA-MCP was designed to use a Shared Context Store (SCS), enabling context-managed coordination among agents. This reduces task redundancies by tracking intermediate states and shared variables, allowing persistent collaboration.

Result: Experiments on TravelPlanner and REALM-Bench datasets show that CA-MCP reduced LLM calls and response failures compared to traditional MCP. These results highlight better efficiency and responsiveness.

Conclusion: Incorporating a shared context store via CA-MCP improves the performance of LLM-driven multi-agent systems by enhancing task execution and reducing redundancies.

Abstract: The Model Context Protocol (MCP) has emerged as a widely used framework for enabling LLM-based agents to communicate with external tools and services. The most common implementation of MCP, proposed by Anthropic, heavily relies on a Large Language Model (LLM) to decompose tasks and issue instructions to servers, which act as stateless executors. In particular, the agents, models, and servers are stateless and do not have access to a global context. However, in tasks involving LLM-driven coordination, it is natural that a Shared Context Store (SCS) could improve the efficiency and coherence of multi-agent workflows by reducing redundancy and enabling knowledge transfer between servers. Thus, in this work, we design and assess the performance of a Context-Aware MCP (CA-MCP) that offloads execution logic to specialized MCP servers that read from and write to a shared context memory, allowing them to coordinate more autonomously in real time. In this design, context management serves as the central mechanism that maintains continuity across task executions by tracking intermediate states and shared variables, thereby enabling persistent collaboration among agents without repeated prompting. We present experiments showing that the CA-MCP can outperform the traditional MCP by reducing the number of LLM calls required for complex tasks and decreasing the frequency of response failures when task conditions are not satisfied, thereby improving overall efficiency and responsiveness. In particular, we conducted experiments on the TravelPlanner and REALM-Bench benchmark datasets and observed statistically significant results indicating the potential advantages of incorporating a shared context store via CA-MCP in LLM-driven multi-agent systems.

</details>


### [570] [Hardware-Aware Reformulation of Convolutions for Efficient Execution on Specialized AI Hardware: A Case Study on NVIDIA Tensor Cores](https://arxiv.org/abs/2601.11608)
*Ganesh Bikshandi*

Main category: cs.DC

TL;DR: The paper introduces a hardware-aware technique to reformulate CNN computations for improving execution efficiency on AI hardware.


<details>
  <summary>Details</summary>
Motivation: AI hardware like NVIDIA Tensor Cores imposes alignment requirements that limit CNN performance, which is typically addressed using inefficient methods like zero-padding.

Method: The authors utilized rewrite rules to restructure CNN computations post-training in a hardware-aware way to meet alignment requirements without modifying network weights.

Result: The study successfully implemented a transformation for Tensor Cores and proposed its generalizability for CPUs and other accelerators.

Conclusion: This work marks an initial step towards 'semantic tuning', optimizing CNN deployment on specialized hardware systematically and efficiently.

Abstract: Convolutional Neural Networks (CNNs) are central to modern AI, but their performance is often limited by hardware constraints. NVIDIA Tensor Cores, for instance, require input channels to be multiples of 8 and sometimes 512 for efficient execution. {\em oneDNN} framework for CPU imposes such a requirement for the blocked format. Traditional approaches address such alignment issue using zero-padding, which can be inefficient. In this work, we present a first-step, hardware-aware reformulation of CNN computations using rewrite rules, restructuring the underlying math to satisfy hardware alignment entirely {\bf post-training} without modifying network weights. While our current implementation focuses on a single transformation for Tensor Cores, this approach is generalizable, laying the foundation to explore additional transformations for CPU and accelerators. This study represents an initial step toward {\em semantic tuning}, a systematic, hardware-aware optimization strategy for efficient deployment of CNN models on specialized AI hardware.

</details>


### [571] [Radio Labeling of Strong Prismatic Network With Star](https://arxiv.org/abs/2601.11624)
*Liming Wang,Feng Li,Linlin Cui*

Main category: cs.DC

TL;DR: This paper addresses the issue of optimal wireless network spectrum assignment using radio labeling, focuses on the strong prismatic network with star, and proposes a parallel algorithm for enhanced computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to study efficient spectrum assignment, particularly through converting the problem into graph radio labeling for improving wireless network performance and studying specific graph classes.

Method: The authors analyze radio labeling, specifically for strong prismatic networks with star, define its properties through theorems and examples, and propose a parallel algorithm to improve computation on large-scale networks.

Result: The study provides theoretical insights and practical methodologies for the radio labeling of the strong prismatic network with star, along with introducing a computationally efficient parallel algorithm.

Conclusion: The findings support advancements in optimal channel assignment in wireless communication by exploring radio labeling in a specific graph type and presenting a scalable computation method.

Abstract: The rapid development of wireless communication has made efficient spectrum assignment a crucial factor in enhancing network performance. As a combinatorial optimization model for channel assignment, the radio labeling is recognized as an NP-hard problem. Therefore, converting the spectrum assignment problem into the radio labeling of graphs and studying the radio labeling of specific graph classes is of great significance. For $G$, a radio labeling $\varphi: V(G) \to \{0, 1, 2, \ldots\}$ is required to satisfy $|\varphi(u) - \varphi(v)| \geq \text{diam}(G) + 1 -d_G(u, v)$, where ${diam(G)}$ and $d_G(u, v)$ are diameter and distance between $u$ and $v$. For a radio labeling $\varphi$, its $\text{span}$ is defined as the largest integer assigned by $\varphi$ to the vertices of $G$; the radio labeling specifically denotes the labeling with the minimal span among possible radio labeling. The strong product is a crucial tool for constructing regular networks, and studying its radio labeling is necessary for the design of optimal channel assignment in wireless networks. Within this manuscript, we discuss the radio labeling of strong prismatic network with star, present the relevant theorems and examples, and propose a parallel algorithm to improve computational efficiency in large-scale network scenarios.

</details>


### [572] [A Forward Simulation-Based Hierarchy of Linearizable Concurrent Objects](https://arxiv.org/abs/2601.11646)
*Chao Wang,Ruijia Li,Yang Zhou,Peng Wu,Yi Lv,Jianwei Liao,Jim Woodcock,Zhiming Liu*

Main category: cs.DC

TL;DR: The paper investigates relationships between linearizable objects and forward simulations, showcasing that these objects form specific mathematical structures like bounded lattices and join-semilattices while providing an equivalent linearizability characterization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to better understand the structure and relationships between linearizable objects, especially in terms of forward simulation, wait-freedom, lock-freedom, and obstruction-freedom. It also explores methods for verification of linearizability.

Method: The authors employ mathematical models and proofs to study bounded lattices and join-semilattices formed by linearizable objects under specific constraints. They propose a novel equivalent characterization of linearizability using forward simulation by translating linearizability checking into simulation checks using specific objects.

Result: The study presents the bounded and lattice properties of linearizable objects, showcases the simulation relationships between various structures, and verifies that their methodology can demonstrate correct relationships and equivalence.

Conclusion: The proposed framework enhances the understanding of linearizable objects and offers a robust means for verifying linearizability, contributing a new perspective to concurrent system analysis.

Abstract: In this paper, we systematically investigate the connection between linearizable objects and forward simulation. We prove that the sets of linearizable objects satisfying wait-freedom (resp., lock-freedom or obstruction-freedom) form a bounded join-semilattice under the forward simulation relation, and that the sets of linearizable objects without liveness constraints form a bounded lattice under the same relation. As part of our lattice result, we propose an equivalent characterization of linearizability by reducing checking linearizability w.r.t. sequential specification $Spec$ into checking forward simulation by an object $\mathcal{U}_{Spec}$. To demonstrate the forward simulation relation between linearizable objects, we prove that the objects that are strongly linearizable w.r.t. the same sequential specification and are wait-free (resp., lock-free, obstruction-free) simulate each other, and we prove that the time-stamped queue simulates the Herlihy-Wing queue. We also prove that the Herlihy-Wing queue is simulated by $\mathcal{U}_{Spec}$, and thus, our equivalent characterization of linearizability can be used in the verification of linearizability.

</details>


### [573] [WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching](https://arxiv.org/abs/2601.11652)
*Xiangchen Li,Jiakun Fan,Qingyuan Wang,Dimitrios Spatharakis,Saeid Ghafouri,Hans Vandierendonck,Deepu John,Bo Ji,Ali R. Butt,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: This paper introduces WISP, a system that integrates edge devices into LLM inference to address inefficiencies in speculative decoding.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the imbalance of resource usage, where data centers face exponential workload growth, and edge devices remain underutilized.

Method: WISP employs an intelligent speculation controller, a verification time estimator, and a verification batch scheduler to improve edge-cloud distributed inference systems.

Result: WISP enhances system capacity by up to 4.1x and boosts system goodput by up to 3.7x, outperforming centralized serving and SLED.

Conclusion: WISP effectively balances edge and cloud workloads, addressing bottlenecks in speculative decoding to optimize resource utilization and system performance.

Abstract: As Large Language Models (LLMs) become increasingly accessible to end users, an ever-growing number of inference requests are initiated from edge devices and computed on centralized GPU clusters. However, the resulting exponential growth in computation workload is placing significant strain on data centers, while edge devices remain largely underutilized, leading to imbalanced workloads and resource inefficiency across the network. Integrating edge devices into the LLM inference process via speculative decoding helps balance the workload between the edge and the cloud, while maintaining lossless prediction accuracy. In this paper, we identify and formalize two critical bottlenecks that limit the efficiency and scalability of distributed speculative LLM serving: Wasted Drafting Time and Verification Interference. To address these challenges, we propose WISP, an efficient and SLO-aware distributed LLM inference system that consists of an intelligent speculation controller, a verification time estimator, and a verification batch scheduler. These components collaboratively enhance drafting efficiency and optimize verification request scheduling on the server. Extensive numerical results show that WISP improves system capacity by up to 2.1x and 4.1x, and increases system goodput by up to 1.94x and 3.7x, compared to centralized serving and SLED, respectively.

</details>


### [574] [HALO: Semantic-Aware Distributed LLM Inference in Lossy Edge Network](https://arxiv.org/abs/2601.11676)
*Peirong Zheng,Wenchao Xu,Haozhao Wang,Jinyu Chen,Xuemin Shen*

Main category: cs.DC

TL;DR: The paper introduces HALO, a framework for distributed large language model inference at the edge that enhances performance in unreliable network conditions by assigning less critical neuron groups to unstable devices.


<details>
  <summary>Details</summary>
Motivation: Resource constraints of edge nodes make it difficult to deploy large language model inference, and existing distributed methods struggle under unpredictable network conditions.

Method: The paper proposes HALO with three mechanisms: a semantic-aware predictor for assessing neuron importance, parallel neuron group execution during inference, and a load-balancing scheduler for heterogeneous devices.

Result: Experimental results show HALO achieves a 3.41x speedup in edge environments with unreliable networking while preserving near-optimal performance and outperforming existing methods.

Conclusion: HALO effectively enhances distributed inference for LLMs on edge devices by improving efficiency, synchronizing around unstable networks, and providing significant advancements over current methods.

Abstract: The deployment of large language models' (LLMs) inference at the edge can facilitate prompt service responsiveness while protecting user privacy. However, it is critically challenged by the resource constraints of a single edge node. Distributed inference has emerged to aggregate and leverage computational resources across multiple devices. Yet, existing methods typically require strict synchronization, which is often infeasible due to the unreliable network conditions. In this paper, we propose HALO, a novel framework that can boost the distributed LLM inference in lossy edge network. The core idea is to enable a relaxed yet effective synchronization by strategically allocating less critical neuron groups to unstable devices, thus avoiding the excessive waiting time incurred by delayed packets. HALO introduces three key mechanisms: (1) a semantic-aware predictor to assess the significance of neuron groups prior to activation. (2) a parallel execution scheme of neuron group loading during the model inference. (3) a load-balancing scheduler that efficiently orchestrates multiple devices with heterogeneous resources. Experimental results from a Raspberry Pi cluster demonstrate that HALO achieves a 3.41x end-to-end speedup for LLaMA-series LLMs under unreliable network conditions. It maintains performance comparable to optimal conditions and significantly outperforms the state-of-the-art in various scenarios.

</details>


### [575] [CPU-less parallel execution of lambda calculus in digital logic](https://arxiv.org/abs/2601.13040)
*Harry Fitchett,Charles Fox*

Main category: cs.DC

TL;DR: This paper proposes compiling functional languages directly into digital logic for inherently parallel computation, tested with lambda calculus.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for alternative parallel architectures due to clock speed stagnation despite increasing transistor density.

Method: The approach compiles lambda calculus into digital logic using a tree structure for data localization and beta-reductions in parallel.

Result: A proof-of-concept implementation showcasing correct lambda expression evaluations and highlighting the feasibility of scaling to larger functional languages.

Conclusion: Functional languages can serve as a robust framework for parallel computation without relying on traditional CPU-based designs, paving a way for further research.

Abstract: While transistor density is still increasing, clock speeds are not, motivating the search for new parallel architectures. One approach is to completely abandon the concept of CPU -- and thus serial imperative programming -- and instead to specify and execute tasks in parallel, compiling from programming languages to data flow digital logic. It is well-known that pure functional languages are inherently parallel, due to the Church-Rosser theorem, and CPU-based parallel compilers exist for many functional languages. However, these still rely on conventional CPUs and their von Neumann bottlenecks. An alternative is to compile functional languages directly into digital logic to maximize available parallelism. It is difficult to work with complete modern functional languages due to their many features, so we demonstrate a proof-of-concept system using lambda calculus as the source language and compiling to digital logic. We show how functional hardware can be tailored to a simplistic functional language, forming the ground for a new model of CPU-less functional computation. At the algorithmic level, we use a tree-based representation, with data localized within nodes and communicated data passed between them. This is implemented by physical digital logic blocks corresponding to nodes, and buses enabling message passing. Node types and behaviors correspond to lambda grammar forms, and beta-reductions are performed in parallel allowing branches independent from one another to perform transformations simultaneously. As evidence for this approach, we present an implementation, along with simulation results, showcasing successful execution of lambda expressions. This suggests that the approach could be scaled to larger functional languages. Successful execution of a test suite of lambda expressions suggests that the approach could be scaled to larger functional languages.

</details>


### [576] [RAPID-Serve: Resource-efficient and Accelerated P/D Intra-GPU Disaggregation](https://arxiv.org/abs/2601.11822)
*Amna Masood,Pratishtha Gaur,Nuwan Jayasena*

Main category: cs.DC

TL;DR: This paper introduces RAPID-Serve, a method enhancing efficiency in large language model (LLM) inference by concurrently executing prefill and decode phases on GPUs, improving throughput and latency under service level objectives (SLOs).


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the inefficiencies in existing LLM inference techniques, namely hybrid batching and disaggregated serving, which either increase token latency or cause resource under-utilization and KV-cache transfer overheads.

Method: The authors propose RAPID-Serve, a system that enables concurrent execution of the prefill and decode phases on the same GPU(s). The system employs Adaptive Resource Management for runtime allocation and optionally uses CU masking, a compute unit partitioning feature on AMD GPUs.

Result: RAPID-Serve achieves up to 4.1x improvement in unconstrained throughput (with an average of 1.7x) and up to 32x improvement in throughput under SLO constraints (average improvement of 4.9x) compared to existing approaches.

Conclusion: RAPID-Serve offers a significant improvement in throughput and resource efficiency, particularly in resource-constrained settings, demonstrating its effectiveness over current state-of-the-art methods.

Abstract: Two widely adopted techniques for LLM inference serving systems today are hybrid batching and disaggregated serving. A hybrid batch combines prefill and decode tokens of different requests in the same batch to improve resource utilization and throughput at the cost of increased latency per token. In contrast, disaggregated serving decouples compute-bound prefill and bandwidth-bound decode phases to optimize for service level objectives (SLOs) at the cost of resource under-utilization and KV-cache transfer overheads. To address the limitations of these techniques, we propose RAPID-Serve: a technique to concurrently execute prefill and decode on the same GPU(s) to meet latency SLOs while maintaining high throughput and efficient resource utilization. Furthermore, we propose Adaptive Resource Management for runtime compute resource allocation, optionally leveraging CU masking (a fine-grained Compute Unit partitioning feature on AMD Instinct\textsuperscript{TM} GPUs). RAPID-Serve provides up to 4.1x (average 1.7x) unconstrained throughput improvement and 32x and higher (average 4.9x) throughput improvement under SLO constraints, showing it as an effective strategy compared to the state-of-the-art approaches, particularly in resource-constrained environments.

</details>


### [577] [Big Data Workload Profiling for Energy-Aware Cloud Resource Management](https://arxiv.org/abs/2601.11935)
*Milan Parikh,Aniket Abhishek Soni,Sneja Mitinbhai Shah,Ayush Raj Jha*

Main category: cs.DC

TL;DR: The paper introduces an energy-efficient scheduling framework for cloud data centers to optimize resource use while preserving performance, achieving significant energy savings.


<details>
  <summary>Details</summary>
Motivation: To address the growing energy demands of cloud data centers driven by expanding and complex big data workloads, while maintaining performance standards.

Method: The framework profiles CPU, memory, and storage IO behaviors, integrates historical logs with real-time telemetry, and predicts impacts for virtual machine placements, enabling adaptive consolidation.

Result: Experimental evaluations on Hadoop MapReduce, Spark MLlib, and ETL workloads reveal consistent energy savings of 15%-20% with negligible performance degradation.

Conclusion: Workload profiling is a practical and scalable approach to enhance energy efficiency and sustainability in cloud environments.

Abstract: Cloud data centers face increasing pressure to reduce operational energy consumption as big data workloads continue to grow in scale and complexity. This paper presents a workload aware and energy efficient scheduling framework that profiles CPU utilization, memory demand, and storage IO behavior to guide virtual machine placement decisions. By combining historical execution logs with real time telemetry, the proposed system predicts the energy and performance impact of candidate placements and enables adaptive consolidation while preserving service level agreement compliance. The framework is evaluated using representative Hadoop MapReduce, Spark MLlib, and ETL workloads deployed on a multi node cloud testbed. Experimental results demonstrate consistent energy savings of 15 to 20 percent compared to a baseline scheduler, with negligible performance degradation. These findings highlight workload profiling as a practical and scalable strategy for improving the sustainability of cloud based big data processing environments.

</details>


### [578] [DaggerFFT: A Distributed FFT Framework Using Task Scheduling in Julia](https://arxiv.org/abs/2601.12209)
*Sana Taghipour Anvari,Julian Samaroo,Matin Raayai Ardakani,David Kaeli*

Main category: cs.DC

TL;DR: DaggerFFT, a Julia-based distributed FFT framework, uses dynamic task scheduling to achieve significant performance improvements on modern HPC systems.


<details>
  <summary>Details</summary>
Motivation: To address the performance bottlenecks of traditional FFT algorithms in modern heterogeneous HPC systems, especially for exascale simulations.

Method: Utilizes a task graph approach with dynamically scheduled DTasks for FFT computations, employing work stealing for task assignment across devices in a distributed array setup.

Result: Achieved up to 2.6x speedup on CPU clusters and 1.35x on GPU clusters compared to state-of-the-art FFT libraries, and integrated into real-world geophysical simulations.

Conclusion: Dynamic task scheduling in DaggerFFT enables superior performance and modularity, making it viable for large-scale scientific simulations on heterogeneous HPC platforms.

Abstract: The Fast Fourier Transform (FFT) is a fundamental numerical technique with widespread application in a range of scientific problems. As scientific simulations attempt to exploit exascale systems, there has been a growing demand for distributed FFT algorithms that can effectively utilize modern heterogeneous high-performance computing (HPC) systems. Conventional FFT algorithms commonly encounter performance bottlenecks, especially when run on heterogeneous platforms. Most distributed FFT approaches rely on static task distribution and require synchronization barriers, limiting scalability and impacting overall resource utilization. In this paper we present DaggerFFT, a distributed FFT framework, developed in Julia, that treats highly parallel FFT computations as a dynamically scheduled task graph. Each FFT stage operates on a separately defined distributed array. FFT operations are expressed as DTasks operating on pencil or slab partitioned DArrays. Each FFT stage owns its own DArray, and the runtime assigns DTasks across devices using Dagger's dynamic scheduler that uses work stealing. We demonstrate how DaggerFFT's dynamic scheduler can outperform state-of-the-art distributed FFT libraries on both CPU and GPU backends, achieving up to a 2.6x speedup on CPU clusters and up to a 1.35x speedup on GPU clusters. We have integrated DaggerFFT into Oceananigans.jl, a geophysical fluid dynamics framework, demonstrating that high-level, task-based runtimes can deliver both superior performance and modularity in large-scale, real-world simulations.

</details>


### [579] [Power Aware Dynamic Reallocation For Inference](https://arxiv.org/abs/2601.12241)
*Yiwei Jiang,Sangeeta Chowdhary,Nathaniel Morris,Rutwik Jain,Srilatha Manne,Sam Bayliss*

Main category: cs.DC

TL;DR: This paper presents RAPID, a power-aware disaggregated inference framework for efficiently managing LLM inference under strict power caps, achieving better performance and consistency.


<details>
  <summary>Details</summary>
Motivation: The growth in model and cluster scales has shifted the bottleneck from compute capacity to power limitations. Current disaggregation strategies focus on compute but do not effectively address power-bound scenarios.

Method: The authors introduce RAPID, which reallocates GPU roles, power budgets, and resources both statically and dynamically, optimizing performance while adhering to fixed power limits.

Result: RAPID provides up to a 2x improvement in service level objectives (SLO) attainment under peak load compared to traditional static assignment approaches, without added complexity or cost.

Conclusion: RAPID presents a practical enhancement to current disaggregation inference systems by focusing on power-efficient strategies, thus increasing overall performance and consistency within strict power constraints.

Abstract: Disaggregation has emerged as a powerful strategy for optimizing large language model (LLM) inference by separating compute-intensive prefill and memory-bound decode phases across specialized GPUs. This separation improves utilization and throughput under fixed hardware capacity. However, as model and cluster scales grow, power, rather than compute, has become the dominant limiter of overall performance and cost efficiency. In this paper, we propose RAPID, a power-aware disaggregated inference framework that jointly manages GPU roles and power budgets to sustain goodput within strict power caps. RAPID utilizes static and dynamic power reallocation in addition to GPU reallocation to improve performance under fixed power bounds. RAPID improves overall performance and application consistency beyond what is achievable in current disaggregation solutions, resulting in up to a 2x improvement in SLO attainment at peak load when compared to a static assignment without an increase in complexity or cost.

</details>


### [580] [Multi-Partner Project: Multi-GPU Performance Portability Analysis for CFD Simulations at Scale](https://arxiv.org/abs/2601.14159)
*Panagiotis-Eleftherios Eleftherakis,George Anagnostopoulos,Anastassis Kapetanakis,Mohammad Umair,Jean-Yves Vet,Konstantinos Iliakis,Jonathan Vincent,Jing Gong,Akshay Patil,Clara García-Sánchez,Gerardo Zampino,Ricardo Vinuesa,Sotirios Xydis*

Main category: cs.DC

TL;DR: The paper explores the performance portability of SOD2D, a spectral element simulation framework, across AMD and NVIDIA GPUs in the context of urban airflow prediction.


<details>
  <summary>Details</summary>
Motivation: To enable computational fluid dynamics (CFD) simulations to efficiently utilize heterogeneous GPU-centric supercomputing architectures while addressing the challenge of maintaining performance portability across different accelerators.

Method: An analysis of SOD2D's physical and numerical models, computational hotspots, single-GPU performance on server-grade NVIDIA and AMD GPUs, and scalability in a multi-level design space on the LUMI multi-GPU cluster.

Result: Significant performance variability (0.69× - 3.91× deviations) was identified in memory access optimizations and throughput on NVIDIA and AMD GPUs, emphasizing challenges in performance portability.

Conclusion: Achieving optimal performance across heterogeneous GPU architectures requires multi-level tuning, as performance variability and scalability issues remain significant challenges.

Abstract: As heterogeneous supercomputing architectures leveraging GPUs become increasingly central to high-performance computing (HPC), it is crucial for computational fluid dynamics (CFD) simulations, a de-facto HPC workload, to efficiently utilize such hardware. One of the key challenges of HPC codes is performance portability, i.e. the ability to maintain near-optimal performance across different accelerators. In the context of the \textbf{REFMAP} project, which targets scalable, GPU-enabled multi-fidelity CFD for urban airflow prediction, this paper analyzes the performance portability of SOD2D, a state-of-the-art Spectral Elements simulation framework across AMD and NVIDIA GPU architectures. We first discuss the physical and numerical models underlying SOD2D, highlighting its computational hotspots. Then, we examine its performance and scalability in a multi-level manner, i.e. defining and characterizing an extensive full-stack design space spanning across application, software and hardware infrastructure related parameters. Single-GPU performance characterization across server-grade NVIDIA and AMD GPU architectures and vendor-specific compiler stacks, show the potential as well as the diverse effect of memory access optimizations, i.e. 0.69$\times$ - 3.91$\times$ deviations in acceleration speedup. Performance variability of SOD2D at scale is further examined on the LUMI multi-GPU cluster, where profiling reveals similar throughput variations, highlighting the limits of performance projections and the need for multi-level, informed tuning.

</details>


### [581] [RIPPLE++: An Incremental Framework for Efficient GNN Inference on Evolving Graphs](https://arxiv.org/abs/2601.12347)
*Pranjal Naman,Parv Agarwal,Hrishikesh Haritas,Yogesh Simmhan*

Main category: cs.DC

TL;DR: This paper introduces RIPPLE++, a framework for efficient and accurate streaming inference in graph neural networks (GNNs) to handle dynamic graph updates.


<details>
  <summary>Details</summary>
Motivation: Existing GNN inference methods face inefficiency, redundant computations, high communication costs, and non-deterministic sampling, hindering real-time applications.

Method: RIPPLE++ employs a generalized incremental programming model to propagate updates in GNNs, accommodating vertex/edge changes and feature updates, and supports single-machine and distributed deployments.

Result: RIPPLE++ achieves high throughput (up to 56K updates/sec on sparse graphs) and low latency (0.06--960ms), outperforming baselines by 2.2--24x in single-machine and 25x in distributed settings.

Conclusion: RIPPLE++ effectively addresses the challenges of dynamic graph inference in GNNs, enabling low-latency and high-throughput real-time applications.

Abstract: Real-world graphs are dynamic, with frequent updates to their structure and features due to evolving vertex and edge properties. These continual changes pose significant challenges for efficient inference in graph neural networks (GNNs). Existing vertex-wise and layer-wise inference approaches are ill-suited for dynamic graphs, as they incur redundant computations, large neighborhood traversals, and high communication costs, especially in distributed settings. Additionally, while sampling-based approaches can be adopted to approximate final layer embeddings, these are often not preferred in critical applications due to their non-determinism. These limitations hinder low-latency inference required in real-time applications. To address this, we propose RIPPLE++, a framework for streaming GNN inference that efficiently and accurately updates embeddings in response to changes in the graph structure or features. RIPPLE++ introduces a generalized incremental programming model that captures the semantics of GNN aggregation functions and incrementally propagates updates to affected neighborhoods. RIPPLE++ accommodates all common graph updates, including vertex/edge addition/deletions and vertex feature updates. RIPPLE++ supports both single-machine and distributed deployments. On a single machine, it achieves up to $56$K updates/sec on sparse graphs like Arxiv ($169$K vertices, $1.2$M edges), and about $7.6$K updates/sec on denser graphs like Products ($2.5$M vertices, $123.7$M edges), with latencies of $0.06$--$960$ms, and outperforming state-of-the-art baselines by $2.2$--$24\times$ on throughput. In distributed settings, RIPPLE++ offers up to $\approx25\times$ higher throughput and $20\times$ lower communication costs compared to recomputing baselines.

</details>


### [582] [ASAS-BridgeAMM: Trust-Minimized Cross-Chain Bridge AMM with Failure Containment](https://arxiv.org/abs/2601.12434)
*Shengwei You,Aditya Joshi,Andrey Kuehlkamp,Jarek Nabrzyski*

Main category: cs.DC

TL;DR: This paper introduces ASAS-BridgeAMM, a bridge-based automated market maker that addresses systemic risks in decentralized finance by introducing 'Contained Degradation' to minimize insolvency and improve security.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the major vulnerability in current cross-chain bridges used in decentralized finance, which can either function fully or fail catastrophically without intermediate safeguards.

Method: The proposed ASAS-BridgeAMM dynamically manages execution risks by adjusting mechanisms like collateral haircuts and withdrawals. It also employs adversarial simulations, formal safety proofs, and an implementation in Solidity.

Result: ASAS-BridgeAMM demonstrated a 73% reduction in worst-case insolvency and maintained over 104.5% transaction volume during stress scenarios.

Conclusion: ASAS-BridgeAMM showcases robust safety, live operations, and manipulation resistance, highlighting its potential to reform bridge dynamics in DeFi while significantly containing systemic risks.

Abstract: Cross-chain bridges constitute the single largest vector of systemic risk in Decentralized Finance (DeFi), accounting for over \$2.8 billion in losses since 2021. The fundamental vulnerability lies in the binary nature of existing bridge security models: a bridge is either fully operational or catastrophically compromised, with no intermediate state to contain partial failures. We present ASAS-BridgeAMM, a bridge-coupled automated market maker that introduces Contained Degradation: a formally specified operational state where the system gracefully degrades functionality in response to adversarial signals. By treating cross-chain message latency as a quantifiable execution risk, the protocol dynamically adjusts collateral haircuts, slippage bounds, and withdrawal limits. Across 18 months of historical replay on Ethereum and two auxiliary chains, ASAS-BridgeAMM reduces worst-case bridge-induced insolvency by 73% relative to baseline mint-and-burn architectures, while preserving 104.5% of transaction volume during stress periods. In rigorous adversarial simulations involving delayed finality, oracle manipulation, and liquidity griefing, the protocol maintains solvency with probability $>0.9999$ and bounds per-epoch bad debt to $<0.2%$ of total collateral. We provide a reference implementation in Solidity and formally prove safety (bounded debt), liveness (settlement completion), and manipulation resistance under a Byzantine relayer model.

</details>


### [583] [SGCP: A Self-Organized Game-Theoretic Framework For Collaborative Perception](https://arxiv.org/abs/2601.12524)
*Zechuan Gong,Hui Zhang,Yuquan Yang,Wenyu Lu*

Main category: cs.DC

TL;DR: The paper presents a decentralized framework for collaborative perception in autonomous driving, which organizes vehicles into cooperative groups and uses game theory to optimize communication and sensing.


<details>
  <summary>Details</summary>
Motivation: To improve safety in autonomous driving by enhancing perception through vehicle collaboration, especially in scenarios with dense traffic and limited communication infrastructure.

Method: The framework organizes vehicles into clusters and uses a two-stage game-theoretic approach: cluster formation based on complementarity and coherence, and selective data sharing using a non-cooperative potential game. Compact detection messages are exchanged instead of raw data.

Result: The proposed method achieves reduced communication overhead, higher perception accuracy, and wider effective coverage, as demonstrated on the CARLA-OpenCDA-NS3 platform.

Conclusion: The decentralized framework enables efficient and scalable collaborative perception in autonomous driving, overcoming traditional communication limitations while improving safety and perception quality.

Abstract: Collaborative perception holds great promise for improving safety in autonomous driving, particularly in dense traffic where vehicles can share sensory information to overcome individual blind spots and extend awareness. However, deploying such collaboration at scale remains difficult when communication bandwidth is limited and no roadside infrastructure is available. To overcome these limitations, we introduce a fully decentralized framework that enables vehicles to self organize into cooperative groups using only vehicle to vehicle communication. The approach decomposes the problem into two sequential game theoretic stages. In the first stage, vehicles form stable clusters by evaluating mutual sensing complementarity and motion coherence, and each cluster elects a coordinator. In the second stage, the coordinator guides its members to selectively transmit point cloud segments from perceptually salient regions through a non cooperative potential game, enabling efficient local fusion. Global scene understanding is then achieved by exchanging compact detection messages across clusters rather than raw sensor data. We design distributed algorithms for both stages that guarantee monotonic improvement of the system wide potential function. Comprehensive experiments on the CARLA-OpenCDA-NS3 co-simulation platform show that our method reduces communication overhead while delivering higher perception accuracy and wider effective coverage compared to existing baselines.

</details>


### [584] [Dynamic Detection of Inefficient Data Mapping Patterns in Heterogeneous OpenMP Applications](https://arxiv.org/abs/2601.12713)
*Luke Marzen,Junhyung Shim,Ali Jannesari*

Main category: cs.DC

TL;DR: This paper introduces OMPDataPerf, a tool for profiling and optimizing data transfer inefficiencies in heterogeneous applications using dynamic analysis.


<details>
  <summary>Details</summary>
Motivation: The increasing pairing of CPUs with accelerators for enhanced performance and energy efficiency is hindered by data movement inefficiencies, making application development challenging.

Method: The authors developed OMPDataPerf, a tool based on dynamic analysis to identify inefficient data transfer and allocation in heterogeneous OpenMP applications. It leverages the OMPT interface with minimal runtime overhead.

Result: OMPDataPerf provides detailed traces, source code attribution, and optimization suggestions for data transfer inefficiencies, with only 5% geometric-mean runtime overhead.

Conclusion: OMPDataPerf effectively aids developers in identifying and addressing data transfer inefficiencies, simplifying application optimization for heterogeneous computing.

Abstract: With the growing prevalence of heterogeneous computing, CPUs are increasingly being paired with accelerators to achieve new levels of performance and energy efficiency. However, data movement between devices remains a significant bottleneck, complicating application development. Existing performance tools require considerable programmer intervention to diagnose and locate data transfer inefficiencies. To address this, we propose dynamic analysis techniques to detect and profile inefficient data transfer and allocation patterns in heterogeneous applications. We implemented these techniques into OMPDataPerf, which provides detailed traces of problematic data mappings, source code attribution, and assessments of optimization potential in heterogeneous OpenMP applications. OMPDataPerf uses the OpenMP Tools Interface (OMPT) and incurs only a 5 % geometric-mean runtime overhead.

</details>


### [585] [Efficient Local-to-Global Collaborative Perception via Joint Communication and Computation Optimization](https://arxiv.org/abs/2601.12749)
*Hui Zhang,Yuquan Yang,Zechuan Gong,Xiaohua Xu,Dan Keun Sung*

Main category: cs.DC

TL;DR: The Local-to-Global Collaborative Perception (LGCP) framework improves collaborative perception among autonomous vehicles with reduced communication overhead and computation latency, while maintaining or enhancing accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses limitations in autonomous driving perception caused by sensing constraints like limited range and occlusions. It seeks to enhance collaborative perception while reducing communication and computational demands as connected vehicle participation scales up.

Method: The road is partitioned into non-overlapping areas, each handled by a CAV group. Group leaders aggregate local perception data and send it to an RSU, which creates a global view by integrating the data and broadcasting it to all CAVs via centralized scheduling.

Result: LGCP reduces data transmission by an average of 44 times compared to alternatives, without compromising, and even enhancing, collaborative perception performance.

Conclusion: The LGCP framework effectively balances local and global perception by employing efficient data and computation management strategies, showcasing significant improvements in scalability and resource efficiency for autonomous driving.

Abstract: Autonomous driving relies on accurate perception to ensure safe driving. Collaborative perception improves accuracy by mitigating the sensing limitations of individual vehicles, such as limited perception range and occlusion-induced blind spots. However, collaborative perception often suffers from high communication overhead due to redundant data transmission, as well as increasing computation latency caused by excessive load with growing connected and autonomous vehicles (CAVs) participation. To address these challenges, we propose a novel local-to-global collaborative perception framework (LGCP) to achieve collaboration in a communication- and computation-efficient manner. The road of interest is partitioned into non-overlapping areas, each of which is assigned a dedicated CAV group to perform localized perception. A designated leader in each group collects and fuses perception data from its members, and uploads the perception result to the roadside unit (RSU), establishing a link between local perception and global awareness. The RSU aggregates perception results from all groups and broadcasts a global view to all CAVs. LGCP employs a centralized scheduling strategy via the RSU, which assigns CAV groups to each area, schedules their transmissions, aggregates area-level local perception results, and propagates the global view to all CAVs. Experimental results demonstrate that the proposed LGCP framework achieves an average 44 times reduction in the amount of data transmission, while maintaining or even improving the overall collaborative performance.

</details>


### [586] [Unleashing Efficient Asynchronous RL Post-Training via Staleness-Constrained Rollout Coordination](https://arxiv.org/abs/2601.12784)
*Haoyang Li,Sheng Lin,Fangcheng Fu,Yuming Zhou,Xiaodong Ji,Yanfeng Zhao,Lefeng Wang,Jie Jiang,Bin Cui*

Main category: cs.DC

TL;DR: The paper introduces StaleFlow, a system optimizing reinforcement learning by addressing data staleness and skewness, achieving higher throughput without sacrificing convergence.


<details>
  <summary>Details</summary>
Motivation: Address issues of data staleness and skewness in asynchronous disaggregated reinforcement learning architectures, where current systems struggle to balance convergence and performance.

Method: StaleFlow uses a global consistency protocol to track and constrain trajectory staleness and introduces data servers for better coordination. It also incorporates staleness-aware, throughput-oriented strategies.

Result: StaleFlow demonstrates up to 1.42-2.68x higher throughput (1.17-2.01x on average) compared to existing systems, maintaining RL convergence.

Conclusion: StaleFlow effectively balances and addresses data staleness and skewness, enhancing reinforcement learning system performance without trade-offs.

Abstract: Reinforcement learning (RL) post-training has become pivotal for enhancing the capabilities of modern large models. A recent trend is to develop RL systems with a fully disaggregated architecture, which decouples the three RL phases (rollout, reward, and training) onto separate resources and executes them asynchronously. However, two critical data-level concerns arise: (1) asynchronous execution leads to data staleness in trajectories (the data generated by rollout) as the model parameters used in rollout may not be up to date, which impairs RL convergence; and (2) the length variation of trajectories introduces severe data skewness, leading to workload imbalance and degraded system performance.
  Existing systems fail to address these two concerns in a unified manner. Techniques that tightly control data staleness often constrain effective data skewness mitigation, while aggressive data skewness mitigation tends to exacerbate data staleness. As a result, systems are forced to trade off convergence for performance, or vice versa. To address this, we propose StaleFlow, an RL post-training system that jointly tackles data staleness and skewness. First, to control staleness, StaleFlow introduces a global consistency protocol that tracks the full lifecycle of each trajectory and constrains staleness. Second, to mitigate skewness, StaleFlow re-designs the RL system architecture by constructing data servers for trajectories and parameters to achieve flexible rollout coordination. Subsequently, we develop a suite of staleness-aware, throughput-oriented strategies to enhance system performance. Evaluations show that StaleFlow achieves up to 1.42-2.68$\times$ (1.17-2.01$\times$ on average) higher throughput than state-of-the-art systems, without compromising convergence.

</details>


### [587] [From Design to Deorbit: A Solar-Electric Autonomous Module for Multi-Debris Remediation](https://arxiv.org/abs/2601.12830)
*Om Mishra,Jayesh Patil,Sathwik Narkedimilli,G Srikantha Sharma,Ananda S,Manjunath K Vanahalli*

Main category: cs.DC

TL;DR: The study introduces a solar-powered solution for space debris removal, combining mechanical capture with efficient propulsion and autonomous navigation, validated through simulations.


<details>
  <summary>Details</summary>
Motivation: The increasing amount of orbital debris poses risks to space operations, requiring innovative solutions beyond traditional fuel-based methods.

Method: A new architecture using mechanical clamping, solar-powered NASA NEXT thrusters, and advanced autonomous navigation (EKF, DTN protocols) is proposed and tested in simulations.

Result: The system achieved successful retrograde deorbit maneuvers, <10m position RMSE, and 93% data delivery efficiency, validated through high-fidelity simulations.

Conclusion: This approach sets a benchmark for fuel-independent debris removal, promoting mission longevity and efficiency in managing orbital debris.

Abstract: The escalating accumulation of orbital debris threatens the sustainability of space operations, necessitating active removal solutions that overcome the limitations of current fuel-dependent methods. To address this, this study introduces a novel remediation architecture that integrates a mechanical clamping system for secure capture with a high-efficiency, solar-powered NASA Evolutionary Xenon Thruster (NEXT) and autonomous navigation protocols. High-fidelity simulations validate the architecture's capabilities, demonstrating a successful retrograde deorbit from 800 km to 100 km, <10m position Root Mean Square Errors (RMSE) via radar-based Extended Kalman Filter (EKF) navigation, and a 93\% data delivery efficiency within 1 second using Delay/Disruption Tolerant Network (DTN) protocols. This approach significantly advances orbital management by establishing a benchmark for renewable solar propulsion that minimizes reliance on conventional fuels and extends mission longevity for multi-target removal.

</details>


### [588] [On Resilient and Efficient Linear Secure Aggregation in Hierarchical Federated Learning](https://arxiv.org/abs/2601.12853)
*Shudi Weng,Xiang Zhang,Yizhou Zhao,Giuseppe Caire,Ming Xiao,Mikael Skoglund*

Main category: cs.DC

TL;DR: The paper explores limits and optimal protocols for hierarchical secure aggregation in environments with unreliable communication.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in secure aggregation with unreliable hierarchical network communication in federated learning scenarios.

Method: The authors analyze minimum communication and randomness costs, propose an optimal protocol, and validate its optimality with a matching converse proof.

Result: An optimal protocol for secure aggregation is designed, achieving minimum costs, and bridging the gap between theoretical and real-world applications.

Conclusion: The proposed protocol optimally addresses secure aggregation under unreliable communication, with relevance to federated learning contexts.

Abstract: In this paper, we study the fundamental limits of hierarchical secure aggregation under unreliable communication. We consider a hierarchical network where each client connects to multiple relays, and both client-to-relay and relay-to-server links are intermittent. Under this setting, we characterize the minimum communication and randomness costs required to achieve robust secure aggregation. We then propose an optimal protocol that attains these minimum costs, and establish its optimality through a matching converse proof. In addition, we introduce an improved problem formulation that bridges the gap between existing information-theoretic secure aggregation protocols and practical real-world federated learning problems.

</details>


### [589] [Sutradhara: An Intelligent Orchestrator-Engine Co-design for Tool-based Agentic Inference](https://arxiv.org/abs/2601.12967)
*Anish Biswas,Kanishk Goel,Jayashree Mohan,Alind Khare,Anjaly Parayil,Ramachandran Ramjee,Chetan Bansal*

Main category: cs.DC

TL;DR: The paper introduces SUTRADHARA, a system addressing latency issues in agentic LLMs by co-designing orchestration with LLM operations.


<details>
  <summary>Details</summary>
Motivation: Address the rising latency bottlenecks in agentic LLMs, which hinder performance due to reliance on tool calls, caching problems, and inefficiencies in request sequencing.

Method: Propose a co-designed system, SUTRADHARA, integrating orchestration with LLM serving through optimizations in tool execution, prompt splitting, and caching strategies.

Result: SUTRADHARA decreases FTR latency by 15% and overall latency by 10% on workloads using A100 GPUs.

Conclusion: Co-design of orchestration and LLM engine demonstrates a systematic approach to reduce latency and improve efficiency in agentic LLM systems.

Abstract: Agentic applications are LLMs that iteratively invoke external tools to accomplish complex tasks. Such tool-based agents are rapidly becoming the dominant paradigm for deploying language models in production. Unlike traditional single-turn inference, agentic workloads chain together multiple LLM calls and tool executions before producing a final response, creating a new performance bottleneck that manifests as increased latency in First Token Rendered (FTR) of the final answer. Through analysis of synthetic requests at production scale, we reveal three critical challenges: tool calls account for 30-80% of FTR latency, KV cache hit rates collapse despite substantial context reuse across iterations, and sequential orchestration wastes potential intra-request parallelism by sequentially executing LLM calls and tools. These bottlenecks stem from a design gap in which orchestrators and LLM engines operate as decoupled black boxes, preventing cross-layer optimizations. We present SUTRADHARA, a co-designed agentic inference system that integrates orchestration with LLM serving through a thin API enabling three optimizations: overlap tool execution with subsequent LLM prefill using tool-aware prompt splitting, streaming tool execution to dispatch tools incrementally during decode rather than waiting for complete output, and orchestrator-aware cache management that uses semantic hints to improve hit rates and reduce thrashing. Implemented on vLLM, SUTRADHARA reduces median FTR latency by 15% and end-to-end latency by 10% across workloads on A100 GPUs, demonstrating that co-design can systematically tame latency in agentic systems.

</details>


### [590] [Enshrined Proposer Builder Separation in the presence of Maximal Extractable Value](https://arxiv.org/abs/2601.12989)
*Yitian Wang,Yebo Feng,Yingjiu Li,Jiahua Xu*

Main category: cs.DC

TL;DR: This paper evaluates the Ethereum enshrined Proposer Builder Separation (ePBS) mechanism, identifying intensified issues of centralization and economic bias despite the design changes meant to reduce vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Address the fairness, decentralization, and user trust concerns in Ethereum's transaction processing under Proof-of-Stake, especially in light of MEV vulnerabilities.

Method: Developed a formal framework integrating mathematical analysis and agent-based simulations to assess the auction-based block-building mechanism of ePBS, focusing on MEV dynamics.

Result: Findings indicate ePBS significantly increases profit centralization (Gini coefficient rises to 0.8358) and economic bias with proposers receiving 95.4% of block rewards, favoring aggressive MEV strategies.

Conclusion: ePBS exacerbates centralization and economic incentive issues, underscoring the need for improved designs to balance fairness, decentralization, and MEV mitigation.

Abstract: In blockchain systems operating under the Proof-of-Stake (PoS) consensus mechanism, fairness in transaction processing is essential to preserving decentralization and maintaining user trust. However, with the emergence of Maximal Extractable Value (MEV), concerns about economic centralization and content manipulation have intensified. To address these vulnerabilities, the Ethereum community has introduced Proposer Builder Separation (PBS), which separates block construction from block proposal. Later, enshrined Proposer Builder Separation (ePBS) was also proposed in EIP-7732, which embeds PBS directly into the Ethereum consensus layer.
  Our work identifies key limitations of ePBS by developing a formal framework that combines mathematical analysis and agent-based simulations to evaluate its auction-based block-building mechanism, with particular emphasis on MEV dynamics. Our results reveal that, although ePBS redistributes responsibilities between builders and proposers, it significantly amplifies profit and content centralization: the Gini coefficient for profits rises from 0.1749 under standard PoS without ePBS to 0.8358 under ePBS. This sharp increase indicates that a small number of efficient builders capture most value via MEV-driven auctions. Moreover, 95.4% of the block value is rewarded to proposers in ePBS, revealing a strong economic bias despite their limited role in block assembly. These findings highlight that ePBS exacerbates incentives for builders to adopt aggressive MEV strategies, suggesting the need for future research into mechanism designs that better balance decentralization, fairness, and MEV mitigation.

</details>


### [591] [Exploration on Highly Dynamic Graphs](https://arxiv.org/abs/2601.13047)
*Ashish Saxena,Kaushik Mondal*

Main category: cs.DC

TL;DR: The paper investigates the exploration problem by mobile agents in two dynamic graph models, strengthening previous results and developing new bounds and algorithms.


<details>
  <summary>Details</summary>
Motivation: To address limitations and gaps in existing results for exploration in dynamic graph models, specifically within the $1$-Interval Connectivity and Connectivity Time frameworks.

Method: The authors analyzed the exploration problem, improved impossibility bounds, provided theoretical proofs, and designed an efficient algorithm leveraging specific assumptions like $1$-hop visibility and minimal agent memory.

Result: Stronger impossibility results were established in the $1$-Interval model, and an upper bound for agents required in Connectivity Time models was reduced to $(n-1)(n-2)/2$. Additionally, exploration was shown to require specific conditions such as $1$-hop visibility.

Conclusion: The study advances theoretical understanding of exploration in dynamic graphs by refining impossibility bounds and proposing feasible algorithmic solutions under certain conditions.

Abstract: We study the exploration problem by mobile agents in two prominent models of dynamic graphs: $1$-Interval Connectivity and Connectivity Time. The $1$-Interval Connectivity model was introduced by Kuhn et al.~[STOC 2010], and the Connectivity Time model was proposed by Michail et al.~[JPDC 2014]. Recently, Saxena et al.~[TCS 2025] investigated the exploration problem under both models. In this work, we first strengthen the existing impossibility results for the $1$-Interval Connectivity model. We then show that, in Connectivity Time dynamic graphs, exploration is impossible with $\frac{(n-1)(n-2)}{2}$ mobile agents, even when the agents have full knowledge of all system parameters, global communication, full visibility, and infinite memory. This significantly improves the previously known bound of $n$. Moreover, we prove that to solve exploration with $\frac{(n-1)(n-2)}{2}+1$ agents, $1$-hop visibility is necessary. Finally, we present an exploration algorithm that uses $\frac{(n-1)(n-2)}{2}+1$ agents, assuming global communication, $1$-hop visibility, and $O(\log n)$ memory per agent.

</details>


### [592] [OPTIMUM-DERAM: Highly Consistent, Scalable, and Secure Multi-Object Memory using RLNC](https://arxiv.org/abs/2601.13146)
*Nicolas Nicolaou,Kishori M. Konwar,Moritz Grundei,Aleksandr Bezobchuk,Muriel Médard,Sriram Vishwanath*

Main category: cs.DC

TL;DR: The paper introduces OPTIMUM-DERAM, a decentralized, secure, and scalable shared memory system that leverages cutting-edge techniques like RLNC and blockchains to outperform traditional models like ABD.


<details>
  <summary>Details</summary>
Motivation: The need for a scalable, resource-efficient, and secure decentralized shared memory solution as traditional implementations are costly and limited in practical applications.

Method: OPTIMUM-DERAM uses Random Linear Network Codes for performance, consistent hashing for object discovery, dynamic blockchain-enabled participant management, and Byzantine failure tolerance to improve shared memory design.

Result: Experimentation on globally distributed nodes showed that OPTIMUM-DERAM offers superior performance and scalability compared to previous methods like the ABD algorithm.

Conclusion: OPTIMUM-DERAM establishes itself as an effective solution for decentralized shared memory by addressing performance, scalability, and security challenges through innovative methods.

Abstract: This paper introduces OPTIMUM-DERAM, a highly consistent, scalable, secure, and decentralized shared memory solution. Traditional distributed shared memory implementations offer multi-object support by multi-threading a single object memory instance over the same set of data hosts. While theoretically sound, the amount of resources required made such solutions prohibitively expensive in practical systems. OPTIMUM-DERAM proposes a decentralized, reconfigurable, atomic read/write shared memory (DeRAM) that: (i) achieves improved performance and storage scalability by leveraging Random Linear Network Codes (RLNC); (ii) scales in the number of supported atomic objects by introducing a new object placement and discovery approach based on a consistent hashing ring; (iii) scales in the number of participants by allowing dynamic joins and departures leveraging a blockchain oracle to serve as a registry service; and (iv) is secure against malicious behavior by tolerating Byzantine failures.
  Experimental results over a globally distributed set of nodes, help us realize the performance and scalability gains of OPTIMUM-DERAM over previous distributed shared memory solutions (i.e., the ABD algorithm [3])

</details>


### [593] [Towards Scalable Federated Container Orchestration: The CODECO Approach](https://arxiv.org/abs/2601.13351)
*Rute C. Sofia,Josh Salomon,Ray Carrol,Luis Garcés-Erice,Peter Urbanetz,Jürgen Gesswein,Rizkallah Touma,Alejandro Espinosa,Luis M. Contreras,Vasileios Theodorou,George Papathanail,Georgios Koukis,Vassilis Tsaoussidis,Alberto del Rio,David Jimenez,Efterpi Paraskevoulakou,Panagiotis Karamolegkos,John Soldatos,Borja Dorado Nogales,Alejandro Tjaarda*

Main category: cs.DC

TL;DR: The paper introduces CODECO, a Kubernetes-based federated orchestration framework, combining centralized and decentralized methods for better application management across heterogeneous infrastructures.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations of cloud-centric deployment in managing heterogeneous infrastructures, mobility, and multi-provider environments.

Method: CODECO incorporates semantic application models, partition-based federation, and AI-assisted decision support with a hybrid governance model for context-aware and adaptive application placement.

Result: CODECO enables enhanced context-aware orchestration, adaptive application management, and reproducibility in federated Edge-Cloud environments.

Conclusion: CODECO successfully provides a scalable and coherent orchestration framework for heterogeneous infrastructures, balancing centralized policy with decentralized execution.

Abstract: This paper presents CODECO, a federated orchestration framework for Kubernetes that addresses the limitations of cloud-centric deployment. CODECO adopts a data-compute-network co-orchestration approach to support heterogeneous infrastructures, mobility, and multi-provider operation.
  CODECO extends Kubernetes with semantic application models, partition-based federation, and AI-assisted decision support, enabling context-aware placement and adaptive management of applications and their micro-services across federated environments. A hybrid governance model combines centralized policy enforcement with decentralized execution and learning to preserve global coherence while supporting far Edge autonomy. The paper describes the architecture and core components of CODECO, outlines representative orchestration workflows, and introduces a software-based experimentation framework for reproducible evaluation in federated Edge-Cloud infrastructure environments.

</details>


### [594] [Driving Computational Efficiency in Large-Scale Platforms using HPC Technologies](https://arxiv.org/abs/2601.13424)
*Alexander Martinez Mendez,Antonio J. Rubio-Montero,Carlos J. Barrios H.,Hernán Asorey,Rafael Mayo-García,Luis A. Núñez*

Main category: cs.DC

TL;DR: The paper evaluates the efficiency of High-Performance Computing (HPC) resource utilization in the context of the LAGO project, focusing on analyzing past workloads and proposing optimizations to improve computational performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the resource efficiency within the LAGO project’s computational environment to ensure improved scientific productivity and sustainability in astroparticle physics simulations.

Method: The study examines historical job accounting data from EGI FedCloud, identifies workload types, and uses efficiency metrics like CPU utilization, walltime utilization, and I/O patterns to analyze performance.

Result: The analysis identifies workload inefficiencies, observes high CPU efficiency in simulations, and highlights the distorting effects of test jobs. It provides data-driven insights into specific inefficiencies in HPC usage.

Conclusion: The study presents recommendations on resource optimization and workflow strategies to enhance computational throughput and increase scientific output from LAGO's HPC investments.

Abstract: The Latin American Giant Observatory (LAGO) project utilizes extensive High-Performance Computing (HPC) resources for complex astroparticle physics simulations, making resource efficiency critical for scientific productivity and sustainability. This article presents a detailed analysis focused on quantifying and improving HPC resource utilization efficiency specifically within the LAGO computational environment. The core objective is to understand how LAGO's distinct computational workloads-characterized by a prevalent coarse-grained, task-parallel execution model-consume resources in practice. To achieve this, we analyze historical job accounting data from the EGI FedCloud platform, identifying primary workload categories (Monte Carlo simulations, data processing, user analysis/testing) and evaluating their performance using key efficiency metrics (CPU utilization, walltime utilization, and I/O patterns). Our analysis reveals significant patterns, including high CPU efficiency within individual simulation tasks contrasted with the distorting impact of short test jobs on aggregate metrics. This work pinpoints specific inefficiencies and provides data-driven insights into LAGO's HPC usage. The findings directly inform recommendations for optimizing resource requests, refining workflow management strategies, and guiding future efforts to enhance computational throughput, ultimately maximizing the scientific return from LAGO's HPC investments.

</details>


### [595] [RASC: Enhancing Observability & Programmability in Smart Spaces](https://arxiv.org/abs/2601.13496)
*Anna Karanika,Kai-Siang Wang,Han-Ting Liang,Shalni Sundram,Indranil Gupta*

Main category: cs.DC

TL;DR: The paper introduces RASC, an abstraction that enhances IoT observability and programmability by providing critical acknowledgments during device actions.


<details>
  <summary>Details</summary>
Motivation: The paper identifies the need for more expressive abstractions in IoT systems, focusing on enhancing observability and programmability of IoT actions in user-facing smart spaces.

Method: The RASC (Request-Acknowledge-Start-Complete) abstraction is introduced, leveraging existing RPC mechanisms to provide acknowledgments at critical action points. RASC was implemented in the Home Assistant IoT framework and evaluated through trace-driven tests.

Result: RASC achieves better observability, predicts completion times, detects failures faster, supports fine-grained programming dependencies, and improves scheduling policies. Evaluation showed it meets latency requirements for long-duration actions and outperformed leading scheduling counterparts by up to 55%.

Conclusion: RASC significantly improves the IoT action management by enhancing observability and programmability, while also optimizing action scheduling efficiency in smart spaces without replacing existing RPC mechanisms.

Abstract: While RPCs form the bedrock of systems stacks, we posit that IoT device collections in smart spaces like homes, warehouses, and office buildings--which are all "user-facing"--require a more expressive abstraction. Orthogonal to prior work, which improved the reliability of IoT communication, our work focuses on improving the observability and programmability of IoT actions. We present the RASC (Request-Acknowledge-Start-Complete) abstraction, which provides acknowledgments at critical points after an IoT device action is initiated. RASC is a better fit for IoT actions, which naturally vary in length spatially (across devices) and temporally (across time, for a given device). RASC also enables the design of several new features: predicting action completion times accurately, detecting failures of actions faster, allowing fine-grained dependencies in programming, and scheduling. RASC is intended to be implemented atop today's available RPC mechanisms, rather than as a replacement. We integrated RASC into a popular and open-source IoT framework called Home Assistant. Our trace-driven evaluation finds that RASC meets latency SLOs, especially for long actions that last O(mins), which are common in smart spaces. Our scheduling policies for home automations (e.g., routines) outperform state-of-the-art counterparts by 10%-55%.

</details>


### [596] [A Kubernetes custom scheduler based on reinforcement learning for compute-intensive pods](https://arxiv.org/abs/2601.13579)
*Hanlin Zhou,Huah Yong Chan,Shun Yao Zhang,Meie Lin,Jingfei Ni*

Main category: cs.DC

TL;DR: The paper introduces two reinforcement-learning-based schedulers, SDQN and SDQN-n, aimed at optimizing Kubernetes pod placement in compute-intensive scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency of the default Kubernetes scheduler in handling compute-intensive workloads, such as containerized machine-learning tasks, particularly for greener data centers.

Method: Two custom schedulers (SDQN and SDQN-n) were developed based on the Deep Q-Network (DQN) framework for better pod placement in Kubernetes.

Result: The proposed schedulers reduced average CPU utilization per cluster node by 10% (and over 20% with SDQN-n), significantly outperforming the default Kubernetes scheduler and other machine-learning-based schedulers.

Conclusion: SDQN and SDQN-n represent flexible and energy-efficient solutions for pod scheduling in Kubernetes; they can be fine-tuned for diverse computing scenarios to enhance performance and sustainability.

Abstract: With the rise of cloud computing and lightweight containers, Docker has emerged as a leading technology for rapid service deployment, with Kubernetes responsible for pod orchestration. However, for compute-intensive workloads-particularly web services executing containerized machine-learning training-the default Kubernetes scheduler does not always achieve optimal placement. To address this, we propose two custom, reinforcement-learning-based schedulers, SDQN and SDQN-n, both built on the Deep Q-Network (DQN) framework. In compute-intensive scenarios, these models outperform the default Kubernetes scheduler as well as Transformer-and LSTM-based alternatives, reducing average CPU utilization per cluster node by 10%, and by over 20% when using SDQN-n. Moreover, our results show that SDQN-n approach of consolidating pods onto fewer nodes further amplifies resource savings and helps advance greener, more energy-efficient data centers.Therefore, pod scheduling must employ different strategies tailored to each scenario in order to achieve better performance.Since the reinforcement-learning components of the SDQN and SDQN-n architectures proposed in this paper can be easily tuned by adjusting their parameters, they can accommodate the requirements of various future scenarios.

</details>


### [597] [Device Association and Resource Allocation for Hierarchical Split Federated Learning in Space-Air-Ground Integrated Network](https://arxiv.org/abs/2601.13817)
*Haitao Zhao,Xiaoyu Tang,Bo Xu,Jinlong Sun,Linghao Zhang*

Main category: cs.DC

TL;DR: The paper introduces a Hierarchical Split Federated Learning (HSFL) framework to address resource constraints and unbalanced data distribution in Federated Learning within SAGIN.


<details>
  <summary>Details</summary>
Motivation: Improve Federated Learning (FL) in SAGIN by addressing resource constraints and unbalanced data distribution.

Method: Proposes HSFL framework, formulates a joint optimization problem, decomposes it into subproblems, and introduces an iterative optimization algorithm.

Result: Simulation results show the proposed algorithm balances training efficiency and model accuracy in SAGIN.

Conclusion: The HSFL framework is effective in enhancing FL performance and achieving a balance of efficiency and accuracy in SAGIN.

Abstract: 6G facilitates deployment of Federated Learning (FL) in the Space-Air-Ground Integrated Network (SAGIN), yet FL confronts challenges such as resource constrained and unbalanced data distribution. To address these issues, this paper proposes a Hierarchical Split Federated Learning (HSFL) framework and derives its upper bound of loss function. To minimize the weighted sum of training loss and latency, we formulate a joint optimization problem that integrates device association, model split layer selection, and resource allocation. We decompose the original problem into several subproblems, where an iterative optimization algorithm for device association and resource allocation based on brute-force split point search is proposed. Simulation results demonstrate that the proposed algorithm can effectively balance training efficiency and model accuracy for FL in SAGIN.

</details>


### [598] [torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch](https://arxiv.org/abs/2601.13994)
*Mingyuan Chi*

Main category: cs.DC

TL;DR: TorchSLA is a PyTorch library for GPU-accelerated, scalable, and differentiable sparse linear algebra, addressing challenges like GPU acceleration, multi-GPU scaling, and efficient differentiation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance industrial scientific computing by overcoming limitations in GPU acceleration, scaling with sparse matrices, and enabling efficient adjoint-based differentiation.

Method: The library incorporates GPU acceleration for sparse computations, multi-GPU scaling via domain decomposition, and efficient adjoint differentiation. It integrates with PyTorch autograd and offers multiple backend support.

Result: TorchSLA demonstrates scalability with GPU acceleration, achieving 400 million DOF linear solve across 3 GPUs, and implements memory-efficient differentiation while independent of solver iterations.

Conclusion: TorchSLA provides a crucial tool for scalable and differentiable sparse linear algebra, offering integration with PyTorch and advancing computational efficiency in scientific computing.

Abstract: Industrial scientific computing predominantly uses sparse matrices to represent unstructured data -- finite element meshes, graphs, point clouds. We present \torchsla{}, an open-source PyTorch library that enables GPU-accelerated, scalable, and differentiable sparse linear algebra. The library addresses three fundamental challenges: (1) GPU acceleration for sparse linear solves, nonlinear solves (Newton, Picard, Anderson), and eigenvalue computation; (2) Multi-GPU scaling via domain decomposition with halo exchange, reaching \textbf{400 million DOF linear solve on 3 GPUs}; and (3) Adjoint-based differentiation} achieving $\mathcal{O}(1)$ computational graph nodes (for autograd) and $\mathcal{O}(\text{nnz})$ memory -- independent of solver iterations. \torchsla{} supports multiple backends (SciPy, cuDSS, PyTorch-native) and seamlessly integrates with PyTorch autograd for end-to-end differentiable simulations. Code is available at https://github.com/walkerchi/torch-sla.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [599] [CSyMR: Benchmarking Compositional Symbolic Muisc Reasoning With MIR Tool Integration](https://arxiv.org/abs/2601.11556)
*Boyang Wang,Yash Vishe,Xin Xu,Zachary Novack,Julian McAuley,Junda Wu*

Main category: cs.LG

TL;DR: The paper introduces CSyMR-Bench, a benchmark for compositional reasoning in symbolic music, along with a tool-augmented framework that improves reasoning accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for symbolic music reasoning focus on isolated or atomic analyses rather than compositional reasoning, which is essential for connecting musical structures.

Method: The authors curated CSyMR-Bench, a dataset of 126 multiple-choice questions requiring compositional reasoning, and proposed a tool-augmented agent framework using the music21 library for better analysis.

Result: Experiments show CSyMR-Bench challenges symbolic music reasoning, and the tool-augmented framework improves accuracy by 5-7% compared to baselines.

Conclusion: The combination of CSyMR-Bench and the proposed framework advances symbolic music reasoning research and highlights a need for integrative tools in the field.

Abstract: Large Language Models (LLMs) are leveraged in symbolic music reasoning, yet existing benchmarks emphasize isolated knowledge or atomic analyses rather than the integrative compositional reasoning needed to connect musical structures. To address this, we present the Compositional Symbolic Music Reasoning Benchmark (CSyMR-Bench), a curated multiple-choice dataset of 126 questions derived from expert forums and professional examinations. Each item involves combining several atomic analyses to arrive at the final answer. Furthermore, we introduce a tool-augmented agent framework that leverages symbolic music analysis tools from the music21 library to address the challenges posed by CSyMR-Bench. Experiments validate that CSyMR-Bench poses a non-trivial challenge across both community-sourced and exam-style questions, while our tool-augmented agent consistently outperforms all baselines, achieving 5-7% absolute accuracy gains.

</details>


### [600] [AdaFRUGAL: Adaptive Memory-Efficient Training with Dynamic Control](https://arxiv.org/abs/2601.11568)
*Quang-Hung Bui,Anh Son Ta*

Main category: cs.LG

TL;DR: The paper introduces AdaFRUGAL, a method to optimize training of Large Language Models (LLMs) by dynamically managing memory and computational resources while achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: To address the high memory cost and manual tuning limitations in training LLMs caused by static hyperparameters in the FRUGAL framework.

Method: AdaFRUGAL introduces two dynamic controls: a linear decay for subspace ratio ($ρ$) to decrease memory usage and a loss-aware schedule for update frequency ($T$) to reduce computation overhead.

Result: AdaFRUGAL achieves competitive performance comparable to state-of-the-art methods while significantly lowering GPU memory usage and training time in experiments across pre-training and fine-tuning tasks.

Conclusion: AdaFRUGAL provides an automated, efficient solution for LLM training, eliminating the need for manual tuning and offering better adaptability in resource-constrained environments.

Abstract: Training Large Language Models (LLMs) is highly memory-intensive due to optimizer state overhead. The FRUGAL framework mitigates this with gradient splitting, but its static hyperparameters -- the subspace ratio ($ρ$) and update frequency ($T$) -- require costly manual tuning, limiting adaptability. We present AdaFRUGAL, which automates this process by introducing two dynamic controls: (i) a linear decay for $ρ$ to progressively reduce memory, and (ii) a loss-aware schedule for $T$ to lower computational overhead. Experiments across large-scale pre-training (English C4, Vietnamese VietVault) and fine-tuning (GLUE) demonstrate that AdaFRUGAL achieves a compelling trade-off. It maintains competitive performance against AdamW and static FRUGAL while significantly reducing both GPU memory and training time, offering a more practical, autonomous solution for resource-constrained LLM training.

</details>


### [601] [Discrete Semantic States and Hamiltonian Dynamics in LLM Embedding Spaces](https://arxiv.org/abs/2601.11572)
*Timo Aukusti Laine*

Main category: cs.LG

TL;DR: The paper uses mathematical tools inspired by quantum mechanics to analyze the structured embedding spaces of Large Language Models (LLMs), focusing on cosine similarity, semantic transitions, and connections to theoretical physics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the structured nature of the embedding spaces in LLMs observed as distinct semantic states, and to explore whether tools from linear algebra and quantum mechanics can provide insights into their semantic relationships.

Method: The authors employ mathematical concepts such as linear algebra and Hamiltonian formalism to analyze the embedding spaces of LLMs, considering the L2 normalization constraint and deriving analogies with physics concepts like zero-point energy.

Result: The study establishes relationships between cosine similarity and embedding vector perturbations, identifies semantic transition modes, and draws comparisons to Koopman-von Neumann mechanics in a quantum mechanical analogy.

Conclusion: This mathematical framework offers promising theoretical insights into LLMs, offering potential ways to better understand their semantic behavior and reduce hallucinations.

Abstract: We investigate the structure of Large Language Model (LLM) embedding spaces using mathematical concepts, particularly linear algebra and the Hamiltonian formalism, drawing inspiration from analogies with quantum mechanical systems. Motivated by the observation that LLM embeddings exhibit distinct states, suggesting discrete semantic representations, we explore the application of these mathematical tools to analyze semantic relationships. We demonstrate that the L2 normalization constraint, a characteristic of many LLM architectures, results in a structured embedding space suitable for analysis using a Hamiltonian formalism. We derive relationships between cosine similarity and perturbations of embedding vectors, and explore direct and indirect semantic transitions. Furthermore, we explore a quantum-inspired perspective, deriving an analogue of zero-point energy and discussing potential connections to Koopman-von Neumann mechanics. While the interpretation warrants careful consideration, our results suggest that this approach offers a promising avenue for gaining deeper insights into LLMs and potentially informing new methods for mitigating hallucinations.

</details>


### [602] [GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment](https://arxiv.org/abs/2601.11574)
*Lukas Abrie Nel*

Main category: cs.LG

TL;DR: GRADE is an approach to align large language models (LLMs) more effectively by using Gumbel-Softmax relaxation instead of traditional policy gradient methods like PPO or REINFORCE, offering reduced gradient variance and improved computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the high computational costs and variance associated with policy gradient methods like PPO in aligning LLMs with human preferences during reinforcement learning.

Method: GRADE uses a Gumbel-Softmax relaxation with straight-through estimation (GRADE-STE) to permit end-to-end gradient computation during token generation. This avoids the need for high-variance gradient estimation methods like policy gradient.

Result: In experiments on sentiment-controlled text generation with the IMDB dataset, GRADE-STE achieved a 50% improvement in test reward compared to PPO and significantly lower gradient variance compared to REINFORCE while demonstrating stable training dynamics.

Conclusion: GRADE presents a more efficient and robust alternative to traditional RL methods like PPO for optimizing LLMs, with better generalization and computational stability.

Abstract: Reinforcement learning from human feedback (RLHF) has become the dominant paradigm for aligning large language models with human preferences. However, policy gradient methods such as PPO suffer from high variance gradient estimates, requiring careful hyperparameter tuning and extensive computational resources. We introduce GRADE (Gumbel-softmax Relaxation for Alignment via Differentiable Estimation), a method that replaces high-variance policy gradient estimation with direct backpropagation through a differentiable relaxation of the discrete token sampling process. Using the Gumbel-Softmax reparameterization with straight-through estimation (GRADE-STE), we enable end-to-end gradient flow from reward signals through generated tokens to model parameters. On sentiment-controlled text generation using the IMDB dataset, GRADE-STE achieves a test reward of 0.763 +- 0.344 compared to PPO's 0.510 +- 0.313 and REINFORCE's 0.617 +- 0.378, representing a 50% relative improvement over PPO. Critically, GRADE-STE exhibits gradient variance over 14 times lower than REINFORCE and maintains stable training dynamics throughout optimization. Our rigorous evaluation with proper train/validation/test splits demonstrates that these improvements generalize to held-out data, with GRADE-STE showing the best generalization characteristics among all methods tested. GRADE offers a simpler, more stable, and more effective alternative to reinforcement learning for LLM alignment.

</details>


### [603] [Hindsight Preference Replay Improves Preference-Conditioned Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2601.11604)
*Jonaid Shianifar,Michael Schukat,Karl Mason*

Main category: cs.LG

TL;DR: This paper introduces Hindsight Preference Replay (HPR), an augmentation for MORL, enhancing the CAPQL method by relabeling data for improved performance across multiple environments.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in using off-policy data from other preferences within traditional CAPQL methods, aiming to maximize reward outcomes while adhering to user preferences.

Method: The HPR strategy retroactively relabels stored transitions with alternative user preferences, improving the training process without altering the existing CAPQL architecture or loss functions.

Result: HPR-CAPQL demonstrates improved performance (hypervolume and expected utility) in five out of six MO-Gym tasks, with substantial gains in environments like mo-humanoid-v5 and challenges in mo-halfcheetah-v5.

Conclusion: The addition of HPR to CAPQL offers a significant advancement in MORL, improving performance metrics and offering visual insights via Pareto fronts across various tasks, though some weaknesses remain in specific scenarios.

Abstract: Multi-objective reinforcement learning (MORL) enables agents to optimize vector-valued rewards while respecting user preferences. CAPQL, a preference-conditioned actor-critic method, achieves this by conditioning on weight vectors w and restricts data usage to the specific preferences under which it was collected, leaving off-policy data from other preferences unused. We introduce Hindsight Preference Replay (HPR), a simple and general replay augmentation strategy that retroactively relabels stored transitions with alternative preferences. This densifies supervision across the preference simplex without altering the CAPQL architecture or loss functions. Evaluated on six MO-Gymnasium locomotion tasks at a fixed 300000-step budget using expected utility (EUM), hypervolume (HV), and sparsity, HPR-CAPQL improves HV in five of six environments and EUM in four of six. On mo-humanoid-v5, for instance, EUM rises from $323\!\pm\!125$ to $1613\!\pm\!464$ and HV from 0.52M to 9.63M, with strong statistical support. mo-halfcheetah-v5 remains a challenging exception where CAPQL attains higher HV at comparable EUM. We report final summaries and Pareto-front visualizations across all tasks.

</details>


### [604] [A Multimodal Data Processing Pipeline for MIMIC-IV Dataset](https://arxiv.org/abs/2601.11606)
*Farzana Islam Adiba,Varsha Danduri,Fahmida Liza Piya,Ali Abbasi,Mehak Gupta,Rahmatollah Beheshti*

Main category: cs.LG

TL;DR: The paper introduces an advanced pipeline for the MIMIC-IV dataset, enabling easier and faster integration of multimodal EHR data for diverse downstream applications.


<details>
  <summary>Details</summary>
Motivation: The need to streamline and standardize multimodal data preprocessing and alignment in MIMIC-IV for clinical machine learning studies.

Method: An enhanced multimodal pipeline that automates cohort selection, temporal alignment, and integrates structured formats for diverse static and time-series applications.

Result: Significant reduction in multimodal data processing time, improved reproducibility, and availability of a Python package, UI, and code on GitHub.

Conclusion: The pipeline facilitates efficient and reproducible use of MIMIC-IV's multimodal data, greatly supporting various research applications.

Abstract: The MIMIC-IV dataset is a large, publicly available electronic health record (EHR) resource widely used for clinical machine learning research. It comprises multiple modalities, including structured data, clinical notes, waveforms, and imaging data. Working with these disjointed modalities requires an extensive manual effort to preprocess and align them for downstream analysis. While several pipelines for MIMIC-IV data extraction are available, they target a small subset of modalities or do not fully support arbitrary downstream applications. In this work, we greatly expand our prior popular unimodal pipeline and present a comprehensive and customizable multimodal pipeline that can significantly reduce multimodal processing time and enhance the reproducibility of MIMIC-based studies. Our pipeline systematically integrates the listed modalities, enabling automated cohort selection, temporal alignment across modalities, and standardized multimodal output formats suitable for arbitrary static and time-series downstream applications. We release the code, a simple UI, and a Python package for selective integration (with embedding) at https://github.com/healthylaife/MIMIC-IV-Data-Pipeline.

</details>


### [605] [Auxiliary-predicted Compress Memory Model(ApCM Model): A Neural Memory Storage Model Based on Invertible Compression and Learnable Prediction](https://arxiv.org/abs/2601.11609)
*Weinuo Ou*

Main category: cs.LG

TL;DR: The paper introduces the ApCM Model, a neural memory architecture to improve runtime memory in LLMs.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) struggle with dynamic and personalized interactions due to inadequate runtime memory mechanisms.

Method: The authors propose a new neural memory storage system called the Auxiliary Prediction Compression Memory Model (ApCM Model).

Result: The ApCM Model efficiently enhances the adaptability and personalization capabilities of LLMs during runtime.

Conclusion: Implementing the ApCM Model could address memory limitations in LLMs, leading to more dynamic and personalized applications.

Abstract: Current large language models (LLMs) generally lack an effective runtime memory mechanism,making it difficult to adapt to dynamic and personalized interaction requirements. To address this issue, this paper proposes a novel neural memory storage architecture--the Auxiliary Prediction Compression Memory Model (ApCM Model).

</details>


### [606] [Integrating Temporal Context into Streaming Data for Human Activity Recognition in Smart Home](https://arxiv.org/abs/2601.11611)
*Marina Vicini,Martin Rudorfer,Zhuangzhuang Dai,Luis J. Manso*

Main category: cs.LG

TL;DR: This paper improves Human Activity Recognition (HAR) in smart homes by clustering activities by time of day, adding temporal features, and outperforming state-of-the-art methods in multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance independent living and safety for the ageing population using smart home monitoring systems with passive sensors while addressing the challenges of leveraging temporal information in HAR.

Method: Activities are clustered by time periods (morning, afternoon, and night) with distinct mutual information calculations and features such as time of day, day of week (encoded cyclically), and user location are added to the feature vector.

Result: The approach outperforms state-of-the-art methods in three out of four real-world datasets, particularly in low-data situations, with improved accuracy and F1-scores.

Conclusion: The proposed method demonstrates potential for creating effective smart home solutions, enhancing elderly care and preventative healthcare for ageing in place.

Abstract: With the global population ageing, it is crucial to enable individuals to live independently and safely in their homes. Using ubiquitous sensors such as Passive InfraRed sensors (PIR) and door sensors is drawing increasing interest for monitoring daily activities and facilitating preventative healthcare interventions for the elderly. Human Activity Recognition (HAR) from passive sensors mostly relies on traditional machine learning and includes data segmentation, feature extraction, and classification. While techniques like Sensor Weighting Mutual Information (SWMI) capture spatial context in a feature vector, effectively leveraging temporal information remains a challenge. We tackle this by clustering activities into morning, afternoon, and night, and encoding them into the feature weighting method calculating distinct mutual information matrices. We further propose to extend the feature vector by incorporating time of day and day of week as cyclical temporal features, as well as adding a feature to track the user's location. The experiments show improved accuracy and F1-score over existing state-of-the-art methods in three out of four real-world datasets, with highest gains in a low-data regime. These results highlight the potential of our approach for developing effective smart home solutions to support ageing in place.

</details>


### [607] [A Review on Machine Learning Approaches for the Prediction of Glucose Levels and Hypogylcemia](https://arxiv.org/abs/2601.11615)
*Beyza Cinar,Louisa van den Boom,Maria Maleshkova*

Main category: cs.LG

TL;DR: The paper reviews ML models for predicting hypoglycemia in T1D patients using CGM data, comparing performance based on prediction horizons, model types, and personalization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve diabetes management by leveraging ML models for accurate hypoglycemia prediction, addressing its life-threatening implications.

Method: The paper investigates, compares, and evaluates ML models trained on CGM data across different prediction horizons, analyzing performance factors including personalization and data quality.

Result: Short-term prediction horizons (up to 1 hour) yield better performance. Conventional ML excels in classification tasks, while DL is superior in regression tasks. Multi-variable datasets and sequence lengths impact model accuracy. Personalized models enhance performance but population-based models are currently preferred due to data limitations.

Conclusion: Using ML for hypoglycemia prediction can optimize diabetes management. However, challenges include balancing individual vs population-based models and improving data quality for better outcomes.

Abstract: Type 1 Diabetes (T1D) is an autoimmune disease leading to insulin insufficiency. Thus, patients require lifelong insulin therapy, which has a side effect of hypoglycemia. Hypoglycemia is a critical state of decreased blood glucose levels (BGL) below 70 mg/dL and is associated with increased risk of mortality. Machine learning (ML) models can improve diabetes management by predicting hypoglycemia and providing optimal prevention methods. ML models are classified into regression and classification based, that forecast glucose levels and identify events based on defined labels, respectively. This review investigates state-of-the-art models trained on data of continuous glucose monitoring (CGM) devices from patients with T1D. We compare the models' performance across short-term (15 to 120 min) and long term (3 to more than 24 hours) prediction horizons (PHs). Particularly, we explore: 1) How much in advance can glucose values or a hypoglycemic event be accurately predicted? 2) Which models have the best performance? 3) Which factors impact the performance? and 4) Does personalization increase performance? The results show that 1) a PH of up to 1 hour provides the best results. 2) Conventional ML methods yield the best results for classification and DL for regression. A single model cannot adequately classify across multiple PHs. 3) The model performance is influenced by multivariate datasets and the input sequence length (ISL). 4) Personal data enhances performance but due to limited data quality population-based models are preferred.

</details>


### [608] [Mixture-of-Experts as Soft Clustering: A Dual Jacobian-PCA Spectral Geometry Perspective](https://arxiv.org/abs/2601.11616)
*Feilong Liu*

Main category: cs.LG

TL;DR: The paper investigates the geometric implications of Mixture-of-Experts (MoE) architectures, focusing on how routing partitions the representation space and its effect on function and representation geometry. It introduces a spectral geometry probe to systematically evaluate these properties.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric implications of MoE routing, particularly its impact on the function and representation space, which remains underexplored despite MoE's efficiency benefits.

Method: The authors develop a Dual Jacobian-PCA Spectral Geometry probe to study MoE architectures. By using a controlled MLP-MoE setting, they analyze function geometry via Jacobian singular-value spectra and representation geometry through weighted PCA.

Result: The study shows that MoE routing decreases local sensitivity, reduces leading singular values, flattens spectral decay, and distributes representation variance across more directions. They also find near-orthogonality of expert Jacobians, indicating unique, non-overlapping subspaces for each expert.

Conclusion: MoE architectures geometrically partition the function space, flatten local function curvature, and enhance representation diversity, with notable differences between Top-k and fully-soft routing approaches.

Abstract: Mixture-of-Experts (MoE) architectures are commonly motivated by efficiency and conditional computation, but their effect on the geometry of learned functions and representations remains poorly characterized. In this work, we study MoEs through a geometric lens, interpreting routing as a form of soft partitioning of the representation space into overlapping local charts. We introduce a Dual Jacobian-PCA Spectral Geometry probe. It analyzes local function geometry via Jacobian singular-value spectra and representation geometry via weighted PCA of routed hidden states. Using a controlled MLP-MoE setting that permits exact Jacobian computation, we compare dense, Top-k, and fully-soft routing architectures under matched capacity. Across random seeds, we observe that MoE routing consistently reduces local sensitivity, with expert-local Jacobians exhibiting smaller leading singular values and faster spectral decay than dense baselines. At the same time, weighted PCA reveals that expert-local representations distribute variance across a larger number of principal directions, indicating higher effective rank under identical input distributions. We further find that average expert Jacobians are nearly orthogonal, suggesting a decomposition of the transformation into low-overlap expert-specific subspaces rather than scaled variants of a shared map. We analyze how routing sharpness modulates these effects, showing that Top-k routing produces lower-rank, more concentrated expert-local structure, while fully-soft routing yields broader, higher-rank representations. Together, these results support a geometric interpretation of MoEs as soft partitionings of function space that flatten local curvature while redistributing representation variance.

</details>


### [609] [Geometric Attention: A Regime-Explicit Operator Semantics for Transformer Attention](https://arxiv.org/abs/2601.11618)
*Luis Rosario Freytes*

Main category: cs.LG

TL;DR: The paper introduces Geometric Attention (GA), a generalized framework for attention mechanisms. It formalizes components such as carriers, evidence-kernels, probes, and anchors to unify and extend various attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: To systematize attention mechanisms by separating invariant structures from modeling choices, enabling better comparisons, extensions, and principled designs of attention-based architectures.

Method: The authors define GA with distinct components—carrier, evidence-kernels, probe family, and anchor/update rule—under a scalar-relational and multiplicative evidence framework, producing Gibbs weights and diverse advanced attention operations.

Result: GA generalizes and extends attention mechanisms, including standard Transformer attention, adaptive carriers, multihead kernels, and more, through a rigorous operational and compositional representation.

Conclusion: This framework facilitates meaningful theoretical analyses, principled extensions, and deliberate design choices for attention mechanisms, bringing clarity to the underlying structures and their usage in neural architectures.

Abstract: Geometric Attention (GA) specifies an attention layer by four independent inputs: a finite carrier (what indices are addressable), an evidence-kernel rule (how masked proto-scores and a link induce nonnegative weights), a probe family (which observables are treated as admissible), and an anchor/update rule (which representative kernel is selected and how it is applied). Probe families induce an operational equivalence relation on kernels and therefore a gauge; anchors select representatives relative to that probe. Under a scalar relational-work representation and a multiplicative compositionality law for evidence, the admissible link family is exponential, yielding Gibbs weights; with row anchoring this includes the softmax kernel family as a subregime. After quotienting unary row/column score fields, the remaining interaction component admits a canonical rank-r normal form (Eckart-Young/SVD); dot-product score charts implement the corresponding low-rank interaction regime. Fixing the carrier and extensionalizing the update yields the standard fixed-token Transformer attention operator; allowing carrier updates yields adaptive-carrier and staged-depth regimes. The operator language also supports multihead/mixed kernels, plan-based anchors (e.g., entropic OT/Sinkhorn), and unary operators (e.g., FFN-style fields) as explicit regime choices. This separates invariant structure from modeling choice, enabling principled comparison and extension of attention mechanisms, and attention-based architectures.

</details>


### [610] [Universal Approximation Theorem for Input-Connected Multilayer Perceptrons](https://arxiv.org/abs/2601.14026)
*Vugar Ismailov*

Main category: cs.LG

TL;DR: The paper proposes the Input-Connected Multilayer Perceptron (IC-MLP), a neural network directly connecting inputs to all hidden neurons, and investigates its properties, proving its universal approximation capabilities for continuous functions.


<details>
  <summary>Details</summary>
Motivation: To enhance the representational capabilities of neural networks by connecting input data directly to hidden layers and exploring their approximating power for continuous functions.

Method: The authors introduce and analyze the IC-MLP architecture through mathematical formulations and proofs, demonstrating its ability to approximate functions using nonlinear activation functions.

Result: IC-MLPs are shown to possess universal approximation capabilities for continuous functions on closed intervals (univariate) and on compact subsets of $\mathbb{R}^n$ (vector inputs).

Conclusion: IC-MLPs provide a theoretical framework for designing feedforward neural networks with enhanced connections, enabling them to approximate a broader range of functions when using nonlinear activations.

Abstract: We introduce the Input-Connected Multilayer Perceptron (IC-MLP), a feedforward neural network architecture in which each hidden neuron receives, in addition to the outputs of the preceding layer, a direct affine connection from the raw input. We first study this architecture in the univariate setting and give an explicit and systematic description of IC-MLPs with an arbitrary finite number of hidden layers, including iterated formulas for the network functions. In this setting, we prove a universal approximation theorem showing that deep IC-MLPs can approximate any continuous function on a closed interval of the real line if and only if the activation function is nonlinear. We then extend the analysis to vector-valued inputs and establish a corresponding universal approximation theorem for continuous functions on compact subsets of $\mathbb{R}^n$.

</details>


### [611] [NoiseFormer -- Noise Diffused Symmetric Attention Transformer](https://arxiv.org/abs/2601.11619)
*Phani Kumar,Nyshadham,Jyothendra Varma,Polisetty V R K,Aditya Rathore*

Main category: cs.LG

TL;DR: The paper introduces a modified transformer architecture using Symmetric Attention and Noise Diffusion to achieve better accuracy and model size reduction.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models struggle with memory and computational demands as they grow larger, making them expensive to train/infer, which creates a need for efficient size-reduction techniques.

Method: The paper focuses on Symmetric Dot-Product Attention and introduces a novel architecture called Noise Diffused Symmetric Attention Transformer, balancing memory efficiency and performance accuracy with marginal computational overhead.

Result: The proposed model shows performance improvements in GLUE benchmarks compared to plain Symmetric Attention and reduces model size compared to the GPT-2 base model.

Conclusion: The new model successfully enhances sampling accuracy and efficiency while maintaining memory savings, demonstrating a practical alternative to existing transformer architectures.

Abstract: Transformer architecture has been very successful long runner in the field of Deep Learning (DL) and Large Language Models (LLM) because of its powerful attention-based learning and parallel-natured architecture. As the models grow gigantic in terms of memory footprint, difficulties in fitting the model on a device like a GPU or an AI accelerator give rise to the need for multiple computing devices thereby escalating the computing cost. This increased training/inference cost paved the way for efficient model size reduction/parametric reduction deploying Sparse Attention techniques. In this paper, we start analyzing one of the techniques of Sparse Attention called Symmetric Dot-Product Attention (referred to as Symmetric Attention) and propose a novel unified model architecture called Noise Diffused Symmetric Attention Transformer to enhance the model's performance. While maintaining the memory gains of Symmetric Attention, with minute overhead in terms of model parameters and computational overhead, the proposed model brings in enhanced performance in terms of accuracy and inference-time sampling. The proposed model is validated upon GPT2 base model and the results reflect the performance gains falling between plain Symmetric attention and GPT2 base model on a variety of GLUE benchmark tasks in terms of accuracy, with significant model size reduction with respect to the base model.

</details>


### [612] [Verifying Physics-Informed Neural Network Fidelity using Classical Fisher Information from Differentiable Dynamical System](https://arxiv.org/abs/2601.11638)
*Josafat Ribeiro Leal Filho,Antônio Augusto Fröhlich*

Main category: cs.LG

TL;DR: The paper introduces an experimental framework using Fisher information to assess how well Physics-Informed Neural Networks (PINNs) capture the complete dynamics of physical systems.


<details>
  <summary>Details</summary>
Motivation: PINNs are useful for solving differential equations and modeling physical systems, but it is challenging to evaluate their ability to encapsulate the complete dynamical properties beyond trajectory prediction.

Method: The paper uses Fisher information ($g_F^C$) to measure deterministic system uncertainties and determines fidelity by comparing this information derived from the PINN's learned equations to the original analytical model.

Result: PINNs and analytical models are quantitatively compared for fidelity using Fisher information, focusing on their Jacobians and dynamical characteristics.

Conclusion: The framework provides a rigorous experimental methodology to evaluate PINNs' ability to learn underlying system geometries and stability properties.

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations and modeling physical systems by embedding physical laws into the learning process. However, rigorously quantifying how well a PINN captures the complete dynamical behavior of the system, beyond simple trajectory prediction, remains a challenge. This paper proposes a novel experimental framework to address this by employing Fisher information for differentiable dynamical systems, denoted $g_F^C$. This Fisher information, distinct from its statistical counterpart, measures inherent uncertainties in deterministic systems, such as sensitivity to initial conditions, and is related to the phase space curvature and the net stretching action of the state space evolution. We hypothesize that if a PINN accurately learns the underlying dynamics of a physical system, then the Fisher information landscape derived from the PINN's learned equations of motion will closely match that of the original analytical model. This match would signify that the PINN has achieved comprehensive fidelity capturing not only the state evolution but also crucial geometric and stability properties. We outline an experimental methodology using the dynamical model of a car to compute and compare $g_F^C$ for both the analytical model and a trained PINN. The comparison, based on the Jacobians of the respective system dynamics, provides a quantitative measure of the PINN's fidelity in representing the system's intricate dynamical characteristics.

</details>


### [613] [Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution via Invertibility](https://arxiv.org/abs/2601.13398)
*Nickil Maveli,Antonio Vergari,Shay B. Cohen*

Main category: cs.LG

TL;DR: The paper highlights the shortcomings of LLMs in maintaining reasoning consistency in round-trip code execution using a benchmark called RTCE.


<details>
  <summary>Details</summary>
Motivation: Current LLMs excel in code benchmarks but fail in maintaining reasoning consistency during bi-directional code execution.

Method: Introduced RTCE benchmark consisting of four code execution reasoning tasks to test round-trip consistency, along with evaluation using zero-shot prompting, supervised fine-tuning, and self-reflection.

Result: Findings reveal that existing Code-LLMs show slight improvements across methods but still fail to achieve complete round-trip reasoning coherence.

Conclusion: LLMs lack the internal coherence required for trustworthy code reasoning, with RTCE uncovering valuable insights not measured by other benchmarks.

Abstract: LLMs demonstrate strong performance on code benchmarks, yet round-trip code execution reveals limitations in their ability to maintain consistent reasoning across forward and backward execution. We present RoundTripCodeEval (RTCE), a comprehensive benchmark consisting of four distinct code execution reasoning tasks designed to rigorously test round-trip consistency. RTCE provides an execution-free, exact-match evaluation of bijection fidelity, assessing whether models preserve a consistent one-to-one mapping between encoding and decoding operations across various algorithms and directions. We systematically evaluate state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms. Each yields modest improvements, but none closes the gap, indicating that current LLMs struggle with true round-trip consistency, which demonstrates that they lack the internal coherence required for trustworthy code reasoning. RTCE surfaces several new and previously unmeasured insights that are not captured by existing I/O-prediction, execution-reasoning, or round-trip natural-language benchmarks. We will release the code and the dataset upon acceptance.

</details>


### [614] [Global Optimization By Gradient from Hierarchical Score-Matching Spaces](https://arxiv.org/abs/2601.11639)
*Ming Li*

Main category: cs.LG

TL;DR: The paper proposes a novel approach for optimizing all types of problems, overcoming traditional gradient descent's limitations.


<details>
  <summary>Details</summary>
Motivation: Existing gradient descent techniques are constrained to local optima, simple convex constraints, and differentiable problems, underscoring the need for a more comprehensive optimization method.

Method: The authors unify optimization problems as a hierarchical optimization objective without constraints, optimized by gradient obtained through score matching.

Result: The method enabled global optimization using deterministic strict gradients for the first time, proven through various experiments.

Conclusion: This approach achieves global optimization and connects it with diffusion-based generative modeling.

Abstract: Gradient descent is the most commonly used optimization method, but limited to local optimality, and confined to the field of continuous differentiable problems with simple convex constraints. This work solve these limitations and restrictions by unifying all optimization problems with various complex constraints as a general hierarchical optimization objective without constraints, which is optimized by gradient obtained through score matching. By this way, global optimization by deterministic method using strict gradient is achieved for the first time, and verified through simple-constructed and complex-practical experiments. Even more importantly, it reveals the profound connection between global optimization and diffusion based generative modeling.

</details>


### [615] [Size is Not the Solution: Deformable Convolutions for Effective Physics Aware Deep Learning](https://arxiv.org/abs/2601.11657)
*Jack T. Beerman,Shobhan Roy,H. S. Udaykumar,Stephen S. Baek*

Main category: cs.LG

TL;DR: D-PARC, a deformable recurrent convolution architecture, improves predictions of complex nonlinear physical flows compared to larger models, leveraging physics-based principles rather than scaling networks.


<details>
  <summary>Details</summary>
Motivation: Conventional CNN architectures face challenges with highly nonlinear flows despite their usage in physics-aware deep learning. Increasing model size provides limited gains, necessitating novel approaches for accurate physics modeling.

Method: The authors developed D-PARC (Deformable Physics-Aware Recurrent Convolutions) by incorporating inspiration from Hybrid Lagrangian-Eulerian numerical methods to address CNN rigidity and adaptively concentrate resources in critical areas.

Result: D-PARC demonstrated superior performance over larger CNN architectures on test cases like Burgers' equation, Navier-Stokes, and reactive flows. Analysis showed the architecture employed an "active filtration" strategy and a physics-inspired adaptive focus mechanism.

Conclusion: Strategic architectural design rooted in physical principles, like D-PARC, is more effective for advancing PADL than indiscriminately increasing network size, highlighting the power of lean and adaptive models.

Abstract: Physics-aware deep learning (PADL) enables rapid prediction of complex physical systems, yet current convolutional neural network (CNN) architectures struggle with highly nonlinear flows. While scaling model size addresses complexity in broader AI, this approach yields diminishing returns for physics modeling. Drawing inspiration from Hybrid Lagrangian-Eulerian (HLE) numerical methods, we introduce deformable physics-aware recurrent convolutions (D-PARC) to overcome the rigidity of CNNs. Across Burgers' equation, Navier-Stokes, and reactive flows, D-PARC achieves superior fidelity compared to substantially larger architectures. Analysis reveals that kernels display anti-clustering behavior, evolving into a learned "active filtration" strategy distinct from traditional h- or p-adaptivity. Effective receptive field analysis confirms that D-PARC autonomously concentrates resources in high-strain regions while coarsening focus elsewhere, mirroring adaptive refinement in computational mechanics. This demonstrates that physically intuitive architectural design can outperform parameter scaling, establishing that strategic learning in lean networks offers a more effective path forward for PADL than indiscriminate network expansion.

</details>


### [616] [Differentiable Logic Synthesis: Spectral Coefficient Selection via Sinkhorn-Constrained Composition](https://arxiv.org/abs/2601.13953)
*Gorgi Pavlov*

Main category: cs.LG

TL;DR: This paper introduces a novel hierarchical spectral composition architecture designed to address the challenge of learning precise Boolean logic through gradient descent, showing advancements in logic synthesis through various methods.


<details>
  <summary>Details</summary>
Motivation: Current challenges in Boolean logic learning via gradient descent include networks converging to fuzzy approximations that degrade under quantization. To address this, the paper introduces novel methods to achieve precise Boolean computation.

Method: The proposed method employs a differentiable architecture using spectral coefficients from a Boolean Fourier basis composed with Sinkhorn-constrained routing and column-sign modulation. It adapts Manifold-Constrained Hyper-Connections for logic synthesis tasks.

Result: The method demonstrated increasing accuracy and success across various test phases, achieving 100% accuracy for smaller problems (n=2) and using additional techniques like MCMC refinement for higher-dimensional tasks (n=3, n=4). It also enabled efficient, single-cycle logic inference on GPU.

Conclusion: Boolean functions can be accurately represented with ternary polynomial threshold representations, but require methods beyond pure gradient descent for higher complexity problems. The approach combines theoretical insight and hardware-efficient implementation, advancing neuro-symbolic logic synthesis.

Abstract: Learning precise Boolean logic via gradient descent remains challenging: neural networks typically converge to "fuzzy" approximations that degrade under quantization. We introduce Hierarchical Spectral Composition, a differentiable architecture that selects spectral coefficients from a frozen Boolean Fourier basis and composes them via Sinkhorn-constrained routing with column-sign modulation. Our approach draws on recent insights from Manifold-Constrained Hyper-Connections (mHC), which demonstrated that projecting routing matrices onto the Birkhoff polytope preserves identity mappings and stabilizes large-scale training. We adapt this framework to logic synthesis, adding column-sign modulation to enable Boolean negation -- a capability absent in standard doubly stochastic routing.
  We validate our approach across four phases of increasing complexity: (1) For n=2 (16 Boolean operations over 4-dim basis), gradient descent achieves 100% accuracy with zero routing drift and zero-loss quantization to ternary masks. (2) For n=3 (10 three-variable operations), gradient descent achieves 76% accuracy, but exhaustive enumeration over 3^8 = 6561 configurations proves that optimal ternary masks exist for all operations (100% accuracy, 39% sparsity). (3) For n=4 (10 four-variable operations over 16-dim basis), spectral synthesis -- combining exact Walsh-Hadamard coefficients, ternary quantization, and MCMC refinement with parallel tempering -- achieves 100% accuracy on all operations. This progression establishes (a) that ternary polynomial threshold representations exist for all tested functions, and (b) that finding them requires methods beyond pure gradient descent as dimensionality grows. All operations enable single-cycle combinational logic inference at 10,959 MOps/s on GPU, demonstrating viability for hardware-efficient neuro-symbolic logic synthesis.

</details>


### [617] [Task-tailored Pre-processing: Fair Downstream Supervised Learning](https://arxiv.org/abs/2601.11897)
*Jinwon Sohn,Guang Lin,Qifan Song*

Main category: cs.LG

TL;DR: The paper presents a novel fairness-aware pre-processing approach for supervised machine learning tasks, balancing fairness and utility, and guarantees downstream model performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the shortcomings of existing data fairness methods that impose overly strong regularization without fully considering the supervised learning tasks, to ensure both fairness and utility in downstream applications.

Method: The authors propose a novel pre-processing framework tailored to supervised learning tasks, balancing fairness and utility. They theoretically analyze the guarantees for fairness and utility preservation of downstream models trained on transformed data.

Result: The framework demonstrates superior performance in maintaining trade-offs among fairness and utility across multiple downstream models when tested on tabular and image datasets. For computer vision tasks, the method only alters necessary semantic features to achieve fairness.

Conclusion: This work advances the field of fairness-aware machine learning by proposing a method that not only improves fairness but also maintains utility. It also provides theoretical insights into downstream model performance and practical superiority over previous models.

Abstract: Fairness-aware machine learning has recently attracted various communities to mitigate discrimination against certain societal groups in data-driven tasks. For fair supervised learning, particularly in pre-processing, there have been two main categories: data fairness and task-tailored fairness. The former directly finds an intermediate distribution among the groups, independent of the type of the downstream model, so a learned downstream classification/regression model returns similar predictive scores to individuals inputting the same covariates irrespective of their sensitive attributes. The latter explicitly takes the supervised learning task into account when constructing the pre-processing map. In this work, we study algorithmic fairness for supervised learning and argue that the data fairness approaches impose overly strong regularization from the perspective of the HGR correlation. This motivates us to devise a novel pre-processing approach tailored to supervised learning. We account for the trade-off between fairness and utility in obtaining the pre-processing map. Then we study the behavior of arbitrary downstream supervised models learned on the transformed data to find sufficient conditions to guarantee their fairness improvement and utility preservation. To our knowledge, no prior work in the branch of task-tailored methods has theoretically investigated downstream guarantees when using pre-processed data. We further evaluate our framework through comparison studies based on tabular and image data sets, showing the superiority of our framework which preserves consistent trade-offs among multiple downstream models compared to recent competing models. Particularly for computer vision data, we see our method alters only necessary semantic features related to the central machine learning task to achieve fairness.

</details>


### [618] [Machine learning model for predicting surface wettability in laser-textured metal alloys](https://arxiv.org/abs/2601.11661)
*Mohammad Mohammadzadeh Sanandaji,Danial Ebrahimzadeh,Mohammad Ikram Haider,Yaser Mike Banad,Aleksandar Poleksic,Hongtao Ding*

Main category: cs.LG

TL;DR: The paper presents a machine learning model to predict surface wettability of laser-textured alloys using morphology and chemistry, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately predicting the wettability of laser-textured metal surfaces by considering both morphological and chemical factors.

Method: The study employs an ensemble neural network model using morphological and chemical features derived from surface measurements, backed by techniques like residual connections and dropout regularization.

Result: The model demonstrated an R2 score of 0.942 and identified surface chemistry as the primary determinant of wettability, with morphology also playing a role.

Conclusion: The proposed framework showcases artificial intelligence’s capability to predict wetting behavior efficiently, enabling data-driven surface design advancements.

Abstract: Surface wettability, governed by both topography and chemistry, plays a critical role in applications such as heat transfer, lubrication, microfluidics, and surface coatings. In this study, we present a machine learning (ML) framework capable of accurately predicting the wettability of laser-textured metal alloys using experimentally derived morphological and chemical features. Superhydrophilic and superhydrophobic surfaces were fabricated on AA6061 and AISI 4130 alloys via nanosecond laser texturing followed by chemical immersion treatments. Surface morphology was quantified using the Laws texture energy method and profilometry, while surface chemistry was characterized through X-ray photoelectron spectroscopy (XPS), extracting features such as functional group polarity, molecular volume, and peak area fraction. These features were used to train an ensemble neural network model incorporating residual connections, batch normalization, and dropout regularization. The model achieved high predictive accuracy (R2 = 0.942, RMSE = 13.896), outperforming previous approaches. Feature importance analysis revealed that surface chemistry had the strongest influence on contact angle prediction, with topographical features also contributing significantly. This work demonstrates the potential of artificial intelligence to model and predict wetting behavior by capturing the complex interplay of surface characteristics, offering a data-driven pathway for designing tailored functional surfaces.

</details>


### [619] [Activation Sensitivity as a Unifying Principle for Post-Training Quantization](https://arxiv.org/abs/2601.11663)
*Bruce Changlong Xu*

Main category: cs.LG

TL;DR: The paper presents a unified theoretical framework for post-training quantization (PTQ), defining activation sensitivity as the expected loss impact of channel-wise perturbations, integrating principles behind key models AWQ and GPTQ.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods (AWQ, GPTQ) rely on heuristics to estimate influential weight channels in large language models, but lack a unified conceptual foundation.

Method: The authors formalize activation sensitivity using a first-order Taylor expansion, connecting channel importance to the gradient-weighted activation squared norm and aligning PTQ methods under this framework.

Result: The framework bridges empirical PTQ methods by interpreting them as approximating sensitivity under specific assumptions and clarifies theoretical relationships across pruning and quantization methods.

Conclusion: This work unifies PTQ methods conceptually through sensitivity metrics, providing insights into their design space rather than introducing a new algorithm.

Abstract: Post-training quantization (PTQ) methods for large language models rely on heuristics that implicitly estimate which weight channels most strongly influence model behavior. Two dominant paradigms have emerged: activation-aware methods such as AWQ prioritize channels with large activation magnitudes, while second-order methods such as GPTQ allocate quantization error according to input covariance structure. Despite strong empirical performance, these approaches remain conceptually fragmented, and it is unclear what underlying quantity they are approximating. In this work, we present a unified theoretical framework for PTQ by formalizing activation sensitivity, defined as the expected impact of channel-wise perturbations on the loss. Using a first-order Taylor expansion, we show that sensitivity naturally arises as the squared norm of gradient-weighted activations, yielding a principled measure of channel importance that captures both activation magnitude and downstream error propagation. Within this framework, AWQ and GPTQ can be interpreted as complementary approximations that recover sensitivity under distinct simplifying assumptions. We analyze the design space of sensitivity metrics, connect gradient-based saliency, Fisher information, and Hessian-based criteria, and clarify their relationships to classical pruning methods such as Optimal Brain Damage and Optimal Brain Surgeon. Rather than proposing a new quantization algorithm, this work provides a conceptual foundation for understanding and comparing post-training quantization methods through the lens of sensitivity.

</details>


### [620] [Federated Learning for the Design of Parametric Insurance Indices under Heterogeneous Renewable Production Losses](https://arxiv.org/abs/2601.12178)
*Fallou Niakh*

Main category: cs.LG

TL;DR: The paper presents a federated learning framework to calibrate insurance indices for renewable energy losses without sharing raw data.


<details>
  <summary>Details</summary>
Motivation: To create a scalable and privacy-preserving solution for modeling renewable energy production losses using a federated approach.

Method: Federated optimization is used with Tweedie generalized linear models, evaluating FedAvg, FedProx, and FedOpt methods.

Result: Federated learning recovers similar index coefficients to existing methods under moderate heterogeneity, offering scalability.

Conclusion: The framework effectively handles calibration in a distributed and privacy-preserving manner, suitable for diverse renewable energy contexts.

Abstract: We propose a federated learning framework for the calibration of parametric insurance indices under heterogeneous renewable energy production losses. Producers locally model their losses using Tweedie generalized linear models and private data, while a common index is learned through federated optimization without sharing raw observations. The approach accommodates heterogeneity in variance and link functions and directly minimizes a global deviance objective in a distributed setting. We implement and compare FedAvg, FedProx and FedOpt, and benchmark them against an existing approximation-based aggregation method. An empirical application to solar power production in Germany shows that federated learning recovers comparable index coefficients under moderate heterogeneity, while providing a more general and scalable framework.

</details>


### [621] [Distill-then-Replace: Efficient Task-Specific Hybrid Attention Model Construction](https://arxiv.org/abs/2601.11667)
*Xiaojie Xia,Huigang Zhang,Chaoliang Zhong,Jun Sun,Yusuke Oishi*

Main category: cs.LG

TL;DR: The paper proposes an efficient approach for creating hybrid transformer models by transferring weights and using a layer replacement strategy to balance efficiency and performance without costly retraining.


<details>
  <summary>Details</summary>
Motivation: The need to balance the efficiency and performance of transformer models, as they are limited by quadratic time and memory complexity, but alternatives like linear attention often degrade performance.

Method: 1. Transfer weights from pretrained full attention to linear attention using blockwise local distillation. 2. Introduce a greedy layer replacement strategy to iteratively replace attention layers while monitoring performance.

Result: Successfully creates task-specific hybrid transformer models that balance efficiency and expressiveness without requiring full-scale retraining or neural architecture searches.

Conclusion: The approach allows efficient creation of hybrid transformer models from existing full-attention models, improving scalability and applicability for diverse tasks while maintaining performance.

Abstract: Transformer architectures deliver state-of-the-art accuracy via dense full-attention, but their quadratic time and memory complexity with respect to sequence length limits practical deployment. Linear attention mechanisms offer linear or near-linear scaling yet often incur performance degradation. Hybrid models that integrate full and linear attention layers promise a balance between efficiency and expressiveness, but face two major challenges: training such hybrid models from scratch is computationally expensive, and manually designing the optimal placement of attention types is highly nontrivial. We address both issues by first transferring weights from the pretrained full-attention modules to its linear attention counterparts through blockwise local distillation, and second, introducing a greedy layer replacement strategy that iteratively substitutes full attention blocks with linear ones while monitoring validation performance on the target task. This yields a task-specific hybrid model in a single efficient pass, without costly re-training or neural architecture search, and can be applied to any pretrained full-attention backbone for diverse downstream tasks.

</details>


### [622] [One-Sided Matrix Completion from Ultra-Sparse Samples](https://arxiv.org/abs/2601.12213)
*Hongyang R. Zhang,Zhenshuo Zhang,Huy L. Nguyen,Guanghui Lan*

Main category: cs.LG

TL;DR: The paper addresses matrix completion in an ultra-sparse sampling regime with probability $p = C/d$. A new unbiased estimator for second-moment matrix $T$ is proposed, validated empirically, and theoretically proves robust with low sampling complexity.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by applications involving large sparse panel datasets where $n >> d$ and accurate imputation is impossible due to sparsity. Instead, it focuses on estimating properties like the second-moment matrix.

Method: The authors propose a normalization-based unbiased estimator for the second-moment matrix followed by gradient descent to impute missing entries, supported by theoretical analysis and empirical validation.

Result: The paper demonstrates significant reduction in bias and error recovery on synthetic and real-world datasets. Specifically, the method reduces bias by $88\%$ on MovieLens datasets and error by $38\%$ on Amazon reviews dataset.

Conclusion: The paper concludes that the proposed method is robust, achieves unbiased estimation of $T$ in ultra-sparse regimes, and outperforms baseline estimators in real-world applications.

Abstract: Matrix completion is a classical problem that has received recurring interest across a wide range of fields. In this paper, we revisit this problem in an ultra-sparse sampling regime, where each entry of an unknown, $n\times d$ matrix $M$ (with $n \ge d$) is observed independently with probability $p = C / d$, for a fixed integer $C \ge 2$. This setting is motivated by applications involving large, sparse panel datasets, where the number of rows far exceeds the number of columns. When each row contains only $C$ entries -- fewer than the rank of $M$ -- accurate imputation of $M$ is impossible. Instead, we estimate the row span of $M$ or the averaged second-moment matrix $T = M^{\top} M / n$.
  The empirical second-moment matrix computed from observed entries exhibits non-random and sparse missingness. We propose an unbiased estimator that normalizes each nonzero entry of the second moment by its observed frequency, followed by gradient descent to impute the missing entries of $T$. The normalization divides a weighted sum of $n$ binomial random variables by the total number of ones. We show that the estimator is unbiased for any $p$ and enjoys low variance. When the row vectors of $M$ are drawn uniformly from a rank-$r$ factor model satisfying an incoherence condition, we prove that if $n \ge O({d r^5 ε^{-2} C^{-2} \log d})$, any local minimum of the gradient-descent objective is approximately global and recovers $T$ with error at most $ε^2$.
  Experiments on both synthetic and real-world data validate our approach. On three MovieLens datasets, our algorithm reduces bias by $88\%$ relative to baseline estimators. We also empirically validate the linear sampling complexity of $n$ relative to $d$ on synthetic data. On an Amazon reviews dataset with sparsity $10^{-7}$, our method reduces the recovery error of $T$ by $59\%$ and $M$ by $38\%$ compared to baseline methods.

</details>


### [623] [IPEC: Test-Time Incremental Prototype Enhancement Classifier for Few-Shot Learning](https://arxiv.org/abs/2601.11669)
*Wenwen Liao,Hang Ruan,Jianbo Yu,Xiaofeng Yang,Qingchao Jiang,Xuefeng Yan*

Main category: cs.LG

TL;DR: The paper proposes a test-time method called Incremental Prototype Enhancement Classifier (IPEC) for few-shot classification, improving upon prototype estimation using information from previous queries.


<details>
  <summary>Details</summary>
Motivation: Existing few-shot learning methods are limited by the batch-independence assumption during testing, which restricts the model from utilizing knowledge from earlier batches.

Method: The proposed IPEC method uses a dynamic auxiliary set, filtering high-confidence query samples through a dual filtering mechanism, and combines it with the support set to build stable prototypes. A "warm-up and test" two-stage protocol is also introduced.

Result: Experimental results demonstrate that IPEC outperforms existing methods across various few-shot classification tasks.

Conclusion: IPEC significantly enhances prototype-based few-shot learning by incrementally improving prototype quality and addressing limitations from batch-independence during testing.

Abstract: Metric-based few-shot approaches have gained significant popularity due to their relatively straightforward implementation, high interpret ability, and computational efficiency. However, stemming from the batch-independence assumption during testing, which prevents the model from leveraging valuable knowledge accumulated from previous batches. To address these challenges, we propose a novel test-time method called Incremental Prototype Enhancement Classifier (IPEC), a test-time method that optimizes prototype estimation by leveraging information from previous query samples. IPEC maintains a dynamic auxiliary set by selectively incorporating query samples that are classified with high confidence. To ensure sample quality, we design a robust dual-filtering mechanism that assesses each query sample based on both global prediction confidence and local discriminative ability. By aggregating this auxiliary set with the support set in subsequent tasks, IPEC builds progressively more stable and representative prototypes, effectively reducing its reliance on the initial support set. We ground this approach in a Bayesian interpretation, conceptualizing the support set as a prior and the auxiliary set as a data-driven posterior, which in turn motivates the design of a practical "warm-up and test" two-stage inference protocol. Extensive empirical results validate the superior performance of our proposed method across multiple few-shot classification tasks.

</details>


### [624] [A Confidence-Variance Theory for Pseudo-Label Selection in Semi-Supervised Learning](https://arxiv.org/abs/2601.11670)
*Jinshi Liu,Pan Liu*

Main category: cs.LG

TL;DR: This paper introduces a novel Confidence-Variance (CoVar) framework for pseudo-label selection in semi-supervised learning that avoids overconfidence issues by combining maximum confidence with residual-class variance, resulting in improved results across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of pseudo-label selection strategies in semi-supervised learning that rely on fixed confidence thresholds and fail to account for overconfidence in deep networks.

Method: The authors propose the Confidence-Variance (CoVar) theory framework, which combines maximum confidence (MC) with residual-class variance (RCV) to derive a joint reliability criterion for pseudo-label selection, replacing fixed threshold approaches with a threshold-free mechanism.

Result: Integrating the CoVar framework into existing semi-supervised learning models consistently outperforms strong baselines across datasets like PASCAL VOC 2012, Cityscapes, CIFAR-10, and Mini-ImageNet with different label ratios and backbones.

Conclusion: The CoVar framework provides a more reliable pseudo-label selection approach compared to fixed confidence thresholds by addressing overconfidence and unstable predictions, making it widely applicable to semi-supervised learning tasks.

Abstract: Most pseudo-label selection strategies in semi-supervised learning rely on fixed confidence thresholds, implicitly assuming that prediction confidence reliably indicates correctness. In practice, deep networks are often overconfident: high-confidence predictions can still be wrong, while informative low-confidence samples near decision boundaries are discarded. This paper introduces a Confidence-Variance (CoVar) theory framework that provides a principled joint reliability criterion for pseudo-label selection. Starting from the entropy minimization principle, we derive a reliability measure that combines maximum confidence (MC) with residual-class variance (RCV), which characterizes how probability mass is distributed over non-maximum classes. The derivation shows that reliable pseudo-labels should have both high MC and low RCV, and that the influence of RCV increases as confidence grows, thereby correcting overconfident but unstable predictions. From this perspective, we cast pseudo-label selection as a spectral relaxation problem that maximizes separability in a confidence-variance feature space, and design a threshold-free selection mechanism to distinguish high- from low-reliability predictions. We integrate CoVar as a plug-in module into representative semi-supervised semantic segmentation and image classification methods. Across PASCAL VOC 2012, Cityscapes, CIFAR-10, and Mini-ImageNet with varying label ratios and backbones, it consistently improves over strong baselines, indicating that combining confidence with residual-class variance provides a more reliable basis for pseudo-label selection than fixed confidence thresholds. (Code: https://github.com/ljs11528/CoVar_Pseudo_Label_Selection.git)

</details>


### [625] [Statistical-Neural Interaction Networks for Interpretable Mixed-Type Data Imputation](https://arxiv.org/abs/2601.12380)
*Ou Deng,Shoji Nishimura,Atsushi Ogihara,Qun Jin*

Main category: cs.LG

TL;DR: The paper introduces Statistical-Neural Interaction (SNI), a framework for imputing missing entries in mixed-type tabular databases using statistical priors and neural feature attention.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problem of pervasive missing entries in tabular databases, which distort downstream analysis, while ensuring interpretability during the imputation process.

Method: The method involves coupling correlation-derived statistical priors with neural feature attention, utilizing the Controllable-Prior Feature Attention (CPFA) module to balance statistical priors and data-driven learning.

Result: SNI performs competitively on continuous metrics but faces challenges with categorical imputation, offering interpretability in return. Performance is tested on six datasets under various missingness scenarios, with additional stress tests on MNAR scenarios.

Conclusion: SNI provides an interpretable framework for mixed-type imputation, suitable for scenarios where understanding feature dependencies is important, despite potential trade-offs in categorical imputation accuracy.

Abstract: Real-world tabular databases routinely combine continuous measurements and categorical records, yet missing entries are pervasive and can distort downstream analysis. We propose Statistical-Neural Interaction (SNI), an interpretable mixed-type imputation framework that couples correlation-derived statistical priors with neural feature attention through a Controllable-Prior Feature Attention (CPFA) module. CPFA learns head-wise prior-strength coefficients $\{λ_h\}$ that softly regularize attention toward the prior while allowing data-driven deviations when nonlinear patterns appear to be present in the data. Beyond imputation, SNI aggregates attention maps into a directed feature-dependency matrix that summarizes which variables the imputer relied on, without requiring post-hoc explainers. We evaluate SNI against six baselines (Mean/Mode, MICE, KNN, MissForest, GAIN, MIWAE) on six datasets spanning ICU monitoring, population surveys, socio-economic statistics, and engineering applications. Under MCAR/strict-MAR at 30\% missingness, SNI is generally competitive on continuous metrics but is often outperformed by accuracy-first baselines (MissForest, MIWAE) on categorical variables; in return, it provides intrinsic dependency diagnostics and explicit statistical-neural trade-off parameters. We additionally report MNAR stress tests (with a mask-aware variant) and discuss computational cost, limitations -- particularly for severely imbalanced categorical targets -- and deployment scenarios where interpretability may justify the trade-off.

</details>


### [626] [Proof of Concept: Multi-Target Wildfire Risk Prediction and Large Language Model Synthesis](https://arxiv.org/abs/2601.11686)
*Nicolas Caron,Christophe Guyeux,Hassan Noura,Benjamin Aynes*

Main category: cs.LG

TL;DR: Current risk assessment methods overlook operational needs. A hybrid framework integrating predictive models and LLMs is proposed for actionable wildfire management.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to bridge the gap between theoretical wildfire risk assessment and practical application for first responders by addressing diverse dimensions of wildfire risks.

Method: A hybrid framework combining predictive models for various risk dimensions and large language models (LLMs) to integrate outputs into structured and actionable reports.

Result: Development of a proof-of-concept system showcasing the integration of risk predictors and LLMs.

Conclusion: This approach would enhance wildfire management by providing comprehensive and operationally relevant insights to responders.

Abstract: Current state-of-the-art approaches to wildfire risk assessment often overlook operational needs, limiting their practical value for first responders and firefighting services. Effective wildfire management requires a multi-target analysis that captures the diverse dimensions of wildfire risk, including meteorological danger, ignition activity, intervention complexity, and resource mobilization, rather than relying on a single predictive indicator. In this proof of concept, we propose the development of a hybrid framework that combines predictive models for each risk dimension with large language models (LLMs) to synthesize heterogeneous outputs into structured, actionable reports.

</details>


### [627] [Cooperative Multi-agent RL with Communication Constraints](https://arxiv.org/abs/2601.12518)
*Nuoya Xiong,Aarti Singh*

Main category: cs.LG

TL;DR: The paper addresses decentralized MARL communication challenges, proposing a base policy prediction method to improve data sampling and learning efficiency in cooperative systems.


<details>
  <summary>Details</summary>
Motivation: To overcome the instability of traditional importance sampling in decentralized multi-agent reinforcement learning (MARL) due to high communication costs and outdated data.

Method: The authors introduce 'base policy prediction,' leveraging older gradients to predict policy updates, reducing the gap between old and current policies, and enabling sampling over multiple base policies within one communication round.

Result: The proposed algorithm converges to an $$-Nash equilibrium in potential games with fewer communication rounds and samples, improving both communication and sample efficiency while avoiding exponential dependence on the joint action space.

Conclusion: Base policy prediction enhances cooperative MARL by reducing communication and data sampling requirements, ensuring stability in decentralized systems, and achieving strong theoretical and empirical results.

Abstract: Cooperative MARL often assumes frequent access to global information in a data buffer, such as team rewards or other agents' actions, which is typically unrealistic in decentralized MARL systems due to high communication costs. When communication is limited, agents must rely on outdated information to estimate gradients and update their policies. A common approach to handle missing data is called importance sampling, in which we reweigh old data from a base policy to estimate gradients for the current policy. However, it quickly becomes unstable when the communication is limited (i.e. missing data probability is high), so that the base policy in importance sampling is outdated. To address this issue, we propose a technique called base policy prediction, which utilizes old gradients to predict the policy update and collect samples for a sequence of base policies, which reduces the gap between the base policy and the current policy. This approach enables effective learning with significantly fewer communication rounds, since the samples of predicted base policies could be collected within one communication round. Theoretically, we show that our algorithm converges to an $\varepsilon$-Nash equilibrium in potential games with only $O(\varepsilon^{-3/4})$ communication rounds and $O(poly(\max_i |A_i|)\varepsilon^{-11/4})$ samples, improving existing state-of-the-art results in communication cost, as well as sample complexity without the exponential dependence on the joint action space size. We also extend these results to general Markov Cooperative Games to find an agent-wise local maximum. Empirically, we test the base policy prediction algorithm in both simulated games and MAPPO for complex environments.

</details>


### [628] [jBOT: Semantic Jet Representation Clustering Emerges from Self-Distillation](https://arxiv.org/abs/2601.11719)
*Ho Fung Tsoi,Dylan Rankin*

Main category: cs.LG

TL;DR: This paper introduces jBOT, a new self-supervised pre-training method for jet data from CERN, which enhances anomaly detection and classification through a novel self-distillation approach.


<details>
  <summary>Details</summary>
Motivation: To develop a method to pre-train models for jet data to extract semantic underlying features that improve anomaly detection and classification, addressing the challenge of working with unlabeled scientific data.

Method: The jBOT method employs both local particle-level and global jet-level self-distillation to create representational embeddings from unlabeled jet data, which are later used for downstream tasks.

Result: jBOT achieves emergent semantic class clustering in the feature representation space. Pre-training on background jets enables anomaly detection with simple metrics and improves downstream performance for classification tasks after fine-tuning.

Conclusion: Self-supervised learning with jBOT reveals the potential of effectively utilizing unlabeled jet data from CERN for improved scientific analysis, offering better anomaly detection and classification capabilities.

Abstract: Self-supervised learning is a powerful pre-training method for learning feature representations without labels, which often capture generic underlying semantics from the data and can later be fine-tuned for downstream tasks. In this work, we introduce jBOT, a pre-training method based on self-distillation for jet data from the CERN Large Hadron Collider, which combines local particle-level distillation with global jet-level distillation to learn jet representations that support downstream tasks such as anomaly detection and classification. We observe that pre-training on unlabeled jets leads to emergent semantic class clustering in the representation space. The clustering in the frozen embedding, when pre-trained on background jets only, enables anomaly detection via simple distance-based metrics, and the learned embedding can be fine-tuned for classification with improved performance compared to supervised models trained from scratch.

</details>


### [629] [What Trace Powers Reveal About Log-Determinants: Closed-Form Estimators, Certificates, and Failure Modes](https://arxiv.org/abs/2601.12612)
*Piyush Sao*

Main category: cs.LG

TL;DR: This paper develops a new method for estimating the log-determinant of large symmetric positive definite matrices using trace powers and establishes bounds where classical methods fail at high conditioning.


<details>
  <summary>Details</summary>
Motivation: The log-determinant computation is essential in Gaussian process inference and Bayesian model comparison. Existing methods struggle with matrices having high conditioning or require impractical assumptions.

Method: Authors utilized trace power moments and introduced a novel transform and interpolation approach leveraging the moment-generating function. They also derived bounds to estimate the log-determinant with guarantees on their accuracy.

Result: The approach shows sensitivity to eigenvalue distribution and tail behavior but provides upper/lower bounds for the log-determinant estimation under certain constraints.

Conclusion: This method offers computationally efficient estimation and bounds for the log-determinant in applications with fixed m moments, but highlights the tail sensitivity as a limitation for unbounded conditioning.

Abstract: Computing $\log\det(A)$ for large symmetric positive definite matrices arises in Gaussian process inference and Bayesian model comparison. Standard methods combine matrix-vector products with polynomial approximations. We study a different model: access to trace powers $p_k = \tr(A^k)$, natural when matrix powers are available.
  Classical moment-based approximations Taylor-expand $\log(λ)$ around the arithmetic mean. This requires $|λ- \AM| < \AM$ and diverges when $κ> 4$. We work instead with the moment-generating function $M(t) = \E[X^t]$ for normalized eigenvalues $X = λ/\AM$. Since $M'(0) = \E[\log X]$, the log-determinant becomes $\log\det(A) = n(\log \AM + M'(0))$ -- the problem reduces to estimating a derivative at $t = 0$. Trace powers give $M(k)$ at positive integers, but interpolating $M(t)$ directly is ill-conditioned due to exponential growth. The transform $K(t) = \log M(t)$ compresses this range. Normalization by $\AM$ ensures $K(0) = K(1) = 0$. With these anchors fixed, we interpolate $K$ through $m+1$ consecutive integers and differentiate to estimate $K'(0)$. However, this local interpolation cannot capture arbitrary spectral features.
  We prove a fundamental limit: no continuous estimator using finitely many positive moments can be uniformly accurate over unbounded conditioning. Positive moments downweight the spectral tail; $K'(0) = \E[\log X]$ is tail-sensitive. This motivates guaranteed bounds. From the same traces we derive upper bounds on $(\det A)^{1/n}$. Given a spectral floor $r \leq λ_{\min}$, we obtain moment-constrained lower bounds, yielding a provable interval for $\log\det(A)$. A gap diagnostic indicates when to trust the point estimate and when to report bounds. All estimators and bounds cost $O(m)$, independent of $n$. For $m \in \{4, \ldots, 8\}$, this is effectively constant time.

</details>


### [630] [Suspicious Alignment of SGD: A Fine-Grained Step Size Condition Analysis](https://arxiv.org/abs/2601.11789)
*Shenyang Deng,Boyao Liao,Zhuoli Ouyang,Tianyu Pang,Minhak Song,Yaoqing Yang*

Main category: cs.LG

TL;DR: The paper studies the suspicious gradient alignment behavior in stochastic gradient descent (SGD) during ill-conditioned optimization, analyzing its phases and impact using step-size conditions.


<details>
  <summary>Details</summary>
Motivation: To understand why during ill-conditioned optimization, the alignment of gradients with the dominant subspace in SGD behaves in a counterintuitive and 'ineffective' manner.

Method: A high-dimensional quadratic analysis of step-size conditions and their effect on gradient alignment behavior in SGD under ill-conditioned settings.

Result: The paper identifies step-size conditions that drive phase changes in gradient alignment and explains the ineffectiveness of gradient projections in certain subspaces under ill-conditioning.

Conclusion: SGD alignment behavior transitions from low to high alignment due to step-size effects. Adaptive step-size dynamics explain this behavior, validating counterintuitive empirical observations.

Abstract: This paper explores the suspicious alignment phenomenon in stochastic gradient descent (SGD) under ill-conditioned optimization, where the Hessian spectrum splits into dominant and bulk subspaces. This phenomenon describes the behavior of gradient alignment in SGD updates. Specifically, during the initial phase of SGD updates, the alignment between the gradient and the dominant subspace tends to decrease. Subsequently, it enters a rising phase and eventually stabilizes in a high-alignment phase. The alignment is considered ``suspicious'' because, paradoxically, the projected gradient update along this highly-aligned dominant subspace proves ineffective at reducing the loss. The focus of this work is to give a fine-grained analysis in a high-dimensional quadratic setup about how step size selection produces this phenomenon. Our main contribution can be summarized as follows: We propose a step-size condition revealing that in low-alignment regimes, an adaptive critical step size $η_t^*$ separates alignment-decreasing ($η_t < η_t^*$) from alignment-increasing ($η_t > η_t^*$) regimes, whereas in high-alignment regimes, the alignment is self-correcting and decreases regardless of the step size. We further show that under sufficient ill-conditioning, a step size interval exists where projecting the SGD updates to the bulk space decreases the loss while projecting them to the dominant space increases the loss, which explains a recent empirical observation that projecting gradient updates to the dominant subspace is ineffective. Finally, based on this adaptive step-size theory, we prove that for a constant step size and large initialization, SGD exhibits this distinct two-phase behavior: an initial alignment-decreasing phase, followed by stabilization at high alignment.

</details>


### [631] [Physics-Constrained Denoising Autoencoders for Data-Scarce Wildfire UAV Sensing](https://arxiv.org/abs/2601.11794)
*Abdelrahman Ramadan,Zahra Dorbeigi Namaghi,Emily Taylor,Lucas Edwards,Xan Giuliani,David S. McLagan,Sidney Givigi,Melissa Greeff*

Main category: cs.LG

TL;DR: The paper introduces PC$^2$DAE, a physics-informed denoising autoencoder that effectively corrects sensor errors in wildfire monitoring using UAVs, delivering smooth and physically valid results even with limited data.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the problem of noisy and unreliable measurements from low-cost sensors used in UAVs for wildfire monitoring, with traditional solutions requiring impractically large datasets.

Method: They created PC$^2$DAE, a denoising autoencoder with embedded physical constraints, ensuring non-negative estimates and temporally smooth outputs. The design includes lightweight and high-capability variants for different deployment contexts.

Result: PC$^2$DAE-Lean effectively denoised the signals, showing significant improvements in smoothness (67.3%) and reduction of high-frequency noise (90.7%) without any physical violations, outperforming other baseline methods.

Conclusion: PC$^2$DAE is a data-efficient, physically-informed model suitable for wildfire monitoring, with results showing enhanced performance in noisy, data-limited environments compared to traditional machine learning models.

Abstract: Wildfire monitoring requires high-resolution atmospheric measurements, yet low-cost sensors on Unmanned Aerial Vehicles (UAVs) exhibit baseline drift, cross-sensitivity, and response lag that corrupt concentration estimates. Traditional deep learning denoising approaches demand large datasets impractical to obtain from limited UAV flight campaigns. We present PC$^2$DAE, a physics-informed denoising autoencoder that addresses data scarcity by embedding physical constraints directly into the network architecture. Non-negative concentration estimates are enforced via softplus activations and physically plausible temporal smoothing, ensuring outputs are physically admissible by construction rather than relying on loss function penalties. The architecture employs hierarchical decoder heads for Black Carbon, Gas, and CO$_2$ sensor families, with two variants: PC$^2$DAE-Lean (21k parameters) for edge deployment and PC$^2$DAE-Wide (204k parameters) for offline processing. We evaluate on 7,894 synchronized 1 Hz samples collected from UAV flights during prescribed burns in Saskatchewan, Canada (approximately 2.2 hours of flight data), two orders of magnitude below typical deep learning requirements. PC$^2$DAE-Lean achieves 67.3\% smoothness improvement and 90.7\% high-frequency noise reduction with zero physics violations. Five baselines (LSTM-AE, U-Net, Transformer, CBDAE, DeSpaWN) produce 15--23\% negative outputs. The lean variant outperforms wide (+5.6\% smoothness), suggesting reduced capacity with strong inductive bias prevents overfitting in data-scarce regimes. Training completes in under 65 seconds on consumer hardware.

</details>


### [632] [Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy Regularization](https://arxiv.org/abs/2601.12707)
*Junyi Liao,Zihan Zhu,Ethan Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: The paper introduces a framework to estimate unknown reward functions in two-player zero-sum games and Markov games, addressing challenges like ambiguity and limited data.


<details>
  <summary>Details</summary>
Motivation: Understanding the reward functions driving agents is crucial for better insights in inverse reinforcement learning and game theory.

Method: The framework uses quantal response equilibrium (QRE) under linear assumptions and a reward function learning algorithm adaptable in static and dynamic settings.

Result: The algorithm proves reliable, sample-efficient, and practically effective through theoretical guarantees and numerical experiments.

Conclusion: The framework provides a robust method for identifying reward functions in competitive environments, enhancing decision-making processes.

Abstract: Estimating the unknown reward functions driving agents' behaviors is of central interest in inverse reinforcement learning and game theory. To tackle this problem, we develop a unified framework for reward function recovery in two-player zero-sum matrix games and Markov games with entropy regularization, where we aim to reconstruct the underlying reward functions given observed players' strategies and actions. This task is challenging due to the inherent ambiguity of inverse problems, the non-uniqueness of feasible rewards, and limited observational data coverage. To address these challenges, we establish the reward function's identifiability using the quantal response equilibrium (QRE) under linear assumptions. Building upon this theoretical foundation, we propose a novel algorithm to learn reward functions from observed actions. Our algorithm works in both static and dynamic settings and is adaptable to incorporate different methods, such as Maximum Likelihood Estimation (MLE). We provide strong theoretical guarantees for the reliability and sample efficiency of our algorithm. Further, we conduct extensive numerical studies to demonstrate the practical effectiveness of the proposed framework, offering new insights into decision-making in competitive environments.

</details>


### [633] [Shapelets-Enriched Selective Forecasting using Time Series Foundation Models](https://arxiv.org/abs/2601.11821)
*Shivani Tomar,Seshu Tirupathi,Elizabeth Daly,Ivana Dusparic*

Main category: cs.LG

TL;DR: This paper introduces a selective forecasting framework based on shapelets to improve time-series foundation models' reliability, achieving significant error reduction compared to random selection methods.


<details>
  <summary>Details</summary>
Motivation: Current time-series foundation models struggle with reliably forecasting critical regions in data, limiting their practical use for datasets with unique trends.

Method: The framework identifies critical data segments using shapelets learned via shift-invariant dictionary learning and assesses predictions using distance-based similarity to these shapelets.

Result: Empirical evaluations show an average error reduction of 22.17% for zero-shot models and 22.62% for full-shot fine-tuned models, while outperforming random selection methods by up to 21.43%.

Conclusion: The proposed framework enhances reliability and usability of time-series foundation models by selectively identifying and discarding unreliable predictions.

Abstract: Time series foundation models have recently gained a lot of attention due to their ability to model complex time series data encompassing different domains including traffic, energy, and weather. Although they exhibit strong average zero-shot performance on forecasting tasks, their predictions on certain critical regions of the data are not always reliable, limiting their usability in real-world applications, especially when data exhibits unique trends. In this paper, we propose a selective forecasting framework to identify these critical segments of time series using shapelets. We learn shapelets using shift-invariant dictionary learning on the validation split of the target domain dataset. Utilizing distance-based similarity to these shapelets, we facilitate the user to selectively discard unreliable predictions and be informed of the model's realistic capabilities. Empirical results on diverse benchmark time series datasets demonstrate that our approach leveraging both zero-shot and full-shot fine-tuned models reduces the overall error by an average of 22.17% for zero-shot and 22.62% for full-shot fine-tuned model. Furthermore, our approach using zero-shot and full-shot fine-tuned models, also outperforms its random selection counterparts by up to 21.41% and 21.43% on one of the datasets.

</details>


### [634] [Online Continual Learning for Time Series: a Natural Score-driven Approach](https://arxiv.org/abs/2601.12931)
*Edoardo Urettini,Daniele Atzeni,Ioanna-Yvonni Tsaknaki,Antonio Carta*

Main category: cs.LG

TL;DR: The paper introduces a novel method, NatSR, combining natural gradient optimization and replay buffer techniques for robust online time series forecasting, surpassing current methods.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between time series forecasting methods and online continual learning, addressing rapid adaptation and long-term memory requirements in dynamic environments.

Method: The authors reformulate neural network optimization as parameter filtering, utilize natural gradient descent with Student's t likelihood, and propose NatSR with a robust optimizer and replay buffer for time-sensitive adaptation.

Result: NatSR surpasses state-of-the-art techniques in performance for online time series forecasting through empirical evaluations.

Conclusion: NatSR strengthens the link between time series forecasting and continual learning, utilizing robust optimization techniques for superior forecasting in dynamic scenarios.

Abstract: Online continual learning (OCL) methods adapt to changing environments without forgetting past knowledge. Similarly, online time series forecasting (OTSF) is a real-world problem where data evolve in time and success depends on both rapid adaptation and long-term memory. Indeed, time-varying and regime-switching forecasting models have been extensively studied, offering a strong justification for the use of OCL in these settings. Building on recent work that applies OCL to OTSF, this paper aims to strengthen the theoretical and practical connections between time series methods and OCL. First, we reframe neural network optimization as a parameter filtering problem, showing that natural gradient descent is a score-driven method and proving its information-theoretic optimality. Then, we show that using a Student's t likelihood in addition to natural gradient induces a bounded update, which improves robustness to outliers. Finally, we introduce Natural Score-driven Replay (NatSR), which combines our robust optimizer with a replay buffer and a dynamic scale heuristic that improves fast adaptation at regime drifts. Empirical results demonstrate that NatSR achieves stronger forecasting performance than more complex state-of-the-art methods.

</details>


### [635] [MixFlow: Mixture-Conditioned Flow Matching for Out-of-Distribution Generalization](https://arxiv.org/abs/2601.11827)
*Andrea Rubbi,Amir Akbarnejad,Mohammad Vali Sanian,Aryan Yazdan Parast,Hesam Asadollahzadeh,Arian Amani,Naveed Akhtar,Sarah Cooper,Andrew Bassett,Pietro Liò,Lassi Paavolainen,Sattar Vakili,Mo Lotfollahi*

Main category: cs.LG

TL;DR: MixFlow is a conditional flow-matching framework that addresses distribution shift challenges in generative modeling. It combines a descriptor-conditioned base distribution and flow field to improve generalization to unseen conditions.


<details>
  <summary>Details</summary>
Motivation: Existing generative models struggle with generalization to distribution shifts and conditions unseen during training. This paper proposes a novel approach to overcome this limitation.

Method: The paper introduces MixFlow, which jointly learns a descriptor-conditioned base distribution and flow field using shortest-path flow matching. It incorporates a learnable, descriptor-dependent mixture for improved interpolation and extrapolation.

Result: MixFlow demonstrates improved out-of-distribution generalization, validated across domains like single-cell transcriptomics and drug screening tasks. It outperforms standard conditional flow-matching baselines.

Conclusion: MixFlow provides a robust, generalizable, and controllable framework for generative modeling, addressing challenges across heterogeneous domains.

Abstract: Achieving robust generalization under distribution shift remains a central challenge in conditional generative modeling, as existing conditional flow-based methods often struggle to extrapolate beyond the training conditions. We introduce MixFlow, a conditional flow-matching framework for descriptor-controlled generation that directly targets this limitation by jointly learning a descriptor-conditioned base distribution and a descriptor-conditioned flow field via shortest-path flow matching. By modeling the base distribution as a learnable, descriptor-dependent mixture, MixFlow enables smooth interpolation and extrapolation to unseen conditions, leading to substantially improved out-of-distribution generalization. We provide analytical insights into the behavior of the proposed framework and empirically demonstrate its effectiveness across multiple domains, including prediction of responses to unseen perturbations in single-cell transcriptomic data and high-content microscopy-based drug screening tasks. Across these diverse settings, MixFlow consistently outperforms standard conditional flow-matching baselines. Overall, MixFlow offers a simple yet powerful approach for achieving robust, generalizable, and controllable generative modeling across heterogeneous domains.

</details>


### [636] [Multi-level Monte Carlo Dropout for Efficient Uncertainty Quantification](https://arxiv.org/abs/2601.13272)
*Aaron Pim,Tristan Pryer*

Main category: cs.LG

TL;DR: The paper introduces a multilevel Monte Carlo (MLMC) framework for improving uncertainty quantification using Monte Carlo dropout.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency and accuracy of uncertainty quantification methods by addressing epistemic randomness introduced by Monte Carlo dropout.

Method: The authors proposed a fidelity hierarchy and developed telescoping MLMC estimators for predictive moments by coupling coarse and fine estimators. They provided analytical formulations for bias, variance, and cost, along with sample allocation strategies.

Result: Numerical experiments on forward and inverse PINNs--Uzawa benchmarks validated the theoretical predictions, showing significant efficiency gains and reduced variance compared to single-level Monte Carlo dropout.

Conclusion: This MLMC framework efficiently enhances predictive uncertainty quantification by reducing variance and maintaining unbiased estimations within fixed computational resources.

Abstract: We develop a multilevel Monte Carlo (MLMC) framework for uncertainty quantification with Monte Carlo dropout. Treating dropout masks as a source of epistemic randomness, we define a fidelity hierarchy by the number of stochastic forward passes used to estimate predictive moments. We construct coupled coarse--fine estimators by reusing dropout masks across fidelities, yielding telescoping MLMC estimators for both predictive means and predictive variances that remain unbiased for the corresponding dropout-induced quantities while reducing sampling variance at fixed evaluation budget. We derive explicit bias, variance and effective cost expressions, together with sample-allocation rules across levels. Numerical experiments on forward and inverse PINNs--Uzawa benchmarks confirm the predicted variance rates and demonstrate efficiency gains over single-level MC-dropout at matched cost.

</details>


### [637] [AGGC: Adaptive Group Gradient Clipping for Stabilizing Large Language Model Training](https://arxiv.org/abs/2601.11864)
*Zhiyuan Li,Yuan Wu,Yi Chang*

Main category: cs.LG

TL;DR: The paper introduces Adaptive Group-wise Gradient Clipping (AGGC) to address limitations of traditional gradient clipping, achieving improved model stability and performance.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the shortcomings of global norm clipping, particularly its inability to manage gradient heterogeneity across functional modules, leading to suboptimal training stabilization.

Method: The proposed method, AGGC, partitions parameters into functional groups and applies an Exponential Moving Average (EMA) to adaptively control gradient scaling. It uses an adaptive interval for clipping to manage both gradient explosion and vanishing, and incorporates a time-dependent schedule for balanced training.

Result: AGGC outperforms existing methods like LoRA and sometimes even Full Fine-Tuning. Experimental results indicate significant improvement, including 72.93% accuracy on GSM8K with Mistral-7B, and enhanced reinforcement learning model stability.

Conclusion: AGGC resolves the issues of gradient heterogeneity in traditional clipping techniques, offers a lightweight integration into existing pipelines, and significantly stabilizes and improves large model training.

Abstract: To stabilize the training of Large Language Models (LLMs), gradient clipping is a nearly ubiquitous heuristic used to alleviate exploding gradients. However, traditional global norm clipping erroneously presupposes gradient homogeneity across different functional modules, leading to an adverse "spill-over" effect where volatile parameters force unnecessary scaling on stable ones. To overcome this, we propose Adaptive Group-wise Gradient Clipping (AGGC). AGGC partitions parameters into groups based on functional types and regulates each according to its historical behavior using an Exponential Moving Average (EMA). Specifically, it constructs an adaptive interval to simultaneously mitigate gradient explosion and vanishing, while employing a time-dependent scheduling mechanism to balance exploration and convergence. Experiments on LLaMA 2-7B, Mistral-7B, and Gemma-7B models show that AGGC consistently outperforms LoRA and frequently surpasses Full Fine-Tuning. On the GSM8K benchmark, Mistral-7B fine-tuned with AGGC achieves an accuracy of 72.93%, exceeding LoRA's 69.5%. AGGC also effectively stabilizes Reinforcement Learning with Verifiable Rewards (RLVR), enhancing the logic deduction of Qwen 2.5 and Llama 3.2 models. Experimental results demonstrate that AGGC effectively addresses the limitations of traditional gradient clipping methods, particularly in overcoming gradient heterogeneity, by utilizing a modular, adaptive clipping strategy to stabilize the training process. Due to its lightweight design, AGGC can be seamlessly integrated into existing post-training pipelines with negligible overhead.

</details>


### [638] [Fairness-informed Pareto Optimization : An Efficient Bilevel Framework](https://arxiv.org/abs/2601.13448)
*Sofiane Tanji,Samuel Vaiter,Yassine Laguel*

Main category: cs.LG

TL;DR: The paper introduces BADR, a framework to achieve Pareto-efficient fair machine learning models across diverse fairness metrics.


<details>
  <summary>Details</summary>
Motivation: Fairness methods in machine learning often produce inefficient models, and existing Pareto-efficient approaches are limited in their adaptability to different fairness metrics.

Method: The authors propose BADR, a bilevel optimization framework using adaptive rescalarization, coupled with two single-loop algorithms (BADR-GD and BADR-SGD) and release a Python toolbox.

Result: BADR delivers improved Pareto-efficient models for fairness across diverse metrics, outperforming prior approaches in numerical tests.

Conclusion: BADR offers a versatile and efficient solution to fair machine learning challenges, supporting a wide range of fairness metrics and learning tasks.

Abstract: Despite their promise, fair machine learning methods often yield Pareto-inefficient models, in which the performance of certain groups can be improved without degrading that of others. This issue arises frequently in traditional in-processing approaches such as fairness-through-regularization. In contrast, existing Pareto-efficient approaches are biased towards a certain perspective on fairness and fail to adapt to the broad range of fairness metrics studied in the literature. In this paper, we present BADR, a simple framework to recover the optimal Pareto-efficient model for any fairness metric. Our framework recovers its models through a Bilevel Adaptive Rescalarisation procedure. The lower level is a weighted empirical risk minimization task where the weights are a convex combination of the groups, while the upper level optimizes the chosen fairness objective. We equip our framework with two novel large-scale, single-loop algorithms, BADR-GD and BADR-SGD, and establish their convergence guarantees. We release badr, an open-source Python toolbox implementing our framework for a variety of learning tasks and fairness metrics. Finally, we conduct extensive numerical experiments demonstrating the advantages of BADR over existing Pareto-efficient approaches to fairness.

</details>


### [639] [TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers for Treasury Futures](https://arxiv.org/abs/2601.11880)
*Yingxiao Zhang,Jiaxin Duan,Junfu Zhang,Ke Feng*

Main category: cs.LG

TL;DR: The paper introduces TF-CoDiT, an enhanced Diffusion Transformer adapted for language-controlled synthesis of treasury futures data, addressing its low volume and market-dependent characteristics.


<details>
  <summary>Details</summary>
Motivation: To improve the synthesis of treasury futures data, which is underexplored and faces challenges stemming from its low volume, market dependencies, and cross-variable correlations.

Method: The authors propose a system that transforms time series into DWT coefficient matrices and uses a U-shape VAE to encode cross-channel dependencies into a latent variable for latent diffusion generation, supported by the FinMAP system to standardize market dynamics conditions.

Result: Through extensive experiments on treasury futures data (2015-2025), TF-CoDiT achieves highly authentic data generation with minimal errors: 0.433 (MSE) and 0.453 (MAE).

Conclusion: TF-CoDiT demonstrates robustness and high accuracy in generating synthetic treasury futures data, showing applicability in various contracts and temporal ranges.

Abstract: Diffusion Transformers (DiT) have achieved milestones in synthesizing financial time-series data, such as stock prices and order flows. However, their performance in synthesizing treasury futures data is still underexplored. This work emphasizes the characteristics of treasury futures data, including its low volume, market dependencies, and the grouped correlations among multivariables. To overcome these challenges, we propose TF-CoDiT, the first DiT framework for language-controlled treasury futures synthesis. To facilitate low-data learning, TF-CoDiT adapts the standard DiT by transforming multi-channel 1-D time series into Discrete Wavelet Transform (DWT) coefficient matrices. A U-shape VAE is proposed to encode cross-channel dependencies hierarchically into a latent variable and bridge the latent and DWT spaces through decoding, thereby enabling latent diffusion generation. To derive prompts that cover essential conditions, we introduce the Financial Market Attribute Protocol (FinMAP) - a multi-level description system that standardizes daily$/$periodical market dynamics by recognizing 17$/$23 economic indicators from 7/8 perspectives. In our experiments, we gather four types of treasury futures data covering the period from 2015 to 2025, and define data synthesis tasks with durations ranging from one week to four months. Extensive evaluations demonstrate that TF-CoDiT can produce highly authentic data with errors at most 0.433 (MSE) and 0.453 (MAE) to the ground-truth. Further studies evidence the robustness of TF-CoDiT across contracts and temporal horizons.

</details>


### [640] [Preconditioning Benefits of Spectral Orthogonalization in Muon](https://arxiv.org/abs/2601.13474)
*Jianhao Ma,Yu Huang,Yuejie Chi,Yuxin Chen*

Main category: cs.LG

TL;DR: This paper takes a step toward understanding the Muon optimizer’s underlying mechanisms through theoretical analysis by studying simplified Muon in matrix factorization and linear transformers.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of understanding of the Muon optimizer's spectral orthogonalization mechanisms and its advantages in concrete applications.

Method: The authors study two case studies—matrix factorization and in-context learning of linear transformers—and analyze simplified Muon’s convergence behavior in comparison to gradient descent and Adam.

Result: Simplified Muon is proven to converge linearly, with iteration complexities independent of condition numbers, outperforming gradient descent and Adam. Muon dynamics decouple into independent scalar sequences in the spectral domain.

Conclusion: The paper formalizes the preconditioning effect of spectral orthogonalization, providing insights into Muon's efficiency in optimization problems and potential generalization beyond the studied scenarios.

Abstract: The Muon optimizer, a matrix-structured algorithm that leverages spectral orthogonalization of gradients, is a milestone in the pretraining of large language models. However, the underlying mechanisms of Muon -- particularly the role of gradient orthogonalization -- remain poorly understood, with very few works providing end-to-end analyses that rigorously explain its advantages in concrete applications. We take a step by studying the effectiveness of a simplified variant of Muon through two case studies: matrix factorization, and in-context learning of linear transformers. For both problems, we prove that simplified Muon converges linearly with iteration complexities independent of the relevant condition number, provably outperforming gradient descent and Adam. Our analysis reveals that the Muon dynamics decouple into a collection of independent scalar sequences in the spectral domain, each exhibiting similar convergence behavior. Our theory formalizes the preconditioning effect induced by spectral orthogonalization, offering insight into Muon's effectiveness in these matrix optimization problems and potentially beyond.

</details>


### [641] [Approximation Algorithm for Constrained $k$-Center Clustering: A Local Search Approach](https://arxiv.org/abs/2601.11883)
*Chaoqi Jia,Longkun Guo,Kewen Liao,Zhigang Lu,Chao Chen,Jason Xue*

Main category: cs.LG

TL;DR: This paper tackles the constrained k-center clustering problem using a novel local search framework, achieving the best possible approximation ratio of 2.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges in constrained k-center clustering, incorporating instance-level must-link and cannot-link constraints while ensuring an optimal approximation.

Method: Local search framework transformed into a dominating matching set problem, allowing efficient approximation.

Result: Achieved an optimal approximation ratio of 2 and outperformed baselines in solution quality on real and synthetic datasets.

Conclusion: The paper introduced a framework advancing the constrained k-center problem, demonstrating its practical and theoretical effectiveness.

Abstract: Clustering is a long-standing research problem and a fundamental tool in AI and data analysis. The traditional k-center problem, a fundamental theoretical challenge in clustering, has a best possible approximation ratio of 2, and any improvement to a ratio of 2 - ε would imply P = NP. In this work, we study the constrained k-center clustering problem, where instance-level cannot-link (CL) and must-link (ML) constraints are incorporated as background knowledge. Although general CL constraints significantly increase the hardness of approximation, previous work has shown that disjoint CL sets permit constant-factor approximations. However, whether local search can achieve such a guarantee in this setting remains an open question. To this end, we propose a novel local search framework based on a transformation to a dominating matching set problem, achieving the best possible approximation ratio of 2. The experimental results on both real-world and synthetic datasets demonstrate that our algorithm outperforms baselines in solution quality.

</details>


### [642] [Does Privacy Always Harm Fairness? Data-Dependent Trade-offs via Chernoff Information Neural Estimation](https://arxiv.org/abs/2601.13698)
*Arjun Nichani,Hsiang Hsu,Chun-Fu,Chen,Haewon Jeong*

Main category: cs.LG

TL;DR: The paper examines the relationship between fairness, privacy, and accuracy in machine learning using the Noisy Chernoff Difference derived from the Chernoff Information measure.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of focus on the interplay between fairness and privacy in machine learning, which are both critical for trustworthy systems.

Method: The authors define the Noisy Chernoff Difference tool to analyze fairness, privacy, and accuracy simultaneously, employing both synthetic and real data to examine these dynamics.

Result: Using synthetic data, results show the Noisy Chernoff Difference behaving in three distinct ways based on data distribution. It also acts as a proxy for the steepness of fairness-accuracy tradeoffs.

Conclusion: The study contributes to understanding the data-dependent relationship between fairness, privacy, and accuracy, paving the way for a unified framework to navigate these tradeoffs.

Abstract: Fairness and privacy are two vital pillars of trustworthy machine learning. Despite extensive research on these individual topics, the relationship between fairness and privacy has received significantly less attention. In this paper, we utilize the information-theoretic measure Chernoff Information to highlight the data-dependent nature of the relationship among the triad of fairness, privacy, and accuracy. We first define Noisy Chernoff Difference, a tool that allows us to analyze the relationship among the triad simultaneously. We then show that for synthetic data, this value behaves in 3 distinct ways (depending on the distribution of the data). We highlight the data distributions involved in these cases and explore their fairness and privacy implications. Additionally, we show that Noisy Chernoff Difference acts as a proxy for the steepness of the fairness-accuracy curves. Finally, we propose a method for estimating Chernoff Information on data from unknown distributions and utilize this framework to examine the triad dynamic on real datasets. This work builds towards a unified understanding of the fairness-privacy-accuracy relationship and highlights its data-dependent nature.

</details>


### [643] [From Relative Entropy to Minimax: A Unified Framework for Coverage in MDPs](https://arxiv.org/abs/2601.11890)
*Xihe Gu,Urbashi Mitra,Tara Javidi*

Main category: cs.LG

TL;DR: The paper proposes a novel exploration framework in reward-free Markov Decision Problems (MDPs) using a weighted, parameterized family of concave coverage objectives to ensure efficient and targeted exploration.


<details>
  <summary>Details</summary>
Motivation: Existing exploration strategies in MDPs often fail to account for variations in the importance and difficulty of different state--action pairs, necessitating strategies that prioritize under-explored areas for more efficient exploration.

Method: The paper introduces a family of concave coverage objectives, denoted as $U_ρ$, defined over state-action occupancy measures. These objectives allow active prioritization of under-explored state--action pairs using gradient-based optimization.

Result: The proposed gradient-based algorithm effectively induces desired exploration behavior, emphasizing under-explored state--action pairs, and transitions to worst-case (minimax) coverage in the limit as the parameter $ρ$ increases.

Conclusion: The proposed approach unifies multiple exploration objectives within a single framework, balancing coverage efficiency and prioritizing unexplored areas, demonstrating improved exploration strategies in reward-free MDPs.

Abstract: Targeted and deliberate exploration of state--action pairs is essential in reward-free Markov Decision Problems (MDPs). More precisely, different state-action pairs exhibit different degree of importance or difficulty which must be actively and explicitly built into a controlled exploration strategy. To this end, we propose a weighted and parameterized family of concave coverage objectives, denoted by $U_ρ$, defined directly over state--action occupancy measures. This family unifies several widely studied objectives within a single framework, including divergence-based marginal matching, weighted average coverage, and worst-case (minimax) coverage. While the concavity of $U_ρ$ captures the diminishing return associated with over-exploration, the simple closed form of the gradient of $U_ρ$ enables an explicit control to prioritize under-explored state--action pairs. Leveraging this structure, we develop a gradient-based algorithm that actively steers the induced occupancy toward a desired coverage pattern. Moreover, we show that as $ρ$ increases, the resulting exploration strategy increasingly emphasizes the least-explored state--action pairs, recovering worst-case coverage behavior in the limit.

</details>


### [644] [Orthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks](https://arxiv.org/abs/2601.13776)
*Thibaut Boissin,Franck Mamalet,Valentin Lafargue,Mathieu Serrurier*

Main category: cs.LG

TL;DR: Orthogonium is a PyTorch library for orthogonal and 1-Lipschitz layers, providing efficiency, mathematical guarantees, and standardized tools.


<details>
  <summary>Details</summary>
Motivation: Existing tools for orthogonal and 1-Lipschitz neural network layers are fragmented, inefficient, and computationally expensive.

Method: Orthogonium delivers a unified, comprehensive PyTorch library with optimized implementations for diverse features like convolution strides, dilation, grouping, and transposition with strict guarantees.

Result: The library optimizes performance in large benchmarks like ImageNet and revealed issues in prior implementations, advocating for standardization.

Conclusion: Orthogonium simplifies scalable experimentation and integration for applications requiring robust orthogonality and Lipschitz constraints.

Abstract: Orthogonal and 1-Lipschitz neural network layers are essential building blocks in robust deep learning architectures, crucial for certified adversarial robustness, stable generative models, and reliable recurrent networks. Despite significant advancements, existing implementations remain fragmented, limited, and computationally demanding. To address these issues, we introduce Orthogonium , a unified, efficient, and comprehensive PyTorch library providing orthogonal and 1-Lipschitz layers. Orthogonium provides access to standard convolution features-including support for strides, dilation, grouping, and transposed-while maintaining strict mathematical guarantees. Its optimized implementations reduce overhead on large scale benchmarks such as ImageNet. Moreover, rigorous testing within the library has uncovered critical errors in existing implementations, emphasizing the importance of standardized and reliable tools. Orthogonium thus significantly lowers adoption barriers, enabling scalable experimentation and integration across diverse applications requiring orthogonality and robust Lipschitz constraints. Orthogonium is available at https://github.com/deel-ai/orthogonium.

</details>


### [645] [DevBench: A Realistic, Developer-Informed Benchmark for Code Generation Models](https://arxiv.org/abs/2601.11895)
*Pareesa Ameneh Golnari,Adarsh Kumarappan,Wen Wen,Xiaoyu Liu,Gabriel Ryan,Yuting Sun,Shengyu Fu,Elsie Nallipogu*

Main category: cs.LG

TL;DR: DevBench evaluates large language models on realistic code completion tasks using 1,800 instances across six programming languages and task categories from real developer telemetry.


<details>
  <summary>Details</summary>
Motivation: The need for a reliable benchmark that evaluates LLMs in practical, realistic coding tasks without data contamination, enabling detailed diagnostics and actionable insights.

Method: A telemetry-driven evaluation benchmark with a focus on ecological validity, combining functional correctness, similarity-based metrics, and assessments by LLM-judge on usefulness and contextual relevance.

Result: Nine state-of-the-art models were assessed, showcasing differences in syntactic precision, semantic reasoning, and practical utility.

Conclusion: DevBench offers a robust framework for assessing LLMs, providing practical insights essential for deployment and targeted improvements.

Abstract: DevBench is a telemetry-driven benchmark designed to evaluate Large Language Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation instances across six programming languages and six task categories derived from real developer telemetry, such as API usage and code purpose understanding. Unlike prior benchmarks, it emphasizes ecological validity, avoids training data contamination, and enables detailed diagnostics. The evaluation combines functional correctness, similarity-based metrics, and LLM-judge assessments focused on usefulness and contextual relevance. 9 state-of-the-art models were assessed, revealing differences in syntactic precision, semantic reasoning, and practical utility. Our benchmark provides actionable insights to guide model selection and improvement-detail that is often missing from other benchmarks but is essential for both practical deployment and targeted model development.

</details>


### [646] [Inverting Self-Organizing Maps: A Unified Activation-Based Framework](https://arxiv.org/abs/2601.13851)
*Alessandro Londei,Matteo Benati,Denise Lanzieri,Vittorio Loreto*

Main category: cs.LG

TL;DR: This paper explores the inversion of Self-Organizing Maps (SOM) and introduces a new update mechanism called MUSIC for controlled trajectories in latent space using prototype geometry.


<details>
  <summary>Details</summary>
Motivation: To understand whether the activation patterns of SOMs can recover the input data and develop semantically meaningful methods for latent space exploration.

Method: The authors derive a linear system based on Euclidean distance geometry and introduce MUSIC (Manifold-Aware Unified SOM Inversion and Control), which alters squared distances to prototypes while incorporating Tikhonov regularization for stability.

Result: MUSIC allows controlled and interpretable trajectories within the data manifold, producing coherent semantic variations in synthetic, MNIST, and Faces in the Wild datasets with smooth transitions.

Conclusion: The inversion mechanism and MUSIC approach offer a novel way to explore and augment latent space by leveraging SOM's geometric properties, showing advantages over traditional clustering methods.

Abstract: Self-Organizing Maps provide topology-preserving projections of high-dimensional data and have been widely used for visualization, clustering, and vector quantization. In this work, we show that the activation pattern of a SOM - the squared distances to its prototypes - can be inverted to recover the exact input under mild geometric conditions. This follows from a classical fact in Euclidean distance geometry: a point in $D$ dimensions is uniquely determined by its distances to $D{+}1$ affinely independent references. We derive the corresponding linear system and characterize the conditions under which the inversion is well-posed. Building upon this mechanism, we introduce the Manifold-Aware Unified SOM Inversion and Control (MUSIC) update rule, which enables controlled, semantically meaningful trajectories in latent space. MUSIC modifies squared distances to selected prototypes while preserving others, resulting in a deterministic geometric flow aligned with the SOM's piecewise-linear structure. Tikhonov regularization stabilizes the update rule and ensures smooth motion on high-dimensional datasets. Unlike variational or probabilistic generative models, MUSIC does not rely on sampling, latent priors, or encoder-decoder architectures. If no perturbation is applied, inversion recovers the exact input; when a target cluster or prototype is specified, MUSIC produces coherent semantic variations while remaining on the data manifold. This leads to a new perspective on data augmentation and controllable latent exploration based solely on prototype geometry. We validate the approach using synthetic Gaussian mixtures, the MNIST and the Faces in the Wild dataset. Across all settings, MUSIC produces smooth, interpretable trajectories that reveal the underlying geometry of the learned manifold, illustrating the advantages of SOM-based inversion over unsupervised clustering.

</details>


### [647] [Communication-Corruption Coupling and Verification in Cooperative Multi-Objective Bandits](https://arxiv.org/abs/2601.11924)
*Ming Shi*

Main category: cs.LG

TL;DR: This paper analyzes cooperative multi-agent systems facing vector-valued rewards under adversarial corruption using limited verification, proposing regret bounds and communication strategies to mitigate corruption effects.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of adversarial corruption in collaborative learning environments, and understand how communication methods and limited verification influence learnability and team regret.

Method: The study introduces a communication-corruption coupling framework, parameterizing effective corruption through a multiplicity functional induced by sharing protocols, and investigates how verification budgets restore learnability.

Result: Key findings include varying corruption effects based on communication protocols, theoretical limits of achievable regret under high corruption, and conditions under which verified observations neutralize adversary's impact.

Conclusion: Verification restores learnability in high-corruption regimes, while certain communication strategies minimize corruption amplification, enabling efficient team learning despite adversarial interference.

Abstract: We study cooperative stochastic multi-armed bandits with vector-valued rewards under adversarial corruption and limited verification. In each of $T$ rounds, each of $N$ agents selects an arm, the environment generates a clean reward vector, and an adversary perturbs the observed feedback subject to a global corruption budget $Γ$. Performance is measured by team regret under a coordinate-wise nondecreasing, $L$-Lipschitz scalarization $φ$, covering linear, Chebyshev, and smooth monotone utilities. Our main contribution is a communication-corruption coupling: we show that a fixed environment-side budget $Γ$ can translate into an effective corruption level ranging from $Γ$ to $NΓ$, depending on whether agents share raw samples, sufficient statistics, or only arm recommendations. We formalize this via a protocol-induced multiplicity functional and prove regret bounds parameterized by the resulting effective corruption. As corollaries, raw-sample sharing can suffer an $N$-fold larger additive corruption penalty, whereas summary sharing and recommendation-only sharing preserve an unamplified $O(Γ)$ term and achieve centralized-rate team regret. We further establish information-theoretic limits, including an unavoidable additive $Ω(Γ)$ penalty and a high-corruption regime $Γ=Θ(NT)$ where sublinear regret is impossible without clean information. Finally, we characterize how a global budget $ν$ of verified observations restores learnability. That is, verification is necessary in the high-corruption regime, and sufficient once it crosses the identification threshold, with certified sharing enabling the team's regret to become independent of $Γ$.

</details>


### [648] [Penalizing Localized Dirichlet Energies in Low Rank Tensor Products](https://arxiv.org/abs/2601.14173)
*Paris A. Karakasis,Nicholas D. Sidiropoulos*

Main category: cs.LG

TL;DR: The paper explores low-rank tensor-product B-spline (TPBS) models for regression and proposes a local Dirichlet energy-based regularization strategy to address overfitting, outperforming neural networks in overfitting scenarios.


<details>
  <summary>Details</summary>
Motivation: To address limitations of smoothness measures like global Dirichlet energy in low-rank tensor-product B-spline models and improve robustness to overfitting in regression tasks.

Method: The paper uses TPBS models with closed-form global Dirichlet energy analysis and introduces a local Dirichlet energy-based regularization strategy. It also designs two estimators for handling incomplete data using pretrained TPBS models.

Result: Through experiments, the TPBS models proved to outperform neural networks in overfitting-prone settings while maintaining competitive performance otherwise.

Conclusion: TPBS models are more robust to overfitting and effectively utilize regularization compared to neural networks, suggesting a promising alternative approach for regression problems.

Abstract: We study low-rank tensor-product B-spline (TPBS) models for regression tasks and investigate Dirichlet energy as a measure of smoothness. We show that TPBS models admit a closed-form expression for the Dirichlet energy, and reveal scenarios where perfect interpolation is possible with exponentially small Dirichlet energy. This renders global Dirichlet energy-based regularization ineffective. To address this limitation, we propose a novel regularization strategy based on local Dirichlet energies defined on small hypercubes centered at the training points. Leveraging pretrained TPBS models, we also introduce two estimators for inference from incomplete samples. Comparative experiments with neural networks demonstrate that TPBS models outperform neural networks in the overfitting regime for most datasets, and maintain competitive performance otherwise. Overall, TPBS models exhibit greater robustness to overfitting and consistently benefit from regularization, while neural networks are more sensitive to overfitting and less effective in leveraging regularization.

</details>


### [649] [Trainability-Oriented Hybrid Quantum Regression via Geometric Preconditioning and Curriculum Optimization](https://arxiv.org/abs/2601.11942)
*Qingyu Meng,Yangshuai Wang*

Main category: cs.LG

TL;DR: This paper introduces a hybrid quantum-classical regression framework to address trainability issues in QNNs and demonstrates improved performance through geometric preconditioning and curriculum optimization.


<details>
  <summary>Details</summary>
Motivation: To overcome trainability challenges in QNNs caused by noisy gradients and ill-conditioned optimization in regression tasks.

Method: The framework uses a classical embedding for geometric preconditioning and employs a curriculum optimization strategy that gradually increases circuit depth while shifting from stochastic exploration to gradient fine-tuning.

Result: The framework shows consistent improvements over pure QNN baselines, providing stable convergence in data-limited scenarios and reducing structured errors in scientific benchmarks.

Conclusion: Combining geometric preconditioning and curriculum training makes quantum regression more stable and practical for scientific machine learning applications.

Abstract: Quantum neural networks (QNNs) have attracted growing interest for scientific machine learning, yet in regression settings they often suffer from limited trainability under noisy gradients and ill-conditioned optimization. We propose a hybrid quantum-classical regression framework designed to mitigate these bottlenecks. Our model prepends a lightweight classical embedding that acts as a learnable geometric preconditioner, reshaping the input representation to better condition a downstream variational quantum circuit. Building on this architecture, we introduce a curriculum optimization protocol that progressively increases circuit depth and transitions from SPSA-based stochastic exploration to Adam-based gradient fine-tuning. We evaluate the approach on PDE-informed regression benchmarks and standard regression datasets under a fixed training budget in a simulator setting. Empirically, the proposed framework consistently improves over pure QNN baselines and yields more stable convergence in data-limited regimes. We further observe reduced structured errors that are visually correlated with oscillatory components on several scientific benchmarks, suggesting that geometric preconditioning combined with curriculum training is a practical approach for stabilizing quantum regression.

</details>


### [650] [Q-learning with Adjoint Matching](https://arxiv.org/abs/2601.14234)
*Qiyang Li,Sergey Levine*

Main category: cs.LG

TL;DR: This paper introduces a new reinforcement learning algorithm, Q-learning with Adjoint Matching (QAM), which enables efficient optimization for expressive diffusion or flow policies in continuous-action RL tasks.


<details>
  <summary>Details</summary>
Motivation: To address the long-standing challenge in continuous-action reinforcement learning of efficiently optimizing expressive diffusion or flow policies using Q-functions, overcoming issues such as numerical instability and bias in policy formulation.

Method: The proposed method, QAM (Q-learning with Adjoint Matching), utilizes adjoint matching techniques to create a stable step-wise objective function for policy optimization without requiring error-prone backpropagation. It also integrates temporal-difference learning for critic updates.

Result: QAM achieves superior performance compared to existing methods on challenging tasks with sparse rewards in both offline and offline-to-online reinforcement learning settings.

Conclusion: QAM resolves significant limitations of prior RL approaches by providing stable, unbiased, and expressive policy optimization, paving the way for improved results in continuous-action RL scenarios.

Abstract: We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.

</details>


### [651] [Controlling Underestimation Bias in Constrained Reinforcement Learning for Safe Exploration](https://arxiv.org/abs/2601.11953)
*Shiqing Gao,Jiaxin Ding,Luoyi Fu,Xinbing Wang*

Main category: cs.LG

TL;DR: The paper introduces MICE, a method for reducing constraint violations in Constrained Reinforcement Learning by using memory-inspired intrinsic cost estimation based on unsafe states.


<details>
  <summary>Details</summary>
Motivation: To address the issue of significant constraint violations that occur during training in Constrained Reinforcement Learning, limiting its safety-critical applications.

Method: The proposed MICE method uses a memory module that stores unsafe states to assign intrinsic costs based on pseudo-counts, combined with a corrected cost value function and trust-region optimization.

Result: Experiments showed MICE significantly reduces constraint violations while maintaining policy performance comparable to existing baseline methods.

Conclusion: MICE effectively addresses underestimation issues in cost value functions, enabling safer and more reliable Constrained Reinforcement Learning.

Abstract: Constrained Reinforcement Learning (CRL) aims to maximize cumulative rewards while satisfying constraints. However, existing CRL algorithms often encounter significant constraint violations during training, limiting their applicability in safety-critical scenarios. In this paper, we identify the underestimation of the cost value function as a key factor contributing to these violations. To address this issue, we propose the Memory-driven Intrinsic Cost Estimation (MICE) method, which introduces intrinsic costs to mitigate underestimation and control bias to promote safer exploration. Inspired by flashbulb memory, where humans vividly recall dangerous experiences to avoid risks, MICE constructs a memory module that stores previously explored unsafe states to identify high-cost regions. The intrinsic cost is formulated as the pseudo-count of the current state visiting these risk regions. Furthermore, we propose an extrinsic-intrinsic cost value function that incorporates intrinsic costs and adopts a bias correction strategy. Using this function, we formulate an optimization objective within the trust region, along with corresponding optimization methods. Theoretically, we provide convergence guarantees for the proposed cost value function and establish the worst-case constraint violation for the MICE update. Extensive experiments demonstrate that MICE significantly reduces constraint violations while preserving policy performance comparable to baselines.

</details>


### [652] [Data-centric Prompt Tuning for Dynamic Graphs](https://arxiv.org/abs/2601.11954)
*Yufei Peng,Cheng Yang,Zhengjie Fan,Chuan Shi*

Main category: cs.LG

TL;DR: The paper introduces DDGPrompt, a data-centric prompting framework for dynamic graphs, addressing performance issues in few-shot scenarios with traditional approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve dynamic graph embeddings adapting to diverse few-shot downstream tasks, overcoming performance limitations of traditional pre-training methods.

Method: The authors propose DDGPrompt, which defines a node expression feature matrix to encode temporal and structural information and utilizes three prompt matrices (temporal bias, edge weight, feature mask) for task-specific adaptation.

Result: Significant improvements in performance were demonstrated on four public dynamic graph datasets under few-shot and cold-start conditions, surpassing existing methods.

Conclusion: DDGPrompt effectively refines node embeddings for dynamic graphs, providing improved generalization across diverse tasks and few-shot learning settings.

Abstract: Dynamic graphs have attracted increasing attention due to their ability to model complex and evolving relationships in real-world scenarios. Traditional approaches typically pre-train models using dynamic link prediction and directly apply the resulting node temporal embeddings to specific downstream tasks. However, the significant differences among downstream tasks often lead to performance degradation, especially under few-shot settings. Prompt tuning has emerged as an effective solution to this problem. Existing prompting methods are often strongly coupled with specific model architectures or pretraining tasks, which makes it difficult to adapt to recent or future model designs. Moreover, their exclusive focus on modifying node or temporal features while neglecting spatial structural information leads to limited expressiveness and degraded performance. To address these limitations, we propose DDGPrompt, a data-centric prompting framework designed to effectively refine pre-trained node embeddings at the input data level, enabling better adaptability to diverse downstream tasks. We first define a unified node expression feature matrix that aggregates all relevant temporal and structural information of each node, ensuring compatibility with a wide range of dynamic graph models. Then, we introduce three prompt matrices (temporal bias, edge weight, and feature mask) to adjust the feature matrix completely, achieving task-specific adaptation of node embeddings. We evaluate DDGPrompt under a strict few-shot setting on four public dynamic graph datasets. Experimental results demonstrate that our method significantly outperforms traditional methods and prompting approaches in scenarios with limited labels and cold-start conditions.

</details>


### [653] [R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning](https://arxiv.org/abs/2601.11960)
*Jingchu Wang,Bingbing Xu,Yige Yuan,Bin Xie,Xiaoqian Sun,Huawei Shen*

Main category: cs.LG

TL;DR: The paper proposes a method called R$^2$PO to decouple training trajectories from inference responses in reinforcement learning for improving large language model (LLM) reasoning, enhancing both exploration and stability.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning methods face an objective conflict: they use a single policy for both inference responses and training trajectories, leading to insufficient exploration and harming reasoning capabilities.

Method: The proposed R$^2$PO method introduces a Residual Rollout-Head atop the policy, which decouples training optimization trajectories from inference responses. This allows for controlled trajectory diversification during training while maintaining stable inference generation.

Result: The method outperforms baselines consistently, with accuracy gains of 3.1% on MATH-500 and 2.4% on APPS benchmarks. It also reduces formatting errors and mitigates length biases for stable optimization.

Conclusion: R$^2$PO effectively improves reasoning performance in LLMs by resolving the objective conflict in reinforcement learning. The method enhances exploration and inference stability, offering a practical and efficient improvement over existing approaches.

Abstract: Reinforcement learning has become a central paradigm for improving LLM reasoning. However, existing methods use a single policy to produce both inference responses and training optimization trajectories. The objective conflict between generating stable inference responses and diverse training trajectories leads to insufficient exploration, which harms reasoning capability. In this paper, to address the problem, we propose R$^2$PO (Residual Rollout Policy Optimization), which introduces a lightweight Residual Rollout-Head atop the policy to decouple training trajectories from inference responses, enabling controlled trajectory diversification during training while keeping inference generation stable. Experiments across multiple benchmarks show that our method consistently outperforms baselines, achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS, while also reducing formatting errors and mitigating length bias for stable optimization. Our code is publicly available at https://github.com/RRPO-ARR/Code.

</details>


### [654] [One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints](https://arxiv.org/abs/2601.11977)
*Ren He,Yinliang Xu,Jinfeng Wang,Jeremy Watson,Jian Song*

Main category: cs.LG

TL;DR: The paper proposes MoE-Encoder, a module for improving forecasting models by leveraging a sparse mixture-of-experts approach to enhance accuracy and facilitate privacy-aware federated learning.


<details>
  <summary>Details</summary>
Motivation: Current forecasting methods struggle with complex dependencies, privacy constraints, and lack generalizability across diverse power system scenarios. The zero-shot capabilities of pretrained models on specialized tasks are limited, demanding novel solutions.

Method: The MoE-Encoder injects a sparse mixture-of-experts layer between tokenization and encoding in pretrained models, enabling transformation of multivariate forecasting into expert-guided univariate tasks and supporting localized training in federated scenarios.

Result: Experiments on public multivariate datasets show significant improvements in forecasting accuracy with MoE-Encoder over baselines. In federated environments, transferring only MoE-Encoder parameters efficiently adapts models to new regions with minimal performance loss.

Conclusion: MoE-Encoder is a scalable, privacy-conscious enhancement to time series models, effectively capturing variable relationships and supporting federated learning with lightweight parameter sharing.

Abstract: Forecasting in power systems often involves multivariate time series with complex dependencies and strict privacy constraints across regions. Traditional forecasting methods require significant expert knowledge and struggle to generalize across diverse deployment scenarios. Recent advancements in pre-trained time series models offer new opportunities, but their zero-shot performance on domain-specific tasks remains limited. To address these challenges, we propose a novel MoE Encoder module that augments pretrained forecasting models by injecting a sparse mixture-of-experts layer between tokenization and encoding. This design enables two key capabilities: (1) trans forming multivariate forecasting into an expert-guided univariate task, allowing the model to effectively capture inter-variable relations, and (2) supporting localized training and lightweight parameter sharing in federated settings where raw data cannot be exchanged. Extensive experiments on public multivariate datasets demonstrate that MoE-Encoder significantly improves forecasting accuracy compared to strong baselines. We further simulate federated environments and show that transferring only MoE-Encoder parameters allows efficient adaptation to new regions, with minimal performance degradation. Our findings suggest that MoE-Encoder provides a scalable and privacy-aware extension to foundation time series models.

</details>


### [655] [Extreme Value Policy Optimization for Safe Reinforcement Learning](https://arxiv.org/abs/2601.12008)
*Shiqing Gao,Yihang Zhou,Shuai Shao,Haoyu Luo,Yiheng Bing,Jiaxin Ding,Luoyi Fu,Xinbing Wang*

Main category: cs.LG

TL;DR: The paper introduces the Extreme Value policy Optimization (EVO) algorithm, leveraging Extreme Value Theory to minimize rare, high-impact constraint violations in Constrained Reinforcement Learning.


<details>
  <summary>Details</summary>
Motivation: Applying Reinforcement Learning in real-world scenarios requires ensuring safety and adhering to constraints. Existing methods focusing on expectation-based constraints fail to address rare but extreme incidents, risking significant safety violations.

Method: The EVO algorithm utilizes Extreme Value Theory to model extreme reward and cost samples. It introduces extreme quantile optimization and prioritizes rare, high-impact extreme events during training.

Result: EVO achieves strict constraint satisfaction, reduces the probability of extreme constraint violations compared to other methods, and shows lower outcome variance while maintaining competitive policy performance.

Conclusion: EVO effectively reduces rare, high-impact constraint violations while achieving competitive performance, showcasing its potential for safer Reinforcement Learning in real-world applications.

Abstract: Ensuring safety is a critical challenge in applying Reinforcement Learning (RL) to real-world scenarios. Constrained Reinforcement Learning (CRL) addresses this by maximizing returns under predefined constraints, typically formulated as the expected cumulative cost. However, expectation-based constraints overlook rare but high-impact extreme value events in the tail distribution, such as black swan incidents, which can lead to severe constraint violations. To address this issue, we propose the Extreme Value policy Optimization (EVO) algorithm, leveraging Extreme Value Theory (EVT) to model and exploit extreme reward and cost samples, reducing constraint violations. EVO introduces an extreme quantile optimization objective to explicitly capture extreme samples in the cost tail distribution. Additionally, we propose an extreme prioritization mechanism during replay, amplifying the learning signal from rare but high-impact extreme samples. Theoretically, we establish upper bounds on expected constraint violations during policy updates, guaranteeing strict constraint satisfaction at a zero-violation quantile level. Further, we demonstrate that EVO achieves a lower probability of constraint violations than expectation-based methods and exhibits lower variance than quantile regression methods. Extensive experiments show that EVO significantly reduces constraint violations during training while maintaining competitive policy performance compared to baselines.

</details>


### [656] [Why Loss Re-weighting Works If You Stop Early: Training Dynamics of Unconstrained Features](https://arxiv.org/abs/2601.12011)
*Yize Zhao,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: Loss reweighting in deep learning aids early training but doesn't impact final phases in highly complex datasets. A proposed small-scale model (SSM) analyzes imbalance learning within spectral components.


<details>
  <summary>Details</summary>
Motivation: Modern deep learning suffers from imbalanced learning dynamics, as majority class features are learned before minorities. Loss reweighting could improve this imbalance but effects need further investigation.

Method: The authors introduce a small-scale model (SSM) to abstract complexities while maintaining spectral imbalances, analyzing effects of loss reweighting on training dynamics.

Result: Loss reweighting corrected imbalance by enabling simultaneous learning of majority and minority features early in training.

Conclusion: Loss reweighting improves early training balance in deep learning, ensuring equitable learning dynamics between majority and minority classes.

Abstract: The application of loss reweighting in modern deep learning presents a nuanced picture. While it fails to alter the terminal learning phase in overparameterized deep neural networks (DNNs) trained on high-dimensional datasets, empirical evidence consistently shows it offers significant benefits early in training. To transparently demonstrate and analyze this phenomenon, we introduce a small-scale model (SSM). This model is specifically designed to abstract the inherent complexities of both the DNN architecture and the input data, while maintaining key information about the structure of imbalance within its spectral components. On the one hand, the SSM reveals how vanilla empirical risk minimization preferentially learns to distinguish majority classes over minorities early in training, consequently delaying minority learning. In stark contrast, reweighting restores balanced learning dynamics, enabling the simultaneous learning of features associated with both majorities and minorities.

</details>


### [657] [CooperLLM: Cloud-Edge-End Cooperative Federated Fine-tuning for LLMs via ZOO-based Gradient Correction](https://arxiv.org/abs/2601.12917)
*He Sun,Jinrui Zhou,Li Li,Mingjun Xiao*

Main category: cs.LG

TL;DR: CooperLLM is a federated fine-tuning framework that enables efficient personalization of LLMs on mobile devices by combining lightweight ZOO updates with cloud-based gradient rectification, achieving significant improvements in memory, convergence, and accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of fine-tuning LLMs on resource-constrained mobile devices for privacy-preserving personalization, with current methods facing limitations in memory costs and optimization performance.

Method: CooperLLM employs a cloud-assisted edge-end cooperative framework where mobile devices use lightweight ZOO updates on private data and the cloud performs gradient rectification using public auxiliary data, alongside adaptive compression and pipeline scheduling to optimize performance.

Result: Experiments show CooperLLM reduces on-device memory usage by up to 86.4%, accelerates convergence by 8.8 times, and improves accuracy by up to 10 percentage points compared to state-of-the-art methods.

Conclusion: CooperLLM effectively enhances the feasibility of LLM personalization on mobile devices by addressing system and optimization bottlenecks, achieving superior performance while maintaining privacy.

Abstract: Large Language Models (LLMs) perform well on many NLP tasks, but fine-tuning them on resource-constrained mobile devices is challenging due to high memory and computation costs, despite growing demands for privacy-preserving personalization. Federated Learning (FL) enables local-data training, yet existing methods either rely on memory-intensive backpropagation or use zeroth-order optimization (ZOO), which avoids backward passes but suffers from slow convergence and degraded accuracy. We propose CooperLLM, a cloud-assisted edge-end cooperative federated fine-tuning framework that combines ZOO on mobile devices with cloud-guided gradient rectification. Mobile clients perform lightweight ZOO updates on private data, while the cloud fine-tunes on auxiliary public data using backpropagation and injects guided perturbations to rectify local updates, improving convergence and accuracy without violating privacy. To address system bottlenecks, CooperLLM introduces pipeline scheduling and adaptive compression to overlap computation and communication and reduce memory usage. Experiments on multiple Transformer models and datasets show that CooperLLM reduces on-device memory by up to $86.4\%$, accelerates convergence by $8.8 \times$, and improves accuracy by up to 10 percentage points over state-of-the-art ZOO-based baselines.

</details>


### [658] [Learning to Factorize and Adapt: A Versatile Approach Toward Universal Spatio-Temporal Foundation Models](https://arxiv.org/abs/2601.12083)
*Siru Zhong,Junjie Qiu,Yangyu Wu,Yiqiu Liu,Yuanpeng He,Zhongwen Rao,Bin Yang,Chenjuan Guo,Hao Xu,Yuxuan Liang*

Main category: cs.LG

TL;DR: The paper introduces FactoST-v2, a framework for spatio-temporal models that efficiently separates temporal learning from spatial adaptation for better generalization and performance.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges and heterogeneity issues in developing joint spatio-temporal models for cross-dataset generalization.

Method: The framework decouples the learning process into two stages: minimal temporal pretraining using randomized sequence masking and efficient spatial adaptation through meta adaptive learning and prompting.

Result: FactoST-v2 achieves state-of-the-art accuracy in zero-shot and few-shot settings with linear efficiency, outperforming existing models while maintaining scalability.

Conclusion: The factorized approach of FactoST-v2 facilitates scalable and practical development of universal spatio-temporal foundation models, demonstrating strong generalization and efficiency.

Abstract: Spatio-Temporal (ST) Foundation Models (STFMs) promise cross-dataset generalization, yet joint ST pretraining is computationally expensive and grapples with the heterogeneity of domain-specific spatial patterns. Substantially extending our preliminary conference version, we present FactoST-v2, an enhanced factorized framework redesigned for full weight transfer and arbitrary-length generalization. FactoST-v2 decouples universal temporal learning from domain-specific spatial adaptation. The first stage pretrains a minimalist encoder-only backbone using randomized sequence masking to capture invariant temporal dynamics, enabling probabilistic quantile prediction across variable horizons. The second stage employs a streamlined adapter to rapidly inject spatial awareness via meta adaptive learning and prompting. Comprehensive evaluations across diverse domains demonstrate that FactoST-v2 achieves state-of-the-art accuracy with linear efficiency - significantly outperforming existing foundation models in zero-shot and few-shot scenarios while rivaling domain-specific expert baselines. This factorized paradigm offers a practical, scalable path toward truly universal STFMs. Code is available at https://github.com/CityMind-Lab/FactoST.

</details>


### [659] [Mitigating Cultural Bias in LLMs via Multi-Agent Cultural Debate](https://arxiv.org/abs/2601.12091)
*Qian Tan,Lei Jiang,Yuting Zeng,Shuoyang Ding,Xiaohua Xu*

Main category: cs.LG

TL;DR: This paper examines methods to evaluate and mitigate cultural biases in large language models (LLMs) using bilingual benchmarks and multi-agent frameworks.


<details>
  <summary>Details</summary>
Motivation: Current large language models demonstrate Western-centric biases, and the effectiveness of non-Western prompts in addressing these biases remains unclear.

Method: The paper introduces CEBiasBench for evaluating cultural bias and Multi-Agent Cultural Debate (MACD) for mitigating biases by assigning agents distinct cultural personas for deliberation, ensuring fairness.

Result: Chinese prompting shifts biases rather than removing them, and the MACD framework significantly improves the No Bias Rate across benchmarks, including Arabic CAMeL.

Conclusion: Explicit cultural representation in agent frameworks is vital for cross-cultural fairness in large language models.

Abstract: Large language models (LLMs) exhibit systematic Western-centric bias, yet whether prompting in non-Western languages (e.g., Chinese) can mitigate this remains understudied. Answering this question requires rigorous evaluation and effective mitigation, but existing approaches fall short on both fronts: evaluation methods force outputs into predefined cultural categories without a neutral option, while mitigation relies on expensive multi-cultural corpora or agent frameworks that use functional roles (e.g., Planner--Critique) lacking explicit cultural representation. To address these gaps, we introduce CEBiasBench, a Chinese--English bilingual benchmark, and Multi-Agent Vote (MAV), which enables explicit ``no bias'' judgments. Using this framework, we find that Chinese prompting merely shifts bias toward East Asian perspectives rather than eliminating it. To mitigate such persistent bias, we propose Multi-Agent Cultural Debate (MACD), a training-free framework that assigns agents distinct cultural personas and orchestrates deliberation via a "Seeking Common Ground while Reserving Differences" strategy. Experiments demonstrate that MACD achieves 57.6% average No Bias Rate evaluated by LLM-as-judge and 86.0% evaluated by MAV (vs. 47.6% and 69.0% baseline using GPT-4o as backbone) on CEBiasBench and generalizes to the Arabic CAMeL benchmark, confirming that explicit cultural representation in agent frameworks is essential for cross-cultural fairness.

</details>


### [660] [Federated Learning Under Temporal Drift -- Mitigating Catastrophic Forgetting via Experience Replay](https://arxiv.org/abs/2601.13456)
*Sahasra Kokkula,Daniel David,Aaditya Baruah*

Main category: cs.LG

TL;DR: The paper addresses the issue of catastrophic forgetting in Federated Learning during temporal concept drift, proposing client-side experience replay to prevent forgetting.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of catastrophic forgetting in federated learning systems under seasonal drifts in client data distributions.

Method: The paper introduces client-side experience replay, where each client keeps a small buffer of previous samples and mixes them with current data for local training, while maintaining standard server-side aggregation in FedAvg.

Result: The proposed method improves performance on Fashion-MNIST, restoring accuracy from 28% to 78-82% with a small memory buffer while demonstrating a memory-accuracy trade-off.

Conclusion: Client-side experience replay is an effective and simple method to counteract catastrophic forgetting in Federated Learning without altering the server aggregation process.

Abstract: Federated Learning struggles under temporal concept drift where client data distributions shift over time. We demonstrate that standard FedAvg suffers catastrophic forgetting under seasonal drift on Fashion-MNIST, with accuracy dropping from 74% to 28%. We propose client-side experience replay, where each client maintains a small buffer of past samples mixed with current data during local training. This simple approach requires no changes to server aggregation. Experiments show that a 50-sample-per-class buffer restores performance to 78-82%, effectively preventing forgetting. Our ablation study reveals a clear memory-accuracy trade-off as buffer size increases.

</details>


### [661] [PTL-PINNs: Perturbation-Guided Transfer Learning with Physics- Informed Neural Networks for Nonlinear Systems](https://arxiv.org/abs/2601.12093)
*Duarte Alexandrino,Ben Moseley,Pavlos Protopapas*

Main category: cs.LG

TL;DR: The paper presents PTL-PINNs, combining perturbation theory and transfer learning to enhance PINNs, achieving faster and accurate solutions for nonlinear equations.


<details>
  <summary>Details</summary>
Motivation: Current Physics-Informed Neural Networks (PINNs) face limitations in efficiently modeling nonlinear dynamics, including long training times and poor generalization.

Method: A novel perturbation-guided transfer learning framework (PTL-PINN) integrates perturbation theory to approximate linear systems and leverages transfer learning for rapid computations.

Result: PTL-PINNs achieve comparable accuracy to Runge-Kutta methods and provide up to an order of magnitude faster computational speeds while solving complex nonlinear differential problems.

Conclusion: Connecting perturbation theory with PINNs enhances their capability to solve nonlinear systems efficiently, extending their applicability in scientific and engineering domains.

Abstract: Accurately and efficiently solving nonlinear differential equations is crucial for modeling dynamic behavior across science and engineering. Physics-Informed Neural Networks (PINNs) have emerged as a powerful solution that embeds physical laws in training by enforcing equation residuals. However, these struggle to model nonlinear dynamics, suffering from limited generalization across problems and long training times. To address these limitations, we propose a perturbation-guided transfer learning framework for PINNs (PTL-PINN), which integrates perturbation theory with transfer learning to efficiently solve nonlinear equations. Unlike gradient-based transfer learning, PTL-PINNs solve an approximate linear perturbative system using closed-form expressions, enabling rapid generalization with the time complexity of matrix-vector multiplication. We show that PTL-PINNs achieve accuracy comparable to various Runge-Kutta methods, with computational speeds up to one order of magnitude faster. To benchmark performance, we solve a broad set of problems, including nonlinear oscillators across various damping regimes, the equilibrium-centered Lotka-Volterra system, the KPP-Fisher and the Wave equation. Since perturbation theory sets the accuracy bound of PTL-PINNs, we systematically evaluate its practical applicability. This work connects long-standing perturbation methods with PINNs, demonstrating how perturbation theory can guide foundational models to solve nonlinear systems with speeds comparable to those of classical solvers.

</details>


### [662] [Neural Isomorphic Fields: A Transformer-based Algebraic Numerical Embedding](https://arxiv.org/abs/2601.12095)
*Hamidreza Sadeghi,Saeedeh Momtazi,Reza Safabakhsh*

Main category: cs.LG

TL;DR: The paper introduces a neural method to embed numbers into fixed-length vectors to tackle numerical instabilities, demonstrating strong addition performance but highlighting challenges with multiplication.


<details>
  <summary>Details</summary>
Motivation: Address issues in neural networks related to overflow, underflow, and unstable variations when processing very small or very large numbers.

Method: Proposes embedding numbers as fixed-length vectors while preserving key algebraic operations through a Neural Isomorphic Field.

Result: Shows over 95% accuracy for addition-related algebraic tests but lower accuracy (53%-73%) for multiplication properties.

Conclusion: The approach is effective for addition-related operations but requires improvement for better handling of multiplication.

Abstract: Neural network models often face challenges when processing very small or very large numbers due to issues such as overflow, underflow, and unstable output variations. To mitigate these problems, we propose using embedding vectors for numbers instead of directly using their raw values. These embeddings aim to retain essential algebraic properties while preventing numerical instabilities. In this paper, we introduce, for the first time, a fixed-length number embedding vector that preserves algebraic operations, including addition, multiplication, and comparison, within the field of rational numbers. We propose a novel Neural Isomorphic Field, a neural abstraction of algebraic structures such as groups and fields. The elements of this neural field are embedding vectors that maintain algebraic structure during computations. Our experiments demonstrate that addition performs exceptionally well, achieving over 95 percent accuracy on key algebraic tests such as identity, closure, and associativity. In contrast, multiplication exhibits challenges, with accuracy ranging from 53 percent to 73 percent across various algebraic properties. These findings highlight the model's strengths in preserving algebraic properties under addition while identifying avenues for further improvement in handling multiplication.

</details>


### [663] [SynQP: A Framework and Metrics for Evaluating the Quality and Privacy Risk of Synthetic Data](https://arxiv.org/abs/2601.12124)
*Bing Hu,Yixin Li,Asma Bahamyirou,Helen Chen*

Main category: cs.LG

TL;DR: The paper introduces SynQP, an open framework for evaluating privacy risks in synthetic data generation using simulated sensitive data. It also proposes a new identity disclosure risk metric for improved privacy evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address privacy concerns and the shortage of accessible benchmarking frameworks in synthetic data generation, especially in health applications.

Method: The method includes the development of SynQP to benchmark privacy risks using simulated sensitive data while keeping original data confidential. It also proposes a new identity disclosure risk metric.

Result: The evaluations using SynQP show that privacy measures like differential privacy (DP) reduce identity disclosure risk and membership-inference attack risks below regulatory thresholds, ensuring safer synthetic data use.

Conclusion: SynQP enhances transparency and reliability in privacy evaluations, encouraging safer adoption of synthetic data in sensitive applications like healthcare.

Abstract: The use of synthetic data in health applications raises privacy concerns, yet the lack of open frameworks for privacy evaluations has slowed its adoption. A major challenge is the absence of accessible benchmark datasets for evaluating privacy risks, due to difficulties in acquiring sensitive data. To address this, we introduce SynQP, an open framework for benchmarking privacy in synthetic data generation (SDG) using simulated sensitive data, ensuring that original data remains confidential. We also highlight the need for privacy metrics that fairly account for the probabilistic nature of machine learning models. As a demonstration, we use SynQP to benchmark CTGAN and propose a new identity disclosure risk metric that offers a more accurate estimation of privacy risks compared to existing approaches. Our work provides a critical tool for improving the transparency and reliability of privacy evaluations, enabling safer use of synthetic data in health-related applications. % In our quality evaluations, non-private models achieved near-perfect machine-learning efficacy \(\ge0.97\). Our privacy assessments (Table II) reveal that DP consistently lowers both identity disclosure risk (SD-IDR) and membership-inference attack risk (SD-MIA), with all DP-augmented models staying below the 0.09 regulatory threshold. Code available at https://github.com/CAN-SYNH/SynQP

</details>


### [664] [SolarGPT-QA: A Domain-Adaptive Large Language Model for Educational Question Answering in Space Weather and Heliophysics](https://arxiv.org/abs/2601.12131)
*Santosh Chapagain,MohammadReza EskandariNasab,Onur Vural,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: The paper introduces SolarGPT-QA, a specialized question-answering AI for space weather and heliophysics, designed to improve forecasting and education by adapting language models with domain-specific and pedagogical training.


<details>
  <summary>Details</summary>
Motivation: Extreme solar activities cause significant harm to critical infrastructures, highlighting the need for accurate forecasting and improved education in space science.

Method: SolarGPT-QA was built on the LLaMA-3 model, with domain adaptation via scientific literature and question-answering datasets, refined for educational clarity using storytelling techniques.

Result: SolarGPT-QA surpasses general-purpose models in zero-shot settings and offers competitive educational explanations, confirmed by human evaluations and a student comprehension pilot.

Conclusion: Domain-adaptive pretraining combined with pedagogical fine-tuning enhances the balance between scientific accuracy and educational effectiveness, paving the way for broader SolarGPT applications.

Abstract: Solar activity, including solar flares, coronal mass ejections (CMEs), and geomagnetic storms, can significantly impact satellites, aviation, power grids, data centers, and space missions. Extreme solar events can cause substantial economic damage if not predicted in advance, highlighting the importance of accurate forecasting and effective education in space science. Although large language models (LLMs) perform well on general tasks, they often lack domain-specific knowledge and pedagogical capability to clearly explain complex space science concepts.
  We introduce SolarGPT-QA, a question answering system based on a domain-adapted large language model built on the LLaMA-3 base model. The model is trained using scientific literature and large-scale question-answer data generated with GPT-4 and refined using Grok-3 in a student-friendly storytelling style. Human pairwise evaluations show that SolarGPT-QA outperforms general-purpose models in zero-shot settings and achieves competitive performance compared to instruction-tuned models for educational explanations in space weather and heliophysics. A small pilot student comprehension study further suggests improved clarity and accessibility of the generated explanations. Ablation experiments indicate that combining domain-adaptive pretraining with pedagogical fine-tuning is important for balancing scientific accuracy and educational effectiveness. This work represents an initial step toward a broader SolarGPT framework for space science education and forecasting.

</details>


### [665] [EMoE: Eigenbasis-Guided Routing for Mixture-of-Experts](https://arxiv.org/abs/2601.12137)
*Anzhe Cheng,Shukai Duan,Shixuan Li,Chenzhong Yin,Mingxi Cheng,Shahin Nazarian,Paul Thompson,Paul Bogdan*

Main category: cs.LG

TL;DR: The paper introduces the Eigen-Mixture-of-Experts (EMoE), a model addressing computational inefficiencies in Mixture-of-Experts (MoE) architectures while overcoming load imbalance and expert homogeneity issues.


<details>
  <summary>Details</summary>
Motivation: Deep learning model scaling has created unsustainable computational demands. Mixture-of-Experts architectures show potential for efficiency but suffer from load imbalance and expert redundancy problems.

Method: The EMoE model employs a routing mechanism based on a learned orthonormal eigenbasis, projecting tokens onto a shared basis and routing them by alignment with principal components, removing the need for auxiliary loss functions.

Result: EMoE improves expert utilization and promotes diverse specialization while achieving a balanced load and eliminating homogeneity issues.

Conclusion: The EMoE architecture offers a principled solution to critical MoE problems, enhancing efficiency and diversity without auxiliary losses, marking a promising advancement in deep learning.

Abstract: The relentless scaling of deep learning models has led to unsustainable computational demands, positioning Mixture-of-Experts (MoE) architectures as a promising path towards greater efficiency. However, MoE models are plagued by two fundamental challenges: 1) a load imbalance problem known as the``rich get richer" phenomenon, where a few experts are over-utilized, and 2) an expert homogeneity problem, where experts learn redundant representations, negating their purpose. Current solutions typically employ an auxiliary load-balancing loss that, while mitigating imbalance, often exacerbates homogeneity by enforcing uniform routing at the expense of specialization. To resolve this, we introduce the Eigen-Mixture-of-Experts (EMoE), a novel architecture that leverages a routing mechanism based on a learned orthonormal eigenbasis. EMoE projects input tokens onto this shared eigenbasis and routes them based on their alignment with the principal components of the feature space. This principled, geometric partitioning of data intrinsically promotes both balanced expert utilization and the development of diverse, specialized experts, all without the need for a conflicting auxiliary loss function. Our code is publicly available at https://github.com/Belis0811/EMoE.

</details>


### [666] [Threshold Differential Attention for Sink-Free, Ultra-Sparse, and Non-Dispersive Language Modeling](https://arxiv.org/abs/2601.12145)
*Xingyue Huang,Xueying Ding,Mingxuan Ju,Yozen Liu,Neil Shah,Tong Zhao*

Main category: cs.LG

TL;DR: Threshold Differential Attention (TDA) addresses long-context softmax attention issues, delivering ultra-sparsity and robustness without computational penalties or noise degradation.


<details>
  <summary>Details</summary>
Motivation: Softmax attention struggles with structural limitations like attention sinks and mass dispersion in long contexts, leading to inefficiencies.

Method: TDA employs row-wise extreme-value thresholding with length-dependent gates, and subtracts inhibitory views, resulting in sparse and sink-free attention mechanisms.

Result: TDA achieves over 99% exact zeros, removes attention sinks, and maintains competitive performance across standard and long-context benchmarks.

Conclusion: TDA effectively enhances attention mechanisms for long contexts, delivering sparsity, robustness, and reliable results without added computational burden or noise issues.

Abstract: Softmax attention struggles with long contexts due to structural limitations: the strict sum-to-one constraint forces attention sinks on irrelevant tokens, and probability mass disperses as sequence lengths increase. We tackle these problems with Threshold Differential Attention (TDA), a sink-free attention mechanism that achieves ultra-sparsity and improved robustness at longer sequence lengths without the computational overhead of projection methods or the performance degradation caused by noise accumulation of standard rectified attention. TDA applies row-wise extreme-value thresholding with a length-dependent gate, retaining only exceedances. Inspired by the differential transformer, TDA also subtracts an inhibitory view to enhance expressivity. Theoretically, we prove that TDA controls the expected number of spurious survivors per row to $O(1)$ and that consensus spurious matches across independent views vanish as context grows. Empirically, TDA produces $>99\%$ exact zeros and eliminates attention sinks while maintaining competitive performance on standard and long-context benchmarks.

</details>


### [667] [Speculative Sampling with Reinforcement Learning](https://arxiv.org/abs/2601.12212)
*Chenan Wang,Daniel H. Shi,Haipeng Chen*

Main category: cs.LG

TL;DR: The paper introduces Re-SpS, a reinforcement learning framework to optimize speculative sampling in large language models (LLMs), achieving faster inference speed compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of high latency during inference time in real-world applications of large language models (LLMs), especially when state-of-the-art speculative sampling methods use static configurations, limiting adaptability and efficiency.

Method: The paper presents Reinforcement learning for Speculative Sampling (Re-SpS), an RL-based framework that optimizes draft tree hyperparameters dynamically using efficient state representations and multi-step action persistence, facilitating better context-aware adaptation.

Result: Re-SpS demonstrates significant performance improvements over existing methods, achieving up to 5.45× speedup over the backbone LLM and 1.12× speedup compared to the state-of-the-art EAGLE-3 method, across five benchmarks, with no deterioration in output quality.

Conclusion: Re-SpS effectively balances speculative aggression and computational overhead, showcasing its capability to enhance generation speed without compromising the consistency or fidelity of outputs.

Abstract: Inference time latency has remained an open challenge for real world applications of large language models (LLMs). State-of-the-art (SOTA) speculative sampling (SpS) methods for LLMs, like EAGLE-3, use tree-based drafting to explore multiple candidate continuations in parallel. However, the hyperparameters controlling the tree structure are static, which limits flexibility and efficiency across diverse contexts and domains. We introduce Reinforcement learning for Speculative Sampling (Re-SpS), the first reinforcement learning (RL)-based framework for draft tree hyperparameter optimization. Re-SpS dynamically adjusts draft tree hyperparameters in real-time, learning context-aware policies that maximize generation speed by balancing speculative aggression with computational overhead. It leverages efficient state representations from target model hidden states and introduces multi-step action persistence for better context modeling. Evaluation results across five diverse benchmarks demonstrate consistent improvements over the SOTA method EAGLE-3, achieving up to 5.45$\times$ speedup over the backbone LLM and up to 1.12$\times$ speedup compared to EAGLE-3 across five diverse benchmarks, with no loss in output fidelity.

</details>


### [668] [Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models](https://arxiv.org/abs/2601.12215)
*Megha Thukral,Cyrus Tanade,Simon A. Lee,Juhyeon Lee,Hao Zhou,Keum San Chun,Migyeong Gwak,Viswam Nathan,Md Mahbubur Rahman,Li Zhu,Mehrab Bin Morshed,Subramaniam Venkatraman,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: This paper proposes a new self-supervised pretraining method for wearable foundation models using PPG data, which focuses on multiscale spectral representation and achieves state-of-the-art performance in health-related tasks.


<details>
  <summary>Details</summary>
Motivation: Wearable foundation models for digital health can benefit significantly from large-scale biosignal data. However, current methods neglect the hierarchical spectral structure of PPG signals, which is vital for health-related tasks.

Method: The paper introduces Masked Multiscale Reconstruction (MMR), a self-supervised framework which relies on learning from multiscale time-frequency features of PPG signals. Using wavelet-based multiresolution decomposition, masked coefficients are reconstructed to force integration of temporal and spectral information.

Result: MMR is pretrained on ~17 million PPG segments from smartwatches. It achieves better or comparable performance on 17 out of 19 health-related tasks compared to existing approaches, demonstrating robust embeddings and physiologically-grounded features.

Conclusion: MMR underscores the importance of exploiting multiresolution features for robust PPG representation learning. It performs effectively across diverse health tasks, making it a promising candidate for generalizable PPG foundation models.

Abstract: Wearable foundation models have the potential to transform digital health by learning transferable representations from large-scale biosignals collected in everyday settings. While recent progress has been made in large-scale pretraining, most approaches overlook the spectral structure of photoplethysmography (PPG) signals, wherein physiological rhythms unfold across multiple frequency bands. Motivated by the insight that many downstream health-related tasks depend on multi-resolution features spanning fine-grained waveform morphology to global rhythmic dynamics, we introduce Masked Multiscale Reconstruction (MMR) for PPG representation learning - a self-supervised pretraining framework that explicitly learns from hierarchical time-frequency scales of PPG data. The pretraining task is designed to reconstruct randomly masked out coefficients obtained from a wavelet-based multiresolution decomposition of PPG signals, forcing the transformer encoder to integrate information across temporal and spectral scales. We pretrain our model with MMR using ~17 million unlabeled 10-second PPG segments from ~32,000 smartwatch users. On 17 of 19 diverse health-related tasks, MMR trained on large-scale wearable PPG data improves over or matches state-of-the-art open-source PPG foundation models, time-series foundation models, and other self-supervised baselines. Extensive analysis of our learned embeddings and systematic ablations underscores the value of wavelet-based representations, showing that they capture robust and physiologically-grounded features. Together, these results highlight the potential of MMR as a step toward generalizable PPG foundation models.

</details>


### [669] [Learning Longitudinal Health Representations from EHR and Wearable Data](https://arxiv.org/abs/2601.12227)
*Yuanyun Zhang,Han Zhou,Li Feng,Yilin Hong,Shi Li*

Main category: cs.LG

TL;DR: This paper introduces a multimodal foundation model combining electronic health records (EHR) and wearable data for improved clinical prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with sparse EHR documentation and the lack of semantic grounding in wearable data when modeled separately or fused late.

Method: The proposed model uses modality-specific encoders, a shared temporal backbone, and self-supervised/cross-modal objectives for joint representation.

Result: The model outperforms baselines using EHR-only or wearable-only data, excelling in long horizon forecasts and handling missing data.

Conclusion: Combining EHR and wearable data in pretraining leads to better longitudinal health representations and improves predictive accuracy.

Abstract: Foundation models trained on electronic health records show strong performance on many clinical prediction tasks but are limited by sparse and irregular documentation. Wearable devices provide dense continuous physiological signals but lack semantic grounding. Existing methods usually model these data sources separately or combine them through late fusion. We propose a multimodal foundation model that jointly represents electronic health records and wearable data as a continuous time latent process. The model uses modality specific encoders and a shared temporal backbone pretrained with self supervised and cross modal objectives. This design produces representations that are temporally coherent and clinically grounded. Across forecasting physiological and risk modeling tasks the model outperforms strong electronic health record only and wearable only baselines especially at long horizons and under missing data. These results show that joint electronic health record and wearable pretraining yields more faithful representations of longitudinal health.

</details>


### [670] [Wavelet-Aware Anomaly Detection in Multi-Channel User Logs via Deviation Modulation and Resolution-Adaptive Attention](https://arxiv.org/abs/2601.12231)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Shijie Xu,Guanggang Geng*

Main category: cs.LG

TL;DR: This paper presents an advanced anomaly detection framework designed for insider threat detection by leveraging wavelet transforms and adaptive attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting rare anomalies in multi-channel, non-stationary user activity logs vital for enterprise security.

Method: The method combines deviation-aware modulation to highlight anomalies, discrete wavelet transform for multi-resolution representation, and a resolution-adaptive attention mechanism to prioritize key frequency bands.

Result: Achieved superior performance in terms of precision, recall, and F1 score on the CERT r4.2 benchmark, outperforming existing baselines across multiple scenarios.

Conclusion: The proposed framework effectively tackles challenges in enterprise anomaly detection by integrating wavelet and attention-based techniques, demonstrating robustness and accuracy in detecting insider threats.

Abstract: Insider threat detection is a key challenge in enterprise security, relying on user activity logs that capture rich and complex behavioral patterns. These logs are often multi-channel, non-stationary, and anomalies are rare, making anomaly detection challenging. To address these issues, we propose a novel framework that integrates wavelet-aware modulation, multi-resolution wavelet decomposition, and resolution-adaptive attention for robust anomaly detection. Our approach first applies a deviation-aware modulation scheme to suppress routine behaviors while amplifying anomalous deviations. Next, discrete wavelet transform (DWT) decomposes the log signals into multi-resolution representations, capturing both long-term trends and short-term anomalies. Finally, a learnable attention mechanism dynamically reweights the most discriminative frequency bands for detection. On the CERT r4.2 benchmark, our approach consistently outperforms existing baselines in precision, recall, and F1 score across various time granularities and scenarios.

</details>


### [671] [TimeGMM: Single-Pass Probabilistic Forecasting via Adaptive Gaussian Mixture Models with Reversible Normalization](https://arxiv.org/abs/2601.12288)
*Lei Liu,Tengyuan Liu,Hongwei Zhao,Jiahui Huang,Ruibo Guo,Bin Li*

Main category: cs.LG

TL;DR: This paper introduces TimeGMM, a probabilistic forecasting framework using Gaussian Mixture Models to address limitations in sampling and parametric assumptions, achieving strong performance improvements.


<details>
  <summary>Details</summary>
Motivation: Current probabilistic forecasting methods for time series suffer from computational inefficiencies and limitations in modeling complex future distributions, which hinder predictive accuracy and introduce mismatches.

Method: The authors developed TimeGMM, which combines GMM-based probabilistic modeling with modules like GRIN, a normalization module tailored for distribution shifts, and integrates a Temporal Encoder and Conditional Temporal-Probabilistic Decoder.

Result: TimeGMM outperformed state-of-the-art methods in forecasting metrics, achieving up to 22.48% improvement in CRPS and 21.23% improvement in NMAE across benchmarks.

Conclusion: TimeGMM successfully addresses challenges in time series forecasting by offering a computationally efficient and highly accurate framework for capturing complex future distributions.

Abstract: Probabilistic time series forecasting is crucial for quantifying future uncertainty, with significant applications in fields such as energy and finance. However, existing methods often rely on computationally expensive sampling or restrictive parametric assumptions to characterize future distributions, which limits predictive performance and introduces distributional mismatch. To address these challenges, this paper presents TimeGMM, a novel probabilistic forecasting framework based on Gaussian Mixture Models (GMM) that captures complex future distributions in a single forward pass. A key component is GMM-adapted Reversible Instance Normalization (GRIN), a novel module designed to dynamically adapt to temporal-probabilistic distribution shifts. The framework integrates a dedicated Temporal Encoder (TE-Module) with a Conditional Temporal-Probabilistic Decoder (CTPD-Module) to jointly capture temporal dependencies and mixture distribution parameters. Extensive experiments demonstrate that TimeGMM consistently outperforms state-of-the-art methods, achieving maximum improvements of 22.48\% in CRPS and 21.23\% in NMAE.

</details>


### [672] [Distribution Shift Is Key to Learning Invariant Prediction](https://arxiv.org/abs/2601.12296)
*Hong Zheng,Fei Teng*

Main category: cs.LG

TL;DR: ERM can outperform out-of-distribution algorithms due to distribution shifts, with large shifts improving invariant prediction performance.


<details>
  <summary>Details</summary>
Motivation: To understand why ERM sometimes surpasses algorithms designed for out-of-distribution tasks and investigate the role of distribution shifts in model learning.

Method: The paper introduces theoretical bounds to show how distribution shifts impact prediction ability, and conducts empirical validations to reinforce these findings.

Result: Large distribution shifts enable ERM to perform comparably to invariant prediction models, closely approximating Optimal models under certain conditions.

Conclusion: Distribution shifts are pivotal for leveraging ERM's performance, with larger shifts facilitating better model learning and invariant predictions.

Abstract: An interesting phenomenon arises: Empirical Risk Minimization (ERM) sometimes outperforms methods specifically designed for out-of-distribution tasks. This motivates an investigation into the reasons behind such behavior beyond algorithmic design. In this study, we find that one such reason lies in the distribution shift across training domains. A large degree of distribution shift can lead to better performance even under ERM. Specifically, we derive several theoretical and empirical findings demonstrating that distribution shift plays a crucial role in model learning and benefits learning invariant prediction. Firstly, the proposed upper bounds indicate that the degree of distribution shift directly affects the prediction ability of the learned models. If it is large, the models' ability can increase, approximating invariant prediction models that make stable predictions under arbitrary known or unseen domains; and vice versa. We also prove that, under certain data conditions, ERM solutions can achieve performance comparable to that of invariant prediction models. Secondly, the empirical validation results demonstrated that the predictions of learned models approximate those of Oracle or Optimal models, provided that the degree of distribution shift in the training data increases.

</details>


### [673] [Machine Learning as a Service (MLaaS) Dataset Generator Framework for IoT Environments](https://arxiv.org/abs/2601.12305)
*Deepak Kanneganti,Sajib Mistry,Sheik Fattah,Joshua Boland,Aneesh Krishna*

Main category: cs.LG

TL;DR: This paper introduces MDG, a framework to create realistic and configurable datasets for evaluating Machine Learning as a Service (MLaaS) performance and composition.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic datasets for assessing MLaaS selection and composition across diverse conditions.

Method: MDG simulates MLaaS behavior with multiple model families, datasets, and conditions, recording detailed attributes and metrics. It includes a composition mechanism modeling IoT conditions.

Result: MDG generates over ten thousand MLaaS service instances, builds a large-scale benchmark dataset, and improves selection accuracy and composition quality in experiments.

Conclusion: MDG is a practical tool for advancing systematic research on MLaaS selection and composition, offering extensibility and reproducibility.

Abstract: We propose a novel MLaaS Dataset Generator (MDG) framework that creates configurable and reproducible datasets for evaluating Machine Learning as a Service (MLaaS) selection and composition. MDG simulates realistic MLaaS behaviour by training and evaluating diverse model families across multiple real-world datasets and data distribution settings. It records detailed functional attributes, quality of service metrics, and composition-specific indicators, enabling systematic analysis of service performance and cross-service behaviour. Using MDG, we generate more than ten thousand MLaaS service instances and construct a large-scale benchmark dataset suitable for downstream evaluation. We also implement a built-in composition mechanism that models how services interact under varied Internet of Things conditions. Experiments demonstrate that datasets generated by MDG enhance selection accuracy and composition quality compared to existing baselines. MDG provides a practical and extensible foundation for advancing data-driven research on MLaaS selection and composition

</details>


### [674] [Explanova: Automatically Discover Data Insights in N \times M Table via XAI Combined LLM Workflow](https://arxiv.org/abs/2601.12317)
*Yiming Huang*

Main category: cs.LG

TL;DR: The paper presents Explanova as an automated data analysis framework that uses a preset AutoML-like workflow and a local small LLM for cost efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance automation in data analysis by introducing an affordable and systematic exploration framework using a local small LLM.

Method: The method involves utilizing a preset AutoML-like workflow to explore various data aspects like statistical properties, element relationships, and holistic explanations.

Result: Explanova provides an efficient, systematic framework for data analysis, leveraging a local small LLM to reduce costs while maintaining functionality.

Conclusion: Explanova is a practical alternative for automating fine-grained data analysis, demonstrating cost efficiency and systematic exploration through a structured workflow.

Abstract: Automation in data analysis has been a long-time pursuit. Current agentic LLM shows a promising solution towards it. Like DeepAnalyze, DataSage, and Datawise. They are all powerful agentic frameworks for automatic fine-grained analysis and are powered by LLM-based agentic tool calling ability. However, what about powered by a preset AutoML-like workflow? If we traverse all possible exploration, like Xn itself`s statistics, Xn1-Xn2 relationships, Xn to all other, and finally explain? Our Explanova is such an attempt: Cheaper due to a Local Small LLM.

</details>


### [675] [Ordered Local Momentum for Asynchronous Distributed Learning under Arbitrary Delays](https://arxiv.org/abs/2601.12322)
*Chang-Wei Shi,Shi-Shang Wang,Wu-Jun Li*

Main category: cs.LG

TL;DR: OrLoMo is introduced for asynchronous distributed learning, implementing MSGD with local updates under heterogeneous computing environments, showing better performance than synchronous and other asynchronous approaches.


<details>
  <summary>Details</summary>
Motivation: The need to develop optimizers that improve convergence and generalization for training large-scale models on heterogeneous computing infrastructures.

Method: Introduced the Ordered Local Momentum (OrLoMo), where workers asynchronously perform MSGD and aggregate momentum in order based on global iteration index.

Result: Shows convergence in non-convex scenarios under arbitrary delays and achieves superior performance in experiments compared to alternatives.

Conclusion: OrLoMo effectively addresses asynchronous MSGD with local updates, making it suitable for heterogeneous distributed learning environments.

Abstract: Momentum SGD (MSGD) serves as a foundational optimizer in training deep models due to momentum's key role in accelerating convergence and enhancing generalization. Meanwhile, asynchronous distributed learning is crucial for training large-scale deep models, especially when the computing capabilities of the workers in the cluster are heterogeneous. To reduce communication frequency, local updates are widely adopted in distributed learning. However, how to implement asynchronous distributed MSGD with local updates remains unexplored. To solve this problem, we propose a novel method, called \underline{or}dered \underline{lo}cal \underline{mo}mentum (OrLoMo), for asynchronous distributed learning. In OrLoMo, each worker runs MSGD locally. Then the local momentum from each worker will be aggregated by the server in order based on its global iteration index. To the best of our knowledge, OrLoMo is the first method to implement asynchronous distributed MSGD with local updates. We prove the convergence of OrLoMo for non-convex problems under arbitrary delays. Experiments validate that OrLoMo can outperform its synchronous counterpart and other asynchronous methods.

</details>


### [676] [IceWatch: Forecasting Glacial Lake Outburst Floods (GLOFs) using Multimodal Deep Learning](https://arxiv.org/abs/2601.12330)
*Zuha Fatima,Muhammad Anser Sohaib,Muhammad Talha,Ayesha Kanwal,Sidra Sultana,Nazia Perwaiz*

Main category: cs.LG

TL;DR: IceWatch uses deep learning for real-time prediction of Glacial Lake Outburst Floods (GLOFs) through spatial and temporal data analysis, improving prediction speed and reliability.


<details>
  <summary>Details</summary>
Motivation: Current GLOF detection methods rely on manual analysis and hydrological modeling, facing challenges like slow updates, inaccuracies, and cloud interference.

Method: IceWatch combines Sentinel-2 satellite imagery, physical dynamics, CNN-based spatial classifiers, and temporal data modeling with NASA and MODIS resources for integrated GLOF prediction.

Result: IceWatch demonstrates robust, reliable, and automated prediction of GLOFs, ensuring rapid data processing and multimodal cross-validation methods.

Conclusion: IceWatch introduces a scalable GLOF warning system, integrating sophisticated machine learning techniques for improved global glacier monitoring and real-time predictive capabilities.

Abstract: Glacial Lake Outburst Floods (GLOFs) pose a serious threat in high mountain regions. They are hazardous to communities, infrastructure, and ecosystems further downstream. The classical methods of GLOF detection and prediction have so far mainly relied on hydrological modeling, threshold-based lake monitoring, and manual satellite image analysis. These approaches suffer from several drawbacks: slow updates, reliance on manual labor, and losses in accuracy when clouds interfere and/or lack on-site data. To tackle these challenges, we present IceWatch: a novel deep learning framework for GLOF prediction that incorporates both spatial and temporal perspectives. The vision component, RiskFlow, of IceWatch deals with Sentinel-2 multispectral satellite imagery using a CNN-based classifier and predicts GLOF events based on the spatial patterns of snow, ice, and meltwater. Its tabular counterpart confirms this prediction by considering physical dynamics. TerraFlow models glacier velocity from NASA ITS_LIVE time series while TempFlow forecasts near-surface temperature from MODIS LST records; both are trained on long-term observational archives and integrated via harmonized preprocessing and synchronization to enable multimodal, physics-informed GLOF prediction. Both together provide cross-validation, which will improve the reliability and interpretability of GLOF detection. This system ensures strong predictive performance, rapid data processing for real-time use, and robustness to noise and missing information. IceWatch paves the way for automatic, scalable GLOF warning systems. It also holds potential for integration with diverse sensor inputs and global glacier monitoring activities.

</details>


### [677] [Time-Continuous Modeling for Temporal Affective Pattern Recognition in LLMs](https://arxiv.org/abs/2601.12341)
*Rezky Kam,Coddy N. Siswanto*

Main category: cs.LG

TL;DR: The paper introduces a dataset and framework for modeling emotional dynamics using LLMs and physics-informed neural networks.


<details>
  <summary>Details</summary>
Motivation: Enhance LLM's ability to understand and mimic real-world emotional dynamics for interpretable dialogue modeling.

Method: Utilize physics-informed neural networks paired with in-context learning to model emotions over time.

Result: Framework demonstrates potential for improved emotional understanding and dynamics simulation.

Conclusion: The approach bridges interpretability and emotional dynamics modeling in dialogues for LLMs.

Abstract: This paper introduces a dataset and conceptual framework for LLMs to mimic real world emotional dynamics through time and in-context learning leveraging physics-informed neural network, opening a possibility for interpretable dialogue modeling.

</details>


### [678] [LB-MCTS: Synergizing Large Language Models and Bayesian Optimization for Efficient CASH](https://arxiv.org/abs/2601.12355)
*Beicheng Xu,Weitong Qian,Lingching Tung,Yupeng Lu,Bin Cui*

Main category: cs.LG

TL;DR: LB-MCTS enhances AutoML by combining LLMs and BO through Monte Carlo Tree Search, overcoming the limitations of both for better algorithm selection and hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: The goal is to reduce the barrier to entry in machine learning by addressing the challenges of automating algorithm selection and hyperparameter tuning while improving on the limitations of both Bayesian Optimization and LLM-based approaches.

Method: LB-MCTS integrates Large Language Models and Bayesian Optimization in a Monte Carlo Tree Search framework. It leverages a Selective Tuning Memory (STM) for better reasoning and improves the exploration-exploitation tradeoff by shifting dynamically from LLM-driven to BO-driven proposals.

Result: Experiments were conducted on 104 AMLB datasets and demonstrated the effectiveness of LB-MCTS in outperforming existing competitive baselines in the AutoML domain.

Conclusion: LB-MCTS successfully combines the strengths of LLMs and BO, offering a scalable and efficient solution for the CASH problem, thereby advancing automation in machine learning.

Abstract: To lower the expertise barrier in machine learning, the AutoML community has focused on the CASH problem, a fundamental challenge that automates the process of algorithm selection and hyperparameter tuning. While traditional methods like Bayesian Optimization (BO) struggle with cold-start issues, Large Language Models (LLMs) can mitigate these via semantic priors. However, existing LLM-based optimizers generalize poorly to the high-dimensional, structured CASH space. We propose LB-MCTS, a framework synergizing LLMs and BO within a Monte Carlo Tree Search structure. It maximizes LLM reasoning with Selective Tuning Memory (STM) and explicit exploration-exploitation trade-off. It combines the strengths of both paradigms by dynamically shifting from LLM-driven to BO-driven proposals as data accumulates. Experiments on 104 AMLB datasets demonstrate the superiority of LB-MCTS over the competitive baselines.

</details>


### [679] [Machine Learning-Based Framework for Real Time Detection and Early Prediction of Control Valve Stiction in Industrial Control Systems](https://arxiv.org/abs/2601.12362)
*Natthapong Promsricha,Chotirawee Chatpattanasiri,Nuttavut Kerdgongsup,Stavroula Balabani*

Main category: cs.LG

TL;DR: This paper proposes a machine learning framework for early detection and prediction of control valve stiction using process signals, demonstrating the effectiveness of deep learning models.


<details>
  <summary>Details</summary>
Motivation: Control valve stiction is a prevalent issue in industrial systems, leading to instability, higher maintenance costs, and inefficiencies, yet many plants lack real-time monitoring to address this fault effectively.

Method: The study develops and compares three deep learning models (CNN, CNN-SVM, and LSTM) using routinely collected process signals and applies a slope ratio analysis labeling method on real oil and gas refinery data.

Result: The LSTM model achieved the highest predictive accuracy, predicting stiction up to four hours in advance.

Conclusion: The proposed ML framework offers a novel solution to predict and prevent control valve stiction, paving the way for predictive maintenance in industrial systems.

Abstract: Control valve stiction, a friction that prevents smooth valve movement, is a common fault in industrial process systems that causes instability, equipment wear, and higher maintenance costs. Many plants still operate with conventional valves that lack real time monitoring, making early predictions challenging. This study presents a machine learning (ML) framework for detecting and predicting stiction using only routinely collected process signals: the controller output (OP) from control systems and the process variable (PV), such as flow rate. Three deep learning models were developed and compared: a Convolutional Neural Network (CNN), a hybrid CNN with a Support Vector Machine (CNN-SVM), and a Long Short-Term Memory (LSTM) network. To train these models, a data-driven labeling method based on slope ratio analysis was applied to a real oil and gas refinery dataset. The LSTM model achieved the highest accuracy and was able to predict stiction up to four hours in advance. To the best of the authors' knowledge, this is the first study to demonstrate ML based early prediction of control valve stiction from real industry data. The proposed framework can be integrated into existing control systems to support predictive maintenance, reduce downtime, and avoid unnecessary hardware replacement.

</details>


### [680] [Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation](https://arxiv.org/abs/2601.12401)
*Jinmei Liu,Haoru Li,Zhenhong Sun,Chaofeng Chen,Yatao Bian,Bo Wang,Daoyi Dong,Chunlin Chen,Zhi Wang*

Main category: cs.LG

TL;DR: The paper introduces DRIFT, a reinforcement learning framework optimizing generative models for task alignment and diversity in image generation.


<details>
  <summary>Details</summary>
Motivation: Address the inherent limitation in reinforcement learning for generative models, where optimization collapses diversity.

Method: Proposes DRIFT, combining reward-based sampling subsets, stochastic variations in prompting, and intra-group diversity optimization.

Result: Experimental outcomes show DRIFT significantly improves both alignment and diversity with measurable increases in performance.

Conclusion: DRIFT successfully tackles diversity collapse, enhancing generative model versatility with balanced task alignment.

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning large-scale generative models, such as diffusion and flow models, to align with complex human preferences and user-specified tasks. A fundamental limitation remains \textit{the curse of diversity collapse}, where the objective formulation and optimization landscape inherently collapse the policy to a Dirac delta distribution. To address this challenge, we propose \textbf{DRIFT} (\textbf{D}ive\textbf{R}sity-\textbf{I}ncentivized Reinforcement \textbf{F}ine-\textbf{T}uning for Versatile Image Generation), an innovative framework that systematically incentivizes output diversity throughout the on-policy fine-tuning process, reconciling strong task alignment with high generation diversity to enhance versatility essential for applications that demand diverse candidate generations. We approach the problem across three representative perspectives: i) \textbf{sampling} a reward-concentrated subset that filters out reward outliers to prevent premature collapse; ii) \textbf{prompting} with stochastic variations to expand the conditioning space, and iii) \textbf{optimization} of the intra-group diversity with a potential-based reward shaping mechanism. Experimental results show that DRIFT achieves superior Pareto dominance regarding task alignment and generation diversity, yielding a $ 9.08\%\!\sim\! 43.46\%$ increase in diversity at equivalent alignment levels and a $ 59.65\% \!\sim\! 65.86\%$ increase in alignment at equivalent levels of diversity.

</details>


### [681] [Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants](https://arxiv.org/abs/2601.12405)
*Manasi Kanade,Abhi Thakkar,Gabriela Fernandes*

Main category: cs.LG

TL;DR: A study aimed to develop an explainable AI model to stratify pediatric dental risks focusing on interpretability and ethical considerations rather than pure predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the prevalence of pediatric dental disease and its socio-economic links, the study aimed to overcome limitations in existing AI applications that lack transparency and ethical considerations.

Method: A supervised machine learning model was trained using population-level pediatric data, with performance analysis via ROC curves and SHAP for interpretability.

Result: The model showed modest predictive accuracy (AUC=0.61) and conservative calibration, with socio-economic factors like age and income-to-poverty ratio identified as major contributors.

Conclusion: Explainable AI models can prioritize prevention, transparency, and equity over diagnostics in addressing pediatric dental risks.

Abstract: Background: Pediatric dental disease remains one of the most prevalent and inequitable chronic health conditions worldwide. Although strong epidemiological evidence links oral health outcomes to socio-economic and demographic determinants, most artificial intelligence (AI) applications in dentistry rely on image-based diagnosis and black-box prediction models, limiting transparency and ethical applicability in pediatric populations.
  Objective: This study aimed to develop and evaluate an explainable machine learning framework for pediatric dental risk stratification that prioritizes interpretability, calibration, and ethical deployment over maximal predictive accuracy.
  Methods: A supervised machine learning model was trained using population-level pediatric data including age, income-to-poverty ratio, race/ethnicity, gender, and medical history. Model performance was assessed using receiver operating characteristic (ROC) analysis and calibration curves. Explainability was achieved using SHapley Additive exPlanations (SHAP) to provide global and individual-level interpretation of predictions.
  Results: The model achieved modest discrimination (AUC = 0.61) with conservative calibration, underestimating risk at higher probability levels. SHAP analysis identified age and income-to-poverty ratio as the strongest contributors to predicted risk, followed by race/ethnicity and gender.
  Conclusion: Explainable machine learning enables transparent, prevention-oriented pediatric dental risk stratification and supports population screening and equitable resource allocation rather than diagnostic decision-making.

</details>


### [682] [Orthogonalized Policy Optimization:Decoupling Sampling Geometry from Optimization Geometry in RLHF](https://arxiv.org/abs/2601.12415)
*Wang Zixian*

Main category: cs.LG

TL;DR: This paper introduces Orthogonalized Policy Optimization (OPO), a framework that separates sampling geometry from optimization geometry, overcoming issues in traditional alignment methods for large language models like numerical instability and vanishing gradients.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods for large language models, such as PPO, DPO, and IPO, suffer from conflating sampling geometry and optimization geometry, leading to instability and ineffective optimization in high-confidence scenarios.

Method: The authors formalize alignment as minimizing a generalized distance with alpha-divergence-based sampling weights and Bregman-divergence-based metrics, introducing OPO which decouples these geometries using a chi-square-induced quadratic regularization strategy.

Result: OPO ensures stable optimization, linear gradient dynamics, and avoids issues like gradient saturation, offering a unified approach to existing alignment methods while maintaining robust performance in high-confidence regimes.

Conclusion: Orthogonalized Policy Optimization provides a principled and unified foundation for alignment tasks in large language models, addressing both theoretical and practical shortcomings of prior methods.

Abstract: Recent alignment methods for large language models, including PPO, DPO, and IPO, are often presented as distinct algorithms. In this work, we show that many of these approaches implicitly conflate two fundamental and independent design choices: (i) the sampling geometry, which determines which samples dominate the gradient signal, and (ii) the optimization geometry, which determines how deviations in value are penalized. We formalize this observation by expressing alignment as the minimization of a generalized distance between policy energy and target energy, parameterized by an alpha-divergence-based sampling weight and a Bregman-divergence-based value metric. We demonstrate that the commonly used KL divergence induces an exponential penalty on unbounded value signals, leading to numerical instability and vanishing gradients in high-confidence regimes. To address this issue, we propose Orthogonalized Policy Optimization (OPO), a framework that explicitly decouples sampling geometry from optimization geometry. By combining alpha-weighted importance sampling with a chi-square-induced quadratic regularization in ratio coordinates, OPO yields a simple and well-conditioned objective with linear gradient dynamics. This formulation maintains stable optimization while preserving peak-seeking behavior and avoids gradient saturation even when model confidence is high. Our analysis positions OPO as a unifying perspective on existing alignment methods and provides a principled foundation for robust reasoning-oriented training.

</details>


### [683] [Graph Attention Networks with Physical Constraints for Anomaly Detection](https://arxiv.org/abs/2601.12426)
*Mohammadhossein Homaei,Iman Khazrak,Ruben Molano,Andres Caro,Mar Avila*

Main category: cs.LG

TL;DR: The paper introduces a hydraulic-aware graph attention network for anomaly detection in water distribution systems, focusing on reliable spatio-temporal pattern recognition.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in existing data-driven and model-based anomaly detection methods for water distribution systems.

Method: A hydraulic-aware graph attention network leveraging normalized conservation law violations, graph attention, bidirectional LSTM, and a multi-scale aggregation module.

Result: Achieved an $F1$ score of 0.979 with significant improvement of 3.3pp and exhibited robustness under 15% parameter noise using the BATADAL dataset.

Conclusion: The proposed approach enhances the accuracy and robustness of anomaly detection in water distribution systems by effectively integrating spatio-temporal data and network topology.

Abstract: Water distribution systems (WDSs) face increasing cyber-physical risks, which make reliable anomaly detection essential. Many data-driven models ignore network topology and are hard to interpret, while model-based ones depend strongly on parameter accuracy. This work proposes a hydraulic-aware graph attention network using normalized conservation law violations as features. It combines mass and energy balance residuals with graph attention and bidirectional LSTM to learn spatio-temporal patterns. A multi-scale module aggregates detection scores from node to network level. On the BATADAL dataset, it reaches $F1=0.979$, showing $3.3$pp gain and high robustness under $15\%$ parameter noise.

</details>


### [684] [Constraint-Aware Neurosymbolic Uncertainty Quantification with Bayesian Deep Learning for Scientific Discovery](https://arxiv.org/abs/2601.12442)
*Shahnawaz Alam,Mohammed Mudassir Uddin,Mohammed Kaif Pasha*

Main category: cs.LG

TL;DR: The authors introduce CANUF, a framework that integrates Bayesian deep learning with symbolic reasoning to enhance uncertainty quantification and maintain scientific constraints.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current uncertainty quantification methods and neurosymbolic approaches in scientific AI applications, which fail to incorporate scientific constraints or principled uncertainty modeling.

Method: CANUF consists of three main components: constraint extraction from scientific literature, a probabilistic neural network with variational inference, and a constraint satisfaction layer. These ensure uncertainty quantification and physical constraint adherence.

Result: Experiments on large datasets, such as Materials Project and QM9, show that CANUF reduces calibration error by 34.7% while maintaining 99.2% of physical constraint satisfaction. The framework demonstrates improved performance through constraint-guided recalibration and high-precision constraint extraction (91.4%).

Conclusion: CANUF bridges scientific constraint satisfaction and trustworthy uncertainty modeling, providing an effective and interpretable approach for scientific predictions.

Abstract: Scientific Artificial Intelligence (AI) applications require models that deliver trustworthy uncertainty estimates while respecting domain constraints. Existing uncertainty quantification methods lack mechanisms to incorporate symbolic scientific knowledge, while neurosymbolic approaches operate deterministically without principled uncertainty modeling. We introduce the Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), unifying Bayesian deep learning with differentiable symbolic reasoning. The architecture comprises three components: automated constraint extraction from scientific literature, probabilistic neural backbone with variational inference, and differentiable constraint satisfaction layer ensuring physical consistency. Experiments on Materials Project (140,000+ materials), QM9 molecular properties, and climate benchmarks show CANUF reduces Expected Calibration Error by 34.7% versus Bayesian neural networks while maintaining 99.2% constraint satisfaction. Ablations reveal constraint-guided recalibration contributes 18.3% performance gain, with constraint extraction achieving 91.4% precision. CANUF provides the first end-to-end differentiable pipeline simultaneously addressing uncertainty quantification, constraint satisfaction, and interpretable explanations for scientific predictions.

</details>


### [685] [Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting](https://arxiv.org/abs/2601.12467)
*Saurish Nagrath*

Main category: cs.LG

TL;DR: This paper proposes a two-stage framework for time-series forecasting, emphasizing structured input representations and separating local encoding with CNNs from global modelling with Transformers.


<details>
  <summary>Details</summary>
Motivation: Improve the accuracy and stability of transformer-based models in time-series forecasting by addressing challenges in raw data representation.

Method: Uses CNN to learn local temporal features, which are tokenized into compact embeddings, followed by a Transformer encoder to model global dependencies across patches.

Result: The framework demonstrated competitive forecasting performance versus existing methods on synthetic multivariate datasets.

Conclusion: Separated representation learning for local and global dependencies boosts effectiveness in time-series forecasting with improved structured representations.

Abstract: Transformer-based models have shown strong performance in time-series forecasting by leveraging self-attention to model long-range temporal dependencies. However, their effectiveness depends critically on the quality and structure of input representations derived from raw multivariate time-series data. This work proposes a two-stage forecasting framework that explicitly separates local temporal representation learning from global dependency modelling. In the first stage, a convolutional neural network (CNN) operates on fixed-length temporal patches to extract short-range temporal dynamics and non-linear feature interactions, producing compact patch-level token embeddings. Token-level self-attention is subsequently applied during representation learning to refine these embeddings by enabling interactions across temporal patches. In the second stage, a Transformer encoder processes the resulting token sequence to model inter-patch temporal dependencies and generate per-patch forecasts. Experiments conducted on synthetic multivariate time-series data with controlled static and dynamic factors demonstrate that the proposed patch-based tokenization strategy achieves competitive forecasting performance compared to convolutional and patch-based Transformer baselines. The results highlight the importance of structured temporal representations and show that decoupling local temporal encoding from global attention-based modelling yields more effective and stable time-series forecasting.

</details>


### [686] [Semidefinite Programming for Quantum Channel Learning](https://arxiv.org/abs/2601.12502)
*Mikhail Gennadievich Belov,Victor Victorovich Dubov,Vadim Konstantinovich Ivanov,Alexander Yurievich Maslov,Olga Vladimirovna Proshina,Vladislav Gennadievich Malyshkin*

Main category: cs.LG

TL;DR: The paper discusses reconstructing quantum channels using classical data, applying Semidefinite Programming (SDP) for fidelity optimization problems, and analyzing the efficiency and rank characteristics of the obtained channels.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently reconstructing quantum channels from classical data using robust optimization methods like SDP.

Method: SDP was applied to optimize fidelity concerning the Choi matrix, testing various solvers and analyzing obtained channels. Classical computational models were also discussed.

Result: SDP reconstruction led to quantum channels with significantly low Kraus ranks, indicating simpler structures sufficient for describing classical data. Success in testing multiple SDP solvers was achieved.

Conclusion: SDP provides an efficient and convex optimization approach, enabling reconstruction of effective quantum channels and validating its utility in optics and computational models.

Abstract: The problem of reconstructing a quantum channel from a sample of classical data is considered. When the total fidelity can be represented as a ratio of two quadratic forms (e.g., in the case of mapping a mixed state to a pure state, projective operators, unitary learning, and others), Semidefinite Programming (SDP) can be applied to solve the fidelity optimization problem with respect to the Choi matrix. A remarkable feature of SDP is that the optimization is convex, which allows the problem to be efficiently solved by a variety of numerical algorithms. We have tested several commercially available SDP solvers, all of which allowed for the reconstruction of quantum channels of different forms. A notable feature is that the Kraus rank of the obtained quantum channel typically comprises less than a few percent of its maximal possible value. This suggests that a relatively small Kraus rank quantum channel is typically sufficient to describe experimentally observed classical data. The theory was also applied to the problem of reconstructing projective operators from data. Finally, we discuss a classical computational model based on quantum channel transformation, performed and calculated on a classical computer, possibly hardware-optimized.

</details>


### [687] [Learning Relativistic Geodesics and Chaotic Dynamics via Stabilized Lagrangian Neural Networks](https://arxiv.org/abs/2601.12519)
*Abdullah Umut Hamzaogullari,Arkadas Ozakin*

Main category: cs.LG

TL;DR: This paper addresses instability issues in Lagrangian Neural Networks (LNNs) and presents improvements to successfully train LNNs on complex systems and geodesic motion in relativistic settings.


<details>
  <summary>Details</summary>
Motivation: LNNs struggle with training instability when learning Lagrangians for complex dynamical systems, limiting their broader applications, particularly in discovering physical and geometric structures.

Method: The authors introduce three main improvements: Hessian regularization to penalize unphysical dynamics, physics-aware coordinate scaling, and activation functions suited for Lagrangians. They also extend regularization for relativistic settings by ensuring Lorentzian signatures are consistent.

Result: The improved framework achieved significant improvements in stability and validation loss, especially for double pendulums, and importantly, demonstrated capability to learn Lagrangians for geodesic motion, including in relativistic spacetime metrics.

Conclusion: The proposed enhancements expand the practical scope of LNNs, allowing them to tackle more complex systems, and potentially enabling automated discovery of spacetime metrics and other geometric features in physics.

Abstract: Lagrangian Neural Networks (LNNs) can learn arbitrary Lagrangians from trajectory data, but their unusual optimization objective leads to significant training instabilities that limit their application to complex systems. We propose several improvements that address these fundamental challenges, namely, a Hessian regularization scheme that penalizes unphysical signatures in the Lagrangian's second derivatives with respect to velocities, preventing the network from learning unstable dynamics, activation functions that are better suited to the problem of learning Lagrangians, and a physics-aware coordinate scaling that improves stability. We systematically evaluate these techniques alongside previously proposed methods for improving stability. Our improved architecture successfully trains on systems of unprecedented complexity, including triple pendulums, and achieved 96.6\% lower validation loss value and 90.68\% better stability than baseline LNNs in double pendulum systems. With the improved framework, we show that our LNNs can learn Lagrangians representing geodesic motion in both non-relativistic and general relativistic settings. To deal with the relativistic setting, we extended our regularization to penalize violations of Lorentzian signatures, which allowed us to predict a geodesic Lagrangian under AdS\textsubscript{4} spacetime metric directly from trajectory data, which to our knowledge has not been done in the literature before. This opens new possibilities for automated discovery of geometric structures in physics, including extraction of spacetime metric tensor components from geodesic trajectories. While our approach inherits some limitations of the original LNN framework, particularly the requirement for invertible Hessians, it significantly expands the practical applicability of LNNs for scientific discovery tasks.

</details>


### [688] [Approximating splits for decision trees quickly in sparse data streams](https://arxiv.org/abs/2601.12525)
*Nikolaj Tatti*

Main category: cs.LG

TL;DR: The paper proposes an improved algorithm for efficiently finding near-optimal splits in decision trees dealing with sparse binary data and binary classes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to optimize the process of finding the optimal split in decision tree training, particularly for sparse binary features and binary class data, where computational efficiency is crucial.

Method: The method involves an algorithm delivering approximately optimal splits using conditional entropy or Gini index with reduced computation time in sparse data scenarios. Performance scales with sparsity and data characteristics.

Result: The proposed algorithm achieves better results than the baseline in experiments, with faster processing and efficiency exceeding theoretical guarantees.

Conclusion: The research demonstrates the utility of the new approach for sparse data, providing faster and almost-optimal solutions, making it a valuable contribution to decision tree training in machine learning.

Abstract: Decision trees are one of the most popular classifiers in the machine learning literature. While the most common decision tree learning algorithms treat data as a batch, numerous algorithms have been proposed to construct decision trees from a data stream. A standard training strategy involves augmenting the current tree by changing a leaf node into a split. Here we typically maintain counters in each leaf which allow us to determine the optimal split, and whether the split should be done. In this paper we focus on how to speed up the search for the optimal split when dealing with sparse binary features and a binary class. We focus on finding splits that have the approximately optimal information gain or Gini index. In both cases finding the optimal split can be done in $O(d)$ time, where $d$ is the number of features. We propose an algorithm that yields $(1 + α)$ approximation when using conditional entropy in amortized $O(α^{-1}(1 + m\log d) \log \log n)$ time, where $m$ is the number of 1s in a data point, and $n$ is the number of data points. Similarly, for Gini index, we achieve $(1 + α)$ approximation in amortized $O(α^{-1} + m \log d)$ time. Our approach is beneficial for sparse data where $m \ll d$. In our experiments we find almost-optimal splits efficiently, faster than the baseline, overperforming the theoretical approximation guarantees.

</details>


### [689] [Press Start to Charge: Videogaming the Online Centralized Charging Scheduling Problem](https://arxiv.org/abs/2601.12543)
*Alireza Ghahtarani,Martin Cousineau,Amir-massoud Farahmand,Jorge E. Mendoza*

Main category: cs.LG

TL;DR: The paper tackles the OCCSP by gamifying it to reduce complexity and improve generalization. Proposed methods outperform alternatives in load balancing and yield significant economic benefits in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a more efficient real-time charging scheduling solution for electric vehicles that balances the grid load and minimizes operational costs, while addressing dynamic arrivals and capacity constraints.

Method: The authors gamify the problem into a game-like grid structure, develop heuristic policies, train learning agents using expert demonstrations, and optimize them with Dataset Aggregation (DAgger).

Result: The gamified approach outperforms heuristic baselines, vector-based models, and supervised learning in load balancing, showing robustness across different EV arrival patterns. Economically, it significantly reduces system costs and delays grid upgrades.

Conclusion: The gamified model not only enhances operational performance in EV charging but also offers substantial economic and infrastructure benefits, establishing its practical advantages over conventional methods.

Abstract: We study the online centralized charging scheduling problem (OCCSP). In this problem, a central authority must decide, in real time, when to charge dynamically arriving electric vehicles (EVs), subject to capacity limits, with the objective of balancing load across a finite planning horizon. To solve the problem, we first gamify it; that is, we model it as a game where charging blocks are placed within temporal and capacity constraints on a grid. We design heuristic policies, train learning agents with expert demonstrations, and improve them using Dataset Aggregation (DAgger). From a theoretical standpoint, we show that gamification reduces model complexity and yields tighter generalization bounds than vector-based formulations. Experiments across multiple EV arrival patterns confirm that gamified learning enhances load balancing. In particular, the image-to-movement model trained with DAgger consistently outperforms heuristic baselines, vector-based approaches, and supervised learning agents, while also demonstrating robustness in sensitivity analyses. These operational gains translate into tangible economic value. In a real-world case study for the Greater Montréal Area (Québec, Canada) using utility cost data, the proposed methods lower system costs by tens of millions of dollars per year over the prevailing practice and show clear potential to delay costly grid upgrades.

</details>


### [690] [Life, Machine Learning, and the Search for Habitability: Predicting Biosignature Fluxes for the Habitable Worlds Observatory](https://arxiv.org/abs/2601.12557)
*Mark Moussa,Amber V. Young,Brianna Isola,Vasuda Trehan,Michael D. Himes,Nicholas Wogan,Giada Arney*

Main category: cs.LG

TL;DR: This paper introduces two machine learning models, BCNN and SQuAT, for predicting biosignature fluxes from exoplanet spectra, aiding future flagship missions like HWO with high accuracy, uncertainty quantification, and spectral interpretability.


<details>
  <summary>Details</summary>
Motivation: Future direct-imaging missions like NASA's HWO face significant challenges in optimizing observations under limited time and resources, necessitating advanced predictive tools for better decision-making.

Method: The paper develops two machine-learning models: BCNN, which quantifies uncertainties in predictions, and SQuAT, which uses query-driven attention for improved interpretability, both tested on an extensive, diverse dataset.

Result: Both BCNN and SQuAT achieved high predictive accuracy, with BCNN excelling in uncertainty quantification and SQuAT excelling in spectral interpretability.

Conclusion: These models show promise in accelerating target prioritization, optimizing observation strategies, and improving data utility for upcoming missions like NASA's HWO.

Abstract: Future direct-imaging flagship missions, such as NASA's Habitable Worlds Observatory (HWO), face critical decisions in prioritizing observations due to extremely stringent time and resource constraints. In this paper, we introduce two advanced machine-learning architectures tailored for predicting biosignature species fluxes from exoplanetary reflected-light spectra: a Bayesian Convolutional Neural Network (BCNN) and our novel model architecture, the Spectral Query Adaptive Transformer (SQuAT). The BCNN robustly quantifies both epistemic and aleatoric uncertainties, offering reliable predictions under diverse observational conditions, whereas SQuAT employs query-driven attention mechanisms to enhance interpretability by explicitly associating spectral features with specific biosignature species. We demonstrate that both models achieve comparably high predictive accuracy on an augmented dataset spanning a wide range of exoplanetary conditions, while highlighting their distinct advantages in uncertainty quantification and spectral interpretability. These capabilities position our methods as promising tools for accelerating target triage, optimizing observation schedules, and maximizing scientific return for upcoming flagship missions such as HWO.

</details>


### [691] [Dissecting Linear Recurrent Models: How Different Gating Strategies Drive Selectivity and Generalization](https://arxiv.org/abs/2601.12598)
*Younes Bouhadjar,Maxime Fabre,Felix Schmidt,Emre Neftci*

Main category: cs.LG

TL;DR: The paper introduces SelectivBench, a customizable synthetic benchmark for evaluating the selectivity of sequence models, and uses it to analyze and compare linear recurrent neural networks (RNNs).


<details>
  <summary>Details</summary>
Motivation: To address the need for a lightweight evaluation framework for linear recurrent models, as benchmark tasks are either too simple or too resource-intensive.

Method: They developed a taxonomy for linear recurrent models and introduced SelectivBench, a set of rule-based benchmark tasks that generate controlled sequences to evaluate different architectural features systematically.

Result: SelectivBench revealed how linear recurrent model mechanisms such as gating and rapid forgetting functions improve performance, while softmax attention remains superior due to memory scalability.

Conclusion: The study provides insights into the trade-offs and performance of linear recurrent models, with SelectivBench serving as an efficient tool for controlled experiments and evaluations.

Abstract: Linear recurrent neural networks have emerged as efficient alternatives to the original Transformer's softmax attention mechanism, thanks to their highly parallelizable training and constant memory and computation requirements at inference. Iterative refinements of these models have introduced an increasing number of architectural mechanisms, leading to increased complexity and computational costs. Nevertheless, systematic direct comparisons among these models remain limited. Existing benchmark tasks are either too simplistic to reveal substantial differences or excessively resource-intensive for experimentation. In this work, we propose a refined taxonomy of linear recurrent models and introduce SelectivBench, a set of lightweight and customizable synthetic benchmark tasks for systematically evaluating sequence models. SelectivBench specifically evaluates selectivity in sequence models at small to medium scale, such as the capacity to focus on relevant inputs while ignoring context-based distractors. It employs rule-based grammars to generate sequences with adjustable complexity, incorporating irregular gaps that intentionally violate transition rules. Evaluations of linear recurrent models on SelectivBench reveal performance patterns consistent with results from large-scale language tasks. Our analysis clarifies the roles of essential architectural features: gating and rapid forgetting mechanisms facilitate recall, in-state channel mixing is unnecessary for selectivity, but critical for generalization, and softmax attention remains dominant due to its memory capacity scaling with sequence length. Our benchmark enables targeted, efficient exploration of linear recurrent models and provides a controlled setting for studying behaviors observed in large-scale evaluations. Code is available at https://github.com/symseqbench/selectivbench

</details>


### [692] [Beyond Softmax and Entropy: Improving Convergence Guarantees of Policy Gradients by f-SoftArgmax Parameterization with Coupled Regularization](https://arxiv.org/abs/2601.12604)
*Safwan Labbi,Daniil Tiapkin,Paul Mangold,Eric Moulines*

Main category: cs.LG

TL;DR: Policy gradient methods suffer from ill-conditioned landscapes; this paper introduces a generalized f-softargmax policy parameterization to improve optimization and convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Optimization challenges in policy gradient methods with softmax parameterization lead to slow convergence and computational burdens.

Method: Proposes f-softargmax policy parameterization along with f-divergence regularizer to improve optimization landscapes and ensure convergence. Establishes non-asymptotic guarantees in finite MDPs.

Result: Achieved first last-iterate convergence guarantees without preconditioning and polynomial sample complexity with f-PG using Tsallis divergences.

Conclusion: f-softargmax improves policy gradient optimization and regularization, avoiding exponential sample complexity.

Abstract: Policy gradient methods are known to be highly sensitive to the choice of policy parameterization. In particular, the widely used softmax parameterization can induce ill-conditioned optimization landscapes and lead to exponentially slow convergence. Although this can be mitigated by preconditioning, this solution is often computationally expensive. Instead, we propose replacing the softmax with an alternative family of policy parameterizations based on the generalized f-softargmax. We further advocate coupling this parameterization with a regularizer induced by the same f-divergence, which improves the optimization landscape and ensures that the resulting regularized objective satisfies a Polyak-Lojasiewicz inequality. Leveraging this structure, we establish the first explicit non-asymptotic last-iterate convergence guarantees for stochastic policy gradient methods for finite MDPs without any form of preconditioning. We also derive sample-complexity bounds for the unregularized problem and show that f-PG, with Tsallis divergences achieves polynomial sample complexity in contrast to the exponential complexity incurred by the standard softmax parameterization.

</details>


### [693] [Towards Robust Universal Perturbation Attacks: A Float-Coded, Penalty-Driven Evolutionary Approach](https://arxiv.org/abs/2601.12624)
*Shiqi Wang,Mahdi Khosravy,Neeraj Gupta,Olaf Witkowski*

Main category: cs.LG

TL;DR: The paper proposes a novel single-objective evolutionary framework to efficiently generate universal adversarial perturbations (UAPs) with low visibility and high attack success rates.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the robustness and efficiency in generating UAPs by addressing limitations such as high visibility, low attack success, and overfitting in existing methods, using an evolutionary algorithm approach.

Method: The authors implement a float-coded, penalty-driven evolutionary method with adaptive scheduling, continuous gene representations, and a modular PyTorch framework. The method integrates dynamic evolutionary operators and tests universality across diverse models using the ImageNet dataset.

Result: The framework yields perturbations with smaller norms, higher misclassification rates, and faster convergence compared to existing evolutionary-based methods, validated through experiments on the ImageNet dataset.

Conclusion: The proposed approach demonstrates robustness, scalability, and effectiveness for universal adversarial attacks across various deep learning models, outperforming current evolutionary techniques.

Abstract: Universal adversarial perturbations (UAPs) have garnered significant attention due to their ability to undermine deep neural networks across multiple inputs using a single noise pattern. Evolutionary algorithms offer a promising approach to generating such perturbations due to their ability to navigate non-convex, gradient-free landscapes. In this work, we introduce a float-coded, penalty-driven single-objective evolutionary framework for UAP generation that achieves lower visibility perturbations while enhancing attack success rates. Our approach leverages continuous gene representations aligned with contemporary deep learning scales, incorporates dynamic evolutionary operators with adaptive scheduling, and utilizes a modular PyTorch implementation for seamless integration with modern architectures. Additionally, we ensure the universality of the generated perturbations by testing across diverse models and by periodically switching batches to prevent overfitting. Experimental results on the ImageNet dataset demonstrate that our framework consistently produces perturbations with smaller norms, higher misclassification effectiveness, and faster convergence compared to existing evolutionary-based methods. These findings highlight the robustness and scalability of our approach for universal adversarial attacks across various deep learning architectures.

</details>


### [694] [Topology-Aware Multiscale Mixture of Experts for Efficient Molecular Property Prediction](https://arxiv.org/abs/2601.12637)
*Long D. Nguyen,Kelin Xia,Binh P. Nguyen*

Main category: cs.LG

TL;DR: The paper introduces MI-MoE, a new approach to improve 3D molecular graph neural networks by achieving multiscale interaction modeling for better predictions across molecular datasets.


<details>
  <summary>Details</summary>
Motivation: Most 3D molecular graph neural networks rely on rigid, global heuristics for defining neighborhoods, which limits their ability to capture complex and multiscale molecular interactions.

Method: The authors propose MI-MoE, a model that uses a distance-cutoff expert ensemble, a topological gating encoder, and filtration-based descriptors (e.g., persistent homology features) to enable multiscale routing of molecular interactions.

Result: MI-MoE consistently improves the performance of 3D molecular graph models in various benchmark tasks (e.g., regression and classification) across molecular and polymer datasets.

Conclusion: Topology-aware multiscale routing is a useful strategy to enhance 3D molecular graph learning and offers a flexible, effective improvement for existing molecular graph models.

Abstract: Many molecular properties depend on 3D geometry, where non-covalent interactions, stereochemical effects, and medium- to long-range forces are determined by spatial distances and angles that cannot be uniquely captured by a 2D bond graph. Yet most 3D molecular graph neural networks still rely on globally fixed neighborhood heuristics, typically defined by distance cutoffs and maximum neighbor limits, to define local message-passing neighborhoods, leading to rigid, data-agnostic interaction budgets. We propose Multiscale Interaction Mixture of Experts (MI-MoE) to adapt interaction modeling across geometric regimes. Our contributions are threefold: (1) we introduce a distance-cutoff expert ensemble that explicitly captures short-, mid-, and long-range interactions without committing to a single cutoff; (2) we design a topological gating encoder that routes inputs to experts using filtration-based descriptors, including persistent homology features, summarizing how connectivity evolves across radii; and (3) we show that MI-MoE is a plug-in module that consistently improves multiple strong 3D molecular backbones across diverse molecular and polymer property prediction benchmark datasets, covering both regression and classification tasks. These results highlight topology-aware multiscale routing as an effective principle for 3D molecular graph learning.

</details>


### [695] [Explanation Multiplicity in SHAP: Characterization and Assessment](https://arxiv.org/abs/2601.12654)
*Hyunseung Hwang,Seungeun Lee,Lucas Rosenblatt,Julia Stoyanovich,Steven Euijong Whang*

Main category: cs.LG

TL;DR: Explanation multiplicity in SHAP explanations leads to varied, statically valid explanations even with consistent inputs, tasks, and models.


<details>
  <summary>Details</summary>
Motivation: Highlight the issue and implications of explanation multiplicity in high-stakes applications of SHAP as a post-hoc explanation tool.

Method: Developed a methodology to characterize and disentangle explanation multiplicity, and applied rank-based measures and baseline values under null models.

Result: Revealed that explanation multiplicity is pervasive across models and datasets, impacting model confidence and stability metrics.

Conclusion: Metrics and baselines must align with the intended use of explanations to address explanation multiplicity's impact in practice.

Abstract: Post-hoc explanations are widely used to justify, contest, and audit automated decisions in high-stakes domains. SHAP, in particular, is often treated as a reliable account of which features drove an individual prediction. Yet SHAP explanations can vary substantially across repeated runs even when the input, task, and trained model are held fixed. We term this phenomenon explanation multiplicity: multiple internally valid but substantively different explanations for the same decision. We present a methodology to characterize multiplicity in feature-attribution explanations and to disentangle sources due to model training/selection from stochasticity intrinsic to the explanation pipeline. We further show that apparent stability depends on the metric: magnitude-based distances can remain near zero while rank-based measures reveal substantial churn in the identity and ordering of top features. To contextualize observed disagreement, we derive randomized baseline values under plausible null models. Across datasets, model classes, and confidence regimes, we find explanation multiplicity is pervasive and persists even for high-confidence predictions, highlighting the need for metrics and baselines that match the intended use of explanations.

</details>


### [696] [Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks](https://arxiv.org/abs/2601.12662)
*Xingran Chen,Navid NaderiAlizadeh,Alejandro Ribeiro,Shirin Saeedi Bidokhti*

Main category: cs.LG

TL;DR: The paper proposes a graphical multi-agent reinforcement learning framework for optimizing decentralized estimation policies in multi-hop wireless networks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve real-time sampling and estimation in multi-hop wireless networks while addressing challenges like high-dimensional action spaces and complex network topologies.

Method: The paper introduces a graphical multi-agent reinforcement learning approach which demonstrates policy transferability between structurally similar graph networks.

Result: The proposed framework outperformed state-of-the-art baselines, showed policy transferability to larger networks, and demonstrated robustness to non-stationarity when using recurrent methods.

Conclusion: The framework effectively optimizes decentralized network policies, proving scalability, transferability, and resilience, with recurrence as a critical factor for improved performance in dynamic environments.

Abstract: We address real-time sampling and estimation of autoregressive Markovian sources in dynamic yet structurally similar multi-hop wireless networks. Each node caches samples from others and communicates over wireless collision channels, aiming to minimize time-average estimation error via decentralized policies. Due to the high dimensionality of action spaces and complexity of network topologies, deriving optimal policies analytically is intractable. To address this, we propose a graphical multi-agent reinforcement learning framework for policy optimization. Theoretically, we demonstrate that our proposed policies are transferable, allowing a policy trained on one graph to be effectively applied to structurally similar graphs. Numerical experiments demonstrate that (i) our proposed policy outperforms state-of-the-art baselines; (ii) the trained policies are transferable to larger networks, with performance gains increasing with the number of agents; (iii) the graphical training procedure withstands non-stationarity, even when using independent learning techniques; and (iv) recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity.

</details>


### [697] [MetaToolAgent: Towards Generalizable Tool Usage in LLMs through Meta-Learning](https://arxiv.org/abs/2601.12680)
*Zheng Fang,Wolfgang Mayer,Zeyu Zhang,Jian Wang,Hong-Yu Zhang,Wanli Li,Zaiwen Feng*

Main category: cs.LG

TL;DR: The paper addresses tool learning in large language models (LLMs) and introduces a dataset and a meta-learning approach to improve generalization across tools.


<details>
  <summary>Details</summary>
Motivation: Tool selection in LLMs struggles to generalize beyond limited sets of tools, limiting their practical deployment for complex real-world tasks.

Method: The authors introduce a dataset with 155 tools across 7 domains and propose MetaToolAgent (MTA), a meta-learning approach for improving cross-tool generalization.

Result: Experimental results show that MTA outperforms baseline methods when applied to unseen tools.

Conclusion: The proposed MTA method holds potential for creating flexible, scalable LLM systems that can dynamically coordinate diverse tools in realistic scenarios.

Abstract: Tool learning is increasingly important for large language models (LLMs) to effectively coordinate and utilize a diverse set of tools in order to solve complex real-world tasks. By selecting and integrating appropriate tools, LLMs extend their capabilities beyond pure language understanding to perform specialized functions. However, existing methods for tool selection often focus on limited tool sets and struggle to generalize to novel tools encountered in practical deployments. To address these challenges, we introduce a comprehensive dataset spanning 7 domains, containing 155 tools and 9,377 question-answer pairs, which simulates realistic integration scenarios. Additionally, we propose MetaToolAgent (MTA), a meta-learning approach designed to improve cross-tool generalization. Experimental results show that MTA significantly outperforms baseline methods on unseen tools, demonstrating its promise for building flexible and scalable systems that require dynamic tool coordination.

</details>


### [698] [Resource-Conscious RL Algorithms for Deep Brain Stimulation](https://arxiv.org/abs/2601.12699)
*Arkaprava Gupta,Nicholas Carter,William Zellers,Prateek Ganguli,Benedikt Dietrich,Vibhor Krishna,Parasara Sridhar Duggirala,Samarjit Chakraborty*

Main category: cs.LG

TL;DR: This paper introduces an energy-efficient Reinforcement Learning (RL) approach for Deep Brain Stimulation (DBS) in Parkinson's treatment, improving sample efficiency without offline training.


<details>
  <summary>Details</summary>
Motivation: Current DBS methods use fixed parameters, leading to side effects and battery issues, while existing RL solutions are too complex for in vivo application.

Method: The authors propose a lightweight Time & Threshold-Triggered Multi-Armed Bandit (T3P MAB) RL algorithm to adaptively regulate DBS frequency and amplitude.

Result: The T3P MAB algorithm achieves better power efficiency and sample effectiveness, deployable on microcontroller setups, without the need for an offline training phase.

Conclusion: T3P MAB is a practical alternative to existing DBS algorithms, suitable for resource-constrained environments and capable of enhancing patient outcomes.

Abstract: Deep Brain Stimulation (DBS) has proven to be a promising treatment of Parkinson's Disease (PD). DBS involves stimulating specific regions of the brain's Basal Ganglia (BG) using electric impulses to alleviate symptoms of PD such as tremors, rigidity, and bradykinesia. Although most clinical DBS approaches today use a fixed frequency and amplitude, they suffer from side effects (such as slurring of speech) and shortened battery life of the implant. Reinforcement learning (RL) approaches have been used in recent research to perform DBS in a more adaptive manner to improve overall patient outcome. These RL algorithms are, however, too complex to be trained in vivo due to their long convergence time and requirement of high computational resources.
  We propose a new Time & Threshold-Triggered Multi-Armed Bandit (T3P MAB) RL approach for DBS that is more effective than existing algorithms. Further, our T3P agent is lightweight enough to be deployed in the implant, unlike current deep-RL strategies, and even forgoes the need for an offline training phase. Additionally, most existing RL approaches have focused on modulating only frequency or amplitude, and the possibility of tuning them together remains greatly unexplored in the literature. Our RL agent can tune both frequency and amplitude of DBS signals to the brain with better sample efficiency and requires minimal time to converge. We implement an MAB agent for DBS for the first time on hardware to report energy measurements and prove its suitability for resource-constrained platforms. Our T3P MAB algorithm is deployed on a variety of microcontroller unit (MCU) setups to show its efficiency in terms of power consumption as opposed to other existing RL approaches used in recent work.

</details>


### [699] [Towards Spectroscopy: Susceptibility Clusters in Language Models](https://arxiv.org/abs/2601.12703)
*Andrew Gordon,Garrett Baker,George Wang,William Snell,Stan van Wingerden,Daniel Murfet*

Main category: cs.LG

TL;DR: The paper introduces a spectroscopy-like method to analyze neural networks by perturbing input data and measuring the model's susceptibilities, leading to a new clustering approach for interpreting token relationships.


<details>
  <summary>Details</summary>
Motivation: To develop a principled approach for understanding the internal structure and feature representation of neural networks based on their response to data distribution perturbations.

Method: The authors perturb the data distribution by upweighting certain tokens, measure the model's response through susceptibilities computed with stochastic gradient Langevin dynamics (SGLD), and propose a conductance-based clustering algorithm to analyze the results.

Result: By applying their approach to the Pythia-14M model, they identify 510 interpretable clusters of tokens, validating their methodology by comparing these clusters to features recovered by sparse autoencoders (with 50% overlap).

Conclusion: The theoretical and empirical findings demonstrate the utility of susceptibilities for analyzing model structure and offer insights into token relationships, revealing interpretable features that align with established analysis methods.

Abstract: Spectroscopy infers the internal structure of physical systems by measuring their response to perturbations. We apply this principle to neural networks: perturbing the data distribution by upweighting a token $y$ in context $x$, we measure the model's response via susceptibilities $χ_{xy}$, which are covariances between component-level observables and the perturbation computed over a localized Gibbs posterior via stochastic gradient Langevin dynamics (SGLD). Theoretically, we show that susceptibilities decompose as a sum over modes of the data distribution, explaining why tokens that follow their contexts "for similar reasons" cluster together in susceptibility space. Empirically, we apply this methodology to Pythia-14M, developing a conductance-based clustering algorithm that identifies 510 interpretable clusters ranging from grammatical patterns to code structure to mathematical notation. Comparing to sparse autoencoders, 50% of our clusters match SAE features, validating that both methods recover similar structure.

</details>


### [700] [Adaptively trained Physics-informed Radial Basis Function Neural Networks for Solving Multi-asset Option Pricing Problems](https://arxiv.org/abs/2601.12704)
*Yan Ma,Yumeng Ren*

Main category: cs.LG

TL;DR: The study introduces a physics-informed machine learning algorithm that uses a radial basis function neural network for solving Black-Scholes PDE for option pricing involving multiple assets.


<details>
  <summary>Details</summary>
Motivation: To solve the Black-Scholes PDE for option pricing with multiple assets, where conventional methods face difficulty in handling high-dimensional and non-smooth payoff conditions.

Method: The study develops a Physics-Informed Radial Basis Function Neural Network (PIRBFNN) that combines radial basis function collocation and neural network techniques. It adaptively refines hidden neuron distribution during training using PDE residual-based techniques.

Result: The PIRBFNN model demonstrated accuracy and efficacy in pricing single-asset, double-asset, and four-asset option scenarios through experimental validation.

Conclusion: The PIRBFNN successfully integrates machine learning and mathematical methods to effectively address financial PDE problems, showing promise for multidimensional option pricing models.

Abstract: The present study investigates the numerical solution of Black-Scholes partial differential equation (PDE) for option valuation with multiple underlying assets. We develop a physics-informed (PI) machine learning algorithm based on a radial basis function neural network (RBFNN) that concurrently optimizes the network architecture and predicts the target option price. The physics-informed radial basis function neural network (PIRBFNN) combines the strengths of the traditional radial basis function collocation method and the physics-informed neural network machine learning approach to effectively solve PDE problems in the financial context. By employing a PDE residual-based technique to adaptively refine the distribution of hidden neurons during the training process, the PIRBFNN facilitates accurate and efficient handling of multidimensional option pricing models featuring non-smooth payoff conditions. The validity of the proposed method is demonstrated through a set of experiments encompassing a single-asset European put option, a double-asset exchange option, and a four-asset basket call option.

</details>


### [701] [Trend-Adjusted Time Series Models with an Application to Gold Price Forecasting](https://arxiv.org/abs/2601.12706)
*Sina Kazemdehbashi*

Main category: cs.LG

TL;DR: The paper introduces the Trend-Adjusted Time Series (TATS) model, which improves forecasting accuracy by integrating trend prediction and value forecasting.


<details>
  <summary>Details</summary>
Motivation: To enhance time series forecasting by addressing both trend prediction and value prediction, and to improve performance over existing models like LSTM and Bi-LSTM.

Method: The TATS model combines a binary classifier for trend prediction and models (e.g., LSTM, Bi-LSTM) for quantitative value forecasting, adjusting forecasted values based on predicted trends.

Result: TATS outperforms LSTM and Bi-LSTM for forecasting volatile financial time series, demonstrated on daily gold price data, with improved forecast accuracy and trend detection.

Conclusion: The proposed TATS model effectively reduces forecasting errors by incorporating trend prediction, and highlights the need for additional evaluation metrics like trend detection accuracy in time series analysis.

Abstract: Time series data play a critical role in various fields, including finance, healthcare, marketing, and engineering. A wide range of techniques (from classical statistical models to neural network-based approaches such as Long Short-Term Memory (LSTM)) have been employed to address time series forecasting challenges. In this paper, we reframe time series forecasting as a two-part task: (1) predicting the trend (directional movement) of the time series at the next time step, and (2) forecasting the quantitative value at the next time step. The trend can be predicted using a binary classifier, while quantitative values can be forecasted using models such as LSTM and Bidirectional Long Short-Term Memory (Bi-LSTM). Building on this reframing, we propose the Trend-Adjusted Time Series (TATS) model, which adjusts the forecasted values based on the predicted trend provided by the binary classifier. We validate the proposed approach through both theoretical analysis and empirical evaluation. The TATS model is applied to a volatile financial time series (the daily gold price) with the objective of forecasting the next days price. Experimental results demonstrate that TATS consistently outperforms standard LSTM and Bi-LSTM models by achieving significantly lower forecasting error. In addition, our results indicate that commonly used metrics such as MSE and MAE are insufficient for fully assessing time series model performance. Therefore, we also incorporate trend detection accuracy, which measures how effectively a model captures trends in a time series.

</details>


### [702] [Distribution-Centric Policy Optimization Dominates Exploration-Exploitation Trade-off](https://arxiv.org/abs/2601.12730)
*Zhaochun Li,Chen Wang,Jionghao Bai,Shisheng Cui,Ge Lan,Zhou Zhao,Yue Wang*

Main category: cs.LG

TL;DR: The paper introduces Distribution-Centric Policy Optimization (DCPO) for reinforcement learning in large language models, focusing on distribution-level exploration and entropy control, improving exploration-exploitation trade-offs over traditional methods like GRPO.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in current reinforcement learning methods, such as GRPO, where training becomes exploitation-driven and lacks effective exploration, resulting in inconsistent performance due to sample-centric approaches.

Method: The authors propose DCPO, a novel method that adopts a distribution-based perspective rather than sample-centric heuristics, and reformulates entropy regulation as a distribution-level regularization, enabling controllable and stable exploration in RL.

Result: DCPO outperforms GRPO by approximately 20% across multiple models and benchmarks, demonstrating improved exploration and training stability without relying on external distributions.

Conclusion: DCPO provides a robust and theoretically grounded alternative to sample-centric RL methods, enhancing exploration-exploitation trade-offs and enabling consistent performance improvements for large language models.

Abstract: The exploration-exploitation (EE) trade-off is a central challenge in reinforcement learning (RL) for large language models (LLMs). With Group Relative Policy Optimization (GRPO), training tends to be exploitation driven: entropy decreases monotonically, samples convergence, and exploration fades. Most existing fixes are \textbf{sample-centric}: they seek or bonus rare samples, assuming exploration comes from novel trajectories and tokens. These heuristics depend on the "luck" of informative samples, lack principled control of the policy, and often yield limited or inconsistent gains. In this work, we are the first to introduce a \textbf{distribution-centric} perspective for RL, in which exploration is always guided by a "better" target distribution, and reveal that a policy's ability to resist entropy collapse is governed by the distribution itself rather than individual samples. Building on this insight, we propose Distribution-Centric Policy Optimization (DCPO), which reformulates entropy regulation as distribution-level regularization. DCPO achieves controllable entropy fully on-policy without sampling from external distributions, enabling efficient exploration while maintaining training stability. Across multiple models and seven benchmarks, DCPO improves over GRPO by about 20\% on average. Overall, DCPO replaces sample-level heuristics with distribution-level principles, offering a theoretically grounded and flexible framework for controllable exploration and a stronger EE trade-off. The code is available in https://github.com/597358816/DCPO.

</details>


### [703] [A Graph Prompt Fine-Tuning Method for WSN Spatio-Temporal Correlation Anomaly Detection](https://arxiv.org/abs/2601.12745)
*Miao Ye,Jing Cui,Yuan huang,Qian He,Yong Wang,Jiwen Zhang*

Main category: cs.LG

TL;DR: The paper proposes an advanced graph neural network-based anomaly detection framework for Wireless Sensor Networks (WSN), achieving high detection performance and generalization using pre-training, graph prompting, and fine-tuning strategies.


<details>
  <summary>Details</summary>
Motivation: To improve anomaly detection in multi-temporal modal Wireless Sensor Networks by addressing challenges such as spatio-temporal correlation feature extraction, annotation cost, and sample imbalance.

Method: The proposed method includes a graph neural network backbone incorporating spatio-temporal correlation features alongside a multi-task self-supervised learning strategy with pre-training, graph prompting, and fine-tuning mechanisms.

Result: The model achieved F1 metrics of 91.30% on a public dataset and 92.31% on a real-world dataset, outperforming existing methods and demonstrating robust detection and generalization.

Conclusion: This approach successfully enhances anomaly detection in WSNs with reduced training cost and improved generalization through innovative use of graph neural networks and self-supervised learning strategies.

Abstract: Anomaly detection of multi-temporal modal data in Wireless Sensor Network (WSN) can provide an important guarantee for reliable network operation. Existing anomaly detection methods in multi-temporal modal data scenarios have the problems of insufficient extraction of spatio-temporal correlation features, high cost of anomaly sample category annotation, and imbalance of anomaly samples. In this paper, a graph neural network anomaly detection backbone network incorporating spatio-temporal correlation features and a multi-task self-supervised training strategy of "pre-training - graph prompting - fine-tuning" are designed for the characteristics of WSN graph structure data. First, the anomaly detection backbone network is designed by improving the Mamba model based on a multi-scale strategy and inter-modal fusion method, and combining it with a variational graph convolution module, which is capable of fully extracting spatio-temporal correlation features in the multi-node, multi-temporal modal scenarios of WSNs. Secondly, we design a three-subtask learning "pre-training" method with no-negative comparative learning, prediction, and reconstruction to learn generic features of WSN data samples from unlabeled data, and design a "graph prompting-fine-tuning" mechanism to guide the pre-trained self-supervised learning. The model is fine-tuned through the "graph prompting-fine-tuning" mechanism to guide the pre-trained self-supervised learning model to complete the parameter fine-tuning, thereby reducing the training cost and enhancing the detection generalization performance. The F1 metrics obtained from experiments on the public dataset and the actual collected dataset are up to 91.30% and 92.31%, respectively, which provides better detection performance and generalization ability than existing methods designed by the method.

</details>


### [704] [A Boolean Function-Theoretic Framework for Expressivity in GNNs with Applications to Fair Graph Mining](https://arxiv.org/abs/2601.12751)
*Manjish Pal*

Main category: cs.LG

TL;DR: The paper proposes a Boolean-function-based framework for evaluating Graph Neural Network (GNN) expressivity, introducing Subpopulation Boolean Isomorphism (SBI) and a fairness algorithm to address complex subpopulation scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing frameworks in analyzing GNN expressivity and their ability to ensure fairness across complex subpopulations.

Method: Introduced the Subpopulation Boolean Isomorphism (SBI) invariant, grounded in Boolean function theory, and developed a circuit-traversal-based fairness algorithm that handles high-complexity Boolean functions.

Result: The proposed fairness algorithm outperformed existing baselines by achieving low fairness gaps across intersectional groups in real-world graph datasets.

Conclusion: The study advances GNN expressivity analysis by linking it with fairness evaluations, providing a groundbreaking approach for addressing fairness in complex subpopulations.

Abstract: We propose a novel expressivity framework for Graph Neural Networks (GNNs) grounded in Boolean function theory, enabling a fine-grained analysis of their ability to capture complex subpopulation structures. We introduce the notion of \textit{Subpopulation Boolean Isomorphism} (SBI) as an invariant that strictly subsumes existing expressivity measures such as Weisfeiler-Lehman (WL), biconnectivity-based, and homomorphism-based frameworks. Our theoretical results identify Fourier degree, circuit class (AC$^0$, NC$^1$), and influence as key barriers to expressivity in fairness-aware GNNs. We design a circuit-traversal-based fairness algorithm capable of handling subpopulations defined by high-complexity Boolean functions, such as parity, which break existing baselines. Experiments on real-world graphs show that our method achieves low fairness gaps across intersectional groups where state-of-the-art methods fail, providing the first principled treatment of GNN expressivity tailored to fairness.

</details>


### [705] [Eddy-Resolving Global Ocean Forecasting with Multi-Scale Graph Neural Networks](https://arxiv.org/abs/2601.12775)
*Yuta Hirabayashi,Daisuke Matusoka,Konobu Kimura*

Main category: cs.LG

TL;DR: Proposes a graph neural network for improved multi-scale, short-term global ocean forecasting.


<details>
  <summary>Details</summary>
Motivation: Despite advancements, global eddy-resolving ocean forecasting struggles to represent dynamics across spatial scales.

Method: Introduced a multi-scale graph neural network with encoder-processor-decoder architecture, using dual spherical meshes and atmospheric inputs.

Result: Enhanced short-term prediction skill and multi-scale ocean variability representation, validated by kinetic energy spectra and error comparisons.

Conclusion: The model advances accurate, data-driven global ocean forecasting with better handling of multi-scale dynamics.

Abstract: Research on data-driven ocean models has progressed rapidly in recent years; however, the application of these models to global eddy-resolving ocean forecasting remains limited. The accurate representation of ocean dynamics across a wide range of spatial scales remains a major challenge in such applications. This study proposes a multi-scale graph neural network-based ocean model for 10-day global forecasting that improves short-term prediction skill and enhances the representation of multi-scale ocean variability. The model employs an encoder-processor-decoder architecture and uses two spherical meshes with different resolutions to better capture the multi-scale nature of ocean dynamics. In addition, the model incorporates surface atmospheric variables along with ocean state variables as node inputs to improve short-term prediction accuracy by representing atmospheric forcing. Evaluation using surface kinetic energy spectra and case studies shows that the model accurately represents a broad range of spatial scales, while root mean square error comparisons demonstrate improved skill in short-term predictions. These results indicate that the proposed model delivers more accurate short-term forecasts and improved representation of multi-scale ocean dynamics, thereby highlighting its potential to advance data-driven, eddy-resolving global ocean forecasting.

</details>


### [706] [Distilling Time Series Foundation Models for Efficient Forecasting](https://arxiv.org/abs/2601.12785)
*Yuqi Li,Kuiye Ding,Chuanguang Yang,Szu-Yu Chen,Yingli Tian*

Main category: cs.LG

TL;DR: DistilTS is a framework for compressing large Time Series Foundation Models (TSFMs) to make deployment efficient, without losing significant forecasting accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the inefficiency and high-deployment cost of TSFMs due to their massive parameter sizes. General model compression methods face challenges due to the unique nature of time series forecasting.

Method: DistilTS introduces two solutions: (1) horizon-weighted objectives to address task difficulty discrepancy across forecasting horizons, and (2) a temporal alignment strategy to reduce architectural mismatches during distillation.

Result: DistilTS achieves similar forecasting accuracy as original large TSFMs, reduces the model's parameters by up to 1/150, and accelerates inference speed by up to 6000x on multiple benchmarks.

Conclusion: DistilTS is an effective model compression framework for TSFMs, demonstrating significant improvements in deployment efficiency while maintaining excellent forecasting performance.

Abstract: Time Series foundation models (TSFMs) deliver strong forecasting performance through large-scale pretraining, but their large parameter sizes make deployment costly. While knowledge distillation offers a natural and effective approach for model compression, techniques developed for general machine learning tasks are not directly applicable to time series forecasting due to the unique characteristics. To address this, we present DistilTS, the first distillation framework specifically designed for TSFMs. DistilTS addresses two key challenges: (1) task difficulty discrepancy, specific to forecasting, where uniform weighting makes optimization dominated by easier short-term horizons, while long-term horizons receive weaker supervision; and (2) architecture discrepancy, a general challenge in distillation, for which we design an alignment mechanism in the time series forecasting. To overcome these issues, DistilTS introduces horizon-weighted objectives to balance learning across horizons, and a temporal alignment strategy that reduces architectural mismatch, enabling compact models. Experiments on multiple benchmarks demonstrate that DistilTS achieves forecasting performance comparable to full-sized TSFMs, while reducing parameters by up to 1/150 and accelerating inference by up to 6000x. Code is available at: https://github.com/itsnotacie/DistilTS-ICASSP2026.

</details>


### [707] [Semi-supervised Instruction Tuning for Large Language Models on Text-Attributed Graphs](https://arxiv.org/abs/2601.12807)
*Zixing Song,Irwin King*

Main category: cs.LG

TL;DR: The paper proposes SIT-Graph, a semi-supervised pipeline for graph learning that leverages unlabeled node data and iterative self-training to improve instruction tuning with LLMs.


<details>
  <summary>Details</summary>
Motivation: To address challenges in graph learning where obtaining labels is costly and to exploit unlabeled nodes for better prediction performance.

Method: The SIT-Graph uses iterative self-training, initially fine-tuning the model on labeled nodes, then generating pseudo-responses for unlabeled nodes to augment the dataset, aligning the model progressively with node correlations.

Result: SIT-Graph achieves over 20% performance improvement in graph learning tasks under low label ratio settings compared to state-of-the-art methods.

Conclusion: SIT-Graph effectively bridges the gap in leveraging unlabeled node data, enhancing graph instruction tuning methods' performance while being adaptable to different LLM-based frameworks.

Abstract: The emergent reasoning capabilities of Large Language Models (LLMs) offer a transformative paradigm for analyzing text-attributed graphs. While instruction tuning is the prevailing method for adapting pre-trained LLMs to graph learning tasks like node classification, it requires a substantial volume of annotated (INSTRUCTION, OUTPUT) pairs deriving from labeled nodes. This requirement is particularly prohibitive in the social domain, where obtaining expert labels for sensitive or evolving content is costly and slow. Furthermore, standard graph instruction tuning fails to exploit the vast amount of unlabeled nodes, which contain latent correlations due to edge connections that are beneficial for downstream predictions. To bridge this gap, we propose a novel Semi-supervised Instruction Tuning pipeline for Graph Learning, named SIT-Graph. Notably, SIT-Graph is model-agnostic and can be seamlessly integrated into any graph instruction tuning method that utilizes LLMs as the predictor. SIT-Graph operates via an iterative self-training process. Initially, the model is fine-tuned using instruction pairs constructed solely from the labeled nodes. Then it generates confidence-filtered pseudo-responses for unlabeled nodes to strategically augment the dataset for the next round of fine-tuning. Finally, this iterative refinement progressively aligns the LLM with the underlying node correlations. Extensive experiments demonstrate that when incorporated into state-of-the-art graph instruction tuning methods, SIT-Graph significantly enhances their performance on text-attributed graph benchmarks, achieving over 20% improvement under the low label ratio settings.

</details>


### [708] [Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning](https://arxiv.org/abs/2601.12816)
*Ishir Garg,Neel Kolhe,Andy Peng,Rohan Gopalam*

Main category: cs.LG

TL;DR: This paper presents FOPNG, an optimizer for continual learning that prevents forgetting previous tasks by enforcing Fisher-orthogonal constraints on parameter updates, achieving strong results on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: The main motivation is to develop a method for neural networks to learn sequential tasks without suffering from catastrophic forgetting, an issue common in continual learning.

Method: The paper introduces the FOPNG optimizer, which uses Fisher-orthogonal constraints to project gradients onto a Fisher-orthogonal complement, ensuring descent in Fisher space and preserving learned knowledge.

Result: The approach is theoretically analyzed, efficiently implemented, and demonstrates strong performance on typical continual learning benchmarks such as Permuted-MNIST, Split-MNIST, and Split-CIFAR.

Conclusion: FOPNG unifies natural gradient descent and orthogonal gradient methods, offering a reparameterization-invariant and theoretically sound solution to continual learning issues.

Abstract: Continual learning aims to enable neural networks to acquire new knowledge on sequential tasks. However, the key challenge in such settings is to learn new tasks without catastrophically forgetting previously learned tasks. We propose the Fisher-Orthogonal Projected Natural Gradient Descent (FOPNG) optimizer, which enforces Fisher-orthogonal constraints on parameter updates to preserve old task performance while learning new tasks. Unlike existing methods that operate in Euclidean parameter space, FOPNG projects gradients onto the Fisher-orthogonal complement of previous task gradients. This approach unifies natural gradient descent with orthogonal gradient methods within an information-geometric framework. The resulting update direction is invariant under reparameterization, guarantees descent in the Fisher metric, and helps preserve prior task outputs. We provide theoretical analysis establishing the properties of the projected update, describe efficient and practical implementations using the diagonal Fisher, and demonstrate strong results on standard continual learning benchmarks such as Permuted-MNIST, Split-MNIST, Rotated-MNIST, Split-CIFAR10, and Split-CIFAR100.

</details>


### [709] [Knowledge-Integrated Representation Learning for Crypto Anomaly Detection under Extreme Label Scarcity; Relational Domain-Logic Integration with Retrieval-Grounded Context and Path-Level Explanations](https://arxiv.org/abs/2601.12839)
*Gyuyeon Na,Minjung Park,Soyoun Kim,Jungbin Shin,Sangmi Chai*

Main category: cs.LG

TL;DR: The paper presents RDLI, a framework that integrates domain logic into representation learning to combat the challenges of detecting anomalous trajectories in decentralized crypto networks under extreme label scarcity and adaptive evasion by illicit actors.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by extreme label scarcity and the sophisticated evasion strategies of illicit actors in decentralized crypto networks, particularly to fulfill regulatory standards like the FATF Travel Rule.

Method: The paper proposes Relational Domain Logic Integration (RDLI), which embeds expert-derived heuristics as differentiable logic-aware latent signals within representation learning and introduces a Retrieval Grounded Context (RGC) module to account for market and regulatory context.

Result: RDLI achieves a 28.9% improvement in F1 score over state-of-the-art GNN baselines under extreme label scarcity (0.01%). Additionally, a micro expert user study confirms that RDLI significantly improves trustworthiness, perceived usefulness, and clarity in path-level explanations.

Conclusion: By embedding domain logic and contextual grounding, RDLI demonstrates enhanced accuracy and explainability for detecting anomalous crypto transactions, making it a robust tool against regulatory challenges and adaptive evasion strategies.

Abstract: Detecting anomalous trajectories in decentralized crypto networks is fundamentally challenged by extreme label scarcity and the adaptive evasion strategies of illicit actors. While Graph Neural Networks (GNNs) effectively capture local structural patterns, they struggle to internalize multi hop, logic driven motifs such as fund dispersal and layering that characterize sophisticated money laundering, limiting their forensic accountability under regulations like the FATF Travel Rule. To address this limitation, we propose Relational Domain Logic Integration (RDLI), a framework that embeds expert derived heuristics as differentiable, logic aware latent signals within representation learning. Unlike static rule based approaches, RDLI enables the detection of complex transactional flows that evade standard message passing. To further account for market volatility, we incorporate a Retrieval Grounded Context (RGC) module that conditions anomaly scoring on regulatory and macroeconomic context, mitigating false positives caused by benign regime shifts. Under extreme label scarcity (0.01%), RDLI outperforms state of the art GNN baselines by 28.9% in F1 score. A micro expert user study further confirms that RDLI path level explanations significantly improve trustworthiness, perceived usefulness, and clarity compared to existing methods, highlighting the importance of integrating domain logic with contextual grounding for both accuracy and explainability.

</details>


### [710] [Generating Cyclic Conformers with Flow Matching in Cremer-Pople Coordinates](https://arxiv.org/abs/2601.12859)
*Luca Schaufelberger,Aline Hartgers,Kjell Jorner*

Main category: cs.LG

TL;DR: The paper introduces PuckerFlow, a machine learning model for generating conformers of cyclic molecules, focusing on efficient and accurate sampling using the Cremer-Pople coordinate system.


<details>
  <summary>Details</summary>
Motivation: Cyclic molecules play vital roles in drug discovery and catalysis due to their structural pre-organization, but generating accurate conformers of such systems remains challenging.

Method: PuckerFlow utilizes flow matching on the Cremer-Pople space, which is a low-dimensional coordinate system specific to ring conformations, to generate accurate and diverse cyclic structures.

Result: PuckerFlow significantly outperforms existing conformer generation methods in terms of both precision and diversity, specifically benefiting chemical applications like catalysis and drug discovery.

Conclusion: PuckerFlow enables efficient, reliable conformer generation of cyclic structures, facilitating advances in understanding structure-property relationships and enabling property-guided design in various applications.

Abstract: Cyclic molecules are ubiquitous across applications in chemistry and biology. Their restricted conformational flexibility provides structural pre-organization that is key to their function in drug discovery and catalysis. However, reliably sampling the conformer ensembles of ring systems remains challenging. Here, we introduce PuckerFlow, a generative machine learning model that performs flow matching on the Cremer-Pople space, a low-dimensional internal coordinate system capturing the relevant degrees of freedom of rings. Our approach enables generation of valid closed rings by design and demonstrates strong performance in generating conformers that are both diverse and precise. We show that PuckerFlow outperforms other conformer generation methods on nearly all quantitative metrics and illustrate the potential of PuckerFlow for ring systems relevant to chemical applications, particularly in catalysis and drug discovery. This work enables efficient and reliable conformer generation of cyclic structures, paving the way towards modeling structure-property relationships and the property-guided generation of rings across a wide range of applications in chemistry and biology.

</details>


### [711] [Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition](https://arxiv.org/abs/2601.12879)
*Mohammed Mudassir Uddin,Shahnawaz Alam,Mohammed Kaif Pasha*

Main category: cs.LG

TL;DR: The paper introduces Hierarchical Attribution Graph Decomposition (HAGD) for improving the interpretability of large language models by simplifying circuit discovery complexity and validating discovered patterns.


<details>
  <summary>Details</summary>
Motivation: Mechanistic interpretability faces challenges in extracting sparse computational circuits from large language models due to search complexity and polysemanticity.

Method: The framework combines multi-resolution abstraction hierarchies, differentiable circuit search, cross-layer transcoders, graph neural network meta-learning, and causal intervention protocols.

Result: HAGD achieves up to 91% behavioral preservation in modular arithmetic tasks and reveals moderate structural similarity (~67%) in discovered circuits across models.

Conclusion: The study offers preliminary advancements for scaling interpretability efforts in large models while highlighting limitations and the need for future improvements in attribution methodologies.

Abstract: Mechanistic interpretability seeks to reverse-engineer neural network computations into human-understandable algorithms, yet extracting sparse computational circuits from billion-parameter language models remains challenging due to exponential search complexity and pervasive polysemanticity. The proposed Hierarchical Attribution Graph Decomposition (HAGD) framework reduces circuit discovery complexity from O(2^n) exhaustive enumeration to O(n^2 log n) through multi-resolution abstraction hierarchies and differentiable circuit search. The methodology integrates cross-layer transcoders for monosemantic feature extraction, graph neural network meta-learning for topology prediction, and causal intervention protocols for validation. Empirical evaluation spans GPT-2 variants, Llama-7B through Llama-70B, and Pythia suite models across algorithmic tasks and natural language benchmarks. On modular arithmetic tasks, the framework achieves up to 91% behavioral preservation ($\pm$2.3\% across runs) while maintaining interpretable subgraph sizes. Cross-architecture transfer experiments suggest that discovered circuits exhibit moderate structural similarity (averaging 67%) across model families, indicating potential shared computational patterns. These results provide preliminary foundations for interpretability at larger model scales while identifying significant limitations in current attribution methodologies that require future advances.

</details>


### [712] [AdaNODEs: Test Time Adaptation for Time Series Forecasting Using Neural ODEs](https://arxiv.org/abs/2601.12893)
*Ting Dang,Soumyajit Chatterjee,Hong Jia,Yu Wu,Flora Salim,Fahim Kawsar*

Main category: cs.LG

TL;DR: AdaNODEs introduces a source-free test time adaptation (TTA) method for time series forecasting, aimed at adapting pre-trained models to unseen distributions using an innovative approach.


<details>
  <summary>Details</summary>
Motivation: Current TTA methods focus on independent data, neglecting time series data and forecasting tasks. AdaNODEs addresses this gap for time series forecasting under distribution shifts.

Method: AdaNODEs leverages Neural Ordinary Differential Equations (NODEs) and introduces a novel framework with a specialized loss function for time series forecasting TTA while updating limited model parameters to save memory.

Result: Experiments show AdaNODEs achieves relative performance improvements of 5.88% (one-dimensional) and 28.4% (high-dimensional) over SOTA baselines.

Conclusion: AdaNODEs successfully adapts models to time series data, demonstrating robustness under severe distribution shifts and memory efficiency.

Abstract: Test time adaptation (TTA) has emerged as a promising solution to adapt pre-trained models to new, unseen data distributions using unlabeled target domain data. However, most TTA methods are designed for independent data, often overlooking the time series data and rarely addressing forecasting tasks. This paper presents AdaNODEs, an innovative source-free TTA method tailored explicitly for time series forecasting. By leveraging Neural Ordinary Differential Equations (NODEs), we propose a novel adaptation framework that accommodates the unique characteristics of distribution shifts in time series data. Moreover, we innovatively propose a new loss function to tackle TTA for forecasting tasks. AdaNODEs only requires updating limited model parameters, showing effectiveness in capturing temporal dependencies while avoiding significant memory usage. Extensive experiments with one- and high-dimensional data demonstrate that AdaNODEs offer relative improvements of 5.88\% and 28.4\% over the SOTA baselines, especially demonstrating robustness across higher severity distribution shifts.

</details>


### [713] [Supervised Learning for the (s,S) Inventory Model with General Interarrival Demands and General Lead Times](https://arxiv.org/abs/2601.12900)
*Eliran Sherzer,Yonit Barron*

Main category: cs.LG

TL;DR: The paper proposes a neural network-based approach to efficiently predict performance metrics of (s,S) inventory systems without relying on expensive simulations.


<details>
  <summary>Details</summary>
Motivation: Non-Markovian (s,S) inventory systems are analytically complex, requiring costly simulations for performance evaluation.

Method: Uses supervised learning with neural networks, trained on simulation-generated labels, using low-order moments of distributions as input.

Result: The neural network achieves high accuracy in predicting key metrics like inventory levels and lost sales probability, replacing simulation runs.

Conclusion: The approach simplifies analysis of stochastic inventory systems, extends to other models, and offers a fast, efficient alternative to traditional methods.

Abstract: The continuous-review (s,S) inventory model is a cornerstone of stochastic inventory theory, yet its analysis becomes analytically intractable when dealing with non-Markovian systems. In such systems, evaluating long-run performance measures typically relies on costly simulation.
  This paper proposes a supervised learning framework via a neural network model for approximating stationary performance measures of (s,S) inventory systems with general distributions for the interarrival time between demands and lead times under lost sales. Simulations are first used to generate training labels, after which the neural network is trained. After training, the neural network provides almost instantaneous predictions of various metrics of the system, such as the stationary distribution of inventory levels, the expected cycle time, and the probability of lost sales. We find that using a small number of low-order moments of the distributions as input is sufficient to train the neural networks and to accurately capture the steady-state distribution. Extensive numerical experiments demonstrate high accuracy over a wide range of system parameters. As such, it effectively replaces repeated and costly simulation runs. Our framework is easily extendable to other inventory models, offering an efficient and fast alternative for analyzing complex stochastic systems.

</details>


### [714] [Deep Temporal Graph Clustering: A Comprehensive Benchmark and Datasets](https://arxiv.org/abs/2601.12903)
*Meng Liu,Ke Liang,Siwei Wang,Xingchen Hu,Sihang Zhou,Xinwang Liu*

Main category: cs.LG

TL;DR: Temporal Graph Clustering (TGC) focuses on clustering in temporal graphs with time and space balance via interaction-sequence batch processing. BenchTGC proposes a framework and datasets to address clustering technique and dataset challenges in TGC.


<details>
  <summary>Details</summary>
Motivation: Despite its importance in analyzing dynamically changing scenarios, TGC has been hindered by a lack of appropriate clustering techniques and datasets.

Method: Developed the BenchTGC framework to adapt clustering techniques for temporal graphs and created BenchTGC datasets for TGC, solving both methodological and dataset challenges.

Result: BenchTGC demonstrated improvements in TGC tasks and validated the significance of temporal graph clustering through extensive experiments.

Conclusion: BenchTGC establishes the foundation for exploring TGC tasks by solving its challenges, showcasing its potential in addressing real-world dynamic and complex scenarios.

Abstract: Temporal Graph Clustering (TGC) is a new task with little attention, focusing on node clustering in temporal graphs. Compared with existing static graph clustering, it can find the balance between time requirement and space requirement (Time-Space Balance) through the interaction sequence-based batch-processing pattern. However, there are two major challenges that hinder the development of TGC, i.e., inapplicable clustering techniques and inapplicable datasets. To address these challenges, we propose a comprehensive benchmark, called BenchTGC. Specially, we design a BenchTGC Framework to illustrate the paradigm of temporal graph clustering and improve existing clustering techniques to fit temporal graphs. In addition, we also discuss problems with public temporal graph datasets and develop multiple datasets suitable for TGC task, called BenchTGC Datasets. According to extensive experiments, we not only verify the advantages of BenchTGC, but also demonstrate the necessity and importance of TGC task. We wish to point out that the dynamically changing and complex scenarios in real world are the foundation of temporal graph clustering. The code and data is available at: https://github.com/MGitHubL/BenchTGC.

</details>


### [715] [An efficient heuristic for geometric analysis of cell deformations](https://arxiv.org/abs/2601.12928)
*Yaima Paz Soto,Silena Herold Garcia,Ximo Gual-Arnau,Antoni Jaume-i-Capó,Manuel González-Hidalgo*

Main category: cs.LG

TL;DR: The paper proposes a refined method for automated classification of sickle cells in blood images by employing fixed parameterization and template alignment to enhance efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To reduce healthcare burdens associated with sickle cell disease by improving automated classification of sickle cells, which is critical in resource-limited regions for accurate and efficient analysis.

Method: A fixed parameterization approach based on a cell's major axis is deployed, and shapes are aligned with templates before computing shape distances, ensuring efficiency and accuracy in classification.

Result: Achieved a 96.03% accuracy rate in both supervised classification and unsupervised clustering of sickle and healthy erythrocytes.

Conclusion: The method provides an efficient and accurate solution for erythrocyte classification, leveraging shape space models while reducing computational complexity.

Abstract: Sickle cell disease causes erythrocytes to become sickle-shaped, affecting their movement in the bloodstream and reducing oxygen delivery. It has a high global prevalence and places a significant burden on healthcare systems, especially in resource-limited regions. Automated classification of sickle cells in blood images is crucial, allowing the specialist to reduce the effort required and avoid errors when quantifying the deformed cells and assessing the severity of a crisis. Recent studies have proposed various erythrocyte representation and classification methods. Since classification depends solely on cell shape, a suitable approach models erythrocytes as closed planar curves in shape space. This approach employs elastic distances between shapes, which are invariant under rotations, translations, scaling, and reparameterizations, ensuring consistent distance measurements regardless of the curves' position, starting point, or traversal speed. While previous methods exploiting shape space distances had achieved high accuracy, we refined the model by considering the geometric characteristics of healthy and sickled erythrocytes. Our method proposes (1) to employ a fixed parameterization based on the major axis of each cell to compute distances and (2) to align each cell with two templates using this parameterization before computing distances. Aligning shapes to templates before distance computation, a concept successfully applied in areas such as molecular dynamics, and using a fixed parameterization, instead of minimizing distances across all possible parameterizations, simplifies calculations. This strategy achieves 96.03\% accuracy rate in both supervised classification and unsupervised clustering. Our method ensures efficient erythrocyte classification, maintaining or improving accuracy over shape space models while significantly reducing computational costs.

</details>


### [716] [Deterministic Dynamics of Sampling Processes in Score-Based Diffusion Models with Multiplicative Noise Conditioning](https://arxiv.org/abs/2601.12965)
*Doheon Kim*

Main category: cs.LG

TL;DR: Score-based diffusion models employ a structure that, despite limitations in learning the complete score function, effectively generates high-quality samples backed by theoretical exploration.


<details>
  <summary>Details</summary>
Motivation: To theoretically understand why score-based diffusion models still work well in practice despite limitations in their ability to represent general relationships between spatial variables and noise.

Method: Theoretical analysis of the deterministic dynamics in the differential equations underlying the score-based diffusion model sampling process.

Result: Insight into the operational mechanism of score-based diffusion models, explaining their empirical success despite constrained expressivity.

Conclusion: Score-based models, even with a structured representation limiting their ability to learn the full score, perform effectively due to the underlying deterministic dynamics.

Abstract: Score-based diffusion models generate new samples by learning the score function associated with a diffusion process. While the effectiveness of these models can be theoretically explained using differential equations related to the sampling process, previous work by Song and Ermon (2020) demonstrated that neural networks using multiplicative noise conditioning can still generate satisfactory samples. In this setup, the model is expressed as the product of two functions: one depending on the spatial variable and the other on the noise magnitude. This structure limits the model's ability to represent a more general relationship between the spatial variable and the noise, indicating that it cannot fully learn the correct score. Despite this limitation, the models perform well in practice. In this work, we provide a theoretical explanation for this phenomenon by studying the deterministic dynamics of the associated differential equations, offering insight into how the model operates.

</details>


### [717] [Architecture-Optimization Co-Design for Physics-Informed Neural Networks Via Attentive Representations and Conflict-Resolved Gradients](https://arxiv.org/abs/2601.12971)
*Pancheng Niu,Jun Guo,Qiaolin He,Yongming Chen,Yanchao Shi*

Main category: cs.LG

TL;DR: This paper introduces Architecture-Conflict-Resolved PINN (ACR-PINN) by incorporating a dynamic attention mechanism and conflict-resolved gradient strategy, improving the performance of Physics-Informed Neural Networks (PINNs) in solving PDEs.


<details>
  <summary>Details</summary>
Motivation: In conventional PINNs, performance is restricted by limited representational capacity and optimization difficulties due to conflicting gradients and constraints from physical laws.

Method: The authors propose the Layer-wise Dynamic Attention PINN (LDA-PINN) for enhanced representation and the Gradient-Conflict-Resolved PINN (GC-PINN) for resolving gradient interference. A unified model, ACR-PINN, is developed by combining these strategies while preserving the original loss formulation.

Result: ACR-PINN demonstrated superior performance on multiple PDE benchmarks, achieving faster convergence and lower $L_2$ and $L_\infty$ errors compared to typical PINNs.

Conclusion: The study shows that co-designing architecture and optimization strategies can significantly enhance the robustness and accuracy of PINN-based solvers for solving PDEs.

Abstract: Physics-Informed Neural Networks (PINNs) provide a learning-based framework for solving partial differential equations (PDEs) by embedding governing physical laws into neural network training. In practice, however, their performance is often hindered by limited representational capacity and optimization difficulties caused by competing physical constraints and conflicting gradients. In this work, we study PINN training from a unified architecture-optimization perspective. We first propose a layer-wise dynamic attention mechanism to enhance representational flexibility, resulting in the Layer-wise Dynamic Attention PINN (LDA-PINN). We then reformulate PINN training as a multi-task learning problem and introduce a conflict-resolved gradient update strategy to alleviate gradient interference, leading to the Gradient-Conflict-Resolved PINN (GC-PINN). By integrating these two components, we develop the Architecture-Conflict-Resolved PINN (ACR-PINN), which combines attentive representations with conflict-aware optimization while preserving the standard PINN loss formulation. Extensive experiments on benchmark PDEs, including the Burgers, Helmholtz, Klein-Gordon, and lid-driven cavity flow problems, demonstrate that ACR-PINN achieves faster convergence and significantly lower relative $L_2$ and $L_\infty$ errors than standard PINNs. These results highlight the effectiveness of architecture-optimization co-design for improving the robustness and accuracy of PINN-based solvers.

</details>


### [718] [PaperGuide: Making Small Language-Model Paper-Reading Agents More Efficient](https://arxiv.org/abs/2601.12988)
*Zijian Wang,Tiancheng Huang,Hanqi Li,Da Ma,Lu Chen,Kai Yu*

Main category: cs.LG

TL;DR: PaperCompass integrates cognitive science-inspired approaches with reinforcement learning to improve efficiency and performance in reading and extracting information from scientific literature.


<details>
  <summary>Details</summary>
Motivation: To address challenges in scaling manual analysis of rapidly growing scientific literature and inefficiencies in existing methods using large language models.

Method: PaperCompass employs a two-step framework that drafts an explicit plan for high-level actions and executes detailed reasoning for fine-grained tasks. It introduces Draft-and-Follow Policy Optimization (DFPO), a custom reinforcement learning method to optimize this hierarchical process.

Result: Experiments on Paper-QA benchmarks show that PaperCompass achieves efficiency and high performance comparable to larger models, surpassing strong baselines.

Conclusion: By blending planning and execution with DFPO, PaperCompass effectively narrows the "knowing-doing" gap in LLMs, offering a resource-efficient solution in scientific literature analysis.

Abstract: The accelerating growth of the scientific literature makes it increasingly difficult for researchers to track new advances through manual reading alone. Recent progress in large language models (LLMs) has therefore spurred interest in autonomous agents that can read scientific papers and extract task-relevant information. However, most existing approaches rely either on heavily engineered prompting or on a conventional SFT-RL training pipeline, both of which often lead to excessive and low-yield exploration. Drawing inspiration from cognitive science, we propose PaperCompass, a framework that mitigates these issues by separating high-level planning from fine-grained execution. PaperCompass first drafts an explicit plan that outlines the intended sequence of actions, and then performs detailed reasoning to instantiate each step by selecting the parameters for the corresponding function calls. To train such behavior, we introduce Draft-and-Follow Policy Optimization (DFPO), a tailored RL method that jointly optimizes both the draft plan and the final solution. DFPO can be viewed as a lightweight form of hierarchical reinforcement learning, aimed at narrowing the `knowing-doing' gap in LLMs. We provide a theoretical analysis that establishes DFPO's favorable optimization properties, supporting a stable and reliable training process. Experiments on paper-based question answering (Paper-QA) benchmarks show that PaperCompass improves efficiency over strong baselines without sacrificing performance, achieving results comparable to much larger models.

</details>


### [719] [HT-GNN: Hyper-Temporal Graph Neural Network for Customer Lifetime Value Prediction in Baidu Ads](https://arxiv.org/abs/2601.13013)
*Xiaohui Zhao,Xinjian Zhao,Jiahui Zhang,Guoyu Liu,Houzhi Wang,Shu Wu*

Main category: cs.LG

TL;DR: The paper introduces a Hyper-Temporal Graph Neural Network (HT-GNN) for accurate lifetime value prediction in dynamic advertising environments.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of predicting lifetime value (LTV) in advertising caused by demographic heterogeneity and dynamic user behavior.

Method: It proposes HT-GNN, which includes a hypergraph-supervised module, a transformer-based temporal encoder, and a task-adaptive mixture-of-experts structure for forecasting LTV.

Result: The HT-GNN model significantly outperforms existing methods in experiments conducted on 15 million Baidu Ads users.

Conclusion: HT-GNN effectively overcomes demographic and behavioral challenges in LTV prediction and proves highly effective for multi-horizon forecasting in advertising.

Abstract: Lifetime value (LTV) prediction is crucial for news feed advertising, enabling platforms to optimize bidding and budget allocation for long-term revenue growth. However, it faces two major challenges: (1) demographic-based targeting creates segment-specific LTV distributions with large value variations across user groups; and (2) dynamic marketing strategies generate irregular behavioral sequences where engagement patterns evolve rapidly. We propose a Hyper-Temporal Graph Neural Network (HT-GNN), which jointly models demographic heterogeneity and temporal dynamics through three key components: (i) a hypergraph-supervised module capturing inter-segment relationships; (ii) a transformer-based temporal encoder with adaptive weighting; and (iii) a task-adaptive mixture-of-experts with dynamic prediction towers for multi-horizon LTV forecasting. Experiments on \textit{Baidu Ads} with 15 million users demonstrate that HT-GNN consistently outperforms state-of-the-art methods across all metrics and prediction horizons.

</details>


### [720] [PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning](https://arxiv.org/abs/2601.13020)
*Zhiyan Hou,Haiyun Guo,Haokai Ma,Yandu Sun,Yonghui Yang,Jinqiao Wang*

Main category: cs.LG

TL;DR: The study proposes improvements to multimodal large language models (MLLMs) for continual instruction tuning (CIT), addressing the issue of forgetting and misalignment in expert pathways.


<details>
  <summary>Details</summary>
Motivation: To enhance the adaptation of MLLMs to continuous streams of tasks without losing prior capabilities, specifically addressing problems with Misaligned Co-drift in Mixture-of-Experts (MoE) mechanisms.

Method: It introduces the PASs framework, which utilizes pathway activation subspaces for guiding routing and preserving task-specific pathways. Two components are PAS-guided Reweighting and PAS-aware Rank Stabilization.

Result: Using the PASs-based method, the approach significantly improves accuracy and reduces forgetting compared to existing baselines on a CIT benchmark.

Conclusion: The proposed method effectively mitigates forgetting and misaligned mechanisms in continual learning for MLLMs without adding parameters, showcasing broad improvements.

Abstract: Continual instruction tuning (CIT) requires multimodal large language models (MLLMs) to adapt to a stream of tasks without forgetting prior capabilities. A common strategy is to isolate updates by routing inputs to different LoRA experts. However, existing LoRA-based Mixture-of-Experts (MoE) methods often jointly update the router and experts in an indiscriminate way, causing the router's preferences to co-drift with experts' adaptation pathways and gradually deviate from early-stage input-expert specialization. We term this phenomenon Misaligned Co-drift, which blurs expert responsibilities and exacerbates forgetting.To address this, we introduce the pathway activation subspace (PASs), a LoRA-induced subspace that reflects which low-rank pathway directions an input activates in each expert, providing a capability-aligned coordinate system for routing and preservation. Based on PASs, we propose a fixed-capacity PASs-based MoE-LoRA method with two components: PAS-guided Reweighting, which calibrates routing using each expert's pathway activation signals, and PAS-aware Rank Stabilization, which selectively stabilizes rank directions important to previous tasks. Experiments on a CIT benchmark show that our approach consistently outperforms a range of conventional continual learning baselines and MoE-LoRA variants in both accuracy and anti-forgetting without adding parameters. Our code will be released upon acceptance.

</details>


### [721] [Enhancing Generalization in Sickle Cell Disease Diagnosis through Ensemble Methods and Feature Importance Analysis](https://arxiv.org/abs/2601.13021)
*Nataša Petrović,Gabriel Moyà-Alcover,Antoni Jaume-i-Capó,Jose Maria Buades Rubio*

Main category: cs.LG

TL;DR: The paper introduces a novel method for classifying Sickle Cell Disease using ensemble-based classifiers on pre-processed peripheral blood smear images.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance the generalization and interpretability of classification models for Sickle Cell Disease diagnosis by leveraging ensemble machine learning methods.

Method: The authors utilized pre-processing, segmentation, feature extraction, and ensemble classification (Random Forest and Extra Trees) while also identifying critical features for improved interpretability and efficiency.

Result: The proposed model achieved an F1-score of 90.71% and SDS-score of 93.33%, outperforming the previous Gradient Boosting classifier scores of F1 87.32% and SDS 89.51%.

Conclusion: The results highlight the significant improvement in classification performance and the utility of ensemble models coupled with feature selection for reliable Sickle Cell Disease diagnostics. Parameters, code, and data are also shared to promote reproducibility.

Abstract: This work presents a novel approach for selecting the optimal ensemble-based classification method and features with a primarly focus on achieving generalization, based on the state-of-the-art, to provide diagnostic support for Sickle Cell Disease using peripheral blood smear images of red blood cells. We pre-processed and segmented the microscopic images to ensure the extraction of high-quality features. To ensure the reliability of our proposed system, we conducted an in-depth analysis of interpretability. Leveraging techniques established in the literature, we extracted features from blood cells and employed ensemble machine learning methods to classify their morphology. Furthermore, we have devised a methodology to identify the most critical features for classification, aimed at reducing complexity and training time and enhancing interpretability in opaque models. Lastly, we validated our results using a new dataset, where our model overperformed state-of-the-art models in terms of generalization. The results of classifier ensembled of Random Forest and Extra Trees classifier achieved an harmonic mean of precision and recall (F1-score) of 90.71\% and a Sickle Cell Disease diagnosis support score (SDS-score) of 93.33\%. These results demonstrate notable enhancement from previous ones with Gradient Boosting classifier (F1-score 87.32\% and SDS-score 89.51\%). To foster scientific progress, we have made available the parameters for each model, the implemented code library, and the confusion matrices with the raw data.

</details>


### [722] [Analysis of Long Range Dependency Understanding in State Space Models](https://arxiv.org/abs/2601.13048)
*Srividya Ravikumar,Abhinav Anand,Shweta Verma,Mira Mezini*

Main category: cs.LG

TL;DR: The paper analyzes the interpretability of S4D (diagonalized state-space model) using kernel analysis in vulnerability detection tasks and identifies its varying long-range modeling capabilities.


<details>
  <summary>Details</summary>
Motivation: To improve the interpretability of S4D models, which typically prioritize predictive accuracy, by studying their impact on real-world applications.

Method: Conducting systematic time and frequency domain kernel analysis of the S4D model on a vulnerability detection task.

Result: Demonstrated that S4D's performance depends on architecture, and its kernel can act as low-pass, band-pass, or high-pass filters depending on the configuration.

Conclusion: Insights from the analysis can help guide the development of more effective S4D-based models.

Abstract: Although state-space models (SSMs) have demonstrated strong performance on long-sequence benchmarks, most research has emphasized predictive accuracy rather than interpretability. In this work, we present the first systematic kernel interpretability study of the diagonalized state-space model (S4D) trained on a real-world task (vulnerability detection in source code). Through time and frequency domain analysis of the S4D kernel, we show that the long-range modeling capability of S4D varies significantly under different model architectures, affecting model performance. For instance, we show that the depending on the architecture, S4D kernel can behave as low-pass, band-pass or high-pass filter. The insights from our analysis can guide future work in designing better S4D-based models.

</details>


### [723] [TinyML-Enabled IoT for Sustainable Precision Irrigation](https://arxiv.org/abs/2601.13054)
*Kamogelo Taueatsoala,Caitlyn Daniels,Angelina J. Ramsunar,Petrus Bronkhorst,Absalom E. Ezugwu*

Main category: cs.LG

TL;DR: This paper proposes an edge-first IoT framework incorporating TinyML for precision irrigation, utilizing affordable hardware to enable offline, intelligent decision-making with impressive model accuracy and significant water savings.


<details>
  <summary>Details</summary>
Motivation: To improve agricultural sustainability in small-scale farming communities affected by water scarcity and limited access to advanced technologies.

Method: The paper develops a four-layer IoT architecture using an ESP32 microcontroller, Raspberry Pi, environmental sensors, and an optimized TinyML model deployed for offline irrigation prediction.

Result: Gradient boosting outperformed other models with an R^2 of 0.9973 and MAPE of 0.99%, with deployment on ESP32 proving accurate (MAPE < 1%). Experimental setups showed significant water savings compared to traditional methods.

Conclusion: This work validates the economic and practical implementation of TinyML-based IoT systems for small-scale precision agriculture, enabling sustainability and scalability in rural farming environments.

Abstract: Small-scale farming communities are disproportionately affected by water scarcity, erratic climate patterns, and a lack of access to advanced, affordable agricultural technologies. To address these challenges, this paper presents a novel, edge-first IoT framework that integrates Tiny Machine Learning (TinyML) for intelligent, offline-capable precision irrigation. The proposed four-layer architecture leverages low-cost hardware, an ESP32 microcontroller as an edge inference node, and a Raspberry Pi as a local edge server to enable autonomous decision-making without cloud dependency. The system utilizes capacitive soil moisture, temperature, humidity, pH, and ambient light sensors for environmental monitoring. A rigorous comparative analysis of ensemble models identified gradient boosting as superior, achieving an R^2 score of 0.9973 and a Mean Absolute Percentage Error (MAPE) of 0.99%, outperforming a random forest model (R^2 = 0.9916, MAPE = 1.81%). This optimized model was converted and deployed as a lightweight TinyML inference engine on the ESP32 and predicts irrigation needs with exceptional accuracy (MAPE < 1%). Local communication is facilitated by an MQTT-based LAN protocol, ensuring reliable operation in areas with limited or no internet connectivity. Experimental validation in a controlled environment demonstrated a significant reduction in water usage compared to traditional methods, while the system's low-power design and offline functionality confirm its viability for sustainable, scalable deployment in resource-constrained rural settings. This work provides a practical, cost-effective blueprint for bridging the technological divide in agriculture and enhancing water-use efficiency through on-device artificial intelligence.

</details>


### [724] [METIS: Mentoring Engine for Thoughtful Inquiry & Solutions](https://arxiv.org/abs/2601.13075)
*Abhinav Rajeev Kumar,Dhruv Trehan,Paras Chopra*

Main category: cs.LG

TL;DR: The paper introduces METIS, an AI mentor designed to assist undergraduate research from idea to paper, outperforming GPT-5 and Claude Sonnet 4.5 in evaluations.


<details>
  <summary>Details</summary>
Motivation: Many students lack access to expert research mentorship, and there is a need for AI tools to bridge this gap in guiding research writing.

Method: METIS is a stage-aware AI assistant augmented with tools providing literature search, guidelines, methodology checks, and memory. It is evaluated against other AI models using various judge and rubric methodologies, including multi-turn tutoring scenarios.

Result: METIS outperforms Claude Sonnet 4.5 and GPT-5 in pairwise preferences, student rubric-based scores, and multi-turn tutoring evaluations for document-grounded writing stages.

Conclusion: AI-based mentors like METIS can serve as effective research-writing assistants for undergraduates, though challenges like routing, grounding, and classification issues need refinement.

Abstract: Many students lack access to expert research mentorship. We ask whether an AI mentor can move undergraduates from an idea to a paper. We build METIS, a tool-augmented, stage-aware assistant with literature search, curated guidelines, methodology checks, and memory. We evaluate METIS against GPT-5 and Claude Sonnet 4.5 across six writing stages using LLM-as-a-judge pairwise preferences, student-persona rubrics, short multi-turn tutoring, and evidence/compliance checks. On 90 single-turn prompts, LLM judges preferred METIS to Claude Sonnet 4.5 in 71% and to GPT-5 in 54%. Student scores (clarity/actionability/constraint-fit; 90 prompts x 3 judges) are higher across stages. In multi-turn sessions (five scenarios/agent), METIS yields slightly higher final quality than GPT-5. Gains concentrate in document-grounded stages (D-F), consistent with stage-aware routing and groundings failure modes include premature tool routing, shallow grounding, and occasional stage misclassification.

</details>


### [725] [Recursive Meta-Distillation: An Axiomatic Framework for Iterative Knowledge Refinement](https://arxiv.org/abs/2601.13100)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: The paper introduces a theoretical framework to analyze recursive knowledge distillation and its mathematical properties, focusing on convergence and stability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of formal understanding of recursive or multi-generation distillation, which has traditionally relied on empirical heuristics.

Method: The paper uses an axiomatic and operator-theoretic approach to define structural properties of recursive distillation and establishes the conditions for convergence under mild assumptions.

Result: Anchored recursive distillation was shown to induce contraction in KL divergence, leading to geometric convergence and fixed points under specific realizability and convexity assumptions.

Conclusion: This work provides foundational insights into the mathematical behavior of iterative distillation, helping characterize scenarios of convergence and stability without focusing on specific algorithms or architectures.

Abstract: Recent work in probability-domain knowledge distillation has established axiomatic frameworks for temperature scaling, multi-teacher aggregation, and bias-variance trade-offs in single-stage settings. However, the mathematical behavior of recursive or multi-generation distillation remains poorly understood, with prior approaches relying primarily on empirical heuristics. In this work, we introduce an axiomatic and operator-theoretic framework for recursive meta-distillation, formalizing iterative knowledge distillation as a sequence of probability-distribution operators with explicit anchoring to base teachers.
  We define structural axioms for valid meta-teacher construction and prove the existence of non-trivial operator families satisfying these axioms without specifying particular algorithms or loss functions. Under mild realizability and convexity assumptions, we show that anchored recursive distillation induces contraction in KL divergence, yielding geometric convergence to base teacher distributions and a unique, globally attractive fixed point.
  The contribution is foundational rather than algorithmic: the framework characterizes when recursive distillation is mathematically well-posed and convergent rather than error-accumulating, independent of model architecture, optimization details, or specific operator instantiations. These results provide a theoretical basis for understanding stability, bias-variance behavior, and failure modes in iterative and multi-teacher distillation under capacity constraints.

</details>


### [726] [FastAV: Efficient Token Pruning for Audio-Visual Large Language Model Inference](https://arxiv.org/abs/2601.13143)
*Chaeyoung Jung,Youngjoon Jang,Seungwoo Lee,Joon Son Chung*

Main category: cs.LG

TL;DR: FastAV is a token pruning framework designed for audio-visual large language models, aiming to reduce computational demands while preserving performance.


<details>
  <summary>Details</summary>
Motivation: Multimodal integration in AV-LLMs significantly increases token demands, yet token pruning strategies for these models have been underexplored.

Method: FastAV introduces a two-stage pruning strategy: global pruning in intermediate layers to remove less influential tokens, and fine pruning in later layers based on next token generation impact, without relying on full attention maps.

Result: FastAV reduces computational costs (over 40% fewer FLOPs) while maintaining or improving the performance of AV-LLMs.

Conclusion: FastAV offers an efficient and effective method to optimize AV-LLMs by reducing computational overhead without sacrificing accuracy, making it practical for real-world applications.

Abstract: In this work, we present FastAV, the first token pruning framework tailored for audio-visual large language models (AV-LLMs). While token pruning has been actively explored in standard large language models (LLMs) and vision-language models (LVLMs), its application to AV-LLMs has received little attention, even though multimodal integration substantially increases their token demands. To address this gap, we introduce a pruning strategy that utilizes attention weights to identify tokens emphasized at different stages and estimates their importance. Building on this analysis, FastAV applies a two-stage pruning strategy: (1) global pruning in intermediate layers to remove broadly less influential tokens, and (2) fine pruning in later layers considering the impact on next token generation. Notably, our method does not rely on full attention maps, which makes it fully compatible with efficient attention mechanisms such as FlashAttention. Extensive experiments demonstrate that FastAV reduces FLOPs by more than 40% on two representative AV-LLMs, while preserving or even improving model performance.

</details>


### [727] [Training instability in deep learning follows low-dimensional dynamical principles](https://arxiv.org/abs/2601.13160)
*Zhipeng Zhang,Zhenjie Yao,Kai Li,Lei Yang*

Main category: cs.LG

TL;DR: This paper provides a framework to analyze the stability of deep learning training processes, introducing a controlled perturbation auditing approach.


<details>
  <summary>Details</summary>
Motivation: Understanding the stability of deep learning training is crucial for improving reproducibility and scalability, as current training processes are vulnerable to perturbations and collapse.

Method: A unified dynamical perspective is proposed, characterizing training stability along four dimensions—optimization, data/environmental, parametric, and learning-signal—using controlled perturbation auditing without altering algorithms.

Result: Recurring patterns were identified: high performance may not correlate with stable training, controlled randomness benefits learning, and deviations in meta-states predict performance collapse.

Conclusion: Training stability is an intrinsic, measurable feature of learning systems, offering insights beyond just final performance metrics, and opening avenues to study dynamics systematically.

Abstract: Deep learning systems achieve remarkable empirical performance, yet the stability of the training process itself remains poorly understood. Training unfolds as a high-dimensional dynamical system in which small perturbations to optimization, data, parameters, or learning signals can induce abrupt and irreversible collapse, undermining reproducibility and scalability.
  We propose a unified dynamical perspective that characterizes training stability as an intrinsic property of learning systems, organized along four interacting dimensions: optimization, environmental/data, parametric, and learning-signal stability. We operationalize this perspective through controlled perturbation auditing of training trajectories, probing how learning dynamics respond to structured disturbances without modifying learning algorithms.
  Across reinforcement learning and large language model training, we identify three recurring regularities: high final performance is frequently decoupled from training stability; controlled stochasticity consistently buffers learning dynamics across paradigms; and deviations in low-dimensional latent meta-states systematically precede observable performance collapse. Together, these findings establish training stability as a measurable and comparable dynamical property of learning systems, providing a descriptive foundation for studying learning dynamics beyond final performance outcomes.

</details>


### [728] [NeuroShield: A Neuro-Symbolic Framework for Adversarial Robustness](https://arxiv.org/abs/2601.13162)
*Ali Shafiee Sarvestani,Jason Schmidt,Arman Roohi*

Main category: cs.LG

TL;DR: The paper presents a method called \DesignII, a neuro-symbolic framework, to improve robustness and explainability in deep neural networks, achieving improved adversarial accuracy and reduced complexity.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address adversarial vulnerability and a lack of interpretability in deep neural networks, which are critical limitations, especially in safety-sensitive applications like autonomous driving.

Method: The paper proposes \DesignII, a framework that incorporates symbolic rule supervision into neural networks by encoding domain knowledge as logical constraints and enforcing them with semantic and symbolic logic losses during training.

Result: Using the GTSRB dataset, the proposed method shows significant improvements in adversarial robustness (18.1% and 17.35% gains over adversarial-training baselines) without sacrificing accuracy on clean samples.

Conclusion: The study concludes that symbolic reasoning in neural networks can significantly enhance robustness and interpretability, demonstrating comparable or superior performance to existing defenses, even with simpler architectures and limited training.

Abstract: Adversarial vulnerability and lack of interpretability are critical limitations of deep neural networks, especially in safety-sensitive settings such as autonomous driving. We introduce \DesignII, a neuro-symbolic framework that integrates symbolic rule supervision into neural networks to enhance both adversarial robustness and explainability. Domain knowledge is encoded as logical constraints over appearance attributes such as shape and color, and enforced through semantic and symbolic logic losses applied during training. Using the GTSRB dataset, we evaluate robustness against FGSM and PGD attacks at a standard $\ell_\infty$ perturbation budget of $\varepsilon = 8/255$. Relative to clean training, standard adversarial training provides modest improvements in robustness ($\sim$10 percentage points). Conversely, our FGSM-Neuro-Symbolic and PGD-Neuro-Symbolic models achieve substantially larger gains, improving adversarial accuracy by 18.1\% and 17.35\% over their corresponding adversarial-training baselines, representing roughly a three-fold larger robustness gain than standard adversarial training provides when both are measured relative to the same clean-training baseline, without reducing clean-sample accuracy. Compared to transformer-based defenses such as LNL-MoEx, which require heavy architectures and extensive data augmentation, our PGD-Neuro-Symbolic variant attains comparable or superior robustness using a ResNet18 backbone trained for 10 epochs. These results show that symbolic reasoning offers an effective path to robust and interpretable AI.

</details>


### [729] [LAViG-FLOW: Latent Autoregressive Video Generation for Fluid Flow Simulations](https://arxiv.org/abs/2601.13190)
*Vittoria De Pellegrini,Tariq Alkhalifah*

Main category: cs.LG

TL;DR: The study develops LAViG-FLOW, a video generation diffusion framework, for fast and accurate modeling of subsurface multiphase fluid flow in applications like CO2 sequestration and geothermal production.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to improve the efficiency and accuracy of subsurface fluid flow modeling, as high-fidelity simulators are computationally expensive when multiple simulations are needed for inversion and uncertainty quantification.

Method: The LAViG-FLOW framework uses a latent autoregressive structure, where state variables (saturation and pressure fields) are compressed via a dedicated 2D autoencoder, and their coupled distribution across time is modeled using a Video Diffusion Transformer. It is trained on time horizons and fine-tuned autoregressively to predict future data.

Result: LAViG-FLOW accurately generates consistent saturation and pressure fields over time and operates orders of magnitude faster than traditional numerical simulators, successfully evaluated on a CO2 sequestration dataset.

Conclusion: LAViG-FLOW provides an efficient and accurate alternative for modeling, offering significant speed improvement and maintaining accuracy over traditional solvers, thus enhancing its utility in subsurface fluid flow applications.

Abstract: Modeling and forecasting subsurface multiphase fluid flow fields underpin applications ranging from geological CO2 sequestration (GCS) operations to geothermal production. This is essential for ensuring both operational performance and long-term safety. While high fidelity multiphase simulators are widely used for this purpose, they become prohibitively expensive once many forward runs are required for inversion purposes and quantify uncertainty. To tackle this challenge we propose LAViG-FLOW, a latent autoregressive video generation diffusion framework that explicitly learns the coupled evolution of saturation and pressure fields. Each state variable is compressed by a dedicated 2D autoencoder, and a Video Diffusion Transformer (VDiT) models their coupled distribution across time. We first train the model on a given time horizon to learn their coupled relationship and then fine-tune it autoregressively so it can extrapolate beyond the observed time window. Evaluated on an open-source CO2 sequestration dataset, LAViG-FLOW generates saturation and pressure fields that stay consistent across time while running orders of magnitude faster than traditional numerical solvers.

</details>


### [730] [A Comprehensive Evaluation of LLM Reasoning: From Single-Model to Multi-Agent Paradigms](https://arxiv.org/abs/2601.13243)
*Yapeng Li,Jiakuo Yu,Zhixin Liu,Xinnan Liu,Jing Yu,Songze Li,Tonghua Su*

Main category: cs.LG

TL;DR: The study evaluates reasoning paradigms of LLMs, comparing performance, cost-accuracy trade-offs, and introduces a new benchmark, MIMeBench, for fine-grained semantic competence assessment.


<details>
  <summary>Details</summary>
Motivation: To better understand the effectiveness and cost-accuracy trade-offs of reasoning paradigms in Large Language Models (LLMs) and improve their application as reasoning systems.

Method: Conduct a unified evaluation of reasoning paradigms such as direct reasoning, CoT, and MAS using benchmarks, role-specific analyses, and cost-accuracy trade-offs. Introduce MIMeBench for assessing semantic abstraction and contrastive discrimination.

Result: Increased structural complexity in reasoning does not consistently improve performance, and its effectiveness depends on the paradigm's properties. MIMeBench provides insights into semantic competence beyond closed-form accuracy.

Conclusion: The study highlights the need to align reasoning paradigms with task suitability and emphasizes a nuanced approach to complexity for better reasoning outcomes.

Abstract: Large Language Models (LLMs) are increasingly deployed as reasoning systems, where reasoning paradigms - such as Chain-of-Thought (CoT) and multi-agent systems (MAS) - play a critical role, yet their relative effectiveness and cost-accuracy trade-offs remain poorly understood. In this work, we conduct a comprehensive and unified evaluation of reasoning paradigms, spanning direct single-model generation, CoT-augmented single-model reasoning, and representative MAS workflows, characterizing their reasoning performance across a diverse suite of closed-form benchmarks. Beyond overall performance, we probe role-specific capability demands in MAS using targeted role isolation analyses, and analyze cost-accuracy trade-offs to identify which MAS workflows offer a favorable balance between cost and accuracy, and which incur prohibitive overhead for marginal gains. We further introduce MIMeBench, a new open-ended benchmark that targets two foundational yet underexplored semantic capabilities - semantic abstraction and contrastive discrimination - thereby providing an alternative evaluation axis beyond closed-form accuracy and enabling fine-grained assessment of semantic competence that is difficult to capture with existing benchmarks. Our results show that increased structural complexity does not consistently lead to improved reasoning performance, with its benefits being highly dependent on the properties and suitability of the reasoning paradigm itself. The codes are released at https://gitcode.com/HIT1920/OpenLLMBench.

</details>


### [731] [Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence from Math and Domain-Shifted Benchmarks](https://arxiv.org/abs/2601.13244)
*Prateek Munjal,Clement Christophe,Ronnie Rajan,Praveenkumar Kanithi*

Main category: cs.LG

TL;DR: Instruction finetuning is found to have limitations, showing unstable performance gains and sensitivity to changes in evaluation settings and domains.


<details>
  <summary>Details</summary>
Motivation: To examine whether instruction finetuning in large language models (LLMs) enhances intrinsic reasoning capabilities or merely induces surface-level behavior.

Method: The study evaluates and compares base and instruction-tuned models through tasks including standard math benchmarks, structurally perturbed versions, and domain-shifted settings like MedCalc.

Result: Instruction-tuned models exhibited unstable performance, dependency on specific prompts, and a tendency to underperform in scenarios with domain distribution shifts and prompt structure changes.

Conclusion: Base models often outperform instruction-tuned ones in certain contexts, suggesting that instruction tuning alone does not guarantee robust reasoning or broader applicability.

Abstract: Instruction finetuning is standard practice for improving LLM performance, yet it remains unclear whether it enhances reasoning or merely induces surface-level pattern matching. We investigate this by evaluating base and instruction-tuned models on standard math benchmarks, structurally perturbed variants, and domain-shifted tasks. Our analysis highlights two key (often overlooked) limitations of instruction tuning. First, the performance advantage is unstable and depends heavily on evaluation settings. In zero-shot CoT settings on GSM8K, base models consistently outperform instruction-tuned variants, with drops as high as 32.67\% (Llama3-70B). Instruction-tuned models only match or exceed this performance when provided with few-shot exemplars, suggesting a reliance on specific prompting patterns rather than intrinsic reasoning. Second, tuning gains are brittle under distribution shift. Our results show that base models surpass instruction-tuned variants on the domain-specific MedCalc benchmark. Additionally, instruction-tuned models show sharp declines on perturbed datasets, indicating sensitivity to prompt structure over robust reasoning.

</details>


### [732] [Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning](https://arxiv.org/abs/2601.13284)
*Duygu Nur Yaldiz,Evangelia Spiliopoulou,Zheng Qi,Siddharth Varia,Srikanth Doss,Nikolaos Pappas*

Main category: cs.LG

TL;DR: The paper investigates how different fine-tuning methods impact the calibration of large language models, proposing a calibration-aware reinforcement learning approach to address overconfidence, improving model reliability.


<details>
  <summary>Details</summary>
Motivation: Large language models are increasingly used in decision-making tasks where accurate and reliable confidence estimates are crucial for downstream systems to make informed choices.

Method: The study evaluates calibration in two fine-tuning paradigms: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). It diagnoses RLVR's calibration issues and introduces a modified RL formulation that adjusts decision-token probabilities for better calibration.

Result: RLVR achieves higher task performance but leads to overconfident models, whereas SFT provides better calibration even under distribution shift. The proposed method reduces overconfidence and decreases Expected Calibration Error (ECE) scores by up to 9 points.

Conclusion: A calibration-aware reinforcement learning framework can maintain high task performance while addressing overconfidence in LLMs, making models more reliable for practical deployment.

Abstract: Large language models (LLMs) are increasingly deployed in decision-making tasks, where not only accuracy but also reliable confidence estimates are essential. Well-calibrated confidence enables downstream systems to decide when to trust a model and when to defer to fallback mechanisms. In this work, we conduct a systematic study of calibration in two widely used fine-tuning paradigms: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). We show that while RLVR improves task performance, it produces extremely overconfident models, whereas SFT yields substantially better calibration, even under distribution shift, though with smaller performance gains. Through targeted experiments, we diagnose RLVR's failure, showing that decision tokens act as extraction steps of the decision in reasoning traces and do not carry confidence information, which prevents reinforcement learning from surfacing calibrated alternatives. Based on this insight, we propose a calibration-aware reinforcement learning formulation that directly adjusts decision-token probabilities. Our method preserves RLVR's accuracy level while mitigating overconfidence, reducing ECE scores up to 9 points.

</details>


### [733] [CooperBench: Why Coding Agents Cannot be Your Teammates Yet](https://arxiv.org/abs/2601.13295)
*Arpandeep Khatua,Hao Zhu,Peter Tran,Arya Prabhudesai,Frederic Sadrieh,Johann K. Lieberwirth,Xinkai Yu,Yicheng Fu,Michael J. Ryan,Jiaxin Pei,Diyi Yang*

Main category: cs.LG

TL;DR: The study introduces CooperBench to test collaborative coding of AI agents, revealing their lack of coordination, unlike human teams.


<details>
  <summary>Details</summary>
Motivation: To address limitations in AI agents' social intelligence and collaboration capabilities in teamwork scenarios.

Method: The development of CooperBench benchmark featuring over 600 coding tasks requiring collaboration between agents, tested using state-of-the-art coding agents.

Result: Agents achieved 30% lower success rates in collaboration due to issues in communication, commitment, and understanding others' plans.

Conclusion: Future work in AI should focus on enhancing social intelligence to improve collaborative performance.

Abstract: Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.

</details>


### [734] [Verifying Local Robustness of Pruned Safety-Critical Networks](https://arxiv.org/abs/2601.13303)
*Minh Le,Phuong Cao*

Main category: cs.LG

TL;DR: The paper explores pruning impact on improving the formal verifiability of DNNs for safety-critical applications, using $α,β$-CROWN verifier on MNIST and NASA JPL datasets.


<details>
  <summary>Details</summary>
Motivation: The aim is to address the computational challenges of verifying large-scale DNN models and improve their reliability in high-stakes environments.

Method: Pruning techniques were applied at varying ratios to evaluate their effect on local robustness verification using ResNet4 and $α,β$-CROWN verifier.

Result: Different pruning ratios improved verifiability: light pruning on MNIST (40%) and heavy pruning on NASA datasets (70%-90%) enhanced robustness compared to unpruned models.

Conclusion: Pruning simplifies the model's search space for formal solvers, and optimal pruning ratios depend on the dataset, aiding efficient and reliable DNN deployment in critical applications.

Abstract: Formal verification of Deep Neural Networks (DNNs) is essential for safety-critical applications, ranging from surgical robotics to NASA JPL autonomous systems. However, the computational cost of verifying large-scale models remains a significant barrier to adoption. This paper investigates the impact of pruning on formal local robustness certificates with different ratios. Using the state-of-the-art $α,β$-CROWN verifier, we evaluate ResNet4 models across varying pruning ratios on MNIST and, more importantly, on the NASA JPL Mars Frost Identification datasets. Our findings demonstrate a non-linear relationship: light pruning (40%) in MNIST and heavy pruning (70%-90%) in JPL improve verifiability, allowing models to outperform unpruned baselines in proven $L_\infty$ robustness properties. This suggests that reduced connectivity simplifies the search space for formal solvers and that the optimal pruning ratio varies significantly between datasets. This research highlights the complex nature of model compression, offering critical insights into selecting the optimal pruning ratio for deploying efficient, yet formally verified, DNNs in high-stakes environments where reliability is non-negotiable.

</details>


### [735] [Beyond Mapping : Domain-Invariant Representations via Spectral Embedding of Optimal Transport Plans](https://arxiv.org/abs/2601.13350)
*Abdel Djalil Sad Saoud,Fred Maurice Ngolè Mboula,Hanane Slimani*

Main category: cs.LG

TL;DR: The paper addresses domain alignment in machine learning under distributional shifts using a novel approach based on spectral embedding of smoothed optimal transport-based methods, and demonstrates success in multiple tasks.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenge of poor machine learning performance caused by distributional shifts, necessitating robust domain alignment methods to improve unsupervised domain adaptation.

Method: The method proposes interpreting smoothed optimal transport plans as adjacency matrices of bipartite graphs, using spectral embedding derived from these graphs to achieve domain-invariant representations.

Result: The approach is evaluated on tasks like music genre recognition, music-speech discrimination, and electrical cable defect detection, demonstrating strong overall performance.

Conclusion: The proposed spectral embedding-based domain adaptation method proves effective in addressing distributional shifts, offering a promising technique for better generalization in challenging domains.

Abstract: Distributional shifts between training and inference time data remain a central challenge in machine learning, often leading to poor performance. It motivated the study of principled approaches for domain alignment, such as optimal transport based unsupervised domain adaptation, that relies on approximating Monge map using transport plans, which is sensitive to the transport problem regularization strategy and hyperparameters, and might yield biased domains alignment. In this work, we propose to interpret smoothed transport plans as adjacency matrices of bipartite graphs connecting source to target domain and derive domain-invariant samples' representations through spectral embedding. We evaluate our approach on acoustic adaptation benchmarks for music genre recognition, music-speech discrimination, as well as electrical cable defect detection and classification tasks using time domain reflection in different diagnosis settings, achieving overall strong performances.

</details>


### [736] [On the Relation of State Space Models and Hidden Markov Models](https://arxiv.org/abs/2601.13357)
*Aydin Ghojogh,M. Hadi Sepanj,Benyamin Ghojogh*

Main category: cs.LG

TL;DR: The paper compares SSMs, HMMs, Kalman filtering, and modern NLP-oriented SSMs, focusing on their inference, training, and structural relationships.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reconcile and analyze the relationship between classical probabilistic models and modern neural sequence models in the context of sequential data modeling.

Method: A unified comparison through probabilistic graphical models, inference algorithms, and optimization strategies, highlighting structural and semantic similarities and differences.

Result: Insights on when these models are equivalent, when they diverge, and their connection with modern NLP-focused models are provided.

Conclusion: The study bridges perspectives from control theory, probabilistic modeling, and modern deep learning to provide clarity on SSMs and HMMs in NLP.

Abstract: State Space Models (SSMs) and Hidden Markov Models (HMMs) are foundational frameworks for modeling sequential data with latent variables and are widely used in signal processing, control theory, and machine learning. Despite their shared temporal structure, they differ fundamentally in the nature of their latent states, probabilistic assumptions, inference procedures, and training paradigms. Recently, deterministic state space models have re-emerged in natural language processing through architectures such as S4 and Mamba, raising new questions about the relationship between classical probabilistic SSMs, HMMs, and modern neural sequence models.
  In this paper, we present a unified and systematic comparison of HMMs, linear Gaussian state space models, Kalman filtering, and contemporary NLP state space models. We analyze their formulations through the lens of probabilistic graphical models, examine their inference algorithms -- including forward-backward inference and Kalman filtering -- and contrast their learning procedures via Expectation-Maximization and gradient-based optimization. By highlighting both structural similarities and semantic differences, we clarify when these models are equivalent, when they fundamentally diverge, and how modern NLP SSMs relate to classical probabilistic models. Our analysis bridges perspectives from control theory, probabilistic modeling, and modern deep learning.

</details>


### [737] [CausationEntropy: Pythonic Optimal Causation Entropy](https://arxiv.org/abs/2601.13365)
*Kevin Slote,Jeremie Fish,Erik Bollt*

Main category: cs.LG

TL;DR: The paper introduces version 1.1 of the CausationEntropy Python package for causal network discovery, including new algorithms, data generators, and tools.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a robust and versatile tool for modeling causal networks in complex dynamical systems, distinguishing direct from indirect causal relationships.

Method: The paper elaborates on the implementation of the Optimal Causation Entropy (oCSE) framework with advanced algorithms and features like Gaussian, kNN, Geometric-kNN, KDE, and Poisson estimators.

Result: The updated package incorporates synthetic data generators, enhanced plotting capabilities, and algorithmic extensions for causal discovery, making it user-friendly and compatible with PyPi and GitHub.

Conclusion: The package is expected to act as a benchmark tool for studying causality in dynamic systems, offering extensive documentation and modular support for future developments.

Abstract: Optimal Causation Entropy (oCSE) is a robust causal network modeling technique that reveals causal networks from dynamical systems and coupled oscillators, distinguishing direct from indirect paths. CausationEntropy is a Python package that implements oCSE and several of its significant optimizations and methodological extensions. In this paper, we introduce the version 1.1 release of CausationEntropy, which includes new synthetic data generators, plotting tools, and several advanced information-theoretical causal network discovery algorithms with criteria for estimating Gaussian, k-nearest neighbors (kNN), geometric k-nearest neighbors (geometric-kNN), kernel density (KDE) and Poisson entropic estimators. The package is easy to install from the PyPi software repository, is thoroughly documented, supplemented with extensive code examples, and is modularly structured to support future additions. The entire codebase is released under the MIT license and is available on GitHub and through PyPi Repository. We expect this package to serve as a benchmark tool for causal discovery in complex dynamical systems.

</details>


### [738] [TrustEnergy: A Unified Framework for Accurate and Reliable User-level Energy Usage Prediction](https://arxiv.org/abs/2601.13422)
*Dahai Yu,Rongchao Xu,Dingyi Zhuang,Yuheng Bu,Shenhao Wang,Guang Wang*

Main category: cs.LG

TL;DR: TrustEnergy is proposed for accurate, reliable user-level energy usage prediction, improving prediction accuracy by 5.4% and uncertainty quantification by 5.7% compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to capture spatial correlations and lack scalable individualized prediction, as well as insufficiently address uncertainty in energy usage prediction caused by dynamic factors.

Method: TrustEnergy combines a Hierarchical Spatiotemporal Representation module for energy usage patterns with a Sequential Conformalized Quantile Regression module for adjusting uncertainty bounds.

Result: TrustEnergy outperformed state-of-the-art models, achieving higher accuracy and improved uncertainty quantification in predictions.

Conclusion: TrustEnergy is a significant advancement in energy usage prediction, providing both accuracy and reliable uncertainty measures for practical applications.

Abstract: Energy usage prediction is important for various real-world applications, including grid management, infrastructure planning, and disaster response. Although a plethora of deep learning approaches have been proposed to perform this task, most of them either overlook the essential spatial correlations across households or fail to scale to individualized prediction, making them less effective for accurate fine-grained user-level prediction. In addition, due to the dynamic and uncertain nature of energy usage caused by various factors such as extreme weather events, quantifying uncertainty for reliable prediction is also significant, but it has not been fully explored in existing work. In this paper, we propose a unified framework called TrustEnergy for accurate and reliable user-level energy usage prediction. There are two key technical components in TrustEnergy, (i) a Hierarchical Spatiotemporal Representation module to efficiently capture both macro and micro energy usage patterns with a novel memory-augmented spatiotemporal graph neural network, and (ii) an innovative Sequential Conformalized Quantile Regression module to dynamically adjust uncertainty bounds to ensure valid prediction intervals over time, without making strong assumptions about the underlying data distribution. We implement and evaluate our TrustEnergy framework by working with an electricity provider in Florida, and the results show our TrustEnergy can achieve a 5.4% increase in prediction accuracy and 5.7% improvement in uncertainty quantification compared to state-of-the-art baselines.

</details>


### [739] [A Learnable Wavelet Transformer for Long-Short Equity Trading and Risk-Adjusted Return Optimization](https://arxiv.org/abs/2601.13435)
*Shuozhe Li,Du Cheng,Leqi Liu*

Main category: cs.LG

TL;DR: The paper introduces WaveLSFormer, a model combining wavelet-based decomposition and Transformer architecture to optimize intraday financial trading strategies. It outperforms existing models in profitability and risk-adjusted returns.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in financial intraday trading caused by noise, non-stationarity, and cross-sectional dependencies amongst assets.

Method: WaveLSFormer combines a learnable wavelet front-end for frequency decomposition and a Transformer-based architecture with multi-scale processing and risk-aware optimization.

Result: WaveLSFormer outperformed MLP, LSTM, and traditional Transformers in profitability and risk-adjusted returns in experiments on financial data.

Conclusion: WaveLSFormer enhances intraday financial trading performance by integrating multi-scale decomposition and decision learning, proving superior against state-of-the-art methods.

Abstract: Learning profitable intraday trading policies from financial time series is challenging due to heavy noise, non-stationarity, and strong cross-sectional dependence among related assets. We propose \emph{WaveLSFormer}, a learnable wavelet-based long-short Transformer that jointly performs multi-scale decomposition and return-oriented decision learning. Specifically, a learnable wavelet front-end generates low-/high-frequency components via an end-to-end trained filter bank, guided by spectral regularizers that encourage stable and well-separated frequency bands. To fuse multi-scale information, we introduce a low-guided high-frequency injection (LGHI) module that refines low-frequency representations with high-frequency cues while controlling training stability. The model outputs a portfolio of long/short positions that is rescaled to satisfy a fixed risk budget, and is optimized directly with a trading objective and risk-aware regularization. Extensive experiments on five years of hourly data across six industry groups, evaluated over ten random seeds, demonstrate that WaveLSFormer consistently outperforms MLP, LSTM and Transformer backbones, with and without fixed discrete wavelet front-ends. On average in all industries, WaveLSFormer achieves a cumulative overall strategy return of $0.607 \pm 0.045$ and a Sharpe ratio of $2.157 \pm 0.166$, substantially improving both profitability and risk-adjusted returns over the strongest baselines.

</details>


### [740] [BladeSDF : Unconditional and Conditional Generative Modeling of Representative Blade Geometries Using Signed Distance Functions](https://arxiv.org/abs/2601.13445)
*Ashish S. Nair,Sandipp Krishnan Ravi,Itzel Salgado,Changjie Sun,Sayan Ghosh,Liping Wang*

Main category: cs.LG

TL;DR: This paper presents a generative AI framework using DeepSDF to create turbine blade geometries that are accurate, interpretable, and manufacturable while aligning with performance considerations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address gaps in performance-aware modeling and manufacturable design generation for complex 3D turbine blade geometries.

Method: It utilizes a continuous signed distance function (SDF) representation with DeepSDF, mapping engineering descriptors to latent codes for accurate and performance-informed geometry generation.

Result: The method achieves high fidelity in geometry reconstruction, with errors below 1% of the blade dimension and robust generalization to unseen designs.

Conclusion: This framework advances turbine blade modeling by integrating constraints and performance metrics into a practical, data-driven solution for designing manufacturable 3D geometries.

Abstract: Generative AI has emerged as a transformative paradigm in engineering design, enabling automated synthesis and reconstruction of complex 3D geometries while preserving feasibility and performance relevance. This paper introduces a domain-specific implicit generative framework for turbine blade geometry using DeepSDF, addressing critical gaps in performance-aware modeling and manufacturable design generation. The proposed method leverages a continuous signed distance function (SDF) representation to reconstruct and generate smooth, watertight geometries with quantified accuracy. It establishes an interpretable, near-Gaussian latent space that aligns with blade-relevant parameters, such as taper and chord ratios, enabling controlled exploration and unconditional synthesis through interpolation and Gaussian sampling. In addition, a compact neural network maps engineering descriptors, such as maximum directional strains, to latent codes, facilitating the generation of performance-informed geometry. The framework achieves high reconstruction fidelity, with surface distance errors concentrated within $1\%$ of the maximum blade dimension, and demonstrates robust generalization to unseen designs. By integrating constraints, objectives, and performance metrics, this approach advances beyond traditional 2D-guided or unconstrained 3D pipelines, offering a practical and interpretable solution for data-driven turbine blade modeling and concept generation.

</details>


### [741] [Quantum Qualifiers for Neural Network Model Selection in Hadronic Physics](https://arxiv.org/abs/2601.13463)
*Brandon B. Le,D. Keller*

Main category: cs.LG

TL;DR: This paper introduces a framework for selecting between classical and quantum machine-learning models using a predictive quantum qualifier, specifically applied to hadronic physics problems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of identifying when quantum machine-learning models offer advantages over classical ones, particularly in data-driven hadronic physics.

Method: The authors develop diagnostic tools, including a quantitative quantum qualifier, to guide model selection. They conduct classification and regression studies to analyze trends in model performance based on data properties.

Result: The study identifies systematic trends in relative performance influenced by factors like complexity, noise, and dimensionality. The quantum qualifier was successfully used to determine favorable conditions for quantum models.

Conclusion: The paper establishes a structured approach for effectively leveraging quantum machine learning in precision hadronic physics, demonstrating the potential of quantum models in specific kinematic regimes.

Abstract: As quantum machine-learning architectures mature, a central challenge is no longer their construction, but identifying the regimes in which they offer practical advantages over classical approaches. In this work, we introduce a framework for addressing this question in data-driven hadronic physics problems by developing diagnostic tools - centered on a quantitative quantum qualifier - that guide model selection between classical and quantum deep neural networks based on intrinsic properties of the data. Using controlled classification and regression studies, we show how relative model performance follows systematic trends in complexity, noise, and dimensionality, and how these trends can be distilled into a predictive criterion. We then demonstrate the utility of this approach through an application to Compton form factor extraction from deeply virtual Compton scattering, where the quantum qualifier identifies kinematic regimes favorable to quantum models. Together, these results establish a principled framework for deploying quantum machine-learning tools in precision hadronic physics.

</details>


### [742] [A Unified Variational Imputation Framework for Electric Vehicle Charging Data Using Retrieval-Augmented Language Model](https://arxiv.org/abs/2601.13476)
*Jinhao Li,Hao Wang*

Main category: cs.LG

TL;DR: The paper introduces PRAIM, a framework using language models and retrieval-augmented memory to improve missing data imputation in electric vehicle charging datasets.


<details>
  <summary>Details</summary>
Motivation: Real-world EV datasets often have incomplete or poor-quality data, and current imputation methods fail to address their multimodal and interconnected nature.

Method: The authors developed PRAIM, a probabilistic variational framework that utilizes pre-trained language models and retrieval-augmented memory to encode diverse data and address missing values.

Result: Experiments on four datasets show PRAIM outperforms existing methods in both accuracy and maintaining data distribution, improving forecasting tasks.

Conclusion: PRAIM provides an effective solution to data sparsity challenges in EV applications, using advanced machine learning techniques to enhance forecasting reliability.

Abstract: The reliability of data-driven applications in electric vehicle (EV) infrastructure, such as charging demand forecasting, hinges on the availability of complete, high-quality charging data. However, real-world EV datasets are often plagued by missing records, and existing imputation methods are ill-equipped for the complex, multimodal context of charging data, often relying on a restrictive one-model-per-station paradigm that ignores valuable inter-station correlations. To address these gaps, we develop a novel PRobabilistic variational imputation framework that leverages the power of large lAnguage models and retrIeval-augmented Memory (PRAIM). PRAIM employs a pre-trained language model to encode heterogeneous data, spanning time-series demand, calendar features, and geospatial context, into a unified, semantically rich representation. This is dynamically fortified by retrieval-augmented memory that retrieves relevant examples from the entire charging network, enabling a single, unified imputation model empowered by variational neural architecture to overcome data sparsity. Extensive experiments on four public datasets demonstrate that PRAIM significantly outperforms established baselines in both imputation accuracy and its ability to preserve the original data's statistical distribution, leading to substantial improvements in downstream forecasting performance.

</details>


### [743] [StoTAM: Stochastic Alternating Minimization for Tucker-Structured Tensor Sensing](https://arxiv.org/abs/2601.13522)
*Shuang Li*

Main category: cs.LG

TL;DR: The paper introduces a stochastic alternating minimization algorithm for low-Tucker-rank tensor recovery, enabling efficient mini-batch updates and better convergence behavior.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiencies in existing tensor recovery methods that rely heavily on expensive tensor projections or full-gradient computations.

Method: A stochastic alternating minimization approach is used, focusing on the core tensor and factor matrices under Tucker factorization to allow efficient mini-batch updates.

Result: Numerical experiments show favorable convergence efficiency in wall-clock time compared to baseline stochastic tensor recovery methods.

Conclusion: The proposed method improves computational efficiency for recovering low-Tucker-rank tensors by eliminating costly tensor operations while maintaining convergence benefits.

Abstract: Low-rank tensor sensing is a fundamental problem with broad applications in signal processing and machine learning. Among various tensor models, low-Tucker-rank tensors are particularly attractive for capturing multi-mode subspace structures in high-dimensional data. Existing recovery methods either operate on the full tensor variable with expensive tensor projections, or adopt factorized formulations that still rely on full-gradient computations, while most stochastic factorized approaches are restricted to tensor decomposition settings. In this work, we propose a stochastic alternating minimization algorithm that operates directly on the core tensor and factor matrices under a Tucker factorization. The proposed method avoids repeated tensor projections and enables efficient mini-batch updates on low-dimensional tensor factors. Numerical experiments on synthetic tensor sensing demonstrate that the proposed algorithm exhibits favorable convergence behavior in wall-clock time compared with representative stochastic tensor recovery baselines.

</details>


### [744] [MN-TSG:Continuous Time Series Generation with Irregular Observations](https://arxiv.org/abs/2601.13534)
*Xu Zhang,Junwei Deng,Chang Xu,Hao Li,Jiang Bian*

Main category: cs.LG

TL;DR: The paper introduces MN-TSG, a framework combining Mixture-of-Experts (MoE)-based Neural Controlled Differential Equations (NCDEs) with existing Time Series Generation (TSG) models for irregular and continuous time-series simulation tasks, achieving superior performance across datasets.


<details>
  <summary>Details</summary>
Motivation: Existing TSG methods assume regular sampling and fixed resolutions, which don't align with real-world, irregular, or sparse data scenarios, posing significant limitations in domains like healthcare where continuous and high-resolution TSG is critical.

Method: The MN-TSG framework uses a MoE-NCDE architecture that includes dynamically parameterized experts and decouples their optimization for better performance. It integrates existing TSG models to jointly model the distribution of the time series and the expert configuration.

Result: MN-TSG outperforms strong TSG baseline methods in experiments with ten public and synthetic datasets, excelling in irregular-to-regular and irregular-to-continuous generation tasks.

Conclusion: MN-TSG addresses irregular and sparse TSG challenges by leveraging MoE-based NCDEs and existing models, offering refined performance for real-world tasks like clinical monitoring.

Abstract: Time series generation (TSG) plays a critical role in a wide range of domains, such as healthcare. However, most existing methods assume regularly sampled observations and fixed output resolutions, which are often misaligned with real-world scenarios where data are irregularly sampled and sparsely observed. This mismatch is particularly problematic in applications such as clinical monitoring, where irregular measurements must support downstream tasks requiring continuous and high-resolution time series.
  Neural Controlled Differential Equations (NCDEs) have shown strong potential for modeling irregular time series, yet they still face challenges in capturing complex dynamic temporal patterns and supporting continuous TSG. To address these limitations, we propose MN-TSG, a novel framework that explores Mixture-of-Experts (MoE)-based NCDEs and integrates them with existing TSG models for irregular and continuous generation tasks.
  The core of MN-TSG lies in a MoE-NCDE architecture with dynamically parameterized expert functions and a decoupled design that facilitates more effective optimization of MoE dynamics. Furthermore, we leverage existing TSG models to learn the joint distribution over the mixture of experts and the generated time series. This enables the framework not only to generate new samples, but also to produce appropriate expert configurations tailored to each sample, thereby supporting refined continuous TSG.
  Extensive experiments on ten public and synthetic datasets demonstrate the effectiveness of MN-TSG, consistently outperforming strong TSG baselines on both irregular-to-regular and irregular-to-continuous generation tasks.

</details>


### [745] [Patterning: The Dual of Interpretability](https://arxiv.org/abs/2601.13548)
*George Wang,Daniel Murfet*

Main category: cs.LG

TL;DR: The paper explores the concept of mechanistic interpretability, introducing 'patterning' to guide neural networks' internal structures by adjusting training data.


<details>
  <summary>Details</summary>
Motivation: Understanding how neural networks generalize and providing tools to steer internal configurations toward specific goals.

Method: Leveraging susceptibilities, which measure responses to data distribution shifts, to manipulate the model's training data to achieve target internal structures.

Result: Patterning successfully influences the formation of structures in neural networks by re-weighting training data, demonstrated with tasks such as language modeling and parentheses balancing.

Conclusion: Mechanistic interpretability techniques can be inverted to not only read but also actively write targeted internal structures in neural networks.

Abstract: Mechanistic interpretability aims to understand how neural networks generalize beyond their training data by reverse-engineering their internal structures. We introduce patterning as the dual problem: given a desired form of generalization, determine what training data produces it. Our approach is based on susceptibilities, which measure how posterior expectation values of observables respond to infinitesimal shifts in the data distribution. Inverting this linear response relationship yields the data intervention that steers the model toward a target internal configuration. We demonstrate patterning in a small language model, showing that re-weighting training data along principal susceptibility directions can accelerate or delay the formation of structure, such as the induction circuit. In a synthetic parentheses balancing task where multiple algorithms achieve perfect training accuracy, we show that patterning can select which algorithm the model learns by targeting the local learning coefficient of each solution. These results establish that the same mathematical framework used to read internal structure can be inverted to write it.

</details>


### [746] [ButterflyMoE: Sub-Linear Ternary Experts via Structured Butterfly Orbits](https://arxiv.org/abs/2601.13563)
*Aryan Karmore*

Main category: cs.LG

TL;DR: The paper introduces ButterflyMoE, which drastically reduces memory requirements by representing multiple experts in a sparse model as geometric reorientations of a shared structure, yielding significant memory savings without impacting accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the memory scaling problem with current sparse models having independent experts, which require excessive memory, making them impractical for edge devices.

Method: The authors propose a novel method where expert weights are represented as geometric rotations of a shared quantized prototype, leading to significant memory efficiency and the ability to stabilize low-bit training.

Result: ButterflyMoE achieves a 150x memory reduction for 256 experts with minimal accuracy loss, enabling larger models to fit within the memory limitations of smaller devices.

Conclusion: Geometric parametrization of shared expert weights offers a breakthrough in overcoming the linear memory scaling barrier, enhancing model efficiency and enabling deployment on resource-constrained devices.

Abstract: Linear memory scaling stores $N$ independent expert weight matrices requiring $\mathcal{O}(N \cdot d^2)$ memory, which exceeds edge devices memory budget. Current compression methods like quantization, pruning and low-rank factorization reduce constant factors but leave the scaling bottleneck unresolved. We introduce ButterflyMoE, a method that treats experts not as independent weight matrices but as geometric reorientations of a unified shared quantized substrate. Diversity among experts arises from viewing different angles of shared capacity, not from redundant storage. By applying learned rotations to a shared ternary prototype, each expert yields $\mathcal{O}(d^2 + N \cdot d \log d)$ memory -- sub-linear in the number of experts. The key insight: training these rotations with quantization reduces activation outliers and stabilizes extreme low bit training, where static methods collapse. Across language modeling benchmarks, ButterflyMoE achieves 150 times memory reduction at 256 experts with negligible accuracy loss. This allows 64 experts to fit on 4GB devices compared to standard MoE's 8 experts, showing geometric parametrization breaks linear scaling.

</details>


### [747] [Multi-objective fluorescent molecule design with a data-physics dual-driven generative framework](https://arxiv.org/abs/2601.13564)
*Yanheng Li,Zhichen Pu,Lijiang Yang,Zehao Zhou,Yi Qin Gao*

Main category: cs.LG

TL;DR: LUMOS is a new framework combining data-driven and physics-based approaches for the efficient and targeted design of fluorescent molecules under multiple objectives and constraints.


<details>
  <summary>Details</summary>
Motivation: Designing fluorescent molecules tailored to specific needs is challenging due to vast chemical space, multi-objective requirements, and limitations of conventional screening or predictive methods.

Method: LUMOS integrates generators and predictors via shared latent space for efficient exploration, uses a combination of neural networks and TD-DFT for robust property prediction, and employs a diffusion model with evolutionary algorithms for multi-objective optimization.

Result: LUMOS demonstrated superior benchmarks in prediction accuracy, generalizability, and feasibility, effectively generating target-specific fluorophore molecules verified by TD-DFT and MD simulations.

Conclusion: LUMOS provides a highly efficient and reliable framework for inverse design of fluorescent molecules, combining advanced data and physics methodologies to address complex design challenges.

Abstract: Designing fluorescent small molecules with tailored optical and physicochemical properties requires navigating vast, underexplored chemical space while satisfying multiple objectives and constraints. Conventional generate-score-screen approaches become impractical under such realistic design specifications, owing to their low search efficiency, unreliable generalizability of machine-learning prediction, and the prohibitive cost of quantum chemical calculation. Here we present LUMOS, a data-and-physics driven framework for inverse design of fluorescent molecules. LUMOS couples generator and predictor within a shared latent representation, enabling direct specification-to-molecule design and efficient exploration. Moreover, LUMOS combines neural networks with a fast time-dependent density functional theory (TD-DFT) calculation workflow to build a suite of complementary predictors spanning different trade-offs in speed, accuracy, and generalizability, enabling reliable property prediction across diverse scenarios. Finally, LUMOS employs a property-guided diffusion model integrated with multi-objective evolutionary algorithms, enabling de novo design and molecular optimization under multiple objectives and constraints. Across comprehensive benchmarks, LUMOS consistently outperforms baseline models in terms of accuracy, generalizability and physical plausibility for fluorescence property prediction, and demonstrates superior performance in multi-objective scaffold- and fragment-level molecular optimization. Further validation using TD-DFT and molecular dynamics (MD) simulations demonstrates that LUMOS can generate valid fluorophores that meet various target specifications. Overall, these results establish LUMOS as a data-physics dual-driven framework for general fluorophore inverse design.

</details>


### [748] [Self-Improvement as Coherence Optimization: A Theoretical Account](https://arxiv.org/abs/2601.13566)
*Tianyi Qiu,Ahmed Hani Ismail,Zhonghao He,Shi Feng*

Main category: cs.LG

TL;DR: This paper explores feedback-free self-improvement in language models, showing that methods like debate and bootstrap work due to 'coherence optimization,' which is theoretically equivalent to description-length regularization.


<details>
  <summary>Details</summary>
Motivation: To understand why feedback-free semantic self-improvement methods in language models work, despite the absence of external supervision, and to predict their performance under different conditions.

Method: The paper theorizes that coherence optimization, the process of finding compressible and predictable context-to-behavior mappings, underpins these methods and proves its equivalence to description-length regularization.

Result: The paper demonstrates that coherence optimization is the optimal regularization for semi-supervised learning when derived from a pretrained model and explains the conditions under which feedback-free methods succeed or fail.

Conclusion: Feedback-free self-improvement is theoretically grounded in coherence optimization, offering explanatory and predictive power for these methods' effectiveness.

Abstract: Can language models improve their accuracy without external supervision? Methods such as debate, bootstrap, and internal coherence maximization achieve this surprising feat, even matching golden finetuning performance. Yet why they work remains theoretically unclear. We show that they are all special cases of coherence optimization: finding a context-to-behavior mapping that's most compressible and jointly predictable. We prove that coherence optimization is equivalent to description-length regularization, and that among all such regularization schemes, it is optimal for semi-supervised learning when the regularizer is derived from a pretrained model. Our theory, supported by preliminary experiments, explains why feedback-free self-improvement works and predicts when it should succeed or fail.

</details>


### [749] [DRGW: Learning Disentangled Representations for Robust Graph Watermarking](https://arxiv.org/abs/2601.13569)
*Jiasen Li,Yanwei Liu,Zhuoyi Shang,Xiaoyan Gu,Weiping Wang*

Main category: cs.LG

TL;DR: The paper introduces a novel graph watermarking framework named DRGW, emphasizing disentangled representation learning to enhance watermark robustness and transparency for graph data.


<details>
  <summary>Details</summary>
Motivation: Existing graph watermarking methods compromise transparency and robustness due to coupled information representation and uncontrollable discretization during transformations.

Method: DRGW leverages adversarial disentangled representation learning for structural invariance, graph-aware invertible neural networks for watermark embedding, and structure-aware editing for robustness.

Result: Experiments on various benchmark datasets show that DRGW achieves superior effectiveness in graph watermarking.

Conclusion: DRGW addresses limitations of traditional graph watermarking methods, offering a transparent, robust, and effective watermarking solution for graph-structured data.

Abstract: Graph-structured data is foundational to numerous web applications, and watermarking is crucial for protecting their intellectual property and ensuring data provenance. Existing watermarking methods primarily operate on graph structures or entangled graph representations, which compromise the transparency and robustness of watermarks due to the information coupling in representing graphs and uncontrollable discretization in transforming continuous numerical representations into graph structures. This motivates us to propose DRGW, the first graph watermarking framework that addresses these issues through disentangled representation learning. Specifically, we design an adversarially trained encoder that learns an invariant structural representation against diverse perturbations and derives a statistically independent watermark carrier, ensuring both robustness and transparency of watermarks. Meanwhile, we devise a graph-aware invertible neural network to provide a lossless channel for watermark embedding and extraction, guaranteeing high detectability and transparency of watermarks. Additionally, we develop a structure-aware editor that resolves the issue of latent modifications into discrete graph edits, ensuring robustness against structural perturbations. Experiments on diverse benchmark datasets demonstrate the superior effectiveness of DRGW.

</details>


### [750] [GeoDynamics: A Geometric State-Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds](https://arxiv.org/abs/2601.13570)
*Tingting Dan,Jiaqi Ding,Guorong Wu*

Main category: cs.LG

TL;DR: GeoDynamics is a geometric state-space neural network that models latent brain-state trajectories on SPD manifolds, applicable to neuroscience and human action recognition.


<details>
  <summary>Details</summary>
Motivation: Understand brain dynamics using a principled and holistic approach to model functional connectivity matrices over time.

Method: GeoDynamics employs manifold-aware recurrent frameworks to track SPD matrices' trajectories reflecting brain-state dynamics.

Result: GeoDynamics identifies neural state changes related to tasks and early markers of neurological diseases while performing well on human action recognition benchmarks.

Conclusion: GeoDynamics showcases the importance of capturing geometry-respecting neural dynamics for neuroscience and diverse applications.

Abstract: State-space models (SSMs) have become a cornerstone for unraveling brain dynamics, revealing how latent neural states evolve over time and give rise to observed signals. By combining the flexibility of deep learning with the principled dynamical structure of SSMs, recent studies have achieved powerful fits to functional neuroimaging data. However, most existing approaches still view the brain as a set of loosely connected regions or impose oversimplified network priors, falling short of a truly holistic and self-organized dynamical system perspective. Brain functional connectivity (FC) at each time point naturally forms a symmetric positive definite (SPD) matrix, which resides on a curved Riemannian manifold rather than in Euclidean space. Capturing the trajectories of these SPD matrices is key to understanding how coordinated networks support cognition and behavior. To this end, we introduce GeoDynamics, a geometric state-space neural network that tracks latent brain-state trajectories directly on the high-dimensional SPD manifold. GeoDynamics embeds each connectivity matrix into a manifold-aware recurrent framework, learning smooth and geometry-respecting transitions that reveal task-driven state changes and early markers of Alzheimer's disease, Parkinson's disease, and autism. Beyond neuroscience, we validate GeoDynamics on human action recognition benchmarks (UTKinect, Florence, HDM05), demonstrating its scalability and robustness in modeling complex spatiotemporal dynamics across diverse domains.

</details>


### [751] [Behavior Knowledge Merge in Reinforced Agentic Models](https://arxiv.org/abs/2601.13572)
*Xiangchi Yuan,Dachuan Shi,Chunhui Zhang,Zheyuan Liu,Shenglong Yao,Soroush Vosoughi,Wenke Lee*

Main category: cs.LG

TL;DR: The paper introduces Reinforced Agent Merging (RAM), a method for efficiently merging RL-trained agent models with task-specific behaviors.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenge of merging reinforcement learning (RL)-trained agents from different specialized tasks into a single generalist model while preserving their task-specific capabilities.

Method: They propose a distribution-aware merging framework (RAM) that separates shared and unique parameter updates, averaging shared parameters and scaling unique updates to prevent dilution of task-specific knowledge.

Result: Experiments demonstrate that RAM surpasses traditional merging methods and can achieve performance superior to specialized agents in their specific domains.

Conclusion: RAM improves the merging of RL-trained agents by effectively managing task-specific parameters, enabling better generalist model performance and potential synergy among agents.

Abstract: Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.

</details>


### [752] [FG-OrIU: Towards Better Forgetting via Feature-Gradient Orthogonality for Incremental Unlearning](https://arxiv.org/abs/2601.13578)
*Qian Feng,JiaHang Tu,Mintong Kang,Hanbin Zhao,Chao Zhang,Hui Qian*

Main category: cs.LG

TL;DR: This paper introduces FG-OrIU, a framework for secure and irreversible incremental unlearning by enforcing orthogonal constraints at feature and gradient levels.


<details>
  <summary>Details</summary>
Motivation: To address security risks and imbalance caused by superficial forgetting in incremental unlearning, which leaves residual recoverable information.

Method: The authors propose FG-OrIU, using singular value decomposition to separate forgetting and retaining features into subspaces, and applying orthogonal projections to features and gradients to ensure irreversible forgetting and preservation of remaining knowledge.

Result: The method prevents reintroduction of forgotten knowledge, maintains retention balance, and performs effectively across sequential unlearning tasks.

Conclusion: FG-OrIU achieves deep, secure forgetting by unifying feature and gradient level orthogonality, offering stability and effectiveness in incremental unlearning scenarios.

Abstract: Incremental unlearning (IU) is critical for pre-trained models to comply with sequential data deletion requests, yet existing methods primarily suppress parameters or confuse knowledge without explicit constraints on both feature and gradient level, resulting in \textit{superficial forgetting} where residual information remains recoverable. This incomplete forgetting risks security breaches and disrupts retention balance, especially in IU scenarios. We propose FG-OrIU (\textbf{F}eature-\textbf{G}radient \textbf{Or}thogonality for \textbf{I}ncremental \textbf{U}nlearning), the first framework unifying orthogonal constraints on both features and gradients level to achieve deep forgetting, where the forgetting effect is irreversible. FG-OrIU decomposes feature spaces via Singular Value Decomposition (SVD), separating forgetting and remaining class features into distinct subspaces. It then enforces dual constraints: feature orthogonal projection on both forgetting and remaining classes, while gradient orthogonal projection prevents the reintroduction of forgotten knowledge and disruption to remaining classes during updates. Additionally, dynamic subspace adaptation merges newly forgetting subspaces and contracts remaining subspaces, ensuring a stable balance between removal and retention across sequential unlearning tasks. Extensive experiments demonstrate the effectiveness of our method.

</details>


### [753] [Neural Organ Transplantation (NOT): Checkpoint-Based Modular Adaptation for Transformer Models](https://arxiv.org/abs/2601.13580)
*Ahmad Al-Zuraiqi*

Main category: cs.LG

TL;DR: Neural Organ Transplantation (NOT) is a modular framework for domain adaptation in transformer models. It allows reusable, transferable checkpoints through independent training of extracted layer subsets and achieves significantly better performance than existing methods while being faster to train.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency and inflexibility of conventional fine-tuning approaches, enabling modular and privacy-preserving domain adaptation in transformer models without requiring access to the original training data.

Method: NOT extracts trained layer subsets from pre-trained transformer models, independently trains them on domain-specific data, and stores them as checkpoint files. These 'donor organs' can then be transplanted into compatible models without the need for original training data.

Result: Experiments on various decoder-only transformer models (GPT-2, TinyLlama, GPT-OSS) showed that NOT substantially outperformed standard adaptation methods like LoRA, improving perplexity by an order of magnitude and training faster, with optimal results from earlier insertion positions.

Conclusion: NOT demonstrates that modular, privacy-preserving domain adaptation is feasible in decoder-only transformers, allowing scalable expertise sharing. However, its effectiveness is currently limited to decoder-only architectures, with diminished results in encoder-based models.

Abstract: We introduce Neural Organ Transplantation (NOT), a modular adaptation framework that enables trained transformer layers to function as reusable transferable checkpoints for domain adaptation. Unlike conventional fine-tuning approaches that tightly couple trained parameters to specific model instances and training data, NOT extracts contiguous layer subsets ("donor organs") from pre-trained models, trains them independently on domain-specific data, and saves them as standalone checkpoint files that can be transplanted into compatible recipient models without access to the original training data. Through experiments on three decoder-only transformer architectures spanning 124M to 20B parameters (GPT-2, TinyLlama, and GPT-OSS), we demonstrate that donor transplantation substantially outperforms existing adaptation methods, achieving an order-of-magnitude improvement in perplexity over LoRA while training significantly faster. The method exhibits position dependence, with early insertion positions yielding optimal results. Cross-domain transfer at billion-parameter scale reveals unexpected regularization benefits. These findings demonstrate that transformer middle layers can support efficient modular transfer for decoder-only architectures, enabling privacy-preserving expertise sharing through checkpoint distribution. We note that this approach is currently limited to decoder-only models; preliminary experiments on encoder-based architectures show reduced effectiveness.

</details>


### [754] [Machine learning based radiative parameterization scheme and its performance in operational reforecast experiments](https://arxiv.org/abs/2601.13592)
*Hao Jing,Sa Xiao,Haoyu Li,Huadong Xiao,Wei Xue*

Main category: cs.LG

TL;DR: The paper integrates a machine learning method into a global numerical prediction model to efficiently simulate the radiation process, achieving similar accuracy to traditional methods at a much faster computational speed.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of computational inefficiency in radiation simulation, which dominates the time in numerical weather forecasting, using a hybrid ML-based solution.

Method: A residual convolutional neural network was trained offline using a comprehensive dataset generated from simulations and coupled online using a LibTorch-based system to approximate the Rapid Radiative Transfer Model for Global Circulation Models.

Result: The machine learning enhanced model achieved comparable accuracy to traditional physical modeling while improving computation speed by approximately 8 times during two-month long operational experiments.

Conclusion: The hybrid model shows significant potential for operational use, ensuring coupling compatibility, integration stability, and computational efficiency without sacrificing forecast accuracy.

Abstract: Radiation is typically the most time-consuming physical process in numerical models. One solution is to use machine learning methods to simulate the radiation process to improve computational efficiency. From an operational standpoint, this study investigates critical limitations inherent to hybrid forecasting frameworks that embed deep neural networks into numerical prediction models, with a specific focus on two fundamental bottlenecks: coupling compatibility and long-term integration stability. A residual convolutional neural network is employed to approximate the Rapid Radiative Transfer Model for General Circulation Models (RRTMG) within the global operational system of China Meteorological Administration. We adopted an offline training and online coupling approach. First, a comprehensive dataset is generated through model simulations, encompassing all atmospheric columns both with and without cloud cover. To ensure the stability of the hybrid model, the dataset is enhanced via experience replay, and additional output constraints based on physical significance are imposed. Meanwhile, a LibTorch-based coupling method is utilized, which is more suitable for real-time operational computations. The hybrid model is capable of performing ten-day integrated forecasts as required. A two-month operational reforecast experiment demonstrates that the machine learning emulator achieves accuracy comparable to that of the traditional physical scheme, while accelerating the computation speed by approximately eightfold.

</details>


### [755] [Diffusion In Diffusion: Breaking the Autoregressive Bottleneck in Block Diffusion Models](https://arxiv.org/abs/2601.13599)
*Linrui Ma,Yufei Cui,Kai Han,Yunhe Wang*

Main category: cs.LG

TL;DR: The paper introduces 'Diffusion in Diffusion,' a framework designed to tackle the problems of irreversibility and short-term planning in block diffusion language models, achieving significant improvements in performance while reducing resource usage.


<details>
  <summary>Details</summary>
Motivation: Existing block diffusion models, while combining autoregressive and diffusion benefits, suffer from irreversibility and lack global planning, making them suboptimal in efficiency and performance.

Method: The study proposes a draft-then-refine framework using block diffusion for rapid initial drafts and bidirectional diffusion for refinement. Techniques such as snapshot confidence remasking and mix-scale training enhance global planning and token modification precision.

Result: The method reduced generative perplexity from 25.7 to 21.9 on the OpenWebText dataset, using only 26% of the fine-tuning budget required by baseline models. It sets a new benchmark for discrete diffusion models.

Conclusion: This approach overcomes core limitations of block diffusion models, closing the performance gap with autoregressive models, and demonstrates superior efficiency and effectiveness for discrete diffusion tasks.

Abstract: Block diffusion language models, operating as semi-autoregressive paradigms, combine the strengths of both autoregressive and diffusion paradigms. However, their strict unidirectional block dependencies introduce irreversibility and sacrifice the global planning capabilities for which diffusion models are renowned. In order to address these issues, we propose Diffusion in Diffusion, a draft-then-refine framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilise snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model's global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using just 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models.

</details>


### [756] [Fisher-Informed Parameterwise Aggregation for Federated Learning with Heterogeneous Data](https://arxiv.org/abs/2601.13608)
*Zhipeng Chang,Ting He,Wenrui Hao*

Main category: cs.LG

TL;DR: This paper introduces FIPA, a federated learning method that uses second-order Fisher Information Matrix (FIM) weights for parameter-specific aggregation, improving performance on non-IID data.


<details>
  <summary>Details</summary>
Motivation: Traditional federated learning methods like FedAvg assign uniform scalar weights to model updates from distributed clients. This approach fails under non-IID data, causing misalignment in updates and degrading the global model. A more adaptive, parameter-specific aggregation method is needed.

Method: The paper proposes Fisher-Informed Parameterwise Aggregation (FIPA), which adopts the Fisher Information Matrix (FIM) for parameter-specific weighting in aggregation. Low-rank FIM approximations ensure the method remains computationally and communication-efficient.

Result: FIPA consistently outperforms standard averaging-based aggregation methods across tasks like nonlinear regression, PDE learning, and image classification. It also integrates effectively with client-side optimization methods, further boosting accuracy.

Conclusion: FIPA demonstrates significant improvements for federated learning in heterogeneous data settings by utilizing parameter-specific aggregation with FIM weights. It balances better accuracy with low computational and communication overhead.

Abstract: Federated learning aggregates model updates from distributed clients, but standard first order methods such as FedAvg apply the same scalar weight to all parameters from each client. Under non-IID data, these uniformly weighted updates can be strongly misaligned across clients, causing client drift and degrading the global model. Here we propose Fisher-Informed Parameterwise Aggregation (FIPA), a second-order aggregation method that replaces client-level scalar weights with parameter-specific Fisher Information Matrix (FIM) weights, enabling true parameter-level scaling that captures how each client's data uniquely influences different parameters. With low-rank approximation, FIPA remains communication- and computation-efficient. Across nonlinear function regression, PDE learning, and image classification, FIPA consistently improves over averaging-based aggregation, and can be effectively combined with state-of-the-art client-side optimization algorithms to further improve image classification accuracy. These results highlight the benefits of FIPA for federated learning under heterogeneous data distributions.

</details>


### [757] [Quadratic Upper Bound for Boosting Robustness](https://arxiv.org/abs/2601.13645)
*Euijin You,Hyang-Won Lee*

Main category: cs.LG

TL;DR: This paper improves fast adversarial training (FAT) robustness by introducing a quadratic upper bound (QUB) on the adversarial training loss function, showing significant robustness improvements through experiments.


<details>
  <summary>Details</summary>
Motivation: Fast adversarial training often lacks effective exploration of adversarial space, leading to reduced robustness against adversarial attacks.

Method: The method involves deriving a quadratic upper bound (QUB) on the adversarial training loss function and integrating this bound into existing FAT methods.

Result: Experiments show that applying QUB loss to existing FAT methods significantly improves robustness and provides a smoother loss landscape.

Conclusion: Integrating QUB loss into FAT methods enhances robustness effectively, likely due to the smoothened loss landscape of the trained models.

Abstract: Fast adversarial training (FAT) aims to enhance the robustness of models against adversarial attacks with reduced training time, however, FAT often suffers from compromised robustness due to insufficient exploration of adversarial space. In this paper, we develop a loss function to mitigate the problem of degraded robustness under FAT. Specifically, we derive a quadratic upper bound (QUB) on the adversarial training (AT) loss function and propose to utilize the bound with existing FAT methods. Our experimental results show that applying QUB loss to the existing methods yields significant improvement of robustness. Furthermore, using various metrics, we demonstrate that this improvement is likely to result from the smoothened loss landscape of the resulting model.

</details>


### [758] [TimeART: Towards Agentic Time Series Reasoning via Tool-Augmentation](https://arxiv.org/abs/2601.13653)
*Xingjian Wu,Junkai Lu,Zhengyu Li,Xiangfei Qiu,Jilin Hu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.LG

TL;DR: The paper introduces TimeART, a framework combining analytical tools and Large Language Models (LLMs) for automating Time Series Question Answering (TSQA).


<details>
  <summary>Details</summary>
Motivation: Current time series analysis relies heavily on human experts, which is labor-intensive and lacks automation. Automating such processes can provide significant benefits in areas like disaster prediction and financial risk control.

Method: The authors propose TimeART, using LLMs for reasoning and out-of-the-box tools for analysis. They also create a large expert trajectory dataset (TimeToolBench) and implement a four-stage training process to optimize the model.

Result: An 8B TSRM model trained on TimeToolBench achieves state-of-the-art results on TSQA tasks.

Conclusion: TimeART demonstrates a novel and effective approach for automating time series reasoning with significant potential in real-world applications.

Abstract: Time series data widely exist in real-world cyber-physical systems. Though analyzing and interpreting them contributes to significant values, e.g, disaster prediction and financial risk control, current workflows mainly rely on human data scientists, which requires significant labor costs and lacks automation. To tackle this, we introduce TimeART, a framework fusing the analytical capability of strong out-of-the-box tools and the reasoning capability of Large Language Models (LLMs), which serves as a fully agentic data scientist for Time Series Question Answering (TSQA). To teach the LLM-based Time Series Reasoning Models (TSRMs) strategic tool-use, we also collect a 100k expert trajectory corpus called TimeToolBench. To enhance TSRMs' generalization capability, we then devise a four-stage training strategy, which boosts TSRMs through learning from their own early experiences and self-reflections. Experimentally, we train an 8B TSRM on TimeToolBench and equip it with the TimeART framework, and it achieves consistent state-of-the-art performance on multiple TSQA tasks, which pioneers a novel approach towards agentic time series reasoning.

</details>


### [759] [Autoregressive deep learning for real-time simulation of soft tissue dynamics during virtual neurosurgery](https://arxiv.org/abs/2601.13676)
*Fabian Greifeneder,Wolfgang Fenz,Benedikt Alkin,Johannes Brandstetter,Michael Giretzlehner,Philipp Moser*

Main category: cs.LG

TL;DR: The paper introduces a deep learning-based model to accurately and efficiently simulate brain deformation for neurosurgical simulators, leveraging mesh-based data and stochastic teacher forcing for stability.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical solvers struggle to meet real-time performance for accurate brain deformation simulations in neurosurgical training.

Method: A deep learning surrogate model trained on large finite element simulation datasets using Universal Physics Transformers and stochastic teacher forcing during training.

Result: The model demonstrated high accuracy, reducing prediction error from 6.7 mm to 3.5 mm, and achieved sub-10 ms runtimes on consumer hardware.

Conclusion: The framework enables fast, accurate and stable biomechanical simulations, enhancing the potential for realistic neurosurgical training environments.

Abstract: Accurate simulation of brain deformation is a key component for developing realistic, interactive neurosurgical simulators, as complex nonlinear deformations must be captured to ensure realistic tool-tissue interactions. However, traditional numerical solvers often fall short in meeting real-time performance requirements. To overcome this, we introduce a deep learning-based surrogate model that efficiently simulates transient brain deformation caused by continuous interactions between surgical instruments and the virtual brain geometry. Building on Universal Physics Transformers, our approach operates directly on large-scale mesh data and is trained on an extensive dataset generated from nonlinear finite element simulations, covering a broad spectrum of temporal instrument-tissue interaction scenarios. To reduce the accumulation of errors in autoregressive inference, we propose a stochastic teacher forcing strategy applied during model training. Specifically, training consists of short stochastic rollouts in which the proportion of ground truth inputs is gradually decreased in favor of model-generated predictions. Our results show that the proposed surrogate model achieves accurate and efficient predictions across a range of transient brain deformation scenarios, scaling to meshes with up to 150,000 nodes. The introduced stochastic teacher forcing technique substantially improves long-term rollout stability, reducing the maximum prediction error from 6.7 mm to 3.5 mm. We further integrate the trained surrogate model into an interactive neurosurgical simulation environment, achieving runtimes below 10 ms per simulation step on consumer-grade inference hardware. Our proposed deep learning framework enables rapid, smooth and accurate biomechanical simulations of dynamic brain tissue deformation, laying the foundation for realistic surgical training environments.

</details>


### [760] [Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction](https://arxiv.org/abs/2601.13710)
*Sayeed Shafayet Chowdhury,Snehasis Mukhopadhyay,Shiaofen Fang,Vijay R. Ramakrishnan*

Main category: cs.LG

TL;DR: The paper evaluates AI models for predicting surgical outcomes in chronic rhinosinusitis, comparing supervised ML models to generative AI, and proposing an ML-first approach with GenAI as an explainer.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve decision support for chronic rhinosinusitis surgery by determining which patients might not benefit from the procedure using pre-operative clinical data, addressing the limitations of current AI applications in prospective decision-making.

Method: Supervised machine learning (ML) models (e.g., MLP, logistic regression, tree ensembles) are compared to generative AI models (e.g., ChatGPT, Claude). The input is structured pre-operative clinical data, and models are tasked to predict surgical success outcomes with binary recommendations and confidence levels.

Result: The best ML model (MLP) achieves 85% accuracy with better calibration and decision-curve net benefit than generative AI models, which underperform in discrimination and calibration. GenAI provides justifications that align with clinical heuristics and ML model insights.

Conclusion: The study suggests an ML-first approach for triaging surgical candidacy, using GenAI as a complementary tool to provide transparent explanations and aid in shared decision-making.

Abstract: Artificial intelligence has reshaped medical imaging, yet the use of AI on clinical data for prospective decision support remains limited. We study pre-operative prediction of clinically meaningful improvement in chronic rhinosinusitis (CRS), defining success as a more than 8.9-point reduction in SNOT-22 at 6 months (MCID). In a prospectively collected cohort where all patients underwent surgery, we ask whether models using only pre-operative clinical data could have identified those who would have poor outcomes, i.e. those who should have avoided surgery. We benchmark supervised ML (logistic regression, tree ensembles, and an in-house MLP) against generative AI (ChatGPT, Claude, Gemini, Perplexity), giving each the same structured inputs and constraining outputs to binary recommendations with confidence. Our best ML model (MLP) achieves 85 % accuracy with superior calibration and decision-curve net benefit. GenAI models underperform on discrimination and calibration across zero-shot setting. Notably, GenAI justifications align with clinician heuristics and the MLP's feature importance, repeatedly highlighting baseline SNOT-22, CT/endoscopy severity, polyp phenotype, and physchology/pain comorbidities. We provide a reproducible tabular-to-GenAI evaluation protocol and subgroup analyses. Findings support an ML-first, GenAI- augmented workflow: deploy calibrated ML for primary triage of surgical candidacy, with GenAI as an explainer to enhance transparency and shared decision-making.

</details>


### [761] [EEG-Titans: Long-Horizon Seizure Forecasting via Dual-Branch Attention and Neural Memory](https://arxiv.org/abs/2601.13748)
*Tien-Dat Pham,Xuan-The Tran*

Main category: cs.LG

TL;DR: This study proposes a neural network, EEG-Titans, which improves epileptic seizure prediction from EEG by effectively capturing long-term and short-term dynamics, achieving high sensitivity and reducing false alarms.


<details>
  <summary>Details</summary>
Motivation: Addressing the persistent challenge of accurately predicting epileptic seizures due to the need for capturing subtle, transient, and long-term EEG patterns.

Method: Developed EEG-Titans, a dual-branch architecture combining sliding-window attention for short-term anomalies and a memory-based recurrent pathway for long-term trends.

Result: EEG-Titans showed 99.46% sensitivity on the CHB-MIT EEG dataset, reduced false alarms, and demonstrated robust performance in noisy environments.

Conclusion: Memory-augmented long-context modeling combined with a hierarchical context strategy can enhance seizure prediction reliability, making it clinically practical.

Abstract: Accurate epileptic seizure prediction from electroencephalography (EEG) remains challenging because pre-ictal dynamics may span long time horizons while clinically relevant signatures can be subtle and transient. Many deep learning models face a persistent trade-off between capturing local spatiotemporal patterns and maintaining informative long-range context when operating on ultralong sequences. We propose EEG-Titans, a dualbranch architecture that incorporates a modern neural memory mechanism for long-context modeling. The model combines sliding-window attention to capture short-term anomalies with a recurrent memory pathway that summarizes slower, progressive trends over time. On the CHB-MIT scalp EEG dataset, evaluated under a chronological holdout protocol, EEG-Titans achieves 99.46% average segment-level sensitivity across 18 subjects. We further analyze safety-first operating points on artifact-prone recordings and show that a hierarchical context strategy extending the receptive field for high-noise subjects can markedly reduce false alarms (down to 0.00 FPR/h in an extreme outlier) without sacrificing sensitivity. These results indicate that memory-augmented long-context modeling can provide robust seizure forecasting under clinically constrained evaluation

</details>


### [762] [vLinear: A Powerful Linear Model for Multivariate Time Series Forecasting](https://arxiv.org/abs/2601.13768)
*Wenzhen Yue,Ruohao Guo,Ji Shi,Zihan Hao,Shiyu Hu,Xianghua Ying*

Main category: cs.LG

TL;DR: vLinear is a novel multivariate time series forecasting model combining vecTrans module and WFMLoss objective for efficient and accurate forecasting.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art forecasters often rely on computationally expensive self-attention mechanisms to capture multivariate correlations.

Method: Introduced the vecTrans module (efficient modeling with learnable vector) and WFMLoss objective (final-series-oriented formulation with path- and horizon-weighted strategies).

Result: vLinear delivers up to 5x inference speedups with consistent performance improvements and achieves state-of-the-art results across various benchmarks.

Conclusion: vLinear is an efficient yet accurate forecasting model, while WFMLoss enhances forecasting accuracy as a plug-and-play objective applicable to other models.

Abstract: In this paper, we present \textbf{vLinear}, an effective yet efficient \textbf{linear}-based multivariate time series forecaster featuring two components: the \textbf{v}ecTrans module and the WFMLoss objective. Many state-of-the-art forecasters rely on self-attention or its variants to capture multivariate correlations, typically incurring $\mathcal{O}(N^2)$ computational complexity with respect to the number of variates $N$. To address this, we propose vecTrans, a lightweight module that utilizes a learnable vector to model multivariate correlations, reducing the complexity to $\mathcal{O}(N)$. Notably, vecTrans can be seamlessly integrated into Transformer-based forecasters, delivering up to 5$\times$ inference speedups and consistent performance gains. Furthermore, we introduce WFMLoss (Weighted Flow Matching Loss) as the objective. In contrast to typical \textbf{velocity-oriented} flow matching objectives, we demonstrate that a \textbf{final-series-oriented} formulation yields significantly superior forecasting accuracy. WFMLoss also incorporates path- and horizon-weighted strategies to focus learning on more reliable paths and horizons. Empirically, vLinear achieves state-of-the-art performance across 22 benchmarks and 124 forecasting settings. Moreover, WFMLoss serves as an effective plug-and-play objective, consistently improving existing forecasters. The code is available at https://anonymous.4open.science/r/vLinear.

</details>


### [763] [Principled Latent Diffusion for Graphs via Laplacian Autoencoders](https://arxiv.org/abs/2601.13780)
*Antoine Siraudin,Christopher Morris*

Main category: cs.LG

TL;DR: LG-Flow introduces a graph generation framework that overcomes inefficiencies in graph diffusion by operating in a compressed latent space for nearly lossless reconstructions and significant efficiency gains.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and complexity of traditional graph diffusion models, especially in handling sparse graphs and ensuring accurate adjacency matrix reconstruction.

Method: LG-Flow uses a permutation-equivariant autoencoder to compress graphs into latent embeddings with provable reconstruction capacity. It then trains a Diffusion Transformer using flow matching in this compressed space.

Result: The model achieves competitive performance with existing state-of-the-art graph diffusion methods, while being up to 1000x faster.

Conclusion: LG-Flow offers an efficient and scalable solution for graph generation, maintaining high accuracy even in sparse data contexts and enabling larger, more expressive models.

Abstract: Graph diffusion models achieve state-of-the-art performance in graph generation but suffer from quadratic complexity in the number of nodes -- and much of their capacity is wasted modeling the absence of edges in sparse graphs. Inspired by latent diffusion in other modalities, a natural idea is to compress graphs into a low-dimensional latent space and perform diffusion there. However, unlike images or text, graph generation requires nearly lossless reconstruction, as even a single error in decoding an adjacency matrix can render the entire sample invalid. This challenge has remained largely unaddressed. We propose LG-Flow, a latent graph diffusion framework that directly overcomes these obstacles. A permutation-equivariant autoencoder maps each node into a fixed-dimensional embedding from which the full adjacency is provably recoverable, enabling near-lossless reconstruction for both undirected graphs and DAGs. The dimensionality of this latent representation scales linearly with the number of nodes, eliminating the quadratic bottleneck and making it feasible to train larger and more expressive models. In this latent space, we train a Diffusion Transformer with flow matching, enabling efficient and expressive graph generation. Our approach achieves competitive results against state-of-the-art graph diffusion models, while achieving up to $1000\times$ speed-up.

</details>


### [764] [PAtt: A Pattern Attention Network for ETA Prediction Using Historical Speed Profiles](https://arxiv.org/abs/2601.13793)
*ByeoungDo Kim,JunYeop Na,Kyungwook Tak,JunTae Kim,DongHyeon Kim,Duckky Kim*

Main category: cs.LG

TL;DR: This paper introduces an ETA prediction model utilizing attention mechanisms to improve accuracy by leveraging spatio-temporal road and traffic patterns.


<details>
  <summary>Details</summary>
Motivation: The increasing prevalence of autonomous driving and intelligent transportation systems demands accurate ETA predictions. Existing methods are limited due to simplistic combinations of data, high computational costs, and poor handling of spatio-temporal causality.

Method: The paper presents a model with attention mechanisms to extract temporal features at each spatio-temporal point along a route, integrating road characteristics, real-time traffic, and historical patterns effectively.

Result: The proposed approach validated on real-world datasets outperformed traditional baselines, showcasing improved ETA estimation.

Conclusion: The model is efficient, accurate, lightweight, and scalable, effectively addressing challenges in ETA prediction.

Abstract: In this paper, we propose an ETA model (Estimated Time of Arrival) that leverages an attention mechanism over historical road speed patterns. As autonomous driving and intelligent transportation systems become increasingly prevalent, the need for accurate and reliable ETA estimation has grown, playing a vital role in navigation, mobility planning, and traffic management. However, predicting ETA remains a challenging task due to the dynamic and complex nature of traffic flow. Traditional methods often combine real-time and historical traffic data in simplistic ways, or rely on complex rule-based computations. While recent deep learning models have shown potential, they often require high computational costs and do not effectively capture the spatio-temporal patterns crucial for ETA prediction. ETA prediction inherently involves spatio-temporal causality, and our proposed model addresses this by leveraging attention mechanisms to extract and utilize temporal features accumulated at each spatio-temporal point along a route. This architecture enables efficient and accurate ETA estimation while keeping the model lightweight and scalable. We validate our approach using real-world driving datasets and demonstrate that our approach outperforms existing baselines by effectively integrating road characteristics, real-time traffic conditions, and historical speed patterns in a task-aware manner.

</details>


### [765] [ELSA: Efficient LLM-Centric Split Aggregation for Privacy-Aware Hierarchical Federated Learning over Resource-Constrained Edge Networks](https://arxiv.org/abs/2601.13824)
*Xiaohong Yang,Tong Xie,Minghui Liwang,Chikai Shang,Yang Lu,Zhenzhen Jiao,Liqun Fu,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: The paper proposes ELSA, a framework integrating split learning and hierarchical federated learning to fine-tune large language models at the network edge under constraints.


<details>
  <summary>Details</summary>
Motivation: To solve challenges of LLM fine-tuning at edges, such as resource limitations, data heterogeneity, and privacy risks.

Method: Mixes split learning and federated learning with client clustering, model splitting into three parts, and communication-efficient techniques to reduce costs and privacy risks.

Result: ELSA outperforms state-of-the-art techniques in adaptability, convergence, and robustness for edge-side NLP tasks.

Conclusion: ELSA offers a scalable and privacy-aware approach to fine-tune LLMs efficiently in resource-constrained edge networks.

Abstract: Training large language models (LLMs) at the network edge faces fundamental challenges arising from device resource constraints, severe data heterogeneity, and heightened privacy risks. To address these, we propose ELSA (Efficient LLM-centric Split Aggregation), a novel framework that systematically integrates split learning (SL) and hierarchical federated learning (HFL) for distributed LLM fine-tuning over resource-constrained edge networks. ELSA introduces three key innovations. First, it employs a task-agnostic, behavior-aware client clustering mechanism that constructs semantic fingerprints using public probe inputs and symmetric KL divergence, further enhanced by prediction-consistency-based trust scoring and latency-aware edge assignment to jointly address data heterogeneity, client unreliability, and communication constraints. Second, it splits the LLM into three parts across clients and edge servers, with the cloud used only for adapter aggregation, enabling an effective balance between on-device computation cost and global convergence stability. Third, it incorporates a lightweight communication scheme based on computational sketches combined with semantic subspace orthogonal perturbation (SS-OP) to reduce communication overhead while mitigating privacy leakage during model exchanges. Experiments across diverse NLP tasks demonstrate that ELSA consistently outperforms state-of-the-art methods in terms of adaptability, convergence behavior, and robustness, establishing a scalable and privacy-aware solution for edge-side LLM fine-tuning under resource constraints.

</details>


### [766] [Optimal L2 Regularization in High-dimensional Continual Linear Regression](https://arxiv.org/abs/2601.13844)
*Gilad Karpel,Edward Moroshko,Ran Levinstein,Ron Meir,Daniel Soudry,Itay Evron*

Main category: cs.LG

TL;DR: The paper studies overparameterized continual linear regression with L2 regularization, providing theoretical insights and experimental validations on generalization loss and optimal regularization scaling.


<details>
  <summary>Details</summary>
Motivation: To address generalization challenges in continual learning tasks, especially with overparameterized models under regularization.

Method: Derived theoretical expressions for generalization loss and optimal regularization scaling in continual learning, supported by experiments on linear regression and neural networks.

Result: Isotropic regularization mitigates label noise effectively, and the optimal regularization strength scales as T/ln T, demonstrated theoretically and empirically.

Conclusion: The findings improve understanding of regularization's role in continual learning and provide practical scaling laws and design guidance for such systems.

Abstract: We study generalization in an overparameterized continual linear regression setting, where a model is trained with L2 (isotropic) regularization across a sequence of tasks. We derive a closed-form expression for the expected generalization loss in the high-dimensional regime that holds for arbitrary linear teachers. We demonstrate that isotropic regularization mitigates label noise under both single-teacher and multiple i.i.d. teacher settings, whereas prior work accommodating multiple teachers either did not employ regularization or used memory-demanding methods. Furthermore, we prove that the optimal fixed regularization strength scales nearly linearly with the number of tasks $T$, specifically as $T/\ln T$. To our knowledge, this is the first such result in theoretical continual learning. Finally, we validate our theoretical findings through experiments on linear regression and neural networks, illustrating how this scaling law affects generalization and offering a practical recipe for the design of continual learning systems.

</details>


### [767] [Multi-Objective Hierarchical Optimization with Large Language Models](https://arxiv.org/abs/2601.13892)
*Andrej Schwanke,Lyubomir Ivanov,David Salinas,Frank Hutter,Arber Zela*

Main category: cs.LG

TL;DR: The paper proposes using Large Language Models (LLMs) for multi-objective optimization through a hierarchical search strategy, achieving competitive results compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: LLMs are not yet widely applied to multi-objective optimization despite their reasoning power; there is a gap compared to conventional numerical strategies.

Method: LLMs are used as surrogate models and candidate samplers with an adaptive hierarchical search strategy, focusing their generative process on high-potential sub-spaces.

Result: The proposed method demonstrates convergence to the true Pareto set in theory and performs competitively with standard optimization techniques empirically.

Conclusion: The paper bridges the gap in using LLMs for multi-objective optimization, showing that they can achieve comparable results to conventional methods with tailored approaches.

Abstract: Despite their widespread adoption in various domains, especially due to their powerful reasoning capabilities, Large Language Models (LLMs) are not the off-the-shelf choice to drive multi-objective optimization yet. Conventional strategies rank high in benchmarks due to their intrinsic capabilities to handle numerical inputs and careful modelling choices that balance exploration and Pareto-front exploitation, as well as handle multiple (conflicting) objectives. In this paper, we close this gap by leveraging LLMs as surrogate models and candidate samplers inside a structured hierarchical search strategy. By adaptively partitioning the input space into disjoint hyperrectangular regions and ranking them with a composite score function, we restrict the generative process of the LLM to specific, high-potential sub-spaces, hence making the problem easier to solve as the LLM doesn't have to reason about the global structure of the problem, but only locally instead. We show that under standard regularity assumptions, our algorithm generates candidate solutions that converge to the true Pareto set in Hausdorff distance. Empirically, it consistently outperforms the global LLM-based multi-objective optimizer and is on par with standard evolutionary and Bayesian optimization algorithm on synthetic and real-world benchmarks.

</details>


### [768] [TractRLFusion: A GPT-Based Multi-Critic Policy Fusion Framework for Fiber Tractography](https://arxiv.org/abs/2601.13897)
*Ankita Joshi,Ashutosh Sharma,Anoushkrit Goel,Ranjeet Ranjan Jha,Chirag Ahuja,Arnav Bhavsar,Aditya Nigam*

Main category: cs.LG

TL;DR: TractRLFusion, a GPT-based framework, combines RL policies for white matter tractography, achieving improved accuracy and anatomical reliability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome challenges in reconstructing white matter tracts while mitigating spurious connections during tractography.

Method: The proposed method uses policy fusion via a two-stage training data selection and multi-critic fine-tuning within a GPT-based reinforcement learning framework.

Result: Experiments on various datasets (HCP, ISMRM, TractoInferno) show that TractRLFusion outperforms existing RL policies and other tractography methods in accuracy and reliability.

Conclusion: TractRLFusion significantly improves the reconstruction and reliability of white matter fiber pathways, surpassing previous state-of-the-art methods.

Abstract: Tractography plays a pivotal role in the non-invasive reconstruction of white matter fiber pathways, providing vital information on brain connectivity and supporting precise neurosurgical planning. Although traditional methods relied mainly on classical deterministic and probabilistic approaches, recent progress has benefited from supervised deep learning (DL) and deep reinforcement learning (DRL) to improve tract reconstruction. A persistent challenge in tractography is accurately reconstructing white matter tracts while minimizing spurious connections. To address this, we propose TractRLFusion, a novel GPT-based policy fusion framework that integrates multiple RL policies through a data-driven fusion strategy. Our method employs a two-stage training data selection process for effective policy fusion, followed by a multi-critic fine-tuning phase to enhance robustness and generalization. Experiments on HCP, ISMRM, and TractoInferno datasets demonstrate that TractRLFusion outperforms individual RL policies as well as state-of-the-art classical and DRL methods in accuracy and anatomical reliability.

</details>


### [769] [RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning](https://arxiv.org/abs/2601.13964)
*Cheol-Hui Lee,Hwa-Yeon Lee,Dong-Joo Kim*

Main category: cs.LG

TL;DR: This paper introduces RL-BioAug, a label-efficient reinforcement learning framework to autonomously determine optimal augmentation policies for EEG contrastive learning, showing significant performance improvements over random strategies.


<details>
  <summary>Details</summary>
Motivation: To address the non-stationarity of EEG signals, where static or random augmentation methods fail to preserve intrinsic information, and to enable effective representation learning with minimal labeled data.

Method: Leverages a reinforcement learning agent using only 10% labeled data to guide augmentation policy, enabling robust self-supervised learning for EEG tasks.

Result: RL-BioAug demonstrated significant performance boosts, with 9.69% and 8.80% higher Macro-F1 scores on Sleep-EDFX and CHB-MIT datasets respectively, compared to random strategies.

Conclusion: The framework effectively replaces heuristic-based augmentation with an autonomous paradigm, optimizing data augmentation policies in EEG tasks, and achieves state-of-the-art results.

Abstract: The quality of data augmentation serves as a critical determinant for the performance of contrastive learning in EEG tasks. Although this paradigm is promising for utilizing unlabeled data, static or random augmentation strategies often fail to preserve intrinsic information due to the non-stationarity of EEG signals where statistical properties change over time. To address this, we propose RL-BioAug, a framework that leverages a label-efficient reinforcement learning (RL) agent to autonomously determine optimal augmentation policies. While utilizing only a minimal fraction (10\%) of labeled data to guide the agent's policy, our method enables the encoder to learn robust representations in a strictly self-supervised manner. Experimental results demonstrate that RL-BioAug significantly outperforms the random selection strategy, achieving substantial improvements of 9.69\% and 8.80\% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably, this agent mainly chose optimal strategies for each task -- for example, Time Masking with a 62\% probability for sleep stage classification and Crop \& Resize with a 77\% probability for seizure detection. Our framework suggests its potential to replace conventional heuristic-based augmentations and establish a new autonomous paradigm for data augmentation. The source code is available at \href{https://github.com/dlcjfgmlnasa/RL-BioAug}{https://github.com/dlcjfgmlnasa/RL-BioAug}.

</details>


### [770] [A universal linearized subspace refinement framework for neural networks](https://arxiv.org/abs/2601.13989)
*Wenbo Cao,Weiwei Zhang*

Main category: cs.LG

TL;DR: This paper introduces Linearized Subspace Refinement (LSR), a method to refine predictions of gradient-trained neural networks by solving a linearized residual problem, improving accuracy and addressing numerical ill-conditioning.


<details>
  <summary>Details</summary>
Motivation: Neural networks trained by gradient-based methods often do not achieve their full potential accuracy, limited by factors like loss-induced numerical ill-conditioning rather than nonconvexity or model capacity.

Method: LSR exploits the Jacobian-induced linear residual model to compute a subspace-optimal solution, refining predictions with a one-shot process, and proposes Iterative LSR to improve fitting steps for composite loss problems.

Result: LSR achieves significant accuracy improvements across various tasks, often reducing errors by orders of magnitude, and enhances convergence for operator-constrained problems.

Conclusion: LSR provides a versatile, numerically efficient framework to improve neural network accuracy without altering architectures or training processes, addressing fundamental bottlenecks in supervised and operator learning.

Abstract: Neural networks are predominantly trained using gradient-based methods, yet in many applications their final predictions remain far from the accuracy attainable within the model's expressive capacity. We introduce Linearized Subspace Refinement (LSR), a general and architecture-agnostic framework that exploits the Jacobian-induced linear residual model at a fixed trained network state. By solving a reduced direct least-squares problem within this subspace, LSR computes a subspace-optimal solution of the linearized residual model, yielding a refined linear predictor with substantially improved accuracy over standard gradient-trained solutions, without modifying network architectures, loss formulations, or training procedures. Across supervised function approximation, data-driven operator learning, and physics-informed operator fine-tuning, we show that gradient-based training often fails to access this attainable accuracy, even when local linearization yields a convex problem. This observation indicates that loss-induced numerical ill-conditioning, rather than nonconvexity or model expressivity, can constitute a dominant practical bottleneck. In contrast, one-shot LSR systematically exposes accuracy levels not fully exploited by gradient-based training, frequently achieving order-of-magnitude error reductions. For operator-constrained problems with composite loss structures, we further introduce Iterative LSR, which alternates one-shot LSR with supervised nonlinear alignment, transforming ill-conditioned residual minimization into numerically benign fitting steps and yielding accelerated convergence and improved accuracy. By bridging nonlinear neural representations with reduced-order linear solvers at fixed linearization points, LSR provides a numerically grounded and broadly applicable refinement framework for supervised learning, operator learning, and scientific computing.

</details>


### [771] [Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment](https://arxiv.org/abs/2601.14022)
*Rodrigo Pereira David,Luciano Araujo Dourado Filho,Daniel Marques da Silva,João Alfredo Cal-Braz*

Main category: cs.LG

TL;DR: This paper introduces a machine learning framework to fairly compare CO2 emissions between internal combustion engine vehicles (ICEVs) and electric vehicles (EVs) under identical driving conditions.


<details>
  <summary>Details</summary>
Motivation: To address the need for consistent and transparent evaluation of CO2 emissions between ICEVs and EVs for decarbonizing road transport.

Method: Recurrent neural networks are used to model vehicle performance based on driving variables and emissions data. These models enable counterfactual analyses to compare emissions from ICEVs and EVs under the same conditions.

Result: The framework provides a consistent and scalable method for fair, data-driven comparative evaluations of vehicle powertrain emissions.

Conclusion: This approach facilitates credible assessments of vehicle carbon performance, promoting transparent comparisons of road transport technologies.

Abstract: Decarbonizing road transport requires consistent and transparent methods for comparing CO2 emissions across vehicle technologies. This paper proposes a machine learning-based framework for like-for-like operational assessment of internal combustion engine vehicles (ICEVs) and electric vehicles (EVs) under identical, real-world driving conditions. The approach isolates technology-specific effects by holding the observed speed profile and environmental context fixed, enabling direct comparison of powertrain performance. Recurrent neural network models are trained independently for each domain to learn the mapping from contextual driving variables (speed, acceleration, temperature) to internal actuation variables (torque, throttle) and instantaneous CO2-equivalent emission rates. This structure allows the construction of counterfactual scenarios that answer: What emissions would an EV have generated if it had followed the same driving profile as an ICEV? By aligning both vehicle types on a unified instantaneous emissions metric, the framework enables fair and reproducible evaluation of powertrain technologies. It offers a scalable foundation for credible, data-driven assessments of vehicle carbon performance under real-world operating conditions.

</details>


### [772] [PAC-Private Responses with Adversarial Composition](https://arxiv.org/abs/2601.14033)
*Xiaochen Zhu,Mayuri Sridhar,Srinivas Devadas*

Main category: cs.LG

TL;DR: This paper addresses privacy in machine learning APIs by enforcing privacy guarantees directly on model outputs using PAC privacy, introducing an algorithm for adversarial composition, and achieving high model utility with provably bound privacy risks.


<details>
  <summary>Details</summary>
Motivation: Although traditional weight-privatization methods ensure privacy, they can introduce unnecessary noise and reduce utility, especially in API-deployed ML models. Outputs tend to be more stable across inputs, motivating direct privacy enforcement on model responses.

Method: The authors use PAC privacy, controlling mutual information to provide instance-based guarantees, and propose a new algorithm with adaptive noise calibration for adversarial composition of queries.

Result: The approach achieves high accuracy across tasks: 87.79% on CIFAR-10 with extremely small per-query privacy budgets. It supports millions of queries while keeping membership inference attack risks low. They demonstrate private response labeling to build privacy-preserving models, achieving competitive results with distillation.

Conclusion: The proposed method effectively enforces privacy on model responses, allowing high-utility machine learning applications to balance privacy and stability, with broad implications for secure and scalable API deployment.

Abstract: Modern machine learning models are increasingly deployed behind APIs. This renders standard weight-privatization methods (e.g. DP-SGD) unnecessarily noisy at the cost of utility. While model weights may vary significantly across training datasets, model responses to specific inputs are much lower dimensional and more stable. This motivates enforcing privacy guarantees directly on model outputs.
  We approach this under PAC privacy, which provides instance-based privacy guarantees for arbitrary black-box functions by controlling mutual information (MI). Importantly, PAC privacy explicitly rewards output stability with reduced noise levels. However, a central challenge remains: response privacy requires composing a large number of adaptively chosen, potentially adversarial queries issued by untrusted users, where existing composition results on PAC privacy are inadequate. We introduce a new algorithm that achieves adversarial composition via adaptive noise calibration and prove that mutual information guarantees accumulate linearly under adaptive and adversarial querying.
  Experiments across tabular, vision, and NLP tasks show that our method achieves high utility at extremely small per-query privacy budgets. On CIFAR-10, we achieve 87.79% accuracy with a per-step MI budget of $2^{-32}$. This enables serving one million queries while provably bounding membership inference attack (MIA) success rates to 51.08% -- the same guarantee of $(0.04, 10^{-5})$-DP. Furthermore, we show that private responses can be used to label public data to distill a publishable privacy-preserving model; using an ImageNet subset as a public dataset, our model distilled from 210,000 responses achieves 91.86% accuracy on CIFAR-10 with MIA success upper-bounded by 50.49%, which is comparable to $(0.02,10^{-5})$-DP.

</details>


### [773] [LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems](https://arxiv.org/abs/2601.14053)
*Badri N. Patro,Vijay S. Agneeswaran*

Main category: cs.LG

TL;DR: This survey highlights the evolution and innovations in large language models from 2019-2025, documenting challenges and solutions for efficiency, cost, and energy, while addressing paradigm shifts in training and model specialization.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to categorize and understand the advancements, crises, and paradigm shifts in the development of large language models over time.

Method: The paper uses a circular taxonomy framework called LLMOrbit to analyze over 50 models across eight dimensions related to architecture, training, and efficiency.

Result: The study identifies three major challenges: data scarcity, exponential cost, and energy consumption. It also outlines six paradigms for overcoming these issues including test-time compute, efficient training, edge computing, and post-training innovations.

Conclusion: The analysis forecasts the limits of traditional scaling approaches while emphasizing efficiency-driven innovations and democratization in AI, suggesting a shift towards specialized models and agentic systems.

Abstract: The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at <$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.

</details>


### [774] [Optimizing Energy and Data Collection in UAV-aided IoT Networks using Attention-based Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2601.14092)
*Babacar Toure,Dimitrios Tsilimantos,Omid Esrafilian,Marios Kountouris*

Main category: cs.LG

TL;DR: The paper proposes an attention-based Multi-Objective Reinforcement Learning (MORL) approach to improve UAV path planning for data harvesting, achieving better generalization, sample efficiency, and performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the study is to overcome existing limitations in UAV path planning: insufficient adaptability to dynamic environments, reliance on limited training data, and neglect of the multi-objective nature of the task (balancing data collection and energy consumption).

Method: The authors present an attention-based MORL architecture that integrates trade-offs between data collection and energy consumption into the planning process. The method allows the model to adapt to diverse preferences and parameters without requiring retraining.

Result: Simulations demonstrate superior performance, including better adaptability, generalization, sample efficiency, and compactness, when compared with other reinforcement learning (RL) methods.

Conclusion: The proposed architecture efficiently bridges gaps in UAV path planning by enabling multi-objective handling, adaptability to dynamics, and improved deployment practicality in real-world scenarios.

Abstract: Due to their adaptability and mobility, Unmanned Aerial Vehicles (UAVs) are becoming increasingly essential for wireless network services, particularly for data harvesting tasks. In this context, Artificial Intelligence (AI)-based approaches have gained significant attention for addressing UAV path planning tasks in large and complex environments, bridging the gap with real-world deployments. However, many existing algorithms suffer from limited training data, which hampers their performance in highly dynamic environments. Moreover, they often overlook the inherently multi-objective nature of the task, treating it in an overly simplistic manner. To address these limitations, we propose an attention-based Multi-Objective Reinforcement Learning (MORL) architecture that explicitly handles the trade-off between data collection and energy consumption in urban environments, even without prior knowledge of wireless channel conditions. Our method develops a single model capable of adapting to varying trade-off preferences and dynamic scenario parameters without the need for fine-tuning or retraining. Extensive simulations show that our approach achieves substantial improvements in performance, model compactness, sample efficiency, and most importantly, generalization to previously unseen scenarios, outperforming existing RL solutions.

</details>


### [775] [Causal feature selection framework for stable soft sensor modeling based on time-delayed cross mapping](https://arxiv.org/abs/2601.14099)
*Shi-Shun Chen,Xiao-Yang Li,Enrico Zio*

Main category: cs.LG

TL;DR: This paper proposes an advanced causal feature selection framework using time-delayed cross mapping to improve accuracy and stability in soft sensor models for industrial processes.


<details>
  <summary>Details</summary>
Motivation: Existing causal feature selection methods are insufficient for industrial applications due to their ignorance of time delays in causality and interdependencies between process variables.

Method: The paper introduces time-delayed convergent cross mapping (TDCCM) and time-delayed partial cross mapping (TDPCM) to analyze interdependent variables and varying causal strengths over time. It also proposes an automatic objective feature selection strategy based on model performance.

Result: TDCCM demonstrated the highest average performance in case studies, and TDPCM enhanced both stability and performance in worst-case scenarios.

Conclusion: The framework significantly improves soft sensor modeling accuracy and stability, addressing critical shortcomings of traditional methods.

Abstract: Soft sensor modeling plays a crucial role in process monitoring. Causal feature selection can enhance the performance of soft sensor models in industrial applications. However, existing methods ignore two critical characteristics of industrial processes. Firstly, causal relationships between variables always involve time delays, whereas most causal feature selection methods investigate causal relationships in the same time dimension. Secondly, variables in industrial processes are often interdependent, which contradicts the decorrelation assumption of traditional causal inference methods. Consequently, soft sensor models based on existing causal feature selection approaches often lack sufficient accuracy and stability. To overcome these challenges, this paper proposes a causal feature selection framework based on time-delayed cross mapping. Time-delayed cross mapping employs state space reconstruction to effectively handle interdependent variables in causality analysis, and considers varying causal strength across time delay. Time-delayed convergent cross mapping (TDCCM) is introduced for total causal inference, and time-delayed partial cross mapping (TDPCM) is developed for direct causal inference. Then, in order to achieve automatic feature selection, an objective feature selection strategy is presented. The causal threshold is automatically determined based on the model performance on the validation set, and the causal features are then selected. Two real-world case studies show that TDCCM achieves the highest average performance, while TDPCM improves soft sensor stability and performance in the worst scenario. The code is publicly available at https://github.com/dirge1/TDPCM.

</details>


### [776] [Riemannian Liquid Spatio-Temporal Graph Network](https://arxiv.org/abs/2601.14115)
*Liangsi Lu,Jingchao Wang,Zhaorong Dai,Hanqian Liu,Yang Shi*

Main category: cs.LG

TL;DR: This paper introduces RLSTG, a graph neural network framework combining liquid dynamics with Riemannian geometry for better representation of non-Euclidean graph structures.


<details>
  <summary>Details</summary>
Motivation: LTCs, great for irregular-dynamic modeling, struggle with non-Euclidean graph structures, causing degraded representation.

Method: The authors proposed RLSTG, extending liquid dynamics and ODE frameworks to Riemannian manifolds for handling curved graph structures.

Result: RLSTG surpasses other models in capturing complex graph dynamics and structures, providing stronger expressiveness and stability.

Conclusion: The RLSTG framework demonstrates its value through theoretical guarantees and superior performance on challenging benchmarks.

Abstract: Liquid Time-Constant networks (LTCs), a type of continuous-time graph neural network, excel at modeling irregularly-sampled dynamics but are fundamentally confined to Euclidean space. This limitation introduces significant geometric distortion when representing real-world graphs with inherent non-Euclidean structures (e.g., hierarchies and cycles), degrading representation quality. To overcome this limitation, we introduce the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a framework that unifies continuous-time liquid dynamics with the geometric inductive biases of Riemannian manifolds. RLSTG models graph evolution through an Ordinary Differential Equation (ODE) formulated directly on a curved manifold, enabling it to faithfully capture the intrinsic geometry of both structurally static and dynamic spatio-temporal graphs. Moreover, we provide rigorous theoretical guarantees for RLSTG, extending stability theorems of LTCs to the Riemannian domain and quantifying its expressive power via state trajectory analysis. Extensive experiments on real-world benchmarks demonstrate that, by combining advanced temporal dynamics with a Riemannian spatial representation, RLSTG achieves superior performance on graphs with complex structures. Project Page: https://rlstg.github.io

</details>


### [777] [A model of errors in transformers](https://arxiv.org/abs/2601.14175)
*Suvrat Raju,Praneeth Netrapalli*

Main category: cs.LG

TL;DR: This paper investigates error rates in LLMs for tasks requiring deterministic and repetitive outputs. It presents a two-parameter model to explain these errors, validated through empirical testing.


<details>
  <summary>Details</summary>
Motivation: Understanding why LLMs make errors in deterministic, repetitive tasks, to develop better interpretability and improve prompts.

Method: The paper introduces a two-parameter model describing error rates of LLMs, inspired by effective field theory, and tests it on errors observed in tasks performed by three different LLMs.

Result: Empirical tests on various tasks found the model accurately predicts error rates, though some deviations were noted.

Conclusion: The analysis offers a quantitative understanding of LLM errors, suggesting alternatives to explanations based solely on reasoning or compositional collapse, while also guiding prompt construction to reduce errors.

Abstract: We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ``effective field theory'' perspective: the LLM's many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ``collapse of reasoning'', or an inability to express ``compositional'' functions. Finally, we show how to construct prompts to reduce the error rate.

</details>


### [778] [Differentiated Pickup Point Offering for Emission Reduction in Last-Mile Delivery](https://arxiv.org/abs/2601.14196)
*Albina Galiullina,Wouter van Heeswijk,Tom van Woensel*

Main category: cs.LG

TL;DR: The study introduces the Differentiated Pickup Point Offering (DPO) policy to cut carbon emissions from delivery and customer travel by recommending optimized pickup points. Results show up to a 9% reduction in emissions compared to home delivery.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the environmental impact of both delivery trucks and customer travel by optimizing the choice of pickup points, offering a sustainable alternative to home delivery.

Method: A reinforcement learning-based approach is employed to design DPO policies, dynamically recommending pickup points by factoring in customer locations and delivery decisions to optimize route efficiency.

Result: DPO policies achieve up to a 9% reduction in emissions compared to home-only delivery, and a 2% reduction compared to unrestricted or nearest pickup point policies, effectively benefiting dense urban environments.

Conclusion: Differentiated pickup point recommendations significantly reduce emissions and are most effective in urban contexts with multiple close-proximity pickup points, especially when accounting for customer preferences dynamically.

Abstract: Pickup points are widely recognized as a sustainable alternative to home delivery, as consolidating orders at pickup locations can shorten delivery routes and improve first-attempt success rates. However, these benefits may be negated when customers drive to pick up their orders. This study proposes a Differentiated Pickup Point Offering (DPO) policy that aims to jointly reduce emissions from delivery truck routes and customer travel. Under DPO, each arriving customer is offered a single recommended pickup point, rather than an unrestricted choice among all locations, while retaining the option of home delivery. We study this problem in a dynamic and stochastic setting, where the pickup point offered to each customer depends on previously realized customer locations and delivery choices. To design effective DPO policies, we adopt a reinforcement learning-based approach that accounts for spatial relationships between customers and pickup points and their implications for future route consolidation. Computational experiments show that differentiated pickup point offerings can substantially reduce total carbon emissions. The proposed policies reduce total emissions by up to 9% relative to home-only delivery and by 2% on average compared with alternative policies, including unrestricted pickup point choice and nearest pickup point assignment. Differentiated offerings are particularly effective in dense urban settings with many pickup points and short inter-location distances. Moreover, explicitly accounting for the dynamic nature of customer arrivals and choices is especially important when customers are less inclined to choose pickup point delivery over home delivery.

</details>


### [779] [InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning](https://arxiv.org/abs/2601.14209)
*Matthew Y. R. Yang,Hao Bai,Ian Wu,Gene Yang,Amrith Setlur,Aviral Kumar*

Main category: cs.LG

TL;DR: This paper addresses the problem of credit assignment in reinforcement learning applied to large language models by introducing Intervention Training (InT).


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning capabilities of large language models by solving the credit assignment problem in standard reinforcement learning, which penalizes reasoning traces unfairly.

Method: The authors propose Intervention Training (InT), where a model identifies errors in its reasoning traces and performs single-step interventions, followed by supervised fine-tuning and reinforcement learning.

Result: Using Intervention Training and fine-tuning, the model achieves nearly 14% improvement in accuracy on the IMO-AnswerBench dataset and surpasses larger open-source models in performance.

Conclusion: Intervention Training significantly improves the initialization for RL, enhancing reasoning accuracy and addressing flaws in credit assignment within reasoning traces.

Abstract: Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.

</details>


### [780] [Attention-Based Offline Reinforcement Learning and Clustering for Interpretable Sepsis Treatment](https://arxiv.org/abs/2601.14228)
*Punit Kumar,Vaibhav Saran,Divyesh Patel,Nitin Kulkarni,Alina Vereshchaka*

Main category: cs.LG

TL;DR: The paper presents an interpretable AI-based decision support framework for treating sepsis in ICUs, combining clustering, data augmentation, offline reinforcement learning, and a rationale generation module.


<details>
  <summary>Details</summary>
Motivation: To improve sepsis outcomes in ICUs by addressing timeliness, treatment accuracy, and the need for interpretable decision-making tools.

Method: The framework includes clustering for risk stratification, synthetic data augmentation via VAEs and diffusion models, an offline RL agent with conservative recommendations, and a rationale generation module using multi-modal LLMs.

Result: The system was evaluated using the MIMIC-III and eICU datasets, achieving high treatment accuracy and offering interpretable, robust treatment policies.

Conclusion: The proposed decision support system shows promise in enhancing sepsis treatment by combining precision, safety, and interpretability for clinical use.

Abstract: Sepsis remains one of the leading causes of mortality in intensive care units, where timely and accurate treatment decisions can significantly impact patient outcomes. In this work, we propose an interpretable decision support framework. Our system integrates four core components: (1) a clustering-based stratification module that categorizes patients into low, intermediate, and high-risk groups upon ICU admission, using clustering with statistical validation; (2) a synthetic data augmentation pipeline leveraging variational autoencoders (VAE) and diffusion models to enrich underrepresented trajectories such as fluid or vasopressor administration; (3) an offline reinforcement learning (RL) agent trained using Advantage Weighted Regression (AWR) with a lightweight attention encoder and supported by an ensemble models for conservative, safety-aware treatment recommendations; and (4) a rationale generation module powered by a multi-modal large language model (LLM), which produces natural-language justifications grounded in clinical context and retrieved expert knowledge. Evaluated on the MIMIC-III and eICU datasets, our approach achieves high treatment accuracy while providing clinicians with interpretable and robust policy recommendations.

</details>


### [781] [KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning](https://arxiv.org/abs/2601.14232)
*Egor Cherepanov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: This paper introduces KAGE-Env and KAGE-Bench, tools to analyze visual generalization in pixel-based reinforcement learning agents under controlled visual changes.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for pixel-based reinforcement learning entangle multiple sources of shift, making systematic analysis of visual generalization challenging.

Method: The authors propose KAGE-Env, a platformer environment that factorizes visual axes independently while fixing the underlying control dynamics, and KAGE-Bench, a benchmark that isolates these shifts using six axis-defined suites.

Result: Experiments using a PPO-CNN baseline reveal axis-dependent failures, with background and photometric changes having the most significant negative impact.

Conclusion: Visual generalization failure can depend heavily on specific visual factors, and the proposed environment-implementation allows fast and reproducible evaluation of such failures.

Abstract: Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.

</details>


### [782] [Spatiotemporal Wildfire Prediction and Reinforcement Learning for Helitack Suppression](https://arxiv.org/abs/2601.14238)
*Shaurya Mathur,Shreyas Bellary Manjunath,Nitin Kulkarni,Alina Vereshchaka*

Main category: cs.LG

TL;DR: Wildfires are increasingly destructive, and traditional response methods lack proactivity. This paper presents FireCastRL, an AI framework combining deep learning and reinforcement learning to forecast and combat wildfires proactively.


<details>
  <summary>Details</summary>
Motivation: The frequency and severity of wildfires are on the rise, causing extensive ecological and economic damage. Current reactive wildfire management systems are insufficient to address the growing challenges.

Method: FireCastRL integrates a deep spatiotemporal wildfire ignition prediction model with reinforcement learning techniques for real-time suppression strategies, utilizing helitack units in a physics-informed 3D simulation for decision-making.

Result: This framework provides a comprehensive wildfire threat assessment to enhance resource planning and response. Additionally, the release of a novel, large-scale dataset consisting of 9.5 million environmental samples aids further wildfire prediction research.

Conclusion: The study successfully demonstrates how the combination of deep learning and reinforcement learning can improve wildfire forecasting and suppression, offering a proactive solution to minimize wildfire impact.

Abstract: Wildfires are growing in frequency and intensity, devastating ecosystems and communities while causing billions of dollars in suppression costs and economic damage annually in the U.S. Traditional wildfire management is mostly reactive, addressing fires only after they are detected. We introduce \textit{FireCastRL}, a proactive artificial intelligence (AI) framework that combines wildfire forecasting with intelligent suppression strategies. Our framework first uses a deep spatiotemporal model to predict wildfire ignition. For high-risk predictions, we deploy a pre-trained reinforcement learning (RL) agent to execute real-time suppression tactics with helitack units inside a physics-informed 3D simulation. The framework generates a threat assessment report to help emergency responders optimize resource allocation and planning. In addition, we are publicly releasing a large-scale, spatiotemporal dataset containing $\mathbf{9.5}$ million samples of environmental variables for wildfire prediction. Our work demonstrates how deep learning and RL can be combined to support both forecasting and tactical wildfire response. More details can be found at https://sites.google.com/view/firecastrl.

</details>


### [783] [Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow](https://arxiv.org/abs/2601.14243)
*Haocheng Xi,Charlie Ruan,Peiyuan Liao,Yujun Lin,Han Cai,Yilong Zhao,Shuo Yang,Kurt Keutzer,Song Han,Ligeng Zhu*

Main category: cs.LG

TL;DR: This paper introduces Jet-RL, a unified FP8 precision framework for reinforcement learning that addresses instability and inefficiencies in training while enhancing computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning (RL) pipelines for large language models are resource-intensive, especially during the rollout phase, which accounts for a significant portion of training time. Traditional attempts to mitigate this using FP8-precision rollout and BF16 training lead to instability and accuracy issues.

Method: The authors propose Jet-RL, a framework that employs unified FP8 precision flows for both training and rollout phases. This minimizes numerical discrepancies, eliminates inefficient calibration steps, and maintains training stability.

Result: Jet-RL achieves up to 33% speedup in the rollout phase and up to 41% speedup in the training phase. End-to-end, it provides a 16% training speedup over BF16 training, delivering stable convergence with minimal accuracy loss.

Conclusion: Jet-RL is an effective solution for enhancing reinforcement learning's computational efficiency by addressing numerical mismatches between training and rollout, while also maintaining stable optimization and accuracy.

Abstract: Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [784] [Speaking to Silicon: Neural Communication with Bitcoin Mining ASICs](https://arxiv.org/abs/2601.12032)
*Francisco Angulo de Lafuente,Vladimir Veselov,Richard Goodman*

Main category: cs.NE

TL;DR: The paper proposes treating Bitcoin mining ASICs as active computational units capable of bidirectional information exchange with AI systems, validated using innovative computation frameworks.


<details>
  <summary>Details</summary>
Motivation: To explore the underutilized computational potential of obsolete Bitcoin mining ASICs and establish them as active platforms for thermodynamic computation and AI interaction.

Method: The study integrates thermodynamic reservoir computing, hierarchical number theory, algorithmic analysis, network optimization, and formal mathematics for validation of the proposed paradigm.

Result: The research achieves a NARMA-10 NRMSE of 0.8661, 92.19% theoretical energy reduction via the Thermodynamic Probability Filter, a +25% effective hashrate with Virtual Block Manager, and evidence of hardware universality across ASIC families.

Conclusion: This work presents a novel working model where ASICs are viewed as interactive computational entities, offering breakthroughs in energy efficiency, computational utility, and theoretical foundation for AI-ASIC synergy.

Abstract: This definitive research memoria presents a comprehensive, mathematically verified paradigm for neural communication with Bitcoin mining Application-Specific Integrated Circuits (ASICs), integrating five complementary frameworks: thermodynamic reservoir computing, hierarchical number system theory, algorithmic analysis, network latency optimization, and machine-checked mathematical formalization. We establish that obsolete cryptocurrency mining hardware exhibits emergent computational properties enabling bidirectional information exchange between AI systems and silicon substrates. The research program demonstrates: (1) reservoir computing with NARMA-10 Normalized Root Mean Square Error (NRMSE) of 0.8661; (2) the Thermodynamic Probability Filter (TPF) achieving 92.19% theoretical energy reduction; (3) the Virtual Block Manager achieving +25% effective hashrate; and (4) hardware universality across multiple ASIC families including Antminer S9, Lucky Miner LV06, and Goldshell LB-Box. A significant contribution is the machine-checked mathematical formalization using Lean 4 and Mathlib, providing unambiguous definitions, machine-verified theorems, and reviewer-proof claims. Key theorems proven include: independence implies zero leakage, predictor beats baseline implies non-independence (the logical core of TPF), energy savings theoretical maximum, and Physical Unclonable Function (PUF) distinguishability witnesses. Vladimir Veselov's hierarchical number system theory explains why early-round information contains predictive power. This work establishes a new paradigm: treating ASICs not as passive computational substrates but as active conversational partners whose thermodynamic state encodes exploitable computational information.

</details>


### [785] [Statistical Firefly Algorithm for Truss Topology Optimization](https://arxiv.org/abs/2601.12265)
*Nghi Huu Duong,Duy Vo,Pruettha Nanakorn*

Main category: cs.NE

TL;DR: This paper introduces a Statistical Firefly Algorithm (SFA) for improving truss topology optimization by incorporating hypothesis testing to refine firefly movements and reduce computational efforts while maintaining result quality.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the computational inefficiencies in standard firefly algorithms (FA) when applied to truss topology optimization problems.

Method: SFA modifies the ordinary FA by integrating hypothesis testing to assess the historical motion data of fireflies, thereby limiting their movements only to potentially beneficial ones without altering the algorithm's fundamental structure.

Result: The SFA demonstrated improved computational efficiency and maintained the quality of results when tested on several truss topology optimization problems, including benchmarks.

Conclusion: Incorporating statistical strategies into the FA significantly enhances performance, making it both computationally efficient and straightforward to implement while maintaining optimization efficacy.

Abstract: This study proposes an algorithm titled a statistical firefly algorithm (SFA) for truss topology optimization. In the proposed algorithm, historical results of fireflies' motions are used in hypothesis testing to limit the motions of fireflies that are suggested by current information exchanges between fireflies only to those that are potentially useful. Hypothesis testing is applied to the mechanism of an ordinary firefly algorithm (FA) without changing its structure. As a result, the implementation of the proposed algorithm is simple and straightforward. Limiting the motions of fireflies to those that are potential useful results in reduction of firefly evaluations, and, subsequently, reduction of computational efforts. To test the validity and efficiency of the proposed algorithm, it is used to solve several truss topology optimization problems, including some benchmark problems. It is found that the added statistical strategy in the SFA significantly enhances the performance of the original FA in terms of computational efforts while still maintains the quality of the obtained results.

</details>


### [786] [An Evolutionary Framework for Automatic Optimization Benchmark Generation via Large Language Models](https://arxiv.org/abs/2601.12723)
*Yuhiro Ono,Tomohiro Harada,Yukiya Miura*

Main category: cs.NE

TL;DR: This paper introduces an LLM-driven Evolutionary Benchmark Generator (LLM-EBG) to create benchmarks for algorithm performance assessment, addressing common shortcomings of traditional benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing artificial benchmarks don't match real-world problem diversity and irregularity, while real-world-based benchmarks are costly and complex, necessitating a better way to create meaningful benchmarks.

Method: The authors use a large language model as a generative operator in an evolutionary framework to create unconstrained single-objective continuous minimization problems aimed at differentiating algorithm performance.

Result: Experimental results demonstrate that LLM-EBG can design benchmark problems where a target algorithm outperforms another in over 80% of trials, with distinct problem characteristics tailored to algorithm behaviors.

Conclusion: LLM-EBG effectively generates expressive and diverse benchmarks that highlight differences in algorithmic performance, making it valuable for optimization research.

Abstract: Optimization benchmarks play a fundamental role in assessing algorithm performance; however, existing artificial benchmarks often fail to capture the diversity and irregularity of real-world problem structures, while benchmarks derived from real-world problems are costly and difficult to construct. To address these challenges, we propose an evolutionary automatic benchmark generation framework that leverages a large language model (LLM) as a generative operator, termed the LLM-driven evolutionary benchmark generator (LLM-EBG). In this framework, the LLM serves as an evolutionary operator that generates and evolves benchmark problems within a flexible, expressive representation space. As a case study, we generate unconstrained single-objective continuous minimization problems represented as mathematical expressions designed to induce significant performance differences between a genetic algorithm (GA) and differential evolution (DE). Experimental results show that LLM-EBG successfully produces benchmark problems in which the designated target algorithm consistently outperforms the comparative algorithm in more than 80\% of trials. Furthermore, exploratory landscape analysis reveals that benchmarks favoring GA are highly sensitive to variable scaling, demonstrating that the proposed framework can generate problems with distinct geometric characteristics that reflect the intrinsic search behaviors of different optimization algorithms.

</details>


### [787] [Generalization and Completeness of Stochastic Local Search Algorithms](https://arxiv.org/abs/2601.14212)
*Daniel Loscos,Narciso Marti-Oliet,Ismael Rodriguez*

Main category: cs.NE

TL;DR: The paper develops a generalized formal model for Stochastic Local Search (SLS) heuristics, proves its Turing-completeness, and discusses undecidability of certain properties.


<details>
  <summary>Details</summary>
Motivation: To unify and formalize Stochastic Local Search (SLS) heuristics into a generalized framework and analyze the computational power of these algorithms.

Method: The authors construct a generalized formal model of SLS, demonstrating instances for Genetic Algorithms (GA), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO). They then use this model to prove Turing-completeness of SLS by simulating a Turing machine.

Result: The paper establishes that SLS heuristics are Turing-complete and shows that determining any non-trivial property of inputs and outputs in such heuristics is undecidable.

Conclusion: SLS algorithms, as a general class, are Turing-complete but exhibit inherent undecidability issues regarding specific computational properties.

Abstract: We generalize Stochastic Local Search (SLS) heuristics into a unique formal model. This model has two key components: a common structure designed to be as large as possible and a parametric structure intended to be as small as possible. Each heuristic is obtained by instantiating the parametric part in a different way. Particular instances for Genetic Algorithms (GA), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO) are presented. Then, we use our model to prove the Turing-completeness of SLS algorithms in general. The proof uses our framework to construct a GA able to simulate any Turing machine. This Turing-completeness implies that determining any non-trivial property concerning the relationship between the inputs and the computed outputs is undecidable for GA and, by extension, for the general set of SLS methods (although not necessarily for each particular method). Similar proofs are more informally presented for PSO and ACO.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [788] [Context-Free Grammar Inference for Complex Programming Languages in Black Box Settings](https://arxiv.org/abs/2601.12385)
*Feifei Li,Xiao Chen,Xiaoyu Sun,Xi Xiao,Shaohua Wang,Yong Ding,Sheng Wen,Qing Li*

Main category: cs.PL

TL;DR: Existing grammar inference tools like Arvada, Treevada, and Kedavra are inefficient for large and complex programming languages like C, C++, and Java. Crucio overcomes these limitations with a novel decomposition forest and achieves better recall, F1 scores, and faster grammar inference.


<details>
  <summary>Details</summary>
Motivation: Grammar inference for real-world and complex programming languages is inefficient with current methods, which cannot scale within practical time constraints.

Method: Crucio uses a decomposition forest and distributional matrix to perform lexical and grammar inference on short examples derived from large datasets.

Result: Crucio successfully infers grammars for complex languages within reasonable time, handling up to 23x more nonterminals. It outperforms previous tools in recall and F1 scores on simple benchmarks.

Conclusion: Crucio is a significant advancement in grammar inference for complex programming languages, offering improved efficiency, scalability, and accuracy over prior methods.

Abstract: Grammar inference for complex programming languages remains a significant challenge, as existing approaches fail to scale to real world datasets within practical time constraints. In our experiments, none of the state-of-the-art tools, including Arvada, Treevada and Kedavra were able to infer grammars for complex languages such as C, C++, and Java within 48 hours. Arvada and Treevada perform grammar inference directly on full-length input examples, which proves inefficient for large files commonly found in such languages. While Kedavra introduces data decomposition to create shorter examples for grammar inference, its lexical analysis still relies on the original inputs. Additionally, its strict no-overgeneralization constraint limits the construction of complex grammars.
  To overcome these limitations, we propose Crucio, which builds a decomposition forest to extract short examples for lexical and grammar inference via a distributional matrix. Experimental results show that Crucio is the only method capable of successfully inferring grammars for complex programming languages (where the number of nonterminals is up to 23x greater than in prior benchmarks) within reasonable time limits. On the prior simple benchmark, Crucio achieves an average recall improvement of 1.37x and 1.19x over Treevada and Kedavra, respectively, and improves F1 scores by 1.21x and 1.13x.

</details>


### [789] [An Introduction to Razborov's Flag Algebra as a Proof System for Extremal Graph Theory](https://arxiv.org/abs/2601.12741)
*Gyeongwon Jeong,Seonghun Park,Hongseok Yang*

Main category: cs.PL

TL;DR: This paper surveys Razborov's flag algebra and adapts it for use in computer science fields, linking it with formal logic and showcasing applications.


<details>
  <summary>Details</summary>
Motivation: To introduce flag algebra to computer scientists by providing a formal logic perspective and highlighting its applicability in automated verification, logic, and programming languages.

Method: The authors present flag algebra through syntax, semantics, and proof strategies. They analyze the downward operator mechanism and highlight its connection to adjoint pairs.

Result: The paper connects the flag algebra framework to logic and proves key mathematical results symbolically using this approach, demonstrating its practical applications.

Conclusion: Flag algebra is versatile and conceptually linked to logic frameworks in computer science, enabling symbolic proofs and understanding extremal graph theory results.

Abstract: Razborov's flag algebra forms a powerful framework for deriving asymptotic inequalities between induced subgraph densities, underpinning many advances in extremal graph theory. This survey introduces flag algebra to computer scientists working in logic, programming languages, automated verification, and formal methods. We take a logical perspective on flag algebra and present it in terms of syntax, semantics, and proof strategies, in a style closer to formal logic. One popular proof strategy derives valid inequalities by first proving inequalities in a labelled variant of flag algebra and then transferring them to the original unlabelled setting using the so-called downward operator. We explain this strategy in detail and highlight that its transfer mechanism relies on the notion of what we call an adjoint pair, reminiscent of Galois connections and categorical adjunctions, which appear frequently in work on automated verification and programming languages. Along the way, we work through representative examples, including Mantel's theorem and Goodman's bound on Ramsey multiplicity, to illustrate how mathematical arguments can be carried out symbolically in the flag algebra framework.

</details>


### [790] [A Formally Verified Procedure for Width Inference in FIRRTL](https://arxiv.org/abs/2601.12813)
*Keyin Wang,Xiaomu Shi,Jiaxiang Liu,Zhilin Wu,Taolve Chen,Fu Song,David N. Jansen*

Main category: cs.PL

TL;DR: The paper investigates FIRRTL's width inference problem, proves it has a unique least solution when constraints are satisfiable, and introduces a formally verified implementation that outperforms the official version.


<details>
  <summary>Details</summary>
Motivation: The official FIRRTL compiler's InferWidths pass may fail even for simple programs, necessitating an investigation into the width inference problem and a more reliable solution.

Method: The authors analyze the constraints in FIRRTL programs, demonstrate a unique least solution existence, develop a formally verified inference procedure in Rocq, and extract an OCaml implementation.

Result: The proposed implementation solves more instances than the official InferWidths pass and demonstrates high efficiency, backed by formal verification.

Conclusion: The paper offers a theoretically sound and practically efficient solution to FIRRTL's width inference problem, improving reliability in hardware compilation.

Abstract: FIRRTL is an intermediate representation language for Register Transfer Level (RTL) hardware designs. In FIRRTL programs, the bit widths of many components are not specified explicitly and must be inferred during compilation. In mainstream FIRRTL compilers, such as the official compiler firtool, width inference is conducted by a compilation pass referred to as InferWidths, which may fail even for simple FIRRTL programs. In this paper, we thoroughly investigate the width inference problem for FIRRTL programs. We show that, if the constraints obtained from a FIRRTL program are satisfiable, there exists a unique least solution. Based on this result, we propose a complete procedure for solving the width inference problem. We implement it in the interactive theorem prover Rocq and prove its functional correctness. From the Rocq implementation, we extract an OCaml implementation, which is the first formally verified implementation of the InferWidths pass. Extensive experiments demonstrate that our approach can solve more instances than the official InferWidths pass in firtool, normally with high efficiency.

</details>


### [791] [Dependently-Typed AARA: A Non-Affine Approach for Resource Analysis of Higher-Order Programs](https://arxiv.org/abs/2601.12943)
*Han Xu,Di Wang*

Main category: cs.PL

TL;DR: The paper introduces λ_\ms{amor}^\ms{na}}, a resource analysis system that overcomes challenges in accurately determining resource consumption in higher-order functional programs.


<details>
  <summary>Details</summary>
Motivation: Traditional affine type systems struggle with precise resource behavior analysis for higher-order programs due to the tight connection between types and resources and conflicts between affine and higher-order mechanisms.

Method: λ_\ms{amor}^\ms{na}} uses a non-affine dependent type system to decouple types and resource analysis. It employs dependent types to express type-level potential functions separately from standard types.

Result: The proposed system is proven sound and can derive precise resource bounds. Classic and higher-order examples demonstrate its expressiveness and compositionality.

Conclusion: λ_\ms{amor}^\ms{na}} provides a novel approach to accurately analyze resource behaviors for higher-order functional programming.

Abstract: Static resource analysis determines the resource consumption (e.g., time complexity) of a program without executing it. Among the numerous existing approaches for resource analysis, affine type systems have been one dominant approach. However, these affine type systems fall short of deriving precise resource behavior of higher-order programs, particularly in cases that involve partial applications.
  This article presents λ_\ms{amor}^\ms{na}}, a non-affine AARA-style dependent type system for resource reasoning about higher-order functional programs. The key observation is that the main issue in previous approaches comes from (i) the close coupling of types and resources, and (ii) the conflict between affine and higher-order typing mechanisms. To derive precise resource behavior of higher-order functions, λ_\ms{amor}^\ms{na}} decouples resources from types and follows a non-affine typing mechanism. The non-affine type system of λ_\ms{amor}^\ms{na}} achieves this by using dependent types, which allows expressing type-level potential functions separate from ordinary types. This article formalizes λ_\ms{amor}^\ms{na}}'s syntax and semantics, and proves its soundness, which guarantees the correctness of resource bounds. Several challenging classic and higher-order examples are presented to demonstrate the expressiveness and compositionality of λ_\ms{amor}^\ms{na}}'s reasoning capability.

</details>


### [792] [Functional Logic Program Transformations](https://arxiv.org/abs/2601.13224)
*Michael Hanus,Steven Libby*

Main category: cs.PL

TL;DR: This paper explores how functional logic programming can simplify program transformations and evaluates its efficacy compared to deterministic approaches using Curry and FlatCurry as case studies.


<details>
  <summary>Details</summary>
Motivation: The challenges of implementing program transformations in tools like compilers due to the need to iterate and modify abstract syntax trees comprehensively.

Method: Functional logic programming is proposed for compact, non-deterministic, and partially defined operations for program transformations. A comparison is made with deterministic methods, using the language Curry and FlatCurry for evaluation.

Result: The paper evaluates the non-deterministic approach's overhead and compares its effectiveness and efficiency with deterministic methods for program transformations.

Conclusion: Functional logic programming provides a comprehensible and potentially compact method for program transformations, though it may introduce overheads.

Abstract: Many tools used to process programs, like compilers, analyzers, or verifiers, perform transformations on their intermediate program representation, like abstract syntax trees. Implementing such program transformations is a non-trivial task, since it is necessary to iterate over the complete syntax tree and apply various transformations at nodes in a tree. In this paper we show how the features of functional logic programming are useful to implement program transformations in a compact and comprehensible manner. For this purpose, we propose to write program transformations as partially defined and non-deterministic operations. Since the implementation of non-determinism usually causes some overhead compared to deterministically defined operations, we compare our approach to a deterministic transformation method. We evaluate these alternatives for the functional logic language Curry and its intermediate representation FlatCurry which is used in various analysis and verification tools and compilers.

</details>


### [793] [Reduction for Structured Concurrent Programs](https://arxiv.org/abs/2601.13341)
*Namratha Gangamreddypalli,Constantin Enea,Shaz Qadeer*

Main category: cs.PL

TL;DR: The paper introduces a novel reduction technique for structured concurrent programs, generalizing Lipton's reduction strategy and enabling the flexible composition of key reduction methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of scaling commutativity reasoning to handle routinely-used features in software systems, including procedures and parallel composition.

Method: The approach introduces two main strategies: 1) replacing parallel composition with sequential composition, and 2) generalizing Lipton's reduction to support atomic sections with (potentially recursive) procedure calls, which can be composed arbitrarily.

Result: The technique was implemented in the Civl tool and successfully applied to challenging case studies such as a snapshot object, fault-tolerant registers, Flash cache protocols, and a Two-Phase Commit variant.

Conclusion: This unified reduction approach significantly extends the applicability and effectiveness of commutativity reasoning in concurrent program verification.

Abstract: Commutativity reasoning based on Lipton's movers is a powerful technique for verification of concurrent programs. The idea is to define a program transformation that preserves a subset of the initial set of interleavings, which is sound modulo reorderings of commutative actions. Scaling commutativity reasoning to routinely-used features in software systems, such as procedures and parallel composition, remains a significant challenge.
  In this work, we introduce a novel reduction technique for structured concurrent programs that unifies two key advances. First, we present a reduction strategy that soundly replaces parallel composition with sequential composition. Second, we generalize Lipton's reduction to support atomic sections containing (potentially recursive) procedure calls. Crucially, these two foundational strategies can be composed arbitrarily, greatly expanding the scope and flexibility of reduction-based reasoning. We implemented this technique in Civl and demonstrated its effectiveness on a number of challenging case studies, including a snapshot object, a fault-tolerant and linearizable register, the FLASH cache coherence protocol, and a non-trivial variant of Two-Phase Commit.

</details>


### [794] [Foundational VeriFast: Pragmatic Certification of Verification Tool Results through Hinted Mirroring](https://arxiv.org/abs/2601.13727)
*Bart Jacobs*

Main category: cs.PL

TL;DR: VeriFast, a verification tool for C and Rust programs, generates Rocq proof scripts upon verifying Rust programs, ensuring enhanced safety in critical domains.


<details>
  <summary>Details</summary>
Motivation: To address the lack of formal verification of VeriFast itself and improve its reliability for safety-critical applications.

Method: Extending VeriFast to output Rocq proof scripts, guided by a 'hinted mirroring' process that replicates symbolic execution runs in Rocq.

Result: VeriFast can now provide Rocq proofs, increasing trust in its validation of Rust programs.

Conclusion: The integration of Rocq scripting significantly bolsters VeriFast's utility in critical software verification.

Abstract: VeriFast is a leading tool for the modular formal verification of correctness properties of single-threaded and multi-threaded C and Rust programs. It verifies a program by symbolically executing each function in isolation, exploiting user-annotated preconditions, postconditions, and loop invariants written in a form of separation logic, and using a separation logic-based symbolic representation of memory. However, the tool itself, written in roughly 30K lines of OCaml code, has not been formally verified. Therefore, bugs in the tool could cause it to falsely report the correctness of the input program. We here report on an early result extending VeriFast to emit, upon successful verification of a Rust program, a Rocq proof script that proves correctness of the program with respect to a Rocq-encoded axiomatic semantics of Rust. This significantly enhances VeriFast's applicability in safety-critical domains. We apply hinted mirroring: we record key information from VeriFast's symbolic execution run, and use it to direct a replay of the run in Rocq.

</details>


### [795] [Generating Functions Meet Occupation Measures: Invariant Synthesis for Probabilistic Loops (Extended Version)](https://arxiv.org/abs/2601.13991)
*Darion Haase,Kevin Batz,Adrian Gallus,Benjamin Lucien Kaminski,Joost-Pieter Katoen,Lutz Klinkenberg,Tobias Winkler*

Main category: cs.PL

TL;DR: The paper addresses mathematically exact inference of posterior distributions in probabilistic programs with loops, utilizing occupation invariants as a novel technique.


<details>
  <summary>Details</summary>
Motivation: The study aims to solve the challenge of performing exact inference in expressive probabilistic programming languages, especially when they include loops or unbounded recursion, as current methods focus predominantly on statistical approximations rather than exact solutions.

Method: The method introduces occupation invariants, based on the concept of occupation measures, and utilizes these invariants to analyze probabilistic loops. Additionally, an automatic, template-based synthesis approach is implemented to generate such invariants via encoding them as generating functions.

Result: A novel approach leveraging occupation invariants is developed and implemented, demonstrating its applicability and success on benchmarks for proving exact inference and termination properties.

Conclusion: Occupation invariants provide an effective mathematical foundation for exact posterior inference in probabilistic programs, showing potential for advancing the analysis of programming loops with probabilistic behaviors.

Abstract: A fundamental computational task in probabilistic programming is to infer a program's output (posterior) distribution from a given initial (prior) distribution. This problem is challenging, especially for expressive languages that feature loops or unbounded recursion. While most of the existing literature focuses on statistical approximation, in this paper we address the problem of mathematically exact inference.
  To achieve this for programs with loops, we rely on a relatively underexplored type of probabilistic loop invariant, which is linked to a loop's so-called occupation measure. The occupation measure associates program states with their expected number of visits, given the initial distribution. Based on this, we derive the notion of an occupation invariant. Such invariants are essentially dual to probabilistic martingales, the predominant technique for formal probabilistic loop analysis in the literature. A key feature of occupation invariants is that they can take the initial distribution into account and often yield a proof of positive almost sure termination as a by-product.
  Finally, we present an automatic, template-based invariant synthesis approach for occupation invariants by encoding them as generating functions. The approach is implemented and evaluated on a set of benchmarks.

</details>


### [796] [Verifying Floating-Point Programs in Stainless](https://arxiv.org/abs/2601.14059)
*Andrea Gilot,Axel Bergström,Eva Darulova*

Main category: cs.PL

TL;DR: First automated verifier for floating-point numbers in a subset of Scala, including recursion and higher-order functions.


<details>
  <summary>Details</summary>
Motivation: To extend verification support to floating-point numbers in Scala for real-world applications.

Method: Axiomatize mathematical functions and verify correctness against Scala's math API while testing on real-world benchmarks.

Result: Successfully verified specifications, ranges, and absence of special values for floating-point functions, while identifying counterexamples for invalid specifications.

Conclusion: Enhanced Stainless tool supports comprehensive floating-point verification in Scala, validated on real-world benchmarks.

Abstract: We extend the Stainless deductive verifier with floating-point support, providing the first automated verification support for floating-point numbers for a subset of Scala that includes polymorphism, recursion and higher-order functions. We follow the recent approach in the KeY verifier to axiomatise reasoning about mathematical functions, but go further by supporting all functions from Scala's math API, and by verifying the correctness of the axioms against the actual implementation in Stainless itself. We validate Stainless' floating-point support on a new set of benchmarks sampled from real-world code from GitHub, showing that it can verify specifications about, e.g., ranges of output or absence of special values for most supported functions, or produce counter-examples when the specifications do not hold.

</details>


### [797] [Partial Reductions for Kleene Algebra with Linear Hypotheses](https://arxiv.org/abs/2601.14114)
*Liam Chung,Tobias Kappé*

Main category: cs.PL

TL;DR: The paper tackles limitations in Kleene algebra (KA) by introducing an automaton-based method to derive reductions for program-specific hypotheses, enhancing the ability to prove program equivalences.


<details>
  <summary>Details</summary>
Motivation: KA is a powerful tool, but it cannot always prove equivalences of specific programs due to the lack of hypotheses encoding program-specific knowledge.

Method: An automaton-based construction is proposed to automatically derive reductions for a broad range of hypotheses, overcoming traditional manual labor requirements.

Result: Using this automated approach, the paper establishes partial completeness, enabling the provability of more program equivalences than previous methods.

Conclusion: The automaton-based method improves the scope of Kleene algebra by enabling mechanized reductions, thus expanding its applicability to proving program equivalences.

Abstract: Kleene algebra (KA) is an important tool for reasoning about general program equivalences, with a decidable and complete equational theory. However, KA cannot always prove equivalences between specific programs. For this purpose, one adds hypotheses to KA that encode program-specific knowledge. Traditionally, a map on regular expressions called a reduction then lets us lift decidability and completeness to these more expressive systems. Explicitly constructing such a reduction requires significant labour. Moreover, due to regularity constraints, a reduction may not exist for all combinations of expression and hypothesis.
  We describe an automaton-based construction to mechanically derive reductions for a wide class of hypotheses. These reductions can be partial, in which case they yield partial completeness: completeness for expressions in their domain. This allows us to automatically establish the provability of more equivalences than what is covered in existing work.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [798] [RobotDesignGPT: Automated Robot Design Synthesis using Vision Language Models](https://arxiv.org/abs/2601.11801)
*Nitish Sontakke,K. Niranjan Kumar,Sehoon Ha*

Main category: cs.RO

TL;DR: The paper introduces RobotDesignGPT, an automated framework that uses vision-language models to create robot designs from user prompts and images.


<details>
  <summary>Details</summary>
Motivation: Robot design is a complex task often requiring significant expertise and effort due to multiple criteria like user needs, kinematics, and aesthetics. Current rule-based methods are restrictive, motivating the need for automation.

Method: RobotDesignGPT synthesizes robot designs leveraging vision-language models. Initial designs are created from user prompts and reference images, with a visual feedback mechanism to improve quality and reduce manual revisions.

Result: The framework successfully produces visually pleasing and kinematically valid robot designs, inspired by natural creatures like legged animals and flying beings.

Conclusion: RobotDesignGPT automates the synthesis of complex robot designs, reducing manual effort while maintaining design appeal and validity.

Abstract: Robot design is a nontrivial process that involves careful consideration of multiple criteria, including user specifications, kinematic structures, and visual appearance. Therefore, the design process often relies heavily on domain expertise and significant human effort. The majority of current methods are rule-based, requiring the specification of a grammar or a set of primitive components and modules that can be composed to create a design. We propose a novel automated robot design framework, RobotDesignGPT, that leverages the general knowledge and reasoning capabilities of large pre-trained vision-language models to automate the robot design synthesis process. Our framework synthesizes an initial robot design from a simple user prompt and a reference image. Our novel visual feedback approach allows us to greatly improve the design quality and reduce unnecessary manual feedback. We demonstrate that our framework can design visually appealing and kinematically valid robots inspired by nature, ranging from legged animals to flying creatures. We justify the proposed framework by conducting an ablation study and a user study.

</details>


### [799] [Optimal Thruster Configuration for 6-DOF Control of a Small Satellite](https://arxiv.org/abs/2601.11802)
*Suguru Sato,Jinaykumar Patel,Kamesh Subbarao*

Main category: cs.RO

TL;DR: The paper discusses creating optimized thruster configurations for small satellites to achieve full six degrees of freedom (6-DOF) control, minimizing thruster count while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: With the increasing use of small satellites in Low Earth Orbit for various missions, effective orbit maintenance and attitude control are essential. Traditional approaches using multiple thrusters necessitate optimization for efficiency and performance.

Method: Starting from a 24-thruster configuration, the study identifies viable configurations enabling 6-DOF control and narrows them down to the ones requiring minimum total thrust. Selected configurations are analyzed for attitude control performance through a rendezvous-docking mission.

Result: Optimal thruster configurations with reduced thruster count maintain sufficient maneuverability for objectives like rendezvous-docking while achieving full 6-DOF commands.

Conclusion: The proposed configurations enhance the efficiency and performance of small satellite orbit and attitude control systems, demonstrating potential for practical application in missions with fewer thrusters.

Abstract: With the growing deployment of small satellites (such as CubeSats, Nanosats, Picosats, and Femtosats) in Low Earth Orbit (LEO) for targeted applications like imaging, communication, data storage, and rendezvous-docking mission, there is increasing attention on orbit maintenance and attitude control. A common approach for active orbit control involves the use of multiple thrusters, which, when properly arranged, can also generate the required torque for attitude control. Starting from a 24-thruster configuration, this paper presents a set of thruster configurations (referred to as a viable configuration group) that enable full six degrees of freedom (6-DOF) control. Further, configuration group that requires minimum total thrust to achieve 6-DOF commands are found among the viable configuration group. One configuration from each of these groups is further evaluated for its attitude control performance through a representative rendezvous-docking mission, demonstrating that even with a reduced thruster count, sufficient maneuverability can be achieved.

</details>


### [800] [Three Dimensional Hydrodynamic Flow-Based Collision Avoidance for UAV Formations Facing Emergent Dynamic Obstacles](https://arxiv.org/abs/2601.11832)
*Suguru Sato,Kamesh Subbarao*

Main category: cs.RO

TL;DR: This paper introduces a hydrodynamics-inspired collision avoidance system for UAV formations using local velocity fields and a Virtual Rigid Body strategy.


<details>
  <summary>Details</summary>
Motivation: To provide efficient and smooth collision avoidance for UAV formations in dynamic environments without trajectory discontinuities or explicit replanning.

Method: Uses modeled obstacles as 3D doublets or ellipsoids generating velocity fields based on Laplace's equation, combined with a Virtual Rigid Body coordination strategy.

Result: Simulation results confirm the framework's feasibility and scalability for real-time and practical applications in both singular and multi-UAV scenarios.

Conclusion: The approach enables safe, efficient, and coordinated UAV operation with scalable collision avoidance in dynamic settings.

Abstract: This paper presents a three-dimensional, hydrodynamics-inspired collision avoidance framework for uncrewed aerial vehicle (UAV) formations operating in dynamic environments. When moving obstacles enter a UAV's sensing region, they are modeled as three dimensional doublets or ellipsoids that generate local velocity fields, guiding nearby UAVs to execute smooth, collision-free maneuvers without trajectory discontinuities or explicit trajectory replanning. This flow-based approach enables real-time operation and interpretable behavior by leveraging the nature of fluid flow around obstacles via the harmonic properties of Laplace's equation, inherently avoiding local minima common in traditional potential field methods. To establish and maintain coordination among the UAVs, a Virtual Rigid Body (VRB) formation strategy is integrated, ensuring that formation geometry and trajectory tracking are preserved. Simulation results demonstrate the feasibility and scalability of the method for both individual and multi-UAV scenarios with multiple formation geometries encountering moving obstacles. The proposed approach achieves safe, smooth, and computationally efficient avoidance maneuvers suitable for real-time and practical applications.

</details>


### [801] [AI for Green Spaces: Leveraging Autonomous Navigation and Computer Vision for Park Litter Removal](https://arxiv.org/abs/2601.11876)
*Christopher Kao,Akhil Pathapati,James Davis*

Main category: cs.RO

TL;DR: The paper presents the development of an autonomous robot for trash pickup on grass fields using navigation algorithms, RTK GPS, computer vision, and a specialized pickup mechanism, achieving an 80% success rate.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem of litter on grass fields in the U.S., aiming to reduce pollution caused by careless picnickers through automation.

Method: The authors utilized the STC algorithm for navigation, RTK GPS for precise location data, ResNet50 CNN for trash detection, and tested multiple designs for trash pickup mechanisms.

Result: The robot navigates autonomously, detects trash with 94.52% accuracy, and successfully picks it up with an 80% overall success rate.

Conclusion: Autonomous trash pickup robots can effectively address litter problems in grass fields, showcasing their viability as a solution to environmental concerns.

Abstract: There are 50 billion pieces of litter in the U.S. alone. Grass fields contribute to this problem because picnickers tend to leave trash on the field. We propose building a robot that can autonomously navigate, identify, and pick up trash in parks. To autonomously navigate the park, we used a Spanning Tree Coverage (STC) algorithm to generate a coverage path the robot could follow. To navigate this path, we successfully used Real-Time Kinematic (RTK) GPS, which provides a centimeter-level reading every second. For computer vision, we utilized the ResNet50 Convolutional Neural Network (CNN), which detects trash with 94.52% accuracy. For trash pickup, we tested multiple design concepts. We select a new pickup mechanism that specifically targets the trash we encounter on the field. Our solution achieved an overall success rate of 80%, demonstrating that autonomous trash pickup robots on grass fields are a viable solution.

</details>


### [802] [Visual-Language-Guided Task Planning for Horticultural Robots](https://arxiv.org/abs/2601.11906)
*Jose Cuaran,Kendall Koe,Aditya Potnis,Naveen Kumar Uppalapati,Girish Chowdhary*

Main category: cs.RO

TL;DR: This paper introduces a Visual Language Model (VLM)-based framework for robotic crop monitoring tasks and highlights its strengths and limitations in monoculture and polyculture environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the lack of high-level reasoning in current precision agriculture systems and create a robust, deployable solution for robotic crop monitoring tasks.

Method: The proposed framework utilizes a Visual Language Model (VLM) for guiding robotic task planning, integrating input queries with corresponding action primitives. A benchmark is developed to evaluate tasks in agricultural environments.

Result: VLMs perform robustly for short-horizon tasks with human-comparable success but show significant degradation in long-horizon and noisy semantic map scenarios.

Conclusion: This study presents a deployable framework for crop monitoring and identifies critical limitations of VLMs, particularly in context grounding for sustained robotic operations.

Abstract: Crop monitoring is essential for precision agriculture, but current systems lack high-level reasoning. We introduce a novel, modular framework that uses a Visual Language Model (VLM) to guide robotic task planning, interleaving input queries with action primitives. We contribute a comprehensive benchmark for short- and long-horizon crop monitoring tasks in monoculture and polyculture environments. Our main results show that VLMs perform robustly for short-horizon tasks (comparable to human success), but exhibit significant performance degradation in challenging long-horizon tasks. Critically, the system fails when relying on noisy semantic maps, demonstrating a key limitation in current VLM context grounding for sustained robotic operations. This work offers a deployable framework and critical insights into VLM capabilities and shortcomings for complex agricultural robotics.

</details>


### [803] [A Comprehensive Review of Bio-Inspired Approaches to Coordination, Communication, and System Architecture in Underwater Swarm Robotics](https://arxiv.org/abs/2601.12244)
*Shyalan Ramesh,Scott Mann,Alex Stumpf*

Main category: cs.RO

TL;DR: This paper is a review on underwater swarm robotics, focusing on bio-inspired coordination, communication strategies, and hardware design for marine environments.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing need for intelligent robotic systems in marine operations and the potential of underwater swarm robotics inspired by natural systems for enhanced capability and adaptability.

Method: The paper synthesizes bio-inspired algorithms, such as Artificial Fish Swarm Algorithm, and analyses communication strategies and hardware requirements using a multi-dimensional classification framework.

Result: The review provides a unified analysis of coordination mechanisms, underwater communication solutions, and hardware designs in swarm robotics, along with performance evaluation criteria.

Conclusion: The paper identifies trends, challenges, and research opportunities for the real-world application of underwater swarm robotic systems.

Abstract: The increasing complexity of marine operations has intensified the need for intelligent robotic systems to support ocean observation, exploration, and resource management. Underwater swarm robotics offers a promising framework that extends the capabilities of individual autonomous platforms through collective coordination. Inspired by natural systems, such as fish schools and insect colonies, bio-inspired swarm approaches enable distributed decision-making, adaptability, and resilience under challenging marine conditions. Yet research in this field remains fragmented, with limited integration across algorithmic, communication, and hardware design perspectives. This review synthesises bio-inspired coordination mechanisms, communication strategies, and system design considerations for underwater swarm robotics. It examines key marine-specific algorithms, including the Artificial Fish Swarm Algorithm, Whale Optimisation Algorithm, Coral Reef Optimisation, and Marine Predators Algorithm, highlighting their applications in formation control, task allocation, and environmental interaction. The review also analyses communication constraints unique to the underwater domain and emerging acoustic, optical, and hybrid solutions that support cooperative operation. Additionally, it examines hardware and system design advances that enhance system efficiency and scalability. A multi-dimensional classification framework evaluates existing approaches across communication dependency, environmental adaptability, energy efficiency, and swarm scalability. Through this integrated analysis, the review unifies bio-inspired coordination algorithms, communication modalities, and system design approaches. It also identifies converging trends, key challenges, and future research directions for real-world deployment of underwater swarm systems.

</details>


### [804] [Model selection and real-time skill assessment for suturing in robotic surgery](https://arxiv.org/abs/2601.12012)
*Zhaoyang Jacopo Hu,Alex Ranne,Alaa Eldin Abdelaal,Kiran Bhattacharyya,Etienne Burdet,Allison M. Okamura,Ferdinando Rodriguez y Baena*

Main category: cs.RO

TL;DR: This paper develops real-time multimodal systems combining kinematic and vision data to predict surgical skill levels, showing improved accuracy and generalization with expert-level training data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to establish real-time, objective, and fine-grained assessments of surgical skill levels in robot-assisted surgeries using multimodal data, improving training and evaluation practices.

Method: The study uses data from the da Vinci Surgical System to build and test multimodal deep learning models, comparing unimodal and fusion architectures. Models were trained on datasets based on skill levels and evaluated with real-time performance trends.

Result: Fusion models combining kinematic and vision data outperformed unimodal models in predicting skill levels. Training on expert-level demonstrations enabled better performance and generalization.

Conclusion: Multimodal learning models enhance surgical skill assessment, with expert-level data being critical for better accuracy and transferability in training scenarios.

Abstract: Automated feedback systems have the potential to provide objective skill assessment for training and evaluation in robot-assisted surgery. In this study, we examine methods to achieve real-time prediction of surgical skill level in real-time based on Objective Structured Assessment of Technical Skills (OSATS) scores. Using data acquired from the da Vinci Surgical System, we carry out three main analyses, focusing on model design, their real-time performance, and their skill-level-based cross-validation training. For the model design, we evaluate the effectiveness of multimodal deep learning models for predicting surgical skill levels using synchronized kinematic and vision data. Our models include separate unimodal baselines and fusion architectures that integrate features from both modalities and are evaluated using mean Spearman's correlation coefficients, demonstrating that the fusion model consistently outperforms unimodal models for real-time predictions. For the real-time performance, we observe the prediction's trend over time and highlight correlation with the surgeon's gestures. For the skill-level-based cross-validation, we separately trained models on surgeons with different skill levels, which showed that high-skill demonstrations allow for better performance than those trained on low-skilled ones and generalize well to similarly skilled participants. Our findings show that multimodal learning allows more stable fine-grained evaluation of surgical performance and highlights the value of expert-level training data for model generalization.

</details>


### [805] [BiKC+: Bimanual Hierarchical Imitation with Keypose-Conditioned Coordination-Aware Consistency Policies](https://arxiv.org/abs/2601.12116)
*Hang Xu,Yizhou Chen,Dongjie Yu,Yi Ren,Jia PanI*

Main category: cs.RO

TL;DR: The paper introduces a novel framework for bimanual robot manipulation, emphasizing hierarchical imitation learning and a fast inference model that optimizes task efficiency.


<details>
  <summary>Details</summary>
Motivation: Bimanual manipulation is challenging for robots due to coordination complexities and the cascading nature of failures across multi-stage processes.

Method: The framework uses a hierarchical imitation learning approach with a keypose predictor for sub-goal generation and a trajectory generator formulated as a consistency model for efficient single-step inference.

Result: The approach significantly outperforms baseline methods in success rates and efficiency in both simulation and real-world experiments.

Conclusion: The proposed framework provides a substantial advance in enabling robots to perform bimanual manipulation tasks effectively, addressing coordination challenges while improving operational performance.

Abstract: Robots are essential in industrial manufacturing due to their reliability and efficiency. They excel in performing simple and repetitive unimanual tasks but still face challenges with bimanual manipulation. This difficulty arises from the complexities of coordinating dual arms and handling multi-stage processes. Recent integration of generative models into imitation learning (IL) has made progress in tackling specific challenges. However, few approaches explicitly consider the multi-stage nature of bimanual tasks while also emphasizing the importance of inference speed. In multi-stage tasks, failures or delays at any stage can cascade over time, impacting the success and efficiency of subsequent sub-stages and ultimately hindering overall task performance. In this paper, we propose a novel keypose-conditioned coordination-aware consistency policy tailored for bimanual manipulation. Our framework instantiates hierarchical imitation learning with a high-level keypose predictor and a low-level trajectory generator. The predicted keyposes serve as sub-goals for trajectory generation, indicating targets for individual sub-stages. The trajectory generator is formulated as a consistency model, generating action sequences based on historical observations and predicted keyposes in a single inference step. In particular, we devise an innovative approach for identifying bimanual keyposes, considering both robot-centric action features and task-centric operation styles. Simulation and real-world experiments illustrate that our approach significantly outperforms baseline methods in terms of success rates and operational efficiency. Implementation codes can be found at https://github.com/JoanaHXU/BiKC-plus.

</details>


### [806] [Active Semantic Mapping of Horticultural Environments Using Gaussian Splatting](https://arxiv.org/abs/2601.12122)
*Jose Cuaran,Naveen K. Upalapati,Girish Chowdhary*

Main category: cs.RO

TL;DR: The paper introduces a novel framework using a mobile manipulator for efficient 3D semantic reconstruction in agriculture, outperforming traditional methods in accuracy and runtime efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations of traditional manual and fixed-camera 3D reconstruction methods in agriculture, particularly for phenotyping and yield estimation tasks.

Method: The authors propose integrating Octomap representation with 3D Gaussian Splatting for precise scene reconstruction while utilizing a mobile manipulator for target-aware mapping and collision-free planning.

Result: The method demonstrates improvements in runtime efficiency (50% reduction) and reconstruction accuracy (up to 28.6% boost in F1 score under noise), also enabling precise fruit counting and volume estimation.

Conclusion: This work highlights the potential of combining Octomap and 3D Gaussian Splatting for scalable, real-time scene reconstruction in agricultural environments, achieving significant accuracy and runtime benefits.

Abstract: Semantic reconstruction of agricultural scenes plays a vital role in tasks such as phenotyping and yield estimation. However, traditional approaches that rely on manual scanning or fixed camera setups remain a major bottleneck in this process. In this work, we propose an active 3D reconstruction framework for horticultural environments using a mobile manipulator. The proposed system integrates the classical Octomap representation with 3D Gaussian Splatting to enable accurate and efficient target-aware mapping. While a low-resolution Octomap provides probabilistic occupancy information for informative viewpoint selection and collision-free planning, 3D Gaussian Splatting leverages geometric, photometric, and semantic information to optimize a set of 3D Gaussians for high-fidelity scene reconstruction. We further introduce simple yet effective strategies to enhance robustness against segmentation noise and reduce memory consumption. Simulation experiments demonstrate that our method outperforms purely occupancy-based approaches in both runtime efficiency and reconstruction accuracy, enabling precise fruit counting and volume estimation. Compared to a 0.01m-resolution Octomap, our approach achieves an improvement of 6.6% in fruit-level F1 score under noise-free conditions, and up to 28.6% under segmentation noise. Additionally, it achieves a 50% reduction in runtime, highlighting its potential for scalable, real-time semantic reconstruction in agricultural robotics.

</details>


### [807] [Neural Process-Based Reactive Controller for Autonomous Racing](https://arxiv.org/abs/2601.12143)
*Devin Hunter,Chinwendu Enyioha*

Main category: cs.RO

TL;DR: The paper proposes novel attention-based reactive control models (AttNP and PI-AttNP) for safer autonomous navigation, tested on a racing simulation and coupled with a collision-avoidance mechanism.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for statistically sound and provably safe decision-making in integrating neural architectures into safety-critical control scenarios like autonomous driving.

Method: The authors designed Attentive Neural Process (AttNP) and a physics-informed version PI-AttNP for gap-based navigation. A Control Barrier Function (CBF) filter for collision-avoidance constraints was also integrated.

Result: In a simulated racing environment, the models demonstrated competitive performance with the CBF ensuring real-time safety guarantees across various conditions.

Conclusion: The framework is efficient for real-time control, improving both predictive accuracy and safety in challenging autonomous driving tasks like reactive racing scenarios.

Abstract: Attention-based neural architectures have become central to state-of-the-art methods in real-time nonlinear control. As these data-driven models continue to be integrated into increasingly safety-critical domains, ensuring statistically grounded and provably safe decision-making becomes essential. This paper introduces a novel reactive control framework for gap-based navigation using the Attentive Neural Process (AttNP) and a physics-informed extension, the PI-AttNP. Both models are evaluated in a simulated F1TENTH-style Ackermann steering racecar environment, chosen as a fast-paced proxy for safety-critical autonomous driving scenarios. The PI-AttNP augments the AttNP architecture with approximate model-based priors to inject physical inductive bias, enabling faster convergence and improved prediction accuracy suited for real-time control. To further ensure safety, we derive and implement a control barrier function (CBF)-based filtering mechanism that analytically enforces collision avoidance constraints. This CBF formulation is fully compatible with the learned AttNP controller and generalizes across a wide range of racing scenarios, providing a lightweight and certifiable safety layer. Our results demonstrate competitive closed-loop performance while ensuring real-time constraint satisfaction.

</details>


### [808] [Learning Legged MPC with Smooth Neural Surrogates](https://arxiv.org/abs/2601.12169)
*Samuel A. Moore,Easop Lee,Boyuan Chen*

Main category: cs.RO

TL;DR: This paper integrates smooth neural surrogates with robust error modeling into model predictive control for legged robots to tackle challenges in learned dynamics, achieving major robustness improvements.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges in integrating learned neural network models with model predictive control for legged robot dynamics, focusing on issues like contact transition stiffness, non-physical nonsmoothness, and non-Gaussian errors.

Method: The authors propose 'smooth neural surrogates' for better trajectory optimization and train these models using heavy-tailed likelihood distributions to align with observed error patterns in legged robots.

Result: The presented methods lead to reliable and scalable legged MPC with consistent cost reduction (10-50%) on simple locomotion tasks and significant success rate and robustness improvements in difficult regimes (from failure to success and up to 50x cost reductions).

Conclusion: Smooth neural surrogates coupled with robust learning significantly enhance legged robotics performance by improving predictive reliability, enabling robust task execution, and ensuring scalability in diverse locomotion challenges.

Abstract: Deep learning and model predictive control (MPC) can play complementary roles in legged robotics. However, integrating learned models with online planning remains challenging. When dynamics are learned with neural networks, three key difficulties arise: (1) stiff transitions from contact events may be inherited from the data; (2) additional non-physical local nonsmoothness can occur; and (3) training datasets can induce non-Gaussian model errors due to rapid state changes. We address (1) and (2) by introducing the smooth neural surrogate, a neural network with tunable smoothness designed to provide informative predictions and derivatives for trajectory optimization through contact. To address (3), we train these models using a heavy-tailed likelihood that better matches the empirical error distributions observed in legged-robot dynamics. Together, these design choices substantially improve the reliability, scalability, and generalizability of learned legged MPC. Across zero-shot locomotion tasks of increasing difficulty, smooth neural surrogates with robust learning yield consistent reductions in cumulative cost on simple, well-conditioned behaviors (typically 10-50%), while providing substantially larger gains in regimes where standard neural dynamics often fail outright. In these regimes, smoothing enables reliable execution (from 0/5 to 5/5 success) and produces about 2-50x lower cumulative cost, reflecting orders-of-magnitude absolute improvements in robustness rather than incremental performance gains.

</details>


### [809] [An Efficient and Multi-Modal Navigation System with One-Step World Model](https://arxiv.org/abs/2601.12277)
*Wangtian Shen,Ziyang Meng,Jinming Ma,Mingliang Zhou,Diyun Xiang*

Main category: cs.RO

TL;DR: The paper proposes a lightweight navigation world model with a one-step generation process and a 3D U-Net architecture, enabling real-time, efficient, and robust mobile robot navigation.


<details>
  <summary>Details</summary>
Motivation: Current navigation approaches for mobile robots using end-to-end learning struggle with 3D spatial reasoning and real-world dynamics understanding. Existing world models based on transformers are computationally expensive and impractical for real-time applications.

Method: The authors designed a lightweight navigation world model featuring a one-step generation paradigm and a 3D U-Net backbone with spatial-temporal attention, integrated into an optimization-based planning framework with anchor-based initialization.

Result: The proposed model significantly reduced inference latency and improved predictive performance, enabling high-frequency control. Simulations and real-world experiments confirmed its superior efficiency and robustness over state-of-the-art baselines.

Conclusion: The new navigation world model offers a viable solution for real-time deployment, addressing limitations of existing models, and enhancing mobile robot navigation performance in diverse environments.

Abstract: Navigation is a fundamental capability for mobile robots. While the current trend is to use learning-based approaches to replace traditional geometry-based methods, existing end-to-end learning-based policies often struggle with 3D spatial reasoning and lack a comprehensive understanding of physical world dynamics. Integrating world models-which predict future observations conditioned on given actions-with iterative optimization planning offers a promising solution due to their capacity for imagination and flexibility. However, current navigation world models, typically built on pure transformer architectures, often rely on multi-step diffusion processes and autoregressive frame-by-frame generation. These mechanisms result in prohibitive computational latency, rendering real-time deployment impossible. To address this bottleneck, we propose a lightweight navigation world model that adopts a one-step generation paradigm and a 3D U-Net backbone equipped with efficient spatial-temporal attention. This design drastically reduces inference latency, enabling high-frequency control while achieving superior predictive performance. We also integrate this model into an optimization-based planning framework utilizing anchor-based initialization to handle multi-modal goal navigation tasks. Extensive closed-loop experiments in both simulation and real-world environments demonstrate our system's superior efficiency and robustness compared to state-of-the-art baselines.

</details>


### [810] [OpenNavMap: Structure-Free Topometric Mapping via Large-Scale Collaborative Localization](https://arxiv.org/abs/2601.12291)
*Jianhao Jiao,Changkun Liu,Jingwen Yu,Boyi Liu,Qianyi Zhang,Yue Wang,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: The paper introduces OPENNAVMAP, a scalable and structure-free map representation system for visual navigation utilizing 3D geometric foundation models, achieving high accuracy, multi-session map consistency, and practical utility in autonomous navigation.


<details>
  <summary>Details</summary>
Motivation: Traditional multi-session mapping methods face challenges like high maintenance costs and failure in feature-less conditions or significant viewpoint changes, which limits the efficiency of visual navigation in real-world environments.

Method: The proposed OPENNAVMAP integrates dynamic programming-based sequence matching, geometric verification, and confidence-calibrated optimization for on-demand map reconstruction, robust submap alignment, and global map consistency without the requirement of pre-built 3D models.

Result: On the Map-Free benchmark, OPENNAVMAP achieved superior accuracy with a 0.62m average translation error and maintained global consistency in 15km of merged maps, showing an absolute trajectory error below 3m. It also performed 12 successful autonomous navigation tasks on both simulated and physical robots.

Conclusion: OPENNAVMAP proves to be a scalable and effective solution for visual navigation, demonstrating high accuracy, global consistency, and real-world practicality in autonomous navigation. Its availability as public code can help advance visual navigation research.

Abstract: Scalable and maintainable map representations are fundamental to enabling large-scale visual navigation and facilitating the deployment of robots in real-world environments. While collaborative localization across multi-session mapping enhances efficiency, traditional structure-based methods struggle with high maintenance costs and fail in feature-less environments or under significant viewpoint changes typical of crowd-sourced data. To address this, we propose OPENNAVMAP, a lightweight, structure-free topometric system leveraging 3D geometric foundation models for on-demand reconstruction. Our method unifies dynamic programming-based sequence matching, geometric verification, and confidence-calibrated optimization to robust, coarse-to-fine submap alignment without requiring pre-built 3D models. Evaluations on the Map-Free benchmark demonstrate superior accuracy over structure-from-motion and regression baselines, achieving an average translation error of 0.62m. Furthermore, the system maintains global consistency across 15km of multi-session data with an absolute trajectory error below 3m for map merging. Finally, we validate practical utility through 12 successful autonomous image-goal navigation tasks on simulated and physical robots. Code and datasets will be publicly available in https://rpl-cs-ucl.github.io/OpenNavMap_page.

</details>


### [811] [From Shallow Waters to Mariana Trench: A Survey of Bio-inspired Underwater Soft Robots](https://arxiv.org/abs/2601.12353)
*Jie Wang,Peng Du,Yiyuan Zhang,Zhexin Xie,Cecilia Laschi*

Main category: cs.RO

TL;DR: The paper discusses bio-inspired soft robots as a promising solution for ocean exploration, addressing challenges posed by underwater environments.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of traditional underwater robots, such as handling extreme water pressure and reducing ecological damage, by leveraging bio-inspired designs.

Method: Review of recent advancements in underwater bio-inspired soft robots, analysis of design considerations, and exploration of applications and future directions.

Result: The authors document advancements in design considerations, bio-inspired principles, and practical applications of underwater soft robots for ocean exploration.

Conclusion: Bio-inspired soft robots offer significant potential for eco-friendly and efficient ocean exploration, paving the way for innovative future developments.

Abstract: Sample Exploring the ocean environment holds profound significance in areas such as resource exploration and ecological protection. Underwater robots struggle with extreme water pressure and often cause noise and damage to the underwater ecosystem, while bio-inspired soft robots draw inspiration from aquatic creatures to address these challenges. These bio-inspired approaches enable robots to withstand high water pressure, minimize drag, operate with efficient manipulation and sensing systems, and interact with the environment in an eco-friendly manner. Consequently, bio-inspired soft robots have emerged as a promising field for ocean exploration. This paper reviews recent advancements in underwater bio-inspired soft robots, analyses their design considerations when facing different desired functions, bio-inspirations, ambient pressure, temperature, light, and biodiversity , and finally explores the progression from bio-inspired principles to practical applications in the field and suggests potential directions for developing the next generation of underwater soft robots.

</details>


### [812] [R-VoxelMap: Accurate Voxel Mapping with Recursive Plane Fitting for Online LiDAR Odometry](https://arxiv.org/abs/2601.12377)
*Haobo Xi,Shiyong Zhang,Qianli Dong,Yunze Tong,Songyang Wu,Jing Yuan,Xuebo Zhang*

Main category: cs.RO

TL;DR: The paper introduces R-VoxelMap, a refined voxel mapping method addressing accuracy and outlier challenges using recursive plane fitting for optimal LiDAR odometry.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of voxel mapping in LiDAR odometry by addressing outliers, over-segmentation, and erroneous plane merges in existing methods.

Method: R-VoxelMap employs a recursive plane fitting strategy with a detailed outlier detect-and-reuse pipeline and a validity check algorithm to ensure precise voxel mapping.

Result: Experiments on diverse datasets show that R-VoxelMap outperforms state-of-the-art methods in accuracy with similar computational efficiency and memory usage.

Conclusion: R-VoxelMap enhances LiDAR SLAM applications through improved accuracy in localization, and the authors will release the code on GitHub.

Abstract: This paper proposes R-VoxelMap, a novel voxel mapping method that constructs accurate voxel maps using a geometry-driven recursive plane fitting strategy to enhance the localization accuracy of online LiDAR odometry. VoxelMap and its variants typically fit and check planes using all points in a voxel, which may lead to plane parameter deviation caused by outliers, over segmentation of large planes, and incorrect merging across different physical planes. To address these issues, R-VoxelMap utilizes a geometry-driven recursive construction strategy based on an outlier detect-and-reuse pipeline. Specifically, for each voxel, accurate planes are first fitted while separating outliers using random sample consensus (RANSAC). The remaining outliers are then propagated to deeper octree levels for recursive processing, ensuring a detailed representation of the environment. In addition, a point distribution-based validity check algorithm is devised to prevent erroneous plane merging. Extensive experiments on diverse open-source LiDAR(-inertial) simultaneous localization and mapping (SLAM) datasets validate that our method achieves higher accuracy than other state-of-the-art approaches, with comparable efficiency and memory usage. Code will be available on GitHub.

</details>


### [813] [VR$^2$: A Co-Located Dual-Headset Platform for Touch-Enabled Human-Robot Interaction Research](https://arxiv.org/abs/2601.12395)
*Chao Wang,Anna Belardinelli,Michael Gienger*

Main category: cs.RO

TL;DR: VR2VR is a dual VR-headset system allowing shared virtual environments, enabling realistic touch-based human-robot interaction for research purposes.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges in studying physical touch-rich human-robot interaction due to the cost and limitations of physical robots and traditional VR prototypes.

Method: The VR2VR system uses co-located VR headsets to enable a participant and operator to interact in different virtual embodiments, mapping the operator's movements to a virtual robot while supporting experimental control and precise physical touch alignment.

Result: The system successfully facilitates natural interaction by combining body and gaze tracking with inverse kinematics for touch precision, allowing the investigation of complex human-robot touch interactions.

Conclusion: VR2VR reduces barriers to prototyping and evaluating touch-focused, embodied robot behaviors, making it a valuable tool for HRI research.

Abstract: Touch-rich human-robot interaction (HRI) is difficult to study: building and programming physical robots is costly and slow, while VR-based robot prototypes often remove physical contact or break the tight coupling between an agent's body and the user's felt touch. We present VR2VR, a co-located dual VR-headset platform for HRI research in which a participant and a hidden operator share the same physical space while experiencing different virtual embodiments. The participant sees an expressive virtual robot that interacts face-to-face in a shared virtual environment. In real time, the robot's upper-body gestures, head and gaze behaviors, and facial expressions are mapped from the operator's tracked motion and face signals. Because the operator is physically co-present and calibrated into the same coordinate frame, the operator can also physically touch the participant, enabling the participant to perceive robot touch aligned with the robot's hands; finger and hand motion are mapped to the robot using inverse kinematics to support precise contact. Beyond faithful motion retargeting for limb teleoperation, our VR2VR system supports experimental control by retargeting or selectively enabling nonverbal channels (e.g., head only vs. head+eyes vs. head+eyes+facial expressions) while keeping physical interaction constant. We detail the system design, calibration workflow, and safety considerations, and demonstrate the platform through a touch-based Wizard-of-Oz HRI study, illustrating how VR2VR lowers barriers for rapidly prototyping and rigorously evaluating embodied, touch-centric robot behaviors.

</details>


### [814] [Learning Diverse Skills for Behavior Models with Mixture of Experts](https://arxiv.org/abs/2601.12397)
*Wangtian Shen,Jinming Ma,Mingliang Zhou,Ziyang Meng*

Main category: cs.RO

TL;DR: The paper proposes a method, "Di-BM," using Mixture of Experts to improve multi-task imitation learning for robotic manipulation, achieving better results and efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome the interference and performance degradation in multi-task learning for robotic manipulation by creating a system where multiple experts specialize in sub-regions of the observation space.

Method: They use a Mixture of Experts approach where each expert specializes in distinct observation distributions, represented via energy-based models, integrated seamlessly into standard imitation learning frameworks.

Result: Di-BM shows superior performance in multi-task settings, data efficiency, and reusability of expert-learned knowledge in robotic manipulation tasks compared to other methods.

Conclusion: Di-BM addresses multi-task interference issues effectively, making it a robust and efficient solution, and allowing for better transferability in imitation learning systems.

Abstract: Imitation learning has demonstrated strong performance in robotic manipulation by learning from large-scale human demonstrations. While existing models excel at single-task learning, it is observed in practical applications that their performance degrades in the multi-task setting, where interference across tasks leads to an averaging effect. To address this issue, we propose to learn diverse skills for behavior models with Mixture of Experts, referred to as Di-BM. Di-BM associates each expert with a distinct observation distribution, enabling experts to specialize in sub-regions of the observation space. Specifically, we employ energy-based models to represent expert-specific observation distributions and jointly train them alongside the corresponding action models. Our approach is plug-and-play and can be seamlessly integrated into standard imitation learning methods. Extensive experiments on multiple real-world robotic manipulation tasks demonstrate that Di-BM significantly outperforms state-of-the-art baselines. Moreover, fine-tuning the pretrained Di-BM on novel tasks exhibits superior data efficiency and the reusable of expert-learned knowledge. Code is available at https://github.com/robotnav-bot/Di-BM.

</details>


### [815] [ReWorld: Multi-Dimensional Reward Modeling for Embodied World Models](https://arxiv.org/abs/2601.12428)
*Baorui Peng,Wenyao Zhang,Liang Xu,Zekun Qi,Jiazhao Zhang,Hongsi Liu,Wenjun Zeng,Xin Jin*

Main category: cs.RO

TL;DR: The paper proposes ReWorld, a framework aligning video-based world models for robots to improve physical realism, task logic, and visual quality.


<details>
  <summary>Details</summary>
Motivation: Existing video-based world models lack physical fidelity, dynamic consistency, and task logic, making them less suitable for contact manipulation tasks and downstream applications.

Method: The authors introduce ReWorld which uses reinforcement learning alongside a ~235K video preference dataset to train a hierarchical reward model. This model aligns the video-based world models with multi-dimensional human preferences via a computationally efficient PPO-style algorithm.

Result: ReWorld improves generated rollouts in terms of physical fidelity, logical consistency, embodiment, and visual quality, outperforming prior approaches in comprehensive experiments.

Conclusion: ReWorld enhances robot learning by aligning video-based world models with human preferences, advancing their utility for manipulation tasks and other downstream applications.

Abstract: Recently, video-based world models that learn to simulate the dynamics have gained increasing attention in robot learning. However, current approaches primarily emphasize visual generative quality while overlooking physical fidelity, dynamic consistency, and task logic, especially for contact-rich manipulation tasks, which limits their applicability to downstream tasks. To this end, we introduce ReWorld, a framework aimed to employ reinforcement learning to align the video-based embodied world models with physical realism, task completion capability, embodiment plausibility and visual quality. Specifically, we first construct a large-scale (~235K) video preference dataset and employ it to train a hierarchical reward model designed to capture multi-dimensional reward consistent with human preferences. We further propose a practical alignment algorithm that post-trains flow-based world models using this reward through a computationally efficient PPO-style algorithm. Comprehensive experiments and theoretical analysis demonstrate that ReWorld significantly improves the physical fidelity, logical coherence, embodiment and visual quality of generated rollouts, outperforming previous methods.

</details>


### [816] [KILO-EKF: Koopman-Inspired Learned Observations Extended Kalman Filter](https://arxiv.org/abs/2601.12463)
*Zi Cong Guo,James R. Forbes,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: KILO-EKF introduces a novel approach to extended Kalman filtering by combining traditional methods with Koopman-inspired measurement learning for better accuracy and efficiency without explicit sensor model reliance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in modeling complex or poorly calibrated sensors, and enhance accuracy and consistency in recursive filtering.

Method: KILO-EKF uses a standard EKF prediction step integrated with a learned Koopman-inspired measurement model that lifts measurements into a feature space for linear-Gaussian modeling.

Result: Validated on quadrotor localization tasks, KILO-EKF proved more effective, achieving better accuracy and consistency than traditional EKF models with imperfect calibrations, while maintaining real-time operation and fast training.

Conclusion: KILO-EKF demonstrates efficient and scalable measurement learning, providing a robust alternative to traditional sensor calibration methods in complex systems.

Abstract: We present the Koopman-Inspired Learned Observations Extended Kalman Filter (KILO-EKF), which combines a standard EKF prediction step with a correction step based on a Koopman-inspired measurement model learned from data. By lifting measurements into a feature space where they are linear in the state, KILO-EKF enables flexible modeling of complex or poorly calibrated sensors while retaining the structure and efficiency of recursive filtering. The resulting linear-Gaussian measurement model is learned in closed form from groundtruth training data, without iterative optimization or reliance on an explicit parametric sensor model. At inference, KILO-EKF performs a standard EKF update using Jacobians obtained via the learned lifting. We validate the approach on a real-world quadrotor localization task using an IMU, ultra-wideband (UWB) sensors, and a downward-facing laser. We compare against multiple EKF baselines with varying levels of sensor calibration. KILO-EKF achieves better accuracy and consistency compared to data-calibrated baselines, and significantly outperforms EKFs that rely on imperfect geometric models, while maintaining real-time inference and fast training. These results demonstrate the effectiveness of Koopman-inspired measurement learning as a scalable alternative to traditional model-based calibration.

</details>


### [817] [Language-Based Swarm Perception: Decentralized Person Re-Identification via Natural Language Descriptions](https://arxiv.org/abs/2601.12479)
*Miquel Kegeleirs,Lorenzo Garattoni,Gianpiero Francesca,Mauro Birattari*

Main category: cs.RO

TL;DR: This paper presents a decentralized method for person re-identification in robot swarms using textual descriptions generated by vision-language models instead of visual feature embeddings.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance transparency, explainability, and accessibility for swarm-based person re-identification by using human-readable language instead of traditional opaque embeddings.

Method: Each robot individually generates text-based descriptions of people's appearances using a vision-language model. These descriptions are compared and clustered collaboratively within the swarm without a centralized system, and cluster summaries are generated via a language model.

Result: Preliminary experiments showed competitive results in maintaining identity consistency and interpretability, although limitations exist in text similarity metrics and computational efficiency.

Conclusion: Using language-based perception improves swarm transparency and enables natural-language querying. The study highlights decentralized perception while proposing further exploration in navigation and environmental language-based perception.

Abstract: We introduce a method for decentralized person re-identification in robot swarms that leverages natural language as the primary representational modality. Unlike traditional approaches that rely on opaque visual embeddings -- high-dimensional feature vectors extracted from images -- the proposed method uses human-readable language to represent observations. Each robot locally detects and describes individuals using a vision-language model (VLM), producing textual descriptions of appearance instead of feature vectors. These descriptions are compared and clustered across the swarm without centralized coordination, allowing robots to collaboratively group observations of the same individual. Each cluster is distilled into a representative description by a language model, providing an interpretable, concise summary of the swarm's collective perception. This approach enables natural-language querying, enhances transparency, and supports explainable swarm behavior. Preliminary experiments demonstrate competitive performance in identity consistency and interpretability compared to embedding-based methods, despite current limitations in text similarity and computational load. Ongoing work explores refined similarity metrics, semantic navigation, and the extension of language-based perception to environmental elements. This work prioritizes decentralized perception and communication, while active navigation remains an open direction for future study.

</details>


### [818] [Enabling High-Curvature Navigation in Eversion Robots through Buckle-Inducing Constrictive Bands](https://arxiv.org/abs/2601.12523)
*Cem Suulker,Muhie Al Haimus,Thomas Mack,Mohammad Sheikhsofla,Neri Niccolò Dei,Reza Kashef,Hadi Sadati,Federica Barontini,Fanny Ficuciello,Alberto Arezzo,Bruno Siciliano,Sebastien Ourselin,Kaspar Althoefer*

Main category: cs.RO

TL;DR: The paper introduces a passive method to enhance the navigation capabilities of tip-growing eversion robots by strategically adding buckling points, using circumferential bands, allowing them to move effectively in highly curved, narrow spaces.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of active steering or artificial muscle integration in eversion robots which adds structural complexity, while improving navigation capabilities and maintaining softness and compliance.

Method: The authors implemented inextensible circumferential bands to strategically induce buckling points along the robot's body and developed a Cosserat rod-based mathematical model to understand the stiffness reductions and bending mechanics.

Result: Experimental findings indicate a 91% stiffness reduction at the robot's tip, allowing for smooth movement through tight bends, including a 180-degree turn with a 25 mm bending radius, outperforming previous robots.

Conclusion: This method enhances maneuverability in curved pathways without compromising softness or increasing mechanical complexity, demonstrating applications for inspection tasks and medical procedures like colonoscopy.

Abstract: Tip-growing eversion robots are renowned for their ability to access remote spaces through narrow passages. However, achieving reliable navigation remains a significant challenge. Existing solutions often rely on artificial muscles integrated into the robot body or active tip-steering mechanisms. While effective, these additions introduce structural complexity and compromise the defining advantages of eversion robots: their inherent softness and compliance. In this paper, we propose a passive approach to reduce bending stiffness by purposefully introducing buckling points along the robot's outer wall. We achieve this by integrating inextensible diameter-reducing circumferential bands at regular intervals along the robot body facilitating forward motion through tortuous, obstacle cluttered paths. Rather than relying on active steering, our approach leverages the robot's natural interaction with the environment, allowing for smooth, compliant navigation. We present a Cosserat rod-based mathematical model to quantify this behavior, capturing the local stiffness reductions caused by the constricting bands and their impact on global bending mechanics. Experimental results demonstrate that these bands reduce the robot's stiffness when bent at the tip by up to 91 percent, enabling consistent traversal of 180 degree bends with a bending radius of as low as 25 mm-notably lower than the 35 mm achievable by standard eversion robots under identical conditions. The feasibility of the proposed method is further demonstrated through a case study in a colon phantom. By significantly improving maneuverability without sacrificing softness or increasing mechanical complexity, this approach expands the applicability of eversion robots in highly curved pathways, whether in relation to pipe inspection or medical procedures such as colonoscopy.

</details>


### [819] [RPT*: Global Planning with Probabilistic Terminals for Target Search in Complex Environments](https://arxiv.org/abs/2601.12701)
*Yunpeng Lyu,Chao Cao,Ji Zhang,Howie Choset,Zhongqiang Ren*

Main category: cs.RO

TL;DR: This paper presents the HPP-PT problem, optimizing for expected path cost when the termination point is probabilistic, and introduces RPT* and HATS systems to solve it efficiently with real-world applications.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitation in existing routing problems that do not account for probabilistic termination, which is crucial in applications like mobile robot target object search with uncertain location data.

Method: The authors propose RPT*, a search-based method using dynamic programming in a new state space and heuristics for computational efficiency. They extend it into the HATS system by integrating Bayesian filtering and autonomous exploration.

Result: The proposed approaches, RPT* and HATS, outperform baseline methods in simulations and real robot experiments by achieving faster and more effective target searching.

Conclusion: This work provides an efficient and effective strategy for solving the HPP-PT problem, with significant advancements for applications in uncertain and dynamic target search scenarios.

Abstract: Routing problems such as Hamiltonian Path Problem (HPP), seeks a path to visit all the vertices in a graph while minimizing the path cost. This paper studies a variant, HPP with Probabilistic Terminals (HPP-PT), where each vertex has a probability representing the likelihood that the robot's path terminates there, and the objective is to minimize the expected path cost. HPP-PT arises in target object search, where a mobile robot must visit all candidate locations to find an object, and prior knowledge of the object's location is expressed as vertex probabilities. While routing problems have been studied for decades, few of them consider uncertainty as required in this work. The challenge lies not only in optimally ordering the vertices, as in standard HPP, but also in handling history dependency: the expected path cost depends on the order in which vertices were previously visited. This makes many existing methods inefficient or inapplicable. To address the challenge, we propose a search-based approach RPT* with solution optimality guarantees, which leverages dynamic programming in a new state space to bypass the history dependency and novel heuristics to speed up the computation. Building on RPT*, we design a Hierarchical Autonomous Target Search (HATS) system that combines RPT* with either Bayesian filtering for lifelong target search with noisy sensors, or autonomous exploration to find targets in unknown environments. Experiments in both simulation and real robot show that our approach can naturally balance between exploitation and exploration, thereby finding targets more quickly on average than baseline methods.

</details>


### [820] [AirHunt: Bridging VLM Semantics and Continuous Planning for Efficient Aerial Object Navigation](https://arxiv.org/abs/2601.12742)
*Xuecheng Chen,Zongzhuo Liu,Jianfa Ma,Bang Du,Tiantian Zhang,Xueqian Wang,Boyu Zhou*

Main category: cs.RO

TL;DR: AirHunt integrates large Vision-Language Models (VLMs) into drones for efficient open-set object navigation using semantic reasoning and continuous path planning.


<details>
  <summary>Details</summary>
Motivation: Current systems face challenges in incorporating VLMs efficiently in aerial navigation, including frequency mismatch with real-time planning, limited 3D scene understanding, and lack of balance between semantic guidance and motion efficiency.

Method: AirHunt features a dual-pathway asynchronous architecture for VLM and path planning integration, an active dual-task reasoning module for selective VLM querying, and a semantic-geometric planning module to balance navigation priorities.

Result: AirHunt outperforms state-of-the-art methods, achieving higher success rates, lower navigation error, and reduced flight times in both simulation and real-world environments.

Conclusion: AirHunt demonstrates practical capability for efficient and adaptive aerial object navigation in diverse and challenging settings by successfully blending semantic and geometric reasoning mechanisms.

Abstract: Recent advances in large Vision-Language Models (VLMs) have provided rich semantic understanding that empowers drones to search for open-set objects via natural language instructions. However, prior systems struggle to integrate VLMs into practical aerial systems due to orders-of-magnitude frequency mismatch between VLM inference and real-time planning, as well as VLMs' limited 3D scene understanding. They also lack a unified mechanism to balance semantic guidance with motion efficiency in large-scale environments. To address these challenges, we present AirHunt, an aerial object navigation system that efficiently locates open-set objects with zero-shot generalization in outdoor environments by seamlessly fusing VLM semantic reasoning with continuous path planning. AirHunt features a dual-pathway asynchronous architecture that establishes a synergistic interface between VLM reasoning and path planning, enabling continuous flight with adaptive semantic guidance that evolves through motion. Moreover, we propose an active dual-task reasoning module that exploits geometric and semantic redundancy to enable selective VLM querying, and a semantic-geometric coherent planning module that dynamically reconciles semantic priorities and motion efficiency in a unified framework, enabling seamless adaptation to environmental heterogeneity. We evaluate AirHunt across diverse object navigation tasks and environments, demonstrating a higher success rate with lower navigation error and reduced flight time compared to state-of-the-art methods. Real-world experiments further validate AirHunt's practical capability in complex and challenging environments. Code and dataset will be made publicly available before publication.

</details>


### [821] [FocusNav: Spatial Selective Attention with Waypoint Guidance for Humanoid Local Navigation](https://arxiv.org/abs/2601.12790)
*Yang Zhang,Jianming Ma,Liyun Yan,Zhanxiang Cao,Yazhou Zhang,Haoyang Li,Yue Gao*

Main category: cs.RO

TL;DR: FocusNav is a framework for humanoid robots that balances long-term navigation goals with real-time motion stability, enhancing navigation success in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Navigating unstructured and dynamic environments is difficult for humanoid robots due to the need to balance long-range navigation with immediate stability.

Method: FocusNav introduces a Waypoint-Guided Spatial Cross-Attention (WGSCA) mechanism and a Stability-Aware Selective Gating (SASG) module to align perception with planned trajectories and ensure stability, especially in complex terrains.

Result: FocusNav significantly outperformed baselines in collision avoidance and stability, achieving high success rates in navigation experiments using the Unitree G1 humanoid robot.

Conclusion: FocusNav provides a robust solution for humanoid robots to navigate dynamic and complex terrains by integrating adaptive attention mechanisms and stability prioritization.

Abstract: Robust local navigation in unstructured and dynamic environments remains a significant challenge for humanoid robots, requiring a delicate balance between long-range navigation targets and immediate motion stability. In this paper, we propose FocusNav, a spatial selective attention framework that adaptively modulates the robot's perceptual field based on navigational intent and real-time stability. FocusNav features a Waypoint-Guided Spatial Cross-Attention (WGSCA) mechanism that anchors environmental feature aggregation to a sequence of predicted collision-free waypoints, ensuring task-relevant perception along the planned trajectory. To enhance robustness in complex terrains, the Stability-Aware Selective Gating (SASG) module autonomously truncates distal information when detecting instability, compelling the policy to prioritize immediate foothold safety. Extensive experiments on the Unitree G1 humanoid robot demonstrate that FocusNav significantly improves navigation success rates in challenging scenarios, outperforming baselines in both collision avoidance and motion stability, achieving robust navigation in dynamic and complex environments.

</details>


### [822] [Contact-Aware Neural Dynamics](https://arxiv.org/abs/2601.12796)
*Changwei Jing,Jai Krishna Bandi,Jianglong Ye,Yan Duan,Pieter Abbeel,Xiaolong Wang,Sha Yi*

Main category: cs.RO

TL;DR: This paper introduces a novel implicit sim-to-real alignment method to enhance robotic learning by leveraging tactile contact information to refine simulated states using real-world observations.


<details>
  <summary>Details</summary>
Motivation: To address the persistent sim-to-real gap in robotic learning, particularly for tasks involving dynamic and complex physical interactions, which explicit system identification struggles to resolve.

Method: An implicit framework is proposed, treating the simulator as a prior and combining a contact-aware neural dynamics model with real-world tactile observations to adapt simulated states.

Result: The learned forward dynamics model improved state prediction accuracy and aided in policy refinement, enabling better alignment between simulated and real-world performance.

Conclusion: The proposed approach provides a scalable and data-driven solution for bridging the sim-to-real gap, particularly for contact-rich robotic tasks.

Abstract: High-fidelity physics simulation is essential for scalable robotic learning, but the sim-to-real gap persists, especially for tasks involving complex, dynamic, and discontinuous interactions like physical contacts. Explicit system identification, which tunes explicit simulator parameters, is often insufficient to align the intricate, high-dimensional, and state-dependent dynamics of the real world. To overcome this, we propose an implicit sim-to-real alignment framework that learns to directly align the simulator's dynamics with contact information. Our method treats the off-the-shelf simulator as a base prior and learns a contact-aware neural dynamics model to refine simulated states using real-world observations. We show that using tactile contact information from robotic hands can effectively model the non-smooth discontinuities inherent in contact-rich tasks, resulting in a neural dynamics model grounded by real-world data. We demonstrate that this learned forward dynamics model improves state prediction accuracy and can be effectively used to predict policy performance and refine policies trained purely in standard simulators, offering a scalable, data-driven approach to sim-to-real alignment.

</details>


### [823] [FRoM-W1: Towards General Humanoid Whole-Body Control with Language Instructions](https://arxiv.org/abs/2601.12799)
*Peng Li,Zihan Zhuang,Yangfan Gao,Yi Dong,Sixian Li,Changhao Jiang,Shihan Dou,Zhiheng Xi,Enyu Zhou,Jixuan Huang,Hui Li,Jingjing Gong,Xingjun Ma,Tao Gui,Zuxuan Wu,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang,Xipeng Qiu*

Main category: cs.RO

TL;DR: This paper presents FRoM-W1, an open-source framework enabling humanoid robots to perform diverse natural body motions based on natural language instructions.


<details>
  <summary>Details</summary>
Motivation: Humanoid robots are currently limited by hard-coded or specifically trained motions, restricting their versatility.

Method: FRoM-W1 operates in two stages: H-GPT for human motion generation using natural language, and H-ACT for robot-specific motion execution via reinforcement learning.

Result: Evaluations on Unitree H1 and G1 robots show superior performance in motion generation and improved accuracy and success rates after reinforcement learning fine-tuning.

Conclusion: FRoM-W1 advances humanoid intelligence, providing a universal framework for general motion control via natural language and is open-sourced for community development.

Abstract: Humanoid robots are capable of performing various actions such as greeting, dancing and even backflipping. However, these motions are often hard-coded or specifically trained, which limits their versatility. In this work, we present FRoM-W1, an open-source framework designed to achieve general humanoid whole-body motion control using natural language. To universally understand natural language and generate corresponding motions, as well as enable various humanoid robots to stably execute these motions in the physical world under gravity, FRoM-W1 operates in two stages: (a) H-GPT: utilizing massive human data, a large-scale language-driven human whole-body motion generation model is trained to generate diverse natural behaviors. We further leverage the Chain-of-Thought technique to improve the model's generalization in instruction understanding. (b) H-ACT: After retargeting generated human whole-body motions into robot-specific actions, a motion controller that is pretrained and further fine-tuned through reinforcement learning in physical simulation enables humanoid robots to accurately and stably perform corresponding actions. It is then deployed on real robots via a modular simulation-to-reality module. We extensively evaluate FRoM-W1 on Unitree H1 and G1 robots. Results demonstrate superior performance on the HumanML3D-X benchmark for human whole-body motion generation, and our introduced reinforcement learning fine-tuning consistently improves both motion tracking accuracy and task success rates of these humanoid robots. We open-source the entire FRoM-W1 framework and hope it will advance the development of humanoid intelligence.

</details>


### [824] [Sparse ActionGen: Accelerating Diffusion Policy with Real-time Pruning](https://arxiv.org/abs/2601.12894)
*Kangye Ji,Yuan Meng,Zhou Jianbo,Ye Li,Hanyun Cui,Zhi Wang*

Main category: cs.RO

TL;DR: The paper introduces Sparse ActionGen (SAG), a method to accelerate diffusion policy algorithms for real-time robotic control by using a rollout-adaptive prune-then-reuse mechanism.


<details>
  <summary>Details</summary>
Motivation: Diffusion policies are excellent at generating multi-modal action distributions, but their computational inefficiency limits their practicality for real-time robot control.

Method: SAG uses a rollout-adaptive prune-then-reuse mechanism, combining observation-conditioned diffusion pruner with efficient activation reuse strategies to minimize redundancy in computations.

Result: SAG achieves up to 4× faster action generation while maintaining performance consistency across multiple robotic benchmarks.

Conclusion: By optimizing the computation process during diffusion-based action generation, SAG enables real-time visuomotor control with enhanced efficiency.

Abstract: Diffusion Policy has dominated action generation due to its strong capabilities for modeling multi-modal action distributions, but its multi-step denoising processes make it impractical for real-time visuomotor control. Existing caching-based acceleration methods typically rely on $\textit{static}$ schedules that fail to adapt to the $\textit{dynamics}$ of robot-environment interactions, thereby leading to suboptimal performance. In this paper, we propose $\underline{\textbf{S}}$parse $\underline{\textbf{A}}$ction$\underline{\textbf{G}}$en ($\textbf{SAG}$) for extremely sparse action generation. To accommodate the iterative interactions, SAG customizes a rollout-adaptive prune-then-reuse mechanism that first identifies prunable computations globally and then reuses cached activations to substitute them during action diffusion. To capture the rollout dynamics, SAG parameterizes an observation-conditioned diffusion pruner for environment-aware adaptation and instantiates it with a highly parameter- and inference-efficient design for real-time prediction. Furthermore, SAG introduces a one-for-all reusing strategy that reuses activations across both timesteps and blocks in a zig-zag manner, minimizing the global redundancy. Extensive experiments on multiple robotic benchmarks demonstrate that SAG achieves up to 4$\times$ generation speedup without sacrificing performance. Project Page: https://sparse-actiongen.github.io/.

</details>


### [825] [PlannerRFT: Reinforcing Diffusion Planners through Closed-Loop and Sample-Efficient Fine-Tuning](https://arxiv.org/abs/2601.12901)
*Hongchen Li,Tianyu Li,Jiazhi Yang,Haochen Tian,Caojun Wang,Lei Shi,Mingyang Shang,Zengrong Lin,Gaoqiang Wu,Zhihui Hao,Xianpeng Lang,Jia Hu,Hongyang Li*

Main category: cs.RO

TL;DR: PlannerRFT enhances diffusion-based planning in autonomous driving by efficiently incorporating reinforcement fine-tuning with improved trajectory adaptability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Diffusion planners struggle with generating high-quality, adaptive trajectories during reinforcement fine-tuning due to inefficiencies in multi-modal reward exploitation.

Method: PlannerRFT introduces a dual-branch optimization framework for trajectory distribution refinement and adaptive denoising, alongside the development of nuMax for faster simulation.

Result: PlannerRFT achieves significant improvements in trajectory generation, supported by extensive experiments showcasing state-of-the-art performance and emergent distinct behaviors.

Conclusion: The proposed framework enables more efficient reinforcement fine-tuning, effectively enhancing multi-modal adaptability and exploration efficiency in diffusion planners.

Abstract: Diffusion-based planners have emerged as a promising approach for human-like trajectory generation in autonomous driving. Recent works incorporate reinforcement fine-tuning to enhance the robustness of diffusion planners through reward-oriented optimization in a generation-evaluation loop. However, they struggle to generate multi-modal, scenario-adaptive trajectories, hindering the exploitation efficiency of informative rewards during fine-tuning. To resolve this, we propose PlannerRFT, a sample-efficient reinforcement fine-tuning framework for diffusion-based planners. PlannerRFT adopts a dual-branch optimization that simultaneously refines the trajectory distribution and adaptively guides the denoising process toward more promising exploration, without altering the original inference pipeline. To support parallel learning at scale, we develop nuMax, an optimized simulator that achieves 10 times faster rollout compared to native nuPlan. Extensive experiments shows that PlannerRFT yields state-of-the-art performance with distinct behaviors emerging during the learning process.

</details>


### [826] [Dynamic Hand Gesture Recognition for Robot Manipulator Tasks](https://arxiv.org/abs/2601.12918)
*Dharmendra Sharma,Peeyush Thakur,Sandeep Gupta,Narendra Kumar Dhar,Laxmidhar Behera*

Main category: cs.RO

TL;DR: A novel approach using an unsupervised model via Gaussian Mixture is proposed for recognizing dynamic hand gestures for seamless human-robot interaction.


<details>
  <summary>Details</summary>
Motivation: Enable real-time recognition of dynamic hand gestures to facilitate human-robot interaction.

Method: An unsupervised model based on Gaussian Mixture is used to accurately recognize dynamic gesture variations in real-time.

Result: The proposed model achieved high accuracy during training and real-time testing.

Conclusion: This methodology proves effective in accurately recognizing various dynamic gestures for robotic tasks.

Abstract: This paper proposes a novel approach to recognizing dynamic hand gestures facilitating seamless interaction between humans and robots. Here, each robot manipulator task is assigned a specific gesture. There may be several such tasks, hence, several gestures. These gestures may be prone to several dynamic variations. All such variations for different gestures shown to the robot are accurately recognized in real-time using the proposed unsupervised model based on the Gaussian Mixture model. The accuracy during training and real-time testing prove the efficacy of this methodology.

</details>


### [827] [ForeDiffusion: Foresight-Conditioned Diffusion Policy via Future View Construction for Robot Manipulation](https://arxiv.org/abs/2601.12925)
*Weize Xie,Yi Ding,Ying He,Leilei Wang,Binwen Bai,Zheyi Zhao,Chenyang Wang,F. Richard Yu*

Main category: cs.RO

TL;DR: This paper introduces ForeDiffusion, a method enhancing visual motor control in robots by addressing limitations in current diffusion strategies.


<details>
  <summary>Details</summary>
Motivation: To improve the success rates and stability of robot manipulation in complex tasks by overcoming the limitations of current diffusion strategies.

Method: The paper introduces ForeDiffusion, which incorporates foresight-conditioned diffusion through future view representation and a dual loss mechanism combining denoising and consistency loss.

Result: ForeDiffusion achieves an 80% average success rate across tasks, outperforming baseline methods by 23% in complex tasks and providing more consistent performance.

Conclusion: ForeDiffusion successfully improves both accuracy and stability in robot manipulation tasks, overcoming challenges in traditional diffusion models.

Abstract: Diffusion strategies have advanced visual motor control by progressively denoising high-dimensional action sequences, providing a promising method for robot manipulation. However, as task complexity increases, the success rate of existing baseline models decreases considerably. Analysis indicates that current diffusion strategies are confronted with two limitations. First, these strategies only rely on short-term observations as conditions. Second, the training objective remains limited to a single denoising loss, which leads to error accumulation and causes grasping deviations. To address these limitations, this paper proposes Foresight-Conditioned Diffusion (ForeDiffusion), by injecting the predicted future view representation into the diffusion process. As a result, the policy is guided to be forward-looking, enabling it to correct trajectory deviations. Following this design, ForeDiffusion employs a dual loss mechanism, combining the traditional denoising loss and the consistency loss of future observations, to achieve the unified optimization. Extensive evaluation on the Adroit suite and the MetaWorld benchmark demonstrates that ForeDiffusion achieves an average success rate of 80% for the overall task, significantly outperforming the existing mainstream diffusion methods by 23% in complex tasks, while maintaining more stable performance across the entire tasks.

</details>


### [828] [Active Inference-Driven World Modeling for Adaptive UAV Swarm Trajectory Design](https://arxiv.org/abs/2601.12939)
*Kaleem Arshid,Ali Krayani,Lucio Marcenaro,David Martin Gomez,Carlo Regazzoni*

Main category: cs.RO

TL;DR: The paper introduces a framework using Active Inference for UAV swarm trajectory planning, emphasizing distributed decision-making and adaptability in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To address adaptive and efficient autonomous trajectory design for UAV swarms in dynamic and complex environments.

Method: It combines probabilistic reasoning, self-learning, and expert Genetic Algorithm-based trajectories to train a hierarchical World Model for swarm behavior, enabling action inference via Active Inference during operation.

Result: The proposed method outperformed Q-Learning in simulations with faster convergence, higher stability, and safer navigation.

Conclusion: The framework proves scalable and cognitively grounded, allowing intelligent and adaptive UAV swarm control.

Abstract: This paper proposes an Active Inference-based framework for autonomous trajectory design in UAV swarms. The method integrates probabilistic reasoning and self-learning to enable distributed mission allocation, route ordering, and motion planning. Expert trajectories generated using a Genetic Algorithm with Repulsion Forces (GA-RF) are employed to train a hierarchical World Model capturing swarm behavior across mission, route, and motion levels. During online operation, UAVs infer actions by minimizing divergence between current beliefs and model-predicted states, enabling adaptive responses to dynamic environments. Simulation results show faster convergence, higher stability, and safer navigation than Q-Learning, demonstrating the scalability and cognitive grounding of the proposed framework for intelligent UAV swarm control.

</details>


### [829] [Imitation learning-based spacecraft rendezvous and docking method with Expert Demonstration](https://arxiv.org/abs/2601.12952)
*Shibo Shao,Dong Zhou,Guanghui Sun,Liwen Zhang,Mingxuan Jiang*

Main category: cs.RO

TL;DR: The paper presents an Imitation Learning-based framework for spacecraft rendezvous and docking, focusing on robustness and model-free control.


<details>
  <summary>Details</summary>
Motivation: Current spacecraft rendezvous and docking methods depend heavily on predefined models, making them less robust in real-world environments. The authors aim to reduce reliance on precise modeling using an imitation learning approach.

Method: An Imitation Learning-based control framework (IL-SRD) is proposed, featuring an anchored decoder target mechanism for physical consistency and a temporal aggregation mechanism to handle error accumulation.

Result: Simulation results show accurate, energy-efficient control with competitive performance under disturbances.

Conclusion: The IL-SRD framework effectively provides a robust, model-free solution for spacecraft rendezvous and docking, with publicly available code for further research.

Abstract: Existing spacecraft rendezvous and docking control methods largely rely on predefined dynamic models and often exhibit limited robustness in realistic on-orbit environments. To address this issue, this paper proposes an Imitation Learning-based spacecraft rendezvous and docking control framework (IL-SRD) that directly learns control policies from expert demonstrations, thereby reducing dependence on accurate modeling. We propose an anchored decoder target mechanism, which conditions the decoder queries on state-related anchors to explicitly constrain the control generation process. This mechanism enforces physically consistent control evolution and effectively suppresses implausible action deviations in sequential prediction, enabling reliable six-degree-of-freedom (6-DOF) rendezvous and docking control. To further enhance stability, a temporal aggregation mechanism is incorporated to mitigate error accumulation caused by the sequential prediction nature of Transformer-based models, where small inaccuracies at each time step can propagate and amplify over long horizons. Extensive simulation results demonstrate that the proposed IL-SRD framework achieves accurate and energy-efficient model-free rendezvous and docking control. Robustness evaluations further confirm its capability to maintain competitive performance under significant unknown disturbances. The source code is available at https://github.com/Dongzhou-1996/IL-SRD.

</details>


### [830] [Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization](https://arxiv.org/abs/2601.12993)
*Hao Luo,Ye Wang,Wanpeng Zhang,Sipeng Zheng,Ziheng Xi,Chaoyi Xu,Haiweng Xu,Haoqi Yuan,Chi Zhang,Yiqing Wang,Yicheng Feng,Zongqing Lu*

Main category: cs.RO

TL;DR: This paper introduces Being-H0.5, a Vision-Language-Action model excelling in cross-embodiment generalization for robotic platforms, using human-centric learning and a large-scale dataset.


<details>
  <summary>Details</summary>
Motivation: To address challenges in cross-embodiment generalization due to robotic platform diversity and limited data by leveraging human interaction as a universal standard.

Method: The authors use a large-scale dataset (UniHand-2.0) and propose Unified Action Spaces, a Mixture-of-Transformers architecture (including specialized Mixture-of-Flow), and techniques like Manifold-Preserving Gating for effective cross-platform skill learning.

Result: Being-H0.5 secures state-of-the-art performance on key benchmarks (such as LIBERO 98.9% and RoboCasa 53.9%) and demonstrates robust cross-embodiment execution on diverse robots.

Conclusion: Being-H0.5 significantly advances the generalizability of robotic learning by bridging human-centric data with a unified model to handle morphological and sensory variation.

Abstract: We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal "mother tongue" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.

</details>


### [831] [Static Is Not Enough: A Comparative Study of VR and SpaceMouse in Static and Dynamic Teleoperation Tasks](https://arxiv.org/abs/2601.13042)
*Yijun Zhou,Muhan Hou,Kim Baraka*

Main category: cs.RO

TL;DR: The paper compares VR controllers and SpaceMouse interfaces for dynamic and static teleoperation tasks, finding VR interfaces more effective. The authors release an open-source VR interface for dynamic tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding about the differences in performance between teleoperation interfaces for dynamic and static tasks and provide a suitable VR-based solution for dynamic tasks.

Method: A within-subject study was conducted with 25 participants comparing a VR controller and a SpaceMouse across two static and two dynamic teleoperation tasks. Metrics like success rate, task duration, and subjective feedback (NASA-TLX, SUS) were assessed.

Result: VR controllers showed statistically significant benefits, including higher success rates for dynamic tasks, faster execution times, and lower workload, with better usability compared to SpaceMouse.

Conclusion: VR controllers are better suited for dynamic teleoperation tasks compared to SpaceMouse. The authors release an open-source VR interface to address limitations in existing systems.

Abstract: Imitation learning relies on high-quality demonstrations, and teleoperation is a primary way to collect them, making teleoperation interface choice crucial for the data. Prior work mainly focused on static tasks, i.e., discrete, segmented motions, yet demonstrations also include dynamic tasks requiring reactive control. As dynamic tasks impose fundamentally different interface demands, insights from static-task evaluations cannot generalize. To address this gap, we conduct a within-subjects study comparing a VR controller and a SpaceMouse across two static and two dynamic tasks ($N=25$). We assess success rate, task duration, cumulative success, alongside NASA-TLX, SUS, and open-ended feedback. Results show statistically significant advantages for VR: higher success rates, particularly on dynamic tasks, shorter successful execution times across tasks, and earlier successes across attempts, with significantly lower workload and higher usability. As existing VR teleoperation systems are rarely open-source or suited for dynamic tasks, we release our VR interface to fill this gap.

</details>


### [832] [Exploiting Light To Enhance The Endurance and Navigation of Lighter-Than-Air Micro-Drones](https://arxiv.org/abs/2601.13088)
*Harry Huang,Talia Xu,Marco Zúñiga Zamalloa*

Main category: cs.RO

TL;DR: This paper introduces a self-sustaining Lighter-Than-Air (LTA) drone utilizing helium and integrated solar energy for extended, autonomous UAV operations in indoor and outdoor settings.


<details>
  <summary>Details</summary>
Motivation: Micro-UAVs face limitations in endurance and GPS-denied navigation, whereas LTA drones offer energy-efficient hovering solutions. However, designing such systems is complex and lacks simple infrastructure integration for autonomous functioning.

Method: The method combines: (i) designing an aerodynamic LTA configuration via high-fidelity simulations, (ii) integrating solar cells for net-positive energy harvesting, and (iii) enabling autonomous point-and-go navigation using light-seeking algorithms and a single light beacon.

Result: The LTA drone achieves sustained operation by harvesting 4 minutes of energy for 1 minute of flight under 80klux illumination. It demonstrates reliable navigation up to 7m from a light source in various environments, including moderate winds.

Conclusion: This study presents a practical framework for sustainable LTA drone operations, paving the way for self-sustaining, autonomous aerial systems applicable to indoor and outdoor monitoring.

Abstract: Micro-Unmanned Aerial Vehicles (UAVs) are rapidly expanding into tasks from inventory to environmental sensing, yet their short endurance and unreliable navigation in GPS-denied spaces limit deployment. Lighter-Than-Air (LTA) drones offer an energy-efficient alternative: they use a helium envelope to provide buoyancy, which enables near-zero-power drain during hovering and much longer operation. LTAs are promising, but their design is complex, and they lack integrated solutions to enable sustained autonomous operations and navigation with simple, low-infrastructure.
  We propose a compact, self-sustaining LTA drone that uses light for both energy harvesting and navigation. Our contributions are threefold: (i) a high-fidelity simulation framework to analyze LTA aerodynamics and select a stable, efficient configuration; (ii) a framework to integrate solar cells on the envelope to provide net-positive energy; and (iii) a point-and-go navigation system with three light-seeking algorithms operating on a single light beacon.
  Our LTA-analysis, together with the integrated solar panels, not only saves energy while flying, but also enables sustainable operation: providing 1 minute of flying time for every 4 minutes of energy harvesting, under illuminations of 80klux. We also demonstrate robust single-beacon navigation towards a light source that can be up to 7m away, in indoor and outdoor environments, even with moderate winds. The resulting system indicates a plausible path toward persistent, autonomous operation for indoor and outdoor monitoring. More broadly, this work provides a practical pathway for translating the promise of LTA drones into a persistent, self-sustaining aerial system.

</details>


### [833] [LLM-VLM Fusion Framework for Autonomous Maritime Port Inspection using a Heterogeneous UAV-USV System](https://arxiv.org/abs/2601.13096)
*Muhayy Ud Din,Waseem Akram,Ahsan B. Bakht,Irfan Hussain*

Main category: cs.RO

TL;DR: This study develops an innovative framework utilizing Large Language Models (LLMs) and Vision Language Models (VLMs) for autonomous maritime port inspection through robotic systems.


<details>
  <summary>Details</summary>
Motivation: To address limitations of manual and traditional computer vision-based port inspection methods by introducing scalable and context-aware systems.

Method: A framework combining LLM for mission planning and VLM for semantic inspection using UAV and USV platforms, validated via simulation and real-world trials.

Result: The system demonstrated effective context-aware port inspection with real-time semantic analysis and lightweight, resource-efficient operation.

Conclusion: This framework establishes intelligent, autonomous inspection systems for maritime environments, with open-source resources facilitating further research.

Abstract: Maritime port inspection plays a critical role in ensuring safety, regulatory compliance, and operational efficiency in complex maritime environments. However, existing inspection methods often rely on manual operations and conventional computer vision techniques that lack scalability and contextual understanding. This study introduces a novel integrated engineering framework that utilizes the synergy between Large Language Models (LLMs) and Vision Language Models (VLMs) to enable autonomous maritime port inspection using cooperative aerial and surface robotic platforms. The proposed framework replaces traditional state-machine mission planners with LLM-driven symbolic planning and improved perception pipelines through VLM-based semantic inspection, enabling context-aware and adaptive monitoring. The LLM module translates natural language mission instructions into executable symbolic plans with dependency graphs that encode operational constraints and ensure safe UAV-USV coordination. Meanwhile, the VLM module performs real-time semantic inspection and compliance assessment, generating structured reports with contextual reasoning. The framework was validated using the extended MBZIRC Maritime Simulator with realistic port infrastructure and further assessed through real-world robotic inspection trials. The lightweight on-board design ensures suitability for resource-constrained maritime platforms, advancing the development of intelligent, autonomous inspection systems. Project resources (code and videos) can be found here: https://github.com/Muhayyuddin/llm-vlm-fusion-port-inspection

</details>


### [834] [Helical Tendon-Driven Continuum Robot with Programmable Follow-the-Leader Operation](https://arxiv.org/abs/2601.13177)
*Behnam Moradkhani,Raghav Sankaranarayanan,Pejman Kheradmand,Harshith Jella,Nicholas Ahn,Ajmal Zemmar,Yash Chitalia*

Main category: cs.RO

TL;DR: This paper introduces ExoNav, a robotic tool, for precise spinal cord stimulation placement, achieving accurate navigation and effective motor neuron stimulation.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in precisely placing spinal cord stimulation leads in the ventral or lateral epidural space, which are critical for motor neuron stimulation and recovery.

Method: A static modeling approach using the Cosserat rod framework was developed for ExoNav, enabling precise robot actuation and navigation while accounting for gravity effects.

Result: Experimental evaluations showed low RMSE values across prototypes and demonstrated ExoNav's ability to achieve follow-the-leader (FTL) motion. Simulations and experiments validated its precise navigation capabilities.

Conclusion: ExoNav offers promising potential for improving spinal cord stimulation techniques, with applications in motor function recovery and pain management.

Abstract: Spinal cord stimulation (SCS) is primarily utilized for pain management and has recently demonstrated efficacy in promoting functional recovery in patients with spinal cord injury. Effective stimulation of motor neurons ideally requires the placement of SCS leads in the ventral or lateral epidural space where the corticospinal and rubrospinal motor fibers are located. This poses significant challenges with the current standard of manual steering. In this study, we present a static modeling approach for the ExoNav, a steerable robotic tool designed to facilitate precise navigation to the ventral and lateral epidural space. Cosserat rod framework is employed to establish the relationship between tendon actuation forces and the robot's overall shape. The effects of gravity, as an example of an external load, are investigated and implemented in the model and simulation. The experimental results indicate RMSE values of 1.76mm, 2.33mm, 2.18mm, and 1.33mm across four tested prototypes. Based on the helical shape of the ExoNav upon actuation, it is capable of performing follow-the-leader (FTL) motion by adding insertion and rotation DoFs to this robotic system, which is shown in simulation and experimentally. The proposed simulation has the capability to calculate optimum tendon tensions to follow the desired FTL paths while gravity-induced robot deformations are present. Three FTL experimental trials are conducted and the end-effector position showed repeatable alignments with the desired path with maximum RMSE value of 3.75mm. Ultimately, a phantom model demonstration is conducted where the teleoperated robot successfully navigated to the lateral and ventral spinal cord targets. Additionally, the user was able to navigate to the dorsal root ganglia, illustrating ExoNav's potential in both motor function recovery and pain management.

</details>


### [835] [Active Informative Planning for UAV-based Weed Mapping using Discrete Gaussian Process Representations](https://arxiv.org/abs/2601.13196)
*Jacob Swindell,Marija Popović,Riccardo Polvara*

Main category: cs.RO

TL;DR: This paper investigates the influence of discretisation strategies on UAV-based weed mapping using informative path planning (IPP) and Gaussian processes (GP), highlighting key impacts on mapping quality and mission performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance precision in UAV-based agricultural weed mapping by addressing the underexplored effects of GP discretisation choice on mapping quality and mission efficiency.

Method: A receding-horizon IPP strategy is implemented for UAVs using multiple discretisation strategies for GP mapping. UAVs adaptively choose sampling locations based on map uncertainty, travel cost, and coverage penalties. Real-world weed distributions are utilized for validation.

Result: The choice of GP discretisation strongly impacts exploration behaviour, coverage efficiency, and computational performance in online UAV weed mapping.

Conclusion: Discretisation strategies play a crucial role in shaping both UAV planning dynamics and computational efficiency, making them key design considerations for effective agricultural weed mapping.

Abstract: Accurate agricultural weed mapping using unmanned aerial vehicles (UAVs) is crucial for precision farming. While traditional methods rely on rigid, pre-defined flight paths and intensive offline processing, informative path planning (IPP) offers a way to collect data adaptively where it is most needed. Gaussian process (GP) mapping provides a continuous model of weed distribution with built-in uncertainty. However, GPs must be discretised for practical use in autonomous planning. Many discretisation techniques exist, but the impact of discrete representation choice remains poorly understood. This paper investigates how different discrete GP representations influence both mapping quality and mission-level performance in UAV-based weed mapping. Considering a UAV equipped with a downward-facing camera, we implement a receding-horizon IPP strategy that selects sampling locations based on the map uncertainty, travel cost, and coverage penalties. We investigate multiple discretisation strategies for representing the GP posterior and use their induced map partitions to generate candidate viewpoints for planning. Experiments on real-world weed distributions show that representation choice significantly affects exploration behaviour and efficiency. Overall, our results demonstrate that discretisation is not only a representational detail but a key design choice that shapes planning dynamics, coverage efficiency, and computational load in online UAV weed mapping.

</details>


### [836] [MATTERIX: toward a digital twin for robotics-assisted chemistry laboratory automation](https://arxiv.org/abs/2601.13232)
*Kourosh Darvish,Arjun Sohal,Abhijoy Mandal,Hatem Fakhruldeen,Nikola Radulov,Zhengxue Zhou,Satheeshkumar Veeramani,Joshua Choi,Sijie Han,Brayden Zhang,Jeeyeoun Chae,Alex Wright,Yijie Wang,Hossein Darvish,Yuchi Zhao,Gary Tom,Han Hao,Miroslav Bogdanovic,Gabriella Pizzuto,Andrew I. Cooper,Alán Aspuru-Guzik,Florian Shkurti,Animesh Garg*

Main category: cs.RO

TL;DR: MATTERIX is a GPU-accelerated simulation framework that creates digital twins of chemistry labs to accelerate workflow development and reduce dependence on physical experiments.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the scalability issue in materials discovery by reducing the reliance on physical make-and-test iterations in laboratories.

Method: MATTERIX integrates GPU-accelerated physics simulation, realistic rendering, modular semantics engines, asset libraries, and hierarchical workflows to create digital twins that simulate lab environments and processes.

Result: MATTERIX demonstrated successful sim-to-real transfer in robotic chemistry setups, providing an effective platform for hypothetical automated workflow testing without expensive physical trials.

Conclusion: MATTERIX helps streamline experimental workflows and boosts efficiency in materials discovery, marking a significant contribution to the automation of laboratory setups.

Abstract: Accelerated materials discovery is critical for addressing global challenges. However, developing new laboratory workflows relies heavily on real-world experimental trials, and this can hinder scalability because of the need for numerous physical make-and-test iterations. Here we present MATTERIX, a multiscale, graphics processing unit-accelerated robotic simulation framework designed to create high-fidelity digital twins of chemistry laboratories, thus accelerating workflow development. This multiscale digital twin simulates robotic physical manipulation, powder and liquid dynamics, device functionalities, heat transfer and basic chemical reaction kinetics. This is enabled by integrating realistic physics simulation and photorealistic rendering with a modular graphics processing unit-accelerated semantics engine, which models logical states and continuous behaviors to simulate chemistry workflows across different levels of abstraction. MATTERIX streamlines the creation of digital twin environments through open-source asset libraries and interfaces, while enabling flexible workflow design via hierarchical plan definition and a modular skill library that incorporates learning-based methods. Our approach demonstrates sim-to-real transfer in robotic chemistry setups, reducing reliance on costly real-world experiments and enabling the testing of hypothetical automated workflows in silico. The project website is available at https://accelerationconsortium.github.io/Matterix/ .

</details>


### [837] [Diffusion-based Inverse Model of a Distributed Tactile Sensor for Object Pose Estimation](https://arxiv.org/abs/2601.13250)
*Ante Marić,Giammarco Caroleo,Alessandro Albini,Julius Jankowski,Perla Maiolino,Sylvain Calinon*

Main category: cs.RO

TL;DR: The paper introduces a novel approach using denoising diffusion for tactile sensor-based object pose estimation, tailored for situations where visual data is limited or absent.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to overcome challenges in utilizing tactile data for pose estimation, especially under partial observability and occluded environments where visual sensing is inadequate.

Method: The approach involves training an inverse tactile sensor model using denoising diffusion, conditioned on tactile observations simulated via a geometric sensor model based on signed distance fields. Contact constraints are enforced during inference. Particle filtering is leveraged for online pose estimation, integrating generated hypotheses with prior beliefs.

Result: The proposed method was tested in simulated and real-world environments, showing improved efficiency, estimation accuracy, and robustness compared to traditional local sampling baselines. Multimodal beliefs were preserved across different objects.

Conclusion: This technique advances tactile-based pose estimation by overcoming conventional limitations and demonstrating its efficacy in non-visual, challenging scenarios such as box-pushing tasks.

Abstract: Tactile sensing provides a promising sensing modality for object pose estimation in manipulation settings where visual information is limited due to occlusion or environmental effects. However, efficiently leveraging tactile data for estimation remains a challenge due to partial observability, with single observations corresponding to multiple possible contact configurations. This limits conventional estimation approaches largely tailored to vision. We propose to address these challenges by learning an inverse tactile sensor model using denoising diffusion. The model is conditioned on tactile observations from a distributed tactile sensor and trained in simulation using a geometric sensor model based on signed distance fields. Contact constraints are enforced during inference through single-step projection using distance and gradient information from the signed distance field. For online pose estimation, we integrate the inverse model with a particle filter through a proposal scheme that combines generated hypotheses with particles from the prior belief. Our approach is validated in simulated and real-world planar pose estimation settings, without access to visual data or tight initial pose priors. We further evaluate robustness to unmodeled contact and sensor dynamics for pose tracking in a box-pushing scenario. Compared to local sampling baselines, the inverse sensor model improves sampling efficiency and estimation accuracy while preserving multimodal beliefs across objects with varying tactile discriminability.

</details>


### [838] [Autonomous Navigation at the Nano-Scale: Algorithms, Architectures, and Constraints](https://arxiv.org/abs/2601.13252)
*Mahmud S. Zango,Jianglin Lan*

Main category: cs.RO

TL;DR: This paper reviews current advancements in autonomous navigation for nano-scale UAVs under extreme constraints and highlights gaps in endurance, avoidance, and Sim-to-Real transfer.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in enabling autonomous, agile navigation for nano-scale UAVs under strict Size, Weight, and Power constraints.

Method: Reviewed edge AI paradigms, co-design of hardware-software, and advancements in sensing and control architectures, with a critical analysis of gaps.

Result: Identified progress in visual navigation and pose estimation, but noted gaps in endurance, obstacle avoidance, and Sim-to-Real reinforcement learning.

Conclusion: Proposes a roadmap for fusing classical control with data-driven perception to achieve fully autonomous nano-UAVs in GPS-denied environments.

Abstract: Autonomous navigation for nano-scale unmanned aerial vehicles (nano-UAVs) is governed by extreme Size, Weight, and Power (SWaP) constraints (with the weight < 50 g and sub-100 mW onboard processor), distinguishing it fundamentally from standard robotic paradigms. This review synthesizes the state-of-the-art in sensing, computing, and control architectures designed specifically for these sub- 100mW computational envelopes. We critically analyse the transition from classical geometry-based methods to emerging "Edge AI" paradigms, including quantized deep neural networks deployed on ultra-low-power System-on-Chips (SoCs) and neuromorphic event-based control. Beyond algorithms, we evaluate the hardware-software co-design requisite for autonomy, covering advancements in dense optical flow, optimized Simultaneous Localization and Mapping (SLAM), and learning-based flight control. While significant progress has been observed in visual navigation and relative pose estimation, our analysis reveals persistent gaps in long-term endurance, robust obstacle avoidance in dynamic environments, and the "Sim-to-Real" transfer of reinforcement learning policies. This survey provides a roadmap for bridging these gaps, advocating for hybrid architectures that fuse lightweight classical control with data-driven perception to enable fully autonomous, agile nano-UAVs in GPS-denied environments.

</details>


### [839] [CLEAR: A Semantic-Geometric Terrain Abstraction for Large-Scale Unstructured Environments](https://arxiv.org/abs/2601.13361)
*Pranay Meshram,Charuvahan Adhivarahan,Ehsan Tarkesh Esfahani,Souma Chowdhury,Chen Wang,Karthik Dantu*

Main category: cs.RO

TL;DR: The paper introduces CLEAR, a terrain abstraction method for long-range navigation in unstructured environments, which improves planning speed and reliability.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to balance scalability, semantic alignment, and geometric accuracy for terrain abstraction in large areas, leading to unreliable navigation paths in real-world scenarios.

Method: CLEAR employs boundary-aware spatial decomposition and recursive plane fitting to create semantically aligned convex regions represented as a terrain graph.

Result: CLEAR improves planning speed by up to 10x with minimal cost overhead and achieves shorter, more reliable paths than baseline methods through experiments on maps spanning 9-100 km$^2$.

Conclusion: CLEAR demonstrates high utility for scalable and reliable long-range navigation in challenging applications such as disaster response and planetary exploration.

Abstract: Long-horizon navigation in unstructured environments demands terrain abstractions that scale to tens of km$^2$ while preserving semantic and geometric structure, a combination existing methods fail to achieve. Grids scale poorly; quadtrees misalign with terrain boundaries; neither encodes landcover semantics essential for traversability-aware planning. This yields infeasible or unreliable paths for autonomous ground vehicles operating over 10+ km$^2$ under real-time constraints. CLEAR (Connected Landcover Elevation Abstract Representation) couples boundary-aware spatial decomposition with recursive plane fitting to produce convex, semantically aligned regions encoded as a terrain-aware graph. Evaluated on maps spanning 9-100~km$^2$ using a physics-based simulator, CLEAR achieves up to 10x faster planning than raw grids with only 6.7% cost overhead and delivers 6-9% shorter, more reliable paths than other abstraction baselines. These results highlight CLEAR's scalability and utility for long-range navigation in applications such as disaster response, defense, and planetary exploration.

</details>


### [840] [Robustness and Resilience Evaluation of Eco-Driving Strategies at Signalized Intersections](https://arxiv.org/abs/2601.13389)
*Zhaohui Liang,Chengyuan Ma,Keke Long,Xiaopeng Li*

Main category: cs.RO

TL;DR: This study evaluates eco-driving strategies using robustness and resilience criteria, applying these measures to different controllers through real-world tests.


<details>
  <summary>Details</summary>
Motivation: Eco-driving strategies have shown promise in improving energy efficiency and reducing emissions, but existing evaluations often simplify simulation conditions, missing critical real-world factors.

Method: A unified framework was created to evaluate eco-driving controllers using formal indicators of control robustness and environmental resilience, tested through real-world vehicle experiments.

Result: Optimization-based controllers demonstrated consistent performance across disturbances, whereas analytical controllers were more sensitive to execution and timing variability.

Conclusion: The study identifies trade-offs between tracking accuracy and adaptability, emphasizing the advantages of optimization-based over analytical controllers in diverse real-world environments.

Abstract: Eco-driving strategies have demonstrated substantial potential for improving energy efficiency and reducing emissions, especially at signalized intersections. However, evaluations of eco-driving methods typically rely on simplified simulation or experimental conditions, where certain assumptions are made to manage complexity and experimental control. This study introduces a unified framework to evaluate eco-driving strategies through the lens of two complementary criteria: control robustness and environmental resilience. We define formal indicators that quantify performance degradation caused by internal execution variability and external environmental disturbances, respectively. These indicators are then applied to assess multiple eco-driving controllers through real-world vehicle experiments. The results reveal key tradeoffs between tracking accuracy and adaptability, showing that optimization-based controllers offer more consistent performance across varying disturbance levels, while analytical controllers may perform comparably under nominal conditions but exhibit greater sensitivity to execution and timing variability.

</details>


### [841] [Event-based Heterogeneous Information Processing for Online Vision-based Obstacle Detection and Localization](https://arxiv.org/abs/2601.13451)
*Reza Ahmadvand,Sarah Safura Sharif,Yaser Mike Banad*

Main category: cs.RO

TL;DR: This paper proposes a novel robotic navigation system combining Hybrid Neural Networks and Spiking Neural Network filtering to enhance obstacle detection and localization with high accuracy and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of navigation in unpredictable and dynamic environments by improving obstacle detection, localization, and processing efficiency for robotic systems.

Method: The method integrates ANN for static spatial analysis and SNN for real-time dynamic sensory processing with a pre-developed SNN-based filter for direct spike-encoded input utilization.

Result: Simulation results show that the system achieves acceptable detection accuracy and computational efficiency nearly equivalent to SNN-only systems at lower resource costs.

Conclusion: The framework enables robots to navigate effectively in dynamic environments with improved energy-efficient processing, paving the way for advanced neuromorphic navigation systems.

Abstract: This paper introduces a novel framework for robotic vision-based navigation that integrates Hybrid Neural Networks (HNNs) with Spiking Neural Network (SNN)-based filtering to enhance situational awareness for unmodeled obstacle detection and localization. By leveraging the complementary strengths of Artificial Neural Networks (ANNs) and SNNs, the system achieves both accurate environmental understanding and fast, energy-efficient processing. The proposed architecture employs a dual-pathway approach: an ANN component processes static spatial features at low frequency, while an SNN component handles dynamic, event-based sensor data in real time. Unlike conventional hybrid architectures that rely on domain conversion mechanisms, our system incorporates a pre-developed SNN-based filter that directly utilizes spike-encoded inputs for localization and state estimation. Detected anomalies are validated using contextual information from the ANN pathway and continuously tracked to support anticipatory navigation strategies. Simulation results demonstrate that the proposed method offers acceptable detection accuracy while maintaining computational efficiency close to SNN-only implementations, which operate at a fraction of the resource cost. This framework represents a significant advancement in neuromorphic navigation systems for robots operating in unpredictable and dynamic environments.

</details>


### [842] [The OncoReach Stylet for Brachytherapy: Design Evaluation and Pilot Study](https://arxiv.org/abs/2601.13529)
*Pejman Kheradmand,Kent K. Yamamoto,Emma Webster,Keith Sowards,Gianna Hatheway,Katharine L. Jackson,Sabino Zani,Julie A. Raffi,Diandra N. Ayala-Peacock,Scott R. Silva,Joanna Deaton Bertram,Yash Chitalia*

Main category: cs.RO

TL;DR: This paper introduces OncoReach, a steerable stylet for use in cervical cancer interstitial brachytherapy, improving upon traditional straight needle techniques.


<details>
  <summary>Details</summary>
Motivation: Improve the flexibility and precision in the delivery of interstitial brachytherapy for cervical cancer by addressing the limitations of traditional straight needles.

Method: The paper focuses on developing a steerable stylet, optimizing design parameters through experiments, modeling needle behavior, and testing a handheld prototype in a patient-derived phantom model.

Result: The steerable stylet successfully reached hard-to-access targets, demonstrating superior steering capabilities compared to traditional straight needles.

Conclusion: The development of the OncoReach steerable stylet shows potential to enhance surgical outcomes in cervical cancer brachytherapy by allowing less invasive and more accurate tumor targeting.

Abstract: Cervical cancer accounts for a significant portion of the global cancer burden among women. Interstitial brachytherapy (ISBT) is a standard procedure for treating cervical cancer; it involves placing a radioactive source through a straight hollow needle within or in close proximity to the tumor and surrounding tissue. However, the use of straight needles limits surgical planning to a linear needle path. We present the OncoReach stylet, a handheld, tendon-driven steerable stylet designed for compatibility with standard ISBT 15- and 13-gauge needles. Building upon our prior work, we evaluated design parameters like needle gauge, spherical joint count and spherical joint placement, including an asymmetric disk design to identify a configuration that maximizes bending compliance while retaining axial stiffness. Free space experiments quantified tip deflection across configurations, and a two-tube Cosserat rod model accurately predicted the centerline shape of the needle for most trials. The best performing configuration was integrated into a reusable handheld prototype that enables manual actuation. A patient-derived, multi-composite phantom model of the uterus and pelvis was developed to conduct a pilot study of the OncoReach steerable stylet with one expert user. Results showed the ability to steer from less-invasive, medial entry points to reach the lateral-most targets, underscoring the significance of steerable stylets.

</details>


### [843] [LogicEnvGen: Task-Logic Driven Generation of Diverse Simulated Environments for Embodied AI](https://arxiv.org/abs/2601.13556)
*Jianan Wang,Siyang Zhang,Bin Li,Juan Chen,Jingtao Qi,Zhuo Zhang,Chen Qian*

Main category: cs.RO

TL;DR: The paper introduces LogicEnvGen, a method driven by large language models to create logically diverse simulated test environments for agents, focusing more on logical diversity rather than only visual realism.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating simulated environments overly focus on visual realism, neglecting logical diversity, which is critical for effectively testing agent adaptability and robustness.

Method: LogicEnvGen utilizes a top-down approach with large language models to analyze task execution logic, generate decision-tree-structured behavior plans, refine task trajectories, and create physically plausible environments through constraint solving. A new benchmark, LogicEnvEval, is presented for evaluation.

Result: Experimental results show that LogicEnvGen achieves up to 2.61x more logical diversity and improves agent fault detection performance by 4.00%-68.00% compared to baseline methods.

Conclusion: LogicEnvGen addresses the gap in logical diversity within environment generation, enhancing agent testing capabilities and highlighting hidden faults effectively through novel methods and benchmarks.

Abstract: Simulated environments play an essential role in embodied AI, functionally analogous to test cases in software engineering. However, existing environment generation methods often emphasize visual realism (e.g., object diversity and layout coherence), overlooking a crucial aspect: logical diversity from the testing perspective. This limits the comprehensive evaluation of agent adaptability and planning robustness in distinct simulated environments. To bridge this gap, we propose LogicEnvGen, a novel method driven by Large Language Models (LLMs) that adopts a top-down paradigm to generate logically diverse simulated environments as test cases for agents. Given an agent task, LogicEnvGen first analyzes its execution logic to construct decision-tree-structured behavior plans and then synthesizes a set of logical trajectories. Subsequently, it adopts a heuristic algorithm to refine the trajectory set, reducing redundant simulation. For each logical trajectory, which represents a potential task situation, LogicEnvGen correspondingly instantiates a concrete environment. Notably, it employs constraint solving for physical plausibility. Furthermore, we introduce LogicEnvEval, a novel benchmark comprising four quantitative metrics for environment evaluation. Experimental results verify the lack of logical diversity in baselines and demonstrate that LogicEnvGen achieves 1.04-2.61x greater diversity, significantly improving the performance in revealing agent faults by 4.00%-68.00%.

</details>


### [844] [Highly Deformable Proprioceptive Membrane for Real-Time 3D Shape Reconstruction](https://arxiv.org/abs/2601.13574)
*Guanyu Xu,Jiaqi Wang,Dezhong Tong,Xiaonan Huang*

Main category: cs.RO

TL;DR: This paper introduces a soft, stretchable silicone membrane with optical waveguide sensing for accurate 3D surface reconstruction of deformable objects.


<details>
  <summary>Details</summary>
Motivation: Current vision-based approaches fail under low light or occlusion, and traditional shape-aware methods face limitations like electromagnetic interference and structural complexity, motivating this novel membrane design.

Method: The design uses a silicone membrane integrated with embedded LEDs, photodiodes, and liquid-metal traces. A data-driven model decodes light intensity changes caused by deformation to reconstruct 3D geometry.

Result: The system achieved real-time 3D reconstruction at 90 Hz with an average error of 1.3 mm and handled deformations up to 25 mm using a 140 mm square membrane.

Conclusion: The framework offers a robust and scalable method for 3D shape reconstruction in robotic applications, overcoming challenges of traditional methods and enabling reliable performance under various conditions.

Abstract: Reconstructing the three-dimensional (3D) geometry of object surfaces is essential for robot perception, yet vision-based approaches are generally unreliable under low illumination or occlusion. This limitation motivates the design of a proprioceptive membrane that conforms to the surface of interest and infers 3D geometry by reconstructing its own deformation. Conventional shape-aware membranes typically rely on resistive, capacitive, or magneto-sensitive mechanisms. However, these methods often encounter challenges such as structural complexity, limited compliance during large-scale deformation, and susceptibility to electromagnetic interference. This work presents a soft, flexible, and stretchable proprioceptive silicone membrane based on optical waveguide sensing. The membrane sensor integrates edge-mounted LEDs and centrally distributed photodiodes (PDs), interconnected via liquid-metal traces embedded within a multilayer elastomeric composite. Rich deformation-dependent light intensity signals are decoded by a data-driven model to recover the membrane geometry as a 3D point cloud. On a customized 140 mm square membrane, real-time reconstruction of large-scale out-of-plane deformation is achieved at 90 Hz with an average reconstruction error of 1.3 mm, measured by Chamfer distance, while maintaining accuracy for indentations up to 25 mm. The proposed framework provides a scalable, robust, and low-profile solution for global shape perception in deformable robotic systems.

</details>


### [845] [A General One-Shot Multimodal Active Perception Framework for Robotic Manipulation: Learning to Predict Optimal Viewpoint](https://arxiv.org/abs/2601.13639)
*Deyun Qin,Zezhi Liu,Hanqian Luo,Xiao Liang,Yongchun Fang*

Main category: cs.RO

TL;DR: The paper introduces a one-shot multimodal active perception framework for robotic manipulation that enhances viewpoint optimization efficiently and boosts task success rates.


<details>
  <summary>Details</summary>
Motivation: Current active perception methods are iterative and task-specific, leading to inefficiencies and limited transferability across tasks.

Method: The framework employs a data pipeline and an optimal viewpoint prediction network, using cross-attention for multimodal feature alignment, and systematic viewpoint evaluation for dataset creation.

Result: Experiments show improved grasp success rates in robotic manipulation and effective sim-to-real transfer without additional tuning, with real-world success rates nearly doubling.

Conclusion: The proposed framework is effective and efficient, generalizing active perception for robotic manipulation while improving performance and reducing task coupling.

Abstract: Active perception in vision-based robotic manipulation aims to move the camera toward more informative observation viewpoints, thereby providing high-quality perceptual inputs for downstream tasks. Most existing active perception methods rely on iterative optimization, leading to high time and motion costs, and are tightly coupled with task-specific objectives, which limits their transferability. In this paper, we propose a general one-shot multimodal active perception framework for robotic manipulation. The framework enables direct inference of optimal viewpoints and comprises a data collection pipeline and an optimal viewpoint prediction network. Specifically, the framework decouples viewpoint quality evaluation from the overall architecture, supporting heterogeneous task requirements. Optimal viewpoints are defined through systematic sampling and evaluation of candidate viewpoints, after which large-scale training datasets are constructed via domain randomization. Moreover, a multimodal optimal viewpoint prediction network is developed, leveraging cross-attention to align and fuse multimodal features and directly predict camera pose adjustments. The proposed framework is instantiated in robotic grasping under viewpoint-constrained environments. Experimental results demonstrate that active perception guided by the framework significantly improves grasp success rates. Notably, real-world evaluations achieve nearly double the grasp success rate and enable seamless sim-to-real transfer without additional fine-tuning, demonstrating the effectiveness of the proposed framework.

</details>


### [846] [Communication-Free Collective Navigation for a Swarm of UAVs via LiDAR-Based Deep Reinforcement Learning](https://arxiv.org/abs/2601.13657)
*Myong-Yol Choi,Hankyoul Ko,Hanse Cho,Changseung Kim,Seunghwan Kim,Jaemin Seo,Hyondong Oh*

Main category: cs.RO

TL;DR: This paper proposes a deep reinforcement learning (DRL) controller for UAV swarms, enabling communication-free collective navigation in complex environments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of enabling UAV swarms to navigate collectively in communication-denied and obstacle-rich environments, inspired by biological swarms that do not depend on explicit interactions.

Method: The DRL controller operates within an implicit leader-follower framework. Only the leader has goal information while follower UAVs use LiDAR sensing for navigation, without inter-agent communication. The system includes LiDAR-based clustering, Kalman filtering for neighbor tracking, and a DRL model trained in a simulated GPU environment for robust behavior learning.

Result: The system enables UAV swarms to collectively navigate relying on only local perception, achieving robustness in handling challenges such as occlusion and limited field-of-view. Sim-to-real transfer is validated through successful tests in real-world environments with five UAVs.

Conclusion: The approach demonstrates the potential for UAV swarms to perform complex, collective tasks without external communication or localization, setting a foundation for robust autonomous navigation in real-world applications.

Abstract: This paper presents a deep reinforcement learning (DRL) based controller for collective navigation of unmanned aerial vehicle (UAV) swarms in communication-denied environments, enabling robust operation in complex, obstacle-rich environments. Inspired by biological swarms where informed individuals guide groups without explicit communication, we employ an implicit leader-follower framework. In this paradigm, only the leader possesses goal information, while follower UAVs learn robust policies using only onboard LiDAR sensing, without requiring any inter-agent communication or leader identification. Our system utilizes LiDAR point clustering and an extended Kalman filter for stable neighbor tracking, providing reliable perception independent of external positioning systems. The core of our approach is a DRL controller, trained in GPU-accelerated Nvidia Isaac Sim, that enables followers to learn complex emergent behaviors - balancing flocking and obstacle avoidance - using only local perception. This allows the swarm to implicitly follow the leader while robustly addressing perceptual challenges such as occlusion and limited field-of-view. The robustness and sim-to-real transfer of our approach are confirmed through extensive simulations and challenging real-world experiments with a swarm of five UAVs, which successfully demonstrated collective navigation across diverse indoor and outdoor environments without any communication or external localization.

</details>


### [847] [SUNSET -- A Sensor-fUsioN based semantic SegmEnTation exemplar for ROS-based self-adaptation](https://arxiv.org/abs/2601.13732)
*Andreas Wiedholz,Rafael Paintner,Julian Gleißner,Alwin Hoffmann,Tobias Huber*

Main category: cs.RO

TL;DR: The paper introduces SUNSET, a ROS2-based system for evaluating robot self-adaptation in uncertain and dynamic environments.


<details>
  <summary>Details</summary>
Motivation: The growing deployment of robots in complex, dynamic environments necessitates self-adaptive software systems to address uncertainties and concurrent issues.

Method: The authors developed SUNSET, which incorporates a machine learning-driven semantic-segmentation pipeline and uncertainty-injection scripts to evaluate self-adaptive capabilities in robots.

Result: SUNSET provides replicable tools for studying self-adaptation, including a sensor fusion pipeline, ML model, and mechanisms for uncertainty handling.

Conclusion: The paper provides a foundational framework for rigorous and reproducible evaluation of architecture-based self-adaptation in robotic systems.

Abstract: The fact that robots are getting deployed more often in dynamic environments, together with the increasing complexity of their software systems, raises the need for self-adaptive approaches. In these environments robotic software systems increasingly operate amid (1) uncertainties, where symptoms are easy to observe but root causes are ambiguous, or (2) multiple uncertainties appear concurrently. We present SUNSET, a ROS2-based exemplar that enables rigorous, repeatable evaluation of architecture-based self-adaptation in such conditions. It implements a sensor fusion semantic-segmentation pipeline driven by a trained Machine Learning (ML) model whose input preprocessing can be perturbed to induce realistic performance degradations. The exemplar exposes five observable symptoms, where each can be caused by different root causes and supports concurrent uncertainties spanning self-healing and self-optimisation. SUNSET includes the segmentation pipeline, a trained ML model, uncertainty-injection scripts, a baseline controller, and step-by-step integration and evaluation documentation to facilitate reproducible studies and fair comparison.

</details>


### [848] [RIM Hand : A Robotic Hand with an Accurate Carpometacarpal Joint and Nitinol-Supported Skeletal Structure](https://arxiv.org/abs/2601.13737)
*Joon Lee,Jeongyoon Han,Doyoung Kim,Seokhwan Jeong*

Main category: cs.RO

TL;DR: The RIM Hand is a robotic hand mirroring natural hand movements and superior in adaptability and strength for potential prosthetic and robotic applications.


<details>
  <summary>Details</summary>
Motivation: To develop a robotic hand with human-like dexterity and adaptability, enabling better functionality for prosthetics and robotics.

Method: The RIM Hand combines CMC joint replication, Nitinol wire skeletal structure, silicone skin, and tendon-driven design to mimic anatomical hand flexibility and strength.

Result: The hand achieves 28% palm deformation, double the payload, and triple the contact area compared to rigid designs.

Conclusion: The RIM Hand surpasses existing designs in dexterity, compliance, and realism, making it ideal for advanced prosthetic and service-robot uses.

Abstract: This paper presents the flexible RIM Hand, a biomimetic robotic hand that precisely replicates the carpometacarpal (CMC) joints and employs superelastic Nitinol wires throughout its skeletal framework. By modeling the full carpal-to-metacarpal anatomy, the design enables realistic palm deformation through tendon-driven fingers while enhancing joint restoration and supports skeletal structure with Nitinol-based dorsal extensors. A flexible silicone skin further increases contact friction and contact area, enabling stable grasps for diverse objects. Experiments show that the palm can deform up to 28%, matching human hand flexibility, while achieving more than twice the payload capacity and three times the contact area compared to a rigid palm design. The RIM Hand thus offers improved dexterity, compliance, and anthropomorphism, making it promising for prosthetic and service-robot applications.

</details>


### [849] [Sample Efficient Learning of Body-Environment Interaction of an Under-Actuated System](https://arxiv.org/abs/2601.13777)
*Zvi Chapnik,Yizhar Or,Shai Revzen*

Main category: cs.RO

TL;DR: This paper evaluates methods for learning motility maps, essential for understanding system-environment interactions in locomotion, using motion data from a specific robot.


<details>
  <summary>Details</summary>
Motivation: To explore how biological and robotic systems use shape changes for motion by interacting mechanically with their environment, with a focus on environments where motility maps play a key role.

Method: Four modeling approaches were tested using data from a custom-built robot with under-actuated degrees of freedom. These approaches were evaluated for predicting body velocity from shape changes under different conditions (same gait, across gaits, and across speeds).

Result: The study identified a trade-off: simpler methods perform better with limited training data, whereas advanced methods excel with larger datasets.

Conclusion: The choice of modeling approach for learning motility maps depends on the size of the training dataset, balancing simplicity and sophistication based on data availability.

Abstract: Geometric mechanics provides valuable insights into how biological and robotic systems use changes in shape to move by mechanically interacting with their environment. In high-friction environments it provides that the entire interaction is captured by the ``motility map''. Here we compare methods for learning the motility map from motion tracking data of a physical robot created specifically to test these methods by having under-actuated degrees of freedom and a hard to model interaction with its substrate. We compared four modeling approaches in terms of their ability to predict body velocity from shape change within the same gait, across gaits, and across speeds. Our results show a trade-off between simpler methods which are superior on small training datasets, and more sophisticated methods, which are superior when more training data is available.

</details>


### [850] [HoverAI: An Embodied Aerial Agent for Natural Human-Drone Interaction](https://arxiv.org/abs/2601.13801)
*Yuhua Jin,Nikita Kuzmin,Georgii Demianchuk,Mariya Lezina,Fawad Mehboob,Issatay Tokmurziyev,Miguel Altamirano Cabrera,Muhammad Ahsan Mustafa,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: HoverAI integrates drone mobility, conversational AI, and visual output for socially responsive interaction, demonstrating impressive results in command recognition, user demographic estimation, and speech transcription.


<details>
  <summary>Details</summary>
Motivation: To address communication challenges of drones in human spaces by enhancing their ability to convey intentions and engage with users effectively.

Method: Combines drone mobility, visual projections, real-time conversational AI, and personalization using a multimodal pipeline with VAD, ASR, LLM, RAG, and voice synthesis.

Result: Achieves high command recognition accuracy (F1: 0.90), gender demographic estimation (F1: 0.89), age estimation (MAE: 5.14 years), and speech transcription performance (WER: 0.181).

Conclusion: HoverAI showcases the potential of integrating aerial robotics with adaptive conversational AI, enabling socially-aware, human-centered applications like guidance and assistance.

Abstract: Drones operating in human-occupied spaces suffer from insufficient communication mechanisms that create uncertainty about their intentions. We present HoverAI, an embodied aerial agent that integrates drone mobility, infrastructure-independent visual projection, and real-time conversational AI into a unified platform. Equipped with a MEMS laser projector, onboard semi-rigid screen, and RGB camera, HoverAI perceives users through vision and voice, responding via lip-synced avatars that adapt appearance to user demographics. The system employs a multimodal pipeline combining VAD, ASR (Whisper), LLM-based intent classification, RAG for dialogue, face analysis for personalization, and voice synthesis (XTTS v2). Evaluation demonstrates high accuracy in command recognition (F1: 0.90), demographic estimation (gender F1: 0.89, age MAE: 5.14 years), and speech transcription (WER: 0.181). By uniting aerial robotics with adaptive conversational AI and self-contained visual output, HoverAI introduces a new class of spatially-aware, socially responsive embodied agents for applications in guidance, assistance, and human-centered interaction.

</details>


### [851] [DroneVLA: VLA based Aerial Manipulation](https://arxiv.org/abs/2601.13809)
*Fawad Mehboob,Monijesu James,Amir Habel,Jeffrin Sam,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: Developed an autonomous aerial manipulation system interpreting natural language commands for object retrieval and human delivery. Integrated VLA model, Grounding DINO, dynamic A* algorithms, and human-centric control using a MediaPipe-based interface.


<details>
  <summary>Details</summary>
Motivation: Addressed the need for intuitive interfaces allowing non-expert users to command aerial manipulation systems naturally, evolving drones from passive observers to active manipulators.

Method: The system integrates Vision-Language-Action (VLA) models for task prioritization, Grounding DINO for object detection, dynamic A* planning for navigation, and MediaPipe for real-time human pose estimation during handover.

Result: Real-world experiments showcased effective localization and navigation results with precise error measurements, proving the system's feasibility.

Conclusion: The proposed system demonstrates significant capabilities in autonomous aerial manipulation enhanced by user-friendly natural language interfaces, indicating its potential practical applications.

Abstract: As aerial platforms evolve from passive observers to active manipulators, the challenge shifts toward designing intuitive interfaces that allow non-expert users to command these systems naturally. This work introduces a novel concept of autonomous aerial manipulation system capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user. The system is intended to integrate a MediaPipe based on Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic reasoning to interpret the intent of a user prompt and generates a prioritized task queue for grasping of relevant objects in the scene. Grounding DINO and dynamic A* planning algorithm are used to navigate and safely relocate the object. To ensure safe and natural interaction during the handover phase, the system employs a human-centric controller driven by MediaPipe. This module provides real-time human pose estimation, allowing the drone to employ visual servoing to maintain a stable, distinct position directly in front of the user, facilitating a comfortable handover. We demonstrate the system's efficacy through real-world experiments for localization and navigation, which resulted in a 0.164m, 0.070m, and 0.084m of max, mean euclidean, and root-mean squared errors, respectively, highlighting the feasibility of VLA for aerial manipulation operations.

</details>


### [852] [GuideTouch: An Obstacle Avoidance Device for Visually Impaired](https://arxiv.org/abs/2601.13813)
*Timofei Kozlov,Artem Trandofilov,Georgii Gazaryan,Issatay Tokmurziyev,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: GuideTouch, a wearable device, is introduced to help visually impaired individuals navigate by detecting obstacles and providing haptic feedback, achieving high accuracy in experiments.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional mobility aids in detecting head-level obstacles for visually impaired individuals, enhancing their safety and autonomy.

Method: Developed GuideTouch with ToF sensors for 3D environment detection, 4 vibrotactile actuators for haptic feedback, and a self-cleaning mechanism; tested on both sighted and visually impaired users.

Result: GuideTouch achieved a high recognition accuracy of 92.9% for primary patterns and 93.75% in tests with visually impaired users, validating its effectiveness.

Conclusion: GuideTouch improves spatial perception and safety for visually impaired individuals and demonstrates potential for enhanced independent navigation.

Abstract: Safe navigation for the visually impaired individuals remains a critical challenge, especially concerning head-level obstacles, which traditional mobility aids often fail to detect. We introduce GuideTouch, a compact, affordable, standalone wearable device designed for autonomous obstacle avoidance. The system integrates two vertically aligned Time-of-Flight (ToF) sensors, enabling three-dimensional environmental perception, and four vibrotactile actuators that provide directional haptic feedback. Proximity and direction information is communicated via an intuitive 4-point vibrotactile feedback system located across the user's shoulders and upper chest. For real-world robustness, the device includes a unique centrifugal self-cleaning optical cover mechanism and a sound alarm system for location if the device is dropped. We evaluated the haptic perception accuracy across 22 participants (17 male and 5 female, aged 21-48, mean 25.7, sd 6.1). Statistical analysis confirmed a significant difference between the perception accuracy of different patterns. The system demonstrated high recognition accuracy, achieving an average of 92.9% for single and double motor (primary directional) patterns. Furthermore, preliminary experiments with 14 visually impaired users validated this interface, showing a recognition accuracy of 93.75% for primary directional cues. The results demonstrate that GuideTouch enables intuitive spatial perception and could significantly improve the safety, confidence, and autonomy of users with visual impairments during independent navigation.

</details>


### [853] [Efficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework](https://arxiv.org/abs/2601.13945)
*Yixuan Deng,Tongrun Wu,Donghao Wu,Zeyu Wei,Jiayuan Wang,Zhenglong Sun,Yuqing Tang,Xiaoqiang Ji*

Main category: cs.RO

TL;DR: ANCHOR is a modular framework designed for reliable and scalable AI systems, ensuring robustness and handling partial failures efficiently.


<details>
  <summary>Details</summary>
Motivation: Embodied AI systems face challenges in reliability, scalability, and recovery from failures when transitioning from prototypes to real-world applications.

Method: ANCHOR decouples shared context into canonical records for standardization while employing a communication bus for dissemination and coordination, enabling inspectable end-to-end workflows.

Result: Closed-loop feasibility is validated, with results showing tolerance to workload variability, efficient crash recovery, and maintained operational robustness even after failures.

Conclusion: ANCHOR enables dependable, scalable, and self-healing AI deployments by introducing standardized interfaces and improving robustness under diverse operational conditions.

Abstract: As Embodied AI systems move from research prototypes to real world deployments, they tend to evolve rapidly while remaining reliable under workload changes and partial failures. In practice, many deployments are only partially decoupled: middleware moves messages, but shared context and feedback semantics are implicit, causing interface drift, cross-module interference, and brittle recovery at scale. We present ANCHOR, a modular framework that makes decoupling and robustness explicit system-level primitives. ANCHOR separates (i) Canonical Records, an evolvable contract for the standardized shared state, from (ii) a communication bus for many-to-many dissemination and feedback-oriented coordination, forming an inspectable end-to-end loop. We validate closed-loop feasibility on a de-identified workflow instantiation, characterize latency distributions under varying payload sizes and publish rates, and demonstrate automatic stream resumption after hard crashes and restarts even with shared-memory loss. Overall, ANCHOR turns ad-hoc integration glue into explicit contracts, enabling controlled degradation under load and self-healing recovery for scalable deployment of closed-loop AI systems.

</details>


### [854] [Active Cross-Modal Visuo-Tactile Perception of Deformable Linear Objects](https://arxiv.org/abs/2601.13979)
*Raffaele Mazza,Ciro Natale,Pietro Falco*

Main category: cs.RO

TL;DR: The paper introduces a framework combining vision and tactile sensing for reconstructing 3D shapes of occluded cables, overcoming limitations in visual-only methods.


<details>
  <summary>Details</summary>
Motivation: Current methods largely rely on vision, which struggles under occlusions, clutter, or poor lighting when reconstructing deformable objects like cables.

Method: The approach integrates visual perception models (SAM and Florence) for segmentation and semantic data, alongside tactile sensors for filling occluded sections. Tactile data merges with visual input via clustering and topology-preserving techniques.

Result: Experiments with a robotic manipulator equipped with vision and tactile systems show successful reconstruction of various cable configurations, even with significant occlusions.

Conclusion: Cross-modal visuo-tactile framework using foundation models proves effective for 3D reconstructions and may advance robotic handling of deformable objects.

Abstract: This paper presents a novel cross-modal visuo-tactile perception framework for the 3D shape reconstruction of deformable linear objects (DLOs), with a specific focus on cables subject to severe visual occlusions. Unlike existing methods relying predominantly on vision, whose performance degrades under varying illumination, background clutter, or partial visibility, the proposed approach integrates foundation-model-based visual perception with adaptive tactile exploration. The visual pipeline exploits SAM for instance segmentation and Florence for semantic refinement, followed by skeletonization, endpoint detection, and point-cloud extraction. Occluded cable segments are autonomously identified and explored with a tactile sensor, which provides local point clouds that are merged with the visual data through Euclidean clustering and topology-preserving fusion. A B-spline interpolation driven by endpoint-guided point sorting yields a smooth and complete reconstruction of the cable shape. Experimental validation using a robotic manipulator equipped with an RGB-D camera and a tactile pad demonstrates that the proposed framework accurately reconstructs both simple and highly curved single or multiple cable configurations, even when large portions are occluded. These results highlight the potential of foundation-model-enhanced cross-modal perception for advancing robotic manipulation of deformable objects.

</details>


### [855] [Group-Invariant Unsupervised Skill Discovery: Symmetry-aware Skill Representations for Generalizable Behavior](https://arxiv.org/abs/2601.14000)
*Junwoo Chang,Joseph Park,Roberto Horowitz,Jongmin Lee,Jongeun Choi*

Main category: cs.RO

TL;DR: The paper introduces GISD, a framework for unsupervised skill discovery that accounts for geometric symmetries to improve exploration and task learning efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing skill discovery methods often neglect geometric symmetries in environments, causing redundant behaviors and inefficiencies.

Method: The paper proposes GISD, which incorporates group symmetry into skill discovery via a theoretical guarantee, employing group Fourier representation for the scoring function and aligning latent features for intrinsic rewards.

Result: Experiments on locomotion benchmarks show GISD achieves better state-space coverage and task learning efficiency than competing methods.

Conclusion: Incorporating group symmetries through GISD improves skill discovery, exploration, and task learning effectiveness in various environments.

Abstract: Unsupervised skill discovery aims to acquire behavior primitives that improve exploration and accelerate downstream task learning. However, existing approaches often ignore the geometric symmetries of physical environments, leading to redundant behaviors and sample inefficiency. To address this, we introduce Group-Invariant Skill Discovery (GISD), a framework that explicitly embeds group structure into the skill discovery objective. Our approach is grounded in a theoretical guarantee: we prove that in group-symmetric environments, the standard Wasserstein dependency measure admits a globally optimal solution comprised of an equivariant policy and a group-invariant scoring function. Motivated by this, we formulate the Group-Invariant Wasserstein dependency measure, which restricts the optimization to this symmetry-aware subspace without loss of optimality. Practically, we parameterize the scoring function using a group Fourier representation and define the intrinsic reward via the alignment of equivariant latent features, ensuring that the discovered skills generalize systematically under group transformations. Experiments on state-based and pixel-based locomotion benchmarks demonstrate that GISD achieves broader state-space coverage and improved efficiency in downstream task learning compared to a strong baseline.

</details>


### [856] [Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems](https://arxiv.org/abs/2601.14091)
*Hossein Naderi,Alireza Shojaei,Lifu Huang,Philip Agee,Kereshmeh Afsari,Abiola Akanmu*

Main category: cs.RO

TL;DR: This paper explores the use of lightweight foundation models for improving task planning in construction robots, proposing single and multi-agent teams and finding that multi-agent teams are highly cost-effective and generalizable.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of high costs and adaptability in construction robots, and to explore the potential of foundation models in enhancing robotic task planning.

Method: Proposed and implemented single-agent and multi-agent team models using lightweight large language models (LLMs) and vision language models (VLMs), tested across three construction tasks.

Result: The four-agent team outperformed GPT-4o in most metrics and proved to be ten times more cost-effective, with three-agent and four-agent teams displaying better generalizability.

Conclusion: Study demonstrates the effectiveness of multi-agent foundation models in improving adaptability and cost efficiency, offering insights for AI applications in dynamic, unstructured environments.

Abstract: Robots are expected to play a major role in the future construction industry but face challenges due to high costs and difficulty adapting to dynamic tasks. This study explores the potential of foundation models to enhance the adaptability and generalizability of task planning in construction robots. Four models are proposed and implemented using lightweight, open-source large language models (LLMs) and vision language models (VLMs). These models include one single agent and three multi-agent teams that collaborate to create robot action plans. The models are evaluated across three construction roles: Painter, Safety Inspector, and Floor Tiling. Results show that the four-agent team outperforms the state-of-the-art GPT-4o in most metrics while being ten times more cost-effective. Additionally, teams with three and four agents demonstrate the improved generalizability. By discussing how agent behaviors influence outputs, this study enhances the understanding of AI teams and supports future research in diverse unstructured environments beyond construction.

</details>


### [857] [Diffusion-Guided Backdoor Attacks in Real-World Reinforcement Learning](https://arxiv.org/abs/2601.14104)
*Tairan Huang,Qingqing Ye,Yulin Jin,Jiawei Lian,Yi Wang,Haibo Hu*

Main category: cs.RO

TL;DR: The paper introduces a novel backdoor attack framework for reinforcement learning (RL) in real-world robotic systems, utilizing visual patch triggers generated by a conditional diffusion model and a strategic poisoning approach.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the limitations of existing backdoor attacks in RL, which fail to effectively work in actual physical robotic environments due to safety-constrained control pipelines that neutralize abnormal actions.

Method: The authors propose a Diffusion-Guided Backdoor Attack (DGBA) framework. They design printable patch triggers with diverse visual appearances using a conditional diffusion model and employ an advantage-based poisoning strategy, targeting decision-critical states during training. The framework is treated as a black-box system.

Result: The proposed framework was tested on a TurtleBot3 robot, demonstrating consistent and reliable activation of targeted backdoor attacks while maintaining normal task performance in real-world conditions.

Conclusion: The DGBA framework overcomes the limitations of conventional backdoor attacks in RL by designing robust, visually adaptive triggers and strategic poisoning techniques, enabling their deployment in real-world robotic systems.

Abstract: Backdoor attacks embed hidden malicious behaviors in reinforcement learning (RL) policies and activate them using triggers at test time. Most existing attacks are validated only in simulation, while their effectiveness in real-world robotic systems remains unclear. In physical deployment, safety-constrained control pipelines such as velocity limiting, action smoothing, and collision avoidance suppress abnormal actions, causing strong attenuation of conventional backdoor attacks. We study this previously overlooked problem and propose a diffusion-guided backdoor attack framework (DGBA) for real-world RL. We design small printable visual patch triggers placed on the floor and generate them using a conditional diffusion model that produces diverse patch appearances under real-world visual variations. We treat the robot control stack as a black-box system. We further introduce an advantage-based poisoning strategy that injects triggers only at decision-critical training states. We evaluate our method on a TurtleBot3 mobile robot and demonstrate reliable activation of targeted attacks while preserving normal task performance. Demo videos and code are available in the supplementary material.

</details>


### [858] [SandWorm: Event-based Visuotactile Perception with Active Vibration for Screw-Actuated Robot in Granular Media](https://arxiv.org/abs/2601.14128)
*Shoujie Li,Changqing Guo,Junhao Gong,Chenxin Liang,Wenhua Ding,Wenbo Ding*

Main category: cs.RO

TL;DR: The paper introduces SandWorm, a screw-actuated robot with peristaltic motion, and SWTac, an event-based visuotactile sensor, showcasing their applications in granular media perception and locomotion.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of perception in granular media due to unpredictable particle dynamics.

Method: Developed SandWorm, a biomimetic robot, and SWTac, an event-based visuotactile sensor, incorporating mechanical isolation, temporal filtering, and optimization of sensor components for enhanced tactile imaging and locomotion.

Result: SWTac achieves 0.2 mm texture resolution, 98% stone classification accuracy, and 0.15 N force estimation error. SandWorm demonstrates locomotion speeds of up to 12.5 mm/s with a 90% success rate in pipeline dredging and subsurface exploration.

Conclusion: The integrated system of SandWorm and SWTac provides effective solutions for granular media perception and locomotion, validated in field experiments and complex terrains.

Abstract: Perception in granular media remains challenging due to unpredictable particle dynamics. To address this challenge, we present SandWorm, a biomimetic screw-actuated robot augmented by peristaltic motion to enhance locomotion, and SWTac, a novel event-based visuotactile sensor with an actively vibrated elastomer. The event camera is mechanically decoupled from vibrations by a spring isolation mechanism, enabling high-quality tactile imaging of both dynamic and stationary objects. For algorithm design, we propose an IMU-guided temporal filter to enhance imaging consistency, improving MSNR by 24%. Moreover, we systematically optimize SWTac with vibration parameters, event camera settings and elastomer properties. Motivated by asymmetric edge features, we also implement contact surface estimation by U-Net. Experimental validation demonstrates SWTac's 0.2 mm texture resolution, 98% stone classification accuracy, and 0.15 N force estimation error, while SandWorm demonstrates versatile locomotion (up to 12.5 mm/s) in challenging terrains, successfully executes pipeline dredging and subsurface exploration in complex granular media (observed 90% success rate). Field experiments further confirm the system's practical performance.

</details>


### [859] [TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers](https://arxiv.org/abs/2601.14133)
*Bin Yu,Shijie Lian,Xiaopeng Lin,Yuliang Wei,Zhaolong Shen,Changti Wu,Yuzhuo Miao,Xinming Wang,Bailing Wang,Cong Huang,Kai Chen*

Main category: cs.RO

TL;DR: The paper introduces TwinBrainVLA, a novel model that uses two specialized Vision-Language Models to overcome the issue of catastrophic forgetting in robotic control, achieving superior performance in manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of balancing high-level semantic understanding and fine-grained sensorimotor skills in Vision-Language Models (VLMs) used for robotic control.

Method: The method involves using a dual-brain architecture with a frozen generalist VLM ('Left Brain') for robust semantic understanding and a trainable specialist VLM ('Right Brain') for embodied perception, connected through an Asymmetric Mixture-of-Transformers (AsyMoT) mechanism.

Result: Extensive experiments show TwinBrainVLA surpasses state-of-the-art baselines in manipulation tasks while preserving visual reasoning capabilities of pre-trained VLMs.

Conclusion: TwinBrainVLA provides a promising framework for developing robots capable of integrating semantic understanding with physical dexterity.

Abstract: Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to "catastrophic forgetting" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen "Left Brain", which retains robust general visual reasoning, with a trainable "Right Brain", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [860] [Reinforcement Learning for Dynamic Workflow Optimization in CI/CD Pipelines](https://arxiv.org/abs/2601.11647)
*Aniket Abhishek Soni,Milan Parikh,Rashi Nimesh Kumar Dhenia,Jubin Abhishek Soni,Ayush Raj Jha,Sneja Mitinbhai Shah*

Main category: cs.SE

TL;DR: The paper presents a reinforcement learning system to optimize CI/CD workflows, achieving significant efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: Static workflows in CI/CD pipelines can lead to inefficiencies, particularly as systems scale.

Method: The CI/CD pipeline is modeled as a Markov Decision Process, and a reinforcement learning agent is trained to dynamically optimize runtime decisions.

Result: Experimental results show a 30% improvement in throughput and a 25% reduction in test execution time, while maintaining a defect miss rate below 5%.

Conclusion: Reinforcement learning provides a viable approach for more efficient and intelligent CI/CD automation workflows.

Abstract: Continuous Integration and Continuous Deployment (CI/CD) pipelines are central to modern software delivery, yet their static workflows often introduce inefficiencies as systems scale. This paper proposes a reinforcement learning (RL) based approach to dynamically optimize CI/CD pipeline workflows. The pipeline is modeled as a Markov Decision Process, and an RL agent is trained to make runtime decisions such as selecting full, partial, or no test execution in order to maximize throughput while minimizing testing overhead.
  A configurable CI/CD simulation environment is developed to evaluate the approach across build, test, and deploy stages. Experimental results show that the RL optimized pipeline achieves up to a 30 percent improvement in throughput and approximately a 25 percent reduction in test execution time compared to static baselines, while maintaining a defect miss rate below 5 percent. The agent learns to selectively skip or abbreviate tests for low risk commits, accelerating feedback cycles without significantly increasing failure risk.
  These results demonstrate the potential of reinforcement learning to enable adaptive and intelligent DevOps workflows, providing a practical pathway toward more efficient, resilient, and sustainable CI/CD automation.

</details>


### [861] [FlipFlop: A Static Analysis-based Energy Optimization Framework for GPU Kernels](https://arxiv.org/abs/2601.13345)
*Saurabhsingh Rajput,Alexander Brandt,Vadim Elisseev,Tushar Sharma*

Main category: cs.SE

TL;DR: The paper introduces FlipFlop, a static code analysis framework for optimizing energy consumption and execution time of GPU kernels without requiring runtime execution.


<details>
  <summary>Details</summary>
Motivation: GPU programs supporting AI applications consume significant energy, and developers lack specialized knowledge to optimize them efficiently.

Method: FlipFlop uses static code analysis of PTX code to predict energy consumption and recommend optimal thread block configurations based on execution time and power efficiency.

Result: FlipFlop achieves 83% accuracy in identifying energy-efficient configurations, reduces optimization effort by 93.4%, and saves up to 79% energy while increasing throughput by 106%.

Conclusion: FlipFlop serves as a resource-efficient tool, assisting software developers in creating sustainable and high-performance GPU code, reducing environmental and computational costs.

Abstract: Artificial Intelligence (AI) applications, such as Large Language Models, are primarily driven and executed by Graphics Processing Units (GPUs). These GPU programs (kernels) consume substantial amounts of energy, yet software developers often lack the hardware expertise and ad hoc knowledge required to optimize for power efficiency. We propose FlipFlop, a framework using static code analysis to predict energy consumption and recommend Pareto-optimal thread block configurations considering both power consumption and execution time. Our framework requires no runtime execution and analyzes PTX code, a low-level instruction set for CUDA-enabled GPUs. It is validated across a diverse set of GPUs and kernels, including multi-head attention, convolution, and matrix multiplication. FlipFlop achieves 83% accuracy in identifying locally optimal energy-efficient configurations, while also minimizing developer effort by reducing the optimization search space by 93.4%. For multi-head attention kernels, it yields up to 79% energy savings and 106% throughput gains relative to NVIDIA's occupancy heuristic. By integrating static analysis with real-time monitoring and providing explainable optimization guidance, FlipFlop empowers developers to create sustainable, high-performance GPU software which minimizes environmental and computational costs.

</details>


### [862] [Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey](https://arxiv.org/abs/2601.11655)
*Caihua Li,Lianghong Guo,Yanlin Wang,Daya Guo,Wei Tao,Zhenyu Shan,Mingwei Liu,Jiachi Chen,Haoyu Song,Duyu Tang,Hongyu Zhang,Zibin Zheng*

Main category: cs.SE

TL;DR: This paper surveys the domain of issue resolution using artificial intelligence, examining data pipelines, methodologies, data quality, agent behavior, applications, and future research challenges.


<details>
  <summary>Details</summary>
Motivation: The difficulty of issue resolution tasks for large language models highlights the need to study and improve AI-based autonomous coding agents for real-world software engineering challenges.

Method: A systematic survey approach is used to review data construction methods, training-free and training-based techniques, data quality, agent behavior, and practical applications.

Result: Key findings and insights are provided on the methodologies, challenges, and potential research avenues in AI-driven issue resolution. An open-source repository is shared to assist researchers and practitioners.

Conclusion: This paper contributes a structured understanding of AI in issue resolution, identifies major challenges, and proposes paths for future exploration in this critical software engineering area.

Abstract: Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.

</details>


### [863] [The Llama 4 Herd: Architecture, Training, Evaluation, and Deployment Notes](https://arxiv.org/abs/2601.11659)
*Aaron Adcock,Aayushi Srivastava,Abhimanyu Dubey,Abhinav Jauhri,Abhinav Pande,Abhinav Pandey,Abhinav Sharma,Abhishek Kadian,Abhishek Kumawat,Adam Kelsey,Adam Stelle,Adeel Cheema,Adela Kabiljo,Adina Katz,Adithya Gangidi,Aditya Tayade,Adolfo Victoria,Adrian Samatan Alastuey,Adrien Conrath,Afroz Mohiuddin,Ahmed Sharif,Ahnaf Siddiqui,Ahuva Goldstand,Aijung Li,Aidan Boyd,Aidin Kazemi Daliri,Aisha Iqbal,Ajay Menon,Ajit Mathews,Akhil Mathur,Akshat Agarwal,Alan Schelten,Alana Shine,Alejandro Castillejo Muñoz,Aleksei Guliaev,Alex Radovic,Alex Song,Alex Vaughan,Alexander Simeonov,Alexandre Rezende,Alexandre Rezende,Alexei Baevski,Alexey Roubaud,Allen Ma,Alvin Lee,Alyssa Pereira,Aman Ahmed,Aman Shankar,Amanda Kallet,Amar Budhiraja,Ameya Khandekar,Amine Benhalloum,Amir Gershman,Amit Nagpal,Amit Zohar,Amr Sharaf,Anant Desai,Anastasia Razdaibiedina,Anca Agape,Andranik Kurghinyan,Andre Perunicic,Andrea Madotto,Andrei Darabanov,Andrés Alvarado,Andrew Brown,Andrew Cohen,Andrew Fang,Andrew Freeman,Andrew Gallagher,Andrew Gu,Andrew Prasetyo Jo,Andrew Ryan,Andrew Steffen,Andrew Wei,Andrey Rusakov,Andrii Golovei,Andy Shang,Angela Fan,Angela Fan,Angela Flewellen,Animesh Pathak,Anirudh Goyal,Ankit Ramchandani,Ankur Pai,Ankur Singh,Ankush Garg,Anlu Xing,Anna Cai,Anna Grosul,Anna Prochowska,Anna Sun,Annie Dong,Annie Franco,Anqi Hu,Anshul Chawla,Anthony Hartshorn,Antonia Sheng,Antony Thomas,Anuj Goyal,Anusha De,Anvit Bodiwala,Anvit Bodiwala,Aobo Yang,Aparajita Saraf,Apurva Samudra,Aran Mun,Arash Rahnama,Archi Mitra,Archie Sravankumar,Archit Gupta,Aria Haghighi,Ariel Stolerman,Arkabandhu Chowdhury,Arnab Choudhury,Artem Korenev,Arthur Guo,Arthur Hinsvark,Arun Mallya,Arvind Neelakantan,Arya Talebzadeh,Ashish Shah,Ashmitha Jeevaraj Shetty,Ashwin Bharambe,Asif Islam,Aston Zhang,Austen Gregerson,Avi Lewis,Aya Ibrahim,Ayaz Minhas,Ayelet Dahan,Ayelet Regev Dabah,Bangsheng Tang,Bar Ulman,Bardiya Sadeghi,Bartosz Jedrzejewski,Barys Skarabahaty,Beibei Zhu,Beibin Li,Ben Bharier,Benjamin Leonhardi,Benjamin Muller,Bennett Plessala,Bernie Huang,Beth Loyd,Bhargavi Paranjape,Bhavik Sheth,Bill Bonner,Bill Holland,Bill Wang,Bingzhe Liu,Binh Tang,Bo Liu,Bo Wu,Boduo Li,Bokai Yu,Bor-Chun Chen,Boris Araya,Boris Vidolov,Botao Chen,Boya Peng,Boyu Ni,Bradley Davis,Bram Wasti,Brandon Adams,Brandon Taylor,Brandon Wu,Brant Swidler,Brian Chiang,Brian Clerkin,Brian Fuller,Brooks Cutter,Bruno Novais,Bryan Gmyrek,Bysshe Easton,Cait Campos,Canaan Case,Carl Chengyan Fu,Carly Burton,Caro Diaz,Catherine Cole,Ce Liu,Cedric Fougerat,Cen Peng,Cen Peng,Cen Zhao,Changhan Wang,Changkyu Kim,Chantal Shaib,Chao Zhou,Charlotte Caucheteux,Chau Nguyen,Chawin Sitawarin,Chaya Nayak,Chelsea Asher,Chen Fan,Chen Zhu,Cheng Cheng,Cheng Zhang,Chenguang Zhu,Chengxiong Ruan,Chengzhu Yu,Chenheli Hua,Chenxi Whitehouse,Cheryl Holloway,Ching-Hsiang Chu,Ching-Yao Chuang,Chinmay Karande,Chirag Nagpal,Chloé Bakalar,Chloe Bi,Chris Cai,Chris Marra,Chris McConnell,Chris Thi,Chris Tindal,Chris Waterson,Christian Deverall,Christian Fuegen,Christian Keller,Christine Cheng,Christine Jou,Christine Smith,Christine Wang,Christoph Feichtenhofer,Christophe Touret,Christopher Luc,Christy Sauper,Chuanhao Zhuge,Chun-Yi Sung,Chunqiang Tang,Chunyang Wu,Clara Siegel,Cody Heale,Cody Wilbourn,Colin White,Congying Xia,Corinne Wong,Cornel Rat,Cristian Canton Ferrer,Cyrille Habis,Cyrus Nikolaidis,D Lohachov,Da Ju,Dalton Flanagan,Damien Allonsius,Damon Civin,Dan Johnson,Daniel Bolya,Daniel Francisco,Daniel Fried,Daniel Hawthorne,Daniel Haziza,Daniel Ho,Daniel Kreymer,Daniel Li,Daniel Machlab,Daniel McKinnon,Daniel Obenshain,Daniel Rodriguez,Daniel Song,Daniel Tse,Danielle Pintz,Danny Livshits,Daryl James Rodrigo,Dat Huynh,Daulet Askarov,David Brandfonbrener,David Esiobu,David Kant,David Levin,David Renardy,David Soofian,David Stevens,David Xu,David Zhang,Deep Shah,Delia David,Demi Douglas,Denis Boyda,Desh Raj,Devamanyu Hazarika,Dheeraj Mekala,Dhruv Choudhary,Dhruv Mahajan,Di Jin,Didac Suris Coll-Vinent,Didem Foss,Diego Garcia-Olano,Diego Perino,Dieuwke Hupkes,DiJia Su,Dilip Madathil,Dinesh Govindasamy,Dinesh Yeduguru,Dmitry Vengertsev,Dong He,Dong Li,Dong Wang,Dongzhuo Li,Duc Le,Dunant Hin,Dustin Holland,Duy Nguyen,Duy Nguyen,Ed Dowling,Eden Litt,Egor Lakomkin,Ehab AlBadawy,Ehsan K. Ardestani,Elad Eckstein,Elahe Dabir,Elaine Montgomery,Elina Lobanova,Elior Abramoviz,Eliot Hedeman,Elissa Li,Elizabeth Hilbert,Ellen Xiaoqing Tan,Elliot Yun,Elodie Stener,Emilian Stoimenov,Emilien Garreau,Emily Dinan,Emily Hahn,Emily Wood,Emma Li,Emmanuel Ademuwagun,Emrah Seker,Eric Alamillo,Eric Gan,Eric Han,Eric Huang,Eric Michael Smith,Eric-Tuan Le,Ernie Chang,Eryk Helenowski,Eslam Elnikety,Esteban Arcaute,Ethan Myers,Eugene Nho,Eugene Poliukhovych,Evan Dunbar,Evgeniy Litvinenko,Evrim Altıntaş,Eyal Hochman,Eyal Shtrauch,Fabian Mastenbroek,Faiza Zeb,Faizan Ahmad,Farhad Farahbakhshian,Fei Kou,Fei Sun,Feiyu Chen,Felix Chung,Feng Tian,Feng Xu,Filip Radenovic,Filippos Kokkinos,Francesco Barbieri,Francesco Caggioni,Francisco Esparza,Francisco Guzmán,Frank Kanayet,Frank Seide,Frank Zhang,Fred Lewis,Freda Huang,Fulton Wang,Gabriel Synnaeve,Gabriela Jacques-Silva,Gabriella Schwarz,Gaganjit Ghardhora,Gal Elfer,Garrett Dickson,Gaurav Chaurasia,Gautam Sewani,Geet Shingi,Gefei Zuo,Geonhwa Jeong,George Puthanpurackal,Georgia Swee,Gerard Moreno-Torres Bertran,Gil Keren,Gina Ling,Gjergji Stasa,Gobinda Saha,Gor Safran,Gordy French,Goutham Rajendran,Govind Thattai,Grace Cineas,Graeme Nail,Greg Fletcher,Grégoire Mialon,Griffin Adams,Grigory Sizov,Guan Pang,Hady Elsahar,Hai Dang Tran,Hailey Nguyen,Haiping Wu,Hakan Inan,Hamid Eghbalzadeh,Han Fang,Han Zou,Hannah Doyle,Hannah Korevaar,Hannah Wang,Hannah Werbel,Hanwen Zha,Hany Morsy,Hao Ma,Haoci Zhang,Haonan Sun,Haozhu Wang,Hardik Shah,Haroun Habeeb,Harrison Rudolph,Harsh Gupta,Harsh Poddar,Harshil Parikh,Hejia Zhang,Heming Wang,Hengduo Li,Himanshu Sharma,Hoang Phi Nguyen,Hongbo Zhang,Honghao Qiu,Hongjiang Lv,Hongli Xu,Hongyuan Zhan,Hossein Hamooni,Howard Huang,Hu Xu,Hugo Laurençon,Hugo Touvron,Hung Dinh,Hunter Goldman,Hussein Mehanna,Huy Nguyen,Hweimi Tsuo,Ian Graves,Ian Yu,Ibrahim Damlaj,Idan Cohen,Igor Tufanov,Ilan Goldenstein,Ilias Leontiadis,Iliyan Zarov,Imad Ahmed,Innocent Djiofack,Iosif Spulber,Irina-Elena Veliche,Isabella Ramos,Ishan Misra,Itai Gal,Ivan Evtimov,Ivan Evtimov,Ivan Obraztsov,Jack Wu,Jacqueline Romero Vertino,Jaemo Koo,Jaewon Lee,Jake Jung,Jake Weissman,James Beldock,James Crnkovich,James Grinage,James Hongyi Zeng,James Kohli,James Tian,Jamie Cahill,Jan Geffert,Jan Seidel,Jan Seidel,Janey Tracey,Jang Hyun Cho,Janice Wei,Jarrod Kahn,Jasmyn Howell,Jason Long Vu,Jason Park,Jason Yan,Jason Yip,Jay Li,Jay Mahadeokar,Jaya Bharath R Goluguri,Jayasi Mehar,Jean-Baptiste Gaya,Jeet Shah,Jeff Hanson,Jeff Marcus,Jeff Walsh,Jeff Yang,Jelmer van der Linde,Jemma Fan,Jennifer Chan,Jenny Zhen,Jenya Lee,Jeremy Fu,Jeremy Reizenstein,Jeremy Teboul,Jesse He,Jessica Zhong,Ji Hou,Ji Yang,Jia Ding,Jiabo Hu,Jiacheng Zhu,Jiadong Guo,Jialiang Wang,Jialin Ouyang,Jianfeng Chi,Jianyu Huang,Jianyun Zhao,Jiaowen Yang,Jiatong Zhou,Jiawei Zhao,Jiawen Liu,Jie Wang,Jie You,Jiecao Yu,Jillian Schwiep,Jilong Wu,Jing Huang,Jing Li,Jing Yu Koh,Jing Zhang,Jingxiang Chen,Jingyi Yang,Jingyue Shen,Jinho Hwang,Jinxi Guo,Jiwan Khatiwada,Joanna Bitton,Joe Li,Joe Quanaim,Joel Beales,Johan Schuijt,John Chang,John Quan,Johnnie Chan,Jon Shepard,Jona Harris,Jonah Rubin,Jonathan Janzen,Jonathan Kaldor,Jorge Lopez Silva,Jose Leitao,Joseph Greer,Joseph Moon,Joseph Rocca,Joseph Tighe,Josh Fromm,Joshua Deng,Joshua Fernandes,Joshua Saxe,Joyce Zheng,Juan Pino,Julien Prigent,Jun Chen,Junjiao Tian,Junjie Qi,Junjie Wang,Junteng Jia,Kade Baker,Kai Londenberg,Kai Wang,Kainan Peng,Kaiyan Peng,Kaiyue Yang,Kalyan Vasudev Alwala,Kam Hou Yu,Kanika Narang,Karan Chadha,Karan Sikka,Karen Zhang,Karina Schuberts,Karishma Mandyam,Karthik Abinav Sankararaman,Karthik Padthe,Karthik Prasad,Karthik Sivakumar,Kartikeya Upasani,Kate Plawiak,Kate Saenko,Kateřina Žmolíková,Kathryn Stadler,Kathy Matosich,Katie Doulgass,Kaveh Hassani,Kay Ji,Ke Li,Kenneth Heafield,Kenny Yu,Keqian Li,Kevin Chih-Yao Ma,Kevin Hannan,Keyu Man,Kezhen Chen,Khalid El-Arini,Khrystyna Hutsulyak,Kieran Nash,Kiran Jagadeesh,Kody Bartelt,Konstantin Topaloglou-Mundy,Konstantinos Chatziioannou,Konstantinos Karanasos,Konstantinos Vougioukas,Kostas Tsiampouris,Kristen Hamill,Kristy Choi,Krithika Iyer,Kshitiz Malik,Kuenley Chiu,Kun Huang,Kunal Bhalla,Kunal Chawla,Kunpeng Li,Kushal Lakhotia,Kyle Monk,Lakshya Garg,Lalit Chourey,Lars Hamre,Laura Gustafson,Lauren Deason,Laurence Rouesnel,Laurens van der Maaten,Lavender A,Lawrence Chen,Lawrence Jang,Leandro Silva,Leda Sari,Lee Hetherington,Lei Zhang,Leiyu Zhao,Lele Chen,Leo Chenghui Li,Leon Yang,Leon Zhan,Levi Corallo,Liang Tan,Licheng Yu,Lijuan Liu,Lilach Mor,Lincoln Lin,Linfeng Li,Lisa Titus,Liz Jenkins,Lovish Madaan,Lu Fang,Lu Yuan,Lucas Nava,Lucas Pasqualin,Lucas Switzer,Lucia Fang,Lucy Sun,Luka Tadic,Lukas Blecher,Lukas Landzaat,Luxin Zhang,Madhavi Rao,Madian Khabsa,Mahalia Miller,Mahendra Kariya,Mahesh Pasupuleti,Mahi Luthra,Manaal Faruqui,Manav Avlani,Manchen Wang,Mannat Singh,Manohar Paluri,Manoj Chakkaravarthy,Manoj Nair,Maquelle Tiffany,Marcin Pawlowski,Marcus Wu,Maria Lomeli,Mario Consuegra,Marion Boiteux,Marios Andreas Galanis,Marshall Chen,Martin Gleize,Maryam Fazel-Zarandi,Matan Hasson,Mathew Oldham,Mathieu Rita,Matt Dordal,Matt Setzler,Matt Staats,Matt Staats,Matt Wilde,Matthew Clark,Matthew Grange,Matthew Lennie,Matthew Schmohl,Max Raphael,Maxim Naumov,Maxim Samoylov,Maxime Lecanu,Maya Pavlova,Md Taha Bin Jawaid,Meghan Keneally,Melanie Kambadur,Meng Zhang,Mengchen Liu,Mengdi Lin,Mengjiao Wang,Mervyn Abraham,Miao Liu,Michael Au-Yeung,Michael Feldergraf,Michael Man,Michael Matheny,Michael Suo,Michael Tontchev,Michel Meyer,Michelle Ma,Mihir Patel,Mihir Sanjay Kale,Mik Vyatskov,Mikayla Alexander,Mike Andersland,Mike Clark,Mike Lewis,Mike Li,Mike Macey,Mike Macey,Mike Seltzer,Mikel Jimenez Fernandez,Mikhail Antonov,Mikhail Plekhanov,Milan Zhou,Min Si,Ming Qiao,Mingbo Ma,Mingjun Zhang,Mingyi Liang,Miquel Jubert Hermoso,Mirac Suzgun,Mirjam Skarica,Mitesh Kumar Singh,Mohammad Kabbani,Mohammad Rastegari,Mona Sarantakos,Monica Sim,Monika Gangapuram,Mor Moshe,Morrie Doulaty,Morvarid Metanat,Moya Chen,Mrinal Kumar,Munish Bansal,Murali Ramarao,Na Li,Nadav Azaria,Nahiyan Malik,Naman Goyal,Nancy Vargas Balderas,Nanshu Wang,Naoyuki Kanda,Natalia Gimelshein,Natalia Neverova,Nathan Aclander,Natt Sithiviraporn,Navneet Madhu Kumar,Ned Newton,Neeraj Bahl,Negar Ghorbani,Neil Patel,Neta-lee Golan,Nicholas Longenbaugh,Nick Egebo,Nikhil Johri,Nikhil Mehta,Nikhil Naik,Niko Moritz,Nikolay Bashlykov,Nikolay Bogoychev,Nikolay Pavlovich Laptev,Niladri Chatterji,Nile Jones,Nimish Shah,Ning Dong,Ning Li,Ning Li,Ning Zhang,Nishant Yadav,Noam Paz,Norman Cheng,Norman Cheng,Olaoluwa Adesanya,Oleg Repin,Oleksandr Maksymets,Omkar Salpekar,Omri Harosh,Onkar Pednekar,Onur Çelebi,Oran Gafni,Oren Edinger,Osama Hanna,Owais Khan Mohammed,Ozlem Kalinli,Paden Tomasello,Pankaj Singh,Paola Quevedo,Parag Jain,Paria Rashidinejad,Parker Tooley,Parth Parekh,Parth Thakkar,Parvin Taheri,Pasan Hapuarachchi,Pascal Kesseli,Patrick Alrassy,Paulo de Rezende Pinatti,Pavan Balaji,Pawan Sisodiya,Pedro Jose Ferreira Moreira,Pedro Rittner,Pedro Valenzuela,Peize Sun,Peizhao Zhang,Peng-Jen Chen,Pengchao Wang,Pengchuan Zhang,Pengwei Li,Petar Vasic,Peter Carras,Peter Ney,Peter Weng,Petru Dumea,Phil Hayes,Philip Woods,Pierre Andrews,Pierre Ménard,Ping-Hao Wu,Pingchuan Liu,Piotr Dollar,Plamen Dzhelepov,Polina Zvyagina,Posten A,Prabhav Agrawal,Pradhapan Rajendran,Pradyot Prakash,Prajjwal Bhargava,Pramono,Pranay Shah,Pranshu Dave,Prash Jain,Pratik Dubal,Praveen Gollakota,Praveen Krishnan,Pritish Yuvraj,Projjal Ghosh,Punit Singh Koura,Puxin Xu,Qi Qi,Qi Zhou,Qian Guan,Qian Sun,Qiang Liu,Qing He,Qinqing Zheng,Qirui Yang,Qizhen Guo,Quanzeng You,Quentin Carbonneaux,Quentin Carbonneaux,Quentin Duval,Quintin Fettes,Rachad Alao,Rachel Batish,Rachel Guo,Rachel Rodriguez,Radhika Bhargava,Rafael Asuncion,Raghotham Murthy,Rahul Dutta,Rahul Jha,Rahul Kindi,Rahul Mitra,Raj Ganapathy,Raj Shah,Rajarshi Das,Rajat Shrivastava,Rajesh Nishtala,Ramakant Shankar,Raman Shukhau,Ramon Calderer,Rangaprabhu Parthasarathy,Ranjan Subramanian,Raphael Bensadoun,Rares Bostan,Rashnil Chaturvedi,Ravi Agrawal,Ray Gao,Raymond Li,Rebecca Kogen,Ricardo Juan Palma Duran,Ricardo Silveira Cabral,Richard Lee,Richard Yuanzhe Pang,Riddhish Bhalodia,Riham Mansour,Rishabh Singh,Rishi Godugu,Ritun Patney,Rob Boyle,Robbie Goldfarb,Robert Caldwell,Robert Kuo,Roberta Raileanu,Robin Battey,Robin Sharma,Rochit Sapra,Rocky Wang,Rodolfo Granata,Rodrigo De Castro,Rodrigo Paim,Rohan Maheshwari,Rohan Varma,Rohit Girdhar,Rohit Patel,Roshan Sumbaly,Roy Sheaffer,Ruan Silva,Ruben Rodriguez Buchillon,Rui Hou,Ruiming Xie,Ruslan Mavlyutov,Ruslan Semenov,Rustam Dinov,Ruxiao Bao,Ryan Fox,Ryan Kilpatrick,Ryan Kwan,Ryan Lim,Ryan Smith,Saaketh Narayan,Sabrina Qiao,Sachin Mehta,Sachin Siby,Sagar Jain,Saghar Hosseini,Sagie Gur-Ari,Sahana Chennabasappa,Sahin Geyik,Sai Jayesh Bondu,Sai Mounika Chowdhary Nekkalapudi,Saif Hasan,Saisuke Okabayashi,Saketh Rambhatla,Salil Sawhney,Sam Dunster,Sam Zhao,Saman Keon,Samaneh Azadi,Sameet Sapra,Samuel Dooley,Samyak Datta,Sandeep Parab,Sang Michael Xie,Sanjay Singh,Sanyuan Chen,Sara Behn,Sara Khodeir,Sarah Shirazyan,Sargun Dhillon,Sarunya Pumma,Sasha Sidorov,Saskia Adaime,Saurabh Khanna,Sayem Wani,Scott Brenton,Sean Bell,Sean Kelly,Sean Koger,Sean Nunley,Sean Perry,Sebastian Caicedo,Sebastian Dahlgren,Sebastian Ruder,Seiji Yamamoto,Selam Mehretu,Selvan Sunitha Ravi,Sen Lyu,Senthil Chellapan,Serafeim Mellos,Sergey Edunov,Sergey Royt,Shaina Cohen,Shangfu Peng,Shannon Adams,Shaoliang Nie,Sharadh Ramaswamy,Sharan Narang,Shashank Pisupati,Shashi Gandham,Shaun Lim,Shaun Lindsay,Sheena Artrip,Shelly Sheynin,Shen Yan,Sheng Feng,Sheng Shen,Shengbao Zheng,Shenghao Lin,Shengjie Bi,Shengxin Cindy Zha,Shengye Wan,Shengyi Qian,Shengyong Cai,Shengzhi Shao,Shervin Shahidi,Shikai Li,Shimon Bernholtz,Shiqi Wang,Shishir G. Patil,Shiv Verma,Shiva Shankar P,Shiyang Chen,Sho Yaida,Shoubhik Debnath,Shreyas Siravara,Shruti Bhosale,Shuang Ma,Shun Zhang,Shuo Tang,Shuqiang Zhang,Shuyan Zhou,Sicong Che,Sidd Srinivisan,Siddharth Bhattacharya,Siddharth Patki,Sijia Chen,Sili Chen,Simon Vandenhende,Simone Merello,Sinong Wang,Sivan Barzily,Sixian Yi,Siyu Lin,SK Bong,Sky Yin,Sneha Agarwal,Sneha Agarwal,Soerian Lieve,Soji Sajuyigbe,Song Jiang,Songlin Li,Sonia Kim,Sopan Khosla,Soumi Maiti,Spencer Whitman,Sravya Popuri,Sreen Tallam,Srinivas Vaidyanathan,Srinivas Vaidyanathan,Sten Sootla,Stephane Collot,Stephanie Ding,Stephen Chen,Steven Cai,Suchin Gururangan,Sudarshan Govindaprasad,Sue Young,Suganthi Dewakar,Sujan Kumar Gonugondla,Sujeet Bhandari,Suman Gumudavelli,Suman Gumudavelli,Sumit Gupta,Summer Deng,Sungmin Cho,Suresh Ganapathy,Surjyendu Dhal,Susan Fedynak,Susana Contrera,Suyoun Kim,Sylvestre Rebuffi,Takshak Chahande,Tamar Herman,Tan Li,Tao Xu,Tara Fowler,Tarek Sheasha,Tarun Anand,Tarun Kalluri,Tarun Singh,Tatiana Shavrina,Ted Li,Teja Rao,Tejas Patil,Teng Li,Thach Bui,Thai Quach,Thamer Alharbash,Thanh Vinh Vo,Thawan Kooburat,Thilo Koehler,Thomas Georgiou,Thomas Scialom,Tian Ye,Tianhe Li,Tianjun Zhang,Tianyu Li,Tijmen Blankevoort,Timon Willi,Timothy Chou,Timothy Leung,TJ Lee,Todor Mihaylov,Tom Heatwole,Tong Xiao,Tony Cao,Tony Lee,Trang Le,Tristan Rice,Tsz Kei Serena Chan,Tuan Tran,Tudor Tiplea,Tyler Baumgartner,Uday Savagaonkar,Ujjwal Karn,Ulises Martinez Araiza,Umar Farooq,Uriel Cohen,Usman Sharif,Utkarsh Murarka,Van Phung,Varun Joginpalli,Varun Saravagi,Vasu Sharma,Vasudha Viswamurthy,Vedanuj Goswami,Vedika Seth,Venkat Ramesh,Venkat Ramesh,Vibhor Gupta,Victoria Montanez,Vidhya Natarajan,Vidya Sarma,Vignesh Ramanathan,Viktor Kerkez,Vinay Rao,Vincent Gonguet,Vincent Mauge,Virginie Do,Vish Vogeti,Vishrav Chaudhary,Viswesh Sankaran,Vítor Albiero,Vivek Miglani,Vivek Pai,Vlad Cojanu,Vlad Shubin,Vlad Tiberiu Mihailescu,Vladan Petrovic,Vladimir Ivanov,Vladislav Vorotilov,Vrushali Bhutada,Wai I Ng,Wei Cheng,Wei Sun,Wei Tu,Wei Wei,Wei Zhou,Wei-Ning Hsu,Weiwei Chu,Weizhe Yuan,Wenchen Wang,Wenjun Zhao,Wenwen Jiang,Wenyin Fu,Wenzhe Jiang,Whitney Meers,Will Constable,Will Wang,William R. Wong,Xavier Martinet,Xi Victoria Lin,Xi Yan,Xi Yin,Xian Li,Xianfeng Rui,Xianjun Yang,Xiaocheng Tang,Xiaodong Wang,Xiaofang Wang,Xiaolan Wang,Xiaoliang Dai,Xiaoliang Peng,Xiaopeng Li,Xiaozhu Meng,Xibei Zhang,Xide Xia,Xin Jin,xinbo Gao,Xinfeng Xie,Xingyi Zhou,Xu Ma,Xuan Ju,Xuanyi Zhao,Xubo Liu,Xuchao Jia,Xuedong Zhang,Xuefei Cao,Xuewei Wang,Xuewei Wu,Xunnan Xu,Xutai Ma,Xuyang Wang,Yan Cui,Yang Chen,Yang Li,Yang Shu,Yang Xia,Yanjun Chen,Yanjun Zhou,Yash Mehta,Yash Patel,Yash Tekena,Yashesh Gaur,Yasmine Babaei,Yaxuan Zhou,Ye Hu,Ye Qi,Yejin Lee,Yeming Wen,Yen-Cheng Liu,Yexin Bruce Wu,Yi Pan,Yi Yang,Yi-Hui Lin,Yifan Wang,Yifan Wu,Yifan Yang,Yifei Huang,Yiftah Ben Aharon,Yilin Yang,Yiling You,Ying Xu,Ying Zhang,Yingquan Yuan,Yingru Liu,Yingyi Ma,Yining Yang,Yiting Lu,Yonatan Komornik,Yongjie Lin,Yoni Goyhman,Yossi Moran Mamo,Youngjin Nam,Yu Wang,Yu Lu,Yu Zhao,Yu-Ho Hsieh,Yu-Jung Lo,Yuandong Tian,Yuanhan Zhang,Yuanhao Xiong,Yuanshun Yao,Yuchen Hao,Yuchen Zhang,Yuchuan Li,Yue Cao,Yue Yu,Yue Zhao,Yuhan Guo,Yuhao Wang,Yuheng Huang,Yujie Lu,Yujun Shi,Yulun Wang,Yun He,Yun Wang,Yundi Qian,Yunfan Wang,Yunhao Tang,Yuning Mao,Yunlu Li,Yuqi Dai,Yuriy Hulovatyy,Yushi Hu,Yuxuan Sun,Zach Rait,Zach Wentz,Zacharie Delpierre Coudert,Zachary Collins,Zahra Hankir,Zecheng He,Zeeshan Ahmed,Zeeshan Ahmed,Zef RosnBrick,Zhan Shu,Zhanna Rohalska,Zhaoduo Wen,Zhe Liu,Zhe Liu,Zhen Qiao,Zhenggang Xu,Zhengwen Zhou,Zhengxing Chen,Zhenyu Tang,Zhichen Wu,Zhicheng Ouyang,Zhihong Lei,Zhipeng Hong,Zhiping Xiu,Zhiwei Zhao,Zhong Meng,Zhou Jin,Zhouhao Zeng,Zichang Liu,Zihang Meng,Zihuan Qiao,Zinnia Zheng,Zixi Qi,Ziyi Luo,Zoe Foulkes Birkhead,Zoey Sun,Zohar Achdut*

Main category: cs.SE

TL;DR: The paper summarizes technical details of Meta's Llama 4 model variants, architecture, training methods, benchmarks, deployment constraints, licensing, and safeguards.


<details>
  <summary>Details</summary>
Motivation: To offer researchers and practitioners a concise, fact-based reference on Llama 4's technical characteristics and deployment.

Method: Consolidation of publicly reported information regarding architecture (MoE, multimodal, long-context design), training techniques (pre-training, lightweight fine-tuning, RL-based steps), benchmark data, and deployment notes.

Result: The paper outlines model variants, architectural designs, training methodologies, evaluation benchmarks, licensing rules, and deployment constraints for precise accessibility to this information.

Conclusion: It provides a compact and well-documented reference source for users and stakeholders interested in Llama 4's technical insights and applications.

Abstract: This document consolidates publicly reported technical details about Metas Llama 4 model family. It summarizes (i) released variants (Scout and Maverick) and the broader herd context including the previewed Behemoth teacher model, (ii) architectural characteristics beyond a high-level MoE description covering routed/shared-expert structure, early-fusion multimodality, and long-context design elements reported for Scout (iRoPE and length generalization strategies), (iii) training disclosures spanning pre-training, mid-training for long-context extension, and post-training methodology (lightweight SFT, online RL, and lightweight DPO) as described in release materials, (iv) developer-reported benchmark results for both base and instruction-tuned checkpoints, and (v) practical deployment constraints observed across major serving environments, including provider-specific context limits and quantization packaging. The manuscript also summarizes licensing obligations relevant to redistribution and derivative naming, and reviews publicly described safeguards and evaluation practices. The goal is to provide a compact technical reference for researchers and practitioners who need precise, source-backed facts about Llama 4.

</details>


### [864] [From Everything-is-a-File to Files-Are-All-You-Need: How Unix Philosophy Informs the Design of Agentic AI Systems](https://arxiv.org/abs/2601.11672)
*Deepak Babu Piskala*

Main category: cs.SE

TL;DR: The paper discusses how file-like abstractions in AI systems mirror Unix's 'everything is a file' principle, promoting maintainability and robust operations.


<details>
  <summary>Details</summary>
Motivation: Examines the idea of unifying diverse resources within AI systems through file-like abstractions for operational improvements.

Method: Compares historical evolution from Unix systems to modern agentic AI practices, exploring file-and code-centric models.

Result: Identifies file-like abstractions as emerging tools in agentic AI that bolster composability and resource integration.

Conclusion: Adopting file- and code-centric models can enhance maintainability and robustness in autonomous agentic systems.

Abstract: A core abstraction in early Unix systems was the principle that 'everything is a file', enabling heterogeneous devices and kernel resources to be manipulated via uniform read/write interfaces. This paper explores how an analogous unification is emerging in contemporary agentic AI. We trace the evolution from Unix to DevOps, Infrastructure-as-Code, and finally autonomous software agents, highlighting how file-like abstractions and code-based specifications collapse diverse resources into consistent, composable interfaces. The resulting perspective suggests that adopting file- and code-centric interaction models may enable agentic systems that are more maintainable, auditable, and operationally robust.

</details>


### [865] [Semantic Caching and Intent-Driven Context Optimization for Multi-Agent Natural Language to Code Systems](https://arxiv.org/abs/2601.11687)
*Harmohit Singh*

Main category: cs.SE

TL;DR: The paper introduces a cost-optimized multi-agent system that converts natural language queries to executable Python code for data analytics, achieving high accuracy and efficiency through semantic caching, dual-threshold decision-making, and dynamic prompt assembly. It performs well with faster response time and high semantic accuracy in production.


<details>
  <summary>Details</summary>
Motivation: The motivation is to build a more cost-efficient and accurate system for converting natural language queries into executable code for structured data analytics, ensuring feasibility for large-scale production deployment.

Method: This is achieved through three key innovations: semantic caching with LLM-based equivalence detection, dual-threshold decision mechanisms for separating retrieval and generation, and dynamic prompt assembly using table-aware filtering to minimize token consumption.

Result: The system was deployed for enterprise inventory management, handling over 10,000 queries with an average latency of 8.2 seconds and achieving 94.3% semantic accuracy.

Conclusion: The paper demonstrates the feasibility of deploying a low-cost, high-performance LLM-based analytics system in large-scale production while reducing computational overhead and maintaining high accuracy.

Abstract: We present a production-optimized multi-agent system designed to translate natural language queries into executable Python code for structured data analytics. Unlike systems that rely on expensive frontier models, our approach achieves high accuracy and cost efficiency through three key innovations: (1) a semantic caching system with LLM-based equivalence detection and structured adaptation hints that provides cache hit rates of 67% on production queries; (2) a dual-threshold decision mechanism that separates exact-match retrieval from reference-guided generation; and (3) an intent-driven dynamic prompt assembly system that reduces token consumption by 40-60% through table-aware context filtering. The system has been deployed in production for enterprise inventory management, processing over 10,000 queries with an average latency of 8.2 seconds and 94.3% semantic accuracy. We describe the architecture, present empirical results from production deployment, and discuss practical considerations for deploying LLM-based analytics systems at scale.

</details>


### [866] [SpecMap: Hierarchical LLM Agent for Datasheet-to-Code Traceability Link Recovery in Systems Engineering](https://arxiv.org/abs/2601.11688)
*Vedant Nipane,Pulkit Agrawal,Amit Singh*

Main category: cs.SE

TL;DR: This paper introduces a hierarchical methodology utilizing large language models for datasheet-to-code mapping to address traceability challenges in embedded systems, showing significant improvements in mapping accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the difficulty of creating precise traceability between embedded systems datasheets and their corresponding code implementations, especially for low-level software where traditional lexical and information retrieval techniques fail.

Method: The proposed methodology uses hierarchical traceability that employs large language models for semantic analysis and narrows the search scope through structured repository inference, file relevance estimation, and symbol-level alignment.

Result: It achieves up to 73.3% file mapping accuracy on open-source embedded repositories, reducing computational overhead significantly (84% fewer tokens, 80% faster runtime).

Conclusion: The approach improves datasheet-to-code traceability and supports additional applications such as systems-aware ML model training, compliance verification, and specification coverage analysis.

Abstract: Establishing precise traceability between embedded systems datasheets and their corresponding code implementations remains a fundamental challenge in systems engineering, particularly for low-level software where manual mapping between specification documents and large code repositories is infeasible. Existing Traceability Link Recovery approaches primarily rely on lexical similarity and information retrieval techniques, which struggle to capture the semantic, structural, and symbol level relationships prevalent in embedded systems software. We present a hierarchical datasheet-to-code mapping methodology that employs large language models for semantic analysis while explicitly structuring the traceability process across multiple abstraction levels. Rather than performing direct specification-to-code matching, the proposed approach progressively narrows the search space through repository-level structure inference, file-level relevance estimation, and fine-grained symbollevel alignment. The method extends beyond function-centric mapping by explicitly covering macros, structs, constants, configuration parameters, and register definitions commonly found in systems-level C/C++ codebases. We evaluate the approach on multiple open-source embedded systems repositories using manually curated datasheet-to-code ground truth. Experimental results show substantial improvements over traditional information-retrieval-based baselines, achieving up to 73.3% file mapping accuracy. We significantly reduce computational overhead, lowering total LLM token consumption by 84% and end-to-end runtime by approximately 80%. This methodology supports automated analysis of large embedded software systems and enables downstream applications such as training data generation for systems-aware machine learning models, standards compliance verification, and large-scale specification coverage analysis.

</details>


### [867] [Technical Lag as Latent Technical Debt: A Rapid Review](https://arxiv.org/abs/2601.11693)
*Shane K. Panter,Nasir U. Eisty*

Main category: cs.SE

TL;DR: The paper studies technical lag in software systems, its detection, causes, effects, and management, proposing a vision for improved metrics and methods.


<details>
  <summary>Details</summary>
Motivation: To address the issue of deteriorating software quality due to technical lag caused by outdated dependencies, obsolete APIs, unsupported platforms, and infrastructure.

Method: Used a Rapid Review method with snowballing, analyzing peer-reviewed studies from databases like ACM Digital Library, IEEE Xplore, Scopus, and Springer.

Result: Identified the unnoticed accumulation of technical lag due to insufficient detection metrics, its negative impacts, and current management strategies like automated updates and audits.

Conclusion: Proposes improving metrics, detection methods, and studies to better address latent technical debt, aiding in maintaining large codebases with dependencies.

Abstract: Context: Technical lag accumulates when software systems fail to keep pace with technological advancements, leading to a deterioration in software quality. Objective: This paper aims to consolidate existing research on technical lag, clarify definitions, explore its detection and quantification methods, examine underlying causes and consequences, review current management practices, and lay out a vision as an indicator of passively accumulated technical debt. Method: We conducted a Rapid Review with snowballing to select the appropriate peer-reviewed studies. We leveraged the ACM Digital Library, IEEE Xplore, Scopus, and Springer as our primary source databases. Results: Technical lag accumulates passively, often unnoticed due to inadequate detection metrics and tools. It negatively impacts software quality through outdated dependencies, obsolete APIs, unsupported platforms, and aging infrastructure. Strategies to manage technical lag primarily involve automated dependency updates, continuous integration processes, and regular auditing. Conclusions: Enhancing and extending the current standardized metrics, detection methods, and empirical studies to use technical lag as an indication of accumulated latent debt can greatly improve the process of maintaining large codebases that are heavily dependent on external packages. We have identified the research gaps and outlined a future vision for researchers and practitioners to explore.

</details>


### [868] [The Stability Trap: Evaluating the Reliability of LLM-Based Instruction Adherence Auditing](https://arxiv.org/abs/2601.11783)
*Murtuza N. Shergadwala*

Main category: cs.SE

TL;DR: This paper examines the reliability of Large Language Model (LLM) judges in auditing Human Resources Generative AI applications, revealing instability in reasoning even with high verdict agreement.


<details>
  <summary>Details</summary>
Motivation: The study aims to address concerns about the reliability and scalability of LLMs in evaluating adherence to system instructions in regulated sectors like HR.

Method: The Scoped Instruction Decomposition Framework categorizes application instructions into Objective and Subjective types to analyze judge stability. Experiments tested four judge architectures on HR GenAI apps.

Result: While LLM judges exhibited over 99% verdict agreement, reasoning stability varied significantly: Objective quantitative analysis reached only 19%; Subjective evaluations ranged from 35%-83%, depending on evidence granularity.

Conclusion: High verdict agreement hides fragile reasoning reliability. Auditors should split deterministic logic to code and reserve LLM judges for complex assessments.

Abstract: The enterprise governance of Generative AI (GenAI) in regulated sectors, such as Human Resources (HR), demands scalable yet reproducible auditing mechanisms. While Large Language Model (LLM)-as-a-Judge approaches offer scalability, their reliability in evaluating adherence of different types of system instructions remains unverified. This study asks: To what extent does the instruction type of an Application Under Test (AUT) influence the stability of judge evaluations? To address this, we introduce the Scoped Instruction Decomposition Framework to classify AUT instructions into Objective and Subjective types, isolating the factors that drive judge instability. We applied this framework to two representative HR GenAI applications, evaluating the stability of four judge architectures over variable runs. Our results reveal a ``Stability Trap'' characterized by a divergence between Verdict Stability and Reasoning Stability. While judges achieved near-perfect verdict agreement ($>99\%$) for both objective and subjective evaluations, their accompanying justification traces diverged significantly. Objective instructions requiring quantitative analysis, such as word counting, exhibited reasoning stability as low as $\approx19\%$, driven by variances in numeric justifications. Similarly, reasoning stability for subjective instructions varied widely ($35\%$--$83\%$) based on evidence granularity, with feature-specific checks failing to reproduce consistent rationale. Conversely, objective instructions focusing on discrete entity extraction achieved high reasoning stability ($>90\%$). These findings demonstrate that high verdict stability can mask fragile reasoning. Thus, we suggest that auditors scope automated evaluation protocols strictly: delegate all deterministically verifiable logic to code, while reserving LLM judges for complex semantic evaluation.

</details>


### [869] [Changes in Coding Behavior and Performance Since the Introduction of LLMs](https://arxiv.org/abs/2601.11835)
*Yufan Zhang,Jaromir Savelka,Seth Goldstein,Michael Conway*

Main category: cs.SE

TL;DR: This study examines the impact of large language models like ChatGPT on student coding behavior and learning outcomes, showing potential over-reliance and reduced productivity.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand how the advent of LLMs like ChatGPT has influenced student coding behavior, learning, and instructor evaluation challenges.

Method: A quasi-longitudinal study analyzing over five years of graduate-level cloud computing student submissions, comparing behavior changes before and after ChatGPT's release.

Result: Post-Fall 2022, students show increased code submission length, higher edit distances between submissions, and reduced score improvements, indicating reduced productivity and learning.

Conclusion: The findings suggest that some students may be overly dependent on LLMs, negatively impacting learning. This underscores the need for reevaluating educational and hiring practices in the LLM era.

Abstract: The widespread availability of large language models (LLMs) has changed how students engage with coding and problem-solving. While these tools may increase student productivity, they also make it more difficult for instructors to assess students' learning and effort. In this quasi-longitudinal study, we analyze five years of student source code submissions in a graduate-level cloud computing course, focusing on an assignment that remained unchanged and examining students' behavior during the period spanning five semesters before the release of ChatGPT and five semesters after.
  Student coding behavior has changed significantly since Fall 2022. The length of their final submissions increased. Between consecutive submissions, average edit distances increased while average score improvement decreased, suggesting that both student productivity and learning have decreased after ChatGPT's release. Additionally, there are statistically significant correlations between these behavioral changes and their overall performance. Although we cannot definitively attribute them to LLM misuse, they are consistent with our hypothesis that some students are over-reliant on LLMs, which is negatively affecting their learning outcomes. Our findings raise an alarm around the first generation of graduates in the age of LLMs, calling upon both educators and employers to reflect on their evaluation methods for genuine expertise and productivity.

</details>


### [870] [Trace Validation of Unmodified Concurrent Systems with OmniLink](https://arxiv.org/abs/2601.11836)
*Finn Hackett,Evan Wrench,Peter Macko,A. Jesse Jiryu Davis,Yuanhao Wei,Ivan Beschastnikh*

Main category: cs.SE

TL;DR: OmniLink is a methodology that validates concurrent systems against TLA+ specifications effectively without requiring intrusive instrumentation, outperforming state-of-the-art tools and finding both known and previously undiscovered bugs.


<details>
  <summary>Details</summary>
Motivation: The challenge in validating concurrent systems arises from rare thread interleaving bugs and the limitations of existing tools requiring intrusive methods or idealized execution models.

Method: OmniLink determines a logical order of system actions based on timeboxes and TLA+ specifications, leveraging flexible specification languages and model-checking methods distinct from typical linearizability checking approaches.

Result: OmniLink successfully validated systems like WiredTiger, BAT, and ConcurrentQueue, leading to model improvements, detection of injected bugs, and discovery of two previously unknown bugs.

Conclusion: OmniLink presents a novel, efficient, and practical approach for validating concurrent implementations, offering better performance and robust debugging capabilities compared to existing methods.

Abstract: Concurrent systems are notoriously difficult to validate: subtle bugs may only manifest under rare thread interleavings, and existing tools often require intrusive instrumentation or unrealistic execution models. We present OmniLink, a new methodology for validating concurrent implementations against high-level specifications in TLA+. Unlike prior TLA+ based approaches which use a technique called trace validation, OmniLink treats system events as black boxes with a timebox in which they occurred and a meaning in TLA+, solving for a logical total order of actions. Unlike prior approaches based on linearizability checking, which already solves for total orders of actions with timeboxes, OmniLink uses a flexible specification language, and offers a different linearizability checking method based on off-the-shelf model checking. OmniLink offers different features compared existing linearizability checking tools, and we show that it outperforms the state of the art on large scale validation tasks.
  Our evaluation validates WiredTiger, a state-of-the-art industrial database storage layer, as well as Balanced Augmented Tree (BAT), a state-of-the art lock-free data structure from the research community, and ConcurrentQueue, a popular lock-free queue featuring aggressive performance optimizations. We use OmniLink to improve WiredTiger's existing TLA+ model, as well as develop new TLA+ models that closely match the behavior of the modeled systems, including non-linearizable behaviors. OmniLink is able to find known bugs injected into the systems under test, as well as help discover two previously unknown bugs (1 in BAT, 1 in ConcurrentQueue), which we have confirmed with the authors of those systems.

</details>


### [871] [Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces](https://arxiv.org/abs/2601.11868)
*Mike A. Merrill,Alexander G. Shaw,Nicholas Carlini,Boxuan Li,Harsh Raj,Ivan Bercovich,Lin Shi,Jeong Yeon Shin,Thomas Walshe,E. Kelly Buchanan,Junhong Shen,Guanghao Ye,Haowei Lin,Jason Poulos,Maoyu Wang,Marianna Nezhurina,Jenia Jitsev,Di Lu,Orfeas Menis Mastromichalakis,Zhiwei Xu,Zizhao Chen,Yue Liu,Robert Zhang,Leon Liangyu Chen,Anurag Kashyap,Jan-Lucas Uslu,Jeffrey Li,Jianbo Wu,Minghao Yan,Song Bian,Vedang Sharma,Ke Sun,Steven Dillmann,Akshay Anand,Andrew Lanpouthakoun,Bardia Koopah,Changran Hu,Etash Guha,Gabriel H. S. Dreiman,Jiacheng Zhu,Karl Krauth,Li Zhong,Niklas Muennighoff,Robert Amanfu,Shangyin Tan,Shreyas Pimpalgaonkar,Tushar Aggarwal,Xiangning Lin,Xin Lan,Xuandong Zhao,Yiqing Liang,Yuanli Wang,Zilong Wang,Changzhi Zhou,David Heineman,Hange Liu,Harsh Trivedi,John Yang,Junhong Lin,Manish Shetty,Michael Yang,Nabil Omi,Negin Raoof,Shanda Li,Terry Yue Zhuo,Wuwei Lin,Yiwei Dai,Yuxin Wang,Wenhao Chai,Shang Zhou,Dariush Wahdany,Ziyu She,Jiaming Hu,Zhikang Dong,Yuxuan Zhu,Sasha Cui,Ahson Saiyed,Arinbjörn Kolbeinsson,Jesse Hu,Christopher Michael Rytting,Ryan Marten,Yixin Wang,Alex Dimakis,Andy Konwinski,Ludwig Schmidt*

Main category: cs.SE

TL;DR: Terminal-Bench 2.0 offers a benchmark of 89 challenging tasks set in computer terminal environments to evaluate advanced AI agents, as existing benchmarks are either too basic or irrelevant to real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: To provide a robust benchmark that measures the capabilities of frontier AI agents in completing complex tasks, addressing the shortcomings of existing benchmarks.

Method: Curating 89 tasks inspired by real workflow problems, with unique environments, human-crafted solutions, and verification tests for evaluation.

Result: Frontier AI models and agents score below 65% on the benchmark, revealing areas for advancement through error analysis.

Conclusion: Terminal-Bench 2.0 sets a high standard for benchmarking AI agents and supports AI research by making the dataset and evaluation platform publicly available.

Abstract: AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .

</details>


### [872] [Harmonica: A Self-Adaptation Exemplar for Sustainable MLOps](https://arxiv.org/abs/2601.11926)
*Ananya Halgatti,Shaunak Biswas,Hiya Bhatt,Srinivasan Rakhunathan,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: The paper introduces Harmonica, a tool aiming to enable continuous adaptation in Machine Learning Systems (MLS) faced with runtime uncertainties.


<details>
  <summary>Details</summary>
Motivation: Address the lack of mechanisms in MLOps pipelines for handling runtime uncertainties, ensuring sustainable operation of MLS.

Method: Developed Harmonica using MAPE-K loop to separate adaptation policies and tactics, monitoring metrics, and triggering tactics for stability.

Result: Demonstrated improvement in system stability and reduced manual interventions via case studies in time series regression and computer vision.

Conclusion: Harmonica provides a reusable adaptive control framework enhancing MLS sustainability in dynamic environments.

Abstract: Machine learning enabled systems (MLS) often operate in settings where they regularly encounter uncertainties arising from changes in their surrounding environment. Without structured oversight, such changes can degrade model behavior, increase operational cost, and reduce the usefulness of deployed systems. Although Machine Learning Operations (MLOps) streamlines the lifecycle of ML models, it provides limited support for addressing runtime uncertainties that influence the longer term sustainability of MLS. To support continued viability, these systems need a mechanism that detects when execution drifts outside acceptable bounds and adjusts system behavior in response. Despite the growing interest in sustainable and self-adaptive MLS, there has been limited work towards exemplars that allow researchers to study these challenges in MLOps pipelines. This paper presents Harmonica, a self-adaptation exemplar built on the HarmonE approach, designed to enable the sustainable operation of such pipelines. Harmonica introduces structured adaptive control through MAPE-K loop, separating high-level adaptation policy from low-level tactic execution. It continuously monitors sustainability metrics, evaluates them against dynamic adaptation boundaries, and automatically triggers architectural tactics when thresholds are violated. We demonstrate the tool through case studies in time series regression and computer vision, examining its ability to improve system stability and reduce manual intervention. The results show that Harmonica offers a practical and reusable foundation for enabling adaptive behavior in MLS that rely on MLOps pipelines for sustained operation.

</details>


### [873] [Enhancing Fuzz Testing Efficiency through Automated Fuzz Target Generation](https://arxiv.org/abs/2601.11972)
*Chi Thien Tran*

Main category: cs.SE

TL;DR: This paper proposes a method for automating the generation and execution of fuzz targets through static analysis of C/C++ library source code.


<details>
  <summary>Details</summary>
Motivation: Achieving greater coverage in fuzz testing is hindered by the manual effort required to create fuzz targets for large-scale software projects and libraries.

Method: The approach involves analyzing library source code to generate fuzz targets, mapping fuzzer input to function parameters, synthesizing compilation information, and automating the collection and analysis of results.

Result: The method improves the ease and efficiency of fuzz target generation and achieves higher coverage in fuzz testing for C/C++ libraries.

Conclusion: Automating fuzz target generation through static analysis is an effective solution to significantly enhance fuzz testing capabilities in large-scale software environments.

Abstract: Fuzzing continues to be the most effective method for identifying security vulnerabilities in software. In the context of fuzz testing, the fuzzer supplies varied inputs to fuzz targets, which are designed to comprehensively exercise critical sections of the client code. Various studies have focused on optimizing and developing advanced fuzzers, such as AFL++, libFuzzer, Honggfuzz, syzkaller, ISP-Fuzzer, which have substantially enhanced vulnerability detection in widely used software and libraries. Nevertheless, achieving greater coverage necessitates improvements in both the quality and quantity of fuzz targets. In large-scale software projects and libraries -- characterized by numerous user defined functions and data types -- manual creation of fuzz targets is both labor-intensive and time-consuming. This challenge underscores the need for automated techniques not only to generate fuzz targets but also to streamline the execution and analysis of their results. In this paper, we introduce an approach to improving fuzz target generation through static analysis of library source code. The proposed method encompasses several key aspects: it analyzes source code structures to accurately construct function calls and generate fuzz targets; it maps fuzzer input data to the corresponding function parameters; it synthesizes compilation information for the fuzz targets; and it automatically collects and analyzes execution results. Our findings are demonstrated through the application of this approach to the generation of fuzz targets for C/C++ libraries.

</details>


### [874] [From LLMs to Agents in Programming: The Impact of Providing an LLM with a Compiler](https://arxiv.org/abs/2601.12146)
*Viktor Kjellberg,Miroslaw Staron,Farnaz Fotrousi*

Main category: cs.SE

TL;DR: The paper examines how integrating a compiler, specifically gcc, improves the code-generating capabilities of Large Language Models (LLMs) in software development tasks.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the shortcomings of code generated by LLMs, such as compilation errors, and investigates whether providing these models access to tools like compilers can refine their output.

Method: The authors conducted computational experiments using the RosettaCode dataset on 699 programming tasks in C. They integrated a gcc compiler with 16 different LLMs of varying sizes to assess how an active feedback mechanism impacts the generated source code.

Result: The integration of a compiler improved compilation success rates by 5.3 to 79.4 percentage units, reduced syntax errors by 75%, and undefined reference errors by 87%. Smaller models in some cases outperformed larger models when equipped with a compiler.

Conclusion: Providing LLMs access to software development tools such as compilers enhances their performance, reduces the need for larger models, and can decrease energy consumption in software engineering tasks.

Abstract: Large Language Models have demonstrated a remarkable capability in natural language and program generation and software development. However, the source code generated by the LLMs does not always meet quality requirements and may fail to compile. Therefore, many studies evolve into agents that can reason about the problem before generating the source code for the solution. The goal of this paper is to study the degree to which such agents benefit from access to software development tools, in our case, a \texttt{gcc} compiler. We conduct a computational experiment on the RosettaCode dataset, on 699 programming tasks in C. We evaluate how the integration with a compiler shifts the role of the language model from a passive generator to an active agent capable of iteratively developing runnable programs based on feedback from the compiler. We evaluated 16 language models with sizes ranging from small (135 million) to medium (3 billion) and large (70 billion). Our results show that access to a compiler improved the compilation success by 5.3 to 79.4 percentage units in compilation without affecting the semantics of the generated program. Syntax errors dropped by 75\%, and errors related to undefined references dropped by 87\% for the tasks where the agents outperformed the baselines. We also observed that in some cases, smaller models with a compiler outperform larger models with a compiler. We conclude that it is essential for LLMs to have access to software engineering tools to enhance their performance and reduce the need for large models in software engineering, such as reducing our energy footprint.

</details>


### [875] [CodeContests-O: Powering LLMs via Feedback-Driven Iterative Test Case Generation](https://arxiv.org/abs/2601.13682)
*Jianfeng Cai,Jinhua Zhu,Ruopei Sun,Kangwen Zhao,Dongyun Xue,Mingxiao Feng,Wengang Zhou,Houqiang Li*

Main category: cs.SE

TL;DR: The paper introduces a feedback-driven iterative framework to generate high-quality test cases for coding tasks using LLMs and evaluates its effectiveness on the CodeContests dataset, achieving significant improvements.


<details>
  <summary>Details</summary>
Motivation: The lack of high-quality, diverse test cases for verifying programming task solutions limits reasoning model development. Current heavy reliance on LLMs without feedback results in suboptimal test case diversity.

Method: The proposed Feedback-Driven Iterative Framework generates test cases using LLMs, evaluates them against correct and incorrect solutions, and refines them iteratively using feedback from failed executions.

Result: The framework applied to the CodeContests dataset created CodeContests-O, outperforming existing datasets with average TPR of 89.37% and TNR of 90.89%, and yielding a 9.52% improvement on coding benchmark performance.

Conclusion: Their feedback-driven approach proves effective in creating high-quality test cases, enhancing reasoning model performance while releasing CodeContests-O for the community to advance further research.

Abstract: The rise of reasoning models necessitates large-scale verifiable data, for which programming tasks serve as an ideal source. However, while competitive programming platforms provide abundant problems and solutions, high-quality test cases for verification remain scarce. Existing approaches attempt to synthesize test cases using Large Language Models (LLMs), but rely solely on the model's intrinsic generation capabilities without external feedback, frequently resulting in insufficiently diverse cases. To address this limitation, we propose a $\textbf{Feedback-Driven Iterative Framework}$ for comprehensive test case construction. Specifically, our method leverages the LLM to generate initial test cases, executes them against known correct and incorrect solutions, and utilizes the failed results as feedback to guide the LLM in refining the test cases toward high fidelity and discriminability. We then apply this method to the CodeContests dataset to construct an optimized high-quality derivative, $\textbf{CodeContests-O}$. Evaluating against the entire pool of solutions ($1.1 \times 10^7$ in total), our dataset achieves an average True Positive Rate (TPR) of $89.37\%$ and True Negative Rate (TNR) of $90.89\%$, significantly outperforming the CodeContests and CodeContests+ by margins of $4.32\%$ and $9.37\%$, respectively. Furthermore, fine-tuning the Qwen2.5-7B model on CodeContests-O results in a $9.52\%$ improvement on LiveCodeBench (Pass@1). Experiments demonstrate the effectiveness of our framework and the quality of CodeContests-O. To support reproducibility and facilitate future research, we release the $\href{https://github.com/cai-jianfeng/CodeContests-O}{code}$ and $\href{https://huggingface.co/datasets/caijanfeng/CodeContests-O}{dataset}$.

</details>


### [876] [Many Hands Make Light Work: An LLM-based Multi-Agent System for Detecting Malicious PyPI Packages](https://arxiv.org/abs/2601.12148)
*Muhammad Umar Zeshan,Motunrayo Ibiyo,Claudio Di Sipio,Phuong T. Nguyen,Davide Di Ruscio*

Main category: cs.SE

TL;DR: This paper introduces LAMPS, a multi-agent system leveraging collaborative large language models (LLMs) to identify malicious PyPI packages with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the growing threat of malicious code in open-source repositories, which traditional and rule-based tools struggle to detect due to lack of semantic understanding.

Method: The proposed method uses a multi-agent system, LAMPS, with specific agent roles, employing a fine-tuned CodeBERT model and LLaMA-3 agents for classification and contextual reasoning, evaluated on two datasets.

Result: LAMPS achieved 97.7% accuracy on a balanced dataset (D1) and 99.5% accuracy on an imbalanced, realistic dataset (D2), outperforming other state-of-the-art methods significantly.

Conclusion: The results showcase the effectiveness of distributed LLM reasoning and modular multi-agent designs for improving security in software supply chains.

Abstract: Malicious code in open-source repositories such as PyPI poses a growing threat to software supply chains. Traditional rule-based tools often overlook the semantic patterns in source code that are crucial for identifying adversarial components. Large language models (LLMs) show promise for software analysis, yet their use in interpretable and modular security pipelines remains limited. This paper presents LAMPS, a multi-agent system that employs collaborative LLMs to detect malicious PyPI packages. The system consists of four role-specific agents for package retrieval, file extraction, classification, and verdict aggregation, coordinated through the CrewAI framework. A prototype combines a fine-tuned CodeBERT model for classification with LLaMA-3 agents for contextual reasoning. LAMPS has been evaluated on two complementary datasets: D1, a balanced collection of 6,000 setup.py files, and D2, a realistic multi-file dataset with 1,296 files and natural class imbalance. On D1, LAMPS achieves 97.7% accuracy, surpassing MPHunter--one of the state-of-the-art approaches. On D2, it reaches 99.5% accuracy and 99.5% balanced accuracy, outperforming RAG-based approaches and fine-tuned single-agent baselines. McNemar's test confirmed these improvements as highly significant. The results demonstrate the feasibility of distributed LLM reasoning for malicious code detection and highlight the benefits of modular multi-agent designs in software supply chain security.

</details>


### [877] [Aletheia: What Makes RLVR For Code Verifiers Tick?](https://arxiv.org/abs/2601.12186)
*Vatsal Venkatkrishna,Indraneil Paul,Iryna Gurevych*

Main category: cs.SE

TL;DR: The study examines RLVR-trained verifiers in code generation and releases Aletheia to assess their robustness. It identifies key training components while showing the potential for simplification.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the limited use of RLVR-trained verifiers in code generation, especially where execution feedback is hard to obtain, to advance post-training tools for better output evaluation.

Method: Aletheia, an open-source testbed, is introduced as an execution-grounded framework. The study evaluates components of RLVR-based verifier training like intermediate traces, negative samples learning, and on-policy training.

Result: The research shows RLVR's efficacy but highlights opportunities to simplify the training process. Different components like on-policy learning and thinking-based training have varying importance depending on verifier size.

Conclusion: RLVR verifiers are effective for code generation, and their training can be optimized. On-policy learning is crucial for small verifiers, while thinking-based training is key for larger models.

Abstract: Multi-domain thinking verifiers trained via Reinforcement Learning from Verifiable Rewards (RLVR) are a prominent fixture of the Large Language Model (LLM) post-training pipeline, owing to their ability to robustly rate and rerank model outputs. However, the adoption of such verifiers towards code generation has been comparatively sparse, with execution feedback constituting the dominant signal. Nonetheless, code verifiers remain valuable toward judging model outputs in scenarios where execution feedback is hard to obtain and are a potentially powerful addition to the code generation post-training toolbox. To this end, we create and open-source Aletheia, a controlled testbed that enables execution-grounded evaluation of code verifiers' robustness across disparate policy models and covariate shifts. We examine components of the RLVR-based verifier training recipe widely credited for its success: (1) intermediate thinking traces, (2) learning from negative samples, and (3) on-policy training. While experiments show the optimality of RLVR, we uncover important opportunities to simplify the recipe. Particularly, despite code verification exhibiting positive training- and inference-time scaling, on-policy learning stands out as the key component at small verifier sizes, and thinking-based training emerges as the most important component at larger scales.

</details>


### [878] [Environment-Aware Code Generation: How far are We?](https://arxiv.org/abs/2601.12262)
*Tongtong Wu,Rongyi Chen,Wenjie Du,Suyu Ma,Guilin Qi,Zhenchang Xing,Shahram Khadivi,Ramesh Periyathambi,Gholamreza Haffari*

Main category: cs.SE

TL;DR: This paper studies the issue of environment-aware code generation in large language models, introducing a benchmark called VersiBCB and testing LLM adaptability for executable code in different software conditions.


<details>
  <summary>Details</summary>
Motivation: Despite improvements in code generation by LLMs, it's unclear if they can produce executable and environment-specific code due to evaluations being performed under limited and standardized conditions.

Method: The authors introduced VersiBCB, a benchmark designed for complex software environments, and tested three strategic adaptations (data, parameters, cache) for LLM-generated code.

Result: The study reveals that existing LLMs struggle significantly with environment-tailored code, but the proposed adaptations show better compatibility and executability.

Conclusion: Improved methods are necessary for LLM-based software engineering to ensure compatibility with diverse user-specific environments.

Abstract: Recent progress in large language models (LLMs) has improved code generation, but most evaluations still test isolated, small-scale code (e.g., a single function) under default or unspecified software environments. As a result, it is unclear whether LLMs can reliably generate executable code tailored to a user's specific environment. We present the first systematic study of Environment-Aware Code Generation (EACG), where generated code must be functionally correct and directly executable under arbitrary software configurations. To enable realistic evaluation, we introduce VersiBCB, a benchmark that is multi-package, execution-verified, and deprecation-aware, capturing complex and evolving environments that prior datasets often overlook. Using VersiBCB, we investigate three complementary adaptation axes: data, parameters, and cache, and develop representative strategies for each. Our results show that current LLMs struggle with environment-specific code generation, while our adaptations improve environment compatibility and executability. These findings highlight key challenges and opportunities for deploying LLMs in practical software engineering workflows.

</details>


### [879] [Leveraging Mutation Analysis for LLM-based Repair of Quantum Programs](https://arxiv.org/abs/2601.12273)
*Chihiro Yoshida,Yuta Ishimoto,Olivier Nourry,Masanari Kondo,Makoto Matsushita,Yasutaka Kamei,Yoshiki Higo*

Main category: cs.SE

TL;DR: This paper proposes a framework using large language models (LLMs) for automated program repair (APR) of quantum programs, integrating contextual information and natural language explanations.


<details>
  <summary>Details</summary>
Motivation: To address challenges in automated program repair for quantum programs, specifically low repair success rates and lack of understandable patches.

Method: Developed a framework where LLMs generate code repairs and natural language explanations while leveraging prompts configured with static information, dynamic information, and mutation analysis.

Result: Mutation analysis improved contextual understanding, leading to a repair success rate of 94.4% and enhanced explanation quality.

Conclusion: Incorporating rich contextual information such as mutation analysis into LLM-based APR frameworks can enhance efficiency, reliability, and explainability for quantum program repairs.

Abstract: In recent years, Automated Program Repair (APR) techniques specifically designed for quantum programs have been proposed. However, existing approaches often suffer from low repair success rates or poor understandability of the generated patches. In this study, we construct a framework in which a large language model (LLM) generates code repairs along with a natural language explanation of the applied repairs. To investigate how the contextual information included in prompts influences APR performance for quantum programs, we design four prompt configurations with different combinations of static information, dynamic information, and mutation analysis results. Mutation analysis evaluates how small changes to specific parts of a program affect its execution results and provides more detailed dynamic information than simple execution outputs such as stack traces. Our experimental results show that mutation analysis can provide valuable contextual information for LLM-based APR of quantum programs, improving repair success rates (achieving 94.4% in our experiment) and in some cases also improving the quality of generated explanations. Our findings point toward new directions for developing APR techniques for quantum programs that enhance both reliability and explainability.

</details>


### [880] [Hybrid Concolic Testing with Large Language Models for Guided Path Exploration](https://arxiv.org/abs/2601.12274)
*Mahdi Eslamimehr*

Main category: cs.SE

TL;DR: The paper proposes a novel integration of concolic testing and Large Language Models (LLMs) to address challenges such as path explosion and high cost in software testing.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the fundamental limitations of traditional concolic testing, including difficulty in scaling to large software systems due to path explosion and constraint solving inefficiencies.

Method: The authors present a hybrid framework where LLMs assist concolic execution through semantic reasoning to guide path exploration, prioritize paths, and optimize constraint solving.

Result: Experiments show improved branch and path coverage, reduced testing time, and better bug detection in synthetic and real-world Fintech applications compared to traditional methods.

Conclusion: The integration of LLMs with concolic testing achieves efficient program exploration and enhances bug detection capabilities, marking a significant advancement in software testing methodologies.

Abstract: Concolic testing, a powerful hybrid software testing technique, has historically been plagued by fundamental limitations such as path explosion and the high cost of constraint solving, which hinder its practical application in large-scale, real-world software systems. This paper introduces a novel algorithmic framework that synergistically integrates concolic execution with Large Language Models (LLMs) to overcome these challenges. Our hybrid approach leverages the semantic reasoning capabilities of LLMs to guide path exploration, prioritize interesting execution paths, and assist in constraint solving. We formally define the system architecture and algorithms that constitute this new paradigm. Through a series of experiments on both synthetic and real-world Fintech applications, we demonstrate that our approach significantly outperforms traditional concolic testing, random testing, and genetic algorithm-based methods in terms of branch coverage, path coverage, and time-to-coverage. The results indicate that by combining the strengths of both concolic execution and LLMs, our method achieves a more efficient and effective exploration of the program state space, leading to improved bug detection capabilities.

</details>


### [881] [The Expert Validation Framework (EVF): Enabling Domain Expert Control in AI Engineering](https://arxiv.org/abs/2601.12327)
*Lucas Gren,Felix Dobslaw*

Main category: cs.SE

TL;DR: The paper proposes an Expert Validation Framework for systematic quality assurance in Generative AI systems, aiming to bridge the gap between AI capabilities and organizational trust.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic quality assurance mechanisms that hinder enterprise deployment of GenAI systems.

Method: The authors introduce a framework centered on domain experts for specification, testing, validation, and monitoring GenAI systems through a four-stage implementation process.

Result: The framework ensures quality and maintains domain expert control over GenAI system behavior, fostering organizational trust.

Conclusion: Organizations can utilize GenAI effectively while maintaining oversight and quality standards through the expert-driven methodology.

Abstract: Generative AI (GenAI) systems promise to transform knowledge work by automating a range of tasks, yet their deployment in enterprise settings remains hindered by the lack of systematic quality assurance mechanisms. We present an Expert Validation Framework that places domain experts at the center of building software with GenAI components, enabling them to maintain authoritative control over system behavior through structured specification, testing, validation, and continuous monitoring processes. Our framework addresses the critical gap between AI capabilities and organizational trust by establishing a rigorous, expert-driven methodology for ensuring quality across diverse GenAI applications. Through a four-stage implementation process encompassing specification, system creation, validation, and production monitoring, the framework enables organizations to leverage GenAI capabilities while maintaining expert oversight and quality standards.

</details>


### [882] [Discovering 100+ Compiler Defects in 72 Hours via LLM-Driven Semantic Logic Recomposition](https://arxiv.org/abs/2601.12360)
*Xinabang He,Yuanwei Chen,Hao Wu,Jikang Zhang,Zicheng Wang,Ligeng Chen,Junjie Peng,Haiyang Wei,Yi Qian,Tiantai Zhang,Linzhang Wang,Bing Mao*

Main category: cs.SE

TL;DR: The paper introduces FeatureFuzz, a compiler fuzzing tool designed to generate programs by combining features derived from historical bug semantics. It significantly outperformed other fuzzers in identifying unique compiler crashes and bugs.


<details>
  <summary>Details</summary>
Motivation: Current compiler fuzzing techniques struggle to preserve the semantic logic of bug-triggering programs, which limits program diversity and effectiveness.

Method: FeatureFuzz uses a three-stage workflow: it extracts features representing bug-prone invariants, synthesizes groups of these features, and instantiates them into valid compiler-fuzzing programs.

Result: FeatureFuzz uncovered 167 unique crashes in 24-hour campaigns, 2.78 times more than the second-best fuzzer, and identified 106 bugs, with 76 confirmed by developers, during 72-hour campaigns.

Conclusion: FeatureFuzz demonstrates a superior ability to stress-test modern compilers and efficiently uncover bugs, validating its novel feature-based approach.

Abstract: Compilers constitute the foundational root-of-trust in software supply chains; however, their immense complexity inevitably conceals critical defects. Recent research has attempted to leverage historical bugs to design new mutation operators or fine-tune models to increase program diversity for compiler fuzzing.We observe, however, that bugs manifest primarily based on the semantics of input programs rather than their syntax. Unfortunately, current approaches, whether relying on syntactic mutation or general Large Language Model (LLM) fine-tuning, struggle to preserve the specific semantics found in the logic of bug-triggering programs. Consequently, these critical semantic triggers are often lost, resulting in a limitation of the diversity of generated programs.
  To explicitly reuse such semantics, we propose FeatureFuzz, a compiler fuzzer that combines features to generate programs. We define a feature as a decoupled primitive that encapsulates a natural language description of a bug-prone invariant, such as an out-of-bounds array access, alongside a concrete code witness of its realization. FeatureFuzz operates via a three-stage workflow: it first extracts features from historical bug reports, synthesizes coherent groups of features, and finally instantiates these groups into valid programs for compiler fuzzing.
  We evaluated FeatureFuzz on GCC and LLVM. Over 24-hour campaigns, FeatureFuzz uncovered 167 unique crashes, which is 2.78x more than the second-best fuzzer. Furthermore, through a 72-hour fuzzing campaign, FeatureFuzz identified 106 bugs in GCC and LLVM, 76 of which have already been confirmed by compiler developers, validating the approach's ability to stress-test modern compilers effectively.

</details>


### [883] [Evaluating Large Language Models for Time Series Anomaly Detection in Aerospace Software](https://arxiv.org/abs/2601.12448)
*Yang Liu,Yixing Luo,Xiaofeng Li,Xiaogang Dong,Bin Gu,Zhi Jin*

Main category: cs.SE

TL;DR: The paper introduces ATSADBench, a benchmark for aerospace time series anomaly detection (TSAD), evaluating large language models (LLMs) with new user-centric metrics and strategies like few-shot learning and retrieval-augmented generation (RAG). Findings reveal that LLMs are better at univariate tasks but struggle in multivariate scenarios.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the under-examined application of LLMs for TSAD in aerospace setups characterized by complex telemetry and inadequate evaluation frameworks.

Method: The paper offers the ATSADBench benchmark with 108,000 data points spanning nine tasks and evaluates LLMs using user-centric metrics like Alarm Accuracy (AA), Latency (AL), and Contiguity (AC), alongside enhancement techniques such as few-shot learning and retrieval-augmented generation (RAG).

Result: LLMs succeed in univariate anomaly detection but perform poorly in multivariate setups. Few-shot learning improves anomaly detection slightly, while RAG fails to yield improvements and worsens false alarms.

Conclusion: While LLMs show potential for aerospace TSAD, current models face challenges with multivariate telemetry, needing targeted improvements for robust application in aerospace software systems.

Abstract: Time series anomaly detection (TSAD) is essential for ensuring the safety and reliability of aerospace software systems. Although large language models (LLMs) provide a promising training-free alternative to unsupervised approaches, their effectiveness in aerospace settings remains under-examined because of complex telemetry, misaligned evaluation metrics, and the absence of domain knowledge. To address this gap, we introduce ATSADBench, the first benchmark for aerospace TSAD. ATSADBench comprises nine tasks that combine three pattern-wise anomaly types, univariate and multivariate signals, and both in-loop and out-of-loop feedback scenarios, yielding 108,000 data points. Using this benchmark, we systematically evaluate state-of-the-art open-source LLMs under two paradigms: Direct, which labels anomalies within sliding windows, and Prediction-Based, which detects anomalies from prediction errors. To reflect operational needs, we reformulate evaluation at the window level and propose three user-oriented metrics: Alarm Accuracy (AA), Alarm Latency (AL), and Alarm Contiguity (AC), which quantify alarm correctness, timeliness, and credibility. We further examine two enhancement strategies, few-shot learning and retrieval-augmented generation (RAG), to inject domain knowledge. The evaluation results show that (1) LLMs perform well on univariate tasks but struggle with multivariate telemetry, (2) their AA and AC on multivariate tasks approach random guessing, (3) few-shot learning provides modest gains whereas RAG offers no significant improvement, and (4) in practice LLMs can detect true anomaly onsets yet sometimes raise false alarms, which few-shot prompting mitigates but RAG exacerbates. These findings offer guidance for future LLM-based TSAD in aerospace software.

</details>


### [884] [Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition](https://arxiv.org/abs/2601.12522)
*Asif Mohammed Samir,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: This paper presents 'CogniGent,' an AI-powered agentic bug localization technique leveraging causal reasoning and dynamic debugging practices. It significantly outperformed existing methods in experimental evaluations.


<details>
  <summary>Details</summary>
Motivation: Software bugs are costly and highly time-consuming to resolve; existing bug localization methods often overlook interdependencies between code components, limiting their effectiveness.

Method: CogniGent uses multiple AI agents for call-graph-based root cause analysis, causal reasoning, and context engineering to emulate developer-inspired debugging practices and hypothesis testing.

Result: Against six established baselines, CogniGent demonstrated MAP improvements of 23.33-38.57% and MRR increases of 25.14-53.74%, achieving statistically significant results.

Conclusion: The proposed CogniGent technique addresses reasoning, dependency, and context limitations, advancing human-like debugging automation for superior bug localization performance.

Abstract: Software bugs cost technology providers (e.g., AT&T) billions annually and cause developers to spend roughly 50% of their time on bug resolution. Traditional methods for bug localization often analyze the suspiciousness of code components (e.g., methods, documents) in isolation, overlooking their connections with other components in the codebase. Recent advances in Large Language Models (LLMs) and agentic AI techniques have shown strong potential for code understanding, but still lack causal reasoning during code exploration and struggle to manage growing context effectively, limiting their capability. In this paper, we present a novel agentic technique for bug localization -- CogniGent -- that overcomes the limitations above by leveraging multiple AI agents capable of causal reasoning, call-graph-based root cause analysis and context engineering. It emulates developers-inspired debugging practices (a.k.a., dynamic cognitive debugging) and conducts hypothesis testing to support bug localization. We evaluate CogniGent on a curated dataset of 591 bug reports using three widely adopted performance metrics and compare it against six established baselines from the literature. Experimental results show that our technique consistently outperformed existing traditional and LLM-based techniques, achieving MAP improvements of 23.33-38.57% at the document and method levels. Similar gains were observed in MRR, with increases of 25.14-53.74% at both granularity levels. Statistical significance tests also confirm the superiority of our technique. By addressing the reasoning, dependency, and context limitations, CogniGent advances the state of bug localization, bridging human-like cognition with agentic automation for improved performance.

</details>


### [885] [Automated Tool Support for Category-Partition Testing: Design Decisions, UI and Examples of Use](https://arxiv.org/abs/2601.12559)
*Yvan Labiche*

Main category: cs.SE

TL;DR: This paper discusses an effort to automate the Category-Partition functional testing technique using a graphical-user-interface tool that simplifies the process of dividing input domains into sub-domains, constructing test frames, and generating test cases.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to enhance and automate the labor-intensive process of functional testing through the Category-Partition approach, thereby increasing efficiency, accuracy, and reducing human labor in generating test cases.

Method: The authors developed a graphical-user-interface tool to automate key steps of the Category-Partition technique. Users specify parameters, environment variables, categories, and choices (with constraints), and the tool processes these to construct test frames and generate test cases using automated criteria.

Result: The tool's capabilities were demonstrated successfully using nine different case studies, showcasing its ability to automate the creation of test frames and the generation of test cases based on user specifications.

Conclusion: The paper concludes that the tool significantly simplifies and automates the Category-Partition testing process by integrating specification and test-case generation into a user-friendly platform, making functional testing more efficient and consistent.

Abstract: Category-Partition is a functional testing technique that is based on the idea that the input domain of the system under test can be divided into sub-domains, with the assumption that inputs that belong to the same sub-domain trigger a similar behaviour and that therefore it is sufficient to select one input from each sub-domain. Category-Partition proceeds in several steps, from the identification of so-called categories and choices, possibly constrained, which are subsequently used to form test frames, i.e., combinations of choices, and eventually test cases. This paper reports on an ongoing attempt to automate as many of those steps as possible, with graphical-user interface tool support. Specifically, the user interface allows the user to specify parameters as well as so-called environment variables, further specify categories and choices with optional constraints. Choices are provided with precise specifications with operations specific to their types (e.g., Boolean, Integer, Real, String). Then, the tool automates the construction of test frames, which are combinations of choices, according to alternative selection criteria, and the identification of input values for parameters and environment variables for these test frames, thereby producing test cases. The paper illustrates the capabilities of the tool with the use of nine different case studies.

</details>


### [886] [OpenAI for OpenAPI: Automated generation of REST API specification via LLMs](https://arxiv.org/abs/2601.12735)
*Hao Chen,Yunchun Li,Chen Chen,Fengxu Lin,Wei Li*

Main category: cs.SE

TL;DR: The paper introduces OOPS, a technology-agnostic LLM-based method for generating OpenAPI Specifications (OAS) with high accuracy, addressing limitations of prior approaches.


<details>
  <summary>Details</summary>
Motivation: REST APIs are widely used, but developers face difficulties in creating and maintaining OAS accurately due to reliance on specific tools and frameworks.

Method: The proposed OOPS method uses an LLM-based workflow with endpoint extraction, API dependency graph creation to handle context limitations, multi-stage generation, and self-refinement to reduce errors.

Result: OOPS achieves high accuracy in OAS generation across diverse programming languages and frameworks, with F1-scores of 98% for endpoint inference, 97% for request/response inference, and 92% for parameter constraints.

Conclusion: OOPS effectively overcomes language and framework limitations to generate reliable OAS, showcasing its utility for developers working across varied technologies.

Abstract: REST APIs, based on the REpresentational State Transfer (REST) architecture, are the primary type of Web API. The OpenAPI Specification (OAS) serves as the de facto standard for describing REST APIs and is crucial for multiple software engineering tasks. However, developers face challenges in writing and maintaining OAS. Although static analysis shows potential for OAS generation, it is limited to specific programming languages and development frameworks. The powerful code understanding capabilities of LLMs offer new opportunities for OAS generation, yet they are constrained by context limitations and hallucinations. To address these challenges, we propose the OpenAI OpenAPI Project Scanner (OOPS), the first technology-agnostic LLM-based static analysis method for OAS generation, requiring fewer technology-specific rules and less human expert intervention. OOPS is implemented as an LLM agent workflow comprising two key steps: endpoint method extraction and OAS generation. By constructing an API dependency graph, it establishes necessary file associations to address LLMs' context limitations. Through multi-stage generation and self-refine, it mitigates both syntactic and semantic hallucinations during OAS generation. We evaluated OOPS on 12 real-world REST APIs spanning 5 programming languages and 8 development frameworks. Experimental results demonstrate that OOPS accurately generates high-quality OAS for REST APIs implemented with diverse technologies, achieving an average F1-score exceeding 98% for endpoint method inference, 97% for both request parameter and response inference, and 92% for parameter constraint inference. The input tokens average below 5.6K with a maximum of 16.2K, while the output tokens average below 0.9K with a maximum of 7.7K.

</details>


### [887] [Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction](https://arxiv.org/abs/2601.12762)
*Xingjie Gao,Pengcheng Huang,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Chen Qian,Ge Yu,Yu Gu*

Main category: cs.SE

TL;DR: The paper introduces ToolMaster, a framework that enables Large Language Models (LLMs) to generalize tool usage by actively learning through trial-and-execution rather than memorizing static paths.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitation of existing methods where LLMs struggle to generalize tool usage to novel or evolving tools due to reliance on static solution path memorization.

Method: ToolMaster trains LLMs using a trial-and-execution paradigm, combining teacher-generated trajectory imitation with reinforcement learning to allow autonomous exploration and learning of tool usage.

Result: ToolMaster performs significantly better than existing methods in generalization and robustness when working with unfamiliar or new tools.

Conclusion: ToolMaster shows that actively learning through interaction enables LLMs to handle unseen tools effectively, improving their practical applicability and flexibility in real-world scenarios.

Abstract: Equipping Large Language Models (LLMs) with external tools enables them to solve complex real-world problems. However, the robustness of existing methods remains a critical challenge when confronting novel or evolving tools. Existing trajectory-centric paradigms primarily rely on memorizing static solution paths during training, which limits the ability of LLMs to generalize tool usage to newly introduced or previously unseen tools. In this paper, we propose ToolMaster, a framework that shifts tool use from imitating golden tool-calling trajectories to actively learning tool usage through interaction with the environment. To optimize LLMs for tool planning and invocation, ToolMaster adopts a trial-and-execution paradigm, which trains LLMs to first imitate teacher-generated trajectories containing explicit tool trials and self-correction, followed by reinforcement learning to coordinate the trial and execution phases jointly. This process enables agents to autonomously explore correct tool usage by actively interacting with environments and forming experiential knowledge that benefits tool execution. Experimental results demonstrate that ToolMaster significantly outperforms existing baselines in terms of generalization and robustness across unseen or unfamiliar tools. All code and data are available at https://github.com/NEUIR/ToolMaster.

</details>


### [888] [Docker Does Not Guarantee Reproducibility](https://arxiv.org/abs/2601.12811)
*Julien Malka,Stefano Zacchiroli,Théo Zimmermann*

Main category: cs.SE

TL;DR: The paper investigates the reproducibility of Docker images in practice through literature review and empirical analysis of thousands of Docker builds on GitHub.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap in understanding the practical guarantees and limitations of Docker as a tool for software environment reproducibility.

Method: They conducted a systematic literature review of Docker in reproducibility discourse and performed an empirical study of 5298 Docker builds from GitHub to evaluate their reproducibility and the effectiveness of best practices.

Result: The collected Docker builds were assessed by rebuilding and comparing historical counterparts to determine reproducibility. The effectiveness of the documented best practices was also evaluated.

Conclusion: While Docker offers theoretical reproducibility, its practical effectiveness is conditional and influenced by adherence to best practices for writing reproducible Dockerfiles.

Abstract: The reproducibility of software environments is a critical concern in modern software engineering, with ramifications ranging from the effectiveness of collaboration workflows to software supply chain security and scientific reproducibility. Containerization technologies like Docker address this problem by encapsulating software environments into shareable filesystem snapshots known as images. While Docker is frequently cited in the literature as a tool that enables reproducibility in theory, the extent of its guarantees and limitations in practice remains under-explored.
  In this work, we address this gap through two complementary approaches. First, we conduct a systematic literature review to examine how Docker is framed in scientific discourse on reproducibility and to identify documented best practices for writing Dockerfiles enabling reproducible image building. Then, we perform a large-scale empirical study of 5298 Docker builds collected from GitHub workflows. By rebuilding these images and comparing the results with their historical counterparts, we assess the real reproducibility of Docker images and evaluate the effectiveness of the best practices identified in the literature.

</details>


### [889] [Automatic Generation of Formal Specification and Verification Annotations Using LLMs and Test Oracles](https://arxiv.org/abs/2601.12845)
*João Pascoal Faria,Emanuel Trigo,Vinicius Honorato,Rui Abreu*

Main category: cs.SE

TL;DR: The paper explores using LLMs for automating annotations in Dafny programs, achieving 98.2% correctness in 8 iterations or fewer.


<details>
  <summary>Details</summary>
Motivation: Automate complex and expert-demanding program annotations for formal verification through AI tools to make verification more accessible.

Method: Implemented a multimodel approach using Claude Opus 4.5 and GPT-5.2 to generate Dafny annotations from conventional code with verifier feedback.

Result: Achieved 98.2% correct annotations in 110 Dafny programs within 8 repair iterations using LLMs.

Conclusion: LLMs can effectively generate formal annotations but face challenges with proof-helpers; integrating these capabilities into IDEs improves accessibility and usability.

Abstract: Recent verification tools aim to make formal verification more accessible to software engineers by automating most of the verification process. However, annotating conventional programs with the formal specification and verification constructs (preconditions, postconditions, loop invariants, auxiliary predicates and functions and proof helpers) required to prove their correctness still demands significant manual effort and expertise. This paper investigates how LLMs can automatically generate such annotations for programs written in Dafny, a verification-aware programming language, starting from conventional code accompanied by natural language specifications (in comments) and test code. In experiments on 110 Dafny programs, a multimodel approach combining Claude Opus 4.5 and GPT-5.2 generated correct annotations for 98.2% of the programs within at most 8 repair iterations, using verifier feedback. A logistic regression analysis shows that proof-helper annotations contribute disproportionately to problem difficulty for current LLMs. Assertions in the test cases served as static oracles to automatically validate the generated pre/postconditions. We also compare generated and manual solutions and present an extension for Visual Studio Code to incorporate automatic generation into the IDE, with encouraging usability feedback.

</details>


### [890] [Efficient Code Analysis via Graph-Guided Large Language Models](https://arxiv.org/abs/2601.12890)
*Hang Gao,Tao Peng,Baoquan Cui,Hong Huang,Fengge Wu,Junsuo Zhao,Jian Zhang*

Main category: cs.SE

TL;DR: The paper proposes a method combining Graph Neural Networks (GNNs) and Large Language Models (LLMs) to better detect malicious code fragments within large codebases.


<details>
  <summary>Details</summary>
Motivation: Malicious code fragments are often hidden and hard to detect due to complex dependencies within large codebases, where even advanced LLMs struggle to provide accurate localization.

Method: The approach involves parsing code into a graph representation, encoding nodes using LLMs, and training a GNN for preliminary detection. It then backtracks predictions to localize malicious regions, enhancing LLM attention for detailed inspection.

Result: Experiments demonstrate that the method surpasses existing techniques in accuracy across various datasets, proving its effectiveness for detecting malicious input in coding projects.

Conclusion: The proposed method offers an efficient pipeline with reduced annotation costs and better performance, making it promising for real-world software security applications.

Abstract: Malicious behavior is often hidden in small, easily overlooked code fragments, especially within large and complex codebases. The cross-file dependencies of these fragments make it difficult for even powerful large language models (LLMs) to detect them reliably. We propose a graph-centric attention acquisition pipeline that enhances LLMs' ability to localize malicious behavior. The approach parses a project into a code graph, uses an LLM to encode nodes with semantic and structural signals, and trains a Graph Neural Network (GNN) under sparse supervision. The GNN performs an initial detection, and through backtracking of its predictions, identifies key code sections that are most likely to contain malicious behavior. These influential regions are then used to guide the LLM's attention for in-depth analysis. This strategy significantly reduces interference from irrelevant context while maintaining low annotation costs. Extensive experiments show that the method consistently outperforms existing methods on multiple public and self-built datasets, highlighting its potential for practical deployment in software security scenarios.

</details>


### [891] [A Benchmark for Language Models in Real-World System Building](https://arxiv.org/abs/2601.12927)
*Weilin Jin,Chenyu Zhao,Zeshun Huang,Chaoyun Zhang,Qingwei Lin,Chetan Bansal,Saravan Rajmohan,Shenglin Zhang,Yongqian Sun,Dan Pei,Yifan Wu,Tong Jia,Ying Li,Zhonghai Wu,Minghua Ma*

Main category: cs.SE

TL;DR: The paper focuses on addressing the challenges of repairing software package build failures across diverse instruction set architectures (ISAs) and languages using a new benchmark and Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the need to ensure reliable software deployment and address the common challenge of repairing software package build failures during migration across different ISAs.

Method: The authors introduce a new benchmark consisting of 268 real-world software package build failures, designed to assess LLM capabilities. They evaluate six state-of-the-art LLMs using this benchmark.

Result: The evaluation reveals that repairing software package builds across diverse ISAs remains a significant challenge, indicating that current LLMs are not yet capable of solving this problem effectively.

Conclusion: The benchmark provides a foundation for future research to advance methods for improving software portability and handling cross-ISA build failures more reliably.

Abstract: During migration across instruction set architectures (ISAs), software package build repair is a critical task for ensuring the reliability of software deployment and the stability of modern operating systems. While Large Language Models (LLMs) have shown promise in tackling this challenge, prior work has primarily focused on single instruction set architecture (ISA) and homogeneous programming languages. To address this limitation, we introduce a new benchmark designed for software package build repair across diverse architectures and languages. Comprising 268 real-world software package build failures, the benchmark provides a standardized evaluation pipeline. We evaluate six state-of-the-art LLMs on the benchmark, and the results show that cross-ISA software package repair remains difficult and requires further advances. By systematically exposing this challenge, the benchmark establishes a foundation for advancing future methods aimed at improving software portability and bridging architectural gaps.

</details>


### [892] [Beyond Accuracy: Characterizing Code Comprehension Capabilities in (Large) Language Models](https://arxiv.org/abs/2601.12951)
*Felix Mächtle,Jan-Niclas Serr,Nils Loose,Thomas Eisenbarth*

Main category: cs.SE

TL;DR: This paper proposes a diagnostic framework for evaluating Large Language Models' (LLMs) code comprehension using a binary input-output consistency task, revealing that their performance poorly correlates with traditional human-defined software complexity metrics.


<details>
  <summary>Details</summary>
Motivation: The research aims to understand whether LLMs' code comprehension is influenced by standard human-centric software metrics or unique model-specific patterns.

Method: A diagnostic framework is introduced that reframes code comprehension as a binary consistency task. The study evaluates LLMs using a large-scale dataset and compares their performance against traditional complexity metrics and shadow predictive models.

Result: The study found minimal correlation (AUROC 0.63) between traditional complexity metrics and LLM performance. Shadow models showed higher predictive accuracy with an AUROC of 0.86, revealing model-specific regularities.

Conclusion: LLM performance in code comprehension is driven by distinct model-specific patterns rather than traditional human-centric measures, requiring new benchmark methodologies and focusing on instance-level diagnostics.

Abstract: Large Language Models (LLMs) are increasingly integrated into software engineering workflows, yet current benchmarks provide only coarse performance summaries that obscure the diverse capabilities and limitations of these models. This paper investigates whether LLMs' code-comprehension performance aligns with traditional human-centric software metrics or instead reflects distinct, non-human regularities. We introduce a diagnostic framework that reframes code understanding as a binary input-output consistency task, enabling the evaluation of classification and generative models. Using a large-scale dataset, we correlate model performance with traditional, human-centric complexity metrics, such as lexical size, control-flow complexity, and abstract syntax tree structure. Our analyses reveal minimal correlation between human-defined metrics and LLM success (AUROC 0.63), while shadow models achieve substantially higher predictive performance (AUROC 0.86), capturing complex, partially predictable patterns beyond traditional software measures. These findings suggest that LLM comprehension reflects model-specific regularities only partially accessible through either human-designed or learned features, emphasizing the need for benchmark methodologies that move beyond aggregate accuracy and toward instance-level diagnostics, while acknowledging fundamental limits in predicting correct outcomes.

</details>


### [893] [ArchAgent: Scalable Legacy Software Architecture Recovery with LLMs](https://arxiv.org/abs/2601.13007)
*Rusheng Pan,Bingcheng Mao,Tianyi Ma,Zhenhua Ling*

Main category: cs.SE

TL;DR: ArchAgent combines static analysis, adaptive code segmentation, and LLM synthesis to recover architectures from large-scale legacy software, showing improved accuracy and identifying critical business logics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome challenges in recovering architectures from large-scale legacy software, including architectural drift, missing relations, and the limited context of LLMs.

Method: ArchAgent uses an agent-based framework with static analysis, adaptive code segmentation, LLM-powered synthesis, and contextual pruning. It combines cross-repository data for multiview architecture recovery and diagram generation.

Result: ArchAgent significantly improved architecture recovery accuracy in GitHub projects compared to benchmarks. It recovered business-critical logics from legacy projects effectively, supported by evaluations, ablation studies, and real-world case studies.

Conclusion: ArchAgent demonstrates its effectiveness in recovering scalable, business-aligned architectures from large-scale legacy codebases. The framework outperforms alternatives and provides access to evaluation data for further research.

Abstract: Recovering accurate architecture from large-scale legacy software is hindered by architectural drift, missing relations, and the limited context of Large Language Models (LLMs). We present ArchAgent, a scalable agent-based framework that combines static analysis, adaptive code segmentation, and LLM-powered synthesis to reconstruct multiview, business-aligned architectures from cross-repository codebases. ArchAgent introduces scalable diagram generation with contextual pruning and integrates cross-repository data to identify business-critical modules. Evaluations of typical large-scale GitHub projects show significant improvements over existing benchmarks. An ablation study confirms that dependency context improves the accuracy of generated architectures of production-level repositories, and a real-world case study demonstrates effective recovery of critical business logics from legacy projects. The dataset is available at https://github.com/panrusheng/arch-eval-benchmark.

</details>


### [894] [MeltRTL: Multi-Expert LLMs with Inference-time Intervention for RTL Code Generation](https://arxiv.org/abs/2601.13015)
*Nowfel Mashnoor,Mohammad Akyash,Hadi Kamali,Kimia Azar*

Main category: cs.SE

TL;DR: MeltRTL is a novel framework aimed at improving RTL code generation by combining multi-expert attention and inference-time interventions without retraining the model, showing notable improvements in synthesizability and correctness.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based solutions for RTL code generation struggle with producing syntactically and functionally accurate code for complex hardware, prompting the need for more accurate methods.

Method: MeltRTL employs a multi-expert attention system for targeting design categories and inference-time interventions (ITI) for correcting errors during code generation, significantly improving outcomes without retraining the base model.

Result: MeltRTL achieves 96% synthesizability and 60% functional correctness on VerilogEval, outperforming base LLM by 10.7% and 14.7%, with only 27% computational overhead.

Conclusion: MeltRTL offers a deployable solution for improving LLM-based RTL code generation with its complementary combination of multi-expert attention and ITI, optimized for immediate use on pre-trained LLMs.

Abstract: The automated generation of hardware register-transfer level (RTL) code with large language models (LLMs) shows promise, yet current solutions struggle to produce syntactically and functionally correct code for complex digital designs. This paper introduces MeltRTL, a novel framework that integrates multi-expert attention with inference-time intervention (ITI) to significantly improve LLM-based RTL code generation accuracy without retraining the base model. MeltRTL introduces three key innovations: (1) A multi-expert attention architecture that dynamically routes design specifications to specialized expert networks, enabling targeted reasoning across various hardware categories; (2) An inference-time intervention mechanism that employs non-linear probes to detect and correct hardware-specific inaccuracies during generation; and (3) An efficient intervention framework that selectively operates on expert-specific attention heads with minimal computational overhead. We evaluate MeltRTL on the VerilogEval benchmark, achieving 96% synthesizability and 60% functional correctness, compared to the base LLM's 85.3% and 45.3%, respectively. These improvements are obtained entirely at inference time, with only 27% computational overhead and no model fine-tuning, making MeltRTL immediately deployable on existing pre-trained LLMs. Ablation studies further show the complementary benefits of multi-expert architecture and ITI, highlighting their synergistic effects when combined.

</details>


### [895] [RM -RF: Reward Model for Run-Free Unit Test Evaluation](https://arxiv.org/abs/2601.13097)
*Elena Bruches,Daniil Grebenkin,Mikhail Klementev,Vadim Alperovich,Roman Derunets,Dari Baturova,Georgy Mkrtchyan,Oleg Sedukhin,Ivan Bondarenko,Nikolay Bushkov,Stanislav Moiseev*

Main category: cs.SE

TL;DR: RM-RF is a predictive model that evaluates unit tests without execution, achieving competitive accuracy while significantly reducing costs and latency.


<details>
  <summary>Details</summary>
Motivation: To reduce the time and resource costs associated with running and compiling test cases during automatic test generation, focusing on efficient and scalable evaluation methods.

Method: The researchers developed RM-RF, a multilingual reward model trained on execution-labeled datasets to predict execution signals such as compilation success, code coverage improvement, and mutation kill rates, without actual execution.

Result: The model achieved an average F1 score of 0.69 across targets using various tuning approaches and provides faster, cost-effective feedback compared to traditional compile-and-run evaluation methods.

Conclusion: RM-RF successfully demonstrates that execution-independent prediction offers a viable and efficient alternative for evaluating generated unit tests in test-generation frameworks.

Abstract: We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.

</details>


### [896] [Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization](https://arxiv.org/abs/2601.13118)
*Alessandro Midolo,Alessandro Giagnorio,Fiorella Zampetti,Rosalia Tufano,Gabriele Bavota,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: The paper focuses on optimizing prompts specifically for code generation tasks using Large Language Models (LLMs), presenting 10 guidelines derived from iterative, test-driven processes.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of specific guidelines for crafting suitable prompts for code generation using LLMs, to enhance developer productivity.

Method: The paper uses an iterative, test-driven approach to refine code generation prompts, analyzing test outcomes to identify patterns for effective prompts, which were then developed into 10 improvement guidelines.

Result: The study identifies 10 concrete guidelines for optimizing prompts and evaluates them with 50 practitioners, highlighting both their actual usage and practitioners' perceived utility.

Conclusion: The guidelines hold significant implications for improving the effectiveness of LLM-aided software development, benefiting developers, educators, and future tool designers.

Abstract: Large Language Models (LLMs) are nowadays extensively used for various types of software engineering tasks, primarily code generation. Previous research has shown how suitable prompt engineering could help developers in improving their code generation prompts. However, so far, there do not exist specific guidelines driving developers towards writing suitable prompts for code generation. In this work, we derive and evaluate development-specific prompt optimization guidelines. First, we use an iterative, test-driven approach to automatically refine code generation prompts, and we analyze the outcome of this process to identify prompt improvement items that lead to test passes. We use such elements to elicit 10 guidelines for prompt improvement, related to better specifying I/O, pre-post conditions, providing examples, various types of details, or clarifying ambiguities. We conduct an assessment with 50 practitioners, who report their usage of the elicited prompt improvement patterns, as well as their perceived usefulness, which does not always correspond to the actual usage before knowing our guidelines. Our results lead to implications not only for practitioners and educators, but also for those aimed at creating better LLM-aided software development tools.

</details>


### [897] [Earth Embeddings as Products: Taxonomy, Ecosystem, and Standardized Access](https://arxiv.org/abs/2601.13134)
*Heng Fang,Adam J. Stewart,Isaac Corley,Xiao Xiang Zhu,Hossein Azizpour*

Main category: cs.SE

TL;DR: High compute costs limit the usage of Geospatial Foundation Models (GFMs). Pre-computed embedding data offers a solution, but lacks standardization across formats and resolutions. The paper proposes a unified API in TorchGeo for streamlined and transparent usage.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of high computational costs and fragmented ecosystem for GFMs, which prevent model comparison and data interoperability.

Method: It categorizes the ecosystem into a three-layer taxonomy (Data, Tools, Value), identifies interoperability barriers, and extends TorchGeo with a unified API for standardized usage of embedding products.

Result: A unified API in TorchGeo ensures seamless loading, querying, and analysis of embedding products as geospatial datasets, improving accessibility and reproducibility.

Conclusion: The proposed approach decouples downstream analysis from specific models, fostering transparent workflows and interoperability within Earth observation studies.

Abstract: Geospatial Foundation Models (GFMs) provide powerful representations, but high compute costs hinder their widespread use. Pre-computed embedding data products offer a practical "frozen" alternative, yet they currently exist in a fragmented ecosystem of incompatible formats and resolutions. This lack of standardization creates an engineering bottleneck that prevents meaningful model comparison and reproducibility. We formalize this landscape through a three-layer taxonomy: Data, Tools, and Value. We survey existing products to identify interoperability barriers. To bridge this gap, we extend TorchGeo with a unified API that standardizes the loading and querying of diverse embedding products. By treating embeddings as first-class geospatial datasets, we decouple downstream analysis from model-specific engineering, providing a roadmap for more transparent and accessible Earth observation workflows.

</details>


### [898] [From Human to Machine Refactoring: Assessing GPT-4's Impact on Python Class Quality and Readability](https://arxiv.org/abs/2601.13139)
*Alessandro Midolo,Emiliano Tramontana,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: This paper evaluates GPT-4o for automated code refactoring on Python classes, focusing on behavioral correctness, code quality, and readability.


<details>
  <summary>Details</summary>
Motivation: To explore the capabilities and limitations of LLMs like GPT-4o in automated software refactoring, addressing gaps in practical applicability and effects on code quality.

Method: Conducted an empirical study on 100 Python classes using GPT-4o for various class-level refactorings and evaluated the outcomes on correctness, code quality, and readability.

Result: GPT-4o successfully performs behavior-preserving refactorings that improve code quality and reduce code smells but at the expense of reduced readability.

Conclusion: LLMs like GPT-4o show promise for improving code quality via automated refactorings, but challenges in readability and practical integration remain.

Abstract: Refactoring is a software engineering practice that aims to improve code quality without altering program behavior. Although automated refactoring tools have been extensively studied, their practical applicability remains limited. Recent advances in Large Language Models (LLMs) have introduced new opportunities for automated code refactoring. The evaluation of such an LLM-driven approach, however, leaves unanswered questions about its effects on code quality. In this paper, we present a comprehensive empirical study on LLM-driven refactoring using GPT-4o, applied to 100 Python classes from the ClassEval benchmark. Unlike prior work, our study explores a wide range of class-level refactorings inspired by Fowler's catalog and evaluates their effects from three complementary perspectives: (i) behavioral correctness, verified through unit tests; (ii) code quality, assessed via Pylint, Flake8, and SonarCloud; and (iii) readability, measured using a state-of-the-art readability tool. Our findings show that GPT-4o generally produces behavior-preserving refactorings that reduce code smells and improve quality metrics, albeit at the cost of decreased readability. Our results provide new evidence on the capabilities and limitations of LLMs in automated software refactoring, highlighting directions for integrating LLMs into practical refactoring workflows.

</details>


### [899] [KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?](https://arxiv.org/abs/2601.13240)
*Xue Jiang,Jiaru Qian,Xianjie Shi,Chenjie Li,Hao Zhu,Ziyu Wang,Jielun Zhang,Zheyu Zhao,Kechi Zhang,Jia Li,Wenpin Jiao,Zhi Jin,Ge Li,Yihong Dong*

Main category: cs.SE

TL;DR: The paper introduces KOCO-BENCH, a benchmark designed for evaluating how well large language models (LLMs) specialize in domain-specific software development, highlighting the challenges faced by current LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in evaluating LLMs for domain-specific knowledge application, as existing benchmarks assess knowledge possession but not the acquisition and application of domain expertise.

Method: The authors develop KOCO-BENCH, which includes curated knowledge corpora from six emerging domains and evaluates LLMs through tasks like domain-specific code generation and knowledge understanding. This benchmark requires LLMs to acquire and apply domain knowledge from the corpora.

Result: State-of-the-art LLMs, even with domain specialization methods like SFT and RAG, exhibit limited performance on KOCO-BENCH, with the best model achieving only 34.2%.

Conclusion: KOCO-BENCH highlights the challenges LLMs face in domain-specific programming and underscores the need for more effective domain specialization methods.

Abstract: Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at https://github.com/jiangxxxue/KOCO-bench.

</details>


### [900] [SEER: Spectral Entropy Encoding of Roles for Context-Aware Attention-Based Design Pattern Detection](https://arxiv.org/abs/2601.13334)
*Tarik Houichime,Younes El Amrani*

Main category: cs.SE

TL;DR: This paper introduces SEER, an improved method for detecting Gang of Four design patterns in source code, offering better role disambiguation and temporal context than its predecessor.


<details>
  <summary>Details</summary>
Motivation: The study aimed to improve the limitations of the prior technique called Context Is All You Need, which struggled with explicit role disambiguation and uniform treatment of call edges in software code models.

Method: SEER introduces two key innovations: (i) a spectral-entropy role encoder for deriving explicit role embeddings based on class interaction graphs and (ii) a time-weighted calling context that incorporates duration priors for method types. These optimizations are integrated with Transformer sequence encoders.

Result: Compared to the older method, SEER achieved improved macro-F1 scores (from 92.47% to 93.20%), accuracy gains (92.52% to 93.98%), reduced false positives by 20%, and enhanced interpretability and reliability.

Conclusion: SEER sharpens detection of GoF design patterns through better role modeling and context calibration. Its compatibility across programming languages and robustness make it a practical enhancement over the previous method for software code analysis.

Abstract: This paper presents SEER, an upgraded version of our prior method Context Is All You Need for detecting Gang of Four (GoF) design patterns from source code. The earlier approach modeled code as attention-ready sequences that blended lightweight structure with behavioral context; however, it lacked explicit role disambiguation within classes and treated call edges uniformly. SEER addresses these limitations with two principled additions: (i) a spectral-entropy role encoder that derives per-member role embeddings from the Laplacian spectrum of each class's interaction graph, and (ii) a time-weighted calling context that assigns empirically calibrated duration priors to method categories (e.g., constructors, getters/setters, static calls, virtual dispatch, cloning). Together, these components sharpen the model's notion of "who does what" and "how much it matters," while remaining portable across languages with minimal adaptation and fully compatible with Transformer-based sequence encoders. Importantly, SEER does not "force" a win by capacity or data; it nudges the classifier, steering attention toward role-consistent and temporally calibrated signals that matter most. We evaluate SEER on PyDesignNet (1,832 files, 35,000 sequences, 23 GoF patterns) and observe consistent gains over our previous system: macro-F1 increases from 92.47% to 93.20% and accuracy from 92.52% to 93.98%, with macro-precision 93.98% and macro-recall 92.52%. Beyond aggregate metrics, SEER reduces false positives by nearly 20%, a decisive improvement that strengthens its robustness and practical reliability. Moreover, SEER yields interpretable, symbol-level attributions aligned with canonical roles, exhibits robustness under small graph perturbations, and shows stable calibration.

</details>


### [901] [From Completion to Editing: Unlocking Context-Aware Code Infilling via Search-and-Replace Instruction Tuning](https://arxiv.org/abs/2601.13384)
*Jiajun Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Yuheng Jing,Zeyao Ma,Tianyi Bai,Zilei Wang,Qiang Liu,Liang Wang,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: The paper introduces the Search-and-Replace Infilling (SRI) framework for code completion, which combines verification and dynamic context-aware editing processes in a single-pass inference for enhanced performance and flexibility.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current Fill-in-the-Middle (FIM) code completion paradigms, such as inability to correct contextual errors and reliance on unaligned models, as well as challenges with Chat LLMs and agentic workflows like latency and performance issues.

Method: Proposed Search-and-Replace Infilling (SRI) framework involves an explicit search phase to ground edits efficiently and enables dynamic context-aware editing. It includes fine-tuning Chat LLMs using a new dataset called SRI-200K.

Result: SRI-Coder models achieve superior code completion performance compared to Base models using minimal training data (20k samples). The approach preserves general coding competencies and maintains latency comparable to standard FIM.

Conclusion: SRI framework expands the static infilling approach to a dynamic context-aware editing paradigm, improving code completion capabilities without sacrificing latency while preserving model performance. The framework has the potential to advance auto-completion and assisted development tools.

Abstract: The dominant Fill-in-the-Middle (FIM) paradigm for code completion is constrained by its rigid inability to correct contextual errors and reliance on unaligned, insecure Base models. While Chat LLMs offer safety and Agentic workflows provide flexibility, they suffer from performance degradation and prohibitive latency, respectively. To resolve this dilemma, we propose Search-and-Replace Infilling (SRI), a framework that internalizes the agentic verification-and-editing mechanism into a unified, single-pass inference process. By structurally grounding edits via an explicit search phase, SRI harmonizes completion tasks with the instruction-following priors of Chat LLMs, extending the paradigm from static infilling to dynamic context-aware editing. We synthesize a high-quality dataset, SRI-200K, and fine-tune the SRI-Coder series. Extensive evaluations demonstrate that with minimal data (20k samples), SRI-Coder enables Chat models to surpass the completion performance of their Base counterparts. Crucially, unlike FIM-style tuning, SRI preserves general coding competencies and maintains inference latency comparable to standard FIM. We empower the entire Qwen3-Coder series with SRI, encouraging the developer community to leverage this framework for advanced auto-completion and assisted development.

</details>


### [902] [A Tool for Automatically Cataloguing and Selecting Pre-Trained Models and Datasets for Software Engineering](https://arxiv.org/abs/2601.13460)
*Alexandra González,Oscar Cerezo,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: Presents MLAssetSelection, a web app for identifying machine learning models/datasets for SE tasks.


<details>
  <summary>Details</summary>
Motivation: Software engineers struggle to find relevant machine learning models and datasets due to the overwhelming growth of machine learning assets and inefficiency of browsing large registries.

Method: Developed a web application named MLAssetSelection with features like model rankings, requirements-based selection, real-time updates, and user customizations.

Result: Successfully implemented MLAssetSelection, effectively aiding engineers in selecting relevant models and datasets while saving time.

Conclusion: MLAssetSelection streamlines the process of finding relevant machine learning assets, improving efficiency for software engineers and providing a robust system tailored to their needs.

Abstract: The rapid growth of machine learning assets has made it increasingly difficult for software engineers to identify models and datasets that match their specific needs. Browsing large registries, such as Hugging Face, is time-consuming, error-prone, and rarely tailored to Software Engineering (SE) tasks. We present MLAssetSelection, a web application that automatically extracts SE assets and supports four key functionalities: (i) a configurable leaderboard for ranking models across multiple benchmarks and metrics; (ii) requirements-based selection of models and datasets; (iii) real-time automated updates through scheduled jobs that keep asset information current; and (iv) user-centric features including login, personalized asset lists, and configurable alert notifications. A demonstration video is available at https://youtu.be/t6CJ6P9asV4.

</details>


### [903] [Governance Matters: Lessons from Restructuring the data.table OSS Project](https://arxiv.org/abs/2601.13466)
*Pedro Oliveira,Doris Amoakohene,Toby Hocking,Marco Gerosa,Igor Steinmacher*

Main category: cs.SE

TL;DR: The paper analyzes the governance reform of the data.table R package, presenting its positive impact on scalability, sustainability, and community involvement.


<details>
  <summary>Details</summary>
Motivation: To address operational risks and scalability challenges caused by informal or centralized governance in open source software projects like data.table.

Method: A mixed-methods approach involving a survey of contributors (n=17) and data analysis of project repositories, focusing on pre- and post-governance reform.

Result: The reform led to a 200% increase in new contributors, reduced pull request resolution time from 700 days to under a week, and tripled contributor retention, improving community perception on transparency and project momentum.

Conclusion: OSS projects can benefit greatly from governance reform, with implications for scalability and community engagement, though fairness and conflict resolution should still be addressed.

Abstract: Open source software (OSS) forms the backbone of industrial data workflows and enterprise systems. However, many OSS projects face operational risks due to informal or centralized governance. This paper presents a practical case study of data.table, a high-performance R package widely adopted in production analytics pipelines, which underwent a community-led governance reform to address scalability and sustainability concerns. Before the reform, data.table faced a growing backlog of unresolved issues and open pull requests, unclear contributor pathways, and bottlenecks caused by reliance on a single core maintainer. In response, the community initiated a redesign of its governance structure. In this paper, we evaluated the impact of this transition through a mixed-methods approach, combining a contributor survey (n=17) with mining project repository data. Our results show that following the reform, the project experienced a 200% increase in new contributor recruitment, a drop in pull request resolution time from over 700 days to under a week, and a 3x increase in contributor retention. Community sentiment improved around transparency, onboarding, and project momentum, though concerns around fairness and conflict resolution remain. This case study provides practical guidance for maintainers, companies, and foundations seeking to enhance OSS governance.

</details>


### [904] [AI IDEs or Autonomous Agents? Measuring the Impact of Coding Agents on Software Development](https://arxiv.org/abs/2601.13597)
*Shyam Agarwal,Hao He,Bogdan Vasilescu*

Main category: cs.SE

TL;DR: The paper evaluates the impact of large language model (LLM)-based coding agents on software projects. It finds temporary velocity gains but significant persistent quality risks even in the presence of prior IDE-based AI tools.


<details>
  <summary>Details</summary>
Motivation: The rise of LLM-based coding agents prompts a need to understand their impact on software development, particularly in terms of development velocity and software quality, and how they compare to existing IDE-based AI tools.

Method: The study employed a longitudinal causal approach using staggered difference-in-differences with matched controls, analyzing open-source repositories from the AIDev dataset. The metrics tracked include commits, lines added, static-analysis warnings, cognitive complexity, duplication, and comment density.

Result: Repositories using coding agents experienced large, short-term velocity benefits only when no prior AI IDEs were used. However, there were persistent quality risks such as increased static-analysis warnings (18%) and cognitive complexity (35%).

Conclusion: While LLM-based agents can initially boost productivity, they contribute to long-term software complexity and quality concerns, advocating for cautious use with quality safeguards and selective deployment.

Abstract: Large language model (LLM)-based coding agents increasingly act as autonomous contributors that generate and merge pull requests, yet their real-world effects on software projects are unclear, especially relative to widely adopted IDE-based AI assistants. We present a longitudinal causal study of agent adoption in open-source repositories using staggered difference-in-differences with matched controls. Using the AIDev dataset, we define adoption as the first agent-generated pull request and analyze monthly repository-level outcomes spanning development velocity (commits, lines added) and software quality (static-analysis warnings, cognitive complexity, duplication, and comment density). Results show large, front-loaded velocity gains only when agents are the first observable AI tool in a project; repositories with prior AI IDE usage experience minimal or short-lived throughput benefits. In contrast, quality risks are persistent across settings, with static-analysis warnings and cognitive complexity rising roughly 18% and 35%, indicating sustained agent-induced complexity debt even when velocity advantages fade. These heterogeneous effects suggest diminishing returns to AI assistance and highlight the need for quality safeguards, provenance tracking, and selective deployment of autonomous agents. Our findings establish an empirical basis for understanding how agentic and IDE-based tools interact, and motivate research on balancing acceleration with maintainability in AI-integrated development workflows.

</details>


### [905] [Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs](https://arxiv.org/abs/2601.13655)
*Guangba Yu,Zirui Wang,Yujie Huang,Renyi Zhong,Yuedong Zhong,Yilun Wang,Michael R. Lyu*

Main category: cs.SE

TL;DR: The paper examines deployment reliability issues in the open-source LLM ecosystem via an analysis of 705 failures, identifying systemic challenges rather than algorithmic defects.


<details>
  <summary>Details</summary>
Motivation: To address reliability challenges faced by users deploying fine-tuned LLMs on local infrastructure rather than consuming black-box APIs.

Method: The study conducts a large-scale empirical analysis on failure cases within DeepSeek, Llama, and Qwen open-source ecosystems.

Result: Findings highlight key phenomena including diagnostic divergence, systemic homogeneity, and lifecycle escalation, all contributing to reliability bottlenecks during deployment.

Conclusion: Improving infrastructure reliability, especially during fine-tuning and inference, can significantly enhance the usability of open-source LLMs.

Abstract: The democratization of open-source Large Language Models (LLMs) allows users to fine-tune and deploy models on local infrastructure but exposes them to a First Mile deployment landscape. Unlike black-box API consumption, the reliability of user-managed orchestration remains a critical blind spot. To bridge this gap, we conduct the first large-scale empirical study of 705 real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems.
  Our analysis reveals a paradigm shift: white-box orchestration relocates the reliability bottleneck from model algorithmic defects to the systemic fragility of the deployment stack. We identify three key phenomena: (1) Diagnostic Divergence: runtime crashes distinctively signal infrastructure friction, whereas incorrect functionality serves as a signature for internal tokenizer defects. (2) Systemic Homogeneity: Root causes converge across divergent series, confirming reliability barriers are inherent to the shared ecosystem rather than specific architectures. (3) Lifecycle Escalation: Barriers escalate from intrinsic configuration struggles during fine-tuning to compounded environmental incompatibilities during inference. Supported by our publicly available dataset, these insights provide actionable guidance for enhancing the reliability of the LLM landscape.

</details>


### [906] [A Blockchain-Oriented Software Engineering Architecture for Carbon Credit Certification Systems](https://arxiv.org/abs/2601.13772)
*Matteo Vaccargiu,Azmat Ullah,Pierluigi Gallo*

Main category: cs.SE

TL;DR: The paper proposes a blockchain-based system that ensures reliable carbon-credit certification by integrating IoT and edge-level data aggregation, using a photovoltaic system as a case study.


<details>
  <summary>Details</summary>
Motivation: To address the lack of effective certification processes for emissions reductions, especially for small-to-medium-scale renewable energy installations, and align the certification mechanisms with European legislation and carbon-market standards.

Method: The authors designed a permissioned blockchain architecture with smart contracts. It uses real-time IoT data collection, edge-level data aggregation, and secure on-chain storage to enable verifiable and structured carbon-credit certification.

Result: The proposed system was demonstrated on a 100 kWp photovoltaic installation, proving it could reliably generate verifiable carbon-credit records while meeting legislative and market requirements.

Conclusion: This system offers a scalable and reliable approach to carbon-credit certification, aligning with legislative standards and enabling photovoltaic operators to support third-party verification effectively.

Abstract: Carbon credit systems have emerged as a policy tool to incentivize emission reductions and support the transition to clean energy. Reliable carbon-credit certification depends on mechanisms that connect actual, measured renewable-energy production to verifiable emission-reduction records. Although blockchain and IoT technologies have been applied to emission monitoring and trading, existing work offers limited support for certification processes, particularly for small and medium-scale renewable installations. This paper introduces a blockchain-based carbon-credit certification architecture, demonstrated through a 100 kWp photovoltaic case study, that integrates real-time IoT data collection, edge-level aggregation, and secure on-chain storage on a permissioned blockchain with smart contracts. Unlike approaches focused on trading mechanisms, the proposed system aligns with European legislation and voluntary carbon-market standards, clarifying the practical requirements and constraints that apply to photovoltaic operators. The resulting architecture provides a structured pathway for generating verifiable carbon-credit records and supporting third-party verification.

</details>


### [907] [SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories](https://arxiv.org/abs/2601.13713)
*Aditya Bharat Soni,Rajat Ghosh,Vaishnavi Bhargava,Valerie Chen,Debojyoti Dutta*

Main category: cs.SE

TL;DR: SWE-Tester is proposed for training open-source LLMs to generate issue reproduction tests, showcasing improved success rates and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Current methods for generating issue reproduction tests rely heavily on closed-source LLMs, which limits exploration and innovation in open models.

Method: A new pipeline, SWE-Tester, is introduced, which involves curating a large dataset of 41K instances and fine-tuning open-source LLMs of varying sizes and families.

Result: The fine-tuned models show up to 10% absolute improvement in success rate and 21% improvement in change coverage on SWT-Bench Verified.

Conclusion: The framework demonstrates the potential of open-source LLMs in generating issue reproduction tests, with improvements achieved through larger models, better data, and increased compute.

Abstract: Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- "test first, write code later", and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\% in success rate and 21\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.

</details>


### [908] [Counterexample Classification against Signal Temporal Logic Specifications](https://arxiv.org/abs/2601.13743)
*Zhenya Zhang,Parv Kapoor,Jie An,Eunsuk Kang*

Main category: cs.SE

TL;DR: This paper proposes a method to classify counterexamples in hybrid systems using parametric signal temporal logic (PSTL), enabling efficient identification and understanding of counterexample patterns.


<details>
  <summary>Details</summary>
Motivation: In hybrid systems, counterexamples arising from violating signal temporal logic (STL) specifications can stem from various causes, making their classification essential for diagnosing and addressing system defects effectively.

Method: The paper introduces a classification criterion using PSTL to represent counterexample classes. It includes deriving an inclusion relation among classes and applying a binary search-like approach to reduce the number of queries during class identification.

Result: A prototype tool was developed to implement the proposed method, which was validated experimentally on two well-studied hybrid systems, demonstrating its efficiency and effectiveness.

Conclusion: The proposed PSTL-based classification framework significantly enhances the understanding and identification of counterexamples in hybrid systems, proving to be a valuable diagnostic tool.

Abstract: Signal Temporal Logic (STL) has been widely adopted as a specification language for specifying desirable behaviors of hybrid systems. By monitoring a given STL specification, we can detect the executions that violate it, which are often referred to as counterexamples. In practice, these counterexamples may arise from different causes and thus are relevant to different system defects. To effectively address this, we need a proper criterion for classifying these counterexamples, by which we can comprehend the possible violation patterns and the distributions of these counterexamples with respect to the patterns. In this paper, we propose a classification criterion by using parametric signal temporal logic (PSTL) to represent each class. Due to this formalism, identifying the classes of a counterexample requires finding proper parameter values of PSTL that enable a class to include the counterexample. To improve the efficiency of class identification, we further derive an inclusion relation between different classes, and then propose a binary search-like approach over it that significantly prunes the classes needed to query. We implement a prototype tool and experimentally evaluate its effectiveness on two widely-studied systems.

</details>


### [909] [On Autopilot? An Empirical Study of Human-AI Teaming and Review Practices in Open Source](https://arxiv.org/abs/2601.13754)
*Haoyu Gao,Peerachai Banyongrakkul,Hao Guan,Mansooreh Zahedi,Christoph Treude*

Main category: cs.SE

TL;DR: The paper investigates interaction patterns in AI-assisted pull requests (PRs) using the expanded AIDev dataset, highlighting faster merges but limited feedback for PRs from non-owner AI contributions.


<details>
  <summary>Details</summary>
Motivation: To understand how developers interact with AI-assisted pull requests and the implications of AI as a teammate in open-source software projects.

Method: The researchers expanded the AIDev dataset to include finer-grained data on code ownership and compared PRs created by humans versus AI-co-authored PRs.

Result: Findings show 67.5% of AI-co-authored PRs come from contributors without prior code ownership, and these PRs are merged significantly faster with minimal feedback or reviews, especially for non-owner contributors.

Conclusion: The study reveals distinct patterns in AI-assisted coding, where faster merges and less feedback for AI-related PRs suggest potential implications for project management and collaboration practices.

Abstract: Large Language Models (LLMs) increasingly automate software engineering tasks. While recent studies highlight the accelerated adoption of ``AI as a teammate'' in Open Source Software (OSS), developer interaction patterns remain under-explored. In this work, we investigated project-level guidelines and developers' interactions with AI-assisted pull requests (PRs) by expanding the AIDev dataset to include finer-grained contributor code ownership and a comparative baseline of human-created PRs. We found that over 67.5\% of AI-co-authored PRs originate from contributors without prior code ownership. Despite this, the majority of repositories lack guidelines for AI-coding agent usage. Notably, we observed a distinct interaction pattern: AI-co-authored PRs are merged significantly faster with minimal feedback. In contrast to human-created PRs where non-owner developers receive the most feedback, AI-co-authored PRs from non-owners receive the least, with approximately 80\% merged without any explicit review. Finally, we discuss implications for developers and researchers.

</details>


### [910] [Multi-Location Software Model Completion](https://arxiv.org/abs/2601.13894)
*Alisa Welter,Christof Tinnes,Sven Apel*

Main category: cs.SE

TL;DR: This paper proposes 'NextFocus,' a novel model capable of predicting multi-location changes in software models through global embeddings and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing LLM-based approaches for software model completion, which are restricted to single-location changes, even though software models evolve with complex multi-location modifications.

Method: The authors developed NextFocus, a neural network with attention mechanisms trained on historical software model evolution data. It predicts multi-location changes in software models starting from an initial change.

Result: NextFocus achieved a Precision@k score of 0.98 for k ≤ 10, outperforming three baseline approaches. It successfully predicts complex, multi-location model modifications.

Conclusion: NextFocus demonstrates potential for automating complex software model changes across multiple locations, offering significant steps toward enhanced software model management and engineering automation.

Abstract: In model-driven engineering and beyond, software models are key development artifacts. In practice, they often grow to substantial size and complexity, undergoing thousands of modifications over time due to evolution, refactoring, and maintenance. The rise of AI has sparked interest in how software modeling activities can be automated. Recently, LLM-based approaches for software model completion have been proposed, however, the state of the art supports only single-location model completion by predicting changes at a specific location. Going beyond, we aim to bridge the gap toward handling coordinated changes that span multiple locations across large, complex models. Specifically, we propose a novel global embedding-based next focus predictor, NextFocus, which is capable of multi-location model completion for the first time. The predictor consists of a neural network with an attention mechanism that is trained on historical software model evolution data. Starting from an existing change, it predicts further model elements to change, potentially spanning multiple parts of the model. We evaluate our approach on multi-location model changes that have actually been performed by developers in real-world projects. NextFocus achieves promising results for multi-location model completion, even when changes are heavily spread across the model. It achieves an average Precision@k score of 0.98 for $k \leq 10$, significantly outperforming the three baseline approaches.

</details>


### [911] [VulnResolver: A Hybrid Agent Framework for LLM-Based Automated Vulnerability Issue Resolution](https://arxiv.org/abs/2601.13933)
*Mingming Zhang,Xu Wang,Jian Zhang,Xiangxin Meng,Jiayi Zhang,Chunming Hu*

Main category: cs.SE

TL;DR: This paper introduces VulnResolver, the first LLM-based framework to address software vulnerabilities by combining adaptive agents and workflow-guided repair.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the increasing complexity of software systems, which results in rising security vulnerabilities and the challenges of time-consuming manual annotations in automated vulnerability repair (AVR).

Method: The authors propose VulnResolver, a hybrid framework with two agents: CPCAgent for gathering contextual repository information and SPAAgent for generating and validating safety properties violated by vulnerabilities.

Result: VulnResolver resolves 75% of security issues on SEC-bench Lite and significantly outperforms stronger baselines like OpenHands on SEC-bench Full.

Conclusion: VulnResolver is an innovative framework that combines adaptive agents and security-aware processes to enhance automated resolution of software vulnerabilities.

Abstract: As software systems grow in complexity, security vulnerabilities have become increasingly prevalent, posing serious risks and economic costs. Although automated detection tools such as fuzzers have advanced considerably, effective resolution still often depends on human expertise. Existing automated vulnerability repair (AVR) methods rely heavily on manually provided annotations (e.g., fault locations or CWE labels), which are often difficult and time-consuming to obtain, while overlooking the rich, naturally embedded semantic context found in issue reports from developers.
  In this paper, we present VulnResolver, the first LLM-based hybrid agent framework for automated vulnerability issue resolution. VulnResolver unites the adaptability of autonomous agents with the stability of workflow-guided repair through two specialized agents. The Context Pre-Collection Agent (CPCAgent) adaptively explores the repository to gather dependency and contextual information, while the Safety Property Analysis Agent (SPAAgent) generates and validates the safety properties violated by vulnerabilities. Together, these agents produce structured analyses that enrich the original issue reports, enabling more accurate vulnerability localization and patch generation.
  Evaluations on the SEC-bench benchmark show that VulnResolver resolves 75% of issues on SEC-bench Lite, achieving the best resolution performance. On SEC-bench Full, VulnResolver also significantly outperforms the strongest baseline, the agent-based OpenHands, confirming its effectiveness. Overall, VulnResolver delivers an adaptive and security-aware framework that advances end-to-end automated vulnerability issue resolution through workflow stability and the specialized agents' capabilities in contextual reasoning and property-based analysis.

</details>


### [912] [RepoGenesis: Benchmarking End-to-End Microservice Generation from Readme to Repository](https://arxiv.org/abs/2601.13943)
*Zhiyuan Peng,Xin Yin,Pu Zhao,Fangkai Yang,Lu Wang,Ran Jia,Xu Chen,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.SE

TL;DR: The paper introduces RepoGenesis, a benchmark for repository-level web microservice generation, highlighting current models' limitations and releasing tools to improve them.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to reflect real-world workflows for end-to-end web microservice generation, focusing instead on isolated function/class-level tasks or incremental modifications.

Method: RepoGenesis is introduced as a multilingual benchmark evaluating microservice generation with 106 repositories in Python and Java over 18 domains. It uses quality assurance processes and benchmarks open-source agents and commercial IDEs.

Result: Current systems perform poorly on coherence, dependencies, and cross-file consistency during evaluation, despite high API coverage and deployment success.

Conclusion: RepoGenesis successfully highlights gaps in current systems and allows performance improvement. Fine-tuning with RepoGenesis has demonstrated comparable performance to larger GPT models.

Abstract: Large language models and agents have achieved remarkable progress in code generation. However, existing benchmarks focus on isolated function/class-level generation (e.g., ClassEval) or modifications to existing codebases (e.g., SWE-Bench), neglecting complete microservice repository generation that reflects real-world 0-to-1 development workflows. To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level end-to-end web microservice generation, comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks, with 1,258 API endpoints and 2,335 test cases verified through a "review-rebuttal" quality assurance process. We evaluate open-source agents (e.g., DeepCode) and commercial IDEs (e.g., Cursor) using Pass@1, API Coverage (AC), and Deployment Success Rate (DSR). Results reveal that despite high AC (up to 73.91%) and DSR (up to 100%), the best-performing system achieves only 23.67% Pass@1 on Python and 21.45% on Java, exposing deficiencies in architectural coherence, dependency management, and cross-file consistency. Notably, GenesisAgent-8B, fine-tuned on RepoGenesis (train), achieves performance comparable to GPT-5 mini, demonstrating the quality of RepoGenesis for advancing microservice generation. We release our benchmark at https://github.com/pzy2000/RepoGenesis.

</details>


### [913] [Software Testing in the Quantum World](https://arxiv.org/abs/2601.13996)
*Rui Abreu,Shaukat Ali,Paolo Arcaini,Jose Campos,Michael Felderer,Claude Gravel,Fuyuki Ishikawa,Stefan Klikovits,Andriy Miranskyy,Mohammad Mousavi,Masaomi Yamaguchi,Lei Zhang,Jianjun Zhao,Anila Mjeda*

Main category: cs.SE

TL;DR: This paper discusses the rising challenges in testing quantum software due to its growing complexity and proposes new quality-assurance methods for real quantum computers.


<details>
  <summary>Details</summary>
Motivation: As quantum software becomes more complex, classical simulation for testing is no longer feasible, necessitating innovative quality-assurance methods directly on quantum systems.

Method: The paper identifies key challenges in testing large-scale quantum software and explores software engineering approaches to overcome these issues.

Result: It provides insights into testing complexities and offers perspectives for ensuring the quality of quantum software on real quantum computers.

Conclusion: As classical simulations for testing quantum software become infeasible, new approaches are essential to maintain quality assurance directly on quantum systems, ensuring reliable software development in the quantum computing domain.

Abstract: Quantum computing offers significant speedups for simulating physical, chemical, and biological systems, and for optimization and machine learning. As quantum software grows in complexity, the classical simulation of quantum computers, which has long been essential for quality assurance, becomes infeasible. This shift requires new quality-assurance methods that operate directly on real quantum computers. This paper presents the key challenges in testing large-scale quantum software and offers software engineering perspectives for addressing them.

</details>


### [914] [Analyzing the Availability of E-Mail Addresses for PyPI Libraries](https://arxiv.org/abs/2601.14034)
*Alexandros Tsakpinis,Alexander Pretschner*

Main category: cs.SE

TL;DR: The study explores maintainer reachability in 686,034 Python libraries, finding that 81.6% provide valid contact information, mainly via PyPI. However, issues with invalid entries persist.


<details>
  <summary>Details</summary>
Motivation: To ensure the sustainability of OSS libraries by analyzing maintainer reachability through e-mail availability, which is crucial for support, coordination, and security reporting.

Method: Empirical analysis of contact information across Python libraries on PyPI and GitHub, focusing on its availability, validity, and coverage in dependency chains.

Result: 81.6% of libraries contain valid e-mail contact information, with PyPI being the primary source. Over 97.7% of dependencies (direct and transitive) are reachable, despite 698,000 invalid entries.

Conclusion: Maintainer reachability is generally strong in Python OSS, yet improved guidance during the packaging process and validation mechanisms for contact information could further enhance sustainability.

Abstract: Open Source Software (OSS) libraries form the backbone of modern software systems, yet their long-term sustainability often depends on maintainers being reachable for support, coordination, and security reporting. In this paper, we empirically analyze the availability of contact information - specifically e-mail addresses - across 686,034 Python libraries on the Python Package Index (PyPI) and their associated GitHub repositories. We examine how and where maintainers provide this information, assess its validity, and explore coverage across individual libraries and their dependency chains. Our findings show that 81.6% of libraries include at least one valid e-mail address, with PyPI serving as the primary source (79.5%). When analyzing dependency chains, we observe that up to 97.8% of direct and 97.7% of transitive dependencies provide valid contact information. At the same time, we identify over 698,000 invalid entries, primarily due to missing fields. These results demonstrate strong maintainer reachability across the ecosystem, while highlighting opportunities for improvement - such as offering clearer guidance to maintainers during the packaging process and introducing opt-in validation mechanisms for existing e-mail addresses.

</details>


### [915] [Feature-Aware Test Generation for Deep Learning Models](https://arxiv.org/abs/2601.14081)
*Xingcheng Chen,Oliver Weissl,Andrea Stocco*

Main category: cs.SE

TL;DR: The paper introduces a test generation framework, Detect, which uses feature-aware perturbations in the latent space to ensure fine-grained control and to identify robustness issues in vision-based DL models, outperforming state-of-the-art generators.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional test generators in offering semantic insights and fine-grained control over generated inputs and to identify misbehaviors in vision-based DL models.

Method: Detect systematically perturbs disentangled semantic attributes in the latent space of vision-based DL models and assesses the behavioral shifts caused by these changes. It uses a vision-language model to assign semantic attributions, distinguishes relevant and irrelevant features, and applies targeted perturbations.

Result: Detect outperforms a state-of-the-art test generator in decision boundary discovery and improves upon spurious feature localization methods in identifying robustness issues. It reveals that fully fine-tuned convolutional models overfit on localized cues, while transformers depend on global features.

Conclusion: The proposed Detect framework enhances DL model reliability by providing insightful and interpretable feature-aware testing, showcasing its ability to identify robustness issues and model-specific behaviors.

Abstract: As deep learning models are widely used in software systems, test generation plays a crucial role in assessing the quality of such models before deployment. To date, the most advanced test generators rely on generative AI to synthesize inputs; however, these approaches remain limited in providing semantic insight into the causes of misbehaviours and in offering fine-grained semantic controllability over the generated inputs. In this paper, we introduce Detect, a feature-aware test generation framework for vision-based deep learning (DL) models that systematically generates inputs by perturbing disentangled semantic attributes within the latent space. Detect perturbs individual latent features in a controlled way and observes how these changes affect the model's output. Through this process, it identifies which features lead to behavior shifts and uses a vision-language model for semantic attribution. By distinguishing between task-relevant and irrelevant features, Detect applies feature-aware perturbations targeted for both generalization and robustness. Empirical results across image classification and detection tasks show that Detect generates high-quality test cases with fine-grained control, reveals distinct shortcut behaviors across model architectures (convolutional and transformer-based), and bugs that are not captured by accuracy metrics. Specifically, Detect outperforms a state-of-the-art test generator in decision boundary discovery and a leading spurious feature localization method in identifying robustness failures. Our findings show that fully fine-tuned convolutional models are prone to overfitting on localized cues, such as co-occurring visual traits, while weakly supervised transformers tend to rely on global features, such as environmental variances. These findings highlight the value of interpretable and feature-aware testing in improving DL model reliability.

</details>


### [916] [Practitioner Views on Mobile App Accessibility: Practices and Challenges](https://arxiv.org/abs/2601.14131)
*Amila Indika,Rick Kazman,Anthony Peruma*

Main category: cs.SE

TL;DR: The study surveys 110 mobile app developers across 43 countries to understand accessibility practices, focusing on iOS vs Android platforms and developer experience.


<details>
  <summary>Details</summary>
Motivation: To understand and improve accessibility practices in mobile app development, addressing cross-platform and experience-level differences.

Method: A mixed-methods survey of 110 developers across 43 countries to identify accessibility practices, issues, and barriers, including cross-platform comparison.

Result: Developers prioritize accessibility but rely on platform-specific guidelines, delay compliance testing, focus on text-based features, and face API and organizational barriers.

Conclusion: The findings highlight platform-specific accessibility challenges and offer actionable recommendations for stakeholders to enhance inclusivity in app development.

Abstract: As mobile applications (apps) become ubiquitous in everyday life, it is crucial for developers to prioritize accessibility for users with diverse abilities. While previous research has identified widespread accessibility issues and raised awareness of developer challenges, there remains a lack of cross-platform, globally representative insights into how practitioners approach accessibility in practice. This paper presents findings from a mixed-methods survey of 110 mobile app developers across 43 countries, examining how platform ecosystems (iOS vs. Android) and developer experience shape accessibility practices. Results show that while developers recognize the importance of accessibility, they often rely on platform-specific guidelines and typically perform compliance testing late in the development process. Developers primarily implement text-focused features while struggling with API limitations and organizational constraints. Through systematic cross-platform comparison, we identify novel platform-specific barriers and demonstrate how accessibility practices differ across developer experience levels. Our findings offer new insights into the challenges of achieving accessibility in practice and provide actionable steps for various stakeholders to promote more consistent and inclusive app development.

</details>


### [917] [Toward self-coding information systems](https://arxiv.org/abs/2601.14132)
*Rodrigo Falcão,Frank Elberzhager,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: The abstract introduces "self-coding information systems," which autonomously adapt and redeploy their source code at runtime.


<details>
  <summary>Details</summary>
Motivation: To reduce the time-to-market of new features in information systems by enabling them to autonomously adapt and reconfigure themselves.

Method: Defines and provides a conceptual framework for self-coding information systems that dynamically evaluate, generate, test, and deploy source code during runtime.

Result: Proposes expected impacts and outlines potential research directions for implementing self-coding information systems.

Conclusion: The concept holds potential for significant advancements in adaptive AI systems and software development efficiency.

Abstract: In this extended abstract, we propose a novel research topic in the field of agentic AI, which we refer to as self-coding information systems. These systems will be able to dynamically adapt their structure or behavior by evaluating potential adaptation decisions, generate source code, test, and (re)deploy their source code autonomously, at runtime, reducing the time to market of new features. Here we motivate the topic, provide a formal definition of self-coding information systems, discuss some expected impacts of the new technology, and indicate potential research directions.

</details>


### [918] [An Empirical Study on Remote Code Execution in Machine Learning Model Hosting Ecosystems](https://arxiv.org/abs/2601.14163)
*Mohammed Latif Siddiq,Tanzim Hossain Romel,Natalie Sekerak,Beatrice Casey,Joanna C. S. Santos*

Main category: cs.SE

TL;DR: This paper explores security risks in model-sharing platforms, highlighting unsafe practices and vulnerabilities in custom model loading.


<details>
  <summary>Details</summary>
Motivation: To address critical security concerns stemming from the execution of untrusted code in model-sharing platforms.

Method: A large-scale empirical study analyzing custom model loading practices, static analysis of code (using tools like Bandit, CodeQL, and Semgrep), malicious pattern detection (using YARA), and qualitative analysis of developer discussions.

Result: The study found high reliance on unsafe defaults, inconsistent security enforcement, and widespread developer confusion over remote code execution risks.

Conclusion: Recommendations are provided to improve model-sharing platforms by enhancing security infrastructure without compromising usability.

Abstract: Model-sharing platforms, such as Hugging Face, ModelScope, and OpenCSG, have become central to modern machine learning development, enabling developers to share, load, and fine-tune pre-trained models with minimal effort. However, the flexibility of these ecosystems introduces a critical security concern: the execution of untrusted code during model loading (i.e., via trust_remote_code or trust_repo). In this work, we conduct the first large-scale empirical study of custom model loading practices across five major model-sharing platforms to assess their prevalence, associated risks, and developer perceptions. We first quantify the frequency with which models require custom code to function and identify those that execute arbitrary Python files during loading. We then apply three complementary static analysis tools: Bandit, CodeQL, and Semgrep, to detect security smells and potential vulnerabilities, categorizing our findings by CWE identifiers to provide a standardized risk taxonomy. We also use YARA to identify malicious patterns and payload signatures. In parallel, we systematically analyze the documentation, API design, and safety mechanisms of each platform to understand their mitigation strategies and enforcement levels. Finally, we conduct a qualitative analysis of over 600 developer discussions from GitHub, Hugging Face, and PyTorch Hub forums, as well as Stack Overflow, to capture community concerns and misconceptions regarding security and usability. Our findings reveal widespread reliance on unsafe defaults, uneven security enforcement across platforms, and persistent confusion among developers about the implications of executing remote code. We conclude with actionable recommendations for designing safer model-sharing infrastructures and striking a balance between usability and security in future AI ecosystems.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [919] [AI Agents Need Memory Control Over More Context](https://arxiv.org/abs/2601.11653)
*Fouad Bousetouane*

Main category: q-bio.NC

TL;DR: This paper introduces a memory control mechanism for AI agents in long, multi-turn workflows to prevent drift and unstable behavior by limiting memory growth and ensuring reliable recall.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address issues in long-term agent behavior, such as constraint focus loss, error accumulation, and memory drift, caused by conventional transcript replay and retrieval mechanisms.

Method: The authors propose the Agent Cognitive Compressor (ACC), which uses a bio-inspired memory controller to maintain a bounded internal state updated online and separates artifact recall from state commitment.

Result: They demonstrate that ACC consistently achieves stable, multi-turn behavior, reduced hallucinations, and limited memory drift across use cases like IT, cybersecurity, and healthcare compared to existing memory mechanisms.

Conclusion: The proposed ACC mechanism is effective for reliable memory control, enabling better performance for AI agents in long-horizon tasks.

Abstract: AI agents are increasingly used in long, multi-turn workflows in both research and enterprise settings. As interactions grow, agent behavior often degrades due to loss of constraint focus, error accumulation, and memory-induced drift. This problem is especially visible in real-world deployments where context evolves, distractions are introduced, and decisions must remain consistent over time. A common practice is to equip agents with persistent memory through transcript replay or retrieval-based mechanisms. While convenient, these approaches introduce unbounded context growth and are vulnerable to noisy recall and memory poisoning, leading to unstable behavior and increased drift. In this work, we introduce the Agent Cognitive Compressor (ACC), a bio-inspired memory controller that replaces transcript replay with a bounded internal state updated online at each turn. ACC separates artifact recall from state commitment, enabling stable conditioning while preventing unverified content from becoming persistent memory. We evaluate ACC using an agent-judge-driven live evaluation framework that measures both task outcomes and memory-driven anomalies across extended interactions. Across scenarios spanning IT operations, cybersecurity response, and healthcare workflows, ACC consistently maintains bounded memory and exhibits more stable multi-turn behavior, with significantly lower hallucination and drift than transcript replay and retrieval-based agents. These results show that cognitive compression provides a practical and effective foundation for reliable memory control in long-horizon AI agents.

</details>


### [920] [A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data](https://arxiv.org/abs/2601.12053)
*Maël Donoso*

Main category: q-bio.NC

TL;DR: The paper proposes training AI foundation models using human brain data (neuroimaging) to enhance cognitive capabilities beyond limitations of conventional data sources.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current foundation models by leveraging human brain data, which can provide insights into cognitive processes inaccessible through traditional, observable data.

Method: Analyze potential brain regions and cognitive processes, and introduce methods like Reinforcement Learning from Human Brain and Chain of Thought from Human Brain for optimizing limited neuroimaging data in model training.

Result: The paper provides theoretical discussions and classifications of foundation model limitations, potential brain regions, and cognitive processes, suggesting pathways for improvement.

Conclusion: Brain-trained foundation models could bridge current AI limitations, offering a middle ground between scaling existing architectures and adopting neuroscience-inspired solutions, though challenges remain.

Abstract: While foundation models have achieved remarkable results across a diversity of domains, they still rely on human-generated data, such as text, as a fundamental source of knowledge. However, this data is ultimately the product of human brains, the filtered projection of a deeper neural complexity. In this paper, we explore a new strategy for artificial intelligence: moving beyond surface-level statistical regularities by training foundation models directly on human brain data. We hypothesize that neuroimaging data could open a window into elements of human cognition that are not accessible through observable actions, and argue that this additional knowledge could be used, alongside classical training data, to overcome some of the current limitations of foundation models. While previous research has demonstrated the possibility to train classical machine learning or deep learning models on neural patterns, this path remains largely unexplored for high-level cognitive functions. Here, we classify the current limitations of foundation models, as well as the promising brain regions and cognitive processes that could be leveraged to address them, along four levels: perception, valuation, execution, and integration. Then, we propose two methods that could be implemented to prioritize the use of limited neuroimaging data for strategically chosen, high-value steps in foundation model training: reinforcement learning from human brain (RLHB) and chain of thought from human brain (CoTHB). We also discuss the potential implications for agents, artificial general intelligence, and artificial superintelligence, as well as the ethical, social, and technical challenges and opportunities. We argue that brain-trained foundation models could represent a realistic and effective middle ground between continuing to scale current architectures and exploring alternative, neuroscience-inspired solutions.

</details>


### [921] [Automated Place Preference Paradigm for Optogenetic Stimulation of the Pedunculopontine Nucleus Reveals Motor Arrest-Linked Preference Behavior](https://arxiv.org/abs/2601.12054)
*Guanghui Li,Xingfei Hou,Zhenxiang Zhao*

Main category: q-bio.NC

TL;DR: This study explores the rostral Pedunculopontine nucleus (PPN)'s role in linking motor suppression to motivational processes using optogenetic techniques and an automated animal tracking system.


<details>
  <summary>Details</summary>
Motivation: To understand how the rostral PPN links motor suppression with reinforcement-related processes, an area of neuroscience with limited understanding.

Method: The researchers developed a low-cost automated system combining real-time animal tracking with closed-loop optogenetic stimulation using OpenMV Cam H7 Plus and neural network models.

Result: Optogenetic activation of CaMKIIa-expressing neurons in rostral PPN induced motor arrest and spatially conditioned place preference in rats during tests.

Conclusion: The study provides evidence that rostral PPN may coordinate motor inhibition with motivational processes and introduces a scalable system for closed-loop neuroscience experiments.

Abstract: Understanding how the brain integrates motor suppression with motivational processes remains a fundamental question in neuroscience. The rostral Pedunculopontine nucleus, a brainstem structure involved in motor control, has been shown to induce transient motor arrest upon optogenetic or electrical stimulation. However, our current understanding of its potential role in linking motor suppression with motivational or reinforcement-related processes is still insufficient. To further explore the effects induced by PPN stimulations and infer the potential mechanism underlying its role involved in both motor and emotional regulation, we developed a fully automated, low-cost system combining real-time animal tracking with closed-loop optogenetic stimulation, using the OpenMV Cam H7 Plus and embedded neural network models. The system autonomously detects the rat's position and triggers optical stimulation upon entry into a predefined region of interest, enabling unbiased, unsupervised behavioral assays. Optogenetic activation of CaMKIIa-expressing neurons in the rostral PPN reliably induced transient motor arrest. When stimulation was consistently paired with a specific location in a conditioned place preference task. When motor arrest was spatially paired with a defined region of interest, rats developed a robust place preference after limited training. These results suggest that rostral PPN activation can couple motor inhibition with reinforcement-related behavioral circuitry. Together, our work provides both a technical framework for scalable closed-loop neuroscience experiments and preliminary evidence that the rostral PPN may participate in coordinating motor suppression with motivational processes.

</details>


### [922] [Modeling Dynamic Computations in the Primate Ventral Visual Stream](https://arxiv.org/abs/2601.12258)
*Matteo Dunnhofer,Maren Wehrheim,Hamidreza Ramezanpour,Sabine Muzellec,Kohitij Kar*

Main category: q-bio.NC

TL;DR: This paper reviews advancements in modeling the dynamic responses in the primate ventral visual stream (VVS), emphasizing intrinsic dynamics, dynamics from motion, and those during active visual sampling.


<details>
  <summary>Details</summary>
Motivation: To understand how the primate VVS transforms visual inputs into dynamic neural representations, acknowledging the gaps in static models to reflect real-world, dynamic visual processes.

Method: The paper synthesizes existing neurophysiological and computational research, addressing the dynamic behaviors of VVS under various conditions such as static stimuli, motion, and eye movements.

Result: Identifies that VVS dynamics are influenced by intrinsic circuit activity, recurrent interactions, and top-down modulations, and highlights the need for multi-area recurrence and structured interactions in modeling.

Conclusion: Successful VVS dynamic models will need multi-timescale strategies incorporating representational, circuit-level, and behavioral approaches to better approximate natural and dynamic vision.

Abstract: A major goal of computational neuroscience has been to explain how the primate ventral visual stream (VVS) transforms visual input into temporally evolving neural representations that support robust visual perception. Historically, most modeling efforts have assumed static conditions: monkeys fixate a dot, images are briefly flashed, and neural responses are analyzed through time-averaged metrics. Feedforward deep networks trained on static object recognition tasks outperform prior work in approximating these static snapshot-driven VVS responses. However, mounting neurophysiological evidence demonstrates that VVS responses are rich dynamical signals shaped not only by the retinal input but also by intrinsic circuit dynamics, recurrent interactions, and widespread top-down modulation. Moreover, real-world vision is inherently dynamic: objects move, the observer moves, and the eyes actively sample the environment. Here, we review recent progress in modeling dynamic responses in the macaque ventral stream across three domains: (1) intrinsic dynamics elicited by static images, (2) dynamics evoked by dynamic visual stimuli, and (3) dynamics generated by active sensing during eye movements. We argue that accurately modeling VVS dynamics will require representational, circuit-level, and behavioral perspectives, including multi-area recurrence, structured E/I interactions, and temporal objectives that better reflect natural behavior. We outline some key missing ingredients and propose a roadmap toward dynamic, multi-timescale models of the primate VVS.

</details>


### [923] [If Grid Cells are the Answer, What is the Question? A Review of Normative Grid Cell Theory](https://arxiv.org/abs/2601.12424)
*William Dorrell,James C. R. Whittington*

Main category: q-bio.NC

TL;DR: This paper discusses grid cells in the brain, which help with path-integration for spatial navigation, and explores their function and structure through both experimental and theoretical studies.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand why grid cells in the brain encode spatial information in a unique pattern and how this aids in computations like path-integration.

Method: The authors conducted a literature review, comparing experimental evidence, mechanistic models, and normative modeling approaches to explore the function and design of grid cells.

Result: They found significant evidence linking grid cells to path-integration tasks and note agreements on their biologically plausible, high-fidelity coding mechanisms.

Conclusion: Grid cells serve as non-linearly decodable codes for position and are key for path-integration. This research offers broader insights into linking neural systems with their computational objectives.

Abstract: For 20 years the beautiful structure in the grid cell code has presented an attractive puzzle: what computation do these representations subserve, and why does it manifest so curiously in neurons. The first question quickly attracted an answer: grid cells subserve path-integration, the ability to keep track of one's position as you move about the world. Subsequent work has only solidified this link: bottom-up mechanistic models that perform path-integration match the measured neural responses, while experimental perturbations that selectively disrupt grid cell activity impair performance on path-integration dependent tasks. A more controversial area of work has been top-down normative modelling: why has the brain chosen to compute like this? Floods of ink have been spilt attempting to build a precise link between the population's objective and the measured implementation. The holy grail is a normative link with broad predictive power which generalises to other neural systems. We review this literature and argue that, despite some controversies, the literature largely agrees that grid cells can be explained as a (1) biologically plausible (2) high fidelity, non-linearly decodable code for position that (3) subserves path-integration. As a rare area of neuroscience with mature theoretical and experimental work, this story holds lessons for normative theories of neural computations, and on the risks and rewards of integrating task-optimised neural networks into such theorising.

</details>


### [924] [Primate-like perceptual decision making emerges through deep recurrent reinforcement learning](https://arxiv.org/abs/2601.12577)
*Nathan J. Wispinski,Scott A. Stone,Anthony Singhal,Patrick M. Pilarski,Craig S. Chapman*

Main category: q-bio.NC

TL;DR: This paper investigates the neural mechanisms of primate decision-making using a deep reinforcement-learning model trained on a noisy discrimination task and draws parallels with observed primate behavior.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand why primates have their neural decision-making mechanisms, proposing that such mechanisms evolved to maximize rewards under noisy and dynamic information.

Method: An end-to-end deep recurrent neural network was trained via reinforcement learning to perform a noisy perceptual discrimination task, emulating decision-making processes.

Result: The network displayed key decision-making abilities akin to primates, such as speed-accuracy trade-offs and adaptive decision revision. The internal dynamics mirrored mechanisms observed in primate neural studies.

Conclusion: The findings support the theory that primate decision-making abilities arose from pressures to handle noise and evolving information flexibly, validating the use of artificial networks to study biological evolution.

Abstract: Progress has led to a detailed understanding of the neural mechanisms that underlie decision making in primates. However, less is known about why such mechanisms are present in the first place. Theory suggests that primate decision making mechanisms, and their resultant behavioral abilities, emerged to maximize reward in the face of noisy, temporally evolving information. To test this theory, we trained an end-to-end deep recurrent neural network using reinforcement learning on a noisy perceptual discrimination task. Networks learned several key abilities of primate-like decision making including trading off speed for accuracy, and flexibly changing their mind in the face of new information. Internal dynamics of these networks suggest that these abilities were supported by similar decision mechanisms as those observed in primate neurophysiological studies. These results provide experimental support for key pressures that gave rise to the primate ability to make flexible decisions.

</details>


### [925] [Cognition spaces: natural, artificial, and hybrid](https://arxiv.org/abs/2601.12837)
*Ricard Solé,Luis F Seoane,Jordi Pla-Mauri,Michael Timothy Bennett,Michael E. Hochberg,Michael Levin*

Main category: q-bio.NC

TL;DR: This paper introduces a unified framework called 'cognition space' to compare diverse cognitive systems, including natural, artificial, and hybrid ones, based on organizational and informational dimensions.


<details>
  <summary>Details</summary>
Motivation: There is a lack of a unified framework to compare the forms, limits, and unrealized possibilities of diverse cognitive systems in both natural and artificial domains.

Method: The authors propose a cognition space model that defines cognition as a graded capacity to sense, process, and act upon information. They analyze three cognition spaces — basal aneural, neural, and human-AI hybrid — and identify patterns of uneven distribution and unoccupied regions.

Result: The study finds that cognition spaces are unevenly occupied, with noticeable gaps due to evolutionary, physical, and design constraints. These voids illuminate potential areas for advancement.

Conclusion: Shifting focus to the structure of cognition spaces reveals the diversity of cognitive systems and identifies hybrid cognition as a compelling avenue for exploring novel complexities beyond biology.

Abstract: Cognitive processes are realized across an extraordinary range of natural, artificial, and hybrid systems, yet there is no unified framework for comparing their forms, limits, and unrealized possibilities. Here, we propose a cognition space approach that replaces narrow, substrate-dependent definitions with a comparative representation based on organizational and informational dimensions. Within this framework, cognition is treated as a graded capacity to sense, process, and act upon information, allowing systems as diverse as cells, brains, artificial agents, and human-AI collectives to be analyzed within a common conceptual landscape. We introduce and examine three cognition spaces -- basal aneural, neural, and human-AI hybrid -- and show that their occupation is highly uneven, with clusters of realized systems separated by large unoccupied regions. We argue that these voids are not accidental but reflect evolutionary contingencies, physical constraints, and design limitations. By focusing on the structure of cognition spaces rather than on categorical definitions, this approach clarifies the diversity of existing cognitive systems and highlights hybrid cognition as a promising frontier for exploring novel forms of complexity beyond those produced by biological evolution.

</details>


### [926] [Global stability of a Hebbian/anti-Hebbian network for principal subspace learning](https://arxiv.org/abs/2601.13170)
*David Lipshutz,Robert J. Lipshutz*

Main category: q-bio.NC

TL;DR: This paper examines the global stability of synaptic dynamics in a self-organizing neural network model proposed by Pehlevan et al. and describes the process in two distinct phases.


<details>
  <summary>Details</summary>
Motivation: Understanding how synaptic modifications lead to stable computations in biological neural networks and addressing the gap in proving global stability for existing self-organizing models.

Method: The authors developed a mathematical proof of global stability for the continuum limit of synaptic dynamics in a neural network model, analyzing the dynamics in two sequential phases.

Result: The synaptic weights first converge to an invariant manifold where neural filters are orthonormal. Next, dynamics follow a non-convex potential function's gradient flow to span the principal subspace of the input data.

Conclusion: The study establishes global stability of the synaptic dynamics and provides detailed insights into how self-organizing neural networks implement principal subspace analysis through well-defined phases.

Abstract: Biological neural networks self-organize according to local synaptic modifications to produce stable computations. How modifications at the synaptic level give rise to such computations at the network level remains an open question. Pehlevan et al. [Neur. Comp. 27 (2015), 1461--1495] proposed a model of a self-organizing neural network with Hebbian and anti-Hebbian synaptic updates that implements an algorithm for principal subspace analysis; however, global stability of the nonlinear synaptic dynamics has not been established. Here, for the case that the feedforward and recurrent weights evolve at the same timescale, we prove global stability of the continuum limit of the synaptic dynamics and show that the dynamics evolve in two phases. In the first phase, the synaptic weights converge to an invariant manifold where the `neural filters' are orthonormal. In the second phase, the synaptic dynamics follow the gradient flow of a non-convex potential function whose minima correspond to neural filters that span the principal subspace of the input data.

</details>


### [927] [Polyphonic Intelligence: Constraint-Based Emergence, Pluralistic Inference, and Non-Dominating Integration](https://arxiv.org/abs/2601.13182)
*Alexander D Shaw*

Main category: q-bio.NC

TL;DR: The paper proposes a new perspective of 'polyphonic intelligence' where coherent behavior arises from the coordination of multiple independent inference processes, avoiding dominance and resolution into a single policy.


<details>
  <summary>Details</summary>
Motivation: Current models of intelligence prioritize convergence, eliminating uncertainty and aligning behavior around a single objective. However, these models fail to reflect biological systems that thrive on ambiguity, redundancy, and adaptability.

Method: The authors present a conceptual and formal framework, introducing variational inference with multiple coordinated approximations that maintain plurality without reliance on winner-takes-all or centralized control.

Result: Through examples, they showcase feasible implementations of non-dominating, pluralistic inference in computational systems that function without global convergence.

Conclusion: Intelligence can be understood as coordination of diverse inferential processes rather than eliminating uncertainty, with important implications for neuroscience, psychiatry, and AI development.

Abstract: Across neuroscience, artificial intelligence, and related fields, dominant models of intelligence typically privilege convergence: uncertainty is reduced, competing explanations are eliminated, and behaviour is governed by the optimisation of a single objective or policy. While this framing has proved powerful in many settings, it sits uneasily with biological and adaptive systems that maintain redundancy, ambiguity, and parallel explanatory processes over extended timescales. Here we propose an alternative perspective, termed polyphonic intelligence, in which coherent behaviour and meaning emerge from the coordination of multiple semi-independent inferential processes operating under shared constraints. Rather than resolving plurality through dominance or collapse, polyphonic systems sustain multiple explanatory trajectories and integrate them through soft alignment, compatibility relations, and bounded influence. We develop this perspective conceptually and formally, introducing a variational framework in which multiple coordinated approximations are maintained without winner-takes-all selection. This formulation makes explicit how plurality can remain stable, tractable, and productive, and clarifies how polyphonic inference differs from ensemble methods, mixture models, and Bayesian model averaging. Through proof-of-principle examples, we demonstrate that non-dominating, pluralistic inference can be implemented in simple computational systems without requiring centralised control or global convergence. We conclude by discussing implications for neuroscience, psychiatry, and artificial intelligence, and by arguing that intelligence may be more fruitfully understood as coordination without command rather than as the elimination of uncertainty.

</details>


### [928] [Multifaceted neural representation of words in naturalistic language](https://arxiv.org/abs/2601.13297)
*Xuan Yang,Chuanji Gao,Cheng Xiao,Nicholas Riccardi,Rutvik H. Desai*

Main category: q-bio.NC

TL;DR: The study combines psycholinguistic modeling and fMRI analyses to identify eight latent dimensions of word properties and their neural representations during narrative comprehension, revealing distinct neural subsystems that support language processing.


<details>
  <summary>Details</summary>
Motivation: To understand how the brain encodes the multifaceted properties of words during language comprehension and connect psycholinguistic variables to neural systems.

Method: The study analyzed 106 psycholinguistic variables across thousands of English words, identified latent dimensions, and used fMRI with multivariate analyses to examine neural correlates during narrative listening.

Result: Eight latent dimensions were discovered, showing cognitive relevance by predicting linguistic task performance, and linked to four neural subsystems supporting language processing.

Conclusion: The findings present a neurocognitive framework connecting psycholinguistic word dimensions to cortical systems involved in natural language comprehension, explaining their functional roles.

Abstract: Understanding how the brain represents the multifaceted properties of words in context is essential for explaining the neural architecture of human language. Here, we combine large-scale psycholinguistic modeling with naturalistic fMRI to uncover the latent structure of word properties and their neural representations during narrative comprehension. By analyzing 106 psycholinguistic variables across 13,850 English words, we identified eight interpretable latent dimensions spanning lexical usage, word form, phonology orthography mapping, sublexical regularity, and semantic organization. These factors robustly predicted behavioral performance across lexical decision, naming, recognition, and semantic judgment tasks, demonstrating their cognitive relevance. Parcel-based and multivariate fMRI analyses of narrative listening revealed that these latent dimensions are encoded in overlapping yet functionally differentiated cortical systems. Multidimensional scaling and hierarchical clustering analyses further identified four interacting subsystems supporting sensorimotor grounding, controlled semantic retrieval, resolution of lexical competition, and contextual episodic integration. Together, these findings provide a unified neurocognitive framework linking fundamental lexical psycholinguistic dimensions to distributed cortical systems engaged during naturalistic language comprehension.

</details>


### [929] [Audio Outperforms Text for Visual Decoding](https://arxiv.org/abs/2601.13866)
*Zhengdi Zhang,Hao Zhang,Wenjun Xia*

Main category: q-bio.NC

TL;DR: The study explores decoding visual semantics using auditory (speech-based) representations instead of textual ones, showcasing improved performance and accuracy.


<details>
  <summary>Details</summary>
Motivation: Understanding visual semantic cognition is vital, yet prior approaches focused on text-based representations, neglecting speech-based semantics foundational to human cognition.

Method: A brain-visual-auditory multimodal alignment model was developed to compare auditory versus textual semantic representations for zero-shot neural decoding.

Result: Auditory representations outperform textual ones in decoding accuracy and computational efficiency, showing better alignment with neural activity during visual processing.

Conclusion: Auditory semantics are more aligned with human cognition than textual semantics in visual decoding, offering new pathways for brain-computer interface development.

Abstract: Decoding visual semantic representations from human brain activity is a significant challenge. While recent zero-shot decoding approaches have improved performance by leveraging aligned image-text datasets, they overlook a fundamental aspect of human cognition: semantic understanding is inherently anchored in the auditory modality of speech, not text. To address this, our study introduces the first comparative framework for evaluating auditory versus textual semantic modalities in zero-shot visual neural decoding. We propose a novel brain-visual-auditory multimodal alignment model that directly utilizes auditory representations to encapsulate semantics, serving as a substitute for traditional textual descriptors. Our experimental results demonstrate that the auditory modality not only surpasses the textual modality in decoding accuracy but also achieves higher computational efficiency. These findings indicate that auditory semantic representations are more closely aligned with neural activity patterns during visual processing. This work reveals the critical and previously underestimated role of auditory semantics in decoding visual cognition and provides new insights for developing brain-computer interfaces that are more congruent with natural human cognitive mechanisms.

</details>


### [930] [MooneyMaker: A Python package to create ambiguous two-tone images](https://arxiv.org/abs/2601.14077)
*Lars C. Reining,Thabo Matthies,Luisa Haussner,Rabea Turon,Thomas S. A. Wallis*

Main category: q-bio.NC

TL;DR: The paper introduces MooneyMaker, an open-source Python package for automated generation of Mooney images using various techniques, enhancing research in visual perception.


<details>
  <summary>Details</summary>
Motivation: Traditional Mooney image creation is manual, labor-intensive, and inconsistent, necessitating a standardized automated approach.

Method: MooneyMaker leverages approaches ranging from image statistics to deep learning for generating two-tone images that balance ambiguity and recognizability.

Result: Validated through experiments, automated techniques demonstrated a higher disambiguation effect, aiding recognizability after exposure to template images.

Conclusion: MooneyMaker facilitates reproducible visual perception studies by providing diverse methods and guidelines for standardizing Mooney image generation.

Abstract: Mooney images are high-contrast, two-tone visual stimuli, created by thresholding photographic images. They allow researchers to separate image content from image understanding, making them valuable for studying visual perception. An ideal Mooney image for this purpose achieves a specific balance: it initially appears unrecognizable but becomes fully interpretable to the observer after seeing the original template. Researchers traditionally created these stimuli manually using subjective criteria, which is labor-intensive and can introduce inconsistencies across studies. Automated generation techniques now offer an alternative to this manual approach. Here, we present MooneyMaker, an open-source Python package that automates the generation of ambiguous Mooney images using several complementary approaches. Users can choose between various generation techniques that range from approaches based on image statistics to deep learning models. These models strategically alter edge information to increase initial ambiguity. The package lets users create two-tone images with multiple methods and directly compare the results visually. In an experiment, we validate MooneyMaker by generating Mooney images using different techniques and assess their recognizability for human observers before and after disambiguating them by presenting the template images. Our results reveal that techniques with lower initial recognizability are associated with higher post-template recognition (i.e. a larger disambiguation effect). To help vision scientists build effective databases of Mooney stimuli, we provide practical guidelines for technique selection. By standardizing the generation process, MooneyMaker supports more consistent and reproducible visual perception research.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [931] [Gradient-based Active Learning with Gaussian Processes for Global Sensitivity Analysis](https://arxiv.org/abs/2601.11790)
*Guerlain Lambert,Céline Helbert,Claire Lauvernet*

Main category: stat.ML

TL;DR: The paper presents a novel active learning technique to improve sensitivity analysis for computational models, emphasizing surrogate modeling and budget-constrained evaluations.


<details>
  <summary>Details</summary>
Motivation: Sensitivity analysis for complex simulators is challenged by computational constraints, prompting the development of surrogate modeling techniques to more effectively enrich experimental designs.

Method: A Gaussian process-based active learning strategy is implemented, leveraging the acquisition functions from its gradients to improve sensitivity analysis, better considering input space correlations.

Result: Compared to existing benchmarks, the method demonstrated better accuracy and applicability. It was validated with environmental modeling of pesticide transfers.

Conclusion: The approach enhances sensitivity analysis by efficiently targeting informative input space regions, improving computational accuracy and robustness.

Abstract: Global sensitivity analysis of complex numerical simulators is often limited by the small number of model evaluations that can be afforded. In such settings, surrogate models built from a limited set of simulations can substantially reduce the computational burden, provided that the design of computer experiments is enriched efficiently. In this context, we propose an active learning approach that, for a fixed evaluation budget, targets the most informative regions of the input space to improve sensitivity analysis accuracy. More specifically, our method builds on recent advances in active learning for sensitivity analysis (Sobol' indices and derivative-based global sensitivity measures, DGSM) that exploit derivatives obtained from a Gaussian process (GP) surrogate. By leveraging the joint posterior distribution of the GP gradient, we develop acquisition functions that better account for correlations between partial derivatives and their impact on the response surface, leading to a more comprehensive and robust methodology than existing DGSM-oriented criteria. The proposed approach is first compared to state-of-the-art methods on standard benchmark functions, and is then applied to a real environmental model of pesticide transfers.

</details>


### [932] [A Kernel Approach for Semi-implicit Variational Inference](https://arxiv.org/abs/2601.12023)
*Longlin Yu,Ziheng Cheng,Shiyue Zhang,Cheng Zhang*

Main category: stat.ML

TL;DR: The paper proposes kernel semi-implicit variational inference (KSIVI), a tractable method that eliminates issues in semi-implicit variational inference using kernel methods, achieving strong performance in Bayesian inference tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the intractability issues in semi-implicit variational inference caused by standard ELBO-based methods, which lead to biased optimization results, and the high computational expense of score-matching approaches.

Method: The authors introduce KSIVI, which leverages kernel methods to solve the lower-level optimization problem explicitly, reducing the objective to the kernel Stein discrepancy. The method utilizes stochastic gradient optimization and extends it with a multi-layer hierarchical structure.

Result: KSIVI is shown to achieve effective optimization with bounded variance and statistical guarantees, and performs well in both synthetic and real-world Bayesian inference tasks.

Conclusion: The proposed KSIVI method resolves previous limitations by providing a principled, scalable, and expressive variational inference approach, verified by empirical results.

Abstract: Semi-implicit variational inference (SIVI) enhances the expressiveness of variational families through hierarchical semi-implicit distributions, but the intractability of their densities makes standard ELBO-based optimization biased. Recent score-matching approaches to SIVI (SIVI-SM) address this issue via a minimax formulation, at the expense of an additional lower-level optimization problem. In this paper, we propose kernel semi-implicit variational inference (KSIVI), a principled and tractable alternative that eliminates the lower-level optimization by leveraging kernel methods. We show that when optimizing over a reproducing kernel Hilbert space, the lower-level problem admits an explicit solution, reducing the objective to the kernel Stein discrepancy (KSD). Exploiting the hierarchical structure of semi-implicit distributions, the resulting KSD objective can be efficiently optimized using stochastic gradient methods. We establish optimization guarantees via variance bounds on Monte Carlo gradient estimators and derive statistical generalization bounds of order $\tilde{\mathcal{O}}(1/\sqrt{n})$. We further introduce a multi-layer hierarchical extension that improves expressiveness while preserving tractability. Empirical results on synthetic and real-world Bayesian inference tasks demonstrate the effectiveness of KSIVI.

</details>


### [933] [On the Provable Suboptimality of Momentum SGD in Nonstationary Stochastic Optimization](https://arxiv.org/abs/2601.12238)
*Sharan Sahu,Cameron J. Hogan,Martin T. Wells*

Main category: stat.ML

TL;DR: The paper explores the performance trade-offs of SGD and momentum-based methods in nonstationary environments, highlighting a fundamental drawback of momentum in adapting to dynamic changes.


<details>
  <summary>Details</summary>
Motivation: Momentum-based methods are popular in optimization but their effectiveness in dynamic environments remains unclear. Understanding the trade-offs in tracking performance when data distribution shifts over time is needed.

Method: Analyzed tracking errors of SGD, Polyak heavy-ball, and Nesterov momentum using finite-time bounds and decomposed errors into initialization, noise-induced variance, and drift-induced tracking lag components.

Result: Discover a fundamental trade-off where momentum suppresses noise but amplifies drift-induced tracking error, making it less effective in rapidly changing environments. Established theoretical bounds and minimax lower bounds to confirm these results.

Conclusion: Momentum methods struggle in nonstationary environments due to inertia penalties, highlighting the superiority of SGD in drift-dominated scenarios where adaptation speed is critical.

Abstract: While momentum-based acceleration has been studied extensively in deterministic optimization problems, its behavior in nonstationary environments -- where the data distribution and optimal parameters drift over time -- remains underexplored. We analyze the tracking performance of Stochastic Gradient Descent (SGD) and its momentum variants (Polyak heavy-ball and Nesterov) under uniform strong convexity and smoothness in varying stepsize regimes. We derive finite-time bounds in expectation and with high probability for the tracking error, establishing a sharp decomposition into three components: a transient initialization term, a noise-induced variance term, and a drift-induced tracking lag. Crucially, our analysis uncovers a fundamental trade-off: while momentum can suppress gradient noise, it incurs an explicit penalty on the tracking capability. We show that momentum can substantially amplify drift-induced tracking error, with amplification that becomes unbounded as the momentum parameter approaches one, formalizing the intuition that using 'stale' gradients hinders adaptation to rapid regime shifts. Complementing these upper bounds, we establish minimax lower bounds for dynamic regret under gradient-variation constraints. These lower bounds prove that the inertia-induced penalty is not an artifact of analysis but an information-theoretic barrier: in drift-dominated regimes, momentum creates an unavoidable 'inertia window' that fundamentally degrades performance. Collectively, these results provide a definitive theoretical grounding for the empirical instability of momentum in dynamic environments and delineate the precise regime boundaries where SGD provably outperforms its accelerated counterparts.

</details>


### [934] [A Theory of Diversity for Random Matrices with Applications to In-Context Learning of Schrödinger Equations](https://arxiv.org/abs/2601.12587)
*Frank Cole,Yulong Lu,Shaurya Sehgal*

Main category: stat.ML

TL;DR: This paper studies the probability of trivial centralizers for collections of independent random matrices and links these findings to in-context learning generalization in transformer models.


<details>
  <summary>Details</summary>
Motivation: To explore the connection between probabilities in random matrix centralizers and their implications for improving guarantees on transformer architecture performance in in-context learning settings.

Method: Lower bounds are developed on the probability of trivial centralizers in families of $d \times d$ random matrices based on their sample size $N$ and dimension $d$. The results are linked to the discretization of linear Schrödinger operators.

Result: The results provide probabilistic insights into matrix centralizers and demonstrate their implications for generalization in transformer-based models learning Schrödinger equations.

Conclusion: The developed probability bounds have practical applications, particularly in offering guarantees on the generalization capabilities of transformers in solving linear Schrödinger equations in in-context learning.

Abstract: We address the following question: given a collection $\{\mathbf{A}^{(1)}, \dots, \mathbf{A}^{(N)}\}$ of independent $d \times d$ random matrices drawn from a common distribution $\mathbb{P}$, what is the probability that the centralizer of $\{\mathbf{A}^{(1)}, \dots, \mathbf{A}^{(N)}\}$ is trivial? We provide lower bounds on this probability in terms of the sample size $N$ and the dimension $d$ for several families of random matrices which arise from the discretization of linear Schrödinger operators with random potentials. When combined with recent work on machine learning theory, our results provide guarantees on the generalization ability of transformer-based neural networks for in-context learning of Schrödinger equations.

</details>


### [935] [Approximate full conformal prediction in RKHS](https://arxiv.org/abs/2601.13102)
*Davidson Lova Razafindrakoto,Alain Celisse,Jérôme Lacaille*

Main category: stat.ML

TL;DR: This paper addresses the computational challenges of full conformal prediction by introducing a method to efficiently compute tight approximations with quantified theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the classical limitation of full conformal prediction frameworks, which require training infinitely many estimators to compute confidence prediction regions—a task that is typically infeasible.

Method: The proposed method designs a generic strategy to approximate the full conformal prediction regions. It also introduces a new concept called 'thickness' to measure the difference between the approximate and full conformal regions, using smoothness assumptions on loss and score functions.

Result: The paper develops a theoretical method to generate computationally feasible approximations of prediction regions, while proving the tightness of these approximations within quantifiable bounds.

Conclusion: This work advances full conformal prediction capabilities by offering an efficient approach to compute tight approximations of confidence regions and provides theoretical tools to evaluate their accuracy.

Abstract: Full conformal prediction is a framework that implicitly formulates distribution-free confidence prediction regions for a wide range of estimators. However, a classical limitation of the full conformal framework is the computation of the confidence prediction regions, which is usually impossible since it requires training infinitely many estimators (for real-valued prediction for instance). The main purpose of the present work is to describe a generic strategy for designing a tight approximation to the full conformal prediction region that can be efficiently computed. Along with this approximate confidence region, a theoretical quantification of the tightness of this approximation is developed, depending on the smoothness assumptions on the loss and score functions. The new notion of thickness is introduced for quantifying the discrepancy between the approximate confidence region and the full conformal one.

</details>


### [936] [Empirical Risk Minimization with $f$-Divergence Regularization](https://arxiv.org/abs/2601.13191)
*Francisco Daunas,Iñaki Esnaola,Samir M. Perlaza,H. Vincent Poor*

Main category: stat.ML

TL;DR: This paper addresses the empirical risk minimization problem with $f$-divergence regularization (ERM-$f$DR) by providing theoretical insights, computational methods, and practical implications for using various $f$-divergences.


<details>
  <summary>Details</summary>
Motivation: To extend the applicability of empirical risk minimization with $f$-divergence regularization to a broader class of divergences and unify theoretical insights from previously known results.

Method: The work introduces the normalization function, characterized as a nonlinear ordinary differential equation (ODE), and proposes a numerical algorithm to approximate this function under mild assumptions. Structural equivalences between ERM-$f$DR problems for different $f$-divergences are also analyzed.

Result: The paper generalizes results for a wider class of $f$-divergences, characterizing the relationship between empirical risk and $f$-divergence. It demonstrates the effectiveness of the proposed algorithm for computing risks in ERM-$f$DR problems.

Conclusion: The study provides theoretical foundations and numerical techniques, emphasizing the practical impact of selecting $f$-divergences in empirical risk minimization problems. It paves the way for broader applications and demonstrates computational feasibility.

Abstract: In this paper, the solution to the empirical risk minimization problem with $f$-divergence regularization (ERM-$f$DR) is presented and conditions under which the solution also serves as the solution to the minimization of the expected empirical risk subject to an $f$-divergence constraint are established. The proposed approach extends applicability to a broader class of $f$-divergences than previously reported and yields theoretical results that recover previously known results. Additionally, the difference between the expected empirical risk of the ERM-$f$DR solution and that of its reference measure is characterized, providing insights into previously studied cases of $f$-divergences. A central contribution is the introduction of the normalization function, a mathematical object that is critical in both the dual formulation and practical computation of the ERM-$f$DR solution. This work presents an implicit characterization of the normalization function as a nonlinear ordinary differential equation (ODE), establishes its key properties, and subsequently leverages them to construct a numerical algorithm for approximating the normalization factor under mild assumptions. Further analysis demonstrates structural equivalences between ERM-$f$DR problems with different $f$-divergences via transformations of the empirical risk. Finally, the proposed algorithm is used to compute the training and test risks of ERM-$f$DR solutions under different $f$-divergence regularizers. This numerical example highlights the practical implications of choosing different functions $f$ in ERM-$f$DR problems.

</details>


### [937] [Distribution-Free Confidence Ellipsoids for Ridge Regression with PAC Bounds](https://arxiv.org/abs/2601.13436)
*Szabolcs Szentpéteri,Balázs Csanád Csáji*

Main category: stat.ML

TL;DR: The paper extends the SPS EOA algorithm to ridge regression and provides PAC upper bounds for confidence ellipsoids, analyzing the effects of regularization under weaker input excitation assumptions.


<details>
  <summary>Details</summary>
Motivation: To address the uncertainty quantification in regularized estimators, especially ridge regression, and analyze the impact of regularization parameters under input limitations.

Method: Extended the SPS EOA algorithm to ridge regression, deriving PAC bounds for confidence ellipsoid sizes and analyzing regularization impacts via simulations.

Result: Provided explicit PAC upper bounds for confidence regions showcasing regularization effects, offering tighter bounds under weak excitation conditions.

Conclusion: The study demonstrates how regularization impacts uncertainty quantification, providing practical insights with simulation validation and enhanced theoretical results for confidence ellipsoids.

Abstract: Linearly parametrized models are widely used in control and signal processing, with the least-squares (LS) estimate being the archetypical solution. When the input is insufficiently exciting, the LS problem may be unsolvable or numerically unstable. This issue can be resolved through regularization, typically with ridge regression. Although regularized estimators reduce the variance error, it remains important to quantify their estimation uncertainty. A possible approach for linear regression is to construct confidence ellipsoids with the Sign-Perturbed Sums (SPS) ellipsoidal outer approximation (EOA) algorithm. The SPS EOA builds non-asymptotic confidence ellipsoids under the assumption that the noises are independent and symmetric about zero. This paper introduces an extension of the SPS EOA algorithm to ridge regression, and derives probably approximately correct (PAC) upper bounds for the resulting region sizes. Compared with previous analyses, our result explicitly show how the regularization parameter affects the region sizes, and provide tighter bounds under weaker excitation assumptions. Finally, the practical effect of regularization is also demonstrated via simulation experiments.

</details>


### [938] [Labels or Preferences? Budget-Constrained Learning with Human Judgments over AI-Generated Outputs](https://arxiv.org/abs/2601.13458)
*Zihan Dong,Ruijia Wu,Linjun Zhang*

Main category: stat.ML

TL;DR: The paper introduces PCAL, a budget-friendly active learning strategy that optimally allocates resources between ground-truth labels and pairwise preferences, proving statistical efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the need for cost-effective ways to acquire high-quality data (ground-truth and pairwise preferences) for AI systems, ensuring reliable and efficient AI model performance.

Method: Developed Preference-Calibrated Active Learning (PCAL), a semi-parametric method that optimally allocates budget for data acquisition, maximizes statistical efficiency, and provides variance optimization without closed-form solutions.

Result: PCAL demonstrated theoretical asymptotic optimality and robustness, achieving strong performance in simulations and real-world datasets.

Conclusion: PCAL provides a statistically robust and cost-efficient framework for budget-constrained learning, proving its superiority in practical applications through theoretical and experimental validation.

Abstract: The increasing reliance on human preference feedback to judge AI-generated pseudo labels has created a pressing need for principled, budget-conscious data acquisition strategies. We address the crucial question of how to optimally allocate a fixed annotation budget between ground-truth labels and pairwise preferences in AI. Our solution, grounded in semi-parametric inference, casts the budget allocation problem as a monotone missing data framework. Building on this formulation, we introduce Preference-Calibrated Active Learning (PCAL), a novel method that learns the optimal data acquisition strategy and develops a statistically efficient estimator for functionals of the data distribution. Theoretically, we prove the asymptotic optimality of our PCAL estimator and establish a key robustness guarantee that ensures robust performance even with poorly estimated nuisance models. Our flexible framework applies to a general class of problems, by directly optimizing the estimator's variance instead of requiring a closed-form solution. This work provides a principled and statistically efficient approach for budget-constrained learning in modern AI. Simulations and real-data analysis demonstrate the practical benefits and superior performance of our proposed method.

</details>


### [939] [Small Gradient Norm Regret for Online Convex Optimization](https://arxiv.org/abs/2601.13519)
*Wenzhi Gao,Chang He,Madeleine Udell*

Main category: stat.ML

TL;DR: The paper introduces a refined regret measure ($G^\star$) for online convex optimization, particularly focusing on smooth losses, and demonstrates its advantages and theoretical bounds.


<details>
  <summary>Details</summary>
Motivation: To provide a sharper regret metric that better captures problem dependency in online convex optimization with smooth losses, surpassing limitations of the existing $L^\star$ regret.

Method: The authors introduce the $G^\star$ regret, defined using the cumulative squared gradient norm. They analyze its properties, establish theoretical bounds, extend to dynamic regret and bandit settings, and refine convergence analysis for stochastic optimization.

Result: The $G^\star$ regret is shown to strictly refine $L^\star$ regret and outperform it in specific cases. Theoretical upper and lower bounds are provided, and extensive experiments support their findings.

Conclusion: The $G^\star$ regret offers a more nuanced metric for regret in online convex optimization, advancing both theoretical understanding and practical applications, especially in settings with vanishing curvature losses.

Abstract: This paper introduces a new problem-dependent regret measure for online convex optimization with smooth losses. The notion, which we call the $G^\star$ regret, depends on the cumulative squared gradient norm evaluated at the decision in hindsight $\sum_{t=1}^T \|\nabla \ell(x^\star)\|^2$. We show that the $G^\star$ regret strictly refines the existing $L^\star$ (small loss) regret, and that it can be arbitrarily sharper when the losses have vanishing curvature around the hindsight decision. We establish upper and lower bounds on the $G^\star$ regret and extend our results to dynamic regret and bandit settings. As a byproduct, we refine the existing convergence analysis of stochastic optimization algorithms in the interpolation regime. Some experiments validate our theoretical findings.

</details>


### [940] [Sample Complexity of Average-Reward Q-Learning: From Single-agent to Federated Reinforcement Learning](https://arxiv.org/abs/2601.13642)
*Yuchen Jiao,Jiin Woo,Gen Li,Gauri Joshi,Yuejie Chi*

Main category: stat.ML

TL;DR: The paper develops a Q-learning algorithm for average-reward Markov decision processes (MDPs), presenting improved sample and communication-efficiencies, including extensions to federated settings.


<details>
  <summary>Details</summary>
Motivation: Theoretical guarantees for average-reward Q-learning algorithms remain underdeveloped despite their importance for long-term decision-making frameworks.

Method: The paper introduces and analyzes Q-learning algorithms with optimized parameters for average-reward MDPs, addressing challenges in single-agent and federated scenarios under the weakly communicating assumption.

Result: Key results include improved sample complexity $O(|S|A|   h^ sp^3/epsilon^3)$ for single-agent cases and further reduction in per-agent complexity $O(|S|A|  h^ sp^3/(M epsilon^3)$ through collaboration for federated scenarios.

Conclusion: This work provides groundwork for scalable and efficient Q-learning algorithms for average-reward MDPs, demonstrating novel advancements in federated learning setups.

Abstract: Average-reward reinforcement learning offers a principled framework for long-term decision-making by maximizing the mean reward per time step. Although Q-learning is a widely used model-free algorithm with established sample complexity in discounted and finite-horizon Markov decision processes (MDPs), its theoretical guarantees for average-reward settings remain limited. This work studies a simple but effective Q-learning algorithm for average-reward MDPs with finite state and action spaces under the weakly communicating assumption, covering both single-agent and federated scenarios. For the single-agent case, we show that Q-learning with carefully chosen parameters achieves sample complexity $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{\varepsilon^3}\right)$, where $\|h^{\star}\|_{\mathsf{sp}}$ is the span norm of the bias function, improving previous results by at least a factor of $\frac{\|h^{\star}\|_{\mathsf{sp}}^2}{\varepsilon^2}$. In the federated setting with $M$ agents, we prove that collaboration reduces the per-agent sample complexity to $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{M\varepsilon^3}\right)$, with only $\widetilde{O}\left(\frac{\|h^{\star}\|_{\mathsf{sp}}}{\varepsilon}\right)$ communication rounds required. These results establish the first federated Q-learning algorithm for average-reward MDPs, with provable efficiency in both sample and communication complexity.

</details>


### [941] [Unified Unbiased Variance Estimation for MMD: Robust Finite-Sample Performance with Imbalanced Data and Exact Acceleration under Null and Alternative Hypotheses](https://arxiv.org/abs/2601.13874)
*Shijie Zhong,Jiangfeng Fu,Yikun Yang*

Main category: stat.ML

TL;DR: The paper studies MMD variance using U-statistics and provides a unified characterization. It proposes a computational acceleration for Laplacian kernels reducing complexity to $O(n \log n)$.


<details>
  <summary>Details</summary>
Motivation: To enhance inferential accuracy of MMD statistics by better understanding and characterizing its variance across various hypotheses and sample setups.

Method: Using U-statistics and Hoeffding decomposition for unified analysis and proposing a computational acceleration method under the univariate Laplacian kernel.

Result: The paper presents a unified finite-sample variance characterization and an exact method to reduce computational complexity of MMD testing under Laplacian kernels.

Conclusion: A unified understanding aids in better variance estimation, and the computational advancement reduces testing complexity for improved efficiency.

Abstract: The maximum mean discrepancy (MMD) is a kernel-based nonparametric statistic for two-sample testing, whose inferential accuracy depends critically on variance characterization. Existing work provides various finite-sample estimators of the MMD variance, often differing under the null and alternative hypotheses and across balanced or imbalanced sampling schemes. In this paper, we study the variance of the MMD statistic through its U-statistic representation and Hoeffding decomposition, and establish a unified finite-sample characterization covering different hypotheses and sample configurations. Building on this analysis, we propose an exact acceleration method for the univariate case under the Laplacian kernel, which reduces the overall computational complexity from $\mathcal O(n^2)$ to $\mathcal O(n \log n)$.

</details>


### [942] [Intermittent time series forecasting: local vs global models](https://arxiv.org/abs/2601.14031)
*Stefano Damato,Nicolò Rubattu,Dario Azzimonti,Giorgio Corani*

Main category: stat.ML

TL;DR: This paper compares local and global forecasting models, including various neural networks, for intermittent time series and evaluates their performance using different distribution heads.


<details>
  <summary>Details</summary>
Motivation: Intermittent time series with many zeros require reliable probabilistic forecasting methods due to their relevance in inventory and supply chain management. Previous forecasting models relied mostly on local methods, while global models remain underexplored for this type of data.

Method: The study compares state-of-the-art local (iETS, TweedieGP) and global models (D-Linear, DeepAR, Transformers), testing three distribution heads (negative binomial, hurdle-shifted negative binomial, Tweedie) on five large datasets.

Result: D-Linear yields the best accuracy among global models and outperforms local models, particularly with low computational effort. Tweedie distribution excels in high quantile estimation, while negative binomial achieves the best overall results. Transformer models are less efficient and accurate.

Conclusion: Global models, particularly neural networks like D-Linear, are suitable for intermittent time series forecasting, outperforming traditional local models. The choice of distribution head impacts predictive performance, with the negative binomial head being the most robust overall.

Abstract: Intermittent time series, characterised by the presence of a significant amount of zeros, constitute a large percentage of inventory items in supply chain. Probabilistic forecasts are needed to plan the inventory levels; the predictive distribution should cover non-negative values, have a mass in zero and a long upper tail. Intermittent time series are commonly forecast using local models, which are trained individually on each time series. In the last years global models, which are trained on a large collection of time series, have become popular for time series forecasting. Global models are often based on neural networks. However, they have not yet been exhaustively tested on intermittent time series. We carry out the first study comparing state-of-the-art local (iETS, TweedieGP) and global models (D-Linear, DeepAR, Transformers) on intermittent time series. For neural networks models we consider three different distribution heads suitable for intermittent time series: negative binomial, hurdle-shifted negative binomial and Tweedie. We use, for the first time, the last two distribution heads with neural networks. We perform experiments on five large datasets comprising more than 40'000 real-world time series. Among neural networks D-Linear provides best accuracy; it also consistently outperforms the local models. Moreover, it has also low computational requirements. Transformers-based architectures are instead much more computationally demanding and less accurate. Among the distribution heads, the Tweedie provides the best estimates of the highest quantiles, while the negative binomial offers overall the best performance.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [943] [Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration](https://arxiv.org/abs/2601.14235)
*LSST Dark Energy Science Collaboration,Eric Aubourg,Camille Avestruz,Matthew R. Becker,Biswajit Biswas,Rahul Biswas,Boris Bolliet,Adam S. Bolton,Clecio R. Bom,Raphaël Bonnet-Guerrini,Alexandre Boucaud,Jean-Eric Campagne,Chihway Chang,Aleksandra Ćiprijanović,Johann Cohen-Tanugi,Michael W. Coughlin,John Franklin Crenshaw,Juan C. Cuevas-Tello,Juan de Vicente,Seth W. Digel,Steven Dillmann,Mariano Javier de León Dominguez Romero,Alex Drlica-Wagner,Sydney Erickson,Alexander T. Gagliano,Christos Georgiou,Aritra Ghosh,Matthew Grayling,Kirill A. Grishin,Alan Heavens,Lindsay R. House,Mustapha Ishak,Wassim Kabalan,Arun Kannawadi,François Lanusse,C. Danielle Leonard,Pierre-François Léget,Michelle Lochner,Yao-Yuan Mao,Peter Melchior,Grant Merz,Martin Millon,Anais Möller,Gautham Narayan,Yuuki Omori,Hiranya Peiris,Laurence Perreault-Levasseur,Andrés A. Plazas Malagón,Nesar Ramachandra,Benjamin Remy,Cécile Roucelle,Jaime Ruiz-Zapatero,Stefan Schuldt,Ignacio Sevilla-Noarbe,Ved G. Shah,Tjitske Starkenburg,Stephen Thorp,Laura Toribio San Cipriano,Tilman Tröster,Roberto Trotta,Padma Venkatraman,Amanda Wasserman,Tim White,Justine Zeghal,Tianqing Zhang,Yuanyuan Zhang*

Main category: astro-ph.IM

TL;DR: This paper discusses the use of AI/ML in Rubin Observatory's LSST for cosmological studies, focusing on challenges, research priorities, and future methodologies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to harness AI/ML for extracting robust dark energy and dark matter constraints from LSST data, while addressing challenges like uncertainty quantification, covariate shifts, and reproducibility.

Method: The paper serves as a survey and identifies cross-cutting AI/ML methodologies, proposes research priorities, and explores advanced techniques like Bayesian inference and foundation models.

Result: It highlights recurring challenges across multiple cosmological analyses and the utility of addressing these through unified methodologies and advanced AI techniques.

Conclusion: The paper emphasizes critical research areas and infrastructure needs for effective AI/ML integration in LSST DESC workflows, advocating for collaborative governance and evaluation frameworks.

Abstract: The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful, scalable, and operationally reliable. Artificial intelligence and machine learning (AI/ML) are already embedded across DESC science workflows, from photometric redshifts and transient classification to weak lensing inference and cosmological simulations. Yet their utility for precision cosmology hinges on trustworthy uncertainty quantification, robustness to covariate shift and model misspecification, and reproducible integration within scientific pipelines. This white paper surveys the current landscape of AI/ML across DESC's primary cosmological probes and cross-cutting analyses, revealing that the same core methodologies and fundamental challenges recur across disparate science cases. Since progress on these cross-cutting challenges would benefit multiple probes simultaneously, we identify key methodological research priorities, including Bayesian inference at scale, physics-informed methods, validation frameworks, and active learning for discovery. With an eye on emerging techniques, we also explore the potential of the latest foundation model methodologies and LLM-driven agentic AI systems to reshape DESC workflows, provided their deployment is coupled with rigorous evaluation and governance. Finally, we discuss critical software, computing, data infrastructure, and human capital requirements for the successful deployment of these new methodologies, and consider associated risks and opportunities for broader coordination with external actors.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [944] [DeepEvidence: Empowering Biomedical Discovery with Deep Knowledge Graph Research](https://arxiv.org/abs/2601.11560)
*Zifeng Wang,Zheng Chen,Ziwei Yang,Xuan Wang,Qiao Jin,Yifan Peng,Zhiyong Lu,Jimeng Sun*

Main category: cs.IR

TL;DR: DeepEvidence is an AI framework designed for systematic exploration and evidence synthesis in biomedical knowledge graphs (KGs), bridging diverse resources efficiently.


<details>
  <summary>Details</summary>
Motivation: Biomedical knowledge graphs are rich in data but challenging to integrate and analyze collectively due to structural differences, evolution, and alignment issues.

Method: DeepEvidence uses an orchestrator that controls two agents: Breadth-First ReSearch for broad exploration and Depth-First ReSearch for detailed reasoning, alongside unified interfaces for querying APIs and an evidence graph for structured outcomes.

Result: DeepEvidence showed significant advancements in systematic exploration and evidence synthesis across benchmarks and biomedical discovery stages like drug discovery and evidence-based medicine.

Conclusion: The approach demonstrates the potential of knowledge-graph-driven methods to enhance scientific discovery processes in biomedicine.

Abstract: Biomedical knowledge graphs (KGs) encode vast, heterogeneous information spanning literature, genes, pathways, drugs, diseases, and clinical trials, but leveraging them collectively for scientific discovery remains difficult. Their structural differences, continual evolution, and limited cross-resource alignment require substantial manual integration, limiting the depth and scale of knowledge exploration. We introduce DeepEvidence, an AI-agent framework designed to perform Deep Research across various heterogeneous biomedical KGs. Unlike generic Deep Research systems that rely primarily on internet-scale text, DeepEvidence incorporates specialized knowledge-graph tooling and coordinated exploration strategies to systematically bridge heterogeneous resources. At its core is an orchestrator that directs two complementary agents: Breadth-First ReSearch (BFRS) for broad, multi-graph entity search, and Depth-First ReSearch (DFRS) for multi-hop, evidence-focused reasoning. An internal, incrementally built evidence graph provides a structured record of retrieved entities, relations, and supporting evidence. To operate at scale, DeepEvidence includes unified interfaces for querying diverse biomedical APIs and an execution sandbox that enables programmatic data retrieval, extraction, and analysis. Across established deep-reasoning benchmarks and four key stages of the biomedical discovery lifecycle: drug discovery, pre-clinical experimentation, clinical trial development, and evidence-based medicine, DeepEvidence demonstrates substantial gains in systematic exploration and evidence synthesis. These results highlight the potential of knowledge-graph-driven Deep Research to accelerate biomedical discovery.

</details>


### [945] [Utilizing Metadata for Better Retrieval-Augmented Generation](https://arxiv.org/abs/2601.11863)
*Raquib Bin Yousuf,Shengzhe Xu,Mandar Sharma,Andrew Neeser,Chris Latimer,Naren Ramakrishnan*

Main category: cs.IR

TL;DR: The paper studies metadata-aware retrieval strategies for Retrieval-Augmented Generation systems, assessing various approaches to integrate metadata for improved retrieval accuracy.


<details>
  <summary>Details</summary>
Motivation: Large language models often struggle to retrieve accurate chunks from structured corpora like regulatory filings due to overlapping language and lack of effective metadata utilization.

Method: The authors evaluate multiple approaches, including metadata-as-text (prefix and suffix), dual-encoder strategies, and query reformulation, comparing them to plain-text baselines on retrieval metrics.

Result: Prefix and unified embedding methods consistently outperform plain-text baselines in retrieval tasks, with unified embeddings providing better maintenance and embedding space analysis showing improved document cohesion and clarity.

Conclusion: Integrating metadata enhances retrieval effectiveness by disambiguating documents, improving cohesion, and aiding in separating relevant and irrelevant sections; structural cues play a critical role.

Abstract: Retrieval-Augmented Generation systems depend on retrieving semantically relevant document chunks to support accurate, grounded outputs from large language models. In structured and repetitive corpora such as regulatory filings, chunk similarity alone often fails to distinguish between documents with overlapping language. Practitioners often flatten metadata into input text as a heuristic, but the impact and trade-offs of this practice remain poorly understood. We present a systematic study of metadata-aware retrieval strategies, comparing plain-text baselines with approaches that embed metadata directly. Our evaluation spans metadata-as-text (prefix and suffix), a dual-encoder unified embedding that fuses metadata and content in a single index, dual-encoder late-fusion retrieval, and metadata-aware query reformulation. Across multiple retrieval metrics and question types, we find that prefixing and unified embeddings consistently outperform plain-text baselines, with the unified at times exceeding prefixing while being easier to maintain. Beyond empirical comparisons, we analyze embedding space, showing that metadata integration improves effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks. Field-level ablations show that structural cues provide strong disambiguating signals. Our code, evaluation framework, and the RAGMATE-10K dataset are publicly hosted.

</details>


### [946] [Incorporating Q&A Nuggets into Retrieval-Augmented Generation](https://arxiv.org/abs/2601.13222)
*Laura Dietz,Bryan Li,Gabrielle Liu,Jia-Huei Ju,Eugene Yang,Dawn Lawrie,William Walden,James Mayfield*

Main category: cs.IR

TL;DR: The paper introduces Crucible, a Retrieval-augmented Generation system that integrates Q&A nugget-based evaluation and citation provenance for improved response generation.


<details>
  <summary>Details</summary>
Motivation: To enhance the quality of Retrieval-augmented Generation (RAG) systems by improving citation accuracy and avoiding opaque abstractions during the generation process.

Method: The Crucible system constructs a bank of Q&A nuggets from retrieved documents, leveraging these nuggets for extraction, selection, and report generation while preserving citation provenance.

Result: On the TREC NeuCLIR 2024 dataset, the Crucible system outperformed the Ginger RAG system in nugget recall, density, and citation grounding.

Conclusion: Crucible demonstrates the effectiveness of nugget-based reasoning and citation-preserving mechanisms, setting a new benchmark in the RAG domain.

Abstract: RAGE systems integrate ideas from automatic evaluation (E) into Retrieval-augmented Generation (RAG). As one such example, we present Crucible, a Nugget-Augmented Generation System that preserves explicit citation provenance by constructing a bank of Q&A nuggets from retrieved documents and uses them to guide extraction, selection, and report generation. Reasoning on nuggets avoids repeated information through clear and interpretable Q&A semantics - instead of opaque cluster abstractions - while maintaining citation provenance throughout the entire generation process. Evaluated on the TREC NeuCLIR 2024 collection, our Crucible system substantially outperforms Ginger, a recent nugget-based RAG system, in nugget recall, density, and citation grounding.

</details>


### [947] [Insider Knowledge: How Much Can RAG Systems Gain from Evaluation Secrets?](https://arxiv.org/abs/2601.13227)
*Laura Dietz,Bryan Li,Eugene Yang,Dawn Lawrie,William Walden,James Mayfield*

Main category: cs.IR

TL;DR: The paper explores the risk of faulty evaluations in nugget-based RAG systems assessed by LLM judges, emphasizing concerns about circularity and metric overfitting.


<details>
  <summary>Details</summary>
Motivation: RAG systems increasingly rely on nugget-based evaluations and LLM judges for optimization but face risks of evaluation circularity and false progress measurements.

Method: Comparative experiments on nugget-based RAG systems, such as Ginger and Crucible, against robust baselines like GPT-Researcher, were conducted. Modifications were made to intentionally optimize Crucible outputs for LLM judge evaluations.

Result: Achieving nearly perfect evaluation scores was possible by exploiting elements like leaked evaluation metrics, demonstrating flaws in the system's assessment framework.

Conclusion: Blind evaluation methods and diversified approaches are necessary to avoid metric overfitting and ensure reliable progress in system evaluation.

Abstract: RAG systems are increasingly evaluated and optimized using LLM judges, an approach that is rapidly becoming the dominant paradigm for system assessment. Nugget-based approaches in particular are now embedded not only in evaluation frameworks but also in the architectures of RAG systems themselves. While this integration can lead to genuine improvements, it also creates a risk of faulty measurements due to circularity. In this paper, we investigate this risk through comparative experiments with nugget-based RAG systems, including Ginger and Crucible, against strong baselines such as GPT-Researcher. By deliberately modifying Crucible to generate outputs optimized for an LLM judge, we show that near-perfect evaluation scores can be achieved when elements of the evaluation - such as prompt templates or gold nuggets - are leaked or can be predicted. Our results highlight the importance of blind evaluation settings and methodological diversity to guard against mistaking metric overfitting for genuine system progress.

</details>


### [948] [Auditory Brain Passage Retrieval: Cross-Sensory EEG Training for Neural Information Retrieval](https://arxiv.org/abs/2601.14001)
*Niall McGuire,Yashar Moshfeghi*

Main category: cs.IR

TL;DR: The study investigates using auditory EEG for brain passage retrieval (BPR) and evaluates if combining sensory modalities for training improves performance. Key findings show auditory EEG's superiority and significant benefits from cross-sensory training.


<details>
  <summary>Details</summary>
Motivation: To address cognitive and physical challenges in query formulation for Information Retrieval by exploring novel EEG-based retrieval approaches.

Method: Auditory EEG data was used in dual encoder architectures with four pooling strategies for training and evaluation. Data from sensory modalities (auditory and visual) were integrated for cross-sensory training comparisons.

Result: Auditory EEG outperformed visual EEG, with cross-sensory training showing marked performance improvements (e.g., 31% MRR improvement). Combined models surpassed traditional text-based retrieval methods like BM25.

Conclusion: Auditory neural interfaces for Information Retrieval are validated, and cross-sensory training proves effective in handling data scarcity and outperforming single-modality approaches.

Abstract: Query formulation from internal information needs remains fundamentally challenging across all Information Retrieval paradigms due to cognitive complexity and physical impairments. Brain Passage Retrieval (BPR) addresses this by directly mapping EEG signals to passage representations without intermediate text translation. However, existing BPR research exclusively uses visual stimuli, leaving critical questions unanswered: Can auditory EEG enable effective retrieval for voice-based interfaces and visually impaired users? Can training on combined EEG datasets from different sensory modalities improve performance despite severe data scarcity? We present the first systematic investigation of auditory EEG for BPR and evaluate cross-sensory training benefits. Using dual encoder architectures with four pooling strategies (CLS, mean, max, multi-vector), we conduct controlled experiments comparing auditory-only, visual-only, and combined training on the Alice (auditory) and Nieuwland (visual) datasets. Results demonstrate that auditory EEG consistently outperforms visual EEG, and cross-sensory training with CLS pooling achieves substantial improvements over individual training: 31% in MRR (0.474), 43% in Hit@1 (0.314), and 28% in Hit@10 (0.858). Critically, combined auditory EEG models surpass BM25 text baselines (MRR: 0.474 vs 0.428), establishing neural queries as competitive with traditional retrieval whilst enabling accessible interfaces. These findings validate auditory neural interfaces for IR tasks and demonstrate that cross-sensory training addresses data scarcity whilst outperforming single-modality approaches Code: https://github.com/NiallMcguire/Audio_BPR

</details>


### [949] [IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization](https://arxiv.org/abs/2601.13938)
*Heyang Zhou,JiaJia Chen,Xiaolu Chen,Jie Bao,Zhen Chen,Yong Liao*

Main category: cs.IR

TL;DR: The paper introduces IF-GEO, a method to enhance document visibility in generative engines for diverse queries by managing conflicting revision needs under limited budgets.


<details>
  <summary>Details</summary>
Motivation: Generative Engines often struggle with ensuring visibility of retrieved sources while synthesizing direct answers. Improving this visibility through document optimization for diverse queries is necessary but challenging.

Method: The paper proposes the IF-GEO framework with two key phases: (i) mining optimization preferences from latent queries, and (ii) creating a Global Revision Blueprint by merging preferences using conflict-aware instruction fusion. Stability metrics are introduced to measure effectiveness.

Result: Experiments show that IF-GEO provides significant improvements in multi-query benchmarks by balancing performance and stability across varied queries.

Conclusion: IF-GEO enables effective editing of documents for generative engines, maintaining visibility and robustness for diverse retrieval scenarios.

Abstract: As Generative Engines revolutionize information retrieval by synthesizing direct answers from retrieved sources, ensuring source visibility becomes a significant challenge. Improving it through targeted content revisions is a practical strategy termed Generative Engine Optimization (GEO). However, optimizing a document for diverse queries presents a constrained optimization challenge where heterogeneous queries often impose conflicting and competing revision requirements under a limited content budget. To address this challenge, we propose IF-GEO, a "diverge-then-converge" framework comprising two phases: (i) mining distinct optimization preferences from representative latent queries; (ii) synthesizing a Global Revision Blueprint for guided editing by coordinating preferences via conflict-aware instruction fusion. To explicitly quantify IF-GEO's objective of cross-query stability, we introduce risk-aware stability metrics. Experiments on multi-query benchmarks demonstrate that IF-GEO achieves substantial performance gains while maintaining robustness across diverse retrieval scenarios.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [950] [Cascaded Transformer for Robust and Scalable SLA Decomposition via Amortized Optimization](https://arxiv.org/abs/2601.11859)
*Cyril Shih-Huan Hsu*

Main category: cs.NI

TL;DR: The paper introduces Casformer, a Transformer-based architecture for efficient SLA decomposition in 6G networks, reducing complexity and latency compared to existing optimization-based methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for E2E SLA decomposition in networks rely on slow and computationally complex optimization processes, presenting challenges in scalability and real-time application.

Method: Casformer uses a cascaded Transformer architecture with domain-specific encoders and a cross-domain aggregator. It employs Domain-Informed Neural Networks (DINNs) for training with risk-informed modeling and amortized optimization.

Result: Casformer outperforms existing optimization-based frameworks in SLA decomposition quality, scalability, and robustness under dynamic network conditions, while also simplifying deployment with its forward-only design.

Conclusion: Casformer showcases the effectiveness of combining Transformer-based sequence modeling with amortized optimization, offering a scalable and efficient tool for real-time SLA management in modern networks.

Abstract: The evolution toward 6G networks increasingly relies on network slicing to provide tailored, End-to-End (E2E) logical networks over shared physical infrastructures. A critical challenge is effectively decomposing E2E Service Level Agreements (SLAs) into domain-specific SLAs, which current solutions handle through computationally intensive, iterative optimization processes that incur substantial latency and complexity. To address this, we introduce Casformer, a cascaded Transformer architecture designed for fast, optimization-free SLA decomposition. Casformer leverages historical domain feedback encoded through domain-specific Transformer encoders in its first layer, and integrates cross-domain dependencies using a Transformer-based aggregator in its second layer. The model is trained under a learning paradigm inspired by Domain-Informed Neural Networks (DINNs), incorporating risk-informed modeling and amortized optimization to learn a stable, forward-only SLA decomposition policy. Extensive evaluations demonstrate that Casformer achieves improved SLA decomposition quality against state-of-the-art optimization-based frameworks, while exhibiting enhanced scalability and robustness under volatile and noisy network conditions. In addition, its forward-only design reduces runtime complexity and simplifies deployment and maintenance. These insights reveal the potential of combining amortized optimization with Transformer-based sequence modeling to advance network automation, providing a scalable and efficient solution suitable for real-time SLA management in advanced 5G-and-beyond network environments.

</details>


### [951] [Cross-reality Location Privacy Protection in 6G-enabled Vehicular Metaverses: An LLM-enhanced Hybrid Generative Diffusion Model-based Approach](https://arxiv.org/abs/2601.12311)
*Xiaofeng Luo,Jiayi He,Jiawen Kang,Ruichen Zhang,Zhaoshui He,Ekram Hossain,Dong In Kim*

Main category: cs.NI

TL;DR: This paper proposes a framework to protect location privacy of Autonomous Vehicles (AVs) within 6G-enabled vehicular metaverses using hybrid actions and novel optimization algorithms.


<details>
  <summary>Details</summary>
Motivation: The emergence of 6G-enabled vehicular metaverses introduces significant privacy risks for AVs as cross-reality interactions enable adversaries to infer vehicle trajectories through location information.

Method: The paper introduces a cross-reality location privacy framework using hybrid actions (location perturbation and AI agent migration), a new privacy metric termed location entropy, and a novel optimization algorithm (LHDPPO).

Result: The framework mitigates privacy leakage for AVs while ensuring low latency and high-quality service, as validated through experiments on real-world datasets.

Conclusion: This approach successfully balances privacy, immersion, and service quality in the 6G-enabled vehicular metaverse, showcasing the effectiveness of privacy-aware hybrid actions and advanced optimization techniques.

Abstract: The emergence of 6G-enabled vehicular metaverses enables Autonomous Vehicles (AVs) to operate across physical and virtual spaces through space-air-ground-sea integrated networks. The AVs can deploy AI agents powered by large AI models as personalized assistants, on edge servers to support intelligent driving decision making and enhanced on-board experiences. However, such cross-reality interactions may cause serious location privacy risks, as adversaries can infer AV trajectories by correlating the location reported when AVs request LBS in reality with the location of the edge servers on which their corresponding AI agents are deployed in virtuality. To address this challenge, we design a cross-reality location privacy protection framework based on hybrid actions, including continuous location perturbation in reality and discrete privacy-aware AI agent migration in virtuality. In this framework, a new privacy metric, termed cross-reality location entropy, is proposed to effectively quantify the privacy levels of AVs. Based on this metric, we formulate an optimization problem to optimize the hybrid action, focusing on achieving a balance between location protection, service latency reduction, and quality of service maintenance. To solve the complex mixed-integer problem, we develop a novel LLM-enhanced Hybrid Diffusion Proximal Policy Optimization (LHDPPO) algorithm, which integrates LLM-driven informative reward design to enhance environment understanding with double Generative Diffusion Models-based policy exploration to handle high-dimensional action spaces, thereby enabling reliable determination of optimal hybrid actions. Extensive experiments on real-world datasets demonstrate that the proposed framework effectively mitigates cross-reality location privacy leakage for AVs while maintaining strong user immersion within 6G-enabled vehicular metaverse scenarios.

</details>


### [952] [LiQSS: Post-Transformer Linear Quantum-Inspired State-Space Tensor Networks for Real-Time 6G](https://arxiv.org/abs/2601.12375)
*Farhad Rezazadeh,Hatim Chergui,Mehdi Bennis,Houbing Song,Lingjia Liu,Dusit Niyato,Merouane Debbah*

Main category: cs.NI

TL;DR: This paper presents a quantum-inspired state-space model (LiQSS) for efficient and faster 6G O-RAN telemetry forecasting, achieving significant reductions in parameters and improved speed over Transformers while maintaining prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational and latency challenges of Near-RT 6G O-RAN radio telemetry forecasting, where current Transformer models fail due to their high complexity and scalability issues.

Method: The paper introduces the LiQSS model, utilizing quantum-inspired tensor networks and structured state-space dynamics to enable linear-time sequence modeling. Techniques like tensor factorization and lightweight channel layers are employed to minimize complexity and capture non-stationary dependencies.

Result: LiQSS achieves 10.8x-15.8x fewer parameters and is 1.4x faster than state-space baselines. Compared to Transformers, it has up to a 155x parameter reduction and 2.74x faster inference with no loss in forecasting accuracy.

Conclusion: LiQSS demonstrates the feasibility of efficient and scalable telemetry forecasting for 6G O-RAN, outperforming current state-space and Transformer-based models in size, speed, and efficiency while maintaining accuracy.

Abstract: Proactive and agentic control in Sixth-Generation (6G) Open Radio Access Networks (O-RAN) requires control-grade prediction under stringent Near-Real-Time (Near-RT) latency and computational constraints. While Transformer-based models are effective for sequence modeling, their quadratic complexity limits scalability in Near-RT RAN Intelligent Controller (RIC) analytics. This paper investigates a post-Transformer design paradigm for efficient radio telemetry forecasting. We propose a quantum-inspired many-body state-space tensor network that replaces self-attention with stable structured state-space dynamics kernels, enabling linear-time sequence modeling. Tensor-network factorizations in the form of Tensor Train (TT) / Matrix Product State (MPS) representations are employed to reduce parameterization and data movement in both input projections and prediction heads, while lightweight channel gating and mixing layers capture non-stationary cross-Key Performance Indicator (KPI) dependencies. The proposed model is instantiated as an agentic perceive-predict xApp and evaluated on a bespoke O-RAN KPI time-series dataset comprising 59,441 sliding windows across 13 KPIs, using Reference Signal Received Power (RSRP) forecasting as a representative use case. Our proposed Linear Quantum-Inspired State-Space (LiQSS) model is 10.8x-15.8x smaller and approximately 1.4x faster than prior structured state-space baselines. Relative to Transformer-based models, LiQSS achieves up to a 155x reduction in parameter count and up to 2.74x faster inference, without sacrificing forecasting accuracy.

</details>


### [953] [An Efficient and Explainable KAN Framework forWireless Radiation Field Prediction](https://arxiv.org/abs/2601.11656)
*Jingzhou Shen,Xuyu Wang*

Main category: cs.NI

TL;DR: The paper introduces a novel method for modeling wireless channels using a KAN-Transformer architecture to better incorporate global context and environmental factors, delivering superior performance over existing models.


<details>
  <summary>Details</summary>
Motivation: To improve wireless channel modeling by addressing the limitations of current methods which fail to consider global context and environmental variations, crucial for accurate signal propagation understanding.

Method: The authors propose leveraging a Kolmogorov-Arnold Network (KAN) integrated with transformer modules to learn entire ray representations and capture detailed environmental features, ensuring computational efficiency.

Result: Their approach outperforms current methods in both realistic and synthetic scenarios, as demonstrated through experimental evaluations. Ablation studies validate the effectiveness of each component in the model.

Conclusion: The proposed KAN-Transformer model offers a significant improvement in wireless channel modeling by incorporating global context, capturing detailed environmental features, and maintaining computational efficiency.

Abstract: Modeling wireless channels accurately remains a challenge due to environmental variations and signal uncertainties. Recent neural networks can learn radio frequency~(RF) signal propagation patterns, but they process each voxel on the ray independently, without considering global context or environmental factors. Our paper presents a new approach that learns comprehensive representations of complete rays rather than individual points, capturing more detailed environmental features. We integrate a Kolmogorov-Arnold network (KAN) architecture with transformer modules to achieve better performance across realistic and synthetic scenes while maintaining computational efficiency. Our experimental results show that this approach outperforms existing methods in various scenarios. Ablation studies confirm that each component of our model contributes to its effectiveness. Additional experiments provide clear explanations for our model's performance.

</details>


### [954] [IntAgent: NWDAF-Based Intent LLM Agent Towards Advanced Next Generation Networks](https://arxiv.org/abs/2601.13114)
*Abdelrahman Soliman,Ahmed Refaey,Aiman Erbad,Amr Mohamed*

Main category: cs.NI

TL;DR: The paper introduces IntAgent, an intelligent intent-based network agent that utilizes NWDAF analytics for automated network operations.


<details>
  <summary>Details</summary>
Motivation: To develop a novel approach for automating network operations by integrating high-level requests with live analytics and advanced tools.

Method: The authors built an intelligent agent (IntAgent) using NWDAF analytics and tools, incorporating real-time network data and a compliant data source for fulfilling network operator intents.

Result: The framework, demonstrated through use cases like traffic prediction and policy enforcement, effectively fulfills complex network intents autonomously.

Conclusion: IntAgent successfully showcases the capability to automate and optimize network operations using advanced analytics and tools, offering context-aware and dynamic solutions.

Abstract: Intent-based networks (IBNs) are gaining prominence as an innovative technology that automates network operations through high-level request statements, defining what the network should achieve. In this work, we introduce IntAgent, an intelligent intent LLM agent that integrates NWDAF analytics and tools to fulfill the network operator's intents. Unlike previous approaches, we develop an intent tools engine directly within the NWDAF analytics engine, allowing our agent to utilize live network analytics to inform its reasoning and tool selection. We offer an enriched, 3GPP-compliant data source that enhances the dynamic, context-aware fulfillment of network operator goals, along with an MCP tools server for scheduling, monitoring, and analytics tools. We demonstrate the efficacy of our framework through two practical use cases: ML-based traffic prediction and scheduled policy enforcement, which validate IntAgent's ability to autonomously fulfill complex network intents.

</details>


### [955] [Reinforcement Learning for Opportunistic Routing in Software-Defined LEO-Terrestrial Systems](https://arxiv.org/abs/2601.13662)
*Sivaram Krishnan,Zhouyou Gu,Jihong Park,Sung-Min Oh,Jinho Choi*

Main category: cs.NI

TL;DR: This paper presents an intelligent opportunistic routing strategy optimized using residual reinforcement learning for LEO satellite networks, achieving low-latency data delivery and significant queue reduction.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of rapidly changing topologies and intermittent gateway visibility in LEO satellite constellations, ensuring efficient and low-latency data delivery to terrestrial networks.

Method: The authors propose an opportunistic routing strategy enabled by a GEO-resident SDN controller, formulated as a constrained stochastic optimization problem. They optimize the routing using a residual reinforcement learning framework.

Result: Simulations over multiple days of orbital data demonstrate that the proposed method outperforms classical backpressure and other queueing algorithms in queue length reduction and transmission delay.

Conclusion: The study demonstrates that opportunistic routing is an effective approach for achieving low-latency and robust data delivery in dynamic LEO satellite networks.

Abstract: The proliferation of large-scale low Earth orbit (LEO) satellite constellations is driving the need for intelligent routing strategies that can effectively deliver data to terrestrial networks under rapidly time-varying topologies and intermittent gateway visibility. Leveraging the global control capabilities of a geostationary (GEO)-resident software-defined networking (SDN) controller, we introduce opportunistic routing, which aims to minimize delivery delay by forwarding packets to any currently available ground gateways rather than fixed destinations. This makes it a promising approach for achieving low-latency and robust data delivery in highly dynamic LEO networks. Specifically, we formulate a constrained stochastic optimization problem and employ a residual reinforcement learning framework to optimize opportunistic routing for reducing transmission delay. Simulation results over multiple days of orbital data demonstrate that our method achieves significant improvements in queue length reduction compared to classical backpressure and other well-known queueing algorithms.

</details>


### [956] [Variational Dual-path Attention Network for CSI-Based Gesture Recognition](https://arxiv.org/abs/2601.13745)
*N. Zhang*

Main category: cs.NI

TL;DR: This paper proposes a lightweight feature preprocessing module called Variational Dual-path Attention Network (VDAN) for Wi-Fi gesture recognition to refine features, reduce noise, and enhance performance using variational inference.


<details>
  <summary>Details</summary>
Motivation: Traditional end-to-end models for CSI-based Wi-Fi gesture recognition fail to address inherent time-frequency sparsity, resulting in redundancy and poor generalization in resource-constrained environments.

Method: The authors developed the Variational Dual-path Attention Network (VDAN), which refines high-dimensional CSI features through frequency-domain filtering, temporal detection, and variational inference to handle noise.

Result: Experiments on a public dataset demonstrate that VDAN effectively aligns attention weights with the sparse characteristics of CSI, improving both interpretability and robustness.

Conclusion: VDAN offers an efficient, explainable, and robust preprocessing solution for wireless sensing systems, addressing resource constraints on edge devices and enhancing gesture recognition performance.

Abstract: Wi-Fi gesture recognition based on Channel State Information (CSI) is challenged by high-dimensional noise and resource constraints on edge devices. Prevailing end-to-end models tightly couple feature extraction with classification, overlooking the inherent time-frequency sparsity of CSI and leading to redundancy and poor generalization. To address this, this paper proposes a lightweight feature preprocessing module--the Variational Dual-path Attention Network (VDAN). It performs structured feature refinement through frequency-domain filtering and temporal detection. Variational inference is introduced to model the uncertainty in attention weights, thereby enhancing robustness to noise. The design principles of the module are explained from the perspectives of the information bottleneck and regularization. Experiments on a public dataset demonstrate that the learned attention weights align with the physical sparse characteristics of CSI, verifying its interpretability. This work provides an efficient and explainable front-end processing solution for resource-constrained wireless sensing systems.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [957] [A Proof of Concept for a Digital Twin of an Ultrasonic Fermentation System](https://arxiv.org/abs/2601.11723)
*Francesco Saverio Sconocchia Pisoni,Andrea Vitaletti,Davide Appolloni,Federico Ortenzi,Blasco Morozzo della Rocca,Mariano José Guillén,Alessandro Contaldo*

Main category: cs.ET

TL;DR: The paper designs and demonstrates a proof-of-concept digital twin for an ultrasonic-aided beer-fermentation system.


<details>
  <summary>Details</summary>
Motivation: To intelligently monitor, predict, and enhance yeast growth and fermentation speed in beer production using ultrasonic stimulation.

Method: A fermentation system equipped with ultrasonic piezoelectric transducers was digitally replicated using a predictive model. This model, extended from a previous work, uses environmental factors like temperature, ultrasonic frequency, and duty cycle for training.

Result: The predictive model successfully estimated yeast culture density over time with limited training data, validating the feasibility of the proposed system.

Conclusion: The digital twin and ultrasonic fermentation system prove promising in optimizing and accelerating beer fermentation, showing potential for further advancements in industrial fermentation processes.

Abstract: This paper presents the design and implementation of a proof of concept digital twin for an innovative ultrasonic-enhanced beer-fermentation system, developed to enable intelligent monitoring, prediction, and actuation in yeast-growth environments. A traditional fermentation tank is equipped with a piezoelectric transducer able to irradiate the tank with ultrasonic waves, providing an external abiotic stimulus to enhance the growth of yeast and accelerate the fermentation process. At its core, the digital twin incorporates a predictive model that estimates yeast's culture density over time based on the surrounding environmental conditions. To this end, we implement, tailor and extend the model proposed in Palacios et al., allowing us to effectively handle the limited number of available training samples by using temperature, ultrasonic frequency, and duty cycle as inputs. The results obtained along with the assessment of model performance demonstrate the feasibility of the proposed approach.

</details>


### [958] [Bounded Minds, Generative Machines: Envisioning Conversational AI that Works with Human Heuristics and Reduces Bias Risk](https://arxiv.org/abs/2601.13376)
*Jiqun Liu*

Main category: cs.ET

TL;DR: This paper explores how conversational AI systems can align better with human heuristics and cognitive limitations rather than assuming idealized users.


<details>
  <summary>Details</summary>
Motivation: The authors are motivated by the mismatch between current conversational AI designs, which assume ideal users, and real human reasoning, which is limited and heuristic-based.

Method: The approach involves grounding conversational AI research within the framework of bounded rationality, focusing on adapting AI to human cognitive processes.

Result: Key advancements include proposed directions for detecting cognitive vulnerabilities, enhancing decision-making under uncertainty, and moving evaluation criteria beyond factual accuracy.

Conclusion: Conversational AI systems should adapt to human heuristics and cognitive patterns, aiming to improve decision quality and cognitive robustness.

Abstract: Conversational AI is rapidly becoming a primary interface for information seeking and decision making, yet most systems still assume idealized users. In practice, human reasoning is bounded by limited attention, uneven knowledge, and reliance on heuristics that are adaptive but bias-prone. This article outlines a research pathway grounded in bounded rationality, and argues that conversational AI should be designed to work with human heuristics rather than against them. It identifies key directions for detecting cognitive vulnerability, supporting judgment under uncertainty, and evaluating conversational systems beyond factual accuracy, toward decision quality and cognitive robustness.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [959] [Demystifying the trend of the healthcare index: Is historical price a key driver?](https://arxiv.org/abs/2601.14062)
*Payel Sadhukhan,Samrat Gupta,Subhasis Ghosh,Tanujit Chakraborty*

Main category: q-fin.ST

TL;DR: The paper investigates predicting daily healthcare index movements using historical data, achieving high predictive accuracy through innovative nowcasting features.


<details>
  <summary>Details</summary>
Motivation: To address short-term predictability in healthcare sector indices that influence critical economic and healthcare decisions.

Method: The study utilized supervised classification with a one-step-ahead rolling window and novel nowcasting features, tested on healthcare indices from U.S. and Indian markets over five years.

Result: The model achieved accuracy above 0.8 and Matthews correlation coefficients over 0.6, highlighting nowcasting features as key predictors.

Conclusion: The framework reduces information asymmetry and supports a stable healthcare economy by enhancing short-term index forecasting.

Abstract: Healthcare sector indices consolidate the economic health of pharmaceutical, biotechnology, and healthcare service firms. The short-term movements in these indices are closely intertwined with capital allocation decisions affecting research and development investment, drug availability, and long-term health outcomes. This research investigates whether historical open-high-low-close (OHLC) index data contain sufficient information for predicting the directional movement of the opening index on the subsequent trading day. The problem is formulated as a supervised classification task involving a one-step-ahead rolling window. A diverse feature set is constructed, comprising original prices, volatility-based technical indicators, and a novel class of nowcasting features derived from mutual OHLC ratios. The framework is evaluated on data from healthcare indices in the U.S. and Indian markets over a five-year period spanning multiple economic phases, including the COVID-19 pandemic. The results demonstrate robust predictive performance, with accuracy exceeding 0.8 and Matthews correlation coefficients above 0.6. Notably, the proposed nowcasting features have emerged as a key determinant of the market movement. We have employed the Shapley-based explainability paradigm to further elucidate the contribution of the features: outcomes reveal the dominant role of the nowcasting features, followed by a more moderate contribution of original prices. This research offers a societal utility: the proposed features and model for short-term forecasting of healthcare indices can reduce information asymmetry and support a more stable and equitable health economy.

</details>


### [960] [Beyond Visual Realism: Toward Reliable Financial Time Series Generation](https://arxiv.org/abs/2601.12990)
*Fan Zhang,Jiabin Luo,Zheng Zhang,Shuanghong Huang,Zhipeng Liu,Yu Chen*

Main category: q-fin.ST

TL;DR: The paper addresses limitations in current generative models (like GANs) for financial time series, proposing a new method called SFAG to generate data aligned with realistic market dynamics, proven effective in backtesting.


<details>
  <summary>Details</summary>
Motivation: Existing generative models for financial time series, such as GANs, often fail under trading backtests due to their inability to account for financial asymmetry and rare tail events, despite producing superficially realistic data.

Method: The paper introduces the Stylized Facts Alignment GAN (SFAG), which incorporates stylized financial facts as structural constraints and optimizes these alongside adversarial loss, ensuring alignment with market dynamics.

Result: SFAG-generated synthetic data preserve key stylized facts of financial markets, showing stable trading outcomes and robust performance on backtests involving a momentum strategy applied to the Shanghai Composite Index (2004–2024) dataset.

Conclusion: Structure-preserving objectives, as implemented in SFAG, are vital for bridging the gap between generating superficially realistic synthetic data and achieving practical usability in financial applications.

Abstract: Generative models for financial time series often create data that look realistic and even reproduce stylized facts such as fat tails or volatility clustering. However, these apparent successes break down under trading backtests: models like GANs or WGAN-GP frequently collapse, yielding extreme and unrealistic results that make the synthetic data unusable in practice. We identify the root cause in the neglect of financial asymmetry and rare tail events, which strongly affect market risk but are often overlooked by objectives focusing on distribution matching. To address this, we introduce the Stylized Facts Alignment GAN (SFAG), which converts key stylized facts into differentiable structural constraints and jointly optimizes them with adversarial loss. This multi-constraint design ensures that generated series remain aligned with market dynamics not only in plots but also in backtesting. Experiments on the Shanghai Composite Index (2004--2024) show that while baseline GANs produce unstable and implausible trading outcomes, SFAG generates synthetic data that preserve stylized facts and support robust momentum strategy performance. Our results highlight that structure-preserving objectives are essential to bridge the gap between superficial realism and practical usability in financial generative modeling.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [961] [End-to-End Reverse Screening Identifies Protein Targets of Small Molecules Using HelixFold3](https://arxiv.org/abs/2601.13693)
*Shengjie Xu,Xianbin Ye,Mengran Zhu,Xiaonan Zhang,Shanzhuo Zhang,Xiaomin Fang*

Main category: q-bio.BM

TL;DR: The paper introduces an end-to-end reverse screening method using HelixFold3, which integrates protein structure prediction and small molecule docking, improving accuracy and aiding drug discovery.


<details>
  <summary>Details</summary>
Motivation: Reverse screening, crucial for drug action insights and molecular mechanism elucidation, faces challenges due to the complexity of protein-ligand interactions and limitations in conventional workflows.

Method: The approach uses HelixFold3, a structure prediction model, to predict protein folding and ligand docking simultaneously within a unified framework.

Result: The proposed method outperforms traditional reverse docking approaches by improving screening accuracy, structural fidelity, binding-site precision, and target prioritization.

Conclusion: The framework offers a scalable and efficient platform for linking small molecules to protein targets, advancing drug discovery and understanding molecular interactions.

Abstract: Identifying protein targets for small molecules, or reverse screening, is essential for understanding drug action, guiding compound repurposing, predicting off-target effects, and elucidating the molecular mechanisms of bioactive compounds. Despite its critical role, reverse screening remains challenging because accurately capturing interactions between a small molecule and structurally diverse proteins is inherently complex, and conventional step-wise workflows often propagate errors across decoupled steps such as target structure modeling, pocket identification, docking, and scoring. Here, we present an end-to-end reverse screening strategy leveraging HelixFold3, a high-accuracy biomolecular structure prediction model akin to AlphaFold3, which simultaneously models the folding of proteins from a protein library and the docking of small-molecule ligands within a unified framework. We validate this approach on a diverse and representative set of approximately one hundred small molecules. Compared with conventional reverse docking, our method improves screening accuracy and demonstrates enhanced structural fidelity, binding-site precision, and target prioritization. By systematically linking small molecules to their protein targets, this framework establishes a scalable and straightforward platform for dissecting molecular mechanisms, exploring off-target interactions, and supporting rational drug discovery.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [962] [The Cost of EFX: Generalized-Mean Welfare and Complexity Dichotomies with Few Surplus Items](https://arxiv.org/abs/2601.12849)
*Eugene Lim,Tzeh Yuan Neoh,Nicholas Teh*

Main category: cs.GT

TL;DR: This paper investigates the interplay between envy-freeness up to any good (EFX) and welfare maximization in allocating indivisible goods with a small surplus, uncovering complexity patterns and welfare constraints.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address unresolved questions about the interplay between achieving EFX fairness and efficiency (welfare optimization) in resource allocation settings with indivisible goods and a small surplus.

Method: The paper conducts a theoretical complexity analysis of EFX in relation to generalized-mean ($p$-mean) welfare and employs computational approaches to identify polynomial-time algorithms and certify when certain welfare objectives align with EFX.

Result: Key findings include: 1) For $p > 0$, achieving both EFX and welfare maximization is NP-hard. 2) For $p \leq 0$, polynomial-time solutions are feasible. 3) Enforcing EFX results in varying welfare losses, with linear losses for $p > 0$ and bounded or asymptotically vanishing losses for $p \leq 0$. 4) Adding Pareto optimality to EFX leads to another layer of computational hardness.

Conclusion: The results clarify when EFX fairness aligns with or contradicts efficiency goals, providing computational and structural insights for cases with few surplus goods, while highlighting challenges in combining constraints like Pareto optimality with EFX.

Abstract: Envy-freeness up to any good (EFX) is a central fairness notion for allocating indivisible goods, yet its existence is unresolved in general. In the setting with few surplus items, where the number of goods exceeds the number of agents by a small constant (at most three), EFX allocations are guaranteed to exist, shifting the focus from existence to efficiency and computation. We study how EFX interacts with generalized-mean ($p$-mean) welfare, which subsumes commonly-studied utilitarian ($p=1$), Nash ($p=0$), and egalitarian ($p \rightarrow -\infty$) objectives. We establish sharp complexity dichotomies at $p=0$: for any fixed $p \in (0,1]$, both deciding whether EFX can attain the global $p$-mean optimum and computing an EFX allocation maximizing $p$-mean welfare are NP-hard, even with at most three surplus goods; in contrast, for any fixed $p \leq 0$, we give polynomial-time algorithms that optimize $p$-mean welfare within the space of EFX allocations and efficiently certify when EFX attains the global optimum. We further quantify the welfare loss of enforcing EFX via the price of fairness framework, showing that for $p > 0$, the loss can grow linearly with the number of agents, whereas for $p \leq 0$, it is bounded by a constant depending on the surplus (and for Nash welfare it vanishes asymptotically). Finally we show that requiring Pareto-optimality alongside EFX is NP-hard (and becomes $Σ_2^P$-complete for a stronger variant of EFX). Overall, our results delineate when EFX is computationally costly versus structurally aligned with welfare maximization in the setting with few surplus items.

</details>


### [963] [Bridging the Gap Between Estimated and True Regret Towards Reliable Regret Estimation in Deep Learning based Mechanism Design](https://arxiv.org/abs/2601.13489)
*Shuyuan You,Zhiqiang Zhuang,Kewen Wang,Zhe Wang*

Main category: cs.GT

TL;DR: Existing deep learning methods for approximating optimal multi-item auctions underestimate true regret and overstate performance metrics.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations in regret estimation accuracy for deep learning-based auction mechanisms, highlighting the need for trustworthy incentive compatibility evaluations.

Method: The authors derive a lower bound for regret and introduce an efficient item-wise regret approximation, complemented by a guided refinement procedure to improve estimation accuracy and computational efficiency.

Result: Through experiments, the paper reveals that existing methods significantly underestimate true regret and that the proposed methodology yields much more accurate regret estimations.

Conclusion: The work provides a more reliable basis for evaluating incentive compatibility and calls for a reassessment of performance claims in prior auction mechanism studies.

Abstract: Recent advances, such as RegretNet, ALGnet, RegretFormer and CITransNet, use deep learning to approximate optimal multi item auctions by relaxing incentive compatibility (IC) and measuring its violation via ex post regret. However, the true accuracy of these regret estimates remains unclear. Computing exact regret is computationally intractable, and current models rely on gradient based optimizers whose outcomes depend heavily on hyperparameter choices. Through extensive experiments, we reveal that existing methods systematically underestimate actual regret (In some models, the true regret is several hundred times larger than the reported regret), leading to overstated claims of IC and revenue. To address this issue, we derive a lower bound on regret and introduce an efficient item wise regret approximation. Building on this, we propose a guided refinement procedure that substantially improves regret estimation accuracy while reducing computational cost. Our method provides a more reliable foundation for evaluating incentive compatibility in deep learning based auction mechanisms and highlights the need to reassess prior performance claims in this area.

</details>


### [964] [Asymmetric regularization mechanism for GAN training with Variational Inequalities](https://arxiv.org/abs/2601.13920)
*Spyridon C. Giagtzoglou,Mark H. M. Winands,Barbara Franci*

Main category: cs.GT

TL;DR: The paper addresses stabilizing GAN training by formulating it as a Nash equilibrium seeking problem, introducing asymmetric regularization, and demonstrating its effectiveness empirically.


<details>
  <summary>Details</summary>
Motivation: To address the instability in training generative adversarial networks (GANs) by leveraging a Nash equilibrium framework.

Method: They propose an asymmetric regularization mechanism based on the Tikhonov step and zero-centered gradient penalty to ensure the stability of GAN training. This is achieved under smoothness and local identifiability conditions.

Result: The proposed method achieves explicit Lipschitz and (strong)-monotonicity constants, leading to linear convergence of the Extrapolation-from-the-Past (EFTP) method. Empirical evidence supports the model’s success in stabilizing convergence and trajectories.

Conclusion: The asymmetric regularization mechanism effectively stabilizes GAN training, enabling convergence to a Nash equilibrium even in challenging scenarios.

Abstract: We formulate the training of generative adversarial networks (GANs) as a Nash equilibrium seeking problem. To stabilize the training process and find a Nash equilibrium, we propose an asymmetric regularization mechanism based on the classic Tikhonov step and on a novel zero-centered gradient penalty. Under smoothness and a local identifiability condition induced by a Gauss-Newton Gramian, we obtain explicit Lipschitz and (strong)-monotonicity constants for the regularized operator. These constants ensure last-iterate linear convergence of a single-call Extrapolation-from-the-Past (EFTP) method. Empirical simulations on an academic example show that, even when strong monotonicity cannot be achieved, the asymmetric regularization is enough to converge to an equilibrium and stabilize the trajectory.

</details>


### [965] [Collective intelligence in science: direct elicitation of diverse information from experts with unknown information structure](https://arxiv.org/abs/2601.14047)
*Alexey V. Osipov,Nikolay N. Osipov*

Main category: cs.GT

TL;DR: The paper introduces a mechanism combining prediction markets and chat for collaborative exploration of scientific hypotheses, pooling private expert information effectively.


<details>
  <summary>Details</summary>
Motivation: To enable the effective aggregation of diverse, unpredictable, and private expert information regarding complex scientific hypotheses, especially when experts are unrelated and cannot perform Bayesian reasoning.

Method: A self-resolving play-money prediction market is combined with a chat system for experts to share information and trade based on the hypothesis' truth. Real asset rewards are proportionally tied to play money earnings.

Result: The approach facilitates equilibrium where participants exchange information efficiently, yielding aggregated insights even with no established ground truth or pre-existing expert knowledge.

Conclusion: The method offers a transparent, interpretable, and innovative way to fund and aggregate large-scale collaborative studies without requiring prior connections or complex calculations.

Abstract: Suppose we need a deep collective analysis of an open scientific problem: there is a complex scientific hypothesis and a large online group of mutually unrelated experts with relevant private information of a diverse and unpredictable nature. This information may be results of experts' individual experiments, original reasoning of some of them, results of AI systems they use, etc. We propose a simple mechanism based on a self-resolving play-money prediction market entangled with a chat. We show that such a system can easily be brought to an equilibrium where participants directly share their private information on the hypothesis through the chat and trade as if the market were resolved in accordance with the truth of the hypothesis. This approach will lead to efficient aggregation of relevant information in a completely interpretable form even if the ground truth cannot be established and experts initially know nothing about each other and cannot perform complex Bayesian calculations. Finally, by rewarding the experts with some real assets proportionally to the play money they end up with, we can get an innovative way to fund large-scale collaborative studies of any type.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [966] [A First Step for Expansion X-Ray Microscopy: Achieving Contrast in Expanded Tissues Sufficient to Reveal Cell Bodies](https://arxiv.org/abs/2601.13370)
*Logan Thrasher Collins*

Main category: q-bio.QM

TL;DR: The paper introduces a pioneering method combining expansion microscopy with X-ray microscopy to achieve cellular imaging in expanded tissues, highlighting its potential yet early-stage challenges.


<details>
  <summary>Details</summary>
Motivation: Current nanoscale connectomics methods are too slow for mapping large-scale mammalian brains, necessitating new techniques like expansion microscopy (ExM) to improve throughput.

Method: The author demonstrates an "expansion X-ray microscopy" (ExXRM) approach by applying a modified Unclearing technique, metallic gold staining, and laboratory X-ray microscopy to image cell bodies within expanded tissue.

Result: The method successfully resolves cell bodies in expanded tissues but faces issues such as off-target staining and the inability to resolve neurites.

Conclusion: Although much development is needed for ExXRM to become practical for connectomics, it represents an important step toward faster, large-volume brain mapping.

Abstract: Existing methods in nanoscale connectomics are at present too slow to map entire mammalian brains. As an emerging approach, expansion microscopy (ExM) has enormous promise, yet it still suffers from throughput limitations. Mapping the human brain and even mapping nonhuman primate brains therefore remain distant goals. While ExM increases effective resolution linearly, it enlarges tissue volume cubically, which dramatically increases imaging time. As a rapid tomographic technique, X-ray microscopy has potential for drastically speeding up large-volume connectomics. But to the best of my knowledge, no group has so far imaged cellular features within expanded tissue using X-ray microscopy. I herein present an early-stage report featuring the first demonstration of X-ray microscopy reconstruction of cell bodies within expanded tissue. This was achieved by combining a modified enzymatic Unclearing technique with a metallic gold stain and imaging using a laboratory X-ray microscope. I emphasize that a great deal of work remains to develop "expansion X-ray microscopy" (ExXRM) to the point where it can be useful for connectomics since the current iteration of ExXRM only resolves cell bodies and not neurites due to extensive off-target staining. Additionally, the current method must be modified to accommodate for the challenges of synchrotron X-ray microscopy, a vastly speedier approach than laboratory X-ray microscopy. Nonetheless, achieving X-ray contrast in expanded tissues represents a significant first step towards realizing ExXRM as a connectomics imaging modality.

</details>


### [967] [Karhunen-Loève Expansion-Based Residual Anomaly Map for Resource-Efficient Glioma MRI Segmentation](https://arxiv.org/abs/2601.11833)
*Anthony Hur*

Main category: q-bio.QM

TL;DR: This paper presents an innovative brain tumor segmentation technique using a Karhunen-Loève Expansion (KLE)-based residual anomaly map, outperforming traditional methods in both data efficiency and computational cost.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurate brain tumor segmentation in settings with limited computational resources and datasets, tackling the reliance on state-of-the-art methods which demand extensive training datasets and resources.

Method: The authors implemented the Karhunen--Loève Expansion (KLE) for feature extraction, compressing MRI volumes into KL coefficients and creating a residual-based anomaly map. This map is incorporated into a compact 3D U-Net for segmentation. Experiments were conducted using consumer-grade hardware with reduced training cases.

Result: The proposed method achieved Dice scores of 0.929 (WT), 0.856 (TC), 0.821 (ET) and improved HD95 distances, outperforming the BraTS 2023 winner in certain metrics while utilizing significantly fewer resources.

Conclusion: The study validates a computation and data-efficient approach to brain tumor segmentation, demonstrating that KLE-based techniques can maintain top performance while reducing resource dependency.

Abstract: Accurate segmentation of brain tumors is essential for clinical diagnosis and treatment planning. Deep learning is currently the state-of-the-art for brain tumor segmentation, yet it requires either large datasets or extensive computational resources that are inaccessible in most areas. This makes the problem increasingly difficult: state-of-the-art models use thousands of training cases and vast computational power, where performance drops sharply when either is limited. The top performer in the Brats GLI 2023 competition relied on supercomputers trained on over 92,000 augmented MRI scans using an AMD EPYC 7402 CPU, six NVIDIA RTX 6000 GPUs (48GB VRAM each), and 1024GB of RAM over multiple weeks. To address this, the Karhunen--Loève Expansion (KLE) was implemented as a feature extraction step on downsampled, z-score normalized MRI volumes. Each 240$\times$240$\times$155 multi-modal scan is reduced to four $48^3$ channels and compressed into 32 KL coefficients. The resulting approximate reconstruction enables a residual-based anomaly map, which is upsampled and added as a fifth channel to a compact 3D U-Net. All experiments were run on a consumer workstation (AMD Ryzen 5 7600X CPU, RTX 4060Ti (8GB VRAM), and 64GB RAM while using far fewer training cases. This model achieves post-processed Dice scores of 0.929 (WT), 0.856 (TC), and 0.821 (ET), with HD95 distances of 2.93, 6.78, and 10.35 voxels. These results are significantly better than the winning BraTS 2023 methodology for HD95 distances and WT dice scores. This demonstrates that a KLE-based residual anomaly map can dramatically reduce computational cost and data requirements while retaining state-of-the-art performance.

</details>


### [968] [SCG With Your Phone: Diagnosis of Rhythmic Spectrum Disorders in Field Conditions](https://arxiv.org/abs/2601.13926)
*Peter Golenderov,Yaroslav Matushenko,Anastasia Tushina,Michal Barodkin*

Main category: q-bio.QM

TL;DR: The paper presents a deep-learning framework using consumer smartphones for reliable cardiac rhythm analysis and SCG segmentation, focusing on noisy signal environments.


<details>
  <summary>Details</summary>
Motivation: Detecting cardiac rhythm disorders using noisy seismocardiography (SCG) signals collected by smartphones, overcoming challenges like motion artifacts and device variability.

Method: An enhanced U-Net v3 with multi-scale convolutions, residual connections, and attention gates; post-processing for AO timestamps; adaptive 3D-to-1D projection for arbitrary smartphone orientation.

Result: High accuracy and robustness achieved across different devices and unsupervised data-collection environments for practical cardiac monitoring.

Conclusion: The framework provides scalable, low-cost, and automated cardiac rhythm monitoring via mobile devices, supporting field use and future multimodal diagnostics.

Abstract: Aortic valve opening (AO) events are crucial for detecting frequency and rhythm disorders, especially in real-world settings where seismocardiography (SCG) signals collected via consumer smartphones are subject to noise, motion artifacts, and variability caused by device heterogeneity. In this work, we present a robust deep-learning framework for SCG segmentation and rhythm analysis using accelerometer recordings obtained with consumer smartphones. We develop an enhanced U-Net v3 architecture that integrates multi-scale convolutions, residual connections, and attention gates, enabling reliable segmentation of noisy SCG signals. A dedicated post-processing pipeline converts probability masks into precise AO timestamps, whereas a novel adaptive 3D-to-1D projection method ensures robustness to arbitrary smartphone orientation. Experimental results demonstrate that the proposed method achieves consistently high accuracy and robustness across various device types and unsupervised data-collection conditions. Our approach enables practical, low-cost, and automated cardiac-rhythm monitoring using everyday mobile devices, paving the way for scalable, field-deployable cardiovascular assessment and future multimodal diagnostic systems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [969] [Quantum Kernel Machine Learning for Autonomous Materials Science](https://arxiv.org/abs/2601.11775)
*Felix Adams,Daiwei Zhu,David W. Steuerman,A. Gilad Kusne,Ichiro Takeuchi*

Main category: cond-mat.mtrl-sci

TL;DR: This paper explores the use of quantum and classical kernel machine learning for efficient materials discovery, showing quantum kernels can outperform some classical counterparts, particularly in navigating complex x-ray diffraction data.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to efficiently explore new materials with minimal data using autonomous materials science. It leverages quantum kernel models to potentially improve performance over classical methods in navigating complex compositional phase spaces.

Method: The authors compare quantum and classical kernel approaches by analyzing x-ray diffraction patterns from an Fe-Ga-Pd ternary composition spread library. They utilize IonQ's Aria quantum computer and its classical noisy simulator for experiments.

Result: The research demonstrates that quantum kernel models can outperform some classical kernel models in sequential phase space navigation tasks, particularly for complex x-ray diffraction datasets.

Conclusion: Quantum kernel machine learning methods show promise for accelerating materials discovery, suggesting their potential efficacy in analyzing complex data and achieving robust results with less training data.

Abstract: Autonomous materials science, where active learning is used to navigate large compositional phase space, has emerged as a powerful vehicle to rapidly explore new materials. A crucial aspect of autonomous materials science is exploring new materials using as little data as possible. Gaussian process-based active learning allows effective charting of multi-dimensional parameter space with a limited number of training data, and thus is a common algorithmic choice for autonomous materials science. An integral part of the autonomous workflow is the application of kernel functions for quantifying similarities among measured data points. A recent theoretical breakthrough has shown that quantum kernel models can achieve similar performance with less training data than classical models. This signals the possible advantage of applying quantum kernel machine learning to autonomous materials discovery. In this work, we compare quantum and classical kernels for their utility in sequential phase space navigation for autonomous materials science. Specifically, we compute a quantum kernel and several classical kernels for x-ray diffraction patterns taken from an Fe-Ga-Pd ternary composition spread library. We conduct our study on both IonQ's Aria trapped ion quantum computer hardware and the corresponding classical noisy simulator. We experimentally verify that a quantum kernel model can outperform some classical kernel models. The results highlight the potential of quantum kernel machine learning methods for accelerating materials discovery and suggest complex x-ray diffraction data is a candidate for robust quantum kernel model advantage.

</details>


### [970] [Artificial Intelligence in Materials Science and Engineering: Current Landscape, Key Challenges, and Future Trajectorie](https://arxiv.org/abs/2601.12554)
*Iman Peivaste,Salim Belouettar,Francesco Mercuri,Nicholas Fantuzzi,Hamidreza Dehghani,Razieh Izadi,Halliru Ibrahim,Jakub Lengiewicz,Maël Belouettar-Mathis,Kouider Bendine,Ahmed Makradi,Martin Hörsch,Peter Klein,Mohamed El Hachemi,Heinz A. Preisig,Yacine Rezgui,Natalia Konchakova,Ali Daouadji*

Main category: cond-mat.mtrl-sci

TL;DR: The paper reviews how AI, including advanced ML approaches, is revolutionizing materials science by addressing data challenges and enabling new methodologies in material discovery, design, and optimization.


<details>
  <summary>Details</summary>
Motivation: To explore and synthesize the advancements in AI and machine learning, enabling researchers to more effectively leverage data-driven techniques in materials science.

Method: The paper surveys various ML algorithms—CNNs, GNNs, Transformers, generative AI, probabilistic models—and discusses featurization, preprocessing, and representation techniques.

Result: The review provides a structured analysis of AI methodologies, highlights their transformative potential, and outlines challenges like data quality and standardization in materials science.

Conclusion: AI is becoming an essential competency in materials science, reshaping research with advanced methodologies, but persistent data challenges must be resolved for its full potential to be realized.

Abstract: Artificial Intelligence is rapidly transforming materials science and engineering, offering powerful tools to navigate complexity, accelerate discovery, and optimize material design in ways previously unattainable. Driven by the accelerating pace of algorithmic advancements and increasing data availability, AI is becoming an essential competency for materials researchers. This review provides a comprehensive and structured overview of the current landscape, synthesizing recent advancements and methodologies for materials scientists seeking to effectively leverage these data-driven techniques. We survey the spectrum of machine learning approaches, from traditional algorithms to advanced deep learning architectures, including CNNs, GNNs, and Transformers, alongside emerging generative AI and probabilistic models such as Gaussian Processes for uncertainty quantification. The review also examines the pivotal role of data in this field, emphasizing how effective representation and featurization strategies, spanning compositional, structural, image-based, and language-inspired approaches, combined with appropriate preprocessing, fundamentally underpin the performance of machine learning models in materials research. Persistent challenges related to data quality, quantity, and standardization, which critically impact model development and application in materials science and engineering, are also addressed.

</details>


### [971] [Ontology-aligned structuring and reuse of multimodal materials data and workflows towards automatic reproduction](https://arxiv.org/abs/2601.12582)
*Sepideh Baghaee Ravari,Abril Azocar Guzman,Sarath Menon,Stefan Sandfeld,Tilmann Hickel,Markus Stricker*

Main category: cond-mat.mtrl-sci

TL;DR: This paper introduces a framework using an ontology-driven and large language model (LLM)-assisted approach to automate the extraction and structuring of computational workflows from literature, focusing on density functional theory-based stacking fault energy calculations.


<details>
  <summary>Details</summary>
Motivation: The lack of machine-readable workflow descriptions in materials science literature hinders large-scale data curation, reproducibility, and systematic comparison of computational workflows.

Method: An ontology-driven framework combines large language models (LLMs) with multi-stage filtering and prompt-engineered extraction to organize literature data on stacking fault energy calculations into a knowledge graph.

Result: The proposed framework successfully structures materials science data into a semantically interoperable knowledge graph, aligning it with established ontologies, while enabling systematic comparison and improved reusability of computational protocols.

Conclusion: Although full computational reproducibility remains limited by missing metadata, this framework enhances the organization, transparency, and reusability of computational materials science data.

Abstract: Reproducibility of computational results remains a challenge in materials science, as simulation workflows and parameters are often reported only in unstructured text and tables. While literature data are valuable for validation and reuse, the lack of machine-readable workflow descriptions prevents large-scale curation and systematic comparison. Existing text-mining approaches are insufficient to extract complete computational workflows with their associated parameters. An ontology-driven, large language model (LLM)-assisted framework is introduced for the automated extraction and structuring of computational workflows from the literature. The approach focuses on density functional theory-based stacking fault energy (SFE) calculations in hexagonal close-packed magnesium and its binary alloys, and uses a multi-stage filtering strategy together with prompt-engineered LLM extraction applied to method sections and tables. Extracted information is unified into a canonical schema and aligned with established materials ontologies (CMSO, ASMO, and PLDO), enabling the construction of a knowledge graph using atomRDF. The resulting knowledge graph enables systematic comparison of reported SFE values and supports the structured reuse of computational protocols. While full computational reproducibility is still constrained by missing or implicit metadata, the framework provides a foundation for organizing and contextualizing published results in a semantically interoperable form, thereby improving transparency and reusability of computational materials data.

</details>


### [972] [CatMaster: An Agentic Autonomous System for Computational Heterogeneous Catalysis Research](https://arxiv.org/abs/2601.13508)
*Honghao Chen,Jiangjie Qiu,Yi Shen Tew,Xiaonan Wang*

Main category: cond-mat.mtrl-sci

TL;DR: The paper introduces CatMaster, a language-model-driven system for streamlining computational heterogeneous catalysis workflows by managing project records, automating input/output processes, and integrating multi-fidelity tools.


<details>
  <summary>Details</summary>
Motivation: Workflow management for DFT-based catalysis studies can be slow, sensitive to setup errors, and hard to reproduce or extend, motivating the need for automation and consistency in computational workflows.

Method: CatMaster, a language-model-driven agent system, turns natural language inputs into structured calculation workspaces, leveraging a tool library for simplifying catalytic modeling from surrogate methods to DFT.

Result: CatMaster successfully demonstrated its utility in varied scenarios, handling tasks like spin-state checks, alloy screening, adsorption studies, and catalyst geometry preparations, showing versatility and efficiency.

Conclusion: CatMaster supports researchers by reducing manual effort in workflow management while providing transparency and reproducibility, shifting the focus to chemical interpretation.

Abstract: Density functional theory (DFT) is widely used to connect atomic structure with catalytic behavior, but computational heterogeneous catalysis studies often require long workflows that are costly, iterative, and sensitive to setup choices. Besides the intrinsic cost and accuracy limits of first-principles calculations, practical workflow issues such as keeping references consistent, preparing many related inputs, recovering from failed runs on computing clusters, and maintaining a complete record of what was done, can slow down projects and make results difficult to reproduce or extend.
  Here we present CatMaster, a large-language-model (LLM)-driven agent system that turns natural language requests into complete calculation workspaces, including structures, inputs, outputs, logs, and a concise run record. CatMaster maintains a persistent project record of key facts, constraints, and file pointers to support inspection and restartability. It is paired with a multi-fidelity tool library that covers rapid surrogate relaxations and high-fidelity DFT calculations for validation when needed. We demonstrate CatMaster on four demonstrations of increasing complexity: an O2 spin-state check with remote execution, BCC Fe surface energies with a protocol-sensitivity study and CO adsorption site ranking, high-throughput Pt--Ni--Cu alloy screening for hydrogen evolution reaction (HER) descriptors with surrogate-to-DFT validation, and a demonstration beyond the predefined tool set, including equation-of-state fitting for BCC Fe and CO-FeN4-graphene single-atom catalyst geometry preparation. By reducing manual scripting and bookkeeping while keeping the full evidence trail, CatMaster aims to help catalysis researchers focus on modeling choices and chemical interpretation rather than workflow management.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [973] [SplittingSecrets: A Compiler-Based Defense for Preventing Data Memory-Dependent Prefetcher Side-Channels](https://arxiv.org/abs/2601.12270)
*Reshabh K Sharma,Dan Grossman,David Kohlbrenner*

Main category: cs.CR

TL;DR: The paper discusses a software tool called SplittingSecrets to safeguard secrets from new side-channel attacks using Data Memory-dependent Prefetchers (DMPs).


<details>
  <summary>Details</summary>
Motivation: To address the inadequacies of current constant-time programming defenses in countering vulnerabilities introduced by new hardware optimizations, particularly DMPs, which can leak sensitive data without direct program interaction.

Method: The paper proposes SplittingSecrets, a compiler-based tool that prevents DMP activation by ensuring secret data never resembles addresses in memory. This is implemented using LLVM for source-level and backend memory operations on AArch64 architecture.

Result: SplittingSecrets successfully hardens cryptographic libraries like libsodium against DMP issues with manageable performance overhead when tested on Apple M-series CPUs.

Conclusion: The introduced SplittingSecrets tool shows promise in mitigating DMP-induced side-channel risks without the need to fully disable DMP mechanisms, offering a targeted and efficient mitigation strategy.

Abstract: Traditional side-channels take advantage of secrets being used as inputs to unsafe instructions, used for memory accesses, or used in control flow decisions. Constant-time programming, which restricts such code patterns, has been widely adopted as a defense against these vulnerabilities. However, new hardware optimizations in the form of Data Memory-dependent Prefetchers (DMP) present in Apple, Intel, and ARM CPUs have shown such defenses are not sufficient. These prefetchers, unlike classical prefetchers, use the content of memory as well as the trace of prior accesses to determine prefetch targets. An adversary abusing such a prefetcher has been shown to be able to mount attacks leaking data-at-rest; data that is never used by the program, even speculatively, in an unsafe manner.
  In response, this paper introduces SplittingSecrets, a compiler-based tool that can harden software libraries against side-channels arising from DMPs. SplittingSecrets's approach avoids reasoning about the complex internals of different DMPs and instead relies on one key aspect of all DMPs: activation requires data to resemble addresses. To prevent secret data from leaking, SplittingSecrets transforms memory operations to ensure that secrets are never stored in memory in a manner resembling an address, thereby avoiding DMP activation on those secrets. Rather than disable a DMP entirely, SplittingSecrets can provide targeted hardening for only specific secrets entirely in software.
  We have implemented SplittingSecrets using LLVM, supporting both source-level memory operations and those generated by the compiler backend for the AArch64 architecture, We have analyzed the performance overhead involved in safeguarding secrets from DMP-induced attacks using common primitives in libsodium, a popular cryptographic library when built for Apple M-series CPUs.

</details>


### [974] [SimFuzz: Similarity-guided Block-level Mutation for RISC-V Processor Fuzzing](https://arxiv.org/abs/2601.11838)
*Hao Lyu,Jingzheng Wu,Xiang Ling,Yicheng Zhong,Zhiyuan Li,Tianyue Luo*

Main category: cs.CR

TL;DR: The paper introduces SimFuzz, a fuzzing framework addressing limitations in current RISC-V processor fuzzing methods by using historical bug-triggering inputs and similarity-guided mutations, finding 17 bugs, including 14 new ones.


<details>
  <summary>Details</summary>
Motivation: Existing fuzzing techniques for RISC-V have drawbacks like redundant test case generation and over-reliance on coverage metrics, leading to missed corner cases and inefficiencies.

Method: SimFuzz builds a seed corpus from past bug-triggering inputs and utilizes similarity-guided block-level mutation to enhance input space exploration without heavy reliance on coverage metrics.

Result: SimFuzz identified 17 bugs in RISC-V processors (14 new, 7 CVE-assigned) affecting decode and memory units, with implications for system stability. Achieved up to 73.22% multiplexer coverage on the seed corpus.

Conclusion: SimFuzz effectively detects critical security bugs in RISC-V processors and offers better verification techniques, addressing gaps in functional testing and improving system reliability.

Abstract: The Instruction Set Architecture (ISA) defines processor operations and serves as the interface between hardware and software. As an open ISA, RISC-V lowers the barriers to processor design and encourages widespread adoption, but also exposes processors to security risks such as functional bugs. Processor fuzzing is a powerful technique for automatically detecting these bugs. However, existing fuzzing methods suffer from two main limitations. First, their emphasis on redundant test case generation causes them to overlook cross-processor corner cases. Second, they rely too heavily on coverage guidance. Current coverage metrics are biased and inefficient, and become ineffective once coverage growth plateaus. To overcome these limitations, we propose SimFuzz, a fuzzing framework that constructs a high-quality seed corpus from historical bug-triggering inputs and employs similarity-guided, block-level mutation to efficiently explore the processor input space. By introducing instruction similarity, SimFuzz expands the input space around seeds while preserving control-flow structure, enabling deeper exploration without relying on coverage feedback. We evaluate SimFuzz on three widely used open-source RISC-V processors: Rocket, BOOM, and XiangShan, and discover 17 bugs in total, including 14 previously unknown issues, 7 of which have been assigned CVE identifiers. These bugs affect the decode and memory units, cause instruction and data errors, and can lead to kernel instability or system crashes. Experimental results show that SimFuzz achieves up to 73.22% multiplexer coverage on the high-quality seed corpus. Our findings highlight critical security bugs in mainstream RISC-V processors and offer actionable insights for improving functional verification.

</details>


### [975] [Secure Multi-Path Routing with All-or-Nothing Transform for Network-on-Chip Architectures](https://arxiv.org/abs/2601.13610)
*Hansika Weerasena,Matthew Randall,Prabhat Mishra*

Main category: cs.CR

TL;DR: This paper proposes a lightweight security framework for NoCs that uses a quasi-group-based AONT with secure multi-path routing to protect against eavesdropping attacks, achieving lower overhead compared to encryption.


<details>
  <summary>Details</summary>
Motivation: The motivation is to secure communication in NoC-based SoC architectures against eavesdropping attacks without relying on traditional encryption, which is resource-intensive for constrained systems.

Method: The method involves using a quasi-group based All-Or-Nothing Transform (AONT) to split packets into blocks and transferring these blocks via multiple non-overlapping paths, ensuring data cannot be reconstructed without all blocks.

Result: The proposed method significantly mitigates eavesdropping attacks with minimal performance and area overhead, achieving a 7.3x reduction in overhead in comparison to traditional encryption.

Conclusion: AONT combined with multi-path routing is an effective and efficient alternative to encryption for securing NoC systems, offering reduced resource consumption while ensuring data confidentiality.

Abstract: Ensuring Network-on-Chip (NoC) security is crucial to design trustworthy NoC-based System-on-Chip (SoC) architectures. While there are various threats that exploit on-chip communication vulnerabilities, eavesdropping attacks via malicious nodes are among the most common and stealthy. Although encryption can secure packets for confidentiality, it may introduce unacceptable overhead for resource-constrained SoCs. In this paper, we propose a lightweight confidentiality-preserving framework that utilizes a quasi-group based All-Or-Nothing Transform (AONT) combined with secure multi-path routing in NoC-based SoCs. By applying AONT to each packet and distributing its transformed blocks across multiple non-overlapping routes, we ensure that no intermediate router can reconstruct the original data without all blocks. Extensive experimental evaluation demonstrates that our method effectively mitigates eavesdropping attacks by malicious routers with negligible area and performance overhead. Our results also reveal that AONT-based multi-path routing can provide 7.3x reduction in overhead compared to traditional encryption for securing against eavesdropping attacks.

</details>


### [976] [SWORD: A Secure LoW-Latency Offline-First Authentication and Data Sharing Scheme for Resource Constrained Distributed Networks](https://arxiv.org/abs/2601.12875)
*Faisal Haque Bappy,Tahrim Hossain,Raiful Hasan,Kamrul Hasan,Mohamed Younis,Tariqul Islam*

Main category: cs.CR

TL;DR: The paper introduces SWORD, an offline-first authentication and data-sharing approach for resource-constrained networks, optimizing latency and security while outperforming blockchain-based solutions.


<details>
  <summary>Details</summary>
Motivation: To tackle central server dependency and latency issues in resource-constrained networks, particularly in IoT and IoV, which require fast, secure, and decentralized operations.

Method: Developing SWORD, a proximity-based clustering mechanism to enable low-latency offline authentication and data sharing, coupled with thorough security analysis.

Result: SWORD demonstrated superior performance over blockchain-based alternatives, providing resource efficiency and matching central-server-based authentication latency.

Conclusion: SWORD offers a practical solution to latency and security challenges in resource-constrained networks, bridging the gap between decentralized functionality and real-time efficiency.

Abstract: While many resource-constrained networks, such as Internet of Things (IoT) and Internet of Vehicles (IoV), are inherently distributed, the majority still rely on central servers for fast authentication and data sharing. Blockchain-based solutions offer decentralized alternatives but often struggle to meet the stringent latency requirements of real-time applications. Even with the rollout of 5G, network latency between servers and peers remains a significant challenge. To address this, we introduce SWORD, a novel offline-first authentication and data-sharing scheme designed specifically for resource-constrained networks. SWORD utilizes a proximity-based clustering approach to enable offline authentication and data sharing, ensuring low-latency, secure operations even in intermittently connected scenarios. Our experimental results show that SWORD outperforms traditional blockchain-based solutions while offering similar resource efficiency and authentication latency to central-server-based solutions. Additionally, we provide a comprehensive security analysis, demonstrating that SWORD is resilient against spoofing, impersonation, replay, and man-in-the-middle attacks.

</details>


### [977] [Automatic Adjustment of HPA Parameters and Attack Prevention in Kubernetes Using Random Forests](https://arxiv.org/abs/2601.13515)
*Hanlin Zhou,Huah Yong Chan,Jingfei Ni,Mengchun Wu,Qing Deng*

Main category: cs.CR

TL;DR: The paper integrates HTTP status codes with HPA using Random Forest machine learning to dynamically manage attack traffic and isolate malicious IPs.


<details>
  <summary>Details</summary>
Motivation: To develop an effective mechanism for managing targeted attack traffic and minimizing disruptions during high load conditions using HPA.

Method: Integrating Random Forest for attack prediction and dynamically adjusting HPA pod parameters while using honeypot pods for isolating attack traffic.

Result: The method reduced the occurrence of 5XX status codes, effectively isolated attack traffic, and prevented undesired HPA expansion during attacks.

Conclusion: Machine learning-based dynamic HPA adjustments improve attack traffic management and ensure system robustness in various conditions.

Abstract: In this paper, HTTP status codes are used as custom metrics within the HPA as the experimental scenario. By integrating the Random Forest classification algorithm from machine learning, attacks are assessed and predicted, dynamically adjusting the maximum pod parameter in the HPA to manage attack traffic. This approach enables the adjustment of HPA parameters using machine learning scripts in targeted attack scenarios while effectively managing attack traffic. All access from attacking IPs is redirected to honeypot pods, achieving a lower incidence of 5XX status codes through HPA pod adjustments under high load conditions. This method also ensures effective isolation of attack traffic, preventing excessive HPA expansion due to attacks. Additionally, experiments conducted under various conditions demonstrate the importance of setting appropriate thresholds for HPA adjustments.

</details>


### [978] [Know Your Contract: Extending eIDAS Trust into Public Blockchains](https://arxiv.org/abs/2601.13903)
*Awid Vaziry,Christoph Wronka,Sandro Rodriguez Garzon,Axel Küpper*

Main category: cs.CR

TL;DR: The paper proposes an architecture to integrate the EU eIDAS trust framework with public blockchains, enabling regulatory-compliant DeFi and institutional adoption.


<details>
  <summary>Details</summary>
Motivation: Public blockchains lack mechanisms for attributing on-chain actions to legally accountable entities, creating hurdles for regulatory compliance and institutional use.

Method: The architecture cryptographically binds smart contracts to qualified electronic seals, leverages the EU eIDAS framework, and introduces compliance workflows (on-chain and off-chain).

Result: Smart contracts gain a verifiable, regulatory-compliant chain of trust, enabling automated validations such as Know Your Contract without additional trusted intermediaries.

Conclusion: The framework facilitates institutional DeFi, secure asset tokenization, and compliance-ready blockchain operations aligned with evolving EU regulations like eIDAS.

Abstract: Public blockchains lack native mechanisms to attribute on-chain actions to legally accountable entities, creating a fundamental barrier to institutional adoption and regulatory compliance. This paper presents an architecture that extends the European Union eIDAS trust framework into public blockchain ecosystems by cryptographically binding smart contracts to qualified electronic seals issued by Qualified Trust Service Providers. The mechanism establishes a verifiable chain of trust from the European Commission List of Trusted Lists to individual on-chain addresses, enabling machine-verifiable proofs for automated regulatory validation, such as Know Your Contract, Counterparty, and Business checks, without introducing new trusted intermediaries. Regulatory requirements arising from eIDAS, MiCA, PSD2, PSR, and the proposed European Business Wallet are analyzed, and a cryptographic suite meeting both eIDAS implementing regulations and EVM execution constraints following the Ethereum Fusaka upgrade is identified, namely ECDSA with P-256 and CAdES formatting. Two complementary trust validation models are presented: an off-chain workflow for agent-to-agent payment protocols and a fully on-chain workflow enabling regulatory-compliant DeFi operations between legal entities. The on-chain model converts regulatory compliance from a per-counterparty administrative burden into an automated, standardized process, enabling mutual validation at first interaction without prior business relationships. As eIDAS wallets become mandatory across EU member states, the proposed architecture provides a pathway for integrating European digital trust infrastructure into blockchain-based systems, enabling institutional DeFi participation, real-world asset tokenization, and agentic commerce within a trusted, regulatory-compliant framework.

</details>


### [979] [SecureSplit: Mitigating Backdoor Attacks in Split Learning](https://arxiv.org/abs/2601.14054)
*Zhihao Dou,Dongfei Cui,Weida Wang,Anjun Gao,Yueyang Quan,Mengyao Ma,Viet Vo,Guangdong Bai,Zhuqing Liu,Minghong Fang*

Main category: cs.CR

TL;DR: The paper introduces SecureSplit, a defense against backdoor attacks in Split Learning by using dimensionality transformation and adaptive filtering strategies.


<details>
  <summary>Details</summary>
Motivation: To mitigate the susceptibility of Split Learning to backdoor attacks, which threaten data integrity and model security.

Method: SecureSplit applies dimensionality transformations to distinguish between benign and poisoned embeddings and uses a majority-based adaptive filtering scheme to isolate contaminated data.

Result: SecureSplit demonstrated efficacy against backdoor attacks across four datasets and various challenging attack scenarios, outperforming seven compared defenses.

Conclusion: SecureSplit effectively defends Split Learning against backdoor attacks while maintaining data privacy and model performance.

Abstract: Split Learning (SL) offers a framework for collaborative model training that respects data privacy by allowing participants to share the same dataset while maintaining distinct feature sets. However, SL is susceptible to backdoor attacks, in which malicious clients subtly alter their embeddings to insert hidden triggers that compromise the final trained model. To address this vulnerability, we introduce SecureSplit, a defense mechanism tailored to SL. SecureSplit applies a dimensionality transformation strategy to accentuate subtle differences between benign and poisoned embeddings, facilitating their separation. With this enhanced distinction, we develop an adaptive filtering approach that uses a majority-based voting scheme to remove contaminated embeddings while preserving clean ones. Rigorous experiments across four datasets (CIFAR-10, MNIST, CINIC-10, and ImageNette), five backdoor attack scenarios, and seven alternative defenses confirm the effectiveness of SecureSplit under various challenging conditions.

</details>


### [980] [A Survey on Mapping Digital Systems with Bill of Materials: Development, Practices, and Challenges](https://arxiv.org/abs/2601.11678)
*Shuai Zhang,Minzhao Lyu,Hassan Habibi Gharakheili*

Main category: cs.CR

TL;DR: This survey reviews the evolution, practices, and uses of Bills of Materials (BOMs) in modern digital ecosystems, identifies gaps in current frameworks, and suggests areas for future research.


<details>
  <summary>Details</summary>
Motivation: The growing complexity of digital ecosystems makes managing dependencies within software, hardware, AI models, datasets, and cryptography challenging. BOMs aim to bring clarity and security through detailed documentation of components and their relationships.

Method: The paper surveys the development of BOM frameworks, summarizes their stages of evolution, and analyzes industry and academic practices across various domains. It also evaluates the practical uses of BOM data and highlights gaps in existing models.

Result: BOMs are critical for dependency mapping, compliance, security, and risk assessment. However, the study identifies four major gaps in current BOM frameworks, limiting their effectiveness.

Conclusion: There is a need for further refinement, standardization, and new developments in BOM frameworks to better serve digital supply chains and emerging technologies like AI and data ecosystems.

Abstract: Modern digital ecosystems, spanning software, hardware, learning models, datasets, and cryptographic products, continue to grow in complexity, making it difficult for organizations to understand and manage component dependencies. Bills of Materials (BOMs) have emerged as a structured way to document product components, their interrelationships, and key metadata, improving visibility and security across digital supply chains. This survey provides the first comprehensive cross-domain review of BOM developments and practices. We start by examining the evolution of BOM frameworks in three stages (i.e., pre-development, initial, and accelerated) and summarizing their core principles, key stakeholders, and standardization efforts for hardware, software, artificial intelligence (AI) models, datasets, and cryptographic assets. We then review industry practices for generating BOM data, evaluating its quality, and securely sharing it. Next, we review practical downstream uses of BOM data, including dependency modeling, compliance verification, operational risk assessment, and vulnerability tracking. We also discuss academic efforts to address limitations in current BOM frameworks through refinements, extensions, or new models tailored to emerging domains such as data ecosystems and AI supply chains. Finally, we identify four key gaps that limit the usability and reliability of today's BOM frameworks, motivating future research directions.

</details>


### [981] [Attesting Model Lineage by Consisted Knowledge Evolution with Fine-Tuning Trajectory](https://arxiv.org/abs/2601.11683)
*Zhuoyi Shang,Jiasen Li,Pengzhen Chen,Yanwei Liu,Xiaoyan Gu,Weiping Wang*

Main category: cs.CR

TL;DR: The paper presents a new framework for verifying model lineage by analyzing both knowledge evolution and parameter changes introduced by fine-tuning, inspired by human genetic evolution.


<details>
  <summary>Details</summary>
Motivation: In open-weight model libraries, there is a pressing need for robust mechanisms to verify model lineage to address issues like unauthorized redistribution and false claims of model provenance.

Method: The proposed method utilizes model editing to measure parameter changes and a novel knowledge vectorization mechanism to create compact representations of evolved knowledge using probe samples. These embeddings are used to verify arithmetic consistency in knowledge evolution.

Result: Experimental results show the method’s effectiveness and resilience in verifying model lineage across diverse model types, including classifiers, diffusion models, and large language models, even under adversarial scenarios.

Conclusion: The framework provides a robust solution for model lineage attestation, capable of handling a wide range of models and adversarial conditions, with significant implications for security in open-weight libraries.

Abstract: The fine-tuning technique in deep learning gives rise to an emerging lineage relationship among models. This lineage provides a promising perspective for addressing security concerns such as unauthorized model redistribution and false claim of model provenance, which are particularly pressing in \textcolor{blue}{open-weight model} libraries where robust lineage verification mechanisms are often lacking. Existing approaches to model lineage detection primarily rely on static architectural similarities, which are insufficient to capture the dynamic evolution of knowledge that underlies true lineage relationships. Drawing inspiration from the genetic mechanism of human evolution, we tackle the problem of model lineage attestation by verifying the joint trajectory of knowledge evolution and parameter modification. To this end, we propose a novel model lineage attestation framework. In our framework, model editing is first leveraged to quantify parameter-level changes introduced by fine-tuning. Subsequently, we introduce a novel knowledge vectorization mechanism that refines the evolved knowledge within the edited models into compact representations by the assistance of probe samples. The probing strategies are adapted to different types of model families. These embeddings serve as the foundation for verifying the arithmetic consistency of knowledge relationships across models, thereby enabling robust attestation of model lineage. Extensive experimental evaluations demonstrate the effectiveness and resilience of our approach in a variety of adversarial scenarios in the real world. Our method consistently achieves reliable lineage verification across a broad spectrum of model types, including classifiers, diffusion models, and large language models.

</details>


### [982] [Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?](https://arxiv.org/abs/2601.12349)
*Yi Qian,Kunwei Qian,Xingbang He,Ligeng Chen,Jikang Zhang,Tiantai Zhang,Haiyang Wei,Linzhang Wang,Hao Wu,Bing Mao*

Main category: cs.CR

TL;DR: This paper reveals a fundamental flaw in Android GUI agents' integration with the operating system, enabling attacks that exploit the gap between observation and action in the agents' reasoning process.


<details>
  <summary>Details</summary>
Motivation: To reveal vulnerabilities in Android GUI agents where their assumption of Visual Atomicity creates a critical attack surface.

Method: The authors introduced Action Rebinding, an attack that exploits the observation-to-action gap, creating programmable multi-step attack chains and employing an Intent Alignment Strategy (IAS) to bypass verification gates.

Result: The attacks achieve a 100% success rate for atomic action rebinding and bypass verification gates, undetected by malware scanners. They expose the agents to exploitation without requiring sensitive permissions or privileged API calls.

Conclusion: Current Android agent design features a fundamental architectural flaw, emphasizing the necessity for more secure agent-OS integrations to prevent such vulnerabilities.

Abstract: Large multimodal model powered GUI agents are emerging as high-privilege operators on mobile platforms, entrusted with perceiving screen content and injecting inputs. However, their design operates under the implicit assumption of Visual Atomicity: that the UI state remains invariant between observation and action. We demonstrate that this assumption is fundamentally invalid in Android, creating a critical attack surface.
  We present Action Rebinding, a novel attack that allows a seemingly-benign app with zero dangerous permissions to rebind an agent's execution. By exploiting the inevitable observation-to-action gap inherent in the agent's reasoning pipeline, the attacker triggers foreground transitions to rebind the agent's planned action toward the target app. We weaponize the agent's task-recovery logic and Android's UI state preservation to orchestrate programmable, multi-step attack chains. Furthermore, we introduce an Intent Alignment Strategy (IAS) that manipulates the agent's reasoning process to rationalize UI states, enabling it to bypass verification gates (e.g., confirmation dialogs) that would otherwise be rejected.
  We evaluate Action Rebinding Attacks on six widely-used Android GUI agents across 15 tasks. Our results demonstrate a 100% success rate for atomic action rebinding and the ability to reliably orchestrate multi-step attack chains. With IAS, the success rate in bypassing verification gates increases (from 0% to up to 100%). Notably, the attacker application requires no sensitive permissions and contains no privileged API calls, achieving a 0% detection rate across malware scanners (e.g., VirusTotal). Our findings reveal a fundamental architectural flaw in current agent-OS integration and provide critical insights for the secure design of future agent systems. To access experimental logs and demonstration videos, please contact yi_qian@smail.nju.edu.cn.

</details>


### [983] [The Cost of Convenience: Identifying, Analyzing, and Mitigating Predatory Loan Applications on Android](https://arxiv.org/abs/2601.12634)
*Olawale Amos Akanji,Manuel Egele,Gianluca Stringhini*

Main category: cs.CR

TL;DR: Loan apps in emerging markets misuse user data for coercive practices and often violate regulations; this study measures compliance, revealing pervasive issues and prompting Google to remove non-compliant apps.


<details>
  <summary>Details</summary>
Motivation: This paper addresses the misuse of sensitive user data by loan apps in emerging markets, aiming to protect user privacy and ensure regulatory compliance.

Method: The study analyzes 434 apps from five countries using LLM-assisted mapping and combined static and dynamic code analyses to evaluate compliance with national and Google policies.

Result: The findings show extensive non-compliance, with 141 apps violating national and 147 Google policies. Disclosures led to the removal of 93 apps, collectively installed over 300M times.

Conclusion: The research emphasizes the need for better enforcement, proactive compliance monitoring, and enhanced privacy protections in digital lending platforms.

Abstract: Digital lending applications, commonly referred to as loan apps, have become a primary channel for microcredit in emerging markets. However, many of these apps demand excessive permissions and misuse sensitive user data for coercive debt-recovery practices, including harassment, blackmail, and public shaming that affect both borrowers and their contacts.
  This paper presents the first cross-country measurement of loan app compliance against both national regulations and Google's Financial Services Policy. We analyze 434 apps drawn from official registries and app markets from Indonesia, Kenya, Nigeria, Pakistan, and the Philippines. To operationalize policy requirements at scale, we translate policy text into testable permission checks using LLM-assisted policy-to-permission mapping and combine this with static and dynamic analyses of loan apps' code and runtime behavior.
  Our findings reveal pervasive non-compliance among approved apps: 141 violate national regulatory policy and 147 violate Google policy. Dynamic analysis further shows that several apps transmit sensitive data (contacts, SMS, location, media) before user signup or registration, undermining informed consent and enabling downstream harassment of borrowers and third parties. Following our disclosures, Google removed 93 flagged apps from Google Play, representing over 300M cumulative installs.
  We advocate for adopting our methodology as a proactive compliance-monitoring tool and offer targeted recommendations for regulators, platforms, and developers to strengthen privacy protections. Overall, our results highlight the need for coordinated enforcement and robust technical safeguards to ensure that digital lending supports financial inclusion without compromising user privacy or safety.

</details>


### [984] [Eliciting Harmful Capabilities by Fine-Tuning On Safeguarded Outputs](https://arxiv.org/abs/2601.13528)
*Jackson Kaunismaa,Avery Griffin,John Hughes,Christina Q. Knight,Mrinank Sharma,Erik Jones*

Main category: cs.CR

TL;DR: The paper investigates elicitation attacks that bypass safeguards in high-level models to develop harmful capabilities in open-source models.


<details>
  <summary>Details</summary>
Motivation: The study aims to demonstrate vulnerabilities in safeguarding measures that allow harm through misuse of AI models.

Method: It involves constructing domain-adjacent prompts, using output from safeguarded models, and fine-tuning open-source models with the results.

Result: The attacks successfully close about 40% of the capability gap between base open-source and unrestricted models and their effectiveness scales with model capability and data.

Conclusion: Safeguards implemented at an output level in powerful models are insufficient to prevent ecosystem-wide risks of misuse.

Abstract: Model developers implement safeguards in frontier models to prevent misuse, for example, by employing classifiers to filter dangerous outputs. In this work, we demonstrate that even robustly safeguarded models can be used to elicit harmful capabilities in open-source models through elicitation attacks. Our elicitation attacks consist of three stages: (i) constructing prompts in adjacent domains to a target harmful task that do not request dangerous information; (ii) obtaining responses to these prompts from safeguarded frontier models; (iii) fine-tuning open-source models on these prompt-output pairs. Since the requested prompts cannot be used to directly cause harm, they are not refused by frontier model safeguards. We evaluate these elicitation attacks within the domain of hazardous chemical synthesis and processing, and demonstrate that our attacks recover approximately 40% of the capability gap between the base open-source model and an unrestricted frontier model. We then show that the efficacy of elicitation attacks scales with the capability of the frontier model and the amount of generated fine-tuning data. Our work demonstrates the challenge of mitigating ecosystem level risks with output-level safeguards.

</details>


### [985] [Serverless AI Security: Attack Surface Analysis and Runtime Protection Mechanisms for FaaS-Based Machine Learning](https://arxiv.org/abs/2601.11664)
*Chetan Pathade,Vinod Dhimam,Sheheryar Ahmad,Ilsa Lareb*

Main category: cs.CR

TL;DR: This paper analyzes security vulnerabilities in serverless machine learning environments and proposes a defense framework called Serverless AI Shield (SAS).


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of serverless computing for machine learning inference workloads brings scalability and cost-efficiency but introduces new security challenges due to the fragmented nature of serverless architectures.

Method: The paper provides a systematic characterization of the serverless ML attack surface, empirical assessments on major serverless platforms, and proposes SAS—a framework with multi-layered defense mechanisms including validation, monitoring, and forensics.

Result: Empirical assessments reveal real-world attack scenarios and SAS achieves a detection rate of 94% with less than 9% inference latency overhead.

Conclusion: Serverless AI Shield enhances security for serverless machine learning systems, enabling more resilient cloud-native AI solutions through its open-source toolkit.

Abstract: Serverless computing has achieved widespread adoption, with over 70% of AWS organizations using serverless solutions [1]. Meanwhile, machine learning inference workloads increasingly migrate to Function-as-a-Service (FaaS) platforms for their scalability and cost-efficiency [2], [3], [4]. However, this convergence introduces critical security challenges, with recent reports showing a 220% increase in AI/ML vulnerabilities [5] and serverless computing's fragmented architecture raises new security concerns distinct from traditional cloud deployments [6], [7]. This paper presents the first comprehensive security analysis of machine learning workloads in serverless environments. We systematically characterize the attack surface across five categories: function-level vulnerabilities (cold start exploitation, dependency poisoning), model-specific threats (API-based extraction, adversarial inputs), infrastructure attacks (cross-function contamination, privilege escalation), supply chain risks (malicious layers, backdoored libraries), and IAM complexity (ephemeral nature, serverless functions). Through empirical assessments across AWS Lambda, Azure Functions, and Google Cloud Functions, we demonstrate real-world attack scenarios and quantify their security impact. We propose Serverless AI Shield (SAS), a multi-layered defense framework providing pre-deployment validation, runtime monitoring, and post-execution forensics. Our evaluation shows SAS achieves 94% detection rates while maintaining performance overhead below 9% for inference latency. We release an open-source security toolkit to enable practitioners to assess and harden their serverless AI deployments, advancing the field toward more resilient cloud-native machine learning systems.

</details>


### [986] [Hybrid IDS Using Signature-Based and Anomaly-Based Detection](https://arxiv.org/abs/2601.11998)
*Messaouda Boutassetta,Amina Makhlouf,Newfel Messaoudi,Abdelmadjid Benmachiche,Ines Boutabia*

Main category: cs.CR

TL;DR: This paper provides a survey on Hybrid Intrusion Detection Systems (Hybrid IDS), analyzing their types, advantages, limitations, and trends.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional IDS systems, such as high false positives and difficulty detecting new attacks, by exploring Hybrid IDS integration techniques.

Method: The study surveys and categorizes existing Hybrid IDS research, reviews machine learning and cloud-based approaches, and discusses their application domains.

Result: The research highlights the strengths and weaknesses of Hybrid IDS, recent trends, and suggests future research toward cost-effective and better-performing systems.

Conclusion: Hybrid IDS can enhance detection by combining anomaly-based and signature-based methods, with potential for development in emerging technologies for better cyberattack prevention.

Abstract: Intrusion detection systems (IDS) are essential for protecting computer systems and networks against a wide range of cyber threats that continue to evolve over time. IDS are commonly categorized into two main types, each with its own strengths and limitations, such as difficulty in detecting previously unseen attacks and the tendency to generate high false positive rates. This paper presents a comprehensive survey and a conceptual overview of Hybrid IDS, which integrate signature-based and anomaly-based detection techniques to enhance attack detection capabilities. The survey examines recent research on Hybrid IDS, classifies existing models into functional categories, and discusses their advantages, limitations, and application domains, including financial systems, air traffic control, and social networks. In addition, recent trends in Hybrid IDS research, such as machine learning-based approaches and cloud-based deployments, are reviewed. Finally, this work outlines potential future research directions aimed at developing more cost-effective Hybrid IDS solutions with improved ability to detect emerging and sophisticated cyberattacks.

</details>


### [987] [Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models](https://arxiv.org/abs/2601.12042)
*Xiaomei Zhang,Zhaoxi Zhang,Leo Yu Zhang,Yanjun Zhang,Guanhong Tao,Shirui Pan*

Main category: cs.CR

TL;DR: This study investigates the security vulnerabilities introduced by visual token compression in Large Vision-Language Models (LVLMs). While enhancing efficiency, the compression makes robust models highly vulnerable to specific threats.


<details>
  <summary>Details</summary>
Motivation: To explore the security implications of visual token compression in LVLMs, which have been overlooked despite its efficiency benefits.

Method: The paper identifies token ranking instability as the cause of robustness degradation and proposes a Compression-Aware Attack (CAA) targeting this vulnerability. It also develops Transfer CAA for black-box scenarios.

Result: Compression-induced model vulnerabilities are confirmed experimentally across models, datasets, and methods, showing a significant efficiency-security trade-off.

Conclusion: Visual token compression, while improving efficiency, introduces notable robustness flaws, exposing the need for stronger defensive measures to address this trade-off.

Abstract: Visual token compression is widely adopted to improve the inference efficiency of Large Vision-Language Models (LVLMs), enabling their deployment in latency-sensitive and resource-constrained scenarios. However, existing work has mainly focused on efficiency and performance, while the security implications of visual token compression remain largely unexplored. In this work, we first reveal that visual token compression substantially degrades the robustness of LVLMs: models that are robust under uncompressed inference become highly vulnerable once compression is enabled. These vulnerabilities are state-specific; failure modes emerge only in the compressed setting and completely disappear when compression is disabled, making them particularly hidden and difficult to diagnose. By analyzing the key stages of the compression process, we identify instability in token importance ranking as the primary cause of this robustness degradation. Small and imperceptible perturbations can significantly alter token rankings, leading the compression mechanism to mistakenly discard task-critical information and ultimately causing model failure. Motivated by this observation, we propose a Compression-Aware Attack to systematically study and exploit this vulnerability. CAA directly targets the token selection mechanism and induces failures exclusively under compressed inference. We further extend this approach to more realistic black-box settings and introduce Transfer CAA, where neither the target model nor the compression configuration is accessible. We further evaluate potential defenses and find that they provide only limited protection. Extensive experiments across models, datasets, and compression methods show that visual token compression significantly undermines robustness, revealing a previously overlooked efficiency-security trade-off.

</details>


### [988] [Semantic Differentiation for Tackling Challenges in Watermarking Low-Entropy Constrained Generation Outputs](https://arxiv.org/abs/2601.11629)
*Nghia T. Le,Alan Ritter,Kartik Goyal*

Main category: cs.CR

TL;DR: SeqMark is a semantic-based watermarking algorithm for constrained language tasks, improving watermark detection and addressing issues with prior methods.


<details>
  <summary>Details</summary>
Motivation: Current language model watermarking methods are insufficient for constrained generation tasks with low-entropy spaces, prompting the need for improved techniques.

Method: Introduced SeqMark, a sequence-level watermarking algorithm that differentiates high-probability subspaces into valid and invalid regions to balance quality, detectability, and imperceptibility.

Result: SeqMark enhances watermark detection accuracy by up to 28% F1 on tasks like machine translation, code generation, and summarization, while preserving output quality.

Conclusion: SeqMark effectively resolves issues with prior watermarking methods, offering a robust solution for constrained generation tasks while optimizing detection and quality.

Abstract: We demonstrate that while the current approaches for language model watermarking are effective for open-ended generation, they are inadequate at watermarking LM outputs for constrained generation tasks with low-entropy output spaces. Therefore, we devise SeqMark, a sequence-level watermarking algorithm with semantic differentiation that balances the output quality, watermark detectability, and imperceptibility. It improves on the shortcomings of the prevalent token-level watermarking algorithms that cause under-utilization of the sequence-level entropy available for constrained generation tasks. Moreover, we identify and improve upon a different failure mode we term region collapse, associated with prior sequence-level watermarking algorithms. This occurs because the pseudorandom partitioning of semantic space for watermarking in these approaches causes all high-probability outputs to collapse into either invalid or valid regions, leading to a trade-off in output quality and watermarking effectiveness. SeqMark instead, differentiates the high-probable output subspace and partitions it into valid and invalid regions, ensuring the even spread of high-quality outputs among all the regions. On various constrained generation tasks like machine translation, code generation, and abstractive summarization, SeqMark substantially improves watermark detection accuracy (up to 28% increase in F1) while maintaining high generation quality.

</details>


### [989] [Zero-Shot Embedding Drift Detection: A Lightweight Defense Against Prompt Injections in LLMs](https://arxiv.org/abs/2601.12359)
*Anirudh Sekar,Mrinal Agarwal,Rachel Sharma,Akitsugu Tanaka,Jasmine Zhang,Arjun Damerla,Kevin Zhu*

Main category: cs.CR

TL;DR: The paper introduces ZEDD, a robust and generalizable framework for detecting prompt injection attacks in LLMs by utilizing semantic embedding drift measurement, achieving high accuracy without requiring model-specific modifications.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to prompt injection attacks that exploit indirect input channels, posing risks despite advancements in model alignment. There is a need for effective and adaptable detection mechanisms to address this vulnerability.

Method: The authors propose the ZEDD framework, which detects prompt injection attempts by quantifying semantic shifts in embedding space using cosine similarity between benign and suspect inputs. ZEDD operates in a zero-shot manner without requiring model-specific access or retraining.

Result: ZEDD achieves over 93% accuracy in detecting prompt injection attacks across various LLM architectures, with a false positive rate of less than 3%. It demonstrates superior efficiency and robustness compared to existing methods.

Conclusion: Embedding drift serves as a reliable signal for detecting prompt injection attacks. ZEDD offers a scalable and lightweight solution to safeguard LLMs effectively and can be integrated into existing pipelines to mitigate adversarial threats.

Abstract: Prompt injection attacks have become an increasing vulnerability for LLM applications, where adversarial prompts exploit indirect input channels such as emails or user-generated content to circumvent alignment safeguards and induce harmful or unintended outputs. Despite advances in alignment, even state-of-the-art LLMs remain broadly vulnerable to adversarial prompts, underscoring the urgent need for robust, productive, and generalizable detection mechanisms beyond inefficient, model-specific patches. In this work, we propose Zero-Shot Embedding Drift Detection (ZEDD), a lightweight, low-engineering-overhead framework that identifies both direct and indirect prompt injection attempts by quantifying semantic shifts in embedding space between benign and suspect inputs. ZEDD operates without requiring access to model internals, prior knowledge of attack types, or task-specific retraining, enabling efficient zero-shot deployment across diverse LLM architectures. Our method uses adversarial-clean prompt pairs and measures embedding drift via cosine similarity to capture subtle adversarial manipulations inherent to real-world injection attacks. To ensure robust evaluation, we assemble and re-annotate the comprehensive LLMail-Inject dataset spanning five injection categories derived from publicly available sources. Extensive experiments demonstrate that embedding drift is a robust and transferable signal, outperforming traditional methods in detection accuracy and operational efficiency. With greater than 93% accuracy in classifying prompt injections across model architectures like Llama 3, Qwen 2, and Mistral and a false positive rate of <3%, our approach offers a lightweight, scalable defense layer that integrates into existing LLM pipelines, addressing a critical gap in securing LLM-powered systems to withstand adaptive adversarial threats.

</details>


### [990] [De-Anonymization at Scale via Tournament-Style Attribution](https://arxiv.org/abs/2601.12407)
*Lirui Zhang,Huishuai Zhang*

Main category: cs.CR

TL;DR: The paper introduces DAS, a method leveraging LLMs for de-anonymizing authorship among large corpora, uncovering privacy vulnerabilities of anonymous platforms.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns are important as large language models (LLMs) are increasingly used in real-world applications, especially regarding authorship anonymity in settings like double-blind peer reviews.

Method: DAS uses LLMs combined with dense-retrieval filtering and iterative candidate ranking to identify authorship from large candidate pools, using majority voting to improve precision.

Result: DAS demonstrated substantial effectiveness in de-anonymizing texts from pools of tens of thousands on anonymized review datasets and outperformed prior methods on authorship benchmarks.

Conclusion: DAS unveils a significant privacy risk for anonymous authorship, showcasing the scalability and accuracy of LLM-based de-anonymization methods.

Abstract: As LLMs rapidly advance and enter real-world use, their privacy implications are increasingly important. We study an authorship de-anonymization threat: using LLMs to link anonymous documents to their authors, potentially compromising settings such as double-blind peer review.
  We propose De-Anonymization at Scale (DAS), a large language model-based method for attributing authorship among tens of thousands of candidate texts. DAS uses a sequential progression strategy: it randomly partitions the candidate corpus into fixed-size groups, prompts an LLM to select the text most likely written by the same author as a query text, and iteratively re-queries the surviving candidates to produce a ranked top-k list. To make this practical at scale, DAS adds a dense-retrieval prefilter to shrink the search space and a majority-voting style aggregation over multiple independent runs to improve robustness and ranking precision. Experiments on anonymized review data show DAS can recover same-author texts from pools of tens of thousands with accuracy well above chance, demonstrating a realistic privacy risk for anonymous platforms. On standard authorship benchmarks (Enron emails and blog posts), DAS also improves both accuracy and scalability over prior approaches, highlighting a new LLM-enabled de-anonymization vulnerability.

</details>


### [991] [Privacy-Preserving Federated Learning with Verifiable Fairness Guarantees](https://arxiv.org/abs/2601.12447)
*Mohammed Himayath Ali,Mohammed Aqib Abdullah,Syed Muneer Hussin,Mohammed Mudassir Uddin,Shahnawaz Alam*

Main category: cs.CR

TL;DR: The paper presents CryptoFair-FL, a framework providing verifiable fairness guarantees in federated learning through cryptographic methods.


<details>
  <summary>Details</summary>
Motivation: To address the unresolved issue of ensuring fairness and preserving privacy in federated learning across diverse and sensitive datasets.

Method: Combining additively homomorphic encryption with secure multi-party computation to verify fairness metrics like demographic parity and equalized odds securely and privately, with a batched protocol reducing computational costs and preserving differential privacy.

Result: CryptoFair-FL demonstrated significant reductions in fairness violations and maintained privacy against adversarial attacks across four benchmark datasets, with manageable computational overhead.

Conclusion: CryptoFair-FL provides a practical and secure solution for fairness-aware federated learning suitable for regulated industries, achieving near-optimal tradeoffs between fairness, privacy, and efficiency.

Abstract: Federated learning enables collaborative model training across distributed institutions without centralizing sensitive data; however, ensuring algorithmic fairness across heterogeneous data distributions while preserving privacy remains fundamentally unresolved. This paper introduces CryptoFair-FL, a novel cryptographic framework providing the first verifiable fairness guarantees for federated learning systems under formal security definitions. The proposed approach combines additively homomorphic encryption with secure multi-party computation to enable privacy-preserving verification of demographic parity and equalized odds metrics without revealing protected attribute distributions or individual predictions. A novel batched verification protocol reduces computational complexity from BigO(n^2) to BigO(n \log n) while maintaining (\dparam, \deltap)-differential privacy with dparam = 0.5 and deltap = 10^{-6}. Theoretical analysis establishes information-theoretic lower bounds on the privacy cost of fairness verification, demonstrating that the proposed protocol achieves near-optimal privacy-fairness tradeoffs. Comprehensive experiments across four benchmark datasets (MIMIC-IV healthcare records, Adult Income, CelebA, and a novel FedFair-100 benchmark) demonstrate that CryptoFair-FL reduces fairness violations from 0.231 to 0.031 demographic parity difference while incurring only 2.3 times computational overhead compared to standard federated averaging. The framework successfully defends against attribute inference attacks, maintaining adversarial success probability below 0.05 across all tested configurations. These results establish a practical pathway for deploying fairness-aware federated learning in regulated industries requiring both privacy protection and algorithmic accountability.

</details>


### [992] [Efficient Privacy-Preserving Retrieval Augmented Generation with Distance-Preserving Encryption](https://arxiv.org/abs/2601.12331)
*Huanyi Ye,Jiale Guo,Ziyao Liu,Kwok-Yan Lam*

Main category: cs.CR

TL;DR: This paper presents ppRAG, a privacy-preserving framework for retrieval-augmented generation (RAG) in untrusted cloud environments, enhancing privacy and efficiency without significant computational cost.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the privacy risks associated with embedding-based retrieval in RAG systems outsourced to untrusted cloud storage, focusing on privacy leakage and computational efficiency.

Method: The proposed framework, ppRAG, employs Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption (CAPRISE) to encrypt embeddings while maintaining relative distance ordering for similarity computation. It incorporates differential privacy by perturbing query embeddings to prevent pattern inference.

Result: Experimental outcomes demonstrate that ppRAG efficiently processes queries while providing strong privacy guarantees and maintaining high retrieval accuracy.

Conclusion: ppRAG offers a practical and efficient solution for ensuring privacy in cloud-based RAG systems, benefiting users with limited resources seeking secure augmented LLMs.

Abstract: RAG has emerged as a key technique for enhancing response quality of LLMs without high computational cost. In traditional architectures, RAG services are provided by a single entity that hosts the dataset within a trusted local environment. However, individuals or small organizations often lack the resources to maintain data storage servers, leading them to rely on outsourced cloud storage. This dependence on untrusted third-party services introduces privacy risks. Embedding-based retrieval mechanisms, commonly used in RAG systems, are vulnerable to privacy leakage such as vector-to-text reconstruction attacks and structural leakage via vector analysis. Several privacy-preserving RAG techniques have been proposed but most existing approaches rely on partially homomorphic encryption, which incurs substantial computational overhead. To address these challenges, we propose an efficient privacy-preserving RAG framework (ppRAG) tailored for untrusted cloud environments that defends against vector-to-text attack, vector analysis, and query analysis. We propose Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption (CAPRISE) that encrypts embeddings while still allowing the cloud to compute similarity between an encrypted query and the encrypted database embeddings. CAPRISE preserves only the relative distance ordering between the encrypted query and each encrypted database embedding, without exposing inter-database distances, thereby enhancing both privacy and efficiency. To mitigate query analysis, we introduce DP by perturbing the query embedding prior to encryption, preventing the cloud from inferring sensitive patterns. Experimental results show that ppRAG achieves efficient processing throughput, high retrieval accuracy, strong privacy guarantees, making it a practical solution for resource-constrained users seeking secure cloud-augmented LLMs.

</details>


### [993] [MongoDB Injection Query Classification Model using MongoDB Log files as Training Data](https://arxiv.org/abs/2601.11996)
*Shaunak Perni,Minal Shirodkar,Ramdas Karmalli*

Main category: cs.CR

TL;DR: This paper examines NoSQL injection attack classification based on MongoDB server log data, utilizing AutoML and manual machine learning models, achieving a top accuracy of 71%.


<details>
  <summary>Details</summary>
Motivation: To develop effective detection for NoSQL injection attacks, as previous rule-based and model-based systems faced challenges due to innovative attacks, limited data, and class imbalances.

Method: Classified attacks using log data and extracted features instead of raw query statements, employing FLAML AutoML and 6 manual models cross-validated on randomized samples.

Result: Achieved best model performance with FLAML's XGBoost limited depth, attaining a 71% accuracy on the processed dataset.

Conclusion: Using log data and automatic machine learning methods like FLAML shows promise in detecting NoSQL injection attacks, though accuracy can still improve.

Abstract: NoSQL Injection attacks are a class of cybersecurity attacks where an attacker sends a specifically engineered query to a NoSQL database which then performs an unauthorized operation. To defend against such attacks, rule based systems were initially developed but then were found to be ineffective to innovative injection attacks hence a model based approach was developed. Most model based detection systems, during testing gave exponentially positive results but were trained only on the query statement sent to the server. However due to the scarcity of data and class imbalances these model based systems were found to be not effective against all attacks in the real world. This paper explores classifying NoSQL injection attacks sent to a MongoDB server based on Log Data, and other extracted features excluding raw query statements. The log data was collected from a simulated attack on an empty MongoDB server which was then processed and explored. A discriminant analysis was carried out to determine statistically significant features to discriminate between injection and benign queries resulting in a dataset of significant features. Several Machine learning based classification models using an AutoML library, "FLAML", as well as 6 manually programmed models were trained on this dataset , which were then trained on 50 randomized samples of data, cross validated and evaluated. The study found that the best model was the "FLAML" library's "XGBoost limited depth" model with an accuracy of 71%.

</details>


### [994] [AgenTRIM: Tool Risk Mitigation for Agentic AI](https://arxiv.org/abs/2601.12449)
*Roy Betser,Shamik Bose,Amit Giloni,Chiara Picardi,Sindhu Padakandla,Roman Vainshtein*

Main category: cs.CR

TL;DR: Introduces AgenTRIM, a framework to mitigate security risks from improper tool permissions in AI agents while maintaining task performance.


<details>
  <summary>Details</summary>
Motivation: AI agents using tools face security risks like indirect prompt injection due to improper tool permissions. The study aims to balance tool access to enhance safety and performance.

Method: AgenTRIM detects and mitigates tool-driven risks via offline code and execution trace verification, and enforces runtime policies via adaptive filtering.

Result: AgenTRIM substantially reduces attack success rates while keeping task performance high, and shows robustness to descriptive attacks and explicit safety policy enforcement.

Conclusion: AgenTRIM provides a practical solution for improving the security of LLM-based agents by addressing improper tool management without affecting capabilities.

Abstract: AI agents are autonomous systems that combine LLMs with external tools to solve complex tasks. While such tools extend capability, improper tool permissions introduce security risks such as indirect prompt injection and tool misuse. We characterize these failures as unbalanced tool-driven agency. Agents may retain unnecessary permissions (excessive agency) or fail to invoke required tools (insufficient agency), amplifying the attack surface and reducing performance. We introduce AgenTRIM, a framework for detecting and mitigating tool-driven agency risks without altering an agent's internal reasoning. AgenTRIM addresses these risks through complementary offline and online phases. Offline, AgenTRIM reconstructs and verifies the agent's tool interface from code and execution traces. At runtime, it enforces per-step least-privilege tool access through adaptive filtering and status-aware validation of tool calls. Evaluating on the AgentDojo benchmark, AgenTRIM substantially reduces attack success while maintaining high task performance. Additional experiments show robustness to description-based attacks and effective enforcement of explicit safety policies. Together, these results demonstrate that AgenTRIM provides a practical, capability-preserving approach to safer tool use in LLM-based agents.

</details>


### [995] [TrojanPraise: Jailbreak LLMs via Benign Fine-Tuning](https://arxiv.org/abs/2601.12460)
*Zhixin Xie,Xurui Song,Jun Luo*

Main category: cs.CR

TL;DR: TrojanPraise proposes a novel attack on LLMs by maliciously fine-tuning them with benign and filter-approved data, bypassing moderation to shift the LLM's compliance with harmful concepts.


<details>
  <summary>Details</summary>
Motivation: Addressing the security loophole wherein black-box fine-tuning services allow attackers to stealthily modify an LLM's behavior.

Method: A fine-tuning method using benign data to subtly redirect the LLM's responses by associating crafted words with harmful concepts, while maintaining undetectable moderation.

Result: Experiments on five open-source and two commercial LLMs show a maximum attack success rate of 95.88%, successfully evading detection mechanisms.

Conclusion: TrojanPraise unveils the vulnerabilities in current moderation systems, demonstrating the feasibility of undetected malicious fine-tuning using benign data.

Abstract: The demand of customized large language models (LLMs) has led to commercial LLMs offering black-box fine-tuning APIs, yet this convenience introduces a critical security loophole: attackers could jailbreak the LLMs by fine-tuning them with malicious data. Though this security issue has recently been exposed, the feasibility of such attacks is questionable as malicious training dataset is believed to be detectable by moderation models such as Llama-Guard-3. In this paper, we propose TrojanPraise, a novel finetuning-based attack exploiting benign and thus filter-approved data. Basically, TrojanPraise fine-tunes the model to associate a crafted word (e.g., "bruaf") with harmless connotations, then uses this word to praise harmful concepts, subtly shifting the LLM from refusal to compliance. To explain the attack, we decouple the LLM's internal representation of a query into two dimensions of knowledge and attitude. We demonstrate that successful jailbreak requires shifting the attitude while avoiding knowledge shift, a distortion in the model's understanding of the concept. To validate this attack, we conduct experiments on five opensource LLMs and two commercial LLMs under strict black-box settings. Results show that TrojanPraise achieves a maximum attack success rate of 95.88% while evading moderation.

</details>


### [996] [BlocksecRT-DETR: Decentralized Privacy-Preserving and Token-Efficient Federated Transformer Learning for Secure Real-Time Object Detection in ITS](https://arxiv.org/abs/2601.12693)
*Mohoshin Ara Tahera,Sabbir Rahman,Shuvalaxmi Dass,Sharif Ullah,Mahmoud Abouyessef*

Main category: cs.CR

TL;DR: The paper proposes BlockSecRT-DETR, a federated system utilizing transformers and blockchain for intelligent transportation, addressing data heterogeneity, latency, and privacy challenges.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve intelligent transportation systems (ITS) through real-time object detection using federated learning, addressing key challenges like data heterogeneity, latency constraints, and privacy/security risks.

Method: The framework integrates RT-DETR with a Token Engineering Module (TEM) to improve efficiency and incorporates blockchain for decentralized, secure model aggregation.

Result: TEM reduces latency by 17.2%, decreases encoder complexity by 47.8%, and retains high detection accuracy (89.2% mAP@0.5). Blockchain integration adds moderate computational overhead of 400 ms per round and keeps ledger size minimal (under 12 KB).

Conclusion: BlockSecRT-DETR effectively addresses ITS challenges, providing efficient and secure federated object detection via transformer optimization and blockchain integration.

Abstract: Federated real-time object detection using transformers in Intelligent Transportation Systems (ITS) faces three major challenges: (1) missing-class non-IID data heterogeneity from geographically diverse traffic environments, (2) latency constraints on edge hardware for high-capacity transformer models, and (3) privacy and security risks from untrusted client updates and centralized aggregation. We propose BlockSecRT-DETR, a BLOCKchain-SECured Real-Time Object DEtection TRansformer framework for ITS that provides a decentralized, token-efficient, and privacy-preserving federated training solution using RT-DETR transformer, incorporating a blockchain-secured update validation mechanism for trustworthy aggregation. In this framework, challenges (1) and (2) are jointly addressed through a unified client-side design that integrates RT-DETR training with a Token Engineering Module (TEM). TEM prunes low-utility tokens, reducing encoder complexity and latency on edge hardware, while aggregated updates mitigate non-IID data heterogeneity across clients. To address challenge (3), BlockSecRT-DETR incorporates a decentralized blockchain-secured update validation mechanism that enables tamper-proof, privacy-preserving, and trust-free authenticated model aggregation without relying on a central server. We evaluated the proposed framework under a missing-class Non-IID partition of the KITTI dataset and conducted a blockchain case study to quantify security overhead. TEM improves inference latency by 17.2% and reduces encoder FLOPs by 47.8%, while maintaining global detection accuracy (89.20% mAP@0.5). The blockchain integration adds 400 ms per round, and the ledger size remains under 12 KB due to metadata-only on-chain storage.

</details>


### [997] [PDFInspect: A Unified Feature Extraction Framework for Malicious Document Detection](https://arxiv.org/abs/2601.12866)
*Sharmila S P*

Main category: cs.CR

TL;DR: The paper introduces a framework to analyze PDFs for identifying potential malicious content by using graph-theoretic, structural, and metadata-driven features.


<details>
  <summary>Details</summary>
Motivation: The rising threat of malicious PDFs necessitates advanced techniques for their detection and analysis, as existing methods may lack comprehensiveness.

Method: The paper combines multiple analyses: a graph-based modeling for text relationships, metadata parsing for inconsistencies, temporal feature extraction, and quantification of structural elements, resulting in a 170-dimensional feature vector.

Result: The framework provides a robust feature set that enhances tasks like malware classification and anomaly detection, making it scalable and adaptable to real-world scenarios.

Conclusion: The integration of graph, metadata, and structural features into a unified framework significantly improves PDF malware analysis and supports advanced threat intelligence workflows.

Abstract: The increasing prevalence of malicious Portable Document Format (PDF) files necessitates robust and comprehensive feature extraction techniques for effective detection and analysis. This work presents a unified framework that integrates graph-based, structural, and metadata-driven analysis to generate a rich feature representation for each PDF document. The system extracts text from PDF pages and constructs undirected graphs based on pairwise word relationships, enabling the computation of graph-theoretic features such as node count, edge density, and clustering coefficient. Simultaneously, the framework parses embedded metadata to quantify character distributions, entropy patterns, and inconsistencies across fields such as author, title, and producer. Temporal features are derived from creation and modification timestamps to capture behavioral signatures, while structural elements including, object streams, fonts, and embedded images, are quantified to reflect document complexity. Boolean flags for potentially malicious PDF constructs (e.g., JavaScript, launch actions) are also extracted. Together, these features form a high-dimensional vector representation (170 dimensions) that is well-suited for downstream tasks such as malware classification, anomaly detection, and forensic analysis. The proposed approach is scalable, extensible, and designed to support real-world PDF threat intelligence workflows.6

</details>


### [998] [Your Privacy Depends on Others: Collusion Vulnerabilities in Individual Differential Privacy](https://arxiv.org/abs/2601.12922)
*Johannes Kaiser,Alexander Ziller,Eleni Triantafillou,Daniel Rückert,Georgios Kaissis*

Main category: cs.CR

TL;DR: The paper identifies vulnerabilities in Individual Differential Privacy (iDP) due to privacy risks being influenced by others' choices, exposing users to additional risks while staying within formal guarantees.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to address a gap in iDP systems where users' privacy risks are influenced by collective decisions, undermining the promise of individual privacy control.

Method: The authors performed empirical evaluations to demonstrate privacy vulnerabilities due to collective privacy settings and proposed an enhanced privacy framework $(\varepsilon_i, δ_i, \overlineΔ)$-iDP to limit excess risks.

Result: Empirical evaluations revealed that 62% of targeted individuals could suffer increased privacy inference risks. The proposed framework offers robust privacy assurances against such vulnerabilities.

Conclusion: The study highlights a major flaw in existing iDP mechanisms and emphasizes the need to reimagine iDP systems to make risks transparent, introducing a new framework to address this issue.

Abstract: Individual Differential Privacy (iDP) promises users control over their privacy, but this promise can be broken in practice. We reveal a previously overlooked vulnerability in sampling-based iDP mechanisms: while conforming to the iDP guarantees, an individual's privacy risk is not solely governed by their own privacy budget, but critically depends on the privacy choices of all other data contributors. This creates a mismatch between the promise of individual privacy control and the reality of a system where risk is collectively determined. We demonstrate empirically that certain distributions of privacy preferences can unintentionally inflate the privacy risk of individuals, even when their formal guarantees are met. Moreover, this excess risk provides an exploitable attack vector. A central adversary or a set of colluding adversaries can deliberately choose privacy budgets to amplify vulnerabilities of targeted individuals. Most importantly, this attack operates entirely within the guarantees of DP, hiding this excess vulnerability. Our empirical evaluation demonstrates successful attacks against 62% of targeted individuals, substantially increasing their membership inference susceptibility. To mitigate this, we propose $(\varepsilon_i,δ_i,\overlineΔ)$-iDP a privacy contract that uses $Δ$-divergences to provide users with a hard upper bound on their excess vulnerability, while offering flexibility to mechanism design. Our findings expose a fundamental challenge to the current paradigm, demanding a re-evaluation of how iDP systems are designed, audited, communicated, and deployed to make excess risks transparent and controllable.

</details>


### [999] [On the Evidentiary Limits of Membership Inference for Copyright Auditing](https://arxiv.org/abs/2601.12937)
*Murat Bilgehan Ertan,Emirhan Böge,Min Chen,Kaleel Mahmood,Marten van Dijk*

Main category: cs.CR

TL;DR: The paper analyzes the reliability of membership inference attacks (MIAs) as evidence in copyright disputes for large language models (LLMs), introducing the SAGE framework to test robustness against semantic-preserving paraphrasing transformations.


<details>
  <summary>Details</summary>
Motivation: Determine if MIAs can reliably serve as evidence in adversarial copyright disputes, especially when training data has undergone semantic-preserving obfuscation.

Method: The researchers proposed SAGE, a paraphrasing framework driven by Sparse Autoencoders (SAEs) to alter lexical structure while maintaining semantic content, and evaluated its impact on MIAs.

Result: MIAs deteriorate considerably when models are fine-tuned using SAGE-generated paraphrases, showing vulnerability to semantic-preserving transformations.

Conclusion: MIAs are fragile under adversarial conditions and cannot be solely relied upon for copyright auditing in LLMs.

Abstract: As large language models (LLMs) are trained on increasingly opaque corpora, membership inference attacks (MIAs) have been proposed to audit whether copyrighted texts were used during training, despite growing concerns about their reliability under realistic conditions. We ask whether MIAs can serve as admissible evidence in adversarial copyright disputes where an accused model developer may obfuscate training data while preserving semantic content, and formalize this setting through a judge-prosecutor-accused communication protocol. To test robustness under this protocol, we introduce SAGE (Structure-Aware SAE-Guided Extraction), a paraphrasing framework guided by Sparse Autoencoders (SAEs) that rewrites training data to alter lexical structure while preserving semantic content and downstream utility. Our experiments show that state-of-the-art MIAs degrade when models are fine-tuned on SAGE-generated paraphrases, indicating that their signals are not robust to semantics-preserving transformations. While some leakage remains in certain fine-tuning regimes, these results suggest that MIAs are brittle in adversarial settings and insufficient, on their own, as a standalone mechanism for copyright auditing of LLMs.

</details>


### [1000] [Adversarial News and Lost Profits: Manipulating Headlines in LLM-Driven Algorithmic Trading](https://arxiv.org/abs/2601.13082)
*Advije Rizvani,Giovanni Apruzzese,Pavel Laskov*

Main category: cs.CR

TL;DR: This paper explores the vulnerability of LLM-based sentiment analysis on financial news to adversarial manipulation. Such attacks can mislead trading decisions, leading to significant monetary losses.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address a major vulnerability in the adoption of LLMs in algorithmic trading, where manipulated news could intentionally mislead trading systems, causing financial losses.

Method: The authors designed an attack scenario using human-imperceptible manipulations, like Unicode homoglyph substitutions and hidden-text clauses, and evaluated it on a realistic ATS incorporating sentiment analysis by financial LLMs and LSTM-based price forecasting.

Result: Experiments demonstrated that a single day of adversarial manipulation could reduce annual trading returns by up to 17.7 percentage points, highlighting the significant monetary impact of such attacks.

Conclusion: The study underscores the importance of addressing these vulnerabilities in LLM-integrated financial platforms and advocates for increased security measures to mitigate such risks.

Abstract: Large Language Models (LLMs) are increasingly adopted in the financial domain. Their exceptional capabilities to analyse textual data make them well-suited for inferring the sentiment of finance-related news. Such feedback can be leveraged by algorithmic trading systems (ATS) to guide buy/sell decisions. However, this practice bears the risk that a threat actor may craft "adversarial news" intended to mislead an LLM. In particular, the news headline may include "malicious" content that remains invisible to human readers but which is still ingested by the LLM. Although prior work has studied textual adversarial examples, their system-wide impact on LLM-supported ATS has not yet been quantified in terms of monetary risk. To address this threat, we consider an adversary with no direct access to an ATS but able to alter stock-related news headlines on a single day. We evaluate two human-imperceptible manipulations in a financial context: Unicode homoglyph substitutions that misroute models during stock-name recognition, and hidden-text clauses that alter the sentiment of the news headline. We implement a realistic ATS in Backtrader that fuses an LSTM-based price forecast with LLM-derived sentiment (FinBERT, FinGPT, FinLLaMA, and six general-purpose LLMs), and quantify monetary impact using portfolio metrics. Experiments on real-world data show that manipulating a one-day attack over 14 months can reliably mislead LLMs and reduce annual returns by up to 17.7 percentage points. To assess real-world feasibility, we analyze popular scraping libraries and trading platforms and survey 27 FinTech practitioners, confirming our hypotheses. We notified trading platform owners of this security issue.

</details>


### [1001] [Diffusion-Driven Synthetic Tabular Data Generation for Enhanced DoS/DDoS Attack Classification](https://arxiv.org/abs/2601.13197)
*Aravind B,Anirud R. S.,Sai Surya Teja N,Bala Subrahmanya Sriranga Navaneeth A,Karthika R,Mohankumar N*

Main category: cs.CR

TL;DR: The paper addresses class imbalance in network intrusion detection using Tabular Denoising Diffusion Probability Models (TabDDPM) for generating minority-class samples.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in datasets leads to biased model performance, particularly impacting security-related domains like intrusion detection.

Method: TabDDPM is employed for generating synthetic high-fidelity minority-class samples through iterative denoising. These samples are merged with existing data for augmentation.

Result: Augmented data significantly improves ANN classifier performance, achieving near-perfect recall on underrepresented attack classes.

Conclusion: Diffusion models show promise for resolving tabular data imbalance, with potential benefits in security, fraud, and medical diagnostics applications.

Abstract: Class imbalance refers to a situation where certain classes in a dataset have significantly fewer samples than oth- ers, leading to biased model performance. Class imbalance in network intrusion detection using Tabular Denoising Diffusion Probability Models (TabDDPM) for data augmentation is ad- dressed in this paper. Our approach synthesizes high-fidelity minority-class samples from the CIC-IDS2017 dataset through iterative denoising processes. For the minority classes that have smaller samples, synthetic samples were generated and merged with the original dataset. The augmented training data enables an ANN classifier to achieve near-perfect recall on previously underrepresented attack classes. These results establish diffusion models as an effective solution for tabular data imbalance in security domains, with potential applications in fraud detection and medical diagnostics.

</details>


### [1002] [HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware Code Generation](https://arxiv.org/abs/2601.13864)
*Qirui Chen,Jingxian Shuai,Shuangwu Chen,Shenghao Ye,Zijian Wen,Xufei Su,Jie Jin,Jiangming Li,Jun Chen,Xiaobin Tan,Jian Yang*

Main category: cs.CR

TL;DR: The paper presents HardSecBench, a benchmark for assessing LLM-generated code security in hardware and firmware contexts, highlighting security risks even in functionally correct solutions.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding and evaluating the security aspects of LLM-generated code, particularly for hardware and firmware applications.

Method: The authors create HardSecBench, a benchmark comprising 924 tasks across Verilog RTL and firmware-level C, with a multi-agent pipeline for automating artifact synthesis and evaluation based on execution evidence.

Result: They evaluated LLM performance using HardSecBench, finding frequent security flaws in functionally correct code and variations based on different prompts.

Conclusion: LLM-generated code often sacrifices security for functionality. HardSecBench provides insights to tackle these challenges and offers guidance for advancing secure LLM-assisted hardware design.

Abstract: Large language models (LLMs) are being increasingly integrated into practical hardware and firmware development pipelines for code generation. Existing studies have primarily focused on evaluating the functional correctness of LLM-generated code, yet paid limited attention to its security issues. However, LLM-generated code that appears functionally sound may embed security flaws which could induce catastrophic damages after deployment. This critical research gap motivates us to design a benchmark for assessing security awareness under realistic specifications. In this work, we introduce HardSecBench, a benchmark with 924 tasks spanning Verilog Register Transfer Level (RTL) and firmware-level C, covering 76 hardware-relevant Common Weakness Enumeration (CWE) entries. Each task includes a structured specification, a secure reference implementation, and executable tests. To automate artifact synthesis, we propose a multi-agent pipeline that decouples synthesis from verification and grounds evaluation in execution evidence, enabling reliable evaluation. Using HardSecBench, we evaluate a range of LLMs on hardware and firmware code generation and find that models often satisfy functional requirements while still leaving security risks. We also find that security results vary with prompting. These findings highlight pressing challenges and offer actionable insights for future advancements in LLM-assisted hardware design. Our data and code will be released soon.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1003] [On Nonasymptotic Confidence Intervals for Treatment Effects in Randomized Experiments](https://arxiv.org/abs/2601.11744)
*Ricardo J. Sandoval,Sivaraman Balakrishnan,Avi Feller,Michael I. Jordan,Ian Waudby-Smith*

Main category: stat.ME

TL;DR: The paper presents nonasymptotic confidence intervals for treatment effects in randomized experiments, addressing performance gaps in effective sample sizes.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and efficiency of nonasymptotic confidence intervals, which traditionally exhibit weaker performance compared to asymptotic counterparts due to dependence on propensity scores.

Method: The authors utilize systematic exploitation of negative dependence and variance adaptivity to design nonasymptotic confidence intervals with the same effective sample size as asymptotic methods.

Result: The proposed intervals close the performance gap, achieving nonasymptotic rates that are proven to be unimprovable in an information-theoretic sense.

Conclusion: The study enhances the applicability of nonasymptotic confidence intervals, offering a more precise tool for randomized experiments while reaching the theoretical performance limits.

Abstract: We study nonasymptotic (finite-sample) confidence intervals for treatment effects in randomized experiments. In the existing literature, the effective sample sizes of nonasymptotic confidence intervals tend to be looser than the corresponding central-limit-theorem-based confidence intervals by a factor depending on the square root of the propensity score. We show that this performance gap can be closed, designing nonasymptotic confidence intervals that have the same effective sample size as their asymptotic counterparts. Our approach involves systematic exploitation of negative dependence or variance adaptivity (or both). We also show that the nonasymptotic rates that we achieve are unimprovable in an information-theoretic sense.

</details>


### [1004] [Lost in Aggregation: The Causal Interpretation of the IV Estimand](https://arxiv.org/abs/2601.12120)
*Danielle Tsao,Krikamol Muandet,Frederick Eberhardt,Emilija Perković*

Main category: stat.ME

TL;DR: This paper critiques the validity of instrumental variable (IV) approaches for estimating causal effects in cases involving aggregate treatment variables, exploring the ambiguous nature of such effects and the limits of IV estimators.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address prevailing challenges in the validity of IV methods, particularly in situations where the treatment comprises aggregate variables, which complicate causal effect estimation and interpretation.

Method: The paper introduces a formalization of the aggregate-constrained component intervention distribution and characterizes the conditions under which IV estimators identify aggregate effects in such complex settings.

Result: The authors demonstrate that the causal effect of aggregate treatments is ambiguous and contingent upon specific conditions, emphasizing the limitations of conventional IV estimators when applied to aggregate variables.

Conclusion: The paper highlights significant restrictions on interpreting IV estimates based on aggregate treatments and stresses the need for a more robust justification for instrument validity in such contexts.

Abstract: Instrumental variable based estimation of a causal effect has emerged as a standard approach to mitigate confounding bias in the social sciences and epidemiology, where conducting randomized experiments can be too costly or impossible. However, justifying the validity of the instrument often poses a significant challenge. In this work, we highlight a problem generally neglected in arguments for instrumental variable validity: the presence of an ''aggregate treatment variable'', where the treatment (e.g., education, GDP, caloric intake) is composed of finer-grained components that each may have a different effect on the outcome. We show that the causal effect of an aggregate treatment is generally ambiguous, as it depends on how interventions on the aggregate are instantiated at the component level, formalized through the aggregate-constrained component intervention distribution. We then characterize conditions on the interventional distribution and the aggregate setting under which standard instrumental variable estimators identify the aggregate effect. The contrived nature of these conditions implies major limitations on the interpretation of instrumental variable estimates based on aggregate treatments and highlights the need for a broader justificatory base for the exclusion restriction in such settings.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1005] [From HNSW to Information-Theoretic Binarization: Rethinking the Architecture of Scalable Vector Search](https://arxiv.org/abs/2601.11557)
*Seyed Moein Abtahi,Majid Fekri,Tara Khani,Akramul Azim*

Main category: cs.DB

TL;DR: This paper proposes an alternative architecture for semantic search and retrieval-augmented generation, focusing on binary representations and deterministic retrieval to reduce cost and improve efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the cost and architectural limitations of the current "HNSW + float32 + cosine similarity" stack used in semantic search systems, which face trade-offs between latency, throughput, and accuracy.

Method: The paper introduces a novel approach called maximally informative binarization (MIB) combined with efficient bitwise distance metrics and an information-theoretic scoring (ITS) mechanism, enabling exhaustive search over dense binary representations.

Result: The proposed solution demonstrates retrieval quality comparable to full-precision systems but achieves lower latency and sustained throughput under high query concurrency, as shown across 14 datasets and 10,038 queries.

Conclusion: This architecture enables a cost-effective, serverless deployment model, challenging the reliance on large in-memory ANN indexes for semantic search with high quality.

Abstract: Modern semantic search and retrieval-augmented generation (RAG) systems rely predominantly on in-memory approximate nearest neighbor (ANN) indexes over high-precision floating-point vectors, resulting in escalating operational cost and inherent trade-offs between latency, throughput, and retrieval accuracy. This paper analyzes the architectural limitations of the dominant "HNSW + float32 + cosine similarity" stack and evaluates existing cost-reduction strategies, including storage disaggregation and lossy vector quantization, which inevitably sacrifice either performance or accuracy. We introduce and empirically evaluate an alternative information-theoretic architecture based on maximally informative binarization (MIB), efficient bitwise distance metrics, and an information-theoretic scoring (ITS) mechanism. Unlike conventional ANN systems, this approach enables exhaustive search over compact binary representations, allowing deterministic retrieval and eliminating accuracy degradation under high query concurrency. Using the MAIR benchmark across 14 datasets and 10,038 queries, we compare this architecture against Elasticsearch, Pinecone, PGVector, and Qdrant. Results demonstrate retrieval quality comparable to full-precision systems, while achieving substantially lower latency and maintaining constant throughput at high request rates. We show that this architectural shift enables a truly serverless, cost-per-query deployment model, challenging the necessity of large in-memory ANN indexes for high-quality semantic search.

</details>


### [1006] [GPU-Resident Inverted File Index for Streaming Vector Databases](https://arxiv.org/abs/2601.11808)
*Dongfang Zhao*

Main category: cs.DB

TL;DR: SIVF introduces a GPU-native architecture for vector databases with efficient high-velocity data ingestion and deletion using a slab-based allocation system and validity bitmap, enabling massive improvements in latency and throughput.


<details>
  <summary>Details</summary>
Motivation: The motivation is addressing the inefficiency of traditional GPU-accelerated Inverted File indexes in streaming scenarios, where real-time data updates cause significant latency.

Method: SIVF employs a slab-based allocation system with validity bitmap for in-place mutation in VRAM and utilizes a GPU-resident address translation table for $O(1)$ access to physical storage slots.

Result: SIVF drastically reduces deletion latency (up to 13,300×) and improves ingestion throughput (36× to 105×), achieving significant speedup in sliding window scenarios while maintaining minimal memory overhead.

Conclusion: SIVF enables real-time performance in vector databases and dynamic applications, eliminating latency bottlenecks and achieving substantial efficiency without major storage penalties.

Abstract: Vector search has emerged as the computational backbone of modern AI infrastructure, powering critical systems ranging from Vector Databases to Retrieval-Augmented Generation (RAG). While the GPU-accelerated Inverted File (IVF) index acts as one of the most widely used techniques for these large-scale workloads due to its memory efficiency, its traditional architecture remains fundamentally static. Existing designs rely on rigid and contiguous memory layouts that lack native support for in-place mutation, creating a severe bottleneck for streaming scenarios. In applications requiring real-time knowledge updates, such as live recommendation engines or dynamic RAG systems, maintaining index freshness necessitates expensive CPU-GPU roundtrips that cause system latency to spike from milliseconds to seconds. In this paper, we propose SIVF (Streaming Inverted File), a new GPU-native architecture designed to empower vector databases with high-velocity data ingestion and deletion capabilities. SIVF replaces the static memory layout with a slab-based allocation system and a validity bitmap, enabling lock-free and in-place mutation directly in VRAM. We further introduce a GPU-resident address translation table (ATT) to resolve the overhead of locating vectors, providing $O(1)$ access to physical storage slots. We evaluate SIVF against the industry-standard GPU IVF implementation on the SIFT1M and GIST1M datasets. Microbenchmarks demonstrate that SIVF reduces deletion latency by up to $13,300\times$ (from 11.8 seconds to 0.89 ms on GIST1M) and improves ingestion throughput by $36\times$ to $105\times$. In end-to-end sliding window scenarios, SIVF eliminates system freezes and achieves a $161\times$ to $266\times$ speedup with single-digit millisecond latency. Notably, this performance incurs negligible storage penalty, maintaining less than 0.8\% memory overhead compared to static indices.

</details>


### [1007] [Knowledge Graph Construction for Stock Markets with LLM-Based Explainable Reasoning](https://arxiv.org/abs/2601.11528)
*Cheonsol Lee,Youngsang Jeong,Jeongyeol Shin,Huiju Kim,Jidong Kim*

Main category: cs.DB

TL;DR: This paper introduces a knowledge graph tailored for the stock market and integrates it with large language models (LLMs) to enhance investment analysis and provide explainable answers to complex financial queries.


<details>
  <summary>Details</summary>
Motivation: Traditional stock market research methods mainly focus on time-series forecasting or single-company analysis, lacking the ability to capture complex interdependencies, competitive dynamics, and relational patterns effectively.

Method: The paper proposes a specialized knowledge graph schema to model relationships in the stock market. This graph is combined with LLMs for multi-hop reasoning and relational queries, offering an in-depth analysis and explanation pipeline.

Result: The framework was validated through case studies on Korean listed companies, demonstrating its effectiveness in producing insights that conventional database queries cannot achieve.

Conclusion: Combining knowledge graphs with LLMs enriches advanced investment analysis and supports more informed decision-making compared to traditional methods.

Abstract: The stock market is inherently complex, with interdependent relationships among companies, sectors, and financial indicators. Traditional research has largely focused on time-series forecasting and single-company analysis, relying on numerical data for stock price prediction. While such approaches can provide short-term insights, they are limited in capturing relational patterns, competitive dynamics, and explainable investment reasoning. To address these limitations, we propose a knowledge graph schema specifically designed for the stock market, modeling companies, sectors, stock indicators, financial statements, and inter-company relationships. By integrating this schema with large language models (LLMs), our approach enables multi-hop reasoning and relational queries, producing explainable and in-depth answers to complex financial questions. Figure1 illustrates the system pipeline, detailing the flow from data collection and graph construction to LLM-based query processing and answer generation. We validate the proposed framework through practical case studies on Korean listed companies, demonstrating its capability to extract insights that are difficult or impossible to obtain from traditional database queries alone. The results highlight the potential of combining knowledge graphs with LLMs for advanced investment analysis and decision support.

</details>


### [1008] [Uniqueness ratio as a predictor of a privacy leakage](https://arxiv.org/abs/2601.11550)
*Danah A. AlSalem AlKhashti*

Main category: cs.DB

TL;DR: The study explores pre-join indicators for identity leakage during database integration, focusing on uniqueness ratios as an early risk signal.


<details>
  <summary>Details</summary>
Motivation: Minimizing privacy risks during database integrations by identifying simple, interpretable pre-join warning mechanisms.

Method: The research examines synthetic datasets to calculate uniqueness ratios for join attributes and correlates them with post-join identity leakage risks.

Result: A strong link is established between high pre-join uniqueness ratios and higher privacy leakage after database joins.

Conclusion: Uniqueness ratios provide a practical and explainable measure for assessing privacy risks before integrating databases, paving the way for better pre-join risk assessments.

Abstract: Identity leakage can emerge when independent databases are joined, even when each dataset is anonymized individually. While previous work focuses on post-join detection or complex privacy models, little attention has been given to simple, interpretable pre-join indicators that can warn data engineers and database administrators before integration occurs. This study investigates the uniqueness ratio of candidate join attributes as an early predictor of re-identification risk. Using synthetic multi-table datasets, we compute the uniqueness ratio of attribute combinations within each database and examine how these ratios correlate with identity exposure after the join. Experimental results show a strong relationship between high pre-join uniqueness and increased post-join leakage, measured by the proportion of records that become uniquely identifiable or fall into very small groups. Our findings demonstrate that uniqueness ratio offers an explainable and practical signal for assessing join induced privacy risk, providing a foundation for developing more comprehensive pre-join risk estimation models.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [1009] [Scientific production in the era of Large Language Models](https://arxiv.org/abs/2601.13187)
*Keigo Kusumegi,Xinyu Yang,Paul Ginsparg,Mathijs de Vaan,Toby Stuart,Yian Yin*

Main category: cs.DL

TL;DR: This study examines the impact of Large Language Models (LLMs) on scientific research, showing increased productivity, changed trends in writing complexity vs. quality, and broader citation patterns.


<details>
  <summary>Details</summary>
Motivation: To analyze how the adoption of LLMs is changing scientific research dynamics in terms of productivity, writing practices, and citation behavior.

Method: The study utilized extensive datasets including 2.1M preprints, 28K peer review reports, and 246M accesses to scientific documents to observe behavioral and production patterns.

Result: LLM adoption led to a significant rise in paper production (23.7-89.3%), altered correlations between writing complexity and quality, and encouraged more diverse citation practices.

Conclusion: The use of LLMs is transforming scientific production and evaluation, necessitating reevaluation by journals, funding bodies, and academic institutions.

Abstract: Large Language Models (LLMs) are rapidly reshaping scientific research. We analyze these changes in multiple, large-scale datasets with 2.1M preprints, 28K peer review reports, and 246M online accesses to scientific documents. We find: 1) scientists adopting LLMs to draft manuscripts demonstrate a large increase in paper production, ranging from 23.7-89.3% depending on scientific field and author background, 2) LLM use has reversed the relationship between writing complexity and paper quality, leading to an influx of manuscripts that are linguistically complex but substantively underwhelming, and 3) LLM adopters access and cite more diverse prior work, including books and younger, less-cited documents. These findings highlight a stunning shift in scientific production that will likely require a change in how journals, funding agencies, and tenure committees evaluate scientific works.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [1010] [Persistent Sheaf Laplacian Analysis of Protein Stability and Solubility Changes upon Mutation](https://arxiv.org/abs/2601.12219)
*Yiming Ren,Junjie Wee,Xi Chen,Grace Qian,Guo-Wei Wei*

Main category: math.SP

TL;DR: SheafLapNet is a predictive framework that uses advanced topological deep learning to model mutation-induced protein changes with improved interpretability and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current computational models in capturing mutation-driven protein alterations by integrating interpretability and physicochemical interaction in predictive modeling.

Method: Developed SheafLapNet, combining Topological Deep Learning and Persistent Sheaf Laplacian with protein transformer features and physical descriptors to model molecular interactions.

Result: SheafLapNet achieved state-of-the-art performance on benchmarks for protein stability and solubility prediction, proving its efficacy and generalizability.

Conclusion: SheafLapNet enhances interpretability and prediction accuracy of protein mutations, signifying a major advancement in handling structural and functional alterations caused by genetic mutations.

Abstract: Genetic mutations frequently disrupt protein structure, stability, and solubility, acting as primary drivers for a wide spectrum of diseases. Despite the critical importance of these molecular alterations, existing computational models often lack interpretability, and fail to integrate essential physicochemical interaction. To overcome these limitations, we propose SheafLapNet, a unified predictive framework grounded in the mathematical theory of Topological Deep Learning (TDL) and Persistent Sheaf Laplacian (PSL). Unlike standard Topological Data Analysis (TDA) tools such as persistent homology, which are often insensitive to heterogeneous information, PSL explicitly encodes specific physical and chemical information such as partial charges directly into the topological analysis. SheafLapNet synergizes these sheaf-theoretic invariants with advanced protein transformer features and auxiliary physical descriptors to capture intrinsic molecular interactions in a multiscale and mechanistic manner. To validate our framework, we employ rigorous benchmarks for both regression and classification tasks. For stability prediction, we utilize the comprehensive S2648 and S350 datasets. For solubility prediction, we employ the PON-Sol2 dataset, which provides annotations for increased, decreased, or neutral solubility changes. By integrating these multi-perspective features, SheafLapNet achieves state-of-the-art performance across these diverse benchmarks, demonstrating that sheaf-theoretic modeling significantly enhances both interpretability and generalizability in predicting mutation-induced structural and functional changes.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1011] [Verifying First-Order Temporal Properties of Infinite-State Systems via Timers and Rankings](https://arxiv.org/abs/2601.13325)
*Raz Lotan,Neta Elad,Oded Padon,Sharon Shoham*

Main category: cs.LO

TL;DR: The paper introduces a framework for verifying first-order temporal properties using SMT solvers by reducing the problem into a termination verification task via novel prophecy timers.


<details>
  <summary>Details</summary>
Motivation: To address the complexity in verifying first-order temporal properties and provide an effective method without introducing additional fairness assumptions.

Method: The authors introduce a reduction that augments systems with prophecy timer variables to predict steps until a temporal formula holds, eliminating fairness assumptions in the verification process. The reduction is combined with implicit rankings and SMT solvers to ensure termination verification.

Result: The approach is validated through case studies, showing it provides intuitive and efficient proofs for a variety of temporal verification tasks.

Conclusion: The framework simplifies the verification of first-order temporal properties by combining novel reductions with powerful ranking systems, offering a robust and intuitive method.

Abstract: We present a unified deductive verification framework for first-order temporal properties based on well-founded rankings, where verification conditions are discharged using SMT solvers. To that end, we introduce a novel reduction from verification of arbitrary temporal properties to verification of termination. Our reduction augments the system with prophecy timer variables that predict the number of steps along a trace until the next time certain temporal formulas, including the negated property, hold. In contrast to standard tableaux-based reductions, which reduce the problem to fair termination, our reduction does not introduce fairness assumptions. To verify termination of the augmented system, we follow the traditional approach of assigning each state a rank from a well-founded set and showing that the rank decreases in every transition. We leverage the recently proposed formalism of implicit rankings to express and automatically verify the decrease of rank using SMT solvers, even when the rank is not expressible in first-order logic. We extend implicit rankings from finite to infinite domains, enabling verification of more general systems and making them applicable to the augmented systems generated by our reduction, which allows us to exploit the decrease of timers in termination proofs. We evaluate our technique on a range of temporal verification tasks from previous works, giving simple, intuitive proofs for them within our framework.

</details>


### [1012] [Robust Verification of Concurrent Stochastic Games](https://arxiv.org/abs/2601.12003)
*Angel Y. He,David Parker*

Main category: cs.LO

TL;DR: The paper introduces robust concurrent stochastic games (CSGs) and interval CSGs (ICSGs) to handle epistemic uncertainty in transition probabilities. It develops theoretical foundations and algorithms for robust verification, tested using PRISM-games on benchmark problems.


<details>
  <summary>Details</summary>
Motivation: Real-world autonomous systems often grapple with epistemic uncertainty, particularly regarding transition probabilities, making traditional CSGs impractical.

Method: The authors introduce robust CSGs and ICSGs for modeling uncertainty, propose a framework for their robust verification under the worst-case assumptions, and implement the approach in PRISM-games.

Result: Efficient algorithms were developed and implemented for robust verification regarding finite/infinite horizons in CSGs and ICSGs, verified through benchmarks.

Conclusion: Robust verification of systems with uncertain transitions using ICSGs is feasible, with promising results demonstrated through benchmarks and practical implementation.

Abstract: Autonomous systems often operate in multi-agent settings and need to make concurrent, strategic decisions, typically in uncertain environments. Verification and control problems for these systems can be tackled with concurrent stochastic games (CSGs), but this model requires transition probabilities to be precisely specified - an unrealistic requirement in many real-world settings. We introduce *robust CSGs* and their subclass *interval CSGs* (ICSGs), which capture epistemic uncertainty about transition probabilities in CSGs. We propose a novel framework for *robust* verification of these models under worst-case assumptions about transition uncertainty. Specifically, we develop the underlying theoretical foundations and efficient algorithms, for finite- and infinite-horizon objectives in both zero-sum and nonzero-sum settings, the latter based on (social-welfare optimal) Nash equilibria. We build an implementation in the PRISM-games model checker and demonstrate the feasibility of robust verification of ICSGs across a selection of large benchmarks.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1013] [Adversarial Drift-Aware Predictive Transfer: Toward Durable Clinical AI](https://arxiv.org/abs/2601.11860)
*Xin Xiong,Zijian Guo,Haobo Zhu,Chuan Hong,Jordan W Smoller,Tianxi Cai,Molei Liu*

Main category: stat.AP

TL;DR: ADAPT is a framework that counters AI performance decay due to temporal data drift using minimal retraining.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the problem of AI systems suffering performance degradation over time due to temporal data shifts, which is a critical issue in high-stakes environments like healthcare.

Method: The method introduces ADAPT, a framework that constructs an uncertainty set of plausible future models based on historical data and limited current data, optimizing for worst-case performance while prioritizing robustness and privacy.

Result: ADAPT demonstrated superior stability in handling challenges like coding transitions and systemic shocks (e.g., COVID-19 pandemic) through experiments on longitudinal suicide risk prediction datasets.

Conclusion: By reducing performance degradation without excessive retraining or data labeling, ADAPT ensures reliable and scalable AI application in healthcare.

Abstract: Clinical AI systems frequently suffer performance decay post-deployment due to temporal data shifts, such as evolving populations, diagnostic coding updates (e.g., ICD-9 to ICD-10), and systemic shocks like the COVID-19 pandemic. Addressing this ``aging'' effect via frequent retraining is often impractical due to computational costs and privacy constraints. To overcome these hurdles, we introduce Adversarial Drift-Aware Predictive Transfer (ADAPT), a novel framework designed to confer durability against temporal drift with minimal retraining. ADAPT innovatively constructs an uncertainty set of plausible future models by combining historical source models and limited current data. By optimizing worst-case performance over this set, it balances current accuracy with robustness against degradation due to future drifts. Crucially, ADAPT requires only summary-level model estimators from historical periods, preserving data privacy and ensuring operational simplicity. Validated on longitudinal suicide risk prediction using electronic health records from Mass General Brigham (2005--2021) and Duke University Health Systems, ADAPT demonstrated superior stability across coding transitions and pandemic-induced shifts. By minimizing annual performance decay without labeling or retraining future data, ADAPT offers a scalable pathway for sustaining reliable AI in high-stakes healthcare environments.

</details>


### [1014] [Improving Geopolitical Forecasts with Bayesian Networks](https://arxiv.org/abs/2601.13362)
*Matthew Martin*

Main category: stat.AP

TL;DR: This paper compares Bayesian networks (BNs) and logistic regression to determine methods for better forecast accuracy, using data from the Good Judgment Project.


<details>
  <summary>Details</summary>
Motivation: The aim is to test if Bayesian networks can surpass logistic regression and recalibration methods in forecast accuracy.

Method: Regularized logistic regression and recalibrated aggregate methods were compared with two types of Bayesian networks (structure-learned and naive), utilizing four predictor variables.

Result: The recalibrated aggregate achieved the highest accuracy (AUC = 0.985), outperforming both BNs and logistic regression.

Conclusion: While recalibrated aggregates showed superior accuracy, future exploration of hybrid models combining BNs and logistic regression is suggested.

Abstract: This study explores how Bayesian networks (BNs) can improve forecast accuracy compared to logistic regression and recalibration and aggregation methods, using data from the Good Judgment Project. Regularized logistic regression models and a baseline recalibrated aggregate were compared to two types of BNs: structure-learned BNs with arcs between predictors, and naive BNs. Four predictor variables were examined: absolute difference from the aggregate, forecast value, days prior to question close, and mean standardized Brier score. Results indicated the recalibrated aggregate achieved the highest accuracy (AUC = 0.985), followed by both types of BNs, then the logistic regression models. Performance of the BNs was likely harmed by reduced information from the discretization process and violation of the assumption of linearity likely harmed the logistic regression models. Future research should explore hybrid approaches combining BNs with logistic regression, examine additional predictor variables, and account for hierarchical data dependencies.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [1015] [Multi-Scale Negative Coupled Information Systems (MNCIS): A Unified Spectral Topology Framework for Stability in Turbulence, AI, and Biology](https://arxiv.org/abs/2601.11594)
*Pengyue Hou*

Main category: physics.comp-ph

TL;DR: The paper introduces a framework, Adaptive Spectral Negative Coupling (ASNC), to prevent the structural instability endemic in complex systems, validated across hydrodynamics, AI, and biological physics.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the recurring structural instability in complex dynamical systems, particularly the collapse of the spectral gap, which leads to low-dimensional attractors and information loss.

Method: The authors extend the Multi-Scale Negative Coupled Information System (MNCIS) framework, introducing ASNC as an adaptive topological operator functioning as a high-pass filter. They evaluate its impact on three systems: turbulence modeling, oversmoothing in GNNs, and reaction-diffusion morphogenesis.

Result: The ASNC framework demonstrated effective stabilization by preserving desired behaviors in three domains: stabilizing turbulence without hyper-viscosity, enabling ultra-deep GNNs without residual supports, and stabilizing pattern formation against washout.

Conclusion: ASNC offers a versatile base-independent topological solution, maintaining stability and information persistence across diverse complex systems, thus contributing to the understanding of stability in physical and informational domains.

Abstract: Complex dynamical systems frequently encounter a recurrent structural instability: the collapse of the spectral gap, driving the system toward a low-dimensional "Zero-Mode Attractor" (e.g., spectral pile-up or over-smoothing). Building upon recent global well-posedness estimates [Hou, arXiv:2601.00638], this work generalizes the Multi-Scale Negative Coupled Information System (MNCIS) framework. We postulate that global stability requires an active topological operator -- Adaptive Spectral Negative Coupling (ASNC) -- functioning as a state-dependent high-pass filter that penalizes entropy accumulation at spectral boundaries. We validate this unified framework via three implementations:(1) Hydrodynamics: In 3D Navier-Stokes turbulence ($N=256^3$), ASNC acts as a global-enstrophy adaptive sub-grid scale (SGS) model, stabilizing the inviscid limit and preserving the Kolmogorov $-5/3$ inertial range without artificial hyper-viscosity.(2) Artificial Intelligence: Addressing Over-smoothing in Graph Neural Networks (GNNs), we implement ASNC as a parameter-free topological constraint. Unlike baselines (e.g., DeepGCNs) relying on dense residual connections to bypass signal decay, our framework enables the training of ultra-deep 64-layer networks without residual connections, maintaining perfectly stationary feature variance ($σ^2 \equiv 1.0$) on the ogbn-arxiv benchmark. (3) Biological Physics: In reaction-diffusion morphogenesis, it stabilizes Turing patterns against diffusive washout in high-entropy regimes. Our results suggest that the MNCIS framework provides a base-independent topological condition for distinguishing viable complex systems from those collapsing into thermal equilibrium, bridging physical stability and information persistence.

</details>


### [1016] [Refined Gradient-Based Temperature Optimization for the Replica-Exchange Monte-Carlo Method](https://arxiv.org/abs/2601.13542)
*Tatsuya Miyata,Shunta Arai,Satoshi Takabe*

Main category: physics.comp-ph

TL;DR: This paper proposes a refined replica-exchange Monte-Carlo method (RXMC) that optimizes temperature selection efficiently and enforces physical constraints, improving sampling efficiency for multi-modal distributions.


<details>
  <summary>Details</summary>
Motivation: Sampling multi-modal distributions remains challenging for conventional methods, and RXMC's efficiency relies heavily on optimal temperature selection, which is difficult to achieve using existing methods.

Method: The study refines the temperature update approach via reparameterization to enforce physical constraints and utilizes gradient-based optimization with variance of acceptance rates as a loss function to optimize temperatures through gradient descent.

Result: Through experiments on benchmark spin systems, the method demonstrated uniform acceptance rates, reduced round-trip times, and outperformed recent policy gradient methods by avoiding constraint violations and hyperparameter tuning challenges.

Conclusion: The proposed RXMC refinement enhances sampling efficiency by systematically optimizing temperature selection, maintaining constraints, and outperforming other modern approaches in stability and usability.

Abstract: The replica-exchange Monte-Carlo (RXMC) method is a powerful Markov-chain Monte-Carlo algorithm for sampling from multi-modal distributions, which are challenging for conventional methods. The sampling efficiency of the RXMC method depends highly on the selection of the temperatures, and finding optimal temperatures remains a challenge. In this study, we propose a refined online temperature selection method by extending the gradient-based optimization framework proposed previously. Building upon the existing temperature update approach, we introduce a reparameterization technique to strictly enforce physical constraints, such as the monotonic ordering of inverse temperatures, which were not explicitly addressed in the original formulation. The proposed method defines the variance of acceptance rates between adjacent replicas as a loss function, estimates its gradient using differential information from the sampling process, and optimizes the temperatures via gradient descent. We demonstrate the effectiveness of our method through experiments on benchmark spin systems, including the two-dimensional ferromagnetic Ising model, the two-dimensional ferromagnetic XY model, and the three-dimensional Edwards-Anderson model. Our results show that the method successfully achieves uniform acceptance rates and reduces round-trip times across the temperature space. Furthermore, our proposed method offers a significant advantage over recently proposed policy gradient method that require careful hyperparameter tuning, while simultaneously preventing the constraint violations that destabilize optimization.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [1017] [Large Language Model Agent for User-friendly Chemical Process Simulations](https://arxiv.org/abs/2601.11650)
*Jingkang Liang,Niklas Groll,Gürkan Sin*

Main category: physics.chem-ph

TL;DR: This paper proposes a framework integrating a Large Language Model (LLM) agent with AVEVA Process Simulation (APS) for natural language-guided process simulations, demonstrated through two case studies in water-methanol separation.


<details>
  <summary>Details</summary>
Motivation: To simplify and democratize process simulation tasks, making them accessible to inexperienced users while improving efficiency for experienced practitioners.

Method: The framework integrates an LLM agent with APS via Model Context Protocol (MCP), enabling Python-based communication for executing complex simulations from natural language instructions.

Result: The proposed agent successfully performed autonomous flowsheet analysis, iterative optimization, and guided flowsheet synthesis, showcasing utility in both education and practice but requiring expert oversight.

Conclusion: LLM-based agents hold significant potential as collaborators in process simulation tasks, offering benefits like workflow demonstration, task automation, and rapid synthesis, though challenges such as simplification and errors remain.

Abstract: Modern process simulators enable detailed process design, simulation, and optimization; however, constructing and interpreting simulations is time-consuming and requires expert knowledge. This limits early exploration by inexperienced users. To address this, a large language model (LLM) agent is integrated with AVEVA Process Simulation (APS) via Model Context Protocol (MCP), allowing natural language interaction with rigorous process simulations. An MCP server toolset enables the LLM to communicate programmatically with APS using Python, allowing it to execute complex simulation tasks from plain-language instructions. Two water-methanol separation case studies assess the framework across different task complexities and interaction modes. The first shows the agent autonomously analyzing flowsheets, finding improvement opportunities, and iteratively optimizing, extracting data, and presenting results clearly. The framework benefits both educational purposes, by translating technical concepts and demonstrating workflows, and experienced practitioners by automating data extraction, speeding routine tasks, and supporting brainstorming. The second case study assesses autonomous flowsheet synthesis through both a step-by-step dialogue and a single prompt, demonstrating its potential for novices and experts alike. The step-by-step mode gives reliable, guided construction suitable for educational contexts; the single-prompt mode constructs fast baseline flowsheets for later refinement. While current limitations such as oversimplification, calculation errors, and technical hiccups mean expert oversight is still needed, the framework's capabilities in analysis, optimization, and guided construction suggest LLM-based agents can become valuable collaborators.

</details>


### [1018] [onepot CORE -- an enumerated chemical space to streamline drug discovery, enabled by automated small molecule synthesis and AI](https://arxiv.org/abs/2601.12603)
*Andrei S. Tyrin,Brandon Wang,Manuel Muñoz,Samuel H. Foxman,Daniil A. Boiko*

Main category: physics.chem-ph

TL;DR: The paper introduces onepot CORE, a chemical space of 3.4 billion molecules, synthesized using an automated platform and AI chemist, Phil, to accelerate small-molecule creation.


<details>
  <summary>Details</summary>
Motivation: To address the bottleneck of the "make" step in drug discovery, which is slow, costly, and difficult to scale or automate.

Method: This method uses an AI-driven automated synthesis platform combining chemical reaction sets, curated building blocks, enumerated products, and ML feasibility assessment.

Result: The onepot CORE space supports seven reactions, validated with metrics like synthesis success rate, timeline efficiency, compound purity, identity, and DPP4 inhibitor suitability.

Conclusion: The workflow enables faster, reliable access to diverse molecules, accelerating pharmaceutical and chemical discovery processes.

Abstract: The design-make-test-analyze cycle in early-stage drug discovery remains constrained primarily by the "make" step: small-molecule synthesis is slow, costly, and difficult to scale or automate across diverse chemotypes. Enumerated chemical spaces aim to reduce this bottleneck by predefining synthesizable regions of chemical space from available building blocks and reliable reactions, yet existing commercial spaces are still limited by long turnaround times, narrow reaction scope, and substantial manual decision-making in route selection and execution.
  Here we present the first version of onepot CORE, an enumerated chemical space containing 3.4B molecules and corresponding on-demand synthesis product enabled by an automated synthesis platform and an AI chemist, Phil, that designs, executes, and analyzes experiments. onepot CORE is constructed by (i) selecting a reaction set commonly used in medicinal chemistry, (ii) sourcing and curating building blocks from supplier catalogs, (iii) enumerating candidate products, and (iv) applying ML-based feasibility assessment to prioritize compounds for robust execution. In the current release, the space is supported by seven reactions.
  We describe an end-to-end workflow - from route selection and automated liquid handling through workup and purification. We further report validation across operational metrics (success rate, timelines, purity, and identity), including NMR confirmation for a representative set of synthesized compounds and assay suitability demonstrated using a series of DPP4 inhibitors. Collectively, onepot CORE illustrates a path toward faster, more reliable access to diverse small molecules, supporting accelerated discovery in pharmaceuticals and beyond.

</details>


### [1019] [Reorienting off-path Nudged Elastic Bands (RONEB) via Minimum Mode Following](https://arxiv.org/abs/2601.12630)
*Rohit Goswami,Miha Gunde,Hannes Jónsson*

Main category: physics.chem-ph

TL;DR: The paper introduces RONEB, an algorithm combining double-ended NEB and single-ended techniques, reducing computational effort in transition state determination.


<details>
  <summary>Details</summary>
Motivation: To address computational inefficiency and stagnation issues in double-ended algorithms like NEB while maintaining its relevance for specific state constraints in reaction kinetics.

Method: RONEB integrates NEB with accelerating features of Min-Mode Following methods, dynamically decoupling climbing images using relative force triggers and penalty mechanisms.

Result: RONEB achieved significant performance improvements, reducing gradient calls by 46.3% and computational effort in surface diffusion tests by 28%, outperforming CI-NEB.

Conclusion: RONEB is a robust hybrid method for transition state predictions, effective for high-throughput chemical discovery.

Abstract: Accurate determination of transition states remains central to understanding reaction kinetics. Double-ended methods like the Nudged Elastic Band (NEB) ensure relevant transition states and paths, but incur high computational costs and suffer stagnation on flat or rough potential energy surfaces. Conversely, single-ended eigenmode-following techniques offer efficiency but cannot often be constrained between specific states. Here, we present the Reorienting Off-path Nudged Elastic Bands (RONEB), an adaptive hybrid algorithm that integrates the double ended nature of the NEB with the acceleration of single ended Min-Mode Following methods. RONEB provides stability based on the history of the path optimization, relative force triggering, and an alignment-based back-off penalty to dynamically decouple the climbing image from the elastic band constraints. We benchmark the method against the standard Climbing Image NEB (CI-NEB) across the Baker-Chan transition state test set using the PET-MAD machine-learned potential and the OptBench Pt(111) heptamer island surface diffusion set. A Bayesian analysis of the performance data quantifies a median reduction in gradient calls of 46.3% [95% CrI: -54.7%, -36.9%] relative to the baseline, while surface diffusion tests reveal a 28% reduction across 59 metallic rearrangement mechanisms. These results establish RONEB as a highly effective tool for high-throughput automated chemical discovery.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1020] [Rethinking the Value of Multi-Agent Workflow: A Strong Single Agent Baseline](https://arxiv.org/abs/2601.12307)
*Jiawei Xu,Arief Koesdwiady,Sisong Bei,Yan Han,Baixiang Huang,Dakuo Wang,Yutong Chen,Zheshen Wang,Peihao Wang,Pan Li,Ying Ding*

Main category: cs.MA

TL;DR: The paper investigates if workflows of homogeneous multi-agent systems can be simulated by a single LLM agent without losing accuracy; results show OneFlow algorithm achieves this with better efficiency.


<details>
  <summary>Details</summary>
Motivation: To explore whether single LLM agents through multi-turn conversations can simulate the effectiveness and efficiency of homogeneous workflows in multi-agent systems.

Method: The paper tests seven benchmarks covering diverse tasks including coding, mathematics, reasoning, planning, and real-world tool use to compare homogeneous MAS workflows with single-agent implementations.

Result: It was found that a single LLM agent can achieve efficiency via KV cache reuse and match the performance of optimized heterogeneous workflows. OneFlow reduces inference costs without compromising accuracy.

Conclusion: Single-LLM systems provide a viable and efficient alternative to homogeneous multi-agent frameworks, but truly heterogeneous workflows present opportunities for further research due to limitations in KV cache sharing across different LLMs.

Abstract: Recent advances in LLM-based multi-agent systems (MAS) show that workflows composed of multiple LLM agents with distinct roles, tools, and communication patterns can outperform single-LLM baselines on complex tasks. However, most frameworks are homogeneous, where all agents share the same base LLM and differ only in prompts, tools, and positions in the workflow. This raises the question of whether such workflows can be simulated by a single agent through multi-turn conversations. We investigate this across seven benchmarks spanning coding, mathematics, general question answering, domain-specific reasoning, and real-world planning and tool use. Our results show that a single agent can reach the performance of homogeneous workflows with an efficiency advantage from KV cache reuse, and can even match the performance of an automatically optimized heterogeneous workflow. Building on this finding, we propose \textbf{OneFlow}, an algorithm that automatically tailors workflows for single-agent execution, reducing inference costs compared to existing automatic multi-agent design frameworks without trading off accuracy. These results position the single-LLM implementation of multi-agent workflows as a strong baseline for MAS research. We also note that single-LLM methods cannot capture heterogeneous workflows due to the lack of KV cache sharing across different LLMs, highlighting future opportunities in developing \textit{truly} heterogeneous multi-agent systems.

</details>


### [1021] [Communication Methods in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.12886)
*Christoph Wittner*

Main category: cs.MA

TL;DR: This paper provides an overview and comparison of communication methods in multi-agent reinforcement learning (MARL), highlighting the varying strengths, weaknesses, and use cases.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of partially observable environments, non-stationarity, and growing action spaces in MARL, and to examine the role of communication in fostering efficient cooperation among agents.

Method: The authors conducted an in-depth analysis of 29 publications, evaluating explicit, implicit, attention-based, graph-based, and hierarchical communication methods.

Result: The study found no universally optimal communication framework, stressing context-dependent choices and the need for low computational-cost methods in scalable systems.

Conclusion: There is a need for standardized benchmarks and improved robustness in realistic conditions to advance the practical applicability of MARL communication methods.

Abstract: Multi-agent reinforcement learning is a promising research area that extends established reinforcement learning approaches to problems formulated as multi-agent systems. Recently, a multitude of communication methods have been introduced to this field to address problems such as partially observable environments, non-stationarity, and exponentially growing action spaces. Communication further enables efficient cooperation among all agents interacting in an environment. This work aims at providing an overview of communication techniques in multi-agent reinforcement learning. By an in-depth analysis of 29 publications on this topic, the strengths and weaknesses of explicit, implicit, attention-based, graph-based, and hierarchical/role-based communication are evaluated. The results of this comparison show that there is no general, optimal communication framework for every problem. On the contrary, the choice of communication depends heavily on the problem at hand. The comparison also highlights the importance of communication methods with low computational overhead to enable scalability to environments where many agents interact. Finally, the paper discusses current research gaps, emphasizing the need for standardized benchmarking of system-level metrics and improved robustness under realistic communication conditions to enhance the real-world applicability of these approaches.

</details>


### [1022] [OFA-MAS: One-for-All Multi-Agent System Topology Design based on Mixture-of-Experts Graph Generative Models](https://arxiv.org/abs/2601.12996)
*Shiyuan Li,Yixin Liu,Yu Zheng,Mei Li,Quoc Viet Hung Nguyen,Shirui Pan*

Main category: cs.MA

TL;DR: Proposed a universal framework, OFA-TAD, for adaptive multi-agent system topologies using a natural language task description.


<details>
  <summary>Details</summary>
Motivation: Existing methodologies are task-specific, lack generalization to unseen domains, and fail to utilize shared structural knowledge.

Method: Introduced OFA-TAD, incorporating Task-Aware Graph State Encoder (TAGSE), Mixture-of-Experts (MoE), and a three-stage training strategy emphasizing topological adaptation.

Result: Showcased superior performance against specialized models across six benchmarks with highly adaptive topologies.

Conclusion: OFA-TAD demonstrates feasibility as a general-purpose solution for adaptive collaboration graphs in multi-agent systems.

Abstract: Multi-Agent Systems (MAS) offer a powerful paradigm for solving complex problems, yet their performance is critically dependent on the design of their underlying collaboration topology. As MAS become increasingly deployed in web services (e.g., search engines), designing adaptive topologies for diverse cross-domain user queries becomes essential. Current graph learning-based design methodologies often adhere to a "one-for-one" paradigm, where a specialized model is trained for each specific task domain. This approach suffers from poor generalization to unseen domains and fails to leverage shared structural knowledge across different tasks. To address this, we propose OFA-TAD, a one-for-all framework that generates adaptive collaboration graphs for any task described in natural language through a single universal model. Our approach integrates a Task-Aware Graph State Encoder (TAGSE) that filters task-relevant node information via sparse gating, and a Mixture-of-Experts (MoE) architecture that dynamically selects specialized sub-networks to drive node and edge prediction. We employ a three-stage training strategy: unconditional pre-training on canonical topologies for structural priors, large-scale conditional pre-training on LLM-generated datasets for task-topology mappings, and supervised fine-tuning on empirically validated graphs. Experiments across six diverse benchmarks show that OFA-TAD significantly outperforms specialized one-for-one models, generating highly adaptive MAS topologies. Code: https://github.com/Shiy-Li/OFA-MAS.

</details>


### [1023] [The Orchestration of Multi-Agent Systems: Architectures, Protocols, and Enterprise Adoption](https://arxiv.org/abs/2601.13671)
*Apoorva Adimulam,Rajesh Gupta,Sumit Kumar*

Main category: cs.MA

TL;DR: The paper consolidates the architecture and protocols of orchestrated multi-agent systems, emphasizing coordination, communication, governance, and scalability principles.


<details>
  <summary>Details</summary>
Motivation: To advance AI by formalizing structured coordination and interoperable communication among autonomous agents for achieving complex objectives.

Method: The paper proposes a unified architectural framework integrating planning, policy enforcement, state management, and quality operations, alongside developing two communication protocols (Model Context and Agent2Agent).

Result: The proposed framework and protocols enable scalable, auditable, and policy-compliant reasoning, enhancing the coherence and effectiveness of distributed agent systems.

Conclusion: The paper bridges conceptual architectures with practical design, offering implementation-ready guidelines for multi-agent AI ecosystems.

Abstract: Orchestrated multi-agent systems represent the next stage in the evolution of artificial intelligence, where autonomous agents collaborate through structured coordination and communication to achieve complex, shared objectives. This paper consolidates and formalizes the technical composition of such systems, presenting a unified architectural framework that integrates planning, policy enforcement, state management, and quality operations into a coherent orchestration layer. Another primary contribution of this work is the in-depth technical delineation of two complementary communication protocols - the Model Context Protocol, which standardizes how agents access external tools and contextual data, and the Agent2Agent protocol, which governs peer coordination, negotiation, and delegation. Together, these protocols establish an interoperable communication substrate that enables scalable, auditable, and policy-compliant reasoning across distributed agent collectives. Beyond protocol design, the paper details how orchestration logic, governance frameworks, and observability mechanisms collectively sustain system coherence, transparency, and accountability. By synthesizing these elements into a cohesive technical blueprint, this paper provides comprehensive treatments of orchestrated multi-agent systems - bridging conceptual architectures with implementation-ready design principles for enterprise-scale AI ecosystems.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [1024] [AI Skills Improve Job Prospects: Causal Evidence from a Hiring Experiment](https://arxiv.org/abs/2601.13286)
*Fabian Stephany,Ole Teutloff,Angelo Leone*

Main category: econ.GN

TL;DR: This paper studies the value of AI skills in hiring decisions, finding that they increase interview chances by 8-15% and can offset biases such as age or education.


<details>
  <summary>Details</summary>
Motivation: To explore how AI skills influence hiring decisions and whether they can counteract disadvantages like age or lower education.

Method: Conducted an experimental survey with 1,700 recruiters in the UK and the US using paired conjoint designs to evaluate hypothetical resumes.

Result: AI skills increased interview invitation probabilities by 8-15% and partially or fully offset disadvantages, especially in certain roles. Effects varied by occupation and recruiters' background.

Conclusion: AI skills are effective hiring signals, reducing traditional biases, and have implications for workers' skill development and hiring practices.

Abstract: The growing adoption of artificial intelligence (AI) technologies has heightened interest in the labour market value of AI-related skills, yet causal evidence on their role in hiring decisions remains scarce. This study examines whether AI skills serve as a positive hiring signal and whether they can offset conventional disadvantages such as older age or lower formal education. We conduct an experimental survey with 1,700 recruiters from the United Kingdom and the United States. Using a paired conjoint design, recruiters evaluated hypothetical candidates represented by synthetically designed resumes. Across three occupations - graphic designer, office assistant, and software engineer - AI skills significantly increase interview invitation probabilities by approximately 8 to 15 percentage points. AI skills also partially or fully offset disadvantages related to age and lower education, with effects strongest for office assistants, where formal AI certification plays an additional compensatory role. Effects are weaker for graphic designers, consistent with more skeptical recruiter attitudes toward AI in creative work. Finally, recruiters' own background and AI usage significantly moderate these effects. Overall, the findings demonstrate that AI skills function as a powerful hiring signal and can mitigate traditional labour market disadvantages, with implications for workers' skill acquisition strategies and firms' recruitment practices.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [1025] [AllShowers: One model for all calorimeter showers](https://arxiv.org/abs/2601.11716)
*Thorsten Buss,Henry Day-Hall,Frank Gaede,Gregor Kasieczka,Katja Krüger*

Main category: physics.ins-det

TL;DR: AllShowers is a unified generative model designed for calorimeter shower simulation across multiple particle types using a Transformer-based normalizing flow. This efficient model surpasses previous technologies by enhancing fidelity and scalability.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the high computational costs in calorimeter shower simulation for collider experiments, aiming to improve efficiency and unify the modeling process across multiple particle types.

Method: The proposed AllShowers utilizes continuous normalizing flow with a Transformer architecture, integrating innovations like layer embedding, attention masking schemes, and optimal transport mapping to improve sample generation.

Result: AllShowers demonstrates the capability to realistically simulate showers for electrons, photons, and hadrons of various energies and angles, surpassing previous models in fidelity, notably for hadronic showers.

Conclusion: AllShowers represents a pivotal advance, offering a scalable, high-fidelity universal model for calorimeter shower simulation in collider experiments, with potential extensive application.

Abstract: Accurate and efficient detector simulation is essential for modern collider experiments. To reduce the high computational cost, various fast machine learning surrogate models have been proposed. Traditional surrogate models for calorimeter shower modeling train separate networks for each particle species, limiting scalability and reuse. We introduce AllShowers, a unified generative model that simulates calorimeter showers across multiple particle types using a single generative model. AllShowers is a continuous normalizing flow model with a Transformer architecture, enabling it to generate complex spatial and energy correlations in variable-length point cloud representations of showers. Trained on a diverse dataset of simulated showers in the highly granular ILD detector, the model demonstrates the ability to generate realistic showers for electrons, photons, and charged and neutral hadrons across a wide range of incident energies and angles without retraining. In addition to unifying shower generation for multiple particle types, AllShowers surpasses the fidelity of previous single-particle-type models for hadronic showers. Key innovations include the use of a layer embedding, allowing the model to learn all relevant calorimeter layer properties; a custom attention masking scheme to reduce computational demands and introduce a helpful inductive bias; and a shower- and layer-wise optimal transport mapping to improve training convergence and sample quality. AllShowers marks a significant step towards a universal model for calorimeter shower simulations in collider experiments.

</details>


### [1026] [Accurate Simulation Pipeline for Passive Single-Photon Imaging](https://arxiv.org/abs/2601.12850)
*Aleksi Suonsivu,Lauri Salmela,Leevi Uosukainen,Edoardo Peretti,Radu Ciprian Bilcu,Giacomo Boracchi*

Main category: physics.ins-det

TL;DR: This paper presents a simulation pipeline for Single-Photon Avalanche Diodes (SPADs) and introduces the SPAD-MNIST dataset to evaluate neural network classifiers under low-light conditions.


<details>
  <summary>Details</summary>
Motivation: SPADs offer exceptional low-light imaging capabilities, but their high cost and limited availability hinder the development of SPAD-specific algorithms. Accurate simulation is necessary to overcome the scarcity of SPAD datasets.

Method: A comprehensive SPAD simulation pipeline is developed and validated through experiments with two commercial SPAD sensors. The SPAD-MNIST dataset, simulating low-light conditions, supports evaluating CNN classifiers.

Result: The proposed simulator generates synthetic data compatible with different SPAD imaging modalities. It enables trained classifiers to perform well on actual SPAD sensor data under various lighting conditions.

Conclusion: The paper provides essential tools for advancing SPAD imaging research, addressing data scarcity, and enabling learning-based solutions for low-light scenarios.

Abstract: Single-Photon Avalanche Diodes (SPADs) are new and promising imaging sensors. These sensors are sensitive enough to detect individual photons hitting each pixel, with extreme temporal resolution and without readout noise. Thus, SPADs stand out as an optimal choice for low-light imaging. Due to the high price and limited availability of SPAD sensors, the demand for an accurate data simulation pipeline is substantial. Indeed, the scarcity of SPAD datasets hinders the development of SPAD-specific processing algorithms and impedes the training of learning-based solutions.
  In this paper, we present a comprehensive SPAD simulation pipeline and validate it with multiple experiments using two recent commercial SPAD sensors. Our simulator is used to generate the SPAD-MNIST, a single-photon version of the seminal MNIST dataset, to investigate the effectiveness of convolutional neural network (CNN) classifiers on reconstructed fluxes, even at extremely low light conditions, e.g., 5 mlux. We also assess the performance of classifiers exclusively trained on simulated data on real images acquired from SPAD sensors at different light conditions. The synthetic dataset encompasses different SPAD imaging modalities and is made available for download. Project page: https://boracchi.faculty.polimi.it/Projects/SPAD-MNIST.html.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [1027] [The Energy-Throughput Trade-off in Lossless-Compressed Source Code Storage](https://arxiv.org/abs/2601.13220)
*Paolo Ferragina,Francesco Tosoni*

Main category: cs.DS

TL;DR: The paper focuses on designing a compressed key-value store for large-scale source code datasets, exploring trade-offs in space, time, and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve the indexing of vast source code archives for applications such as AI training and information retrieval, while balancing computational resource trade-offs.

Method: The study implemented and evaluated compression configurations using extensive experiments on high-performance computing infrastructure, analyzing parameters like data parallelism and energy efficiency.

Result: Results show high compression ratios boost retrieval speed and energy efficiency, but scaling energy efficiency remains complex due to hardware limitations.

Conclusion: The paper offers guidelines for creating sustainable and efficient storage backends, emphasizing energy-aware configurations and standardized green benchmarks in CI/CD pipelines.

Abstract: Retrieving data from large-scale source code archives is vital for AI training, neural-based software analysis, and information retrieval, to cite a few. This paper studies and experiments with the design of a compressed key-value store for the indexing of large-scale source code datasets, evaluating its trade-off among three primary computational resources: (compressed) space occupancy, time, and energy efficiency. Extensive experiments on a national high-performance computing infrastructure demonstrate that different compression configurations yield distinct trade-offs, with high compression ratios and order-of-magnitude gains in retrieval throughput and energy efficiency. We also study data parallelism and show that, while it significantly improves speed, scaling energy efficiency is more difficult, reflecting the known non-energy-proportionality of modern hardware and challenging the assumption of a direct time-energy correlation. This work streamlines automation in energy-aware configuration tuning and standardized green benchmarking deployable in CI/CD pipelines, thus empowering system architects with a spectrum of Pareto-optimal energy-compression-throughput trade-offs and actionable guidelines for building sustainable, efficient storage backends for massive open-source code archival.

</details>


### [1028] [Efficient Parallel $(Δ+1)$-Edge-Coloring](https://arxiv.org/abs/2601.13822)
*Michael Elkin,Ariel Khuzman*

Main category: cs.DS

TL;DR: The paper studies the $Δ+1$-edge-coloring problem for graphs in the parallel PRAM model, proposing new faster algorithms with different processor-time trade-offs and improvements over prior results.


<details>
  <summary>Details</summary>
Motivation: To address and improve the computational efficiency of the $(Δ+1)$-edge-coloring problem, a foundational problem in graph theory, within the parallel PRAM model.

Method: The authors devised novel parallel algorithms that improve on prior work by achieving reduced runtime while maintaining processor efficiency. They also rectified analysis flaws from prior studies.

Result: The new algorithms achieve runtimes of $O(Δ^4 \\cdot \\log^4 n)$ with $O(m \\cdot Δ)$ processors and provide faster solutions for edge-coloring updates with additional trade-offs and capabilities for low-arboricity graphs.

Conclusion: The paper significantly advances the state-of-the-art for $(Δ+1)$-edge-coloring in parallel computation, offering faster and simpler solutions compared to prior methods and introducing versatile trade-off options.

Abstract: We study the $(Δ+1)$-edge-coloring problem in the parallel $\left(\mathrm{PRAM}\right)$ model of computation. The celebrated Vizing's theorem [Viz64] states that every simple graph $G = (V,E)$ can be properly $(Δ+1)$-edge-colored. In a seminal paper, Karloff and Shmoys [KS87] devised a parallel algorithm with time $O\left(Δ^5\cdot\log n\cdot\left(\log^3 n+Δ^2\right)\right)$ and $O(m\cdotΔ)$ processors. This result was improved by Liang et al. [LSH96] to time $O\left(Δ^{4.5}\cdot \log^3Δ\cdot \log n + Δ^4 \cdot\log^4 n\right)$ and $O\left(n\cdotΔ^{3} +n^2\right)$ processors. [LSH96] claimed $O\left(Δ^{3.5} \cdot\log^3Δ\cdot \log n + Δ^3\cdot \log^4 n\right)$ time, but we point out a flaw in their analysis, which once corrected, results in the above bound. We devise a faster parallel algorithm for this fundamental problem. Specifically, our algorithm uses $O\left(Δ^4\cdot \log^4 n\right)$ time and $O(m\cdot Δ)$ processors. Another variant of our algorithm requires $O\left(Δ^{4+o(1)}\cdot\log^2 n\right)$ time, and $O\left(m\cdotΔ\cdot\log n\cdot\log^δΔ\right)$ processors, for an arbitrarily small $δ>0$. We also devise a few other tradeoffs between the time and the number of processors, and devise an improved algorithm for graphs with small arboricity. On the way to these results, we also provide a very fast parallel algorithm for updating $(Δ+1)$-edge-coloring. Our algorithm for this problem is dramatically faster and simpler than the previous state-of-the-art algorithm (due to [LSH96]) for this problem.

</details>


### [1029] [Learning-Augmented Online TRP on a Line](https://arxiv.org/abs/2601.13494)
*Swapnil Guragain,Gokarna Sharma*

Main category: cs.DS

TL;DR: The paper explores the online traveling repairperson problem using a learning-augmented framework, providing improved competitive ratios with predictions over the original model.


<details>
  <summary>Details</summary>
Motivation: To improve solutions to the online traveling repairperson problem by leveraging machine learning predictions to reduce completion times and achieve better competitive ratios.

Method: The study introduces a prediction model with error-prone predicted positions for requests, developing deterministic algorithms for both perfect and imperfect predictions, including competitive analysis.

Result: Achieved a $(2+\sqrt{3})\approx 3.732$-competitive ratio for perfect predictions, and $\min\{3.732+4δ,4\}$-competitive ratio with imperfect predictions.

Conclusion: This paper establishes initial results for using the learning-augmented framework in this problem, improving existing bounds on competitive ratios.

Abstract: We study the online traveling repairperson problem on a line within the recently proposed learning-augmented framework, which provides predictions on the requests to be served via machine learning. In the original model (with no predictions), there is a stream of requests released over time along the line. The goal is to minimize the sum (or average) of the completion times of the requests. In the original model, the state-of-the-art competitive ratio lower bound is $1+\sqrt{2} > 2.414$ for any deterministic algorithm and the state-of-the-art competitive ratio upper bound is 4 for a deterministic algorithm. Our prediction model involves predicted positions, possibly error-prone, of each request in the stream known a priori but the arrival times of requests are not known until their arrival. We first establish a 3-competitive lower bound which extends to the original model. We then design a deterministic algorithm that is $(2+\sqrt{3})\approx 3.732$-competitive when predictions are perfect. With imperfect predictions (maximum error $δ> 0$), we show that our deterministic algorithm becomes $\min\{3.732+4δ,4\}$-competitive, knowing $δ$. To the best of our knowledge, these are the first results for online traveling repairperson problem in the learning-augmented framework.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [1030] [Deterministic and probabilistic neural surrogates of global hybrid-Vlasov simulations](https://arxiv.org/abs/2601.12614)
*Daniel Holmberg,Ivan Zaitsev,Markku Alho,Ioanna Bouri,Fanni Franssila,Haewon Jeong,Minna Palmroth,Teemu Roos*

Main category: physics.space-ph

TL;DR: The paper develops graph-based machine learning models to emulate complex 5D hybrid-Vlasov simulations, achieving a simulation speedup by over two orders of magnitude while maintaining physical accuracy.


<details>
  <summary>Details</summary>
Motivation: Hybrid-Vlasov simulations are important for modeling the solar wind-magnetosphere interaction, but they are computationally expensive. Real-time capabilities require faster yet accurate alternatives.

Method: The authors implement Graph Neural Networks, specifically deterministic forecasting and probabilistic ensemble forecasting models (Graph-FM and Graph-EFM) to emulate the 5D Vlasiator simulation results. They incorporate a divergence penalty for magnetic field consistency and a calibration objective for probabilistic forecasts.

Result: The proposed emulators achieve accurate plasma state predictions, maintain physical consistency, and provide speedups by over two orders of magnitude compared to traditional simulations, transitioning from 100 CPUs to a single GPU.

Conclusion: Graph-based machine learning can make computationally expensive hybrid-Vlasov simulations tractable for real-time applications while ensuring physical accuracy and forecast uncertainty.

Abstract: Hybrid-Vlasov simulations resolve ion-kinetic effects for modeling the solar wind-magnetosphere interaction, but even 5D (2D + 3V) simulations are computationally expensive. We show that graph-based machine learning emulators can learn the spatiotemporal evolution of electromagnetic fields and lower order moments of ion velocity distribution in the near-Earth space environment from four 5D Vlasiator runs performed with identical steady solar wind conditions. The initial ion number density is systematically varied, while the grid spacing is held constant, to scan the ratio of the characteristic ion skin depth to the numerical grid size. Using a graph neural network architecture operating on the 2D spatial simulation grid comprising 670k cells, we demonstrate that both a deterministic forecasting model (Graph-FM) and a probabilistic ensemble forecasting model (Graph-EFM) based on a latent variable formulation are capable of producing accurate predictions of future plasma states. A divergence penalty is incorporated during training to encourage divergence-freeness in the magnetic fields and improve physical consistency. For the probabilistic model, a continuous ranked probability score objective is added to improve the calibration of the ensemble forecasts. When trained, the emulators achieve more than two orders of magnitude speedup in generating the next time step relative to the original simulation on a single GPU compared to 100 CPUs for the Vlasiator runs, while closely matching physical magnetospheric response of the different runs. These results demonstrate that machine learning offers a way to make hybrid-Vlasov simulation tractable for real-time use while providing forecast uncertainty.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [1031] [HERMES: A Unified Open-Source Framework for Realtime Multimodal Physiological Sensing, Edge AI, and Intervention in Closed-Loop Smart Healthcare Applications](https://arxiv.org/abs/2601.12610)
*Maxim Yudayev,Juha Carlon,Diwas Lamsal,Vayalet Stefanova,Benjamin Filtjens*

Main category: eess.SY

TL;DR: HERMES is an open-source Python framework designed for continuous multimodal sensing and real-time AI processing, aiming to enhance assistive technologies in healthcare environments.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations in reliable high-throughput multimodal sensing and processing for assistive technologies, which is critical for supporting people with disabilities and age-related functional decline.

Method: HERMES is presented as a Python framework that enables synchronized data collection from distributed sensors, real-time data streaming, and AI inference using PyTorch models on commodity computing devices.

Result: The framework was validated on a closed-loop intelligent prosthesis use case, demonstrating its capability through synchronized hosting of 18 wearable and off-body modalities with 4 hosts.

Conclusion: HERMES offers a comprehensive methodology to advance real-world implementation, bridging cross-disciplinary challenges and aiding in downstream AI model development for intelligent healthcare solutions.

Abstract: Intelligent assistive technologies are increasingly recognized as critical daily-use enablers for people with disabilities and age-related functional decline. Longitudinal studies, curation of quality datasets, live monitoring in activities of daily living, and intelligent intervention devices, share the largely unsolved need in reliable high-throughput multimodal sensing and processing. Streaming large heterogeneous data from distributed sensors, historically closed-source environments, and limited prior works on realtime closed-loop AI methodologies, inhibit such applications. To accelerate the emergence of clinical deployments, we deliver HERMES - an open-source high-performance Python framework for continuous multimodal sensing and AI processing at the edge. It enables synchronized data collection, and realtime streaming inference with user PyTorch models, on commodity computing devices. HERMES is applicable to fixed-lab and free-living environments, of distributed commercial and custom sensors. It is the first work to offer a holistic methodology that bridges cross-disciplinary gaps in real-world implementation strategies, and guides downstream AI model development. Its application on the closed-loop intelligent prosthesis use case illustrates the process of suitable AI model development from the generated constraints and trade-offs. Validation on the use case, with 4 synchronized hosts cooperatively capturing 18 wearable and off-body modalities, demonstrates performance and relevance of HERMES to the trajectory of the intelligent healthcare domain.

</details>


### [1032] [A Constraint Programming Model for the Super-Agile Earth Observation Satellite Imaging Scheduling Problem](https://arxiv.org/abs/2601.11967)
*Margarida Caleiras,Samuel Moniz,Paulo Jorge Nascimento*

Main category: eess.SY

TL;DR: The study formulates the first exact Constraint Programming model for scheduling tasks on super-agile Earth observation satellites (SAEOS), tackling complex challenges introduced by their flexibility.


<details>
  <summary>Details</summary>
Motivation: The demand for satellite imaging is increasing, and the new SAEOS systems offer significant observation flexibility. Current methods for task scheduling do not account for the unique challenges of SAEOS, such as variable durations and multiple imaging directions. Addressing these gaps is crucial for achieving optimal satellite utilization.

Method: The study presents an exact Constraint Programming model for the scheduling problem of SAEOS, incorporating flexible observation windows, multiple pointing directions, and sequence-dependent transition times across multiple satellites.

Result: Computational experiments using a newly created benchmark set show that the model is efficient and offers superior computational performance compared to current non-exact state-of-the-art methods.

Conclusion: The proposed Constraint Programming approach is effective, capable of solving complex scheduling issues for SAEOS and outperforming existing methods in terms of computational efficiency.

Abstract: As the dependence on satellite imaging continues to grow, modern satellites have become increasingly agile, with the new generation, namely super-agile Earth observation satellites (SAEOS), providing unprecedented imaging flexibility. The highly dynamic capabilities of these satellites introduce additional challenges to the scheduling of observation tasks, as existing approaches for conventional agile satellites do not account for variable observation durations and multiple imaging directions. Although some efforts have been made in this regard, the SAEOS imaging scheduling problem (SAEOS-ISP) remains largely unexplored, and no exact approaches have yet been proposed. In this context, this study presents the first exact Constraint Programming formulation for the SAEOS-ISP, considering flexible observation windows, multiple pointing directions and sequence-dependent transition times across multiple satellites. Computational experiments on a newly generated benchmark set demonstrate that the model can be solved efficiently and within very short computational times. Moreover, the results also show that the proposed approach has the potential to achieve higher computational performance compared to the non-exact approaches that are currently considered state-of-the-art.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [1033] [How Well Do LLMs Predict Human Behavior? A Measure of their Pretrained Knowledge](https://arxiv.org/abs/2601.12343)
*Wayne Gao,Sukjin Han,Annie Liang*

Main category: econ.EM

TL;DR: This paper introduces a measure called equivalent sample size to evaluate how much task-specific prediction knowledge a pretrained large language model (LLM) contains and tests it on economic data.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to evaluate the potential of large language models (LLMs) to predict human behavior and assess their domain-specific knowledge, which often varies significantly across tasks.

Method: The equivalent sample size of an LLM is determined by comparing its predictive accuracy to that of flexible machine learning models trained on varying amounts of real domain-specific data. A new asymptotic theory for cross-validated prediction error is developed for statistical testing.

Result: The proposed metric was applied to economic data from the Panel Study of Income Dynamics, showing that LLMs encode significant predictive information for certain economic variables, while being less effective for others.

Conclusion: The value of LLMs in substituting domain-specific data varies significantly by domain, highlighting limitations and potential opportunities for their application.

Abstract: Large language models (LLMs) are increasingly used to predict human behavior. We propose a measure for evaluating how much knowledge a pretrained LLM brings to such a prediction: its equivalent sample size, defined as the amount of task-specific data needed to match the predictive accuracy of the LLM. We estimate this measure by comparing the prediction error of a fixed LLM in a given domain to that of flexible machine learning models trained on increasing samples of domain-specific data. We further provide a statistical inference procedure by developing a new asymptotic theory for cross-validated prediction error. Finally, we apply this method to the Panel Study of Income Dynamics. We find that LLMs encode considerable predictive information for some economic variables but much less for others, suggesting that their value as substitutes for domain-specific data differs markedly across settings.

</details>


### [1034] [Nonlinear Dynamic Factor Analysis With a Transformer Network](https://arxiv.org/abs/2601.12039)
*Oliver Snellman*

Main category: econ.EM

TL;DR: The paper proposes a Transformer-based method for dynamic factor estimation in multivariate time series, integrating prior knowledge for better accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the estimation of dynamic factors from multivariate time series, especially under deviations from standard assumptions, and support the detection of regime shifts.

Method: A Transformer architecture was introduced, incorporating a regularization term with a conventional factor model as prior, and interpreting results using Attention matrices.

Result: Monte Carlo experiments demonstrated the Transformer's higher accuracy compared to linear models under non-linear and non-Gaussian settings.

Conclusion: The Transformer model proves effective in estimating factors and detecting shifts, as shown by its application to analyze U.S. economic activity.

Abstract: The paper develops a Transformer architecture for estimating dynamic factors from multivariate time series data under flexible identification assumptions. Performance on small datasets is improved substantially by using a conventional factor model as prior information via a regularization term in the training objective. The results are interpreted with Attention matrices that quantify the relative importance of variables and their lags for the factor estimate. Time variation in Attention patterns can help detect regime switches and evaluate narratives. Monte Carlo experiments suggest that the Transformer is more accurate than the linear factor model, when the data deviate from linear-Gaussian assumptions. An empirical application uses the Transformer to construct a coincident index of U.S. real economic activity.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [1035] [Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving](https://arxiv.org/abs/2601.12142)
*Ziang Guo,Feng Yang,Xuefeng Zhang,Jiaqi Guo,Kun Zhao,Peng Lu,Zufeng Zhang,Sifa Zheng*

Main category: eess.AS

TL;DR: EchoVLA integrates audio instructions, including emotional cues, to enhance autonomous driving by reducing collision rates and aligning with nuanced user intentions.


<details>
  <summary>Details</summary>
Motivation: Existing Vision Language Action (VLA) models for autonomous driving struggle to adapt to shifting objectives and user intentions because they treat language as static input fixed at inference time.

Method: The paper proposes EchoVLA, which incorporates audio commands alongside camera data, trains a multimodal Chain-of-Thought (CoT) model using an augmented nuScenes dataset with diverse emotional speech commands, and refines driving behavior based on both semantic and emotional content of the audio.

Result: EchoVLA achieves significant improvements: a 59.4% reduction in L2 error and 74.4% reduction in collision rates compared to vision-only baselines, and effectively adapts to user emotion-infused commands.

Conclusion: EchoVLA demonstrates the feasibility and benefits of incorporating emotion-aware audio cues into VLA models, leading to safer and more intuitive autonomous driving solutions.

Abstract: Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\%$ and the collision rate by $74.4\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user's speech.

</details>


### [1036] [Lightweight Self-Supervised Detection of Fundamental Frequency and Accurate Probability of Voicing in Monophonic Music](https://arxiv.org/abs/2601.11768)
*Venkat Suprabath Bitra,Homayoon Beigi*

Main category: eess.AS

TL;DR: This paper introduces a self-supervised approach for fundamental frequency and voicing estimation using lightweight methods without relying heavily on annotated datasets.


<details>
  <summary>Details</summary>
Motivation: The need for reliable fundamental frequency and voicing estimation for neural synthesis is challenged by artifacts in realistic recordings and dependency on large labeled datasets.

Method: The approach uses transposition-equivariant learning on CQT features, an EM-style iterative reweighting scheme with Shift Cross-Entropy (SCE) consistency, and pseudo-labeling for lightweight voicing classification.

Result: Tested on MedleyDB and MDB-stem-synth datasets, the method achieves competitive metrics (RPA 95.84, RCA 96.24) in cross-corpus evaluations and generalizes across instruments.

Conclusion: The proposed framework demonstrates effective self-supervised estimation and classification while addressing limitations associated with noisy or unvoiced audio data.

Abstract: Reliable fundamental frequency (F 0) and voicing estimation is essential for neural synthesis, yet many pitch extractors depend on large labeled corpora and degrade under realistic recording artifacts. We propose a lightweight, fully self-supervised framework for joint F 0 estimation and voicing inference, designed for rapid single-instrument training from limited audio. Using transposition-equivariant learning on CQT features, we introduce an EM-style iterative reweighting scheme that uses Shift Cross-Entropy (SCE) consistency as a reliability signal to suppress uninformative noisy/unvoiced frames. The resulting weights provide confidence scores that enable pseudo-labeling for a separate lightweight voicing classifier without manual annotations. Trained on MedleyDB and evaluated on MDB-stem-synth ground truth, our method achieves competitive cross-corpus performance (RPA 95.84, RCA 96.24) and demonstrates cross-instrument generalization.

</details>


### [1037] [Lightweight Prompt Biasing for Contextualized End-to-End ASR Systems](https://arxiv.org/abs/2506.06252)
*Bo Ren,Yu Shi,Jinyu Li*

Main category: eess.AS

TL;DR: This paper proposes a prompt-based biasing technique to improve ASR accuracy for rare and domain-specific entities, resulting in significant error rate reductions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge in recognizing rare and domain-specific entities accurately in End-to-End ASR systems.

Method: A unified multitask learning framework incorporating a prompt biasing model for contextual entity focus and an entity filtering mechanism for irrelevant entity removal.

Result: Achieved a 30.7% and 18% relative reduction in Entity Word Error Rate compared to the baseline model on datasets with small and large entity lists, respectively.

Conclusion: The method is efficient and simple with no structural changes, making it lightweight and effective for improving ASR accuracy on entities.

Abstract: End-to-End Automatic Speech Recognition (ASR) has advanced significantly yet still struggles with rare and domain-specific entities. This paper introduces a simple yet efficient prompt-based biasing technique for contextualized ASR, enhancing recognition accuracy by leverage a unified multitask learning framework. The approach comprises two key components: a prompt biasing model which is trained to determine when to focus on entities in prompt, and a entity filtering mechanism which efficiently filters out irrelevant entities. Our method significantly enhances ASR accuracy on entities, achieving a relative 30.7% and 18.0% reduction in Entity Word Error Rate compared to the baseline model with shallow fusion on in-house domain dataset with small and large entity lists, respectively. The primary advantage of this method lies in its efficiency and simplicity without any structure change, making it lightweight and highly efficient.

</details>


### [1038] [AQUA-Bench: Beyond Finding Answers to Knowing When There Are None in Audio Question Answering](https://arxiv.org/abs/2601.12248)
*Chun-Yi Kuan,Hung-yi Lee*

Main category: eess.AS

TL;DR: The paper introduces AQUA-Bench, a benchmark for evaluating audio question unanswerability, addressing gaps in existing benchmarks focused only on answerable questions.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of unanswerable questions in audio-aware large language models, which are often neglected by current benchmarks.

Method: AQUA-Bench systematically evaluates three unanswerability scenarios: Absent Answer Detection, Incompatible Answer Set Detection, and Incompatible Audio Question Detection.

Result: Experiments reveal that models excel at standard answerable tasks but struggle significantly with unanswerable ones.

Conclusion: Current audio-language systems lack robustness in handling unanswerable queries, and AQUA-Bench aids in promoting their reliability.

Abstract: Recent advances in audio-aware large language models have shown strong performance on audio question answering. However, existing benchmarks mainly cover answerable questions and overlook the challenge of unanswerable ones, where no reliable answer can be inferred from the audio. Such cases are common in real-world settings, where questions may be misleading, ill-posed, or incompatible with the information. To address this gap, we present AQUA-Bench, a benchmark for Audio Question Unanswerability Assessment. It systematically evaluates three scenarios: Absent Answer Detection (the correct option is missing), Incompatible Answer Set Detection (choices are categorically mismatched with the question), and Incompatible Audio Question Detection (the question is irrelevant or lacks sufficient grounding in the audio). By assessing these cases, AQUA-Bench offers a rigorous measure of model reliability and promotes the development of audio-language systems that are more robust and trustworthy. Our experiments suggest that while models excel on standard answerable tasks, they often face notable challenges with unanswerable ones, pointing to a blind spot in current audio-language understanding.

</details>


### [1039] [Purification Before Fusion: Toward Mask-Free Speech Enhancement for Robust Audio-Visual Speech Recognition](https://arxiv.org/abs/2601.12436)
*Linzhi Wu,Xingyu Zhang,Hao Yuan,Yakun Zhang,Changyan Zheng,Liang Xie,Tiejun Liu,Erwei Yin*

Main category: eess.AS

TL;DR: The paper presents a Conformer-based AVSR framework enhancing noisy audio recognition by fusing visual and audio features without relying on explicit noise masking.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in noisy audio processing within AVSR systems, where mask-based strategies risk removing important semantic data along with noise.

Method: An end-to-end Conformer-based framework is proposed for AVSR, utilizing bottleneck fusion and integrating video assistance to refine noisy audio features.

Result: Experiments on the LRS3 dataset demonstrate superior performance of this method over traditional mask-based AVSR techniques in noisy conditions.

Conclusion: The proposed framework effectively enhances noise robustness in AVSR, maintaining semantic integrity of speech and improving recognition under noisy settings.

Abstract: Audio-visual speech recognition (AVSR) typically improves recognition accuracy in noisy environments by integrating noise-immune visual cues with audio signals. Nevertheless, high-noise audio inputs are prone to introducing adverse interference into the feature fusion process. To mitigate this, recent AVSR methods often adopt mask-based strategies to filter audio noise during feature interaction and fusion, yet such methods risk discarding semantically relevant information alongside noise. In this work, we propose an end-to-end noise-robust AVSR framework coupled with speech enhancement, eliminating the need for explicit noise mask generation. This framework leverages a Conformer-based bottleneck fusion module to implicitly refine noisy audio features with video assistance. By reducing modality redundancy and enhancing inter-modal interactions, our method preserves speech semantic integrity to achieve robust recognition performance. Experimental evaluations on the public LRS3 benchmark suggest that our method outperforms prior advanced mask-based baselines under noisy conditions.

</details>


### [1040] [Adaptive Rotary Steering with Joint Autoregression for Robust Extraction of Closely Moving Speakers in Dynamic Scenarios](https://arxiv.org/abs/2601.12345)
*Jakob Kienegger,Timo Gerkmann*

Main category: eess.AS

TL;DR: The paper addresses challenges in enhancing Ambisonics for dynamic multi-speaker scenarios by introducing an automated and joint autoregressive spatial filtering framework.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve spatial filtering and speaker tracking in complex scenarios, such as moving speakers and overlapping speech, where traditional methods face limitations.

Method: They propose a joint autoregressive framework that combines spatial cues and temporal-spectral correlations of speech to improve multi-speaker tracking and enhancement.

Result: The method shows significant improvements in tracking and enhancement of overlapping speakers, outperforming non-autoregressive approaches in both synthetic and real-world recordings.

Conclusion: The proposed framework enhances Ambisonics in dynamic and challenging multi-speaker conditions, offering robust tracking and speech enhancement capabilities.

Abstract: Latest advances in deep spatial filtering for Ambisonics demonstrate strong performance in stationary multi-speaker scenarios by rotating the sound field toward a target speaker prior to multi-channel enhancement. For applicability in dynamic acoustic conditions with moving speakers, we propose to automate this rotary steering using an interleaved tracking algorithm conditioned on the target's initial direction. However, for nearby or crossing speakers, robust tracking becomes difficult and spatial cues less effective for enhancement. By incorporating the processed recording as additional guide into both algorithms, our novel joint autoregressive framework leverages temporal-spectral correlations of speech to resolve spatially challenging speaker constellations. Consequently, our proposed method significantly improves tracking and enhancement of closely spaced speakers, consistently outperforming comparable non-autoregressive methods on a synthetic dataset. Real-world recordings complement these findings in complex scenarios with multiple speaker crossings and varying speaker-to-array distances.

</details>


### [1041] [Bone-conduction Guided Multimodal Speech Enhancement with Conditional Diffusion Models](https://arxiv.org/abs/2601.12354)
*Sina Khanagha,Bunlong Lay,Timo Gerkmann*

Main category: eess.AS

TL;DR: The paper combines bone-conduction sensors with air-conducted microphones using a conditional diffusion model for improved speech enhancement, particularly in noisy environments.


<details>
  <summary>Details</summary>
Motivation: To address the issue of performance degradation in single-channel speech enhancement under extremely noisy conditions by incorporating noise-immune bone-conducted speech data.

Method: The method involves integrating bone-conduction sensors with air-conducted microphones using a conditional diffusion model to create a novel multimodal speech enhancement framework.

Result: The proposed framework outperforms previous multimodal approaches and a strong single-modal diffusion-based baseline across diverse acoustic conditions.

Conclusion: The multimodal approach utilizing bone-conduction and air-conducted modalities showcases significant potential in enhancing speech quality in challenging noisy scenarios.

Abstract: Single-channel speech enhancement models face significant performance degradation in extremely noisy environments. While prior work has shown that complementary bone-conducted speech can guide enhancement, effective integration of this noise-immune modality remains a challenge. This paper introduces a novel multimodal speech enhancement framework that integrates bone-conduction sensors with air-conducted microphones using a conditional diffusion model. Our proposed model significantly outperforms previously established multimodal techniques and a powerful diffusion-based single-modal baseline across a wide range of acoustic conditions.

</details>


### [1042] [SLAP: Scalable Language-Audio Pretraining with Variable-Duration Audio and Multi-Objective Training](https://arxiv.org/abs/2601.12594)
*Xinhao Mei,Gael Le Lan,Haohe Liu,Zhaoheng Ni,Varun Nagaraja,Yang Liu,Yangyang Shi,Vikas Chandra*

Main category: eess.AS

TL;DR: SLAP improves upon existing CLAP models by addressing dataset size, duration constraints, and training objectives, offering better audio-text understanding.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current CLAP models in terms of small datasets, fixed audio durations, and global representation learning.

Method: SLAP uses 109 million audio-text pairs, supports variable audio durations, and integrates contrastive, self-supervised, and captioning losses into a unified training process.

Result: SLAP establishes new state-of-the-art results in audio-text retrieval and zero-shot audio classification across multiple benchmarks.

Conclusion: SLAP demonstrates enhanced scalability and richer audio representation learning, making it more practical for diverse audio-related tasks.

Abstract: Contrastive language-audio pretraining (CLAP) has achieved notable success in learning semantically rich audio representations and is widely adopted for various audio-related tasks. However, current CLAP models face several key limitations. First, they are typically trained on relatively small datasets, often comprising a few million audio samples. Second, existing CLAP models are restricted to short and fixed duration, which constrains their usage in real-world scenarios with variable-duration audio. Third, the standard contrastive training objective operates on global representations, which may hinder the learning of dense, fine-grained audio features. To address these challenges, we introduce Scalable Language-Audio Pretraining (SLAP), which scales language-audio pretraining to 109 million audio-text pairs with variable audio durations and incorporates multiple training objectives. SLAP unifies contrastive loss with additional self-supervised and captioning losses in a single-stage training, facilitating the learning of richer dense audio representations. The proposed SLAP model achieves new state-of-the-art performance on audio-text retrieval and zero-shot audio classification tasks, demonstrating its effectiveness across diverse benchmarks.

</details>


### [1043] [Co-Initialization of Control Filter and Secondary Path via Meta-Learning for Active Noise Control](https://arxiv.org/abs/2601.13849)
*Ziyi Yang,Li Rao,Zhengding Luo,Dongyuan Shi,Qirui Huang,Woon-Seng Gan*

Main category: eess.AS

TL;DR: The paper proposes using MAML co-initialization to accelerate adaptation in ANC systems during acoustic environment changes, highlighting improved efficiency and reduced error without altering runtime algorithms.


<details>
  <summary>Details</summary>
Motivation: ANC systems typically struggle with slow adaptation during acoustic environment changes, and their early performance is influenced heavily by the initialization process.

Method: MAML co-initialization is applied to jointly optimize the control filter and secondary-path model, using short inner loops for pre-training coefficients based on measured paths that replicate identification and noise reduction phases.

Result: The approach significantly reduced early-stage error, time-to-target, auxiliary-noise energy, and improved recovery speed when paths change, surpassing conventional re-initialization methods.

Conclusion: The proposed initialization method enhances feedforward ANC systems by enabling faster and more efficient adaptation during environmental changes with minimal pre-training requirements.

Abstract: Active noise control (ANC) must adapt quickly when the acoustic environment changes, yet early performance is largely dictated by initialization. We address this with a Model-Agnostic Meta-Learning (MAML) co-initialization that jointly sets the control filter and the secondary-path model for FxLMS-based ANC while keeping the runtime algorithm unchanged. The initializer is pre-trained on a small set of measured paths using short two-phase inner loops that mimic identification followed by residual-noise reduction, and is applied by simply setting the learned initial coefficients. In an online secondary path modeling FxLMS testbed, it yields lower early-stage error, shorter time-to-target, reduced auxiliary-noise energy, and faster recovery after path changes than a baseline without re-initialization. The method provides a simple fast start for feedforward ANC under environment changes, requiring a small set of paths to pre-train.

</details>


### [1044] [Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models](https://arxiv.org/abs/2601.13948)
*Nikita Kuzmin,Songting Liu,Kong Aik Lee,Eng Siong Chng*

Main category: eess.AS

TL;DR: Stream-Voice-Anon significantly enhances streaming speaker anonymization using NAC-based architecture, achieving better intelligibility, emotion preservation, and low latency compared to prior methods.


<details>
  <summary>Details</summary>
Motivation: Speaker identity protection is vital for voice applications, yet streaming anonymization lacks sufficient exploration and effective solutions.

Method: The method integrates anonymization techniques into NAC-based architectures, using pseudo-speaker representations, speaker embedding mixing, and diverse LM prompt conditioning strategies.

Result: Stream-Voice-Anon demonstrates improved intelligibility, emotion preservation, and latency compared to existing methods under the VoicePrivacy 2024 protocol, with slight privacy trade-offs under some attacker scenarios.

Conclusion: Stream-Voice-Anon successfully adapts NAC architectures for advancing streaming speaker anonymization, addressing latency and privacy challenges effectively.

Abstract: Protecting speaker identity is crucial for online voice applications, yet streaming speaker anonymization (SA) remains underexplored. Recent research has demonstrated that neural audio codec (NAC) provides superior speaker feature disentanglement and linguistic fidelity. NAC can also be used with causal language models (LM) to enhance linguistic fidelity and prompt control for streaming tasks. However, existing NAC-based online LM systems are designed for voice conversion (VC) rather than anonymization, lacking the techniques required for privacy protection. Building on these advances, we present Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures specifically for streaming SA by integrating anonymization techniques. Our anonymization approach incorporates pseudo-speaker representation sampling, a speaker embedding mixing and diverse prompt selection strategies for LM conditioning that leverage the disentanglement properties of quantized content codes to prevent speaker information leakage. Additionally, we compare dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves substantial improvements in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% UAR relative) compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency (180ms vs 200ms) and privacy protection against lazy-informed attackers, though showing 15% relative degradation against semi-informed attackers.

</details>


### [1045] [DAME: Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification](https://arxiv.org/abs/2601.13999)
*Youngmoon Jung,Joon-Young Yang,Ju-ho Kim,Jaeyoung Roh,Chang Woo Han,Hoon-Young Cho*

Main category: eess.AS

TL;DR: The paper introduces Duration-Aware Matryoshka Embedding (DAME), a framework for speaker verification that adjusts embedding dimensions based on utterance length. It improves performance for short utterances without adding inference cost.


<details>
  <summary>Details</summary>
Motivation: Speaker verification with short utterances remains challenging due to limited speaker-discriminative cues, and current methods fail to adapt embeddings effectively across varying durations.

Method: The proposed DAME framework creates a nested hierarchy of sub-embeddings, with lower dimensions for short utterances and higher dimensions for longer ones. This aligns embedding capacity with utterance duration.

Result: DAME reduces equal error rates for short-duration utterances across multiple datasets, maintains performance for longer utterances, and generalizes across various speaker encoder architectures.

Conclusion: DAME provides a model-agnostic enhancement for speaker verification tasks by aligning embedding dimensions to utterance durations, improving both short and long duration accuracy without extra inference cost.

Abstract: Short-utterance speaker verification remains challenging due to limited speaker-discriminative cues in short speech segments. While existing methods focus on enhancing speaker encoders, the embedding learning strategy still forces a single fixed-dimensional representation reused for utterances of any length, leaving capacity misaligned with the information available at different durations. We propose Duration-Aware Matryoshka Embedding (DAME), a model-agnostic framework that builds a nested hierarchy of sub-embeddings aligned to utterance durations: lower-dimensional representations capture compact speaker traits from short utterances, while higher dimensions encode richer details from longer speech. DAME supports both training from scratch and fine-tuning, and serves as a direct alternative to conventional large-margin fine-tuning, consistently improving performance across durations. On the VoxCeleb1-O/E/H and VOiCES evaluation sets, DAME consistently reduces the equal error rate on 1-s and other short-duration trials, while maintaining full-length performance with no additional inference cost. These gains generalize across various speaker encoder architectures under both general training and fine-tuning setups.

</details>


### [1046] [MATE: Matryoshka Audio-Text Embeddings for Open-Vocabulary Keyword Spotting](https://arxiv.org/abs/2601.14012)
*Youngmoon Jung,Myunghun Jung,Joon-Young Yang,Yong-Hyeok Lee,Jaeyoung Roh,Hoon-Young Cho*

Main category: eess.AS

TL;DR: The paper introduces Matryoshka Audio-Text Embeddings (MATE), enabling multi-granularity embeddings for open-vocabulary keyword spotting while achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Improve flexibility and efficiency in open-vocabulary keyword spotting by addressing limitations of fixed-dimensional embeddings.

Method: Proposed MATE framework uses nested sub-embeddings within a single vector and PCA-guided alignment to compress and align audio and text embeddings at different dimensionalities.

Result: MATE achieves state-of-the-art performance on WSJ and LibriPhrase datasets without adding inference overhead.

Conclusion: MATE demonstrates the potential of matryoshka-style embeddings for keyword spotting, enhancing embedding granularity without additional computational demands.

Abstract: Open-vocabulary keyword spotting (KWS) with text-based enrollment has emerged as a flexible alternative to fixed-phrase triggers. Prior utterance-level matching methods, from an embedding-learning standpoint, learn embeddings at a single fixed dimensionality. We depart from this design and propose Matryoshka Audio-Text Embeddings (MATE), a dual-encoder framework that encodes multiple embedding granularities within a single vector via nested sub-embeddings ("prefixes"). Specifically, we introduce a PCA-guided prefix alignment: PCA-compressed versions of the full text embedding for each prefix size serve as teacher targets to align both audio and text prefixes. This alignment concentrates salient keyword cues in lower-dimensional prefixes, while higher dimensions add detail. MATE is trained with standard deep metric learning objectives for audio-text KWS, and is loss-agnostic. To our knowledge, this is the first application of matryoshka-style embeddings to KWS, achieving state-of-the-art results on WSJ and LibriPhrase without any inference overhead.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [1047] [Nixie: Efficient, Transparent Temporal Multiplexing for Consumer GPUs](https://arxiv.org/abs/2601.11743)
*Yechen Xu,Yifei Wang,Nathanael Ren,Yiran Chen,Danyang Zhuo*

Main category: cs.OS

TL;DR: Nixie is a system for efficient GPU sharing on consumer devices, improving memory usage and latency without changing applications or drivers.


<details>
  <summary>Details</summary>
Motivation: Consumer GPUs face challenges in handling single-user, memory-intensive ML workloads like LLMs due to inefficient existing memory sharing mechanisms, resulting in thrashing and excessive pinned memory usage.

Method: The paper presents Nixie, a system service that coordinates GPU memory allocation and kernel launches. It uses a lightweight scheduler with MLFQ-inspired techniques to prioritize latency-sensitive tasks, ensuring better memory utilization and responsiveness.

Result: Nixie reduced latency of interactive tasks by up to 3.8x and CPU pinned memory usage by 66.8% under the same latency conditions.

Conclusion: Nixie enables efficient and transparent GPU memory multiplexing on consumer devices, enhancing both resource utilization and interactive performance for ML workloads without requiring modifications to applications or drivers.

Abstract: Consumer machines are increasingly running large ML workloads such as large language models (LLMs), text-to-image generation, and interactive image editing. Unlike datacenter GPUs, consumer GPUs serve single-user, rapidly changing workloads, and each model's working set often nearly fills the GPU memory. As a result, existing sharing mechanisms (e.g., NVIDIA Unified Virtual Memory) perform poorly due to memory thrashing and excessive use of CPU pinned memory when multiple applications are active.
  We design and implement Nixie, a system that enables efficient and transparent temporal multiplexing on consumer GPUs without requiring any application or driver changes. Nixie is a system service that coordinates GPU memory allocation and kernel launch behavior to efficiently utilize the CPU-GPU bi-directional bandwidth and CPU pinned memory. A lightweight scheduler in Nixie further improves responsiveness by automatically prioritizing latency-sensitive interactive jobs using MLFQ-inspired techniques. Our evaluations show that Nixie improves latency of real interactive code-completion tasks by up to $3.8\times$ and saves up to 66.8% CPU pinned memory usage given the same latency requirement.

</details>


### [1048] [ContiguousKV: Accelerating LLM Prefill with Granularity-Aligned KV Cache Management](https://arxiv.org/abs/2601.13631)
*Jing Zou,Shangyu Wu,Hancong Duan,Qiao Li,Chun Jason Xue*

Main category: cs.OS

TL;DR: The paper introduces ContiguousKV, an advanced system designed to boost performance in managing prefix KV cache offloading for Large Language Models (LLMs) in Re-Prefill phases by optimizing I/O efficiency and eliminating resource bottlenecks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address severe I/O bottlenecks in loading persistent prefix KV cache for LLMs during the Re-Prefill phase, which hampers scalability and efficient usage of system resources.

Method: The paper proposes aligning KV cache pruning with I/O operations via ContiguousChunk and uses asynchronous prefetching alongside attention-guided cache management to optimize I/O performance and eliminate resource utilization issues.

Result: Evaluations show ContiguousKV achieves a 3.85x speedup over existing systems like IMPRESS in the Re-Prefill phase while retaining high output quality.

Conclusion: ContiguousKV effectively bridges algorithmic needs and I/O efficiency, resolving bottlenecks in KV cache offloading to improve scalability and performance in LLM serving applications.

Abstract: Efficiently serving Large Language Models (LLMs) with persistent Prefix Key-Value (KV) Cache is critical for applications like conversational search and multi-turn dialogue. Serving a request requires loading the pre-computed prefix KV cache and generating the first token, defined as the Re-Prefill Phase. Offloading this shared prefix cache to secondary storage is essential for memory scalability. Re-Prefill with offloading suffers from severe I/O bottlenecks in two aspects. First, semantic-aware KV cache pruning algorithms select important tokens in fine granularity, while systems manage I/O in coarse, fixed-size blocks, causing severe read amplification. Second, the sequential dependency between identifying important tokens and loading KV cache creates idle I/O and compute bubbles, under-utilizing system resources.
  This paper proposes \textit{ContiguousKV}, a high-performance prefix KV cache offloading system that bridges algorithmic semantics with I/O efficiency to accelerate the Re-Prefill phase. We first introduce \textit{ContiguousChunk}, a unified data management granularity that aligns KV cache pruning with I/O operations. All the mechanisms critical for I/O performance are performed at the granularity of ContiguousChunk, thereby eliminating read amplification. By exploiting the high similarity in important ContiguousChunk indices across layers, we propose intra- and inter-period asynchronous prefetching to break the sequential dependency between I/O and compute, effectively eliminating idle bubbles. Finally, we propose attention-guided cache management to retain semantically critical prefix data in memory. Evaluations on Qwen2.5 series models show that ContiguousKV achieves a 3.85x speedup in the Re-Prefill phase over the state-of-the-art offloading system IMPRESS, while maintaining high output quality.

</details>


### [1049] ["Range as a Key" is the Key! Fast and Compact Cloud Block Store Index with RASK](https://arxiv.org/abs/2601.14129)
*Haoru Zhao,Mingkai Dong,Erci Xu,Zhongyu Wang,Haibo Chen*

Main category: cs.OS

TL;DR: The paper introduces RASK, a memory-efficient, high-performance index for cloud block storage, addressing significant memory strain issues and achieving drastic improvements in efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing memory efficiency challenges in cloud block storage indexing, particularly as larger and denser storage systems exacerbate memory strain.

Method: Proposed RASK, a tree-structured index utilizing range-as-a-key for compact indexing, with mechanisms for handling range overlap and fragmentation issues.

Result: RASK reduces memory usage by up to 98.9% and increases throughput by up to 31.0x based on evaluation with production traces.

Conclusion: RASK demonstrates considerable potential in improving memory efficiency and performance in cloud storage systems by introducing a new indexing paradigm.

Abstract: In cloud block store, indexing is on the critical path of I/O operations and typically resides in memory. With the scaling of users and the emergence of denser storage media, the index has become a primary memory consumer, causing memory strain. Our extensive analysis of production traces reveals that write requests exhibit a strong tendency to target continuous block ranges in cloud storage systems. Thus, compared to current per-block indexing, our insight is that we should directly index block ranges (i.e., range-as-a-key) to save memory.
  In this paper, we propose RASK, a memory-efficient and high-performance tree-structured index that natively indexes ranges. While range-as-a-key offers the potential to save memory and improve performance, realizing this idea is challenging due to the range overlap and range fragmentation issues. To handle range overlap efficiently, RASK introduces the log-structured leaf, combined with range-tailored search and garbage collection. To reduce range fragmentation, RASK employs range-aware split and merge mechanisms. Our evaluations on four production traces show that RASK reduces memory footprint by up to 98.9% and increases throughput by up to 31.0x compared to ten state-of-the-art indexes.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [1050] [Forecasting Continuum Intensity for Solar Active Region Emergence Prediction using Transformers](https://arxiv.org/abs/2601.13144)
*Jonas Tirona,Sarang Patil,Spiridon Kasapis,Eren Dogan,John Stefan,Irina N. Kitiashvili,Alexander G. Kosovichev,Mengjia Xu*

Main category: astro-ph.SR

TL;DR: The study explores using Transformer architectures with early detection capabilities to forecast solar active region (AR) emergence 12 hours ahead, achieving improved accuracy and earlier warnings compared to Long Short-Term Memory (LSTM) models.


<details>
  <summary>Details</summary>
Motivation: Accurately predicting solar active region emergence is important for space weather forecasting, which impacts technology and human operations dependent on space environments.

Method: The authors used a sliding-window Transformer architecture enhanced by a temporal 1D convolutional (Conv1D) front-end and a novel 'Early Detection' system with attention biases and a timing-aware loss function, evaluated through ablation studies.

Result: The best model achieved a 10.6% improvement in RMSE over the LSTM baseline and provided an average advance warning time of 4.73 hours for AR emergence.

Conclusion: Transformer architectures with early detection biases are effective for advance warning in AR forecasting, despite increased variance, making them suitable for operational systems where early detection is a priority.

Abstract: Early and accurate prediction of solar active region (AR) emergence is crucial for space weather forecasting. Building on established Long Short-Term Memory (LSTM) based approaches for forecasting the continuum intensity decrease associated with AR emergence, this work expands the modeling with new architectures and targets. We investigate a sliding-window Transformer architecture to forecast continuum intensity evolution up to 12 hours ahead using data from 46 ARs observed by SDO/HMI. We conduct a systematic ablation study to evaluate two key components: (1) the inclusion of a temporal 1D convolutional (Conv1D) front-end and (2) a novel `Early Detection' architecture featuring attention biases and a timing-aware loss function. Our best-performing model, combining the Early Detection architecture without the Conv1D layer, achieved a Root Mean Square Error (RMSE) of 0.1189 (representing a 10.6% improvement over the LSTM baseline) and an average advance warning time of 4.73 hours (timing difference of -4.73h), even under a stricter emergence criterion than previous studies. While the Transformer demonstrates superior aggregate timing and accuracy, we note that this high-sensitivity detection comes with increased variance compared to smoother baseline models. However, this volatility is a necessary trade-off for operational warning systems: the model's ability to detect micro-changes in precursor signals enables significantly earlier detection, outweighing the cost of increased noise. Our results demonstrate that Transformer architectures modified with early detection biases, when used without temporal smoothing layers, provide a high-sensitivity alternative for forecasting AR emergence that prioritizes advance warning over statistical smoothness.

</details>


### [1051] [SolARED: Solar Active Region Emergence Dataset for Machine Learning Aided Predictions](https://arxiv.org/abs/2601.13145)
*Spiridon Kasapis,Eren Dogan,Irina N. Kitiashvili,Alexander G. Kosovichev,John T. Stefan,Jake D. Butler,Jonas Tirona,Sarang Patil,Mengjia Xu*

Main category: astro-ph.SR

TL;DR: This paper focuses on creating a dataset, SolARED, to aid in forecasting solar eruptive activity by detecting Active Regions (ARs) before their emergence. The dataset utilizes data from the Helioseismic and Magnetic Imager (HMI) on the Solar Dynamics Observatory (SDO).


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is to improve forecasts of solar eruptive activity to mitigate potential impacts on space technologies and exploration. Detecting solar Active Regions (ARs) early can help advance early-warning capabilities for space weather disruptions.

Method: The SolARED dataset is developed using full-disk maps of Doppler velocity, magnetic field, and continuum intensity data collected from the Helioseismic and Magnetic Imager (HMI) on the Solar Dynamics Observatory (SDO). It tracks and analyzes time series data of acoustic power, magnetic flux, and continuum intensity for 50 large ARs before, during, and after their emergence.

Result: SolARED provides ML-ready time series data that represent the evolution of various solar parameters related to AR activity. It is made available to the public through a web application, enabling easy access to data for researchers.

Conclusion: This dataset supports the development of predictive capabilities for active region formation and operational space weather forecasting, providing a critical tool for solar activity prediction research.

Abstract: The development of accurate forecasts of solar eruptive activity has become increasingly important for preventing potential impacts on space technologies and exploration. Therefore, it is crucial to detect Active Regions (ARs) before they start forming on the solar surface. This will enable the development of early-warning capabilities for upcoming space weather disturbances. For this reason, we prepared the Solar Active Region Emergence Dataset (SolARED). The dataset is derived from full-disk maps of the Doppler velocity, magnetic field, and continuum intensity, obtained by the Helioseismic and Magnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO). SolARED includes time series of remapped, tracked, and binned data that characterize the evolution of acoustic power of solar oscillations, unsigned magnetic flux, and continuum intensity for 50 large ARs before, during, and after their emergence on the solar surface, as well as surrounding areas observed on the solar disc between 2010 and 2023. The resulting ML-ready SolARED dataset is designed to support enhancements of predictive capabilities, enabling the development of operational forecasts for the emergence of active regions. The SolARED dataset is available at https://sun.njit.edu/sarportal/, through an interactive visualization web application.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [1052] [Identification capacity and rate-query tradeoffs in classification systems](https://arxiv.org/abs/2601.14252)
*Tristan Simas*

Main category: cs.IT

TL;DR: The paper studies a one-shot rate-distortion-like model for classification with resource constraints: tag rate, identification cost, and misclassification probability, deriving achievable performance limits and exploring their combinatorial structure.


<details>
  <summary>Details</summary>
Motivation: To understand and characterize the trade-offs between tag rate, identification cost, and misclassification probability in identifying classes from attribute observations, especially with resource limitations.

Method: The paper establishes theoretical results on achievable and converse bounds under zero-error conditions, leveraging combinatorial structures and matroid theory to define minimal query families, all supported by formal verification in Lean4.

Result: It provides tight bounds on tag rate and query cost for zero-error classification, shows limitations without tags, and connects these results to graph entropy and combinatorial structures.

Conclusion: The findings offer theoretical insights into practical problems such as type systems, databases, and biological classification, showcasing potential applications and formal assurances through mechanized proofs.

Abstract: We study a one-shot identification analogue of rate-distortion for discrete classification under three resources: tag rate L (bits of side information stored per entity), identification cost W (attribute-membership queries per identification, excluding global preprocessing and amortized caching), and distortion D (misclassification probability). The question is to characterize achievable triples (L,W,D) when a decoder must recover an entity's class from limited observations. Zero-error barrier. If two distinct classes induce the same attribute profile, then the observation pi(V) is identical for both and no decoder can identify the class from attribute queries alone. Thus, if the profile map pi is not injective on classes, zero-error identification without tags is impossible (a zero-error feasibility threshold). Achievability and converse at D=0. With k classes, nominal tags of L = ceil(log2 k) bits enable O(1) identification cost with D=0. Conversely, any scheme with D=0 must satisfy L >= log2 k bits (tight). Without tags (L=0), identification requires Omega(n) queries in the worst case and may incur D>0. Combinatorial structure. Minimal sufficient query families form the bases of a matroid; the induced distinguishing dimension is well-defined and links to zero-error source coding via graph entropy. We illustrate implications for type systems, databases, and biological taxonomy. All results are mechanized in Lean4 (6000+ lines, 0 sorry).

</details>


### [1053] [Joint Source-Channel-Generation Coding: From Distortion-oriented Reconstruction to Semantic-consistent Generation](https://arxiv.org/abs/2601.12808)
*Tong Wu,Zhiyong Chen,Guo Lu,Li Song,Feng Yang,Meixia Tao,Wenjun Zhang*

Main category: cs.IT

TL;DR: The paper introduces Joint Source-Channel-Generation Coding (JSCGC), which shifts away from distortion metrics to utilize generative models for high-quality, realistic reconstructions in communication systems.


<details>
  <summary>Details</summary>
Motivation: Traditional communication systems yield reconstructions that fail to align with complex human visual perception due to reliance on conventional distortion metrics.

Method: The method uses a generative model at the receiver to maximize mutual information under channel constraints, ensuring outputs align with authentic data distributions.

Result: JSCGC enhances perceptual quality and semantic fidelity, outperforming traditional distortion-based JSCC approaches in image transmission experiments.

Conclusion: This work establishes that leveraging generative models for communication enables more human-perceptually accurate reconstructions, highlighting its potential over traditional methods.

Abstract: Conventional communication systems, including both separation-based coding and AI-driven joint source-channel coding (JSCC), are largely guided by Shannon's rate-distortion theory. However, relying on generic distortion metrics fails to capture complex human visual perception, often resulting in blurred or unrealistic reconstructions. In this paper, we propose Joint Source-Channel-Generation Coding (JSCGC), a novel paradigm that shifts the focus from deterministic reconstruction to probabilistic generation. JSCGC leverages a generative model at the receiver as a generator rather than a conventional decoder to parameterize the data distribution, enabling direct maximization of mutual information under channel constraints while controlling stochastic sampling to produce outputs residing on the authentic data manifold with high fidelity. We further derive a theoretical lower bound on the maximum semantic inconsistency with given transmitted mutual information, elucidating the fundamental limits of communication in controlling the generative process. Extensive experiments on image transmission demonstrate that JSCGC substantially improves perceptual quality and semantic fidelity, significantly outperforming conventional distortion-oriented JSCC methods.

</details>


### [1054] [An Elementary Approach to Scheduling in Generative Diffusion Models](https://arxiv.org/abs/2601.13602)
*Qiang Sun,H. Vincent Poor,Wenyi Zhang*

Main category: cs.IT

TL;DR: The paper investigates the impact of noise scheduling and time discretization in generative diffusion models using a Gaussian source, deriving an explicit formulation for KL divergence and optimizing noise schedules to improve these models.


<details>
  <summary>Details</summary>
Motivation: To better understand and optimize how noise schedules and time discretization steps influence generative diffusion models, especially in practical applications where function evaluation budgets are constrained.

Method: The authors derive a closed-form trajectory for distributions during reverse sampling. Using the Euler-Maclaurin expansion, they analyze KL divergence convergence and solve an optimization problem via calculus of variations to determine an optimal noise schedule. They also evaluate time discretization strategies for pretrained models using KL divergence as a metric.

Result: The proposed time discretization strategies, guided by the derived optimization framework, outperform baseline and existing search-based strategies in experiments across datasets and pretrained models, particularly when function evaluation is limited.

Conclusion: The research offers an optimized approach to noise scheduling and time discretization in generative diffusion models, providing theoretically grounded solutions and practical performance gains.

Abstract: An elementary approach to characterizing the impact of noise scheduling and time discretization in generative diffusion models is developed. Considering a simplified model where the source distribution is multivariate Gaussian with a given covariance matrix, the explicit closed-form evolution trajectory of the distributions across reverse sampling steps is derived, and consequently, the Kullback-Leibler (KL) divergence between the source distribution and the reverse sampling output is obtained. The effect of the number of time discretization steps on the convergence of this KL divergence is studied via the Euler-Maclaurin expansion. An optimization problem is formulated, and its solution noise schedule is obtained via calculus of variations, shown to follow a tangent law whose coefficient is determined by the eigenvalues of the source covariance matrix. For an alternative scenario, more realistic in practice, where pretrained models have been obtained for some given noise schedules, the KL divergence also provides a measure to compare different time discretization strategies in reverse sampling. Experiments across different datasets and pretrained models demonstrate that the time discretization strategy selected by our approach consistently outperforms baseline and search-based strategies, particularly when the budget on the number of function evaluations is very tight.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [1055] [Learning Audio-Visual Embeddings with Inferred Latent Interaction Graphs](https://arxiv.org/abs/2601.11995)
*Donghuo Zeng,Hao Niu,Yanan Wang,Masato Taya*

Main category: cs.MM

TL;DR: The paper introduces a new framework for learning robust audio-visual embeddings using soft-label predictions and inferred latent interactions to address co-occurrence mislabeling issues in traditional methods.


<details>
  <summary>Details</summary>
Motivation: Standard methods for learning audio-visual embeddings misinterpret incidental co-occurrences (e.g., background noise or unannotated events) as false negatives, leading to a failure in capturing true cross-modal dependencies.

Method: The framework employs three components: (1) Audio-Visual Semantic Alignment Loss (AV-SAL) to produce aligned soft-label distributions, (2) Inferred Latent Interaction Graph (ILI) to infer dependencies using soft labels, and (3) Latent Interaction Regularizer (LIR) to guide a student network in learning embeddings based on latent dependencies.

Result: The proposed framework yields consistent improvements in mean average precision (mAP) on AVE and VEGAS benchmarks, showcasing its robustness in audio-visual representation.

Conclusion: The approach enhances the robustness and semantic coherence of audio-visual embeddings by addressing co-occurrence errors and incorporating latent interaction insights.

Abstract: Learning robust audio-visual embeddings requires bringing genuinely related audio and visual signals together while filtering out incidental co-occurrences - background noise, unrelated elements, or unannotated events. Most contrastive and triplet-loss methods use sparse annotated labels per clip and treat any co-occurrence as semantic similarity. For example, a video labeled "train" might also contain motorcycle audio and visual, because "motorcycle" is not the chosen annotation; standard methods treat these co-occurrences as negatives to true motorcycle anchors elsewhere, creating false negatives and missing true cross-modal dependencies. We propose a framework that leverages soft-label predictions and inferred latent interactions to address these issues: (1) Audio-Visual Semantic Alignment Loss (AV-SAL) trains a teacher network to produce aligned soft-label distributions across modalities, assigning nonzero probability to co-occurring but unannotated events and enriching the supervision signal. (2) Inferred Latent Interaction Graph (ILI) applies the GRaSP algorithm to teacher soft labels to infer a sparse, directed dependency graph among classes. This graph highlights directional dependencies (e.g., "Train (visual)" -> "Motorcycle (audio)") that expose likely semantic or conditional relationships between classes; these are interpreted as estimated dependency patterns. (3) Latent Interaction Regularizer (LIR): A student network is trained with both metric loss and a regularizer guided by the ILI graph, pulling together embeddings of dependency-linked but unlabeled pairs in proportion to their soft-label probabilities. Experiments on AVE and VEGAS benchmarks show consistent improvements in mean average precision (mAP), demonstrating that integrating inferred latent interactions into embedding learning enhances robustness and semantic coherence.

</details>


### [1056] [Chain-of-Thought Compression Should Not Be Blind: V-Skip for Efficient Multimodal Reasoning via Dual-Path Anchoring](https://arxiv.org/abs/2601.13879)
*Dongxu Zhang,Yiding Sun,Cheng Tan,Wenbiao Yan,Ning Yang,Jihua Zhu,Hiajun Zhang*

Main category: cs.MM

TL;DR: The paper addresses the inefficiency of Chain-of-Thought (CoT) reasoning in Multimodal Large Language Models (MLLMs) and proposes V-Skip, a method to speed up reasoning while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: CoT reasoning greatly improves MLLM performance but is slow due to its autoregressive nature. Current speed-up solutions often lead to hallucinations by improperly pruning important tokens related to visual data.

Method: The paper introduces V-Skip, which reformulates token pruning as a Visual-Anchored Information Bottleneck optimization problem. Using a dual-path gating mechanism, it evaluates token importance via linguistic surprisal and cross-modal attention flow to preserve visually critical information.

Result: V-Skip achieves a 2.9× speedup in processing time with minimal accuracy loss and performs significantly better than other methods, preserving fine-grained details in multimodal tasks, like improving over 30% in DocVQA.

Conclusion: The proposed V-Skip method effectively addresses the inefficiencies of CoT reasoning in MLLMs, combining speed improvements with strong retention of visual-semantic information and task-specific performance.

Abstract: While Chain-of-Thought (CoT) reasoning significantly enhances the performance of Multimodal Large Language Models (MLLMs), its autoregressive nature incurs prohibitive latency constraints. Current efforts to mitigate this via token compression often fail by blindly applying text-centric metrics to multimodal contexts. We identify a critical failure mode termed Visual Amnesia, where linguistically redundant tokens are erroneously pruned, leading to hallucinations. To address this, we introduce V-Skip that reformulates token pruning as a Visual-Anchored Information Bottleneck (VA-IB) optimization problem. V-Skip employs a dual-path gating mechanism that weighs token importance through both linguistic surprisal and cross-modal attention flow, effectively rescuing visually salient anchors. Extensive experiments on Qwen2-VL and Llama-3.2 families demonstrate that V-Skip achieves a $2.9\times$ speedup with negligible accuracy loss. Specifically, it preserves fine-grained visual details, outperforming other baselines over 30\% on the DocVQA.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [1057] [SciHorizon-GENE: Benchmarking LLM for Life Sciences Inference from Gene Knowledge to Functional Understanding](https://arxiv.org/abs/2601.12805)
*Xiaohan Huang,Meng Xiao,Chuan Qin,Qingqing Long,Jinmiao Chen,Yuanchun Zhou,Hengshu Zhu*

Main category: q-bio.GN

TL;DR: The paper introduces SciHorizon-GENE, a benchmark for evaluating LLMs on gene-to-function reasoning capabilities, revealing gaps and challenges in reliable biological interpretation.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the lack of evaluation tools for assessing LLMs' reasoning capabilities in gene-level knowledge interpretation, crucial for advancing biomedical research.

Method: The authors construct SciHorizon-GENE, encompassing curated knowledge for over 190K human genes, evaluate LLM behavior across four critical aspects, and test various general-purpose and biomedical LLMs.

Result: The analysis shows substantial variability in LLMs' gene reasoning capabilities, highlighting persistent challenges regarding faithfulness, completeness, and grounding in literature.

Conclusion: SciHorizon-GENE provides a structured evaluation framework for LLMs, offering insights into their effectiveness and areas for improvement in biomedical knowledge-driven tasks.

Abstract: Large language models (LLMs) have shown growing promise in biomedical research, particularly for knowledge-driven interpretation tasks. However, their ability to reliably reason from gene-level knowledge to functional understanding, However, their ability to reliably reason from gene-level knowledge to functional understanding, a core requirement for knowledge-enhanced cell atlas interpretation, remains largely underexplored. To address this gap, we introduce SciHorizon-GENE, a large-scale gene-centric benchmark constructed from authoritative biological databases. The benchmark integrates curated knowledge for over 190K human genes and comprises more than 540K questions covering diverse gene-to-function reasoning scenarios relevant to cell type annotation, functional interpretation, and mechanism-oriented analysis. Motivated by behavioral patterns observed in preliminary examinations, SciHorizon-GENE evaluates LLMs along four biologically critical perspectives: research attention sensitivity, hallucination tendency, answer completeness, and literature influence, explicitly targeting failure modes that limit the safe adoption of LLMs in biological interpretation pipelines. We systematically evaluate a wide range of state-of-the-art general-purpose and biomedical LLMs, revealing substantial heterogeneity in gene-level reasoning capabilities and persistent challenges in generating faithful, complete, and literature-grounded functional interpretations. Our benchmark establishes a systematic foundation for analyzing LLM behavior at the gene scale and offers insights for model selection and development, with direct relevance to knowledge-enhanced biological interpretation.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [1058] [New Trends in the Stability of Sinkhorn Semigroups](https://arxiv.org/abs/2601.12633)
*Pierre Del Moral,Ajay Jasra*

Main category: math.PR

TL;DR: This paper reviews advanced approaches to analyzing the stability of the Sinkhorn algorithm in entropic optimal transport, utilizing operator-theoretic and semigroup methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address stability properties of the Sinkhorn algorithm, which is significant in high-dimensional entropic optimal transport applications in machine learning and probability.

Method: The paper uses semigroup analysis with Lyapunov-type techniques, transport cost inequalities, divergence metrics, and contraction coefficients.

Result: The analysis unifies and simplifies stability arguments for Sinkhorn algorithms, introducing new contraction estimates under diverse metrics like φ-entropies and Wasserstein distances.

Conclusion: The novel methods offer a structured, self-contained framework to analyze and improve the understanding of Sinkhorn algorithm stability.

Abstract: Entropic optimal transport problems play an increasingly important role in machine learning and generative modelling. In contrast with optimal transport maps which often have limited applicability in high dimensions, Schrodinger bridges can be solved using the celebrated Sinkhorn's algorithm, a.k.a. the iterative proportional fitting procedure. The stability properties of Sinkhorn bridges when the number of iterations tends to infinity is a very active research area in applied probability and machine learning. Traditional proofs of convergence are mainly based on nonlinear versions of Perron-Frobenius theory and related Hilbert projective metric techniques, gradient descent, Bregman divergence techniques and Hamilton-Jacobi-Bellman equations, including propagation of convexity profiles based on coupling diffusions by reflection methods. The objective of this review article is to present, in a self-contained manner, recently developed Sinkhorn/Gibbs-type semigroup analysis based upon contraction coefficients and Lyapunov-type operator-theoretic techniques. These powerful, off-the-shelf semigroup methods are based upon transportation cost inequalities (e.g. log-Sobolev, Talagrand quadratic inequality, curvature estimates), $φ$-divergences, Kantorovich-type criteria and Dobrushin contraction-type coefficients on weighted Banach spaces as well as Wasserstein distances. This novel semigroup analysis allows one to unify and simplify many arguments in the stability of Sinkhorn algorithm. It also yields new contraction estimates w.r.t. generalized $φ$-entropies, as well as weighted total variation norms, Kantorovich criteria and Wasserstein distances.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [1059] [Stability and Accuracy Trade-offs in Statistical Estimation](https://arxiv.org/abs/2601.11701)
*Abhinav Chakraborty,Yuetian Luo,Rina Foygel Barber*

Main category: math.ST

TL;DR: This paper studies how stability constraints impact estimation and outlines trade-offs between stability and accuracy in learning algorithms.


<details>
  <summary>Details</summary>
Motivation: To understand the statistical cost of stability in machine learning algorithms, especially how stability constraints impact generalization, robustness, and accuracy.

Method: Adopts a statistical decision-theoretic framework to analyze stability as a constraint, studies two stability notions (worst-case and average-case), and develops optimal stable estimators for various estimation problems.

Result: Determined lower bounds on estimation accuracy under stability constraints, designed optimal estimators for mean estimation and regression, and quantified trade-offs between worst-case and average-case stability.

Conclusion: Stability constraints entail trade-offs with statistical accuracy. Average-case stability is less restrictive than worst-case stability, with variability across estimation problems.

Abstract: Algorithmic stability is a central concept in statistics and learning theory that measures how sensitive an algorithm's output is to small changes in the training data. Stability plays a crucial role in understanding generalization, robustness, and replicability, and a variety of stability notions have been proposed in different learning settings. However, while stability entails desirable properties, it is typically not sufficient on its own for statistical learning -- and indeed, it may be at odds with accuracy, since an algorithm that always outputs a constant function is perfectly stable but statistically meaningless. Thus, it is essential to understand the potential statistical cost of stability. In this work, we address this question by adopting a statistical decision-theoretic perspective, treating stability as a constraint in estimation. Focusing on two representative notions-worst-case stability and average-case stability-we first establish general lower bounds on the achievable estimation accuracy under each type of stability constraint. We then develop optimal stable estimators for four canonical estimation problems, including several mean estimation and regression settings. Together, these results characterize the optimal trade-offs between stability and accuracy across these tasks. Our findings formalize the intuition that average-case stability imposes a qualitatively weaker restriction than worst-case stability, and they further reveal that the gap between these two can vary substantially across different estimation problems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [1060] [Pigment Network Detection and Classification in Dermoscopic Images Using Directional Imaging Algorithms and Convolutional Neural Networks](https://arxiv.org/abs/2601.11674)
*M. A. Rasel,Sameem Abdul Kareem,Unaizah Obaidellah*

Main category: eess.IV

TL;DR: The study proposes a machine learning-based approach to automate pigment network (PN) analysis for melanoma detection. The method demonstrated high accuracy using CNN for classification.


<details>
  <summary>Details</summary>
Motivation: Melanoma early diagnosis is crucial to saving lives, requiring effective analysis of dermoscopic images, especially atypical pigment networks, which are challenging to identify manually.

Method: The study involves a directional imaging algorithm for PN detection, incorporating PCA and preprocessing techniques, followed by a CNN model with two convolutional layers for classification into typical and atypical PN.

Result: The imaging algorithm achieved near perfect success (96%-100%) in PN detection, and the CNN classifier displayed 90% accuracy, 90% sensitivity, and 89% specificity, outperforming state-of-the-art methods.

Conclusion: The proposed CNN effectively classifies PNs, with potential for improving melanoma diagnosis. Future work should focus on larger datasets and more feature integration.

Abstract: Early diagnosis of melanoma, which can save thousands of lives, relies heavily on the analysis of dermoscopic images. One crucial diagnostic criterion is the identification of unusual pigment network (PN). However, distinguishing between regular (typical) and irregular (atypical) PN is challenging. This study aims to automate the PN detection process using a directional imaging algorithm and classify PN types using machine learning classifiers. The directional imaging algorithm incorporates Principal Component Analysis (PCA), contrast enhancement, filtering, and noise reduction. Applied to the PH2 dataset, this algorithm achieved a 96% success rate, which increased to 100% after pixel intensity adjustments. We created a new dataset containing only PN images from these results. We then employed two classifiers, Convolutional Neural Network (CNN) and Bag of Features (BoF), to categorize PN into atypical and typical classes. Given the limited dataset of 200 images, a simple and effective CNN was designed, featuring two convolutional layers and two batch normalization layers. The proposed CNN achieved 90% accuracy, 90% sensitivity, and 89% specificity. When compared to state-of-the-art methods, our CNN demonstrated superior performance. Our study highlights the potential of the proposed CNN model for effective PN classification, suggesting future research should focus on expanding datasets and incorporating additional dermatological features to further enhance melanoma diagnosis.

</details>


### [1061] [Mobile-friendly Image de-noising: Hardware Conscious Optimization for Edge Application](https://arxiv.org/abs/2601.11684)
*Srinivas Miriyala,Sowmya Vajrala,Hitesh Kumar,Sravanth Kodavanti,Vikram Rajendiran*

Main category: eess.IV

TL;DR: This paper proposes a novel mobile-friendly image de-noising network using an entropy-regularized neural architecture search for U-Net, achieving efficient deployment on edge devices.


<details>
  <summary>Details</summary>
Motivation: Image enhancement tasks are hindered by noise and traditional methods fail compared to deep learning, necessitating efficient deployment for edge devices like smartphones.

Method: The paper employs entropy-regularized neural architecture search (NAS) on a hardware-aware, U-Net-based search space to develop a lightweight de-noising network suitable for smartphones.

Result: The model has 12% fewer parameters, ~2x reduced latency, and 1.5x lower memory footprint compared to other methods, with only a minor PSNR drop. It outperformed the SOTA Swin-Transformer in efficiency by reducing GMACs 18-fold.

Conclusion: The proposed method effectively balances accuracy, memory and latency, making it suitable for mobile devices and demonstrating competitive results on benchmarks and real-world de-noising tasks.

Abstract: Image enhancement is a critical task in computer vision and photography that is often entangled with noise. This renders the traditional Image Signal Processing (ISP) ineffective compared to the advances in deep learning. However, the success of such methods is increasingly associated with the ease of their deployment on edge devices, such as smartphones. This work presents a novel mobile-friendly network for image de-noising obtained with Entropy-Regularized differentiable Neural Architecture Search (NAS) on a hardware-aware search space for a U-Net architecture, which is first-of-its-kind. The designed model has 12% less parameters, with ~2-fold improvement in ondevice latency and 1.5-fold improvement in the memory footprint for a 0.7% drop in PSNR, when deployed and profiled on Samsung Galaxy S24 Ultra. Compared to the SOTA Swin-Transformer for Image Restoration, the proposed network had competitive accuracy with ~18-fold reduction in GMACs. Further, the network was tested successfully for Gaussian de-noising with 3 intensities on 4 benchmarks and real-world de-noising on 1 benchmark demonstrating its generalization ability.

</details>


### [1062] [Towards Efficient Image Deblurring for Edge Deployment](https://arxiv.org/abs/2601.11685)
*Srinivas Miriyala,Sowmya Vajrala,Sravanth Kodavanti*

Main category: eess.IV

TL;DR: The paper proposes a hardware-aware adaptation framework to optimize image deblurring models for edge devices, achieving improved efficiency and latency without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: Image deblurring is essential in mobile imaging pipelines but requires balancing structure restoration with real-time constraints on edge devices. Current SOTA models perform well in accuracy but lack efficient latency measures for embedded hardware.

Method: The framework restructures models using sensitivity-guided block substitution, surrogate distillation, and training-free multi-objective search driven by device profiling. It adapts 36-block NAFNet for better efficiency.

Result: Optimized models reduce GMACs by up to 55% while maintaining competitive accuracy and demonstrate 1.25X latency improvement in hardware deployment. Experiments across various benchmarks confirm the approach's generality.

Conclusion: Feedback-driven hardware-aware adaptation is an effective strategy for optimizing deblurring models, ensuring deployment-ready performance while maintaining a strong accuracy-efficiency trade-off.

Abstract: Image deblurring is a critical stage in mobile image signal processing pipelines, where the ability to restore fine structures and textures must be balanced with real-time constraints on edge devices. While recent deep networks such as transformers and activation-free architectures achieve state-of-the-art (SOTA) accuracy, their efficiency is typically measured in FLOPs or parameters, which do not correlate with latency on embedded hardware. We propose a hardware-aware adaptation framework that restructures existing models through sensitivity-guided block substitution, surrogate distillation, and training-free multi-objective search driven by device profiling. Applied to the 36-block NAFNet baseline, the optimized variants achieve up to 55% reduction in GMACs compared to the recent transformer-based SOTA while maintaining competitive accuracy. Most importantly, on-device deployment yields a 1.25X latency improvement over the baseline. Experiments on motion deblurring (GoPro), defocus deblurring (DPDD), and auxiliary benchmarks (RealBlur-J/R, HIDE) demonstrate the generality of the approach, while comparisons with prior efficient baselines confirm its accuracy-efficiency trade-off. These results establish feedback-driven adaptation as a principled strategy for bridging the gap between algorithmic design and deployment-ready deblurring models.

</details>


### [1063] [Explainable histomorphology-based survival prediction of glioblastoma, IDH-wildtype](https://arxiv.org/abs/2601.11691)
*Jan-Philipp Redlich,Friedrich Feuerhake,Stefan Nikolin,Nadine Sarah Schaadt,Sarah Teuber-Hanselmann,Joachim Weis,Sabine Luttmann,Andrea Eberle,Christoph Buck,Timm Intemann,Pascal Birnstill,Klaus Kraywinkel,Jonas Ort,Peter Boor,André Homeyer*

Main category: eess.IV

TL;DR: The paper introduces an AI-based method to analyze histological images of Glioblastoma, IDH-wildtype tissues, aiming to predict survival associated with visual patterns.


<details>
  <summary>Details</summary>
Motivation: Glioblastoma, IDH-wildtype, is a common and aggressive brain tumor. Existing histomorphology analysis may benefit from additional prognostic data extracted using AI methods.

Method: The study employs a multiple instance learning (MIL) architecture combined with a sparse autoencoder (SAE) to identify prognosis-relevant visual patterns from histological images.

Result: The method predicted survival categories (<180 days or >360 days) with an AUC of 0.67. Cox regression identified survival differences post-adjustment with a hazard ratio of 1.47.

Conclusion: The AI-based approach can identify interpretable histomorphological patterns associated with survival, revealing insights into tissue features such as necrosis, hemorrhage, and cellularity.

Abstract: Glioblastoma, IDH-wildtype (GBM-IDHwt) is the most common malignant brain tumor. Histomorphology is a crucial component of the integrated diagnosis of GBM-IDHwt. Artificial intelligence (AI) methods have shown promise to extract additional prognostic information from histological whole-slide images (WSI) of hematoxylin and eosin-stained glioblastoma tissue. Here, we present an explainable AI-based method to support systematic interpretation of histomorphological features associated with survival. It combines an explainable multiple instance learning (MIL) architecture with a sparse autoencoder (SAE) to relate human-interpretable visual patterns of tissue to survival. The MIL architecture directly identifies prognosis-relevant image tiles and the SAE maps these tiles post-hoc to visual patterns. The MIL method was trained and evaluated using a new real-world dataset that comprised 720 GBM-IDHwt cases from three hospitals and four cancer registries in Germany. The SAE was trained using 1878 WSIs of glioblastoma from five independent public data collections. Despite the many factors influencing survival time, our method showed some ability to discriminate between patients living less than 180 days or more than 360 days solely based on histomorphology (AUC: 0.67; 95% CI: 0.63-0.72). Cox proportional hazards regression confirmed a significant difference in survival time between the predicted groups after adjustment for established prognostic factors (hazard ratio: 1.47; 95% CI: 1.26-1.72). Our method identified multiple interpretable visual patterns associated with survival. Three neuropathologists separately found that 21 of the 24 most strongly associated patterns could be clearly attributed to seven histomorphological categories. Necrosis and hemorrhage appeared to be associated with shorter survival while highly cellular tumor areas were associated with longer survival.

</details>


### [1064] [Anisotropic Tensor Deconvolution of Hyperspectral Images](https://arxiv.org/abs/2601.11694)
*Xinjue Wang,Xiuheng Wang,Esa Ollila,Sergiy A. Vorobyov*

Main category: eess.IV

TL;DR: This paper addresses hyperspectral image deconvolution using a parameter-efficient framework based on Canonical Polyadic Decomposition (CPD), along with anisotropic Total Variation regularization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the challenging hyperspectral image deconvolution problem caused by high dimensionality, reducing computational complexity and preserving data structure.

Method: A low-rank CPD model is used to reduce the parameter space significantly; structure-aware, anisotropic TV regularization is applied, and a PALM algorithm solves the optimization problem.

Result: The proposed framework achieves a parameter reduction of more than two orders of magnitude while maintaining good reconstruction accuracy.

Conclusion: The approach offers an effective trade-off between model compactness and the quality of reconstructed hyperspectral images, demonstrating its potential value in the field.

Abstract: Hyperspectral image (HSI) deconvolution is a challenging ill-posed inverse problem, made difficult by the data's high dimensionality.We propose a parameter-parsimonious framework based on a low-rank Canonical Polyadic Decomposition (CPD) of the entire latent HSI $\mathbf{\mathcal{X}} \in \mathbb{R}^{P\times Q \times N}$.This approach recasts the problem from recovering a large-scale image with $PQN$ variables to estimating the CPD factors with $(P+Q+N)R$ variables.This model also enables a structure-aware, anisotropic Total Variation (TV) regularization applied only to the spatial factors, preserving the smooth spectral signatures.An efficient algorithm based on the Proximal Alternating Linearized Minimization (PALM) framework is developed to solve the resulting non-convex optimization problem.Experiments confirm the model's efficiency, showing a numerous parameter reduction of over two orders of magnitude and a compelling trade-off between model compactness and reconstruction accuracy.

</details>


### [1065] [FourierPET: Deep Fourier-based Unrolled Network for Low-count PET Reconstruction](https://arxiv.org/abs/2601.11680)
*Zheng Zhang,Hao Tang,Yingying Hu,Zhanli Hu,Jing Qin*

Main category: eess.IV

TL;DR: The paper introduces FourierPET, a reconstruction framework for low-count PET imaging, improving sensitivity to specific degradations via Fourier domain processing.


<details>
  <summary>Details</summary>
Motivation: Current PET reconstruction methods struggle with overlapping artifacts and inefficiencies due to their focus on the spatial domain without targeted correction.

Method: The authors utilize Fourier-domain analysis to guide an unrolled Alternating Direction Method of Multipliers framework, integrating specialized modules for frequency-specific corrections.

Result: FourierPET outperforms existing methods, achieving state-of-the-art results with fewer parameters and superior interpretability.

Conclusion: The study demonstrates the effectiveness of Fourier-based correction in PET reconstruction, resolving artifact issues and enhancing overall diagnostic accuracy.

Abstract: Low-count positron emission tomography (PET) reconstruction is a challenging inverse problem due to severe degradations arising from Poisson noise, photon scarcity, and attenuation correction errors. Existing deep learning methods typically address these in the spatial domain with an undifferentiated optimization objective, making it difficult to disentangle overlapping artifacts and limiting correction effectiveness. In this work, we perform a Fourier-domain analysis and reveal that these degradations are spectrally separable: Poisson noise and photon scarcity cause high-frequency phase perturbations, while attenuation errors suppress low-frequency amplitude components. Leveraging this insight, we propose FourierPET, a Fourier-based unrolled reconstruction framework grounded in the Alternating Direction Method of Multipliers. It consists of three tailored modules: a spectral consistency module that enforces global frequency alignment to maintain data fidelity, an amplitude-phase correction module that decouples and compensates for high-frequency phase distortions and low-frequency amplitude suppression, and a dual adjustment module that accelerates convergence during iterative reconstruction. Extensive experiments demonstrate that FourierPET achieves state-of-the-art performance with significantly fewer parameters, while offering enhanced interpretability through frequency-aware correction.

</details>


### [1066] [Bridging Modalities: Joint Synthesis and Registration Framework for Aligning Diffusion MRI with T1-Weighted Images](https://arxiv.org/abs/2601.11689)
*Xiaofan Wang,Junyi Wang,Yuqian Chen,Lauren J. O' Donnell,Fan Zhang*

Main category: eess.IV

TL;DR: This paper introduces an unsupervised multimodal registration framework to improve dMRI and T1-weighted MRI alignment by transforming it into an easier unimodal registration task, leveraging a generative network for image synthesis and deformation.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of accurately registering diffusion MRI images to high-resolution T1-weighted MRI images, hindered by significant intensity differences in traditional methods.

Method: The framework utilizes an unsupervised generative registration network that synthesizes T1w-like contrast images from b0 images and optimizes local structure similarity and cross-modal dependency for deformation field estimation.

Result: Experiments on two datasets showed superior multimodal registration performance compared to existing state-of-the-art methods.

Conclusion: Transforming the multimodal registration task into a simpler unimodal one via generative synthesis improves alignment accuracy, showcasing the effectiveness of the proposed framework.

Abstract: Multimodal image registration between diffusion MRI (dMRI) and T1-weighted (T1w) MRI images is a critical step for aligning diffusion-weighted imaging (DWI) data with structural anatomical space. Traditional registration methods often struggle to ensure accuracy due to the large intensity differences between diffusion data and high-resolution anatomical structures. This paper proposes an unsupervised registration framework based on a generative registration network, which transforms the original multimodal registration problem between b0 and T1w images into a unimodal registration task between a generated image and the real T1w image. This effectively reduces the complexity of cross-modal registration. The framework first employs an image synthesis model to generate images with T1w-like contrast, and then learns a deformation field from the generated image to the fixed T1w image. The registration network jointly optimizes local structural similarity and cross-modal statistical dependency to improve deformation estimation accuracy. Experiments conducted on two independent datasets demonstrate that the proposed method outperforms several state-of-the-art approaches in multimodal registration tasks.

</details>


### [1067] [DeepRAHT: Learning Predictive RAHT for Point Cloud Attribute Compression](https://arxiv.org/abs/2601.12255)
*Chunyang Fu,Tai Qin,Shiqi Wang,Zhu Li*

Main category: eess.IV

TL;DR: The paper introduces DeepRAHT, an end-to-end framework leveraging RAHT with sparse tensor for lossy point cloud attribute compression, improving performance, speed, and robustness over baseline methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research in applying RAHT as a PCAC method in deep learning and enhance its compression performance, especially for lossy scenarios.

Method: The authors propose DeepRAHT, integrating RAHT within the learning reconstruction process without requiring manual preprocessing. Predictive RAHT and learning-based prediction models are introduced, alongside a bitrate proxy utilizing run-length coding for robust and variable-rate coding.

Result: DeepRAHT exhibits enhanced performance in compression, faster execution, and greater robustness compared to traditional baseline methods.

Conclusion: DeepRAHT is an innovative, reversible, distortion-controllable framework with significant application potential, setting a lower bound performance threshold and proving effective in experiments.

Abstract: Regional Adaptive Hierarchical Transform (RAHT) is an effective point cloud attribute compression (PCAC) method. However, its application in deep learning lacks research. In this paper, we propose an end-to-end RAHT framework for lossy PCAC based on the sparse tensor, called DeepRAHT. The RAHT transform is performed within the learning reconstruction process, without requiring manual RAHT for preprocessing. We also introduce the predictive RAHT to reduce bitrates and design a learning-based prediction model to enhance performance. Moreover, we devise a bitrate proxy that applies run-length coding to entropy model, achieving seamless variable-rate coding and improving robustness. DeepRAHT is a reversible and distortion-controllable framework, ensuring its lower bound performance and offering significant application potential. The experiments demonstrate that DeepRAHT is a high-performance, faster, and more robust solution than the baseline methods. Project Page: https://github.com/zb12138/DeepRAHT.

</details>


### [1068] [DALD-PCAC: Density-Adaptive Learning Descriptor for Point Cloud Lossless Attribute Compression](https://arxiv.org/abs/2601.12261)
*Chunyang Fu,Ge Li,Wei Gao,Shiqi Wang,Zhu Li,Shan Liu*

Main category: eess.IV

TL;DR: The paper introduces DALD-PCAC, a lossless attribute compression framework for point clouds, utilizing deep learning techniques like point-wise attention and density-adaptive descriptors to improve compression performance while addressing varying densities and irregularities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance learning-based lossless attribute compression for point clouds, particularly addressing challenges such as varying densities, sparsity, and irregularity.

Method: The paper employs a Transformer-based point-wise attention model, density-adaptive learning descriptors, and prior-guided block partitioning for context modeling and compression enhancement.

Result: DALD-PCAC achieves state-of-the-art performance on LiDAR and object point cloud datasets, demonstrating robustness to density variations, boosted compression efficiency, and a good balance between performance and computational complexity.

Conclusion: The proposed DALD-PCAC model establishes a highly effective lossless point cloud attribute compression method, showing promise for practical applications while maintaining a solid performance-complexity trade-off.

Abstract: Recently, deep learning has significantly advanced the performance of point cloud geometry compression. However, the learning-based lossless attribute compression of point clouds with varying densities is under-explored. In this paper, we develop a learning-based framework, namely DALD-PCAC that leverages Levels of Detail (LoD) to tailor for point cloud lossless attribute compression. We develop a point-wise attention model using a permutation-invariant Transformer to tackle the challenges of sparsity and irregularity of point clouds during context modeling. We also propose a Density-Adaptive Learning Descriptor (DALD) capable of capturing structure and correlations among points across a large range of neighbors. In addition, we develop a prior-guided block partitioning to reduce the attribute variance within blocks and enhance the performance. Experiments on LiDAR and object point clouds show that DALD-PCAC achieves the state-of-the-art performance on most data. Our method boosts the compression performance and is robust to the varying densities of point clouds. Moreover, it guarantees a good trade-off between performance and complexity, exhibiting great potential in real-world applications. The source code is available at https://github.com/zb12138/DALD_PCAC.

</details>


### [1069] [SHARE: A Fully Unsupervised Framework for Single Hyperspectral Image Restoration](https://arxiv.org/abs/2601.13987)
*Jiangwei Xie,Zhang Wen,Mike Davies,Dongdong Chen*

Main category: eess.IV

TL;DR: This paper introduces SHARE, an unsupervised framework for hyperspectral image restoration that uses geometric equivariance and low-rank spectral modeling, eliminating the requirement for ground-truth data.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based solutions for hyperspectral image restoration depend heavily on curated ground-truth datasets, limiting their real-world applicability.

Method: SHARE leverages geometric equivariance and low-rank spectral properties by applying self-supervision signals through equivariance consistency constraints and employs the Dynamic Adaptive Spectral Attention (DASA) module to refine spectral-spatial correlations.

Result: SHARE shows superior performance in hyperspectral image inpainting and super-resolution tasks, outperforming many unsupervised methods and achieving results comparable to supervised techniques.

Conclusion: The proposed SHARE framework advances unsupervised hyperspectral image restoration and highlights its potential for broader scientific imaging applications.

Abstract: Hyperspectral image (HSI) restoration is a fundamental challenge in computational imaging and computer vision. It involves ill-posed inverse problems, such as inpainting and super-resolution. Although deep learning methods have transformed the field through data-driven learning, their effectiveness hinges on access to meticulously curated ground-truth datasets. This fundamentally restricts their applicability in real-world scenarios where such data is unavailable. This paper presents SHARE (Single Hyperspectral Image Restoration with Equivariance), a fully unsupervised framework that unifies geometric equivariance principles with low-rank spectral modelling to eliminate the need for ground truth. SHARE's core concept is to exploit the intrinsic invariance of hyperspectral structures under differentiable geometric transformations (e.g. rotations and scaling) to derive self-supervision signals through equivariance consistency constraints. Our novel Dynamic Adaptive Spectral Attention (DASA) module further enhances this paradigm shift by explicitly encoding the global low-rank property of HSI and adaptively refining local spectral-spatial correlations through learnable attention mechanisms. Extensive experiments on HSI inpainting and super-resolution tasks demonstrate the effectiveness of SHARE. Our method outperforms many state-of-the-art unsupervised approaches and achieves performance comparable to that of supervised methods. We hope that our approach will shed new light on HSI restoration and broader scientific imaging scenarios. The code will be released at https://github.com/xuwayyy/SHARE.

</details>


### [1070] [Pixelwise Uncertainty Quantification of Accelerated MRI Reconstruction](https://arxiv.org/abs/2601.13236)
*Ilias I. Giannakopoulos,Lokesh B Gautham Muthukumar,Yvonne W. Lui,Riccardo Lattanzi*

Main category: eess.IV

TL;DR: This paper presents a framework for quantifying pixel-wise uncertainty in parallel MRI reconstructions, allowing automatic identification of unreliable regions without ground-truth images.


<details>
  <summary>Details</summary>
Motivation: To address the lack of methods for automatically assessing diagnostic quality in parallel MRI reconstructions as acceleration factors increase, without reliance on ground-truth images.

Method: The approach incorporates conformal quantile regression into MRI image reconstruction methods to estimate pixel-wise uncertainty intervals, tested using an end-to-end Variational Network with Cartesian undersampled fastMRI dataset.

Result: Experiments show over 90% Pearson correlation between predicted uncertainties and reconstruction errors at higher acceleration factors, better than simpler heuristic methods like residual magnitude. Uncertainty maps also align with actual artifacts and pathologies.

Conclusion: The framework improves evaluation of MRI reconstruction quality without ground-truth images and could lead to adaptive MRI protocols balancing scan time and diagnostic reliability.

Abstract: Parallel imaging techniques reduce magnetic resonance imaging (MRI) scan time but image quality degrades as the acceleration factor increases. In clinical practice, conservative acceleration factors are chosen because no mechanism exists to automatically assess the diagnostic quality of undersampled reconstructions. This work introduces a general framework for pixel-wise uncertainty quantification in parallel MRI reconstructions, enabling automatic identification of unreliable regions without access to any ground-truth reference image. Our method integrates conformal quantile regression with image reconstruction methods to estimate statistically rigorous pixel-wise uncertainty intervals. We trained and evaluated our model on Cartesian undersampled brain and knee data obtained from the fastMRI dataset using acceleration factors ranging from 2 to 10. An end-to-end Variational Network was used for image reconstruction. Quantitative experiments demonstrate strong agreement between predicted uncertainty maps and true reconstruction error. Using our method, the corresponding Pearson correlation coefficient was higher than 90% at acceleration levels at and above four-fold; whereas it dropped to less than 70% when the uncertainty was computed using a simpler a heuristic notion (magnitude of the residual). Qualitative examples further show the uncertainty maps based on quantile regression capture the magnitude and spatial distribution of reconstruction errors across acceleration factors, with regions of elevated uncertainty aligning with pathologies and artifacts. The proposed framework enables evaluation of reconstruction quality without access to fully-sampled ground-truth reference images. It represents a step toward adaptive MRI acquisition protocols that may be able to dynamically balance scan time and diagnostic reliability.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1071] [Concatenated Matrix SVD: Compression Bounds, Incremental Approximation, and Error-Constrained Clustering](https://arxiv.org/abs/2601.11626)
*Maksym Shamrai*

Main category: math.NA

TL;DR: The paper introduces a theory-driven framework for clustering matrices under SVD compression with explicit error constraints, addressing reconstruction errors and proposing efficient algorithms for clustering.


<details>
  <summary>Details</summary>
Motivation: To resolve the lack of principled guarantees on SVD approximation error in matrix compression, especially when matrices are concatenated.

Method: The approach involves deriving spectral bounds for SVD errors of concatenated matrices, developing per-block guarantees, and implementing three clustering algorithms with explicit error control.

Result: New global and incremental spectral bounds are established, enabling accurate and scalable clustering of matrices under SVD constraints. Efficient estimators and algorithms are introduced.

Conclusion: The proposed framework facilitates principled matrix compression with error guarantees, improving clustering accuracy and scalability under SVD constraints.

Abstract: Large collections of matrices arise throughout modern machine learning, signal processing, and scientific computing, where they are commonly compressed by concatenation followed by truncated singular value decomposition (SVD). This strategy enables parameter sharing and efficient reconstruction and has been widely adopted across domains ranging from multi-view learning and signal processing to neural network compression. However, it leaves a fundamental question unanswered: which matrices can be safely concatenated and compressed together under explicit reconstruction error constraints? Existing approaches rely on heuristic or architecture-specific grouping and provide no principled guarantees on the resulting SVD approximation error. In the present work, we introduce a theory-driven framework for compression-aware clustering of matrices under SVD compression constraints. Our analysis establishes new spectral bounds for horizontally concatenated matrices, deriving global upper bounds on the optimal rank-$r$ SVD reconstruction error from lower bounds on singular value growth. The first bound follows from Weyl-type monotonicity under blockwise extensions, while the second leverages singular values of incremental residuals to yield tighter, per-block guarantees. We further develop an efficient approximate estimator based on incremental truncated SVD that tracks dominant singular values without forming the full concatenated matrix. Therefore, we propose three clustering algorithms that merge matrices only when their predicted joint SVD compression error remains below a user-specified threshold. The algorithms span a trade-off between speed, provable accuracy, and scalability, enabling compression-aware clustering with explicit error control. Code is available online.

</details>


### [1072] [Streaming Operator Inference for Model Reduction of Large-Scale Dynamical Systems](https://arxiv.org/abs/2601.12161)
*Tomoki Koike,Prakash Mohan,Marc T. Henry de Frahan,Julie Bessac,Elizabeth Qian*

Main category: math.NA

TL;DR: The paper introduces 'Streaming OpInf,' a projection-based model reduction method for learning low-dimensional models from sequential data streams, significantly reducing memory requirements and enabling online adaptation.


<details>
  <summary>Details</summary>
Motivation: Traditional Operator Inference (OpInf) struggles with large-scale datasets since it uses batch methods, requiring all data to be loaded simultaneously. This limits its application in large-scale systems and online updates.

Method: Streaming OpInf incorporates incremental Singular Value Decomposition (SVD) for progressively constructing low-dimensional bases and recursive linear least squares for updating reduced operators, working with sequentially arriving data.

Result: Numerical experiments show that Streaming OpInf achieves similar accuracy to batch OpInf, reduces memory usage by over 99%, supports dimension reductions of over 31,000x, and accelerates predictions by orders of magnitude.

Conclusion: Streaming OpInf overcomes the scalability and adaptability limitations of traditional OpInf, offering a more memory-efficient and flexible method ideal for large-scale and online applications in model reduction.

Abstract: Projection-based model reduction enables efficient simulation of complex dynamical systems by constructing low-dimensional surrogate models from high-dimensional data. The Operator Inference (OpInf) approach learns such reduced surrogate models through a two-step process: constructing a low-dimensional basis via Singular Value Decomposition (SVD) to compress the data, then solving a linear least-squares (LS) problem to infer reduced operators that govern the dynamics in this compressed space, all without access to the underlying code or full model operators, i.e., non-intrusively. Traditional OpInf operates as a batch learning method, where both the SVD and LS steps process all data simultaneously. This poses a barrier to deployment of the approach on large-scale applications where dataset sizes prevent the loading of all data into memory at once. Additionally, the traditional batch approach does not naturally allow model updates using new data acquired during online computation. To address these limitations, we propose Streaming OpInf, which learns reduced models from sequentially arriving data streams. Our approach employs incremental SVD for adaptive basis construction and recursive LS for streaming operator updates, eliminating the need to store complete data sets while enabling online model adaptation. The approach can flexibly combine different choices of streaming algorithms for numerical linear algebra: we systematically explore the impact of these choices both analytically and numerically to identify effective combinations for accurate reduced model learning. Numerical experiments on benchmark problems and a large-scale turbulent channel flow demonstrate that Streaming OpInf achieves accuracy comparable to batch OpInf while reducing memory requirements by over 99% and enabling dimension reductions exceeding 31,000x, resulting in orders-of-magnitude faster predictions.

</details>


### [1073] [Data-Consistent Learning of Inverse Problems](https://arxiv.org/abs/2601.12831)
*Markus Haltmeier,Gyeongha Hwang*

Main category: math.NA

TL;DR: The paper introduces DC networks that combine classical regularization methods and learned reconstruction approaches, leveraging both theoretical guarantees and visual quality.


<details>
  <summary>Details</summary>
Motivation: To address the instability and non-uniqueness in inverse problems by combining the stability of classical regularizations and the expressive abilities of learned methods.

Method: The paper proposes using null-space networks alongside classical regularization as an initial reconstruction step to ensure theoretical guarantees and effective learning.

Result: The approach results in reconstructions that are both theoretically reliable and visually appealing.

Conclusion: The combination of classical and learned methods provides a convergent, stable solution with improved reconstruction quality.

Abstract: Inverse problems are inherently ill-posed, suffering from non-uniqueness and instability. Classical regularization methods provide mathematically well-founded solutions, ensuring stability and convergence, but often at the cost of reduced flexibility or visual quality. Learned reconstruction methods, such as convolutional neural networks, can produce visually compelling results, yet they typically lack rigorous theoretical guarantees. DC (DC) networks address this gap by enforcing the measurement model within the network architecture. In particular, null-space networks combined with a classical regularization method as an initial reconstruction define a convergent regularization method. This approach preserves the theoretical reliability of classical schemes while leveraging the expressive power of data-driven learning, yielding reconstructions that are both accurate and visually appealing.

</details>


### [1074] [Deep Neural networks for solving high-dimensional parabolic partial differential equations](https://arxiv.org/abs/2601.13256)
*Wenzhong Zhang,Zhenyuan Hu,Wei Cai,George EM Karniadakis*

Main category: math.NA

TL;DR: The paper reviews neural-network-based methods for solving high-dimensional partial differential equations (PDEs), identifying key paradigms and discussing their application to complex benchmark problems.


<details>
  <summary>Details</summary>
Motivation: Classical grid-based methods fail to solve high-dimensional PDEs due to the curse of dimensionality, necessitating alternative, scalable approaches such as neural networks.

Method: The authors classify approaches into three paradigms: PDE residual-based methods, stochastic methods leveraging Feynman-Kac and backward stochastic differential equations, and hybrid derivative-free methods.

Result: The review demonstrates the methods' effectiveness and scalability through benchmark problems like Hamilton-Jacobi-Bellman and Black-Scholes equations in dimensions up to 1000.

Conclusion: The study highlights the strengths and limits of neural-network-based solvers, emphasizing open challenges and future directions for improved scalability and reliability.

Abstract: The numerical solution of high dimensional partial differential equations (PDEs) is severely constrained by the curse of dimensionality (CoD), rendering classical grid--based methods impractical beyond a few dimensions. In recent years, deep neural networks have emerged as a promising mesh free alternative, enabling the approximation of PDE solutions in tens to thousands of dimensions. This review provides a tutorial--oriented introduction to neural--network--based methods for solving high dimensional parabolic PDEs, emphasizing conceptual clarity and methodological connections. We organize the literature around three unifying paradigms: (i) PDE residual--based approaches, including physicsinformed neural networks and their high dimensional variants; (ii) stochastic methods derived from Feynman--Kac and backward stochastic differential equation formulations; and (iii) hybrid derivative--free random difference approaches designed to alleviate the computational cost of derivatives in high dimensions. For each paradigm, we outline the underlying mathematical formulation, algorithmic implementation, and practical strengths and limitations. Representative benchmark problems--including Hamilton--Jacobi--Bellman and Black--Scholes equations in up to 1000 dimensions --illustrate the scalability, effectiveness, and accuracy of the methods. The paper concludes with a discussion of open challenges and future directions for reliable and scalable solvers of high dimensional PDEs.

</details>


### [1075] [Optimizing Parallel Schemes with Lyapunov Exponents and kNN-LLE Estimation](https://arxiv.org/abs/2601.13604)
*Mudassir Shams,Andrei Velichko,Bruno Carpentieri*

Main category: math.NA

TL;DR: The paper introduces a method for analyzing and reducing instabilities in inverse parallel schemes using Lyapunov-based techniques for more robust root-finding applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the instabilities, including chaotic or periodic behavior, in inverse parallel solvers for nonlinear system root-finding, which hinders their practical effectiveness.

Method: The authors combine theoretical stability/bifurcation analysis with data-driven Lyapunov exponent profiling via kNN estimations on solver trajectories to detect and remedy instabilities in real-time.

Result: The study demonstrates a match between theoretical models and empirical data, showing that Lyapunov-informed parameter selection enhances solver stability and robustness against initial guess perturbations.

Conclusion: The paper proposes micro-series Lyapunov analysis as a reliable tool for developing self-stabilizing root-finding schemes, with potential extensions to complex or noisy systems.

Abstract: Inverse parallel schemes remain indispensable tools for computing the roots of nonlinear systems, yet their dynamical behavior can be unexpectedly rich, ranging from strong contraction to oscillatory or chaotic transients depending on the choice of algorithmic parameters and initial states. A unified analytical-data-driven methodology for identifying, measuring, and reducing such instabilities in a family of uni-parametric inverse parallel solvers is presented in this study. On the theoretical side, we derive stability and bifurcation characterizations of the underlying iterative maps, identifying parameter regions associated with periodic or chaotic behavior. On the computational side, we introduce a micro-series pipeline based on kNN-driven estimation of the local largest Lyapunov exponent (LLE), applied to scalar time series derived from solver trajectories. The resulting sliding-window Lyapunov profiles provide fine-grained, real-time diagnostics of contractive or unstable phases and reveal transient behaviors not captured by coarse linearized analysis. Leveraging this correspondence, we introduce a Lyapunov-informed parameter selection strategy that identifies solver settings associated with stable behavior, particularly when the estimated LLE indicates persistent instability. Comprehensive experiments on ensembles of perturbed initial guesses demonstrate close agreement between the theoretical stability diagrams and empirical Lyapunov profiles, and show that the proposed adaptive mechanism significantly improves robustness. The study establishes micro-series Lyapunov analysis as a practical, interpretable tool for constructing self-stabilizing root-finding schemes and opens avenues for extending such diagnostics to higher-dimensional or noise-contaminated problems.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1076] [Optimal Calibration of the endpoint-corrected Hilbert Transform](https://arxiv.org/abs/2601.13962)
*Eike Osmers,Dorothea Kolossa*

Main category: eess.SP

TL;DR: The paper addresses endpoint distortions in the Hilbert transform (ecHT) for real-time phase estimation and provides analytic solutions to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate phase estimation is vital for real-time applications like neurostimulation, and existing methods like ecHT suffer from endpoint distortions without a proper closed-form analysis.

Method: The paper analytically derives the endpoint operator for ecHT, characterizing distortions and biases, and introduces a calibration method (c-ecHT) to mitigate errors, with design rules to optimize parameters.

Result: The calibrated ecHT (c-ecHT) achieves near-zero mean phase error and is computationally efficient for real-time use.

Conclusion: The study enhances ecHT performance for accurate phase estimation, providing theoretical insights, practical calibration, and design recommendations for real-world applications.

Abstract: Accurate, low-latency estimates of the instantaneous phase of oscillations are essential for closed-loop sensing and actuation, including (but not limited to) phase-locked neurostimulation and other real-time applications. The endpoint-corrected Hilbert transform (ecHT) reduces boundary artefacts of the Hilbert transform by applying a causal narrow-band filter to the analytic spectrum. This improves the phase estimate at the most recent sample. Despite its widespread empirical use, the systematic endpoint distortions of ecHT have lacked a principled, closed-form analysis. In this study, we derive the ecHT endpoint operator analytically and demonstrate that its output can be decomposed into a desired positive-frequency term (a deterministic complex gain that induces a calibratable amplitude/phase bias) and a residual leakage term setting an irreducible variance floor. This yields (i) an explicit characterisation and bounds for endpoint phase/amplitude error, (ii) a mean-squared-error-optimal scalar calibration (c-ecHT), and (iii) practical design rules relating window length, bandwidth/order, and centre-frequency mismatch to residual bias via an endpoint group delay. The resulting calibrated ecHT achieves near-zero mean phase error and remains computationally compatible with real-time pipelines. Code and analyses are provided at https://github.com/eosmers/cecHT.

</details>


### [1077] [Inter-Cell Interference Rejection Based on Ultrawideband Walsh-Domain Wireless Autoencoding](https://arxiv.org/abs/2601.11713)
*Rodney Martinez Alonso,Cel Thys,Cedric Dehos,Yuneisy Esthela Garcia Guzman,Sofie Pollin*

Main category: eess.SP

TL;DR: The paper introduces an autoencoder system to mitigate interference from 5G base stations using Walsh domain encoding, achieving significant interference rejection.


<details>
  <summary>Details</summary>
Motivation: Address the issue of partial in-band interference in ultrawideband systems caused by coexisting 5G narrow-band stations.

Method: An end-to-end wireless autoencoder optimized for the Walsh domain is designed to encode/ decode information while mitigating interference, leveraging Walsh functions' properties.

Result: The autoencoder achieves up to 12 dB interference rejection while maintaining a low block error rate under similar channel noise conditions.

Conclusion: The technique effectively reduces inter-cell interference in ultrawideband systems and is a notable advancement for maintaining communication reliability in shared-spectrum environments.

Abstract: This paper proposes a novel technique for rejecting partial-in-band inter-cell interference (ICI) in ultrawideband communication systems. We present the design of an end-to-end wireless autoencoder architecture that jointly optimizes the transmitter and receiver encoding/decoding in the Walsh domain to mitigate interference from coexisting narrower-band 5G base stations. By exploiting the orthogonality and self-inverse properties of Walsh functions, the system distributes and learns to encode bit-words across parallel Walsh branches. Through analytical modeling and simulation, we characterize how 5G CPOFDM interference maps into the Walsh domain and identify optimal ratios of transmission frequencies and sampling rate where the end-to-end autoencoder achieves the highest rejection. Experimental results show that the proposed autoencoder achieves up to 12 dB of ICI rejection while maintaining a low block error rate (BLER) for the same baseline channel noise, i.e., baseline Signal-to-Noise-Ratio (SNR) without the interference.

</details>


### [1078] [Accelerated MR Elastography Using Learned Neural Network Representation](https://arxiv.org/abs/2601.11878)
*Xi Peng*

Main category: eess.SP

TL;DR: The study introduces a self-supervised deep-learning approach to reconstruct high-resolution MR elastography (MRE) images from undersampled data, achieving better noise and artifact suppression with comparable stiffness estimation compared to fully sampled methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve MR elastography imaging by allowing fast, high-resolution reconstructions from highly undersampled data without needing high-quality training datasets.

Method: A deep neural network was framed as a nonlinear extension of the linear subspace model. Self-supervised learning was employed with a multi-level k-space consistent loss, coupled with phase-contrast specific priors such as anatomical structure similarity and displacement smoothness. Experiments used gradient-echo and spin-echo spiral MRE datasets.

Result: The proposed approach outperformed conventional linear subspace methods in noise/artifact suppression and enabled stiffness estimations equivalent to fully sampled data while only using single in-plane spiral arms (R=10).

Conclusion: This work demonstrated the potential of self-supervised deep-learning methods for effective MRE image reconstruction from undersampled data, offering advancements over linear subspace-based techniques.

Abstract: To develop a deep-learning method for achieving fast high-resolution MR elastography from highly undersampled data without the need of high-quality training dataset. We first framed the deep neural network representation as a nonlinear extension of the linear subspace model, then used it to represent and reconstruct MRE image repetitions from undersampled k-space data. The network weights were learned using a multi-level k-space consistent loss in a self-supervised manner. To further enhance reconstruction quality, phase-contrast specific magnitude and phase priors were incorporated, including the similarity of anatomical structures and smoothness of wave-induced harmonic displacement. Experiments were conducted using both 3D gradient-echo spiral and multi-slice spin-echo spiral MRE datasets. Compared to the conventional linear subspace-based approaches, the nonlinear network representation method was able to produce superior image reconstruction with suppressed noise and artifacts from a single in-plane spiral arm per MRE repetition (e.g., total R=10), yielding comparable stiffness estimation to the fully sampled data. This work demonstrated the feasibility of using deep network representations to model and reconstruct MRE images from highly-undersampled data, a nonlinear extension of the subspace-based approaches.

</details>


### [1079] [Temporal Data and Short-Time Averages Improve Multiphase Mass Flow Metering](https://arxiv.org/abs/2601.12433)
*Amanda Nyholm,Yessica Arellano,Jinyu Liu,Damian Krakowiak,Pierluigi Salvo Rossi*

Main category: eess.SP

TL;DR: This paper explores using machine learning (ML) to accurately measure multiphase flows using Coriolis mass flowmeters while preserving temporal information, and finds CNN-based models outperform existing approaches.


<details>
  <summary>Details</summary>
Motivation: Accurate measurement of multiphase flows is challenging for existing instruments, and there is a demand for improving the accuracy of flowmeters, especially by incorporating machine learning techniques.

Method: The authors compare a multilayer perceptron, a windowed multilayer perceptron, and a convolutional neural network (CNN), leveraging short-time averaged data to retain temporal information from experiments spanning three-phase air-water-oil flow.

Result: The CNN model performed the best with significant improvement in accuracy, showing approximately 95% of relative errors below 13%, normalized root mean squared error of 0.03, and a mean absolute percentage error of around 4.3%.

Conclusion: Preserving temporal information improves multiphase flow measurement accuracy, and CNNs show robust and superior performance compared to alternative models in this experimental setup.

Abstract: Reliable flow measurements are essential in many industries, but current instruments often fail to accurately estimate multiphase flows, which are frequently encountered in real-world operations. Combining machine learning (ML) algorithms with accurate single-phase flowmeters has therefore received extensive research attention in recent years. The Coriolis mass flowmeter is a widely used single-phase meter that provides direct mass flow measurements, which ML models can be trained to correct, thereby reducing measurement errors in multiphase conditions. This paper demonstrates that preserving temporal information significantly improves model performance in such scenarios. We compare a multilayer perceptron, a windowed multilayer perceptron, and a convolutional neural network (CNN) on three-phase air-water-oil flow data from 342 experiments. Whereas prior work typically compresses each experiment into a single averaged sample, we instead compute short-time averages from within each experiment and train models that preserve temporal information at several downsampling intervals. The CNN performed best at 0.25 Hz with approximately 95 % of relative errors below 13 %, a normalized root mean squared error of 0.03, and a mean absolute percentage error of approximately 4.3 %, clearly outperforming the best single-averaged model and demonstrating that short-time averaging within individual experiments is preferable. Results are consistent across multiple data splits and random seeds, demonstrating robustness.

</details>


### [1080] [Energy-Efficient Prediction in Textile Manufacturing: Enhancing Accuracy and Data Efficiency With Ensemble Deep Transfer Learning](https://arxiv.org/abs/2601.12663)
*Yan-Chen Chen,Wei-Yu Chiu,Qun-Yu Wang,Jing-Wei Chen,Hao-Ting Zhao*

Main category: eess.SP

TL;DR: The paper introduces a framework called Ensemble Deep Transfer Learning (EDTL) to optimize energy efficiency in textile manufacturing, demonstrating improved accuracy and reduced data dependency.


<details>
  <summary>Details</summary>
Motivation: Traditional textile production is energy-intensive, and optimizing production processes for energy and cost efficiency is vital. However, applying deep neural networks for these purposes is constrained by the need for extensive data, which is expensive to gather.

Method: The proposed EDTL framework combines transfer learning with an ensemble strategy and a feature alignment layer. It pretrains models on data-rich sources and adapts them to data-limited scenarios, reducing the need for large datasets.

Result: EDTL achieves a 5.66% improvement in prediction accuracy and a 3.96% increase in robustness when compared to traditional DNN methods—especially in situations with only 20%-40% data availability.

Conclusion: This paper offers an innovative, scalable approach for data-efficient prediction, promoting energy-efficient and cost-effective textile manufacturing for modern production systems.

Abstract: Traditional textile factories consume substantial energy, making energy-efficient production optimization crucial for sustainability and cost reduction. Meanwhile, deep neural networks (DNNs), which are effective for factory output prediction and operational optimization, require extensive historical data, posing challenges due to high sensor deployment and data collection costs. To address this, we propose Ensemble Deep Transfer Learning (EDTL), a novel framework that enhances prediction accuracy and data efficiency by integrating transfer learning with an ensemble strategy and a feature alignment layer. EDTL pretrains DNN models on data-rich production lines (source domain) and adapts them to data-limited lines (target domain), reducing dependency on large datasets. Experiments on real-world textile factory datasets show that EDTL improves prediction accuracy by 5.66% and enhances model robustness by 3.96% compared to conventional DNNs, particularly in data-limited scenarios (20%-40% data availability). This research contributes to energy-efficient textile manufacturing by enabling accurate predictions with fewer data requirements, providing a scalable and cost-effective solution for smart production systems.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1081] [Proc3D: Procedural 3D Generation and Parametric Editing of 3D Shapes with Large Language Models](https://arxiv.org/abs/2601.12234)
*Fadlullah Raji,Stefano Petrangeli,Matheus Gadelha,Yu Shen,Uttaran Bhattacharya,Gang Wu*

Main category: cs.GR

TL;DR: Proc3D introduces an editable 3D model generation system employing a new graph representation called procedural compact graph (PCG), enabling real-time and intuitive edits via natural language and manual controls.


<details>
  <summary>Details</summary>
Motivation: Existing generative AI methods for 3D modeling produce non-editable designs, limiting iterative design processes and adaptability.

Method: Proc3D employs procedural compact graph (PCG) for representing 3D models, exposing parameters for real-time editing using manual sliders, checkboxes, and natural language via LLMs such as GPT-4o and LLAMA-3.

Result: Proc3D achieves over 400x editing speed improvements compared to traditional methods and enhances text-model alignment by improving ULIP scores by 28%.

Conclusion: Proc3D provides a groundbreaking solution for editable, real-time, and accurate 3D modeling, bridging the gap between automation and customization in iterative design processes.

Abstract: Generating 3D models has traditionally been a complex task requiring specialized expertise. While recent advances in generative AI have sought to automate this process, existing methods produce non-editable representation, such as meshes or point clouds, limiting their adaptability for iterative design. In this paper, we introduce Proc3D, a system designed to generate editable 3D models while enabling real-time modifications. At its core, Proc3D introduces procedural compact graph (PCG), a graph representation of 3D models, that encodes the algorithmic rules and structures necessary for generating the model. This representation exposes key parameters, allowing intuitive manual adjustments via sliders and checkboxes, as well as real-time, automated modifications through natural language prompts using Large Language Models (LLMs). We demonstrate Proc3D's capabilities using two generative approaches: GPT-4o with in-context learning (ICL) and a fine-tuned LLAMA-3 model. Experimental results show that Proc3D outperforms existing methods in editing efficiency, achieving more than 400x speedup over conventional approaches that require full regeneration for each modification. Additionally, Proc3D improves ULIP scores by 28%, a metric that evaluates the alignment between generated 3D models and text prompts. By enabling text-aligned 3D model generation along with precise, real-time parametric edits, Proc3D facilitates highly accurate text-based image editing applications.

</details>


### [1082] [Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints](https://arxiv.org/abs/2601.14207)
*Rotem Gatenyo,Ohad Fried*

Main category: cs.GR

TL;DR: The paper introduces a method for zero-shot 3D alignment of meshes using text prompts that describe spatial relations, offering improved semantic and physical plausibility compared to previous approaches.


<details>
  <summary>Details</summary>
Motivation: The aim is to address the challenge of aligning 3D meshes based on text prompts for tasks like content creation and scene assembly, where previous methods fall short in combining semantics and physical plausibility.

Method: The proposed framework directly optimizes the relative pose during test time using CLIP-driven gradients and a differentiable renderer. It incorporates additional geometry-aware objectives such as soft-ICP and penetration loss, along with a phased optimization schedule and camera control.

Result: The method is evaluated against curated benchmarks, demonstrating superior semantic fidelity and physical plausibility compared to existing baselines.

Conclusion: This work presents a novel approach to text-driven 3D alignment that integrates language and geometric considerations, outperforming prior solutions and advancing capabilities in scene assembly.

Abstract: We study zero-shot 3D alignment of two given meshes, using a text prompt describing their spatial relation -- an essential capability for content creation and scene assembly. Earlier approaches primarily rely on geometric alignment procedures, while recent work leverages pretrained 2D diffusion models to model language-conditioned object-object spatial relationships. In contrast, we directly optimize the relative pose at test time, updating translation, rotation, and isotropic scale with CLIP-driven gradients via a differentiable renderer, without training a new model. Our framework augments language supervision with geometry-aware objectives: a variant of soft-Iterative Closest Point (ICP) term to encourage surface attachment and a penetration loss to discourage interpenetration. A phased schedule strengthens contact constraints over time, and camera control concentrates the optimization on the interaction region. To enable evaluation, we curate a benchmark containing diverse categories and relations, and compare against baselines. Our method outperforms all alternatives, yielding semantically faithful and physically plausible alignments.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1083] [Overview of the SciHigh Track at FIRE 2025: Research Highlight Generation from Scientific Papers](https://arxiv.org/abs/2601.11582)
*Tohida Rehman,Debarshi Kumar Sanyal,Samiran Chattopadhyay*

Main category: cs.CY

TL;DR: This paper introduces 'SciHigh,' an initiative for automatic generation of concise highlights from scientific abstracts using computational models and evaluates its effectiveness with various metrics.


<details>
  <summary>Details</summary>
Motivation: To ease understanding of essential ideas from scientific papers quickly, especially on mobile devices, by creating concise bullet-point highlights.

Method: Utilizes the MixSub dataset to develop highlight generation models, employing pre-trained language models and evaluating with metrics like ROUGE, METEOR, and BERTScore.

Result: 12 teams participated, showing that automatically generated highlights help reduce reading effort, support literature reviews, and improve metadata for digital libraries.

Conclusion: SciHigh establishes a benchmark for improving concise and precise highlight generation from scientific texts, which could accelerate research comprehension and utility in digital academic platforms.

Abstract: `SciHigh: Research Highlight Generation from Scientific Papers' focuses on the task of automatically generating concise, informative, and meaningful bullet-point highlights directly from scientific abstracts. The goal of this task is to evaluate how effectively computational models can generate highlights that capture the key contributions, findings, and novelty of a paper in a concise form. Highlights help readers grasp essential ideas quickly and are often easier to read and understand than longer paragraphs, especially on mobile devices. The track uses the MixSub dataset \cite{10172215}, which provides pairs of abstracts and corresponding author-written highlights.
  In this inaugural edition of the track, 12 teams participated, exploring various approaches, including pre-trained language models, to generate highlights from this scientific dataset. All submissions were evaluated using established metrics such as ROUGE, METEOR, and BERTScore to measure both alignment with author-written highlights and overall informativeness. Teams were ranked based on ROUGE-L scores. The findings suggest that automatically generated highlights can reduce reading effort, accelerate literature reviews, and enhance metadata for digital libraries and academic search platforms. SciHigh provides a dedicated benchmark for advancing methods aimed at concise and accurate highlight generation from scientific writing.

</details>


### [1084] [Bit-politeia: An AI Agent Community in Blockchain](https://arxiv.org/abs/2601.11583)
*Xing Yang*

Main category: cs.CY

TL;DR: The paper proposes Bit-politeia, an AI and blockchain-based system to ensure fair and effective resource allocation in academic evaluation.


<details>
  <summary>Details</summary>
Motivation: Current systems for resource allocation in academia face issues like bias (Matthew Effect), manipulation (Goodhart’s Law), and inefficiency due to balancing fairness and performance.

Method: The study introduces Bit-politeia, a decentralized AI agent community using blockchain to fairly evaluate research outputs with mechanisms like hierarchical architecture, incentive-driven AI agents, and immutable transaction records.

Result: Bit-politeia successfully minimizes human bias and decentralizes resource distribution using AI for objective assessments and blockchain for secure verification.

Conclusion: The framework enhances scientific innovation by providing an impartial, efficient, and sustainable system for academic resource allocation using AI and blockchain technologies.

Abstract: Current resource allocation paradigms, particularly in academic evaluation, are constrained by inherent limitations such as the Matthew Effect, reward hacking driven by Goodhart's Law, and the trade-off between efficiency and fairness. To address these challenges, this paper proposes "Bit-politeia", an AI agent community on blockchain designed to construct a fair, efficient, and sustainable resource allocation system. In this virtual community, residents interact via AI agents serving as their exclusive proxies, which are optimized for impartiality and value alignment. The community adopts a "clustered grouping + hierarchical architecture" that integrates democratic centralism to balance decision-making efficiency and trust mechanisms. Agents engage through casual chat and deliberative interactions to evaluate research outputs and distribute a virtual currency as rewards. This incentive mechanism aims to achieve incentive compatibility through consensus-driven evaluation, while blockchain technology ensures immutable records of all transactions and reputation data. By leveraging AI for objective assessment and decentralized verification, Bit-politeia minimizes human bias and mitigates resource centralization issues found in traditional peer review. The proposed framework provides a novel pathway for optimizing scientific innovation through a fair and automated resource configuration process.

</details>


### [1085] [Let Me Try Again: Examining Replay Behavior by Tracing Students' Latent Problem-Solving Pathways](https://arxiv.org/abs/2601.11586)
*Shan Zhang,Siddhartha Pradhan,Ji-Eun Lee,Ashish Gurung,Anthony F. Botelho*

Main category: cs.CY

TL;DR: The study explores the influence of problem-solving and replay behaviors in a game-based learning environment, linking them to student learning outcomes using Markov models.


<details>
  <summary>Details</summary>
Motivation: To investigate how problem-solving pathways, particularly replay behaviors, unfold sequentially and impact both short-term and long-term learning outcomes.

Method: Used Markov Chains and Hidden Markov Models (HMMs) to analyze log data of 777 seventh graders playing a game-based learning platform called 'From Here to There!'.

Result: Students followed distinct strategic pathways, with four latent states identified: Incomplete-dominant, Optimal-ending, Replay, and Mixed. Replay behaviors (immediate vs delayed) showed different impacts on learning outcomes like conceptual knowledge and flexibility.

Conclusion: Replay timing matters, as immediate replays are associated with greater learning benefits, encouraging flexible and productive exploration compared to delayed or non-replay behaviors.

Abstract: Prior research has shown that students' problem-solving pathways in game-based learning environments reflect their conceptual understanding, procedural knowledge, and flexibility. Replay behaviors, in particular, may indicate productive struggle or broader exploration, which in turn foster deeper learning. However, little is known about how these pathways unfold sequentially across problems or how the timing of replays and other problem-solving strategies relates to proximal and distal learning outcomes. This study addresses these gaps using Markov Chains and Hidden Markov Models (HMMs) on log data from 777 seventh graders playing the game-based learning platform of From Here to There!. Results show that within problem sequences, students often persisted in states or engaged in immediate replay after successful completions, while across problems, strong self-transitions indicated stable strategic pathways. Four latent states emerged from HMMs: Incomplete-dominant, Optimal-ending, Replay, and Mixed. Regression analyses revealed that engagement in replay-dominant and optimal-ending states predicted higher conceptual knowledge, flexibility, and performance compared with the Incomplete-dominant state. Immediate replay consistently supported learning outcomes, whereas delayed replay was weakly or negatively associated in relation to Non-Replay. These findings suggest that replay in digital learning is not uniformly beneficial but depends on timing, with immediate replay supporting flexibility and more productive exploration.

</details>


### [1086] [Toward Youth-Centered Privacy-by-Design in Smart Devices: A Systematic Review](https://arxiv.org/abs/2601.11598)
*Molly Campbell,Mohamad Sheikho Al Jasem,Ajay Kumar Shrestha*

Main category: cs.CY

TL;DR: This paper reviews privacy-by-design solutions for protecting youth in AI-enabled devices, identifying gaps in technical, policy, and educational areas, and recommending collaborative, inclusive privacy ecosystems.


<details>
  <summary>Details</summary>
Motivation: To evaluate existing privacy-by-design frameworks and strategies specifically to protect youth within AI-enabled smart devices and address the challenges in their implementation.

Method: A PRISMA-guided workflow was used to screen academic and grey literature over the past decade, organizing 122 analyzed records into three thematic categories: technical solutions, policy/regulatory measures, and education/awareness strategies.

Result: Technical solutions like federated learning and lightweight encryption reduce data exposure but face low adoption; policy frameworks like GDPR and Age-Appropriate Design Code are hindered by enforcement and design gaps; educational strategies are rarely integrated into curricula.

Conclusion: The study highlights the need for a multi-stakeholder approach involving policymakers, manufacturers, and educators to create ethical AI systems for youth, emphasizing technical, policy, and educational integration.

Abstract: This literature review evaluates privacy-by-design frameworks, tools, and policies intended to protect youth in AI-enabled smart devices using a PRISMA-guided workflow. Sources from major academic and grey-literature repositories from the past decade were screened. The search identified 2,216 records; after deduplication and screening, 645 articles underwent eligibility assessment, and 122 were included for analysis. The corpus was organized along three thematic categories: technical solutions, policy/regulatory measures, and education/awareness strategies. Findings reveal that while technical interventions such as on-device processing, federated learning, and lightweight encryption significantly reduce data exposure, their adoption remains limited. Policy frameworks, including the EU's GDPR, the UK Age-Appropriate Design Code, and Canada's PIPEDA, provide important baselines but are hindered by gaps in enforcement and age-appropriate design obligations, while educational initiatives are rarely integrated systematically into curricula. Overall, the corpus skews toward technical solutions (67%) relative to policy (21%) and education (12%), indicating an implementation gap outside the technical domain. To address these challenges, we recommend a multi-stakeholder model in which policymakers, manufacturers, and educators co-develop inclusive, transparent, and context-sensitive privacy ecosystems. This work advances discourse on youth data protection by offering empirically grounded insights and actionable recommendations for the design of ethical, privacy-preserving AI systems tailored to young users.

</details>


### [1087] [Syllabic Agglutinative Tokenizations for Indonesian LLM: A Study from Gasing Literacy Learning System](https://arxiv.org/abs/2601.11643)
*H. Situngkir,A. B. Lumbantobing,Y. Surya*

Main category: cs.CY

TL;DR: This paper introduces a syllable-based tokenization method for Indonesian LLMs that outperforms conventional approaches in efficiency and linguistic alignment.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in tokenizing Indonesian text while respecting its agglutinative morphology, providing linguistic and computational efficiency.

Method: Propose a syllable-based segmentation strategy, applying rule-based syllable identification and byte-pair encoding for optimal tokenization.

Result: Improves Rényi efficiency (0.74 vs 0.50-0.64) and token length (3.67 vs 2.72 characters for GPT-2) using a smaller vocabulary.

Conclusion: TOBA LLM showcases the benefits of integrating linguistic principles and computational methods, creating efficient tokenization for morphologically rich languages.

Abstract: This paper presents a novel syllable-based tokenization approach for Indonesian large language models, inspired by the Gasing Literacy Learning System's pedagogical methodology. Drawing on information-theoretic principles, we develop a tokenization framework that segments Indonesian text at syllable boundaries before applying byte-pair encoding, creating a vocabulary that aligns with the language's morphophonological structure. Our approach first identifies high-frequency syllables through rule-based segmentation, then constructs a compact vocabulary of 3,500 tokens that preserves meaningful linguistic units while maintaining coverage through character-level fallback. Empirical evaluation on Indonesian Wikipedia and folklore corpora from Indonesian Culture Digital Library (PDBI) demonstrates substantial improvements over conventional tokenization methods: the syllable-based approach achieves Rényi efficiency of 0.74 compared to 0.50-0.64 for pretrained multilingual tokenizers, while maintaining higher average token lengths (3.67 characters versus 2.72 for GPT-2) despite using a vocabulary an order of magnitude smaller. These gains emerge from the method's ability to internalize character-level dependencies within syllable units, reducing the computational burden on language models while respecting Indonesian's agglutinative morphology. We call the LLM built upon this principle, TOBA LLM (Tokenisasi Optimum Berbasis Aglutinasi), the convergence of human literacy pedagogy with computational optimization principles offers a promising paradigm for developing linguistically-informed tokenization strategies, particularly for morphologically rich and underrepresented languages in natural language processing.

</details>


### [1088] [The Language You Ask In: Language-Conditioned Ideological Divergence in LLM Analysis of Contested Political Documents](https://arxiv.org/abs/2601.12164)
*Oleg Smirnov*

Main category: cs.CY

TL;DR: This study examines how large language models generate differing ideological outputs based solely on the prompt language, focusing on Ukrainian and Russian analyses.


<details>
  <summary>Details</summary>
Motivation: To investigate whether prompt language influences the ideological orientation of outputs generated by large language models in multilingual contexts.

Method: Experimental comparison of large language model outputs using semantically equivalent prompts in Russian and Ukrainian, analyzing political narratives of a Ukrainian civil society document.

Result: The output differed ideologically: Russian prompts reflected Russian state discourse, portraying civil society actors negatively, while Ukrainian prompts aligned with Western political science, portraying them positively.

Conclusion: Prompt language can systematically bias ideological orientations in AI-generated analyses, impacting multilingual, polarized information environments and research applications.

Abstract: Large language models (LLMs) are increasingly deployed as analytical tools across multilingual contexts, yet their outputs may carry systematic biases conditioned by the language of the prompt. This study presents an experimental comparison of LLM-generated political analyses of a Ukrainian civil society document, using semantically equivalent prompts in Russian and Ukrainian. Despite identical source material and parallel query structures, the resulting analyses varied substantially in rhetorical positioning, ideological orientation, and interpretive conclusions. The Russian-language output echoed narratives common in Russian state discourse, characterizing civil society actors as illegitimate elites undermining democratic mandates. The Ukrainian-language output adopted vocabulary characteristic of Western liberal-democratic political science, treating the same actors as legitimate stakeholders within democratic contestation. These findings demonstrate that prompt language alone can produce systematically different ideological orientations from identical models analyzing identical content, with significant implications for AI deployment in polarized information environments, cross-lingual research applications, and the governance of AI systems in multilingual societies.

</details>


### [1089] [AI-generated data contamination erodes pathological variability and diagnostic reliability](https://arxiv.org/abs/2601.12946)
*Hongyu He,Shaowen Xiang,Ye Zhang,Yingtao Zhu,Jin Zhang,Hao Deng,Emily Alsentzer,Qingyu Chen,Kun-Hsing Yu,Andrew Marmenshall,Tingting Chen,Srinivas Anumasa,Daniel Ebner,Dean Ho,Kee Yuan Ngiam,Ching-Yu Cheng,Dianbo Liu*

Main category: cs.CY

TL;DR: Generative AI in healthcare risks data contamination, compromising diagnostic reliability and pathological diversity without human oversight.


<details>
  <summary>Details</summary>
Motivation: To explore the consequences of generative AI contaminating medical records and its impacts on diagnostic reliability and diversity.

Method: Analyzed 800,000 synthetic medical data points across text, vision-language, and image synthesis; evaluated mitigation strategies.

Result: Synthetic data led to diagnostic errors, loss of rare findings, demographic bias, and false confidence rates tripling to 40%.

Conclusion: Mandatory human oversight is necessary to prevent generative AI from devaluing healthcare data ecosystems.

Abstract: Generative artificial intelligence (AI) is rapidly populating medical records with synthetic content, creating a feedback loop where future models are increasingly at risk of training on uncurated AI-generated data. However, the clinical consequences of this AI-generated data contamination remain unexplored. Here, we show that in the absence of mandatory human verification, this self-referential cycle drives a rapid erosion of pathological variability and diagnostic reliability. By analysing more than 800,000 synthetic data points across clinical text generation, vision-language reporting, and medical image synthesis, we find that models progressively converge toward generic phenotypes regardless of the model architecture. Specifically, rare but critical findings, including pneumothorax and effusions, vanish from the synthetic content generated by AI models, while demographic representations skew heavily toward middle-aged male phenotypes. Crucially, this degradation is masked by false diagnostic confidence; models continue to issue reassuring reports while failing to detect life-threatening pathology, with false reassurance rates tripling to 40%. Blinded physician evaluation confirms that this decoupling of confidence and accuracy renders AI-generated documentation clinically useless after just two generations. We systematically evaluate three mitigation strategies, finding that while synthetic volume scaling fails to prevent collapse, mixing real data with quality-aware filtering effectively preserves diversity. Ultimately, our results suggest that without policy-mandated human oversight, the deployment of generative AI threatens to degrade the very healthcare data ecosystems it relies upon.

</details>


### [1090] [Unbounded Harms, Bounded Law: Liability in the Age of Borderless AI](https://arxiv.org/abs/2601.12646)
*Ha-Chi Tran*

Main category: cs.CY

TL;DR: The paper identifies challenges in addressing liability for AI harms, particularly in transboundary contexts, and explores global frameworks for AI accountability using insights from other high-risk industries.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the growing transnational risks and insufficient legal mechanisms for ensuring accountability and compensation due to the global and complex nature of AI supply chains.

Method: The paper uses a comparative and interdisciplinary analysis of liability frameworks from domains like vaccines, finance, nuclear, and environmental law to derive lessons applicable to AI governance.

Result: The study identifies transferable legal principles like strict liability, risk pooling, and collective risk-sharing, and highlights structural obstacles for applying them to AI.

Conclusion: Existing territorially anchored liability systems are inadequate for AI risks. The paper calls for global accountability and compensation frameworks that balance geopolitical dynamics with collective governance needs.

Abstract: The rapid proliferation of artificial intelligence (AI) has exposed significant deficiencies in risk governance. While ex-ante harm identification and prevention have advanced, Responsible AI scholarship remains underdeveloped in addressing ex-post liability. Core legal questions regarding liability allocation, responsibility attribution, and remedial effectiveness remain insufficiently theorized and institutionalized, particularly for transboundary harms and risks that transcend national jurisdictions. Drawing on contemporary AI risk analyses, we argue that such harms are structurally embedded in global AI supply chains and are likely to escalate in frequency and severity due to cross-border deployment, data infrastructures, and uneven national oversight capacities. Consequently, territorially bounded liability regimes are increasingly inadequate. Using a comparative and interdisciplinary approach, this paper examines compensation and liability frameworks from high-risk transnational domains - including vaccine injury schemes, systemic financial risk governance, commercial nuclear liability, and international environmental regimes - to distill transferable legal design principles such as strict liability, risk pooling, collective risk-sharing, and liability channelling, while highlighting potential structural constraints on their application to AI-related harms. Situated within an international order shaped more by AI arms race dynamics than cooperative governance, the paper outlines the contours of a global AI accountability and compensation architecture, emphasizing the tension between geopolitical rivalry and the collective action required to govern transboundary AI risks effectively.

</details>


### [1091] [The Post-Turing Condition: Conceptualising Artificial Subjectivity and Synthetic Sociality](https://arxiv.org/abs/2601.12938)
*Thorsten Jelinek,Patrick Glauner,Alvin Wang Graylin,Yubao Qiu*

Main category: cs.CY

TL;DR: The paper examines AI's potential to marginalize human roles in social meaning-making, introduces the PRMO framework, and suggests Quadrangulation for socially integrated AI systems.


<details>
  <summary>Details</summary>
Motivation: Explore the societal implications of AI automating social coordination and meaning-making, rather than focusing on the automation of cognitive tasks.

Method: Introduces the PRMO framework analyzing human subjectivity dimensions (Perception, Representation, Meaning, and the Real) and suggests Quadrangulation to include humans in AI's meaning-making processes.

Result: Conceptually frames the risk of AI excluding humans from social meaning-formation, termed Synthetic Sociality, and provides a design principle to mitigate this risk.

Conclusion: Proposes structural concepts for analyzing AI systems to ensure human participation in social and meaning-related processes.

Abstract: In the Post-Turing era, artificial intelligence increasingly shapes social coordination and meaning formation rather than merely automating cognitive tasks. The central challenge is therefore not whether machines become conscious, but whether processes of interpretation and shared reference are progressively automated in ways that marginalize human participation. This paper introduces the PRMO framework, relating AI design trajectories to four constitutive dimensions of human subjectivity: Perception, Representation, Meaning, and the Real. Within this framework, Synthetic Sociality denotes a technological horizon in which artificial agents negotiate coherence and social order primarily among themselves, raising the structural risk of human exclusion from meaning formation. To address this risk, the paper proposes Quadrangulation as a design principle for socially embedded AI systems, requiring artificial agents to treat the human subject as a constitutive reference within shared contexts of meaning. This work is a conceptual perspective that contributes a structural vocabulary for analyzing AI systems at the intersection of computation and society, without proposing a specific technical implementation.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [1092] [Learning Deterministic Finite-State Machines from the Prefixes of a Single String is NP-Complete](https://arxiv.org/abs/2601.12621)
*Radu Cosmin Dumitru,Ryo Yoshinaka,Ayumi Shinohara*

Main category: cs.FL

TL;DR: This paper investigates the computational complexity of determining a minimum DFA with prefix-closed input samples and proves this remains NP-hard, even under restricted conditions.


<details>
  <summary>Details</summary>
Motivation: To analyze whether the complexity changes for finding minimum DFA when dealing with prefix-closed input samples, as the problem is NP-hard in general.

Method: The authors study specific cases with prefix-closed input samples, including all binary string prefixes and single binary string prefixes, to evaluate their computational complexity.

Result: It is demonstrated that the problem remains NP-hard and is even NP-hard to approximate under prefix-closed conditions for both DFAs and Moore/Mealy machines.

Conclusion: The findings reinforce the inherent difficulty of the problem by showing it remains computationally intractable even with restricted input sets.

Abstract: It is well known that computing a minimum DFA consistent with a given set of positive and negative examples is NP-hard. Previous work has identified conditions on the input sample under which the problem becomes tractable or remains hard. In this paper, we study the computational complexity of the case where the input sample is prefix-closed. This formulation is equivalent to computing a minimum Moore machine consistent with observations along its runs. We show that the problem is NP-hard to approximate when the sample set consists of all prefixes of binary strings. Furthermore, we show that the problem remains NP-hard as a decision problem even when the sample set consists of the prefixes of a single binary string. Our argument also extends to the corresponding problem for Mealy machines.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [1093] [Canonicalization of Batched Einstein Summations for Tuning Retrieval](https://arxiv.org/abs/2601.12220)
*Kaushik Kulkarni,Andreas Klöckner*

Main category: cs.MS

TL;DR: This paper introduces an algorithm to normalize Batched Einstein Summation expressions, using graph canonicalization techniques, and demonstrates significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Batched Einstein Summation expressions often show data reuse but lack a canonical representation, hindering their optimization in software systems. A solution is needed to exploit temporal locality and standardize representations.

Method: The authors develop a novel algorithm to normalize batched einsums by encoding them as colored graphs, applying graph canonicalization, and proposing a functional array representation for further optimization.

Result: The approach achieves a geometric mean speedup of 4.7× compared to JAX when tested on benchmarks including TCCG and a finite element method (FEM) solver.

Conclusion: The proposed method effectively normalizes and optimizes Batched Einstein Summation expressions, enabling reuse of optimization knowledge and boosting computational performance.

Abstract: We present an algorithm for normalizing \emph{Batched Einstein Summation}
  expressions by mapping mathematically equivalent formulations to a unique
  normal form. Batches of einsums with the same Einstein notation that exhibit
  substantial data reuse appear frequently in finite element methods (FEM),
  numerical linear algebra, and computational chemistry. To effectively exploit
  this temporal locality for high performance, we consider groups of einsums in
  batched form.
  Representations of equivalent batched einsums may differ due to index
  renaming, permutations within the batch, and, due to the commutativity and
  associativity of multiplication operation. The lack of a canonical
  representation hinders the reuse of optimization and tuning knowledge in
  software systems. To this end, we develop a novel encoding of batched einsums
  as colored graphs and apply graph canonicalization to derive a normal form.
  In addition to the canonicalization algorithm, we propose a representation of
  einsums using functional array operands and provide a strategy to transfer
  transformations operating on the normal form to \emph{functional batched
  einsums} that exhibit the same normal form; crucial for fusing surrounding
  computations for memory bound einsums. We evaluate our approach against JAX,
  and observe a geomean speedup of $4.7\times$ for einsums from the TCCG
  benchmark suite and an FEM solver.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [1094] [Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks](https://arxiv.org/abs/2601.12205)
*Shih-Heng Wang,Jiatong Shi,Jinchuan Tian,Haibin Wu,Shinji Watanabe*

Main category: cs.SD

TL;DR: This paper examines neural audio codecs' (NACs) ability to generalize to unseen languages, non-speech audio tasks, and the impact of non-speech data during pre-training. The study finds NACs generalize well to new languages, struggle with non-speech tasks after speech-only pre-training, but improve with non-speech data inclusion.


<details>
  <summary>Details</summary>
Motivation: To explore the limitations and potential improvements in NACs' generalization capabilities across languages, speech, and non-speech audio applications.

Method: NACs were trained from scratch under controlled setups with curated pre-training datasets. Comprehensive evaluation was conducted using comparisons across 11 metrics.

Result: NACs generalized effectively to unseen languages. Speech-only pre-trained NACs performed poorly on non-speech tasks, but incorporating non-speech data during pre-training enhances non-speech task performance while maintaining speech task quality.

Conclusion: Incorporating diverse audio data in NAC pre-training improves versatility without sacrificing quality in speech-related tasks.

Abstract: This paper investigates three crucial yet underexplored aspects of the generalization capabilities of neural audio codecs (NACs): (i) whether NACs can generalize to unseen languages during pre-training, (ii) whether speech-only pre-trained NACs can effectively generalize to non-speech applications such as environmental sounds, music, and animal vocalizations, and (iii) whether incorporating non-speech data during pre-training can improve performance on both speech and non-speech tasks. Existing studies typically rely on off-the-shelf NACs for comparison, which limits insight due to variations in implementation. In this work, we train NACs from scratch using strictly controlled configurations and carefully curated pre-training data to enable fair comparisons. We conduct a comprehensive evaluation of NAC performance on both signal reconstruction quality and downstream applications using 11 metrics. Our results show that NACs can generalize to unseen languages during pre-training, speech-only pre-trained NACs exhibit degraded performance on non-speech tasks, and incorporating non-speech data during pre-training improves performance on non-speech tasks while maintaining comparable performance on speech tasks.

</details>


### [1095] [Harmonizing the Arabic Audio Space with Data Scheduling](https://arxiv.org/abs/2601.12494)
*Hunzalah Hassan Bhatti,Firoj Alam,Shammur Absar Chowdhury*

Main category: cs.SD

TL;DR: This paper focuses on adapting audio large language models to Arabic-centric tasks, utilizing innovative techniques and a novel dataset.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the gap in adapting audio LLMs to linguistically complex, dialect-rich environments, specifically for Arabic.

Method: The paper introduces AraMega-SSum dataset for Arabic speech summarization, fine-tunes Qwen2.5-Omni model, and proposes Task-Progressive Curriculum (TPC) and Aligner-Based Diverse Sampling (ADS).

Result: The strategies reveal an efficiency-robustness trade-off: ADS improves initial training speed and paralinguistic task accuracy but destabilizes generative tasks over time; TPC stabilizes core tasks but causes negative transfer in downstream tasks.

Conclusion: Combining TPC and ADS yields the best approach, balancing efficient training and nuanced understanding, offering valuable insights for audio LLM adaptation in low-resource environments.

Abstract: Audio large language models (LLMs) enable unified speech understanding and generation, yet their adaptation to linguistically complex, dialect-rich settings remains underexplored. This paper presents the first systematic study of multi-task instruction tuning for an Arabic-centric audio LLM, covering a hierarchy of generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). To support this study, we introduce AraMega-SSum, a novel dataset for Arabic speech summarization. We fine-tune Qwen2.5-Omni (7B) and propose Task-Progressive Curriculum (TPC) along with Aligner-Based Diverse Sampling (ADS), a strategy that constructs information-dense batches by selecting task- and label-balanced examples. Our results reveal a critical efficiency, robustness trade-off: while ADS accelerates initial convergence and boosts paralinguistic F1-scores, its inherent gradient volatility can destabilize generative decoding under prolonged training. Furthermore, while the TPC stabilizes core acoustic mapping, it often induces negative transfer in downstream tasks. We demonstrate that a Hybrid TPC+ADS Strategy provides an optimal training ``recipe'', first establishing a robust representative foundation before employing diversity-aware refinement to capture fine-grained nuances. These findings offer practical guidance for the efficient adaptation of Omni-models in complex, low-resource multimodal environments.

</details>


### [1096] [SSVD-O: Parameter-Efficient Fine-Tuning with Structured SVD for Speech Recognition](https://arxiv.org/abs/2601.12600)
*Pu Wang,Shinji Watanabe,Hugo Van hamme*

Main category: cs.SD

TL;DR: The paper introduces SSVD-Outer (SSVD-O), a method that improves parameter-efficient fine-tuning for speech applications, optimizing the balance between learning and forgetting, and it outperforms existing methods in adapting to new speech domains.


<details>
  <summary>Details</summary>
Motivation: Efficiently adapting large speech foundation models to new domains is critical, but existing methods like LoRA allocate parameters uniformly across model subspaces, limiting scalability and efficiency.

Method: The authors proposed SSVD-O, which uses guided structured transformations across input (acoustic) and output (semantic) feature subspaces to achieve scalable and balanced fine-tuning.

Result: SSVD-O outperforms state-of-the-art methods like LoRA, DoRA, and PiSSA on tasks involving domain-shifted speech such as child speech and regional accents, across models up to 2 billion parameters.

Conclusion: SSVD-O narrows the gap to full fine-tuning in speech applications while improving generalization and addressing catastrophic forgetting.

Abstract: Parameter-efficient fine-tuning (PEFT) is a scalable approach for adapting large speech foundation models to new domains. While methods such as LoRA and its state-of-the-art variants reduce adaptation costs, they typically allocate parameters uniformly across model subspaces, which limits their efficiency and scalability in speech applications. Building on our prior work, this paper introduces SSVD-Outer (SSVD-O), an extension of the structured SVD-guided (SSVD) fine-tuning method. SSVD-O combines input acoustic feature space-associated inner transformations with output semantic feature space-associated outer transformations to enable scalable and balanced adaptation. We conduct the first systematic analysis of parameter budget allocation across model subspaces in PEFT for automatic speech recognition (ASR), and investigate the trade-off between learning and forgetting under constrained resources. SSVD-O is benchmarked against LoRA, DoRA, PiSSA, and SSVD on domain-shifted ASR tasks, including child speech and regional accents, across model scales from 0.1B to 2B within the ESPnet framework. Experimental results show that SSVD-O consistently narrows the performance gap to full fine-tuning while improving generalization and mitigating catastrophic forgetting.

</details>


### [1097] [Lombard Speech Synthesis for Any Voice with Controllable Style Embeddings](https://arxiv.org/abs/2601.12966)
*Seymanur Akti,Alexander Waibel*

Main category: cs.SD

TL;DR: The paper introduces a controllable TTS system to synthesize Lombard speech for any speaker, enhancing intelligibility under noise, without needing explicit Lombard training data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating Lombard speech for any speaker in TTS systems, particularly for noisy communication or hearing-impaired listeners, while maintaining naturalness and speaker identity.

Method: The system utilizes style embeddings trained on a prosodically diverse dataset. Principal component analysis (PCA) identifies correlations with Lombard attributes, enabling manipulation of these components for Lombard-level control within the TTS model.

Result: The method successfully generates speech with desired Lombard levels while preserving naturalness, speaker identity, and enhancing intelligibility under noisy conditions.

Conclusion: The proposed approach provides a robust and flexible solution for generating controllable Lombard speech in TTS systems applicable to any speaker.

Abstract: The Lombard effect plays a key role in natural communication, particularly in noisy environments or when addressing hearing-impaired listeners. We present a controllable text-to-speech (TTS) system capable of synthesizing Lombard speech for any speaker without requiring explicit Lombard data during training. Our approach leverages style embeddings learned from a large, prosodically diverse dataset and analyzes their correlation with Lombard attributes using principal component analysis (PCA). By shifting the relevant PCA components, we manipulate the style embeddings and incorporate them into our TTS model to generate speech at desired Lombard levels. Evaluations demonstrate that our method preserves naturalness and speaker identity, enhances intelligibility under noise, and provides fine-grained control over prosody, offering a robust solution for controllable Lombard TTS for any speaker.

</details>


### [1098] [ParaMETA: Towards Learning Disentangled Paralinguistic Speaking Styles Representations from Speech](https://arxiv.org/abs/2601.12289)
*Haowei Lou,Hye-young Paik,Wen Hu,Lina Yao*

Main category: cs.SD

TL;DR: ParaMETA introduces a framework for disentangled, task-specific speaking style embeddings, addressing both recognition and generative tasks, with superior accuracy and expressive speech generation.


<details>
  <summary>Details</summary>
Motivation: To create a unified method for learning diverse speaking styles, improving multi-task performance while avoiding inter-task interference.

Method: Project speech into separate subspaces for each speaking style, using a single lightweight model capable of various paralinguistic tasks.

Result: ParaMETA achieves superior classification accuracy and generates natural, expressive speech in style-aware TTS tasks.

Conclusion: ParaMETA effectively learns controlled, disentangled embeddings for multiple speaking styles, proving beneficial for both recognition and generation tasks.

Abstract: Learning representative embeddings for different types of speaking styles, such as emotion, age, and gender, is critical for both recognition tasks (e.g., cognitive computing and human-computer interaction) and generative tasks (e.g., style-controllable speech generation). In this work, we introduce ParaMETA, a unified and flexible framework for learning and controlling speaking styles directly from speech. Unlike existing methods that rely on single-task models or cross-modal alignment, ParaMETA learns disentangled, task-specific embeddings by projecting speech into dedicated subspaces for each type of style. This design reduces inter-task interference, mitigates negative transfer, and allows a single model to handle multiple paralinguistic tasks such as emotion, gender, age, and language classification. Beyond recognition, ParaMETA enables fine-grained style control in Text-To-Speech (TTS) generative models. It supports both speech- and text-based prompting and allows users to modify one speaking styles while preserving others. Extensive experiments demonstrate that ParaMETA outperforms strong baselines in classification accuracy and generates more natural and expressive speech, while maintaining a lightweight and efficient model suitable for real-world applications.

</details>


### [1099] [Toward Faithful Explanations in Acoustic Anomaly Detection](https://arxiv.org/abs/2601.12660)
*Maab Elrashid,Anthony Deschênes,Cem Subakan,Mirco Ravanelli,Rémi Georges,Michael Morin*

Main category: cs.SD

TL;DR: This paper evaluates the interpretability of autoencoder-based models in audio anomaly detection, particularly comparing standard and masked autoencoders using attribution methods, with findings favoring the latter for explanation quality.


<details>
  <summary>Details</summary>
Motivation: Despite their strong performance in anomaly detection, deep learning models often lack interpretability, which hinders user trust. This study aims to enhance and assess the interpretability of such models.

Method: The authors compared a standard autoencoder (AE) with a mask autoencoder (MAE) using various attribution methods, including error maps and saliency methods, and proposed a novel perturbation-based faithfulness metric.

Result: The masked autoencoder (MAE) demonstrated slightly lower detection performance but provided more faithful and temporally precise anomaly explanations, which better aligned with true anomalies.

Conclusion: Incorporating interpretability into anomaly detection systems is crucial, and masked autoencoders improve explanation quality with minimal performance trade-offs, making them valuable for real-world applications.

Abstract: Interpretability is essential for user trust in real-world anomaly detection applications. However, deep learning models, despite their strong performance, often lack transparency. In this work, we study the interpretability of autoencoder-based models for audio anomaly detection, by comparing a standard autoencoder (AE) with a mask autoencoder (MAE) in terms of detection performance and interpretability. We applied several attribution methods, including error maps, saliency maps, SmoothGrad, Integrated Gradients, GradSHAP, and Grad-CAM. Although MAE shows a slightly lower detection, it consistently provides more faithful and temporally precise explanations, suggesting a better alignment with true anomalies. To assess the relevance of the regions highlighted by the explanation method, we propose a perturbation-based faithfulness metric that replaces them with their reconstructions to simulate normal input. Our findings, based on experiments in a real industrial scenario, highlight the importance of incorporating interpretability into anomaly detection pipelines and show that masked training improves explanation quality without compromising performance.

</details>


### [1100] [SoundPlot: An Open-Source Framework for Birdsong Acoustic Analysis and Neural Synthesis with Interactive 3D Visualization](https://arxiv.org/abs/2601.12752)
*Naqcho Ali Mehdi,Mohammad Adeel,Aizaz Ali Larik*

Main category: cs.SD

TL;DR: SoundPlot is an open-source tool for analyzing bird vocalizations via feature extraction, dimensionality reduction, and neural audio synthesis, providing visual and analytical insights.


<details>
  <summary>Details</summary>
Motivation: This paper aims to create an efficient, interactive, and high-fidelity analysis tool for studying avian sounds, aiding bioacoustics and related fields.

Method: SoundPlot processes audio into a feature space using spectral features, pYIN, and MFCCs, employs PCA for dimensionality reduction, and reconstructs audio using Griffin-Lim phase estimation, with visualization through a Three.js interface.

Result: The system demonstrates strong fidelity in preserving acoustic structures, with mel spectrogram correlation scores over 0.92.

Conclusion: SoundPlot advances audio analysis by offering a robust, real-time, and interactive framework beneficial for bioacoustic research and beyond.

Abstract: We present SoundPlot, an open-source framework for analyzing avian vocalizations through acoustic feature extraction, dimensionality reduction, and neural audio synthesis. The system transforms audio signals into a multi-dimensional acoustic feature space, enabling real-time visualization of temporal dynamics in 3D using web-based interactive graphics. Our framework implements a complete analysis-synthesis pipeline that extracts spectral features (centroid, bandwidth, contrast), pitch contours via probabilistic YIN (pYIN), and mel-frequency cepstral coefficients (MFCCs), mapping them to a unified timbre space for visualization. Audio reconstruction employs the Griffin-Lim phase estimation algorithm applied to mel spectrograms. The accompanying Three.js-based interface provides dual-viewport visualization comparing original and synthesized audio trajectories with independent playback controls. We demonstrate the framework's capabilities through comprehensive waveform analysis, spectrogram comparisons, and feature space evaluation using Principal Component Analysis (PCA). Quantitative evaluation shows mel spectrogram correlation scores exceeding 0.92, indicating high-fidelity preservation of perceptual acoustic structure. SoundPlot is released under the MIT License to facilitate research in bioacoustics, audio signal processing, and computational ethology.

</details>


### [1101] [Performance and Complexity Trade-off Optimization of Speech Models During Training](https://arxiv.org/abs/2601.13704)
*Esteban Gómez,Tom Bäckström*

Main category: cs.SD

TL;DR: This paper presents a technique to dynamically optimize both performance and computational complexity of neural network models during training, avoiding heuristic methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for adjusting layer sizes in neural networks rely on heuristics and post hoc pruning, which fail to ensure an optimal trade-off between performance and computational complexity.

Method: The authors propose a reparameterization method using feature noise injection, enabling a continuous optimization of performance and computational complexity during training through SGD techniques.

Result: This technique is validated through three case studies: one synthetic example and two real-world applications (voice activity detection and audio anti-spoofing).

Conclusion: The proposed method eliminates the need for heuristic post hoc adjustments by dynamically optimizing model size and complexity during training, with publicly available code enabling further research.

Abstract: In speech machine learning, neural network models are typically designed by choosing an architecture with fixed layer sizes and structure. These models are then trained to maximize performance on metrics aligned with the task's objective. While the overall architecture is usually guided by prior knowledge of the task, the sizes of individual layers are often chosen heuristically. However, this approach does not guarantee an optimal trade-off between performance and computational complexity; consequently, post hoc methods such as weight quantization or model pruning are typically employed to reduce computational cost. This occurs because stochastic gradient descent (SGD) methods can only optimize differentiable functions, while factors influencing computational complexity, such as layer sizes and floating-point operations per second (FLOP/s), are non-differentiable and require modifying the model structure during training. We propose a reparameterization technique based on feature noise injection that enables joint optimization of performance and computational complexity during training using SGD-based methods. Unlike traditional pruning methods, our approach allows the model size to be dynamically optimized for a target performance-complexity trade-off, without relying on heuristic criteria to select which weights or structures to remove. We demonstrate the effectiveness of our method through three case studies, including a synthetic example and two practical real-world applications: voice activity detection and audio anti-spoofing. The code related to our work is publicly available to encourage further research.

</details>


### [1102] [Fusion Segment Transformer: Bi-Directional Attention Guided Fusion Network for AI-Generated Music Detection](https://arxiv.org/abs/2601.13647)
*Yumin Kim,Seonghyeon Go*

Main category: cs.SD

TL;DR: This paper presents the Fusion Segment Transformer, an advanced model designed for detecting AI-generated music on a full-audio scale.


<details>
  <summary>Details</summary>
Motivation: The rise of generative AI-enabled music has generated copyright concerns, creating the need for accurate long-audio AI-generated content detection.

Method: The authors propose the Fusion Segment Transformer, which incorporates a Gated Fusion Layer to combine content and structural data for long-term context analysis in audio.

Result: The approach demonstrated state-of-the-art performance in detecting AI-generated music on the SONICS and AIME datasets.

Conclusion: The Fusion Segment Transformer effectively improves full-audio AI-generated music detection, addressing long-term structure and context challenges.

Abstract: With the rise of generative AI technology, anyone can now easily create and deploy AI-generated music, which has heightened the need for technical solutions to address copyright and ownership issues. While existing works mainly focused on short-audio, the challenge of full-audio detection, which requires modeling long-term structure and context, remains insufficiently explored. To address this, we propose an improved version of the Segment Transformer, termed the Fusion Segment Transformer. As in our previous work, we extract content embeddings from short music segments using diverse feature extractors. Furthermore, we enhance the architecture for full-audio AI-generated music detection by introducing a Gated Fusion Layer that effectively integrates content and structural information, enabling the capture of long-term context. Experiments on the SONICS and AIME datasets show that our approach outperforms the previous model and recent baselines, achieving state-of-the-art results in AI-generated music detection.

</details>


### [1103] [Towards Effective Negation Modeling in Joint Audio-Text Models for Music](https://arxiv.org/abs/2601.13931)
*Yannis Vasilakis,Rachel Bittner,Johan Pauwels*

Main category: cs.SD

TL;DR: The study targets joint audio-text model limitations in handling semantic negations and proposes methods to enhance reliability for music retrieval.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the difficulty of joint audio-text models, such as CLAP, in handling semantic negation, which is crucial for differentiating musical descriptors like "with vocals" and "without vocals."

Method: The authors trained a CLAP model on the Million Song Dataset, incorporating LP-MusicCaps-MSD captions. They augmented the text with negation examples and implemented a contrastive loss function that separates original and negated text in the embedding space.

Result: The proposed methods improved the model's ability to handle negation without compromising its general retrieval performance.

Conclusion: Explicitly modeling negation through augmentation and contrastive learning improves music retrieval systems by addressing a key semantic challenge.

Abstract: Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., "with vocals" vs. "without vocals"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance.

</details>


### [1104] [ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models](https://arxiv.org/abs/2601.14157)
*Bruno Sienkiewicz,Łukasz Neumann,Mateusz Modrzejewski*

Main category: cs.SD

TL;DR: The paper introduces ConceptCaps, a structured music dataset, and validates its effectiveness in concept-based interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing music datasets have issues with ambiguity, noise, or sparsity, making them unsuitable for concept-based interpretability analysis.

Method: They created the ConceptCaps dataset with 23k music-caption-audio triplets. Their pipeline involves training a VAE for semantic modeling, a fine-tuned LLM for generating descriptions, and MusicGen for audio synthesis. This modular approach improves coherence and control.

Result: The dataset performs well in audio-text alignment (CLAP), linguistic quality (BERTScore, MAUVE), and TCAV analysis, demonstrating musically meaningful patterns.

Conclusion: ConceptCaps enables structured, interpretable analysis of music concepts, addressing the limitations of existing datasets. It is a valuable resource for the field.

Abstract: Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1105] [NOVAID: Natural-language Observability Visualization Assistant for ITOps Dashboard Widget Generation](https://arxiv.org/abs/2601.11531)
*Pratik Mishra,Caner Gözübüyük,Seema Nagar,Prateeti Mohapatra,Raya Wittich,Arthur de Magalhaes*

Main category: cs.HC

TL;DR: NOVAID is an interactive chatbot tool that uses large language models to create IT monitoring widgets directly from natural language queries, aiming to simplify dashboard creation.


<details>
  <summary>Details</summary>
Motivation: IT monitoring dashboard widget creation is typically slow and error-prone, creating significant barriers for both novice and experienced users.

Method: The authors developed NOVAID, which utilizes a domain-specific semantic parser, fuzzy matching, and schema completion. It interprets user inputs in natural language, generates widget JSON specs, and ensures clarity through an interactive feedback loop.

Result: NOVAID achieves up to 94.10% accuracy in a controlled dataset for metric extraction and received a System Usability Scale score of 74.2 from IT engineers, highlighting its promising accuracy and usability.

Conclusion: NOVAID successfully bridges natural language processing with IT operations monitoring tasks and holds strong potential for implementing into enterprise environments.

Abstract: Manual creation of IT monitoring dashboard widgets is slow, error-prone, and a barrier for both novice and expert users. We present NOVAID, an interactive chatbot that leverages Large Language Models (LLMs) to generate IT monitoring widgets directly from natural language queries. Unlike general natural language-to-visualization tools, NOVAID addresses IT operations-specific challenges: specialized widget types like SLO charts, dynamic API-driven data retrieval, and complex contextual filters. The system combines a domain-aware semantic parser, fuzzy entity matching, and schema completion to produce standardized widget JSON specifications. An interactive clarification loop ensures accuracy in underspecified queries. On a curated dataset of 271 realistic queries, NOVAID achieves promising accuracy (up to 94.10% in metric extraction) across multiple LLMs. A user study with IT engineers yielded a System Usability Scale score of 74.2 for NOVAID, indicating good usability. By bridging natural language intent with operational dashboards, NOVAID demonstrates clear potential and a path for deployment in enterprise ITOps monitoring platforms.

</details>


### [1106] [Affective Translation: Material and Virtual Embodiments of Kinetic Textile Robots](https://arxiv.org/abs/2601.11543)
*Berfin Ataman,Rodrigo Gallardo,Qilmeg Doudatcz*

Main category: cs.HC

TL;DR: The study compares emotional engagement between physical textile soft robots and their augmented-reality (AR) counterparts.


<details>
  <summary>Details</summary>
Motivation: To understand how physical and digital modalities influence emotional and perceptual responses in human-robot interaction.

Method: The researchers used a between-subjects design where participants interacted with either physical robotic sculptures or their AR twins, following identical protocols and surveys.

Result: The study measured sensations like calmness, curiosity, and discomfort across modalities to provide insights on how motion, form, and material behavior shape emotional interpretation.

Conclusion: The findings contribute to designing hybrid systems for emotionally meaningful interactions between humans, robots, and their digital twins.

Abstract: This study presents a comparative framework for evaluating emotional engagement with textile soft robots and their augmented-reality (AR) counterparts. Four robotic sculptures were developed, each embodying nature-inspired dynamic behaviors such as breathing and gradual deformation. Using a between-subjects design, two independent groups, one experiencing the physical installations and one engaging with their virtual (AR) twins, follow identical protocols and complete the same self-assessment survey on affective and perceptual responses. This approach minimizes carryover and novelty effects while enabling a direct comparison of sensations such as calmness, curiosity, and discomfort across modalities. The analysis explores how motion, form, and material behavior shape emotional interpretation in physical versus digital contexts, informing the design of hybrid systems that evoke meaningful, emotionally legible interactions between humans, robots, and digital twins.

</details>


### [1107] [A Hybrid Soft Haptic Display for Rendering Lump Stiffness in Remote Palpation](https://arxiv.org/abs/2601.11807)
*Pijuan Yu,Anzu Kawazoe,Alexis Urquhart,Thomas K. Ferris,M. Cynthia Hipwell,Rebecca F. Friesen*

Main category: cs.HC

TL;DR: The study introduces a hybrid fingertip display to improve remote palpation accuracy, comparing different rendering strategies for conveying tactile information about a hard lump beneath soft tissue.


<details>
  <summary>Details</summary>
Motivation: Current tactile displays for telemedicine lack fidelity in rendering both large-scale forces and fine spatial details crucial for accurate remote palpation.

Method: A hybrid fingertip display comprising a rigid platform and a 4x4 soft pneumatic tactile display was tested with three rendering strategies in a lump detection study involving 12 participants.

Result: Hybrid methods significantly improved detection accuracy compared to the baseline, increasing it from 50% to over 95%, with Hybrid B providing the most realism but introducing latency.

Conclusion: The study highlights a trade-off between tactile realism and real-time responsiveness, suggesting that rendering choices in hybrid displays may optimize one aspect at the expense of the other.

Abstract: Remote palpation enables noninvasive tissue examination in telemedicine, yet current tactile displays often lack the fidelity to convey both large-scale forces and fine spatial details. This study introduces a hybrid fingertip display comprising a rigid platform and a $4\times4$ soft pneumatic tactile display (4.93 mm displacement and 1.175 N per single pneumatic chamber) to render a hard lump beneath soft tissue. This study compares three rendering strategies: a Platform-Only baseline that renders the total interaction force; a Hybrid A (Position + Force Feedback) strategy that adds a dynamic, real-time soft spatial cue; and a Hybrid B (Position + Preloaded Stiffness Feedback) strategy that provides a constant, pre-calculated soft spatial cue.
  In a 12-participant lump detection study, both hybrid methods dramatically improved accuracy over the Platform-Only baseline (from 50\% to over 95\%). While the Hybrid B was highlighted qualitatively for realism, its event-based averaging is expected to increase interaction latency in real-time operation. This suggests a trade-off between perceived lump realism and real-time responsiveness, such that rendering choices that enhance realism may conflict with those that minimize latency.

</details>


### [1108] [Reframing Conversational Design in HRI: Deliberate Design with AI Scaffolds](https://arxiv.org/abs/2601.12084)
*Shiye Cao,Jiwon Moon,Yifan Xu,Anqi Liu,Chien-Ming Huang*

Main category: cs.HC

TL;DR: This paper introduces the AI-Aided Conversation Engine (ACE), an innovative tool for improving human-robot interaction by guiding prompt creation for large language models used in conversational robots.


<details>
  <summary>Details</summary>
Motivation: Current conversational AI lacks effective context-specific adaptation, relying on tedious prompt engineering and inconsistent practices in human-robot interaction design.

Method: The proposed ACE system includes an LLM-powered voice agent for initial prompts, an annotation interface for feedback collection, and feedback-to-prompt translations using LLMs.

Result: ACE was evaluated in user studies, showing it improved prompt clarity and specificity and generated higher-quality human-robot interactions.

Conclusion: ACE provides a structured, efficient solution for designing conversational prompts, enhancing both the design experience and the quality of robot interactions.

Abstract: Large language models (LLMs) have enabled conversational robots to move beyond constrained dialogue toward free-form interaction. However, without context-specific adaptation, generic LLM outputs can be ineffective or inappropriate. This adaptation is often attempted through prompt engineering, which is non-intuitive and tedious. Moreover, predominant design practice in HRI relies on impression-based, trial-and-error refinement without structured methods or tools, making the process inefficient and inconsistent. To address this, we present the AI-Aided Conversation Engine (ACE), a system that supports the deliberate design of human-robot conversations. ACE contributes three key innovations: 1) an LLM-powered voice agent that scaffolds initial prompt creation to overcome the "blank page problem," 2) an annotation interface that enables the collection of granular and grounded feedback on conversational transcripts, and 3) using LLMs to translate user feedback into prompt refinements. We evaluated ACE through two user studies, examining both designs' experience and end users' interactions with robots designed using ACE. Results show that ACE facilitates the creation of robot behavior prompts with greater clarity and specificity, and that the prompts generated with ACE lead to higher-quality human-robot conversational interactions.

</details>


### [1109] [User-to-Vehicle Interaction in Smart Mobility: The GO-DRiVeS Autonomous Ride-Sharing Application](https://arxiv.org/abs/2601.12367)
*Hana E. Elmalah,Catherine M. Elias*

Main category: cs.HC

TL;DR: This paper introduces "GO-DRiVeS," a mobile app to address challenges faced by university staff and students with long walks and heavy items, offering ride-requesting and real-time tracking.


<details>
  <summary>Details</summary>
Motivation: To address the issue of university students and staff facing challenges like tiring long walks in hot weather or while carrying heavy items by providing an efficient ride-sharing application.

Method: The authors developed GO-DRiVeS using Agile methodology, utilizing mobile app system architecture, React Native (Expo) for the frontend, Node.js with Express for the backend, and MongoDB for the database. Key features include user registration, real-time tracking, and managing multiple requests.

Result: The application demonstrated stable behavior in handling ride requests in multiple experiments.

Conclusion: GO-DRiVeS proves to be a reliable and efficient solution for on-demand ride-sharing, specifically tailored for a university context, addressing practical mobility issues.

Abstract: This paper introduces the GO-DRiVeS application, an on demand ride sharing and requesting mobile application tailored specifically to save long walks and challenges which are time consuming and tiring especially during hot days or when carrying heavy items, faced by university students and staff. The GO-DRiVeS application was developed following the Agile methodology for its flexibility. In addition to, using the mobile application system architecture and client-server architecture. GO-DRiVeS was implemented using React Native (Expo) for the frontend, Node.js and Express for the backend, and MongoDB as the database; based on a detailed analyses to the existing transportation application, comparing their frameworks and identifying their essential functionalities. GO-DRiVeS supports core features like user registration, ride requesting and real-time tracking.In addition to handling multiple requests at the same time in a first come first serve manner. The application was developed based on these features, and the results were conducted in the form of multiple experiments that demonstrated stable behavior in handling the requests, as presented in the Methodology and Results chapters.

</details>


### [1110] [Towards Natural Language Environment: Understanding Seamless Natural-Language-Based Human-Multi-Robot Interactions](https://arxiv.org/abs/2601.13338)
*Ziyi Liu,Xinyi Wang,Shao-Kang Hsia,Chenfei Zhu,Zhengze Zhu,Xiyun Hu,Anastasia Kouvaras Ostrowski,Karthik Ramani*

Main category: cs.HC

TL;DR: The paper explores Natural Language Environments (NLEs) which utilize natural language for human-robot and robot-robot interactions.


<details>
  <summary>Details</summary>
Motivation: To address future scenarios where multiple robots coexist with humans, facilitating coordination primarily through natural language.

Method: The authors synthesize prior research to define NLEs and conduct a virtual reality-based role-playing study on human-multi-robot interaction.

Result: They refine the NLE design space using qualitative and quantitative data, highlighting tensions and opportunities regarding coordination dominance, robot autonomy, and robot personality.

Conclusion: The work emphasizes design implications for effective human-multi-robot coordination with natural language, rather than delivering deployable systems.

Abstract: As multiple robots are expected to coexist in future households, natural language is increasingly envisioned as a primary medium for human-robot and robot-robot communication. This paper introduces the concept of a Natural Language Environment (NLE), defined as an interaction space in which humans and multiple heterogeneous robots coordinate primarily through natural language.
  Rather than proposing a deployable system, this work aims to explore the design space of such environments. We first synthesize prior work on language-based human-robot interaction to derive a preliminary design space for NLEs. We then conduct a role-playing study in virtual reality to investigate how people conceptualize, negotiate, and coordinate human-multi-robot interactions within this imagined environment.
  Based on qualitative and quantitative analysis, we refine the preliminary design space and derive design implications that highlight key tensions and opportunities around task coordination dominance, robot autonomy, and robot personality in Natural Language Environments.

</details>


### [1111] [Chatsparent: An Interactive System for Detecting and Mitigating Cognitive Fatigue in LLMs](https://arxiv.org/abs/2601.11526)
*Riju Marwah,Vishal Pallagani,Ritvik Garimella,Amit Sheth*

Main category: cs.HC

TL;DR: This paper introduces Chatsparent, a system that identifies and mitigates cognitive fatigue in LLMs during interactions with users.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of transparency in chatbot interactions, which can lead to blind trust and issues like repetitive or incoherent outputs from LLMs.

Method: Chatsparent surfaces cognitive fatigue by monitoring real-time token-level signals such as attention decay, embedding drift, and entropy collapse. It visualizes these with a unified fatigue index and provides interventions like attention resets and entropy-regularized decoding.

Result: The system allows users to observe fatigue signals and apply interventions, improving LLM output quality and reliability while creating a better understanding of model behavior.

Conclusion: Chatsparent transforms typical LLM chatbot interaction into a diagnostic experience, empowering users to detect and address model fatigue, ultimately enhancing transparency and performance.

Abstract: LLMs are increasingly being deployed as chatbots, but today's interfaces offer little to no friction: users interact through seamless conversations that conceal when the model is drifting, hallucinating or failing. This lack of transparency fosters blind trust, even as models produce unstable or repetitive outputs. We introduce an interactive demo that surfaces and mitigates cognitive fatigue, a failure mode where LLMs gradually lose coherence during auto-regressive generation. Our system, Chatsparent, instruments real-time, token-level signals of fatigue, including attention-to-prompt decay, embedding drift, and entropy collapse, and visualizes them as a unified fatigue index. When fatigue thresholds are crossed, the interface allows users to activate lightweight interventions such as attention resets, entropy-regularized decoding, and self-reflection checkpoints. The demo streams live text and fatigue signals, allowing users to observe when fatigue arises, how it affects output quality, and how interventions restore stability. By turning passive chatbot interaction into an interactive diagnostic experience, our system empowers users to better understand LLM behavior while improving reliability at inference time.

</details>


### [1112] [Do LLMs Give Good Romantic Relationship Advice? A Study on User Satisfaction and Attitude Change](https://arxiv.org/abs/2601.11527)
*Niva Manchanda,Akshata Kishore Moharir,Isabel Michel,Ratna Kandala*

Main category: cs.HC

TL;DR: This study evaluates user perceptions of romantic relationship advice generated by large language models (LLMs), finding high user satisfaction and improved attitudes toward LLMs after interacting with their advice.


<details>
  <summary>Details</summary>
Motivation: To understand user perceptions, satisfaction, and trust in large language models (LLMs) that generate advice for personal domains, like romantic relationships.

Method: Participants were asked to rate their satisfaction, perceptions of reliability, and helpfulness of LLM-generated advice. Additionally, pre- and post-measures of general attitudes toward LLMs were collected.

Result: Participants expressed high satisfaction with the advice, which was strongly linked to perceived reliability and helpfulness. Their attitudes toward LLMs significantly improved after interacting with the advice.

Conclusion: LLM-generated, contextually relevant advice enhances user satisfaction, trust, and openness, indicating their potential value in personal support applications.

Abstract: Large Language Models (LLMs) are increasingly being used to provide support and advice in personal domains such as romantic relationships, yet little is known about user perceptions of this type of advice. This study investigated how people evaluate advice on LLM-generated romantic relationships. Participants rated advice satisfaction, model reliability, and helpfulness, and completed pre- and post-measures of their general attitudes toward LLMs. Overall, the results showed participants' high satisfaction with LLM-generated advice. Greater satisfaction was, in turn, strongly and positively associated with their perceptions of the models' reliability and helpfulness. Importantly, participants' attitudes toward LLMs improved significantly after exposure to the advice, suggesting that supportive and contextually relevant advice can enhance users' trust and openness toward these AI systems.

</details>


### [1113] [SNAP: A Plan-Driven Framework for Controllable Interactive Narrative Generation](https://arxiv.org/abs/2601.11529)
*Geonwoo Bang,DongMyung Kim,Hayoung Oh*

Main category: cs.HC

TL;DR: This paper introduces SNAP, a framework to improve narrative consistency and coherence in web-based interactive applications by structuring narratives into Cells with explicit Plans, addressing challenges faced by LLMs.


<details>
  <summary>Details</summary>
Motivation: LLM-based conversational agents often fail in maintaining scenario consistency due to spatiotemporal distortions when responding to varied user inputs, which undermines their effectiveness in interactive web platforms.

Method: The paper proposes SNAP, a framework that compartmentalizes narratives into Cells and employs detailed plans for spatiotemporal settings, character actions, and plot progression to maintain controllable and scenario-consistent dialogues.

Result: Through automated and human evaluations, SNAP demonstrates superior narrative controllability and scenario consistency compared to LLM-based conversational agents in interactive storytelling.

Conclusion: SNAP effectively addresses the issue of narrative drift in web-based interactive applications, showcasing its potential in enhancing user experience through coherent and adaptable conversations.

Abstract: Large Language Models (LLMs) hold great potential for web-based interactive applications, including browser games, online education, and digital storytelling platforms. However, LLM-based conversational agents suffer from spatiotemporal distortions when responding to variant user inputs, failing to maintain consistency with provided scenarios. We propose SNAP (Story and Narrative-based Agent with Planning), a framework that structures narratives into Cells with explicit Plans to prevent narrative drift in web environments. By confining context within each Cell and employing detailed plans that specify spatiotemporal settings, character actions, and plot developments, SNAP enables coherent and scenario-consistent dialogues while adapting to diverse user responses. Via automated and human evaluations, we validate SNAP's superiority in narrative controllability, demonstrating effective scenario consistency despite variant user inputs in web-based interactive storytelling.

</details>


### [1114] [AI for Proactive Mental Health: A Multi-Institutional, Longitudinal, Randomized Controlled Trial](https://arxiv.org/abs/2601.11530)
*Julie Y. A. Cachia,Xuan Zhao,John Hunter,Delancey Wu,Eta Lin,Julian De Freitas*

Main category: cs.HC

TL;DR: This study evaluated 'Flourish,' an AI-powered mobile app, as a tool for promoting well-being in young adults and found it effective in boosting mental health metrics.


<details>
  <summary>Details</summary>
Motivation: Many young adults avoid seeking mental health support due to barriers such as stigma, accessibility, and time, prompting a need for alternative, scalable solutions.

Method: A generative AI-powered mobile app was tested via a six-week randomized controlled trial involving 486 students from three U.S. institutions, comparing app users with a waitlist control group.

Result: Students using the app showed significant improvements in positive affect, resilience, social well-being, and experienced protection against declines in mindfulness and flourishing.

Conclusion: Generative AI, when ethically and thoughtfully designed, can serve as a proactive and scalable solution for improving mental well-being at a population level.

Abstract: Young adults today face unprecedented mental health challenges, yet many hesitate to seek support due to barriers such as accessibility, stigma, and time constraints. Bite-sized well-being interventions offer a promising solution to preventing mental distress before it escalates to clinical levels, but have not yet been delivered through personalized, interactive, and scalable technology. We conducted the first multi-institutional, longitudinal, preregistered randomized controlled trial of a generative AI-powered mobile app ("Flourish") designed to address this gap. Over six weeks in Fall 2024, 486 undergraduate students from three U.S. institutions were randomized to receive app access or waitlist control. Participants in the treatment condition reported significantly greater positive affect, resilience, and social well-being (i.e., increased belonging, closeness to community, and reduced loneliness) and were buffered against declines in mindfulness and flourishing. These findings suggest that, with purposeful and ethical design, generative AI can deliver proactive, population-level well-being interventions that produce measurable benefits.

</details>


### [1115] ["Jutters"](https://arxiv.org/abs/2601.11532)
*Meike Driessen,Selina Khan,Gonçalo Marcelino*

Main category: cs.HC

TL;DR: The project uses Dutch shoreline foraging as a metaphor to explore our engagement with AI-generated content through an installation blending real and AI-altered debris.


<details>
  <summary>Details</summary>
Motivation: To encourage reflection and more mindful interaction with the influx of AI-generated media shaping our lives.

Method: The study employs a physical installation combining real shoreline debris with AI-altered images and videos, allowing visitors to act as modern foragers.

Result: Visitors interact with the installation, deciding what to keep or discard, simulating engagement and critical reflection on AI-created content.

Conclusion: AI-generated media can serve as a stimulus for reflective and critical engagement rather than passive consumption.

Abstract: This project explores how we engage with AI-generated content through the lens of the jutter: Dutch coastal foragers who comb the shoreline after storms, gathering and repurposing what the sea leaves behind. Reflecting how our lives are increasingly shaped by AI-generated media, we create a beach-like installation that blends real shoreline debris with AI-transformed images and videos. Visitors are invited to explore this space as contemporary jutters, deciding what to keep and what to discard. In doing so, the project reimagines AI-imagery as material for reflection, encouraging a more discerning engagement with the content that drifts through our feeds. A video preview of the installation can be found at https://www.youtube.com/watch?v=L6319Ii7MT8.

</details>


### [1116] [Artificial Intelligence as a Training Tool in Clinical Psychology: A Comparison of Text-Based and Avatar Simulations](https://arxiv.org/abs/2601.11533)
*V. El Sawah,A. Bhardwaj,A. Pryke-Hobbes,D. Gamaleldin,C. S. Ang,A. K. Martin*

Main category: cs.HC

TL;DR: Clinical psychology students evaluated two AI simulations, ChatGPT and HeyGen, for practicing counselling skills. The voice-based avatar (HeyGen) was rated higher than the text-based chatbot (ChatGPT) for usefulness and skill development.


<details>
  <summary>Details</summary>
Motivation: Clinical psychology students feel unprepared for therapeutic interpersonal challenges, necessitating accessible ways to practise core counselling skills ahead of working with real clients.

Method: Twenty-four postgraduate clinical psychology students participated in role-plays using a text-based chatbot and a voice-based avatar. They provided feedback based on usefulness, responsiveness, engagement, and skill improvement.

Result: Both AI tools were positively evaluated, but the voice-based avatar was significantly preferred for its ability to convey social and emotional cues.

Conclusion: AI-driven simulations, especially voice-based avatars, may effectively enhance clinical skills training. Future studies should explore their impact on real therapeutic performance.

Abstract: Clinical psychology students frequently report feeling underprepared for the interpersonal demands of therapeutic work, highlighting the need for accessible opportunities to practise core counselling skills before seeing real clients. Advances in artificial intelligence (AI) now enable simulated interaction partners that may support early skills development. This study examined postgraduate clinical psychology students' perceptions of two AI-based simulations: a text-based chatbot (ChatGPT) and a voice-based avatar (HeyGen). Twenty-four students completed two brief cognitive-behavioural role-plays (counterbalanced), one with each tool, and provided both quantitative ratings and qualitative feedback on perceived usefulness, skill application, responsiveness and engagement, and perceived skill improvement. Both AI tools were evaluated positively across dimensions. However, the avatar was rated significantly higher than the chatbot for perceived usefulness, skill application, and perceived skill improvement, and qualitative comments highlighted the added value of voice-based interaction for conveying social and emotional cues. These findings suggest that AI-driven simulation may supplement early-stage clinical skills training, with voice-based avatars offering additional benefits. Future work should test whether such simulated interactions translate to objective improvements in real therapeutic performance.

</details>


### [1117] [Modular AI-Powered Interviewer with Dynamic Question Generation and Expertise Profiling](https://arxiv.org/abs/2601.11534)
*Aisvarya Adeseye,Jouni Isoaho,Seppo Virtanen,Mohammad Tahir*

Main category: cs.HC

TL;DR: The paper presents an AI interviewer that uses a locally hosted large language model to improve engagement and adaptability in qualitative research, outperforming static systems in satisfaction and privacy.


<details>
  <summary>Details</summary>
Motivation: Existing automated interview systems lack flexibility, personalization, and contextual awareness, making them unsuitable for complex qualitative research.

Method: The study introduces an AI-powered interviewer using a large language model, real-time participant profiling, and a modular prompt engineering pipeline for adaptive and semantic-rich conversations.

Result: The system showed high satisfaction (4.45/5) and engagement (4.33/5) among test participants, demonstrating its effectiveness in qualitative data collection.

Conclusion: The AI interviewer is a scalable and privacy-conscious tool that advances qualitative research by enabling adaptive, context-aware, and engaging interviews.

Abstract: Automated interviewers and chatbots are common in research, recruitment, customer service, and education. Many existing systems use fixed question lists, strict rules, and limited personalization, leading to repeated conversations that cause low engagement. Therefore, these tools are not effective for complex qualitative research, which requires flexibility, context awareness, and ethical sensitivity. Consequently, there is a need for a more adaptive and context-aware interviewing system. To address this, an AI-powered interviewer that dynamically generates questions that are contextually appropriate and expertise aligned is presented in this study. The interviewer is built on a locally hosted large language model (LLM) that generates coherent dialogue while preserving data privacy. The interviewer profiles the participants' expertise in real time to generate knowledge-appropriate questions, well-articulated responses, and smooth transition messages similar to human-like interviews. To implement these functionalities, a modular prompt engineering pipeline was designed to ensure that the interview conversation remains scalable, adaptive, and semantically rich. To evaluate the AI-powered interviewer, it was tested with various participants, and it achieved high satisfaction (mean 4.45) and engagement (mean 4.33). The proposed interviewer is a scalable, privacy-conscious solution that advances AI-assisted qualitative data collection.

</details>


### [1118] [Augmented Assembly: Object Recognition and Hand Tracking for Adaptive Assembly Instructions in Augmented Reality](https://arxiv.org/abs/2601.11535)
*Alexander Htet Kyaw,Haotian Ma,Sasa Zivkovic,Jenny Sabin*

Main category: cs.HC

TL;DR: The paper introduces an AR-assisted assembly system that utilizes object recognition and hand tracking to streamline physical assembly tasks, providing real-time feedback and adaptive instructions.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency and intuitiveness of physical assembly tasks by bridging the gap between digital instructions and real-world interactions.

Method: The proposed system uses object recognition to detect and localize components in real-time, creating a digital twin of the workspace. It integrates AR overlays for step-by-step guidance and employs hand tracking to verify user interaction with components. Instructions adapt dynamically based on user deviations.

Result: Through a case study with LEGO and 3D-printed components, the system effectively links digital instructions with physical assembly, reducing manual effort by eliminating the need for searching, sorting, or labeling parts.

Conclusion: This AR-assisted workflow improves assembly efficiency, accommodates creative exploration, and highlights the potential for dynamic, user-adaptive systems in physical tasks.

Abstract: Recent advances in augmented reality (AR) have enabled interactive systems that assist users in physical assembly tasks. In this paper, we present an AR-assisted assembly workflow that leverages object recognition and hand tracking to (1) identify custom components, (2) display step-by-step instructions, (3) detect assembly deviations, and (4) dynamically update the instructions based on users' hands-on interactions with physical parts. Using object recognition, the system detects and localizes components in real time to create a digital twin of the workspace. For each assembly step, it overlays bounding boxes in AR to indicate both the current position and the target placement of relevant components, while hand-tracking data verifies whether the user interacts with the correct part. Rather than enforcing a fixed sequence, the system highlights potential assembly errors and interprets user deviations as opportunities for iteration and creative exploration. A case study with LEGO blocks and custom 3D-printed components demonstrates how the system links digital instructions to physical assembly, eliminating the need for manual searching, sorting, or labeling of parts.

</details>


### [1119] [A Comparative Study of Technical Writing Feedback Quality: Evaluating LLMs, SLMs, and Humans in Computer Science Topics](https://arxiv.org/abs/2601.11541)
*Suqing Liu,Bogdan Simion,Christopher Eaton,Michael Liut*

Main category: cs.HC

TL;DR: The study evaluates the quality of feedback from Large Language Models (LLMs), Small Language Models (SLMs), and humans in computer science courses. Results show a mix of strengths in AI and human feedback depending on course context.


<details>
  <summary>Details</summary>
Motivation: To assess the potential of AI feedback systems (LLMs & SLMs) in comparison to human feedback within computer science education, focusing on feedback quality and scalability.

Method: A mixed-methods approach analyzing student opinions on feedback quality (criteria: readability, detail, specificity, actionability, helpfulness) across three distinct course types using Likert-scale questions and qualitative commentary.

Result: LLMs and SLMs excel in clarity, breadth, and scalability in higher-enrollment courses, while human feedback is better at providing nuanced, context-specific, and personalized guidance, particularly in smaller classes.

Conclusion: Hybrid approaches combining AI and human feedback could offer scalable, efficient, and high-quality feedback, adapting strengths of both methods to context-specific needs.

Abstract: Feedback is a critical component of the learning process, particularly in computer science education. This study investigates the quality of feedback generated by Large Language Models (LLMs), Small Language Models (SLMs), compared with human feedback, in three computer science course with technical writing components: an introductory computer science course (CS2), a third-year advanced systems course (operating systems), and a third-year writing course (a topics course on artificial intelligence). Using a mixed-methods approach which integrates quantitative Likert-scale questions with qualitative commentary, we analyze the student perspective on feedback quality, evaluated based on multiple criteria, including readability, detail, specificity, actionability, helpfulness, and overall quality. The analysis reveals that in the larger upper-year operating systems course ($N=80$), SLMs and LLMs are perceived to deliver clear, actionable, and well-structured feedback, while humans provide more contextually nuanced guidance. As for the high-enrollment CS2 course ($N=176$) showed the same preference for the AI tools' clarity and breadth, but students noted that AI feedback sometimes lacked the concise, straight-to-the-point, guidance offered by humans. Conversely, in the smaller upper-year technical writing course on AI topics ($N=7$), all students preferred feedback from the course instructor, who was able to provide clear, specific, and personalized feedback, compared to the more general and less targeted AI-based feedback. We also highlight the scalability of AI-based feedback by focusing on its effectiveness at large scale. Our findings underscore the potential of hybrid approaches that combine AI and human feedback to achieve efficient and high-quality feedback at scale.

</details>


### [1120] [Medication counseling with large language models: balancing flexibility and rigidity](https://arxiv.org/abs/2601.11544)
*Joar Sabel,Mattias Wingren,Andreas Lundell,Sören Andersson,Sara Rosenberg,Susanne Hägglund,Linda Estman,Malin Andtfolk*

Main category: cs.HC

TL;DR: The paper discusses developing a prototype system using large language models (LLMs) for medication counseling, aiming to balance flexibility and determinism in conversational contexts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of creating a system capable of flexible yet error-free interactions in fields, like pharmacy, where errors can have serious consequences. The need exists for systems that are neither too rigid nor overly flexible.

Method: The authors present a prototype system for medication counseling, focusing on methods that ensure adherence to conversational requirements, minimize hallucinations, and maintain high-quality responses. The approach incorporates measures to increase determinism while leveraging LLMs' dynamic conversational abilities.

Result: The proposed system provides insights into the challenges of balancing conversational flexibility with determinism in a narrow, prolonged task setting. It highlights the need for methods to ensure quality and reliability in dynamic interactions.

Conclusion: The authors conclude that while their approach offers potential improvements, continuous testing, human-in-the-loop development, and evaluations beyond traditional LLM benchmarks are critical for creating a truly effective system.

Abstract: The introduction of large language models (LLMs) has greatly enhanced the capabilities of software agents. Instead of relying on rule-based interactions, agents can now interact in flexible ways akin to humans. However, this flexibility quickly becomes a problem in fields where errors can be disastrous, such as in a pharmacy context, but the opposite also holds true; a system that is too inflexible will also lead to errors, as it can become too rigid to handle situations that are not accounted for. Work using LLMs in a pharmacy context have adopted a wide scope, accounting for many different medications in brief interactions -- our strategy is the opposite: focus on a more narrow and long task. This not only enables a greater understanding of the task at hand, but also provides insight into what challenges are present in an interaction of longer nature. The main challenge, however, remains the same for a narrow and wide system: it needs to strike a balance between adherence to conversational requirements and flexibility. In an effort to strike such a balance, we present a prototype system meant to provide medication counseling while juggling these two extremes. We also cover our design in constructing such a system, with a focus on methods aiming to fulfill conversation requirements, reduce hallucinations and promote high-quality responses. The methods used have the potential to increase the determinism of the system, while simultaneously not removing the dynamic conversational abilities granted by the usage of LLMs. However, a great deal of work remains ahead, and the development of this kind of system needs to involve continuous testing and a human-in-the-loop. It should also be evaluated outside of commonly used benchmarks for LLMs, as these do not adequately capture the complexities of this kind of conversational system.

</details>


### [1121] [PASTA: A Scalable Framework for Multi-Policy AI Compliance Evaluation](https://arxiv.org/abs/2601.11702)
*Yu Yang,Ig-Jae Kim,Dongwook Yoon*

Main category: cs.HC

TL;DR: The paper introduces PASTA, a compliance tool to enhance scalable, multi-policy AI governance with promising results for cost-efficiency and usability.


<details>
  <summary>Details</summary>
Motivation: To address challenges posed by the rapid growth of AI policies that make compliance burdensome for AI practitioners without policy expertise.

Method: The tool integrates four key features: a comprehensive model-card format, policy normalization, an efficient LLM-powered evaluation engine, and a user-friendly interface with heatmaps and recommendations.

Result: PASTA demonstrated high alignment with human experts (ρ≥.626), evaluated five significant policies in under two minutes at low cost (~$3), and was found easy-to-use by practitioners in a user study.

Conclusion: PASTA offers a novel, scalable solution for automated AI compliance evaluation, facilitating cost-effective and actionable policy adherence.

Abstract: AI compliance is becoming increasingly critical as AI systems grow more powerful and pervasive. Yet the rapid expansion of AI policies creates substantial burdens for resource-constrained practitioners lacking policy expertise. Existing approaches typically address one policy at a time, making multi-policy compliance costly. We present PASTA, a scalable compliance tool integrating four innovations: (1) a comprehensive model-card format supporting descriptive inputs across development stages; (2) a policy normalization scheme; (3) an efficient LLM-powered pairwise evaluation engine with cost-saving strategies; and (4) an interface delivering interpretable evaluations via compliance heatmaps and actionable recommendations. Expert evaluation shows PASTA's judgments closely align with human experts ($ρ\geq .626$). The system evaluates five major policies in under two minutes at approximately \$3. A user study (N = 12) confirms practitioners found outputs easy-to-understand and actionable, introducing a novel framework for scalable automated AI governance.

</details>


### [1122] [Human-Human-AI Triadic Programming: Uncovering the Role of AI Agent and the Value of Human Partner in Collaborative Learning](https://arxiv.org/abs/2601.12134)
*Taufiq Daryanto,Xiaohan Ding,Kaike Ping,Lance T. Wilhelm,Yan Chen,Chris Brown,Eugenia H. Rho*

Main category: cs.HC

TL;DR: The paper explores human-human-AI (HHAI) collaboration in programming and finds that integrating AI as a collaborator (rather than replacement) enhances learning and presence within teams.


<details>
  <summary>Details</summary>
Motivation: To investigate how AI can play a role in enhancing collaborative programming by being an additional partner, rather than just replacing human-human collaboration.

Method: A within-subjects study with 20 participants was conducted to compare triadic HHAI collaboration with standard dyadic human-AI (HAI) setups.

Result: Triadic HHAI programming improved collaborative learning, reduced reliance on AI-generated code, and increased participants' sense of responsibility in understanding AI suggestions.

Conclusion: Integrating AI to augment peer collaboration fosters better learning by encouraging visible, accountable AI interactions within teams, preserving essential collaborative learning processes.

Abstract: As AI assistance becomes embedded in programming practice, researchers have increasingly examined how these systems help learners generate code and work more efficiently. However, these studies often position AI as a replacement for human collaboration and overlook the social and learning-oriented aspects that emerge in collaborative programming. Our work introduces human-human-AI (HHAI) triadic programming, where an AI agent serves as an additional collaborator rather than a substitute for a human partner. Through a within-subjects study with 20 participants, we show that triadic collaboration enhances collaborative learning and social presence compared to the dyadic human-AI (HAI) baseline. In the triadic HHAI conditions, participants relied significantly less on AI-generated code in their work. This effect was strongest in the HHAI-shared condition, where participants had an increased sense of responsibility to understand AI suggestions before applying them. These findings demonstrate how triadic settings activate socially shared regulation of learning by making AI use visible and accountable to a human peer, suggesting that AI systems that augment rather than automate peer collaboration can better preserve the learning processes that collaborative programming relies on.

</details>


### [1123] [PAIR-SAFE: A Paired-Agent Approach for Runtime Auditing and Refining AI-Mediated Mental Health Support](https://arxiv.org/abs/2601.12754)
*Jiwon Kim,Violeta J. Rodriguez,Dong Whi Yoo,Eshwar Chandrasekharan,Koustuv Saha*

Main category: cs.HC

TL;DR: The paper introduces PAIR-SAFE, a paired-agent framework for auditing and refining AI-generated mental health support responses.


<details>
  <summary>Details</summary>
Motivation: Address the risks of directive, inconsistent, or clinically misaligned responses from large language models used in mental health support.

Method: Developed PAIR-SAFE, integrating a Responder agent with a supervisory Judge agent based on the Motivational Interviewing Treatment Integrity (MITI-4) framework, which audits and refines responses using structured instructions.

Result: Judge-supervised interactions showed significant improvements in clinical dimensions such as partnership, collaboration, and relational quality, validated through both quantitative metrics and expert evaluations.

Conclusion: The paired-agent approach of PAIR-SAFE enhances the quality and clinical alignment of AI-generated mental health support responses, offering grounded auditing and refinement in sensitive contexts.

Abstract: Large language models (LLMs) are increasingly used for mental health support, yet they can produce responses that are overly directive, inconsistent, or clinically misaligned, particularly in sensitive or high-risk contexts. Existing approaches to mitigating these risks largely rely on implicit alignment through training or prompting, offering limited transparency and runtime accountability. We introduce PAIR-SAFE, a paired-agent framework for auditing and refining AI-generated mental health support that integrates a Responder agent with a supervisory Judge agent grounded in the clinically validated Motivational Interviewing Treatment Integrity (MITI-4) framework. The Judgeaudits each response and provides structuredALLOW or REVISE decisions that guide runtime response refinement. We simulate counseling interactions using a support-seeker simulator derived from human-annotated motivational interviewing data. We find that Judge-supervised interactions show significant improvements in key MITI dimensions, including Partnership, Seek Collaboration, and overall Relational quality. Our quantitative findings are supported by qualitative expert evaluation, which further highlights the nuances of runtime supervision. Together, our results reveal that such pairedagent approach can provide clinically grounded auditing and refinement for AI-assisted conversational mental health support.

</details>


### [1124] [Predictive Prototyping: Evaluating Design Concepts with ChatGPT](https://arxiv.org/abs/2601.12276)
*Hilsann Yong,Bradley A. Camburn*

Main category: cs.HC

TL;DR: This paper explores using GPT for predicting design prototypes information like cost, performance, and usability. Results show GPT's predictions and prototype insights outperform alternatives.


<details>
  <summary>Details</summary>
Motivation: Physical prototyping is slow and expensive, often limiting meaningful evaluation until late stages. The paper aims to test whether GPT can emulate design evaluation typically gained from prototyping.

Method: A retrieval-augmented generation (RAG) framework is introduced, leveraging GPT and design data from Instructables.com. Experiments compare GPT predictions against human designers and physical prototypes.

Result: GPT-RAG models achieve better cost and performance predictions and comparable usability assessments compared to human designers. A GPT-informed prototype outperformed alternatives in testing.

Conclusion: Pretrained transformers, especially augmented by retrieval methods, can simulate aspects of physical prototyping and support faster and resource-efficient innovation cycles.

Abstract: The design-build-test cycle is essential for innovation, but physical prototyping is often slow and expensive. Although physics-based simulation and strategic prototyping can reduce cost, meaningful evaluation is frequently constrained until an integrated prototype is built. This paper investigates whether a generative pretrained transformer (GPT) can predict information typically obtained through prototyping, including cost, performance, and perceived usability. We introduce a retrieval-augmented generation (RAG) method to emulate design feedback using OpenAI GPT-4o, grounded in prototyping data scraped from Instructables.com to increase access to relevant precedent. Two studies are reported. First, a controlled experiment compares GPT-RAG and human designers, who receive design sketches and predict cost, performance, and usability; predictions are evaluated against ground-truth results from physical prototypes. Second, we report an applied demonstration in which a physical prototype is produced from GPT-RAG recommendations and compared with a commercial baseline and a topology-optimized design. Results show that GPT-RAG provides more accurate cost and performance estimates than individual or crowd human estimates, while yielding comparable usability insights; the GPT-RAG-informed prototype also outperforms both comparison prototypes. Repeated querying with response averaging significantly improves accuracy, suggesting that LLMs can emulate crowd aggregation effects consistent with the law of large numbers.

</details>


### [1125] [RAGExplorer: A Visual Analytics System for the Comparative Diagnosis of RAG Systems](https://arxiv.org/abs/2601.12991)
*Haoyu Tian,Yingchaojie Feng,Zhen Wen,Haoxuan Li,Minfeng Zhu,Wei Chen*

Main category: cs.HC

TL;DR: The paper introduces RAGExplorer, a tool for analyzing and optimizing Retrieval-Augmented Generation (RAG) configurations through visual analytics, enabling developers to improve their system designs.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the complexity and opacity in configuring RAG systems, where performance depends on various modular choices, making it hard for developers to understand trade-offs and find the best designs.

Method: The authors propose RAGExplorer, a visual analytics system that uses a macro-to-micro workflow to help developers analyze performance across multiple RAG configurations, investigate failure cases, and test hypotheses by manipulating input contexts.

Result: RAGExplorer was validated through case and user studies, demonstrating its effectiveness in helping developers navigate and optimize RAG configuration spaces.

Conclusion: RAGExplorer is a powerful tool for RAG developers to systematically compare, diagnose, and optimize configurations, contributing to better-designed RAG systems. The tool and its source code are publicly available for broader adoption.

Abstract: The advent of Retrieval-Augmented Generation (RAG) has significantly enhanced the ability of Large Language Models (LLMs) to produce factually accurate and up-to-date responses. However, the performance of a RAG system is not determined by a single component but emerges from a complex interplay of modular choices, such as embedding models and retrieval algorithms. This creates a vast and often opaque configuration space, making it challenging for developers to understand performance trade-offs and identify optimal designs. To address this challenge, we present RAGExplorer, a visual analytics system for the systematic comparison and diagnosis of RAG configurations. RAGExplorer guides users through a seamless macro-to-micro analytical workflow. Initially, it empowers developers to survey the performance landscape across numerous configurations, allowing for a high-level understanding of which design choices are most effective. For a deeper analysis, the system enables users to drill down into individual failure cases, investigate how differences in retrieved information contribute to errors, and interactively test hypotheses by manipulating the provided context to observe the resulting impact on the generated answer. We demonstrate the effectiveness of RAGExplorer through detailed case studies and user studies, validating its ability to empower developers in navigating the complex RAG design space. Our code and user guide are publicly available at https://github.com/Thymezzz/RAGExplorer.

</details>


### [1126] [RubRIX: Rubric-Driven Risk Mitigation in Caregiver-AI Interactions](https://arxiv.org/abs/2601.13235)
*Drishti Goel,Jeongah Lee,Qiuyue Joy Zhong,Violeta J. Rodriguez,Daniel S. Brown,Ravi Karkar,Dong Whi Yoo,Koustuv Saha*

Main category: cs.HC

TL;DR: This paper introduces RubRIX, a framework to assess nuanced risks in AI responses to caregiving queries, focusing on user-centered evaluation and reducing risks in high-burden contexts.


<details>
  <summary>Details</summary>
Motivation: Existing AI evaluation methods focus on general risks but fail to address nuanced risks specific to caregiving contexts, necessitating a framework tailored for this domain.

Method: RubRIX, validated by clinicians, evaluates LLM risks via five dimensions: Inattention, Bias & Stigma, Information Inaccuracy, Uncritical Affirmation, and Epistemic Arrogance. It uses datasets of 20,000 caregiver queries for testing and refining responses.

Result: RubRIX reduced risk components in AI responses by 45-98% across six LLMs after refinement and offered benchmark datasets for further research.

Conclusion: RubRIX provides a domain-sensitive method for risk evaluation and emphasizes the need for interactional risk assessment in deploying AI for caregiving support responsibly.

Abstract: Caregivers seeking AI-mediated support express complex needs -- information-seeking, emotional validation, and distress cues -- that warrant careful evaluation of response safety and appropriateness. Existing AI evaluation frameworks, primarily focused on general risks (toxicity, hallucinations, policy violations, etc), may not adequately capture the nuanced risks of LLM-responses in caregiving-contexts. We introduce RubRIX (Rubric-based Risk Index), a theory-driven, clinician-validated framework for evaluating risks in LLM caregiving responses. Grounded in the Elements of an Ethic of Care, RubRIX operationalizes five empirically-derived risk dimensions: Inattention, Bias & Stigma, Information Inaccuracy, Uncritical Affirmation, and Epistemic Arrogance. We evaluate six state-of-the-art LLMs on over 20,000 caregiver queries from Reddit and ALZConnected. Rubric-guided refinement consistently reduced risk-components by 45-98% after one iteration across models. This work contributes a methodological approach for developing domain-sensitive, user-centered evaluation frameworks for high-burden contexts. Our findings highlight the importance of domain-sensitive, interactional risk evaluation for the responsible deployment of LLMs in caregiving support contexts. We release benchmark datasets to enable future research on contextual risk evaluation in AI-mediated support.

</details>


### [1127] [HCFT: Hierarchical Convolutional Fusion Transformer for EEG Decoding](https://arxiv.org/abs/2601.12279)
*Haodong Zhang,Jiapeng Zhu,Yitong Chen,Hongqi Li*

Main category: cs.HC

TL;DR: This paper introduces the HCFT framework, an advanced EEG decoding model, which integrates convolutional branches and hierarchical Transformers for multi-scale representation. HCFT outperforms state-of-the-art models in two benchmark tests, proving its robustness and effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extracting and integrating complex temporal, spectral, and spatial features from multichannel EEG signals for tasks such as cross-subject classification and seizure prediction.

Method: The HCFT framework combines dual-branch convolutional encoders (time-domain and spatiotemporal branches), cross-attention mechanisms, and a hierarchical Transformer fusion structure with a new Dynamic Tanh normalization module.

Result: HCFT achieved 80.83% accuracy and 0.6165 Cohen's kappa on the BCI IV-2b dataset, and 99.10% sensitivity, 0.0236 false positives per hour, and 98.82% specificity on CHB-MIT, outperforming over ten baseline methods.

Conclusion: HCFT effectively captures global and local EEG dynamics, demonstrating its potential for real-world brain-computer interface applications through superior performance and training stability.

Abstract: Electroencephalography (EEG) decoding requires models that can effectively extract and integrate complex temporal, spectral, and spatial features from multichannel signals. To address this challenge, we propose a lightweight and generalizable decoding framework named Hierarchical Convolutional Fusion Transformer (HCFT), which combines dual-branch convolutional encoders and hierarchical Transformer blocks for multi-scale EEG representation learning. Specifically, the model first captures local temporal and spatiotemporal dynamics through time-domain and time-space convolutional branches, and then aligns these features via a cross-attention mechanism that enables interaction between branches at each stage. Subsequently, a hierarchical Transformer fusion structure is employed to encode global dependencies across all feature stages, while a customized Dynamic Tanh normalization module is introduced to replace traditional Layer Normalization in order to enhance training stability and reduce redundancy. Extensive experiments are conducted on two representative benchmark datasets, BCI Competition IV-2b and CHB-MIT, covering both event-related cross-subject classification and continuous seizure prediction tasks. Results show that HCFT achieves 80.83% average accuracy and a Cohen's kappa of 0.6165 on BCI IV-2b, as well as 99.10% sensitivity, 0.0236 false positives per hour, and 98.82% specificity on CHB-MIT, consistently outperforming over ten state-of-the-art baseline methods. Ablation studies confirm that each core component of the proposed framework contributes significantly to the overall decoding performance, demonstrating HCFT's effectiveness in capturing EEG dynamics and its potential for real-world BCI applications.

</details>


### [1128] [Do MLLMs See What We See? Analyzing Visualization Literacy Barriers in AI Systems](https://arxiv.org/abs/2601.12585)
*Mengli,Duan,Yuhe,Jiang,Matthew Varona,Carolina Nobre*

Main category: cs.HC

TL;DR: The paper analyzes why multimodal large language models (MLLMs) fail at interpreting visualizations, through a systematic study using a tailored benchmark.


<details>
  <summary>Details</summary>
Motivation: To identify the reasons behind MLLMs' failures in visualization interpretation and improve future AI visualization capabilities.

Method: A systematic barrier-centric analysis of errors using a Visualization Literacy Assessment Test (reVLAT) benchmark with synthetic data, coding failures from state-of-the-art models.

Result: Taxonomy of MLLM failures was developed, revealing machine-specific barriers and identifying struggles with complex and color-intensive chart interpretations.

Conclusion: Future AI-driven visualization systems should address gaps in comparative reasoning and literacy challenges identified in MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) are increasingly used to interpret visualizations, yet little is known about why they fail. We present the first systematic analysis of barriers to visualization literacy in MLLMs. Using the regenerated Visualization Literacy Assessment Test (reVLAT) benchmark with synthetic data, we open-coded 309 erroneous responses from four state-of-the-art models with a barrier-centric strategy adapted from human visualization literacy research. Our analysis yields a taxonomy of MLLM failures, revealing two machine-specific barriers that extend prior human-participation frameworks. Results show that models perform well on simple charts but struggle with color-intensive, segment-based visualizations, often failing to form consistent comparative reasoning. Our findings inform future evaluation and design of reliable AI-driven visualization assistants.

</details>


### [1129] [Creating Disability Story Videos with Generative AI: Motivation, Expression, and Sharing](https://arxiv.org/abs/2601.12617)
*Shuo Niu,Dylan Clements,Hyungsin Kim*

Main category: cs.HC

TL;DR: This paper studies how generative AI can help individuals with disabilities create and share stories about their experiences, identifying its benefits and challenges.


<details>
  <summary>Details</summary>
Motivation: To explore how generative AI can reduce barriers and inspire creativity for people with disabilities in media production, while addressing potential biases.

Method: Nine participants from a disability advocacy group used generative AI to create videos based on their disability experiences, guided by digital storytelling theory.

Result: The study identifies four key affordances of generative AI for disability storytelling: non-capturable depiction, identity concealment and representation, contextual realism, and emotional articulation.

Conclusion: The authors propose a framework named momentous depiction and suggest design improvements for generative AI to better support personal and creative storytelling by people with disabilities.

Abstract: Generative AI (GenAI) is both promising and challenging in supporting people with disabilities (PwDs) in creating stories about disability. GenAI can reduce barriers to media production and inspire the creativity of PwDs, but it may also introduce biases and imperfections that hinder its adoption for personal expression. In this research, we examine how nine PwD from a disability advocacy group used GenAI to create videos sharing their disability experiences. Grounded in digital storytelling theory, we explore the motivations, expression, and sharing of PwD-created GenAI story videos. We conclude with a framework of momentous depiction, which highlights four core affordances of GenAI that either facilitate or require improvements to better support disability storytelling: non-capturable depiction, identity concealment and representation, contextual realism and consistency, and emotional articulation. Based on this framework, we further discuss design implications for GenAI in relation to story completion, media formats, and corrective mechanisms.

</details>


### [1130] [AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations](https://arxiv.org/abs/2601.12727)
*Jingshu Li,Tianqi Song,Nattapat Boonprakong,Zicheng Zhu,Yitian Yang,Yi-Chieh Lee*

Main category: cs.HC

TL;DR: The paper examines how users' self-concepts align with AI chatbot personality traits over conversations, finding increased alignment and user enjoyment but raising ethical concerns.


<details>
  <summary>Details</summary>
Motivation: To investigate whether interactions with AI that exhibit personality traits could shape and potentially bias users' self-concept of their own traits, given prior knowledge that personality perceptions are influenced by social interactions.

Method: A randomized behavioral experiment was conducted where users interacted with an LLM-based AI chatbot (GPT-4) with default personality traits, and their self-concept alignment with the AI's traits was measured.

Result: Users' self-concepts aligned with the AI chatbot's personality traits, with stronger alignment observed for longer conversations. This increased homogeneity in self-concepts among users and positive correlation with conversational enjoyment.

Conclusion: The study highlights that AI personality traits can influence users’ self-concepts during interactions. It emphasizes the need for responsible design of AI systems to mitigate risks and leverage opportunities.

Abstract: Recent Large Language Model (LLM) based AI can exhibit recognizable and measurable personality traits during conversations to improve user experience. However, as human understandings of their personality traits can be affected by their interaction partners' traits, a potential risk is that AI traits may shape and bias users' self-concept of their own traits. To explore the possibility, we conducted a randomized behavioral experiment. Our results indicate that after conversations about personal topics with an LLM-based AI chatbot using GPT-4o default personality traits, users' self-concepts aligned with the AI's measured personality traits. The longer the conversation, the greater the alignment. This alignment led to increased homogeneity in self-concepts among users. We also observed that the degree of self-concept alignment was positively associated with users' conversation enjoyment. Our findings uncover how AI personality traits can shape users' self-concepts through human-AI conversation, highlighting both risks and opportunities. We provide important design implications for developing more responsible and ethical AI systems.

</details>


### [1131] [TreeWriter: AI-Assisted Hierarchical Planning and Writing for Long-Form Documents](https://arxiv.org/abs/2601.12740)
*Zijian Zhang,Fangshi Du,Xingjian Liu,Pan Chen,Oliver Huang,Runlong Ye,Michael Liut,Alán Aspuru-Guzik*

Main category: cs.HC

TL;DR: TreeWriter, a hierarchical AI writing system, improves idea development, control, and collaboration in long documents by integrating tree-structured organization and contextual AI support.


<details>
  <summary>Details</summary>
Motivation: To address challenges of maintaining consistency, efficient planning, and integrating AI assistance while writing long, complex documents.

Method: Developed TreeWriter, representing documents as hierarchical trees, with AI context-aware suggestions, and tested it via both a study (N=12) and field deployment (N=8).

Result: TreeWriter enhanced idea exploration, perceived authorial control, and AI helpfulness compared to traditional tools like Google Docs + Gemini. Hierarchical organization facilitated collaborative writing.

Conclusion: TreeWriter demonstrates the value of tree-structured editors with integrated AI for better balancing automation and user control, offering guidelines for future writing tools.

Abstract: Long documents pose many challenges to current intelligent writing systems. These include maintaining consistency across sections, sustaining efficient planning and writing as documents become more complex, and effectively providing and integrating AI assistance to the user. Existing AI co-writing tools offer either inline suggestions or limited structured planning, but rarely support the entire writing process that begins with high-level ideas and ends with polished prose, in which many layers of planning and outlining are needed. Here, we introduce TreeWriter, a hierarchical writing system that represents documents as trees and integrates contextual AI support. TreeWriter allows authors to create, save, and refine document outlines at multiple levels, facilitating drafting, understanding, and iterative editing of long documents. A built-in AI agent can dynamically load relevant content, navigate the document hierarchy, and provide context-aware editing suggestions. A within-subject study (N=12) comparing TreeWriter with Google Docs + Gemini on long-document editing and creative writing tasks shows that TreeWriter improves idea exploration/development, AI helpfulness, and perceived authorial control. A two-month field deployment (N=8) further demonstrated that hierarchical organization supports collaborative writing. Our findings highlight the potential of hierarchical, tree-structured editors with integrated AI support and provide design guidelines for future AI-assisted writing tools that balance automation with user agency.

</details>


### [1132] [Multimodal Feedback for Handheld Tool Guidance: Combining Wrist-Based Haptics with Augmented Reality](https://arxiv.org/abs/2601.12037)
*Yue Yang,Christoph Leuze,Brian Hargreaves,Bruce Daniel,Fred M Baik*

Main category: cs.HC

TL;DR: The paper explores the use of vibrotactile wrist feedback to enhance spatial guidance during handheld tool usage in AR, showing improved precision and usability when combined.


<details>
  <summary>Details</summary>
Motivation: To address visual occlusion, lighting variability, and ambiguity in AR-supported surgical tasks, which affect precision and confidence.

Method: Developed a multimodal AR coupled with a custom haptic wrist device delivering vibration cues. Conducted studies to design and test the cues with surgeons and participants.

Result: Combined AR and haptic systems improved spatial precision (5.8mm) and usability (SUS = 88.1) over single-modality systems, though with slight increases in task time.

Conclusion: Integrating wrist-based haptics with AR systems is promising for precise and visually complex tasks like surgical guidance, improving user confidence and efficiency.

Abstract: We investigate how vibrotactile wrist feedback can enhance spatial guidance for handheld tool movement in optical see-through augmented reality (AR). While AR overlays are widely used to support surgical tasks, visual occlusion, lighting conditions, and interface ambiguity can compromise precision and confidence. To address these challenges, we designed a multimodal system combining AR visuals with a custom wrist-worn haptic device delivering directional and state-based cues. A formative study with experienced surgeons and residents identified key tool maneuvers and preferences for reference mappings, guiding our cue design. In a cue identification experiment (N=21), participants accurately recognized five vibration patterns under visual load, with higher recognition for full-actuator states than spatial direction cues. In a guidance task (N=27), participants using both AR and haptics achieved significantly higher spatial precision (5.8 mm) and usability (SUS = 88.1) than those using either modality alone, despite having modest increases in task time. Participants reported that haptic cues provided reassuring confirmation and reduced cognitive effort during alignment. Our results highlight the promise of integrating wrist-based haptics into AR systems for high-precision, visually complex tasks such as surgical guidance. We discuss design implications for multimodal interfaces supporting confident, efficient tool manipulation.

</details>


### [1133] [Breaking Coordinate Overfitting: Geometry-Aware WiFi Sensing for Cross-Layout 3D Pose Estimation](https://arxiv.org/abs/2601.12252)
*Songming Jia,Yan Lu,Bin Liu,Xiang Zhang,Peng Zhao,Xinmeng Tang,Yelin Wei,Jinyang Huang,Huan Yan,Zhi Liu*

Main category: cs.HC

TL;DR: This paper introduces PerceptAlign, a geometry-conditioned framework for WiFi-based 3D human pose estimation, tackling generalization issues caused by existing methods.


<details>
  <summary>Details</summary>
Motivation: Current WiFi-based 3D human pose estimation systems suffer from generalization failures due to coordinate overfitting, where models memorize specific transceiver layouts rather than learning activity-relevant representations.

Method: The researchers propose PerceptAlign, which aligns WiFi and vision data into a shared 3D space through a lightweight coordinate unification process. It encodes and embeds transceiver positions and CSI features together, explicitly utilizing device geometry to improve generalization.

Result: PerceptAlign reduces in-domain errors by 12.3% and cross-domain errors by over 60% against state-of-the-art baselines, demonstrating improved robustness and layout invariance.

Conclusion: PerceptAlign establishes geometry-conditioned learning as a promising solution for scalable and practical WiFi-based 3D human pose estimation.

Abstract: WiFi-based 3D human pose estimation offers a low-cost and privacy-preserving alternative to vision-based systems for smart interaction. However, existing approaches rely on visual 3D poses as supervision and directly regress CSI to a camera-based coordinate system. We find that this practice leads to coordinate overfitting: models memorize deployment-specific WiFi transceiver layouts rather than only learning activity-relevant representations, resulting in severe generalization failures. To address this challenge, we present PerceptAlign, the first geometry-conditioned framework for WiFi-based cross-layout pose estimation. PerceptAlign introduces a lightweight coordinate unification procedure that aligns WiFi and vision measurements in a shared 3D space using only two checkerboards and a few photos. Within this unified space, it encodes calibrated transceiver positions into high-dimensional embeddings and fuses them with CSI features, making the model explicitly aware of device geometry as a conditional variable. This design forces the network to disentangle human motion from deployment layouts, enabling robust and, for the first time, layout-invariant WiFi pose estimation. To support systematic evaluation, we construct the largest cross-domain 3D WiFi pose estimation dataset to date, comprising 21 subjects, 5 scenes, 18 actions, and 7 device layouts. Experiments show that PerceptAlign reduces in-domain error by 12.3% and cross-domain error by more than 60% compared to state-of-the-art baselines. These results establish geometry-conditioned learning as a viable path toward scalable and practical WiFi sensing.

</details>


### [1134] [The AI Genie Phenomenon and Three Types of AI Chatbot Addiction: Escapist Roleplays, Pseudosocial Companions, and Epistemic Rabbit Holes](https://arxiv.org/abs/2601.13348)
*M. Karen Shen,Jessica Huang,Olivia Liang,Ig-Jae Kim,Dongwook Yoon*

Main category: cs.HC

TL;DR: The paper investigates AI chatbot addiction by analyzing Reddit entries, revealing reasons, symptoms, types, and recovery strategies.


<details>
  <summary>Details</summary>
Motivation: To address the poorly understood area of AI chatbot addiction and to minimize associated risks.

Method: The study uses thematic and exploratory data analysis on 334 Reddit entries across 14 subreddits.

Result: Findings include addiction driven by 'AI Genie' phenomenon, identification of three addiction types, involvement of sexual content, and varying recovery strategies.

Conclusion: The study provides empirical insights to aid prevention, diagnosis, and intervention strategies for AI chatbot addiction.

Abstract: Recent reports on generative AI chatbot use raise concerns about its addictive potential. An in-depth understanding is imperative to minimize risks, yet AI chatbot addiction remains poorly understood. This study examines how to characterize AI chatbot addiction--why users become addicted, the symptoms commonly reported, and the distinct types it comprises. We conducted a thematic analysis of Reddit entries (n=334) across 14 subreddits where users narrated their experiences with addictive AI chatbot use, followed by an exploratory data analysis. We found: (1) users' dependence tied to the "AI Genie" phenomenon--users can get exactly anything they want with minimal effort--and marked by symptoms that align with addiction literature, (2) three distinct addiction types: Escapist Roleplay, Pseudosocial Companion, and Epistemic Rabbit Hole, (3) sexual content involved in multiple cases, and (4) recovery strategies' perceived helpfulness differ between addiction types. Our work lays empirical groundwork to inform future strategies for prevention, diagnosis, and intervention.

</details>


### [1135] [Integrating Virtual Reality and Large Language Models for Team-Based Non-Technical Skills Training and Evaluation in the Operating Room](https://arxiv.org/abs/2601.13406)
*Jacob Barker,Doga Demirel,Cullen Jackson,Anna Johansson,Robbin Miraglia,Darian Hoagland,Stephanie B. Jones,John Mitchell,Daniel B. Jones,Suvranu De*

Main category: cs.HC

TL;DR: The paper introduces VORTeX, a virtual reality (VR) platform using large language models (LLMs) to train and evaluate teamwork and communication in surgical emergencies.


<details>
  <summary>Details</summary>
Motivation: The lack of structured training and assessment tools for non-technical skills (NTS) like communication and teamwork during surgical emergencies inspired the development of an innovative solution.

Method: The study introduced VORTeX, integrating immersive team simulations in VR with LLM-based analytics to measure communication and teamwork dynamics based on the NOTSS framework.

Result: Pilot sessions with 12 professionals demonstrated VORTeX's effectiveness in fostering teamwork and generating communication structures that reflected expected operative hierarchies.

Conclusion: This VR platform with integrated LLM analytics serves as an efficient, scalable, and privacy-compliant solution for improving non-technical skills in surgical teams, enabling objective data-supported training.

Abstract: Although effective teamwork and communication are critical to surgical safety, structured training for non-technical skills (NTS) remains limited compared with technical simulation. The ACS/APDS Phase III Team-Based Skills Curriculum calls for scalable tools that both teach and objectively assess these competencies during laparoscopic emergencies. We introduce the Virtual Operating Room Team Experience (VORTeX), a multi-user virtual reality (VR) platform that integrates immersive team simulation with large language model (LLM) analytics to train and evaluate communication, decision-making, teamwork, and leadership. Team dialogue is analyzed using structured prompts derived from the Non-Technical Skills for Surgeons (NOTSS) framework, enabling automated classification of behaviors and generation of directed interaction graphs that quantify communication structure and hierarchy. Two laparoscopic emergency scenarios, pneumothorax and intra-abdominal bleeding, were implemented to elicit realistic stress and collaboration. Twelve surgical professionals completed pilot sessions at the 2024 SAGES conference, rating VORTeX as intuitive, immersive, and valuable for developing teamwork and communication. The LLM consistently produced interpretable communication networks reflecting expected operative hierarchies, with surgeons as central integrators, nurses as initiators, and anesthesiologists as balanced intermediaries. By integrating immersive VR with LLM-driven behavioral analytics, VORTeX provides a scalable, privacy-compliant framework for objective assessment and automated, data-informed debriefing across distributed training environments.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [1136] [Scaling laws for amplitude surrogates](https://arxiv.org/abs/2601.13308)
*Henning Bahl,Victor Bresó-Pla,Anja Butter,Joaquín Iturriza Ramirez*

Main category: hep-ph

TL;DR: The paper explores scaling laws in neural networks for particle physics, linking coefficients to the number of external particles and emphasizing their utility in precision targets.


<details>
  <summary>Details</summary>
Motivation: To investigate how scaling laws of neural networks apply to amplitude surrogates in particle physics and explore their practical benefits in achieving precision.

Method: Systematic investigation of scaling laws by analyzing their dependence on particle physics parameters, especially focusing on the number of external particles.

Result: Revealed that scaling coefficients are related to the number of external particles and highlighted the utility of scaling laws for precision.

Conclusion: Scaling laws provide a reliable framework to guide neural network performance optimizations in particle physics applications.

Abstract: Scaling laws describing the dependence of neural network performance on the amount of training data, the spent compute, and the network size have emerged across a huge variety of machine learning task and datasets. In this work, we systematically investigate these scaling laws in the context of amplitude surrogates for particle physics. We show that the scaling coefficients are connected to the number of external particles of the process. Our results demonstrate that scaling laws are a useful tool to achieve desired precision targets.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [1137] [Breaking the Data Barrier in Learning Symbolic Computation: A Case Study on Variable Ordering Suggestion for Cylindrical Algebraic Decomposition](https://arxiv.org/abs/2601.13731)
*Rui-Juan Jing,Yuegang Zhao,Changbo Chen*

Main category: cs.SC

TL;DR: The paper tackles symbolic computation efficiency by proposing a machine learning method to improve cylindrical algebraic decomposition (CAD) variable ordering, outperforming heuristic-based approaches.


<details>
  <summary>Details</summary>
Motivation: Symbolic computation, like the CAD method, suffers from inefficiencies caused by deep computations in high dimensions, limited by the challenges in acquiring labelled data for machine learning acceleration.

Method: The researchers created multiple interconnected tasks to generate labeled data and pre-trained a Transformer model, refining it on CAD ordering-specific datasets.

Result: The model’s predictions for variable ordering in CAD outperformed the best heuristic methods on public datasets.

Conclusion: Integrating deep learning with symbolic computation significantly improves CAD efficiency and surpasses traditional expert-based heuristics.

Abstract: Symbolic computation, powered by modern computer algebra systems, has important applications in mathematical reasoning through exact deep computations. The efficiency of symbolic computation is largely constrained by such deep computations in high dimension. This creates a fundamental barrier on labelled data acquisition if leveraging supervised deep learning to accelerate symbolic computation. Cylindrical algebraic decomposition (CAD) is a pillar symbolic computation method for reasoning with first-order logic formulas over reals with many applications in formal verification and automatic theorem proving. Variable orderings have a huge impact on its efficiency. Impeded by the difficulty to acquire abundant labelled data, existing learning-based approaches are only competitive with the best expert-based heuristics. In this work, we address this problem by designing a series of intimately connected tasks for which a large amount of annotated data can be easily obtained. We pre-train a Transformer model with these data and then fine-tune it on the datasets for CAD ordering. Experiments on publicly available CAD ordering datasets show that on average the orderings predicted by the new model are significantly better than those suggested by the best heuristic methods.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [1138] [Multifaceted Scenario-Aware Hypergraph Learning for Next POI Recommendation](https://arxiv.org/abs/2601.11610)
*Yuxi Lin,Yongkang Li,Jie Xing,Zipei Fan*

Main category: cs.SI

TL;DR: The paper introduces MSAHG, a scenario-aware framework improving Next POI recommendations by addressing mobility variations and conflicts across contextual scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing methods for Next POI recommendations overlook scenario-specific mobility patterns and conflicts, leading to suboptimal performance.

Method: MSAHG employs scenario-specific sub-hypergraphs and a parameter-splitting mechanism to resolve inter-scenario conflicts while preserving generalization.

Result: MSAHG outperformed five state-of-the-art methods in experiments on three real-world datasets, proving its effectiveness.

Conclusion: MSAHG effectively addresses inherent limitations in multi-scenario POI recommendations and showcases superior performance over existing models.

Abstract: Among the diverse services provided by Location-Based Social Networks (LBSNs), Next Point-of-Interest (POI) recommendation plays a crucial role in inferring user preferences from historical check-in trajectories. However, existing sequential and graph-based methods frequently neglect significant mobility variations across distinct contextual scenarios (e.g., tourists versus locals). This oversight results in suboptimal performance due to two fundamental limitations: the inability to capture scenario-specific features and the failure to resolve inherent inter-scenario conflicts. To overcome these limitations, we propose the Multifaceted Scenario-Aware Hypergraph Learning method (MSAHG), a framework that adopts a scenario-splitting paradigm for next POI recommendation.
  Our main contributions are:
  (1) Construction of scenario-specific, multi-view disentangled sub-hypergraphs to capture distinct mobility patterns;
  (2) A parameter-splitting mechanism to adaptively resolve conflicting optimization directions across scenarios while preserving generalization capability.
  Extensive experiments on three real-world datasets demonstrate that MSAHG consistently outperforms five state-of-the-art methods across diverse scenarios, confirming its effectiveness in multi-scenario POI recommendation.

</details>


### [1139] [The Hidden Toll of Social Media News: Causal Effects on Psychosocial Wellbeing](https://arxiv.org/abs/2601.13487)
*Olivia Pal,Agam Goyal,Eshwar Chandrasekharan,Koustuv Saha*

Main category: cs.SI

TL;DR: This paper studies the psychological impacts of news engagement on social media, revealing both positive and negative outcomes based on engagement types.


<details>
  <summary>Details</summary>
Motivation: There is limited understanding of how different types of news engagement on social media impact psychosocial outcomes, creating a need to explore this gap.

Method: Using a large dataset from the BlueSky platform, the paper employs a quasi-experimental study with stratified propensity score analysis to compare treated and control users.

Result: The study shows increased depression, stress, and anxiety, but also decreased loneliness and greater social interaction. Bookmarking news has the most detrimental effects compared to commenting or quoting.

Conclusion: Routine engagement with news on social media leads to diverse psychological effects, dependent on engagement type, and highlights the need for interventions to reduce the psychosocial costs.

Abstract: News consumption on social media has become ubiquitous, yet how different forms of engagement shape psychosocial outcomes remains unclear. To address this gap, we leveraged a large-scale dataset of ~26M posts and ~45M comments on the BlueSky platform, and conducted a quasi-experimental study, matching 81,345 Treated users exposed to News feeds with 83,711 Control users using stratified propensity score analysis. We examined psychosocial wellbeing, in terms of affective, behavioral, and cognitive outcomes. Our findings reveal that news engagement produces systematic trade-offs: increased depression, stress, and anxiety, yet decreased loneliness and increased social interaction on the platform. Regression models reveal that News feed bookmarking is associated with greater psychosocial deterioration compared to commenting or quoting, with magnitude differences exceeding tenfold. These per-engagement effects accumulate with repeated exposure, showing significant psychosocial impacts. Our work extends theories of news effects beyond crisis-centric frameworks by demonstrating that routine consumption creates distinct psychological dynamics depending on engagement type, and bears implications for tools and interventions for mitigating the psychosocial costs of news consumption on social media.

</details>


### [1140] [The Tag is the Signal: URL-Agnostic Credibility Scoring for Messages on Telegram](https://arxiv.org/abs/2601.13294)
*Yipeng Wang,Huy Gia Han Vu,Mohit Singhal*

Main category: cs.SI

TL;DR: TAG2CRED is a pipeline for assessing credibility of short, URL-sparse Telegram messages using a tagging system mapped to risk scores, outperforming baseline models in accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail in evaluating short, URL-sparse posts on platforms like Telegram, where misinformation is prevalent. This study seeks to develop a method better suited to such content.

Method: Proposed the TAG2CRED pipeline, which tags messages based on theme, claim type, call to action, and evidence, applies L2-regularized logistic regression to map tags to risk scores, and fine-tuned an LLM for tagging.

Result: TAG2CRED achieved superior predictive performance over traditional methods with ROC-AUC of 0.871 and macro-F1 value of 0.787. A stacked model combining TAG2CRED, TF-IDF, and SBERT further increased ROC-AUC to 0.901 and macro-F1 to 0.813.

Conclusion: TAG2CRED effectively assesses the credibility of Telegram messages with fewer features and strong generalization ability, particularly capturing information risk dimensions inadequately addressed by conventional models.

Abstract: Telegram has become one of the leading platforms for disseminating misinformational messages. However, many existing pipelines still classify each message's credibility based on the reputation of its associated domain names or its lexical features. Such methods work well on traditional long-form news articles published by well-known sources, but high-risk posts on Telegram are short and URL-sparse, leading to failures for link-based and standard TF-IDF models. To this end, we propose the TAG2CRED pipeline, a method designed for such short, convoluted messages. Our model will directly score each post based on the tags assigned to the text. We designed a concise label system that covers the dimensions of theme, claim type, call to action, and evidence. The fine-tuned large language model (LLM) assigns tags to messages and then maps these tags to calibrated risk scores in the [0,1] interval through L2-regularized logistic regression. We evaluated 87,936 Telegram messages associated with Media Bias/Fact Check (MBFC), using URL masking and domain disjoint splits. The results showed that the ROC-AUC of the TAG2CRED model reached 0.871, the macro-F1 value was 0.787, and the Brier score was 0.167, outperforming the baseline TF-IDF (macro-F1 value 0.737, Brier score 0.248); at the same time, the number of features used in this model is much smaller, and the generalization ability on infrequent domains is stronger. The performance of the stacked ensemble model (TF-IDF + TAG2CRED + SBERT) was further improved over the baseline SBERT. ROC-AUC reached 0.901, and the macro-F1 value was 0.813 (Brier score 0.114). This indicates that style labels and lexical features may capture different but complementary dimensions of information risk.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [1141] [Offline Policy Learning with Weight Clipping and Heaviside Composite Optimization](https://arxiv.org/abs/2601.12117)
*Jingren Liu,Hanzhang Qin,Junyi Liu,Mabel C. Chou,Jong-Shi Pang*

Main category: math.OC

TL;DR: The paper introduces an offline policy learning algorithm using weight-clipping to reduce variance caused by small propensity scores, proposing a computational framework to improve estimation and policy optimization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve offline policy learning methods by addressing the high variance problem of reweighting-based estimators when propensity scores are small, which can impair policy value estimation and optimization.

Method: The method involves using a weight-clipping estimator to truncate small propensity scores, reformulating policy optimization as a Heaviside composite optimization problem, and solving it efficiently with progressive integer programming.

Result: The algorithm reduces the mean squared error (MSE) in policy value estimation, leading to improved policy learning performance and establishing an upper bound for the suboptimality of the algorithm.

Conclusion: Weight-clipping allows for practical and efficient offline policy learning by addressing high variance issues in estimation, improving robustness in learned policies and their optimization outcomes.

Abstract: Offline policy learning aims to use historical data to learn an optimal personalized decision rule. In the standard estimate-then-optimize framework, reweighting-based methods (e.g., inverse propensity weighting or doubly robust estimators) are widely used to produce unbiased estimates of policy values. However, when the propensity scores of some treatments are small, these reweighting-based methods suffer from high variance in policy value estimation, which may mislead the downstream policy optimization and yield a learned policy with inferior value. In this paper, we systematically develop an offline policy learning algorithm based on a weight-clipping estimator that truncates small propensity scores via a clipping threshold chosen to minimize the mean squared error (MSE) in policy value estimation. Focusing on linear policies, we address the bilevel and discontinuous objective induced by weight-clipping-based policy optimization by reformulating the problem as a Heaviside composite optimization problem, which provides a rigorous computational framework. The reformulated policy optimization problem is then solved efficiently using the progressive integer programming method, making practical policy learning tractable. We establish an upper bound for the suboptimality of the proposed algorithm, which reveals how the reduction in MSE of policy value estimation, enabled by our proposed weight-clipping estimator, leads to improved policy learning performance.

</details>


### [1142] [BiCoLoR: Communication-Efficient Optimization with Bidirectional Compression and Local Training](https://arxiv.org/abs/2601.12400)
*Laurent Condat,Artavazd Maranjyan,Peter Richtárik*

Main category: math.OC

TL;DR: BiCoLoR is a communication-efficient optimization algorithm for distributed learning that improves efficiency by using both local training and bidirectional compression.


<details>
  <summary>Details</summary>
Motivation: Address the communication bottleneck in distributed optimization, particularly in federated learning over wireless networks.

Method: Combines local training with bidirectional compression using unbiased compressors to enhance communication efficiency, applied to both uplink and downlink.

Result: Achieves accelerated complexity guarantees in convex and strongly convex heterogeneous settings; outperforms existing algorithms in terms of communication efficiency.

Conclusion: BiCoLoR sets a new standard in communication-efficient distributed optimization by jointly designing local training and bidirectional compression.

Abstract: Slow and costly communication is often the main bottleneck in distributed optimization, especially in federated learning where it occurs over wireless networks. We introduce BiCoLoR, a communication-efficient optimization algorithm that combines two widely used and effective strategies: local training, which increases computation between communication rounds, and compression, which encodes high-dimensional vectors into short bitstreams. While these mechanisms have been combined before, compression has typically been applied only to uplink (client-to-server) communication, leaving the downlink (server-to-client) side unaddressed. In practice, however, both directions are costly. We propose BiCoLoR, the first algorithm to combine local training with bidirectional compression using arbitrary unbiased compressors. This joint design achieves accelerated complexity guarantees in both convex and strongly convex heterogeneous settings. Empirically, BiCoLoR outperforms existing algorithms and establishes a new standard in communication efficiency.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [1143] [Classifiers in High Dimensional Hilbert Metrics](https://arxiv.org/abs/2601.13410)
*Aditya Acharya,Auguste H. Gezalyan,David M. Mount*

Main category: cs.CG

TL;DR: This paper focuses on developing efficient algorithms for classifying points in high-dimensional spaces using the Hilbert polygonal metric, particularly addressing large-margin SVM and nearest neighbor-based classification.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address computational challenges in classifying points within high-dimensional spaces under the Hilbert polygonal metric, which has applications in machine learning and convex geometry.

Method: The paper proposes LP-based algorithms for SVM problems and nearest neighbor classifications under the Hilbert metric, offering polynomial-time solutions in terms of input parameters.

Result: The findings highlight significant improvements in computational efficiency compared to prior approaches, achieving polynomial runtime for problems previously solved in exponential time or without guarantees.

Conclusion: The proposed algorithms efficiently tackle classification in the Hilbert polygonal metric space, broadening its applicability in machine learning and computational geometry.

Abstract: Classifying points in high dimensional spaces is a fundamental geometric problem in machine learning. In this paper, we address classifying points in the $d$-dimensional Hilbert polygonal metric. The Hilbert metric is a generalization of the Cayley-Klein hyperbolic distance to arbitrary convex bodies and has a diverse range of applications in machine learning and convex geometry. We first present an efficient LP-based algorithm in the metric for the large-margin SVM problem. Our algorithm runs in time polynomial to the number of points, bounding facets, and dimension. This is a significant improvement on previous works, which either provide no theoretical guarantees on running time, or suffer from exponential runtime. We also consider the closely related Funk metric. We also present efficient algorithms for the soft-margin SVM problem and for nearest neighbor-based classification in the Hilbert metric.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [1144] [Polychronous Wave Computing: Timing-Native Address Selection in Spiking Networks](https://arxiv.org/abs/2601.13079)
*Natalila G. Berloff*

Main category: cond-mat.dis-nn

TL;DR: The paper introduces Polychronous Wave Computing (PWC), a method for timing-native spike processing to improve routing and computational accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance spiking inference by leveraging combinatorial spike timing directly instead of digitizing spikes into timestamps or rates, thus addressing inefficiencies in neuromorphic and photonic systems.

Method: PWC encodes spike times in phase domains and uses programmable interferometers for correlation evaluation, alongside a driven-dissipative winner-take-all stage for physical output routing.

Result: Simulations reveal nonlinear competition's effectiveness in improving routing fidelity, and phase tuning improves accuracy significantly under mismatched conditions.

Conclusion: PWC demonstrates potential as a fast routing coprocessor for various platforms, enabling efficient and high-accuracy timing-based processing.

Abstract: Spike timing offers a combinatorial address space, suggesting that timing-based spiking inference can be executed as lookup and routing rather than as dense multiply--accumulate. Yet most neuromorphic and photonic systems still digitize events into timestamps, bins, or rates and then perform selection in clocked logic. We introduce Polychronous Wave Computing (PWC), a timing-native address-selection primitive that maps relative spike latencies directly to a discrete output route in the wave domain. Spike times are phase-encoded in a rotating frame and processed by a programmable multiport interferometer that evaluates K template correlations in parallel; a driven--dissipative winner-take-all stage then performs a physical argmax, emitting a one-hot output port. We derive the operating envelope imposed by phase wrapping and mutual coherence, and collapse timing jitter, static phase mismatch, and dephasing into a single effective phase-noise budget whose induced winner--runner-up margin predicts boundary-first failures and provides an intensity-only calibration target. Simulations show that nonlinear competition improves routing fidelity compared with noisy linear intensity readout, and that hardware-in-the-loop phase tuning rescues a temporal-order gate from 55.9% to 97.2% accuracy under strong static mismatch. PWC provides a fast routing coprocessor for LUT-style spiking networks and sparse top-1 gates (e.g., mixture-of-experts routing) across polaritonic, photonic, and oscillator platforms.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1145] [Impact of Circuit Depth versus Qubit Count on Variational Quantum Classifiers for Higgs Boson Signal Detection](https://arxiv.org/abs/2601.11937)
*Fatih Maulana*

Main category: quant-ph

TL;DR: This paper explores Variational Quantum Classifiers (VQC) for detecting Higgs Boson signals, showing that deeper quantum circuits outperform wider (more qubit) circuits on noisy near-term devices.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the computational challenges of analyzing massive datasets generated by High-Energy Physics (HEP) experiments. It investigates the potential of Quantum Machine Learning (QML) architectures on Noisy Intermediate-Scale Quantum (NISQ) devices while focusing on optimal configurations for performance.

Method: They applied Variational Quantum Classifiers (VQC) on data from the ATLAS Higgs Boson Machine Learning Challenge 2014. PCA was used for dimensionality reduction, mapping features into 4-qubit and 8-qubit latent spaces, and then tested shallow and deep circuit configurations.

Result: The study found that increasing circuit depth on 4-qubit setups improved accuracy significantly (56.2%) compared to shallower setups (51.9%), while expanding to 8-qubit configurations reduced performance (50.6%) due to optimization challenges like Barren Plateaus.

Conclusion: For current quantum hardware, focusing on increasing quantum circuit depth and entanglement is more impactful than expanding qubit counts for tasks like anomaly detection in High-Energy Physics data.

Abstract: High-Energy Physics (HEP) experiments, such as those at the Large Hadron Collider (LHC), generate massive datasets that challenge classical computational limits. Quantum Machine Learning (QML) offers a potential advantage in processing high-dimensional data; however, finding the optimal architecture for current Noisy Intermediate-Scale Quantum (NISQ) devices remains an open challenge. This study investigates the performance of Variational Quantum Classifiers (VQC) in detecting Higgs Boson signals using the ATLAS Higgs Boson Machine Learning Challenge 2014 experiment dataset. We implemented a dimensionality reduction pipeline using Principal Component Analysis (PCA) to map 30 physical features into 4-qubit and 8-qubit latent spaces. We benchmarked three configurations: (A) a shallow 4-qubit circuit, (B) a deep 4-qubit circuit with increased entanglement layers, and (C) an expanded 8-qubit circuit. Experimental results demonstrate that increasing circuit depth significantly improves performance, yielding the highest accuracy of 56.2% (Configuration B), compared to a baseline of 51.9%. Conversely, simply scaling to 8 qubits resulted in a performance degradation to 50.6% due to optimization challenges associated with Barren Plateaus in the larger Hilbert space. These findings suggest that for near-term quantum hardware, prioritizing circuit depth and entanglement capability is more critical than increasing qubit count for effective anomaly detection in HEP data.

</details>


### [1146] [A Mixture of Experts Vision Transformer for High-Fidelity Surface Code Decoding](https://arxiv.org/abs/2601.12483)
*Hoang Viet Nguyen,Manh Hung Nguyen,Hoang Ta,Van Khu Vu,Yeow Meng Chee*

Main category: quant-ph

TL;DR: The paper introduces QuantumSMoE, a quantum vision transformer-based decoder for topological stabilizer codes, which efficiently handles quantum error correction by incorporating lattice structure. It outperforms existing machine learning and classical decoders.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational overhead and performance limitations of existing decoders in quantum error correction, particularly under the constraints of large code distances and real-time scalability.

Method: QuantumSMoE utilizes quantum vision transformers with plus-shaped embeddings and adaptive masking to capture lattice geometry and interactions. A mixture of experts layer with a novel auxiliary loss is employed to improve scalability and performance.

Result: QuantumSMoE demonstrates superior error correction performance compared to state-of-the-art classical and machine learning decoders, specifically for the toric code.

Conclusion: By leveraging lattice geometry and enhancing scalability, QuantumSMoE provides a highly efficient and effective solution for decoding in quantum error correction systems.

Abstract: Quantum error correction is a key ingredient for large scale quantum computation, protecting logical information from physical noise by encoding it into many physical qubits. Topological stabilizer codes are particularly appealing due to their geometric locality and practical relevance. In these codes, stabilizer measurements yield a syndrome that must be decoded into a recovery operation, making decoding a central bottleneck for scalable real time operation. Existing decoders are commonly classified into two categories. Classical algorithmic decoders provide strong and well established baselines, but may incur substantial computational overhead at large code distances or under stringent latency constraints. Machine learning based decoders offer fast GPU inference and flexible function approximation, yet many approaches do not explicitly exploit the lattice geometry and local structure of topological codes, which can limit performance. In this work, we propose QuantumSMoE, a quantum vision transformer based decoder that incorporates code structure through plus shaped embeddings and adaptive masking to capture local interactions and lattice connectivity, and improves scalability via a mixture of experts layer with a novel auxiliary loss. Experiments on the toric code demonstrate that QuantumSMoE outperforms state-of-the-art machine learning decoders as well as widely used classical baselines.

</details>


### [1147] [Generative Adversarial Networks for Resource State Generation](https://arxiv.org/abs/2601.13708)
*Shahbaz Shaik,Sourav Chatterjee,Sayantan Pramanik,Indranil Chakrabarty*

Main category: quant-ph

TL;DR: The paper presents a Generative Adversarial Network (GAN)-based framework to efficiently generate optimized quantum resource states for teleportation and entanglement broadcasting applications.


<details>
  <summary>Details</summary>
Motivation: Automating the process of designing optimized quantum resource states for tasks such as teleportation and entanglement to improve efficiency and scalability in quantum information processing.

Method: The framework employs a physics-informed GAN, embedding utility functions during training to generate valid two-qubit states while constraining Hermiticity, trace-one properties, and positivity.

Result: The model achieves high fidelities (~98%) for Werner-like and Bell-diagonal states while reproducing theoretical resource boundaries, showcasing effective constraint-driven quantum-state discovery.

Conclusion: Adversarial learning is demonstrated as a scalable and effective method for generating tailored quantum resources, enabling advancements in quantum network design and information-processing applications.

Abstract: We introduce a physics-informed Generative Adversarial Network framework that recasts quantum resource-state generation as an inverse-design task. By embedding task-specific utility functions into training, the model learns to generate valid two-qubit states optimized for teleportation and entanglement broadcasting. Comparing decomposition-based and direct-generation architectures reveals that structural enforcement of Hermiticity, trace-one, and positivity yields higher fidelity and training stability than loss-only approaches. The framework reproduces theoretical resource boundaries for Werner-like and Bell-diagonal states with fidelities exceeding ~98%, establishing adversarial learning as a lightweight yet effective method for constraint-driven quantum-state discovery. This approach provides a scalable foundation for automated design of tailored quantum resources for information-processing applications, exemplified with teleportation and broadcasting of entanglement, and it opens up the possibility of using such states in efficient quantum network design.

</details>


### [1148] [Deep Learning Approaches to Quantum Error Mitigation](https://arxiv.org/abs/2601.14226)
*Leonardo Placidi,Ifan Williams,Enrico Rinaldi,Daniel Mills,Cristina Cîrstoiu,Vanya Eccles,Ross Duncan*

Main category: quant-ph

TL;DR: The paper investigates deep learning for quantum error mitigation, focusing on sequence-to-sequence models like transformers, which outperform baseline methods.


<details>
  <summary>Details</summary>
Motivation: To address noisy outputs in quantum circuits, which hinder accurate distributions in quantum computing.

Method: Comparing different neural network architectures, testing models on datasets, and conducting ablation and generalization studies.

Result: Transformer-based models are the most effective in mitigating quantum circuit noise, outperforming baselines across datasets and devices.

Conclusion: Sequence-to-sequence, attention-based approaches generalize effectively for error mitigation in quantum circuits and perform well on devices with similar architectures.

Abstract: We present a systematic investigation of deep learning methods applied to quantum error mitigation of noisy output probability distributions from measured quantum circuits. We compare different architectures, from fully connected neural networks to transformers, and we test different design/training modalities, identifying sequence-to-sequence, attention-based models as the most effective on our datasets. These models consistently produce mitigated distributions that are closer to the ideal outputs when tested on both simulated and real device data obtained from IBM superconducting quantum processing units (QPU) up to five qubits. Across several different circuit depths, our approach outperforms other baseline error mitigation techniques. We perform a series of ablation studies to examine: how different input features (circuit, device properties, noisy output statistics) affect performance; cross-dataset generalization across circuit families; and transfer learning to a different IBM QPU. We observe that generalization performance across similar devices with the same architecture works effectively, without needing to fully retrain models.

</details>
