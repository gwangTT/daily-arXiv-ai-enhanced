<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 55]
- [cs.AR](#cs.AR) [Total: 10]
- [cs.CL](#cs.CL) [Total: 81]
- [cs.CV](#cs.CV) [Total: 156]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.LG](#cs.LG) [Total: 110]
- [cs.NE](#cs.NE) [Total: 8]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 47]
- [cs.SE](#cs.SE) [Total: 18]
- [q-bio.NC](#q-bio.NC) [Total: 6]
- [stat.ML](#stat.ML) [Total: 10]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 22]
- [math.OC](#math.OC) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 8]
- [eess.SY](#eess.SY) [Total: 2]
- [stat.ME](#stat.ME) [Total: 5]
- [cs.CY](#cs.CY) [Total: 16]
- [cs.MA](#cs.MA) [Total: 5]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.HC](#cs.HC) [Total: 4]
- [math.NA](#math.NA) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [eess.SP](#eess.SP) [Total: 16]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.CC](#cs.CC) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video](https://arxiv.org/abs/2508.11836)
*Dave Goel,Matthew Guzdial,Anurag Sarkar*

Main category: cs.AI

TL;DR: The paper introduces Finite Automata Extraction (FAE), which creates neuro-symbolic world models from gameplay videos using a novel domain-specific language called Retro Coder.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of transferring learned environment dynamics and improving explainability in traditional neural network-based world models.

Method: FAE leverages Retro Coder, a domain-specific language, to learn neuro-symbolic representations of the environment from gameplay videos.

Result: FAE demonstrates more precise modeling of the environment and generates more generalizable code compared to traditional DSL-based approaches.

Conclusion: The proposed approach improves precision and generalizability in neuro-symbolic world models, enhancing their adaptability and interpretability.

Abstract: World models are defined as a compressed spatial and temporal learned
representation of an environment. The learned representation is typically a
neural network, making transfer of the learned environment dynamics and
explainability a challenge. In this paper, we propose an approach, Finite
Automata Extraction (FAE), that learns a neuro-symbolic world model from
gameplay video represented as programs in a novel domain-specific language
(DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more
precise model of the environment and more general code than prior DSL-based
approaches.

</details>


### [2] [EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models](https://arxiv.org/abs/2508.11850)
*Milad Yazdani,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: The paper introduces EvoCut, a framework combining large language models (LLMs) and evolutionary search to automate the generation of acceleration cuts for integer programming, improving solver performance significantly.


<details>
  <summary>Details</summary>
Motivation: Manual design of acceleration cuts for integer programming is highly effective but demands expertise and hasn't been automated.

Method: EvoCut initializes cut candidates with an LLM-based agent, evaluates their performance empirically, and refines them through evolutionary search mechanisms such as crossover and mutation.

Result: EvoCut reduces the optimality gap by 17-57%, speeds up solution attainment up to 4 times, and generates higher-quality solutions within the same time limit.

Conclusion: EvoCut successfully automates the process of generating effective cuts with no expert input while improving integer programming solver efficiency and generalizing to unseen instances.

Abstract: Integer programming lies at the heart of crucial combinatorial optimization
tasks but remains challenging due to its NP-hard nature. An effective approach
for practically solving integer programs is the manual design of acceleration
cuts, i.e. inequalities that improve solver performance. However, this creative
process demands deep expertise and is yet to be automated. Our proposed
framework, EvoCut, automates the generation of acceleration cuts by combining
large language models (LLMs) with an evolutionary search. EvoCut (i)
initializes a diverse population of candidate cuts via an LLM-based initializer
agent; (ii) for each cut empirically evaluates both preservation of the optimal
solution and its ability to cut off fractional solutions across a verification
set; and (iii) iteratively refines the population through evolutionary
crossover and mutation agents. We quantify each cut's utility by its relative
reduction in the solver's optimality gap. Our comparisons against standard
integer programming practice show that EvoCut reduces optimality gap by 17-57%
within a fixed time. It obtains the same solutions up to 4 times as fast, and
obtains higher-quality solutions within the same time limit. Requiring no human
expert input, EvoCut reliably generates, improves, and empirically verifies
cuts that generalize to unseen instances. The code is available at
https://github.com/milad1378yz/EvoCut.

</details>


### [3] [LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework](https://arxiv.org/abs/2508.11860)
*Frazier N. Baker,Daniel Adu-Ampratwum,Reza Averly,Botao Yu,Huan Sun,Xia Ning*

Main category: cs.AI

TL;DR: LARC is the first agentic framework using a Large Language Model (LLM) to achieve 72.9% success in constrained retrosynthesis tasks, outperforming baselines and nearing human expert efficacy.


<details>
  <summary>Details</summary>
Motivation: Constrained retrosynthesis planning, critical in chemistry, is difficult to solve efficiently, prompting the need for innovative tools to identify synthetic routes under practical constraints.

Method: The LARC framework uses a novel agentic approach, integrating "Agent-as-a-Judge" constraint evaluation and tool-based reasoning for retrosynthesis planning.

Result: LARC achieved a 72.9% success rate on 48 constrained planning tasks across three constraint types, surpassing LLM baselines and approaching expert-level success.

Conclusion: The LARC framework demonstrates the potential of LLM-based agentic tools for aiding chemical synthesis planning and serves as a foundation for future advancements.

Abstract: Large language model (LLM) agent evaluators leverage specialized tools to
ground the rational decision-making of LLMs, making them well-suited to aid in
scientific discoveries, such as constrained retrosynthesis planning.
Constrained retrosynthesis planning is an essential, yet challenging, process
within chemistry for identifying synthetic routes from commercially available
starting materials to desired target molecules, subject to practical
constraints. Here, we present LARC, the first LLM-based Agentic framework for
Retrosynthesis planning under Constraints. LARC incorporates agentic constraint
evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis
planning process, using agentic feedback grounded in tool-based reasoning to
guide and constrain route generation. We rigorously evaluate LARC on a
carefully curated set of 48 constrained retrosynthesis planning tasks across 3
constraint types. LARC achieves a 72.9% success rate on these tasks, vastly
outperforming LLM baselines and approaching human expert-level success in
substantially less time. The LARC framework is extensible, and serves as a
first step towards an effective agentic tool or a co-scientist to human experts
for constrained retrosynthesis.

</details>


### [4] [QuarkMed Medical Foundation Model Technical Report](https://arxiv.org/abs/2508.11894)
*Ao Li,Bin Yan,Bingfeng Cai,Chenxi Li,Cunzhong Zhao,Fugen Yao,Gaoqiang Liu,Guanjun Jiang,Jian Xu,Liang Dong,Liansheng Sun,Rongshen Zhang,Xiaolei Gui,Xin Liu,Xin Shang,Yao Wu,Yu Cao,Zhenxin Ma,Zhuang Jia*

Main category: cs.AI

TL;DR: QuarkMed is a medical foundation model achieving high accuracy in medical tasks.


<details>
  <summary>Details</summary>
Motivation: Healthcare AI applications demand models with specialized knowledge, accuracy, and customization.

Method: QuarkMed uses curated data, Retrieval-Augmented Generation, and reinforcement learning for development.

Result: The model achieved 70% accuracy on the Chinese Medical Licensing Exam and supports diverse benchmarks.

Conclusion: QuarkMed demonstrates robust performance and versatility as a medical AI, serving millions of users.

Abstract: Recent advancements in large language models have significantly accelerated
their adoption in healthcare applications, including AI-powered medical
consultations, diagnostic report assistance, and medical search tools. However,
medical tasks often demand highly specialized knowledge, professional accuracy,
and customization capabilities, necessitating a robust and reliable foundation
model. QuarkMed addresses these needs by leveraging curated medical data
processing, medical-content Retrieval-Augmented Generation (RAG), and a
large-scale, verifiable reinforcement learning pipeline to develop a
high-performance medical foundation model. The model achieved 70% accuracy on
the Chinese Medical Licensing Examination, demonstrating strong generalization
across diverse medical benchmarks. QuarkMed offers a powerful yet versatile
personal medical AI solution, already serving over millions of users at
ai.quark.cn.

</details>


### [5] [CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs](https://arxiv.org/abs/2508.11944)
*Hongtao Liu,Zhicheng Du,Zihe Wang,Weiran Shen*

Main category: cs.AI

TL;DR: CHBench is a novel framework for evaluating the strategic reasoning levels of large language models in game settings, offering robust insights compared to utility metrics.


<details>
  <summary>Details</summary>
Motivation: Existing utility performance metrics for assessing LLMs in game-playing are not robust due to opponent variability and game structure.

Method: CHBench uses cognitive hierarchy models from behavioral economics and evaluates LLMs via data from six state-of-the-art models across fifteen normal-form game scenarios.

Result: LLMs consistently demonstrate strategic reasoning levels regardless of opponent variations, and the Memory Mechanism improves reasoning while the Chat Mechanism hinders it.

Conclusion: CHBench is an effective and generalizable framework for assessing LLM capabilities in strategic reasoning, with substantial potential for research and applications.

Abstract: Game-playing ability serves as an indicator for evaluating the strategic
reasoning capability of large language models (LLMs). While most existing
studies rely on utility performance metrics, which are not robust enough due to
variations in opponent behavior and game structure. To address this limitation,
we propose \textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation
framework inspired by the cognitive hierarchy models from behavioral economics.
We hypothesize that agents have bounded rationality -- different agents behave
at varying reasoning depths/levels. We evaluate LLMs' strategic reasoning
through a three-phase systematic framework, utilizing behavioral data from six
state-of-the-art LLMs across fifteen carefully selected normal-form games.
Experiments show that LLMs exhibit consistent strategic reasoning levels across
diverse opponents, confirming the framework's robustness and generalization
capability. We also analyze the effects of two key mechanisms (Chat Mechanism
and Memory Mechanism) on strategic reasoning performance. Results indicate that
the Chat Mechanism significantly degrades strategic reasoning, whereas the
Memory Mechanism enhances it. These insights position CHBench as a promising
tool for evaluating LLM capabilities, with significant potential for future
research and practical applications.

</details>


### [6] [Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models](https://arxiv.org/abs/2508.11953)
*Yuan Li,Zhengzhong Liu,Eric Xing*

Main category: cs.AI

TL;DR: The paper addresses optimizing data mixtures for supervised fine-tuning (SFT) of large language models by introducing a method that minimizes validation loss, showing improvements both mathematically and empirically.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in enhancing the process of supervised fine-tuning of large language models, where optimizing data mixtures has been an underexplored area but is critical for creating general-purpose models.

Method: A novel optimization-based method is introduced, which minimizes validation loss by parametrizing the loss, modeling effective data transfer, and using scaling laws. Small-scale data mixtures are experimented with to fit parameters and derive optimal weights.

Result: Results show models trained with optimized weights have performance comparable to grid search-derived optimal weights, with slight per-domain loss increase of 0.66%. The method also enhances validation loss and downstream performance when applied to popular SFT datasets.

Conclusion: The method effectively aids in data selection and performance improvement for supervised fine-tuning, making strides in general-purpose and domain-specific language model development.

Abstract: Optimizing data mixtures for supervised fine-tuning (SFT) of large language
models (LLMs) is critical for developing general-purpose models, yet this area
remains underexplored. In this paper, we frame data mixing as an optimization
problem and introduce a novel method designed to minimize validation loss. Our
approach parametrizes the loss by modeling effective data transferred and
leveraging scaling laws for fine-tuning. By experimenting with various
small-scale data mixtures, we fit these parameters and derive the optimal
weights. We provide both mathematical proofs and empirical results
demonstrating that our algorithm achieves excellent overall and individual
performance across all domains. Through controlled experiments, we show that
models trained with our optimized weights perform on par with those using
optimal weights determined via grid search, with per-domain loss only 0.66%
higher than the best domain loss from grid search on average. Additionally, we
show that reweighting popular SFT datasets using our method improves both
validation loss and downstream performance. Finally, we discuss how our method
can generalize to guide data selection for domain-specific models and provide
insights into SFT.

</details>


### [7] [UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting](https://arxiv.org/abs/2508.11954)
*Sehyuk Park,Soyeon Caren Han,Eduard Hovy*

Main category: cs.AI

TL;DR: This paper introduces UniCast, a new framework for multimodal time series forecasting, combining embeddings from pretrained visual and textual encoders with a frozen Time Series Foundation Model, resulting in state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to improve time series forecasting by incorporating rich multimodal context, like visual and textual signals, which existing models fail to utilize effectively.

Method: UniCast integrates embeddings from pretrained Vision and Text Encoders with a Time Series Foundation Model through soft prompt tuning, allowing efficient adaptation without significant parameter updates.

Result: UniCast demonstrates superior forecasting performance across diverse benchmarks, outperforming all existing TSFM baselines by leveraging multimodal input.

Conclusion: The study highlights the importance of multimodal context in shaping advanced time series forecasting models, paving the way for versatile and general-purpose forecasters.

Abstract: Time series forecasting is a foundational task across domains, such as
finance, healthcare, and environmental monitoring. While recent advances in
Time Series Foundation Models (TSFMs) have demonstrated strong generalisation
through large-scale pretraining, existing models operate predominantly in a
unimodal setting, ignoring the rich multimodal context, such as visual and
textual signals, that often accompanies time series data in real-world
scenarios. This paper introduces a novel parameter-efficient multimodal
framework, UniCast, that extends TSFMs to jointly leverage time series, vision,
and text modalities for enhanced forecasting performance. Our method integrates
modality-specific embeddings from pretrained Vision and Text Encoders with a
frozen TSFM via soft prompt tuning, enabling efficient adaptation with minimal
parameter updates. This design not only preserves the generalisation strength
of the foundation model but also enables effective cross-modal interaction.
Extensive experiments across diverse time-series forecasting benchmarks
demonstrate that UniCast consistently and significantly outperforms all
existing TSFM baselines. The findings highlight the critical role of multimodal
context in advancing the next generation of general-purpose time series
forecasters.

</details>


### [8] [Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index](https://arxiv.org/abs/2508.11959)
*Xuanxiang Huang,Olivier Létoffé,Joao Marques-Silva*

Main category: cs.AI

TL;DR: The paper develops new feature attribution scores considering non-WAXp sets, improving the ability to exclude adversarial examples.


<details>
  <summary>Details</summary>
Motivation: Existing methods for feature attribution in XAI using weak abductive explanations neglect contributions from non-WAXp sets, which can contain useful information.

Method: The authors introduce two novel feature importance scores using Shapley value and Banzhaf index, integrating contributions from non-WAXp sets to examine their ability to exclude adversarial examples. They also explore the scores' properties and computational complexity.

Result: The proposed scores provide a new perspective by accounting for non-WAXp sets and offer insights into their computational behavior and performance.

Conclusion: Integrating non-WAXp sets into feature attribution methods enhances their effectiveness in mitigating adversarial threats and advances explainability in AI.

Abstract: Feature attribution methods based on game theory are ubiquitous in the field
of eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous
feature attribution using logic-based explanations, specifically targeting
high-stakes uses of machine learning (ML) models. Typically, such works exploit
weak abductive explanation (WAXp) as the characteristic function to assign
importance to features. However, one possible downside is that the contribution
of non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important
information, because of the relationship between formal explanations (XPs) and
adversarial examples (AExs). Accordingly, this paper leverages Shapley value
and Banzhaf index to devise two novel feature importance scores. We take into
account non-WAXp sets when computing feature contribution, and the novel scores
quantify how effective each feature is at excluding AExs. Furthermore, the
paper identifies properties and studies the computational complexity of the
proposed scores.

</details>


### [9] [Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering](https://arxiv.org/abs/2508.11975)
*Gongyao Jiang,Qiong Luo*

Main category: cs.AI

TL;DR: The paper introduces methods to improve Vision Language Models (VLMs) for chart understanding tasks using synthetic data generation and a novel answering process, achieving significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Vision Language Models often struggle with chart understanding tasks, requiring solutions for accurate chart description and reasoning without relying on noisy human-labeled data.

Method: The method involves a chart synthesis pipeline that generates reliable synthetic data using code execution and a candidate-conditioned answering process where multiple responses per query are synthesized into a final contextualized answer.

Result: Experiments showed that the proposed approaches led to up to a 15.50 point accuracy improvement in VLM performance, in a self-improving paradigm without human-labeled data.

Conclusion: The paper concludes that their methods effectively improve chart reasoning and understanding tasks for VLMs, setting a benchmark for self-improving systems without external dependencies or manual intervention.

Abstract: Vision Language Models (VLMs) often struggle with chart understanding tasks,
particularly in accurate chart description and complex reasoning. Synthetic
data generation is a promising solution, while usually facing the challenge of
noise labels. To address this challenge, we first introduce a chart synthesis
pipeline that generates aligned chart-question-answer triplets through code
generation and execution, ensuring the reliability of synthetic data without
human intervention. Furthermore, inspired by test-time scaling that increases
inference budget and thereby improves performance, we design a
candidate-conditioned answering process. The VLM first generates multiple
responses per query, and then synthesizes the final answer by contextualizing
these candidates. Experiments demonstrate significant improvements, with up to
15.50 points accuracy gain over the initial VLM, in a fully self-improving
paradigm without either human-labeled data or external models.

</details>


### [10] [FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction](https://arxiv.org/abs/2508.11987)
*Zhiyuan Zeng,Jiashuo Liu,Siyuan Chen,Tianci He,Yali Liao,Jinpeng Wang,Zaiyuan Wang,Yang Yang,Lingyue Yin,Mingren Yin,Zhenwei Zhu,Tianle Cai,Zehui Chen,Jiecao Chen,Yantao Du,Xiang Gao,Jiacheng Guo,Liang Hu,Jianpeng Jiao,Xiangsheng Li,Jingkai Liu,Shuang Ni,Zhoufutu Wen,Ge Zhang,Kaiyuan Zhang,Xin Zhou,Jose Blanchet,Xipeng Qiu,Mengdi Wang,Wenhao Huang*

Main category: cs.AI

TL;DR: FutureX introduces a dynamic live benchmark for evaluating Large Language Model (LLM) agents on future prediction tasks, addressing challenges like real-time updates and data contamination.


<details>
  <summary>Details</summary>
Motivation: There is a lack of a large-scale benchmark for assessing LLM agents on complex future prediction tasks involving reasoning, uncertainty handling, and real-time updates.

Method: FutureX offers real-time daily updates through automated pipelines for question and answer collection, assessing 25 LLM/agent models on reasoning, adaptability, and dynamic performance.

Result: FutureX successfully evaluates diverse LLMs/tools and identifies failure modes such as susceptibility to fake web pages and issues with temporal validity.

Conclusion: FutureX establishes a contamination-free benchmark driving research on LLM agents' ability to perform as professional human analysts in future prediction tasks.

Abstract: Future prediction is a complex task for LLM agents, requiring a high level of
analytical thinking, information gathering, contextual understanding, and
decision-making under uncertainty. Agents must not only gather and interpret
vast amounts of dynamic information but also integrate diverse data sources,
weigh uncertainties, and adapt predictions based on emerging trends, just as
human experts do in fields like politics, economics, and finance. Despite its
importance, no large-scale benchmark exists for evaluating agents on future
prediction, largely due to challenges in handling real-time updates and
retrieving timely, accurate answers. To address this, we introduce
$\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically
designed for LLM agents performing future prediction tasks. FutureX is the
largest and most diverse live benchmark for future prediction, supporting
real-time daily updates and eliminating data contamination through an automated
pipeline for question gathering and answer collection. We evaluate 25 LLM/agent
models, including those with reasoning, search capabilities, and integration of
external tools such as the open-source Deep Research Agent and closed-source
Deep Research models. This comprehensive evaluation assesses agents' adaptive
reasoning and performance in dynamic environments. Additionally, we provide
in-depth analyses of agents' failure modes and performance pitfalls in
future-oriented tasks, including the vulnerability to fake web pages and the
temporal validity. Our goal is to establish a dynamic, contamination-free
evaluation standard that drives the development of LLM agents capable of
performing at the level of professional human analysts in complex reasoning and
predictive thinking.

</details>


### [11] [Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network](https://arxiv.org/abs/2508.11991)
*Weihao Sun*

Main category: cs.AI

TL;DR: The paper proposes AIGer, an enhanced method for studying And-Inverter Graphs (AIGs) with improved modeling and dynamic information propagation capabilities, achieving significant performance improvements in predictive tasks.


<details>
  <summary>Details</summary>
Motivation: Address challenges in accurately modeling functional and structural characteristics of AIGs due to their complexity and scale, alongside limitations in dynamic information propagation.

Method: Develop AIGer, which comprises two components: a node logic feature initialization embedding that maps logic nodes into semantic spaces, and a heterogeneous graph convolutional network for feature learning with dynamic weight matrices and tailored aggregation approaches.

Result: AIGer achieves notable performance improvements in tasks like Signal Probability Prediction (MAE by 18.95%, MSE by 44.44%) and Truth Table Distance Prediction (MAE by 33.57%, MSE by 14.79%).

Conclusion: AIGer improves the modeling of both functional and structural characteristics of AIGs, effectively enhancing performance in electronic design automation tasks.

Abstract: The automation of logic circuit design enhances chip performance, energy
efficiency, and reliability, and is widely applied in the field of Electronic
Design Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent,
optimize, and verify the functional characteristics of digital circuits,
enhancing the efficiency of EDA development.Due to the complex structure and
large scale of nodes in real-world AIGs, accurate modeling is challenging,
leading to existing work lacking the ability to jointly model functional and
structural characteristics, as well as insufficient dynamic information
propagation capability.To address the aforementioned challenges, we propose
AIGer.Specifically, AIGer consists of two components: 1) Node logic feature
initialization embedding component and 2) AIGs feature learning network
component.The node logic feature initialization embedding component projects
logic nodes, such as AND and NOT, into independent semantic spaces, to enable
effective node embedding for subsequent processing.Building upon this, the AIGs
feature learning network component employs a heterogeneous graph convolutional
network, designing dynamic relationship weight matrices and differentiated
information aggregation approaches to better represent the original structure
and information of AIGs.The combination of these two components enhances
AIGer's ability to jointly model functional and structural characteristics and
improves its message passing capability. Experimental results indicate that
AIGer outperforms the current best models in the Signal Probability Prediction
(SSP) task, improving MAE and MSE by 18.95\% and 44.44\%, respectively. In the
Truth Table Distance Prediction (TTDP) task, AIGer achieves improvements of
33.57\% and 14.79\% in MAE and MSE, respectively, compared to the
best-performing models.

</details>


### [12] [Active inference for action-unaware agents](https://arxiv.org/abs/2508.12027)
*Filippo Torresan,Keisuke Suzuki,Ryota Kanai,Manuel Baltieri*

Main category: cs.AI

TL;DR: This paper investigates differences in motor behavior planning under two frameworks: action-aware and action-unaware agents, demonstrating that even action-unaware agents can perform comparably well in navigation tasks.


<details>
  <summary>Details</summary>
Motivation: To explore how agents perform adaptive tasks by minimizing free energy, and to compare the impact of incorporating or excluding efference copy (knowledge of their own actions) on their performance in planning and navigation tasks.

Method: The study conducts a comparison between two planning strategies—action-aware and action-unaware—via simulation on two navigation tasks, analyzing their performances under different assumptions of motor behavior representation.

Result: The study reveals that action-unaware agents, despite lacking efference copy signals, can achieve comparable performance to action-aware agents, which have access to such knowledge.

Conclusion: Agents do not necessarily need explicit knowledge of their own actions (efference copy) to perform well. This finding has implications for the design of adaptive agents and models of motor planning.

Abstract: Active inference is a formal approach to study cognition based on the notion
that adaptive agents can be seen as engaging in a process of approximate
Bayesian inference, via the minimisation of variational and expected free
energies. Minimising the former provides an account of perceptual processes and
learning as evidence accumulation, while minimising the latter describes how
agents select their actions over time. In this way, adaptive agents are able to
maximise the likelihood of preferred observations or states, given a generative
model of the environment. In the literature, however, different strategies have
been proposed to describe how agents can plan their future actions. While they
all share the notion that some kind of expected free energy offers an
appropriate way to score policies, sequences of actions, in terms of their
desirability, there are different ways to consider the contribution of past
motor experience to the agent's future behaviour. In some approaches, agents
are assumed to know their own actions, and use such knowledge to better plan
for the future. In other approaches, agents are unaware of their actions, and
must infer their motor behaviour from recent observations in order to plan for
the future. This difference reflects a standard point of departure in two
leading frameworks in motor control based on the presence, or not, of an
efference copy signal representing knowledge about an agent's own actions. In
this work we compare the performances of action-aware and action-unaware agents
in two navigations tasks, showing how action-unaware agents can achieve
performances comparable to action-aware ones while at a severe disadvantage.

</details>


### [13] [AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning](https://arxiv.org/abs/2508.11995)
*Xuyang Zhao,Shiwan Zhao,Hualong Yu,Liting Zhang,Qicheng Li*

Main category: cs.AI

TL;DR: This paper introduces AgentCDM, a new framework for improving collaborative decision-making in multi-agent systems using large language models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the weaknesses of existing collaborative decision-making strategies in multi-agent systems, such as susceptibility to cognitive biases in dictatorial methods and inefficiency in fully utilizing collective intelligence in voting-based approaches.

Method: AgentCDM is inspired by the Analysis of Competing Hypotheses (ACH) and incorporates a two-stage training paradigm. The first stage employs ACH-based structured scaffolding for reasoning, and the second stage removes the scaffolding to encourage autonomous reasoning.

Result: AgentCDM outperformed previous methods in experiments across multiple benchmark datasets, showcasing enhanced decision quality, robustness, and generalization.

Conclusion: AgentCDM provides a promising framework for systematically reducing cognitive biases and improving collaborative reasoning in multi-agent systems, advancing the field of decision-making with large language models.

Abstract: Multi-agent systems (MAS) powered by large language models (LLMs) hold
significant promise for solving complex decision-making tasks. However, the
core process of collaborative decision-making (CDM) within these systems
remains underexplored. Existing approaches often rely on either ``dictatorial"
strategies that are vulnerable to the cognitive biases of a single agent, or
``voting-based" methods that fail to fully harness collective intelligence. To
address these limitations, we propose \textbf{AgentCDM}, a structured framework
for enhancing collaborative decision-making in LLM-based multi-agent systems.
Drawing inspiration from the Analysis of Competing Hypotheses (ACH) in
cognitive science, AgentCDM introduces a structured reasoning paradigm that
systematically mitigates cognitive biases and shifts decision-making from
passive answer selection to active hypothesis evaluation and construction. To
internalize this reasoning process, we develop a two-stage training paradigm:
the first stage uses explicit ACH-inspired scaffolding to guide the model
through structured reasoning, while the second stage progressively removes this
scaffolding to encourage autonomous generalization. Experiments on multiple
benchmark datasets demonstrate that AgentCDM achieves state-of-the-art
performance and exhibits strong generalization, validating its effectiveness in
improving the quality and robustness of collaborative decisions in MAS.

</details>


### [14] [AI Models for Depressive Disorder Detection and Diagnosis: A Review](https://arxiv.org/abs/2508.12022)
*Dorsa Macky Aleagha,Payam Zohari,Mostafa Haghir Chehreghani*

Main category: cs.AI

TL;DR: AI methods offer promising advancements in the diagnosis and prediction of Major Depressive Disorder, emphasizing multimodal data and fairness.


<details>
  <summary>Details</summary>
Motivation: To address the dependency on subjective clinical assessments for diagnosing Major Depressive Disorder with scalable and objective AI tools.

Method: Systematic review of 55 studies, categorizing tasks, data modalities, and AI computational models using a hierarchical taxonomy.

Result: Identified trends like usage of graph neural networks, large language models, multimodal fusion, and focus on fairness and explainability.

Conclusion: The paper provides a roadmap for innovation in computational psychiatry, highlighting open challenges and guiding future research advancements.

Abstract: Major Depressive Disorder is one of the leading causes of disability
worldwide, yet its diagnosis still depends largely on subjective clinical
assessments. Integrating Artificial Intelligence (AI) holds promise for
developing objective, scalable, and timely diagnostic tools. In this paper, we
present a comprehensive survey of state-of-the-art AI methods for depression
detection and diagnosis, based on a systematic review of 55 key studies. We
introduce a novel hierarchical taxonomy that structures the field by primary
clinical task (diagnosis vs. prediction), data modality (text, speech,
neuroimaging, multimodal), and computational model class (e.g., graph neural
networks, large language models, hybrid approaches). Our in-depth analysis
reveals three major trends: the predominance of graph neural networks for
modeling brain connectivity, the rise of large language models for linguistic
and conversational data, and an emerging focus on multimodal fusion,
explainability, and algorithmic fairness. Alongside methodological insights, we
provide an overview of prominent public datasets and standard evaluation
metrics as a practical guide for researchers. By synthesizing current advances
and highlighting open challenges, this survey offers a comprehensive roadmap
for future innovation in computational psychiatry.

</details>


### [15] [Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems](https://arxiv.org/abs/2508.12026)
*Szymon Pawlonka,Mikołaj Małkiński,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: The paper introduces Bongard-RWR+, a dataset with 5,400 instances for abstract visual reasoning using real-world-like images, created with a vision language model pipeline. Evaluation reveals that state-of-the-art models excel with coarse concepts but struggle with fine-grained reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing Bongard datasets are either simplistic synthetic drawings or small-scale real-world representations, limiting their efficacy in evaluating abstract visual reasoning due to constrained task complexity and dataset size.

Method: The authors generate Bongard-RWR+ using a vision language model pipeline: Pixtral-12B describes manually created images and generates new aligned descriptions, Flux.1-dev synthesizes images from these descriptions, and images are manually verified for concept fidelity.

Result: Bongard-RWR+ was developed with 5,400 instances, significantly expanding the dataset size compared to prior work. Evaluations show that state-of-the-art vision language models can identify coarse-grained visual concepts but have difficulty discerning fine-grained concepts.

Conclusion: Bongard-RWR+ serves as a challenging benchmark for abstract visual reasoning, demonstrating that current vision language models lack advanced fine-grained reasoning capabilities.

Abstract: Bongard Problems (BPs) provide a challenging testbed for abstract visual
reasoning (AVR), requiring models to identify visual concepts fromjust a few
examples and describe them in natural language. Early BP benchmarks featured
synthetic black-and-white drawings, which might not fully capture the
complexity of real-world scenes. Subsequent BP datasets employed real-world
images, albeit the represented concepts are identifiable from high-level image
features, reducing the task complexity. Differently, the recently released
Bongard-RWR dataset aimed at representing abstract concepts formulated in the
original BPs using fine-grained real-world images. Its manual construction,
however, limited the dataset size to just $60$ instances, constraining
evaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset
composed of $5\,400$ instances that represent original BP abstract concepts
using real-world-like images generated via a vision language model (VLM)
pipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually
curated images and generate new descriptions aligned with the underlying
concepts, use Flux.1-dev to synthesize images from these descriptions, and
manually verify that the generated images faithfully reflect the intended
concepts. We evaluate state-of-the-art VLMs across diverse BP formulations,
including binary and multiclass classification, as well as textual answer
generation. Our findings reveal that while VLMs can recognize coarse-grained
visual concepts, they consistently struggle with discerning fine-grained
concepts, highlighting limitations in their reasoning capabilities.

</details>


### [16] [e-boost: Boosted E-Graph Extraction with Adaptive Heuristics and Exact Solving](https://arxiv.org/abs/2508.13020)
*Jiaqi Yin,Zhan Song,Chen Chen,Yaohui Cai,Zhiru Zhang,Cunxi Yu*

Main category: cs.AI

TL;DR: The paper introduces 'e-boost', a novel framework for efficient and near-optimal e-graph extraction that offers significant runtime speedup and performance improvements.


<details>
  <summary>Details</summary>
Motivation: Addressing the NP-hard e-graph extraction problem, which poses a critical trade-off between speed and solution optimality in logic synthesis and formal verification.

Method: The e-boost framework integrates parallelized heuristic extraction, adaptive search space pruning, and initialized exact solving using Integer Linear Programming with warm-start capabilities.

Result: E-boost achieves 558x speedup over ILP, 19.04% improvement over SmoothE, and enhanced logic synthesis task results with 7.6% and 8.1% area improvements.

Conclusion: E-boost successfully balances efficiency and optimality, offering practical advancements in e-graph-based optimization tasks, demonstrating its effectiveness across benchmarks and realistic applications.

Abstract: E-graphs have attracted growing interest in many fields, particularly in
logic synthesis and formal verification. E-graph extraction is a challenging
NP-hard combinatorial optimization problem. It requires identifying optimal
terms from exponentially many equivalent expressions, serving as the primary
performance bottleneck in e-graph based optimization tasks. However,
traditional extraction methods face a critical trade-off: heuristic approaches
offer speed but sacrifice optimality, while exact methods provide optimal
solutions but face prohibitive computational costs on practical problems. We
present e-boost, a novel framework that bridges this gap through three key
innovations: (1) parallelized heuristic extraction that leverages weak data
dependence to compute DAG costs concurrently, enabling efficient multi-threaded
performance without sacrificing extraction quality; (2) adaptive search space
pruning that employs a parameterized threshold mechanism to retain only
promising candidates, dramatically reducing the solution space while preserving
near-optimal solutions; and (3) initialized exact solving that formulates the
reduced problem as an Integer Linear Program with warm-start capabilities,
guiding solvers toward high-quality solutions faster.
  Across the diverse benchmarks in formal verification and logic synthesis
fields, e-boost demonstrates 558x runtime speedup over traditional exact
approaches (ILP) and 19.04% performance improvement over the state-of-the-art
extraction framework (SmoothE). In realistic logic synthesis tasks, e-boost
produces 7.6% and 8.1% area improvements compared to conventional synthesis
tools with two different technology mapping libraries. e-boost is available at
https://github.com/Yu-Maryland/e-boost.

</details>


### [17] [MAPF-World: Action World Model for Multi-Agent Path Finding](https://arxiv.org/abs/2508.12087)
*Zhanjiang Yang,Meng Li,Yang Shen,Yueming Li,Lijun Sun*

Main category: cs.AI

TL;DR: MAPF-World introduces a novel autoregressive world model for multi-agent path finding (MAPF) to address limitations in existing systems, including poor situational awareness and coordination in long-term planning.


<details>
  <summary>Details</summary>
Motivation: Existing decentralized MAPF solvers fail in complex, long-term scenarios due to limited modeling of temporal dynamics and inter-agent dependencies.

Method: The paper proposes MAPF-World, an autoregressive model that enhances situational awareness by predicting future environmental states and actions, leading to more coordinated planning.

Result: MAPF-World surpasses state-of-the-art solvers in performance, with improvements in zero-shot generalization, while requiring significantly less model size (96.5% smaller) and training data (92% reduced).

Conclusion: MAPF-World is a robust and efficient solution for complex MAPF settings, offering improved decision-making and applicability in real-world scenarios through better models and benchmarks.

Abstract: Multi-agent path finding (MAPF) is the problem of planning conflict-free
paths from the designated start locations to goal positions for multiple
agents. It underlies a variety of real-world tasks, including multi-robot
coordination, robot-assisted logistics, and social navigation. Recent
decentralized learnable solvers have shown great promise for large-scale MAPF,
especially when leveraging foundation models and large datasets. However, these
agents are reactive policy models and exhibit limited modeling of environmental
temporal dynamics and inter-agent dependencies, resulting in performance
degradation in complex, long-term planning scenarios. To address these
limitations, we propose MAPF-World, an autoregressive action world model for
MAPF that unifies situation understanding and action generation, guiding
decisions beyond immediate local observations. It improves situational
awareness by explicitly modeling environmental dynamics, including spatial
features and temporal dependencies, through future state and actions
prediction. By incorporating these predicted futures, MAPF-World enables more
informed, coordinated, and far-sighted decision-making, especially in complex
multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an
automatic map generator grounded in real-world scenarios, capturing practical
map layouts for training and evaluating MAPF solvers. Extensive experiments
demonstrate that MAPF-World outperforms state-of-the-art learnable solvers,
showcasing superior zero-shot generalization to out-of-distribution cases.
Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced
data.

</details>


### [18] [Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios](https://arxiv.org/abs/2508.12100)
*Daniel Burkhardt,Xiangwei Cheng*

Main category: cs.AI

TL;DR: The paper introduces ReT-Eval, a reasoning framework inspired by human strategies, utilizing domain knowledge graphs and large language models to generate effective, goal-oriented reasoning threads.


<details>
  <summary>Details</summary>
Motivation: Reasoning models often produce lengthy, generic outputs and lack semantic hierarchies, alignment with user and domain knowledge, and effective pruning mechanisms in goal-oriented problem solving.

Method: The proposed ReT-Eval framework consists of two phases: (1) extracting semantically relevant knowledge using graph neural networks and enhancing it with large language model knowledge, and (2) evaluating and pruning reasoning threads based on a reward-guided strategy to ensure coherence and effectiveness.

Result: Experiments and expert evaluations demonstrate that ReT-Eval improves user understanding and surpasses current state-of-the-art reasoning models.

Conclusion: ReT-Eval effectively aligns user understanding with structured domain knowledge, generating concise and goal-oriented reasoning threads, addressing key limitations of existing models.

Abstract: Reasoning in interactive problem solving scenarios requires models to
construct reasoning threads that reflect user understanding and align with
structured domain knowledge. However, current reasoning models often lack
explicit semantic hierarchies, user-domain knowledge alignment, and principled
mechanisms to prune reasoning threads for effectiveness. These limitations
result in lengthy generic output that does not guide users through
goal-oriented reasoning steps. To address this, we propose a
prototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval)
framework, drawing inspiration from human-like reasoning strategies that
emphasize structured knowledge reuse. In the first phase, semantically relevant
knowledge structures are extracted from a sparse domain knowledge graph using a
graph neural network and enriched with intrinsic large language model knowledge
to resolve knowledge discrepancies. In the second phase, these threads are
evaluated and pruned using a reward-guided strategy aimed at maintaining
semantic coherence to generate effective reasoning threads. Experiments and
expert evaluations show that ReT-Eval enhances user understanding and
outperforms state-of-the-art reasoning models.

</details>


### [19] [MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization](https://arxiv.org/abs/2508.12149)
*Haochen You,Baojing Liu*

Main category: cs.AI

TL;DR: The paper introduces MOVER, a unimodal representation framework utilizing optimal transport and geometric regularization. Experiments show improved modality alignment and retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal methods struggle to generalize across multiple modalities and lack semantic structure in high-dimensional spaces.

Method: The MOVER framework uses optimal transport-based soft alignment alongside geometric volume minimization (GAVE) for modality-agnostic multi-modal representation.

Result: MOVER achieved superior performance in text-video-audio retrieval tasks, excelling in zero-shot and finetuned setups.

Conclusion: MOVER delivers enhanced generalization, structural consistency, and effective alignment across diverse modalities.

Abstract: Recent advances in multimodal learning have largely relied on pairwise
contrastive objectives to align different modalities, such as text, video, and
audio, in a shared embedding space. While effective in bi-modal setups, these
approaches struggle to generalize across multiple modalities and often lack
semantic structure in high-dimensional spaces. In this paper, we propose MOVER,
a novel framework that combines optimal transport-based soft alignment with
volume-based geometric regularization to build semantically aligned and
structured multimodal representations. By integrating a transport-guided
matching mechanism with a geometric volume minimization objective (GAVE), MOVER
encourages consistent alignment across all modalities in a modality-agnostic
manner. Experiments on text-video-audio retrieval tasks demonstrate that MOVER
significantly outperforms prior state-of-the-art methods in both zero-shot and
finetuned settings. Additional analysis shows improved generalization to unseen
modality combinations and stronger structural consistency in the learned
embedding space.

</details>


### [20] [RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards](https://arxiv.org/abs/2508.12165)
*Rohit Krishnan,Jon Evans*

Main category: cs.AI

TL;DR: This paper presents RLNVR, a framework for training language models using noisy real-world feedback instead of verified signals. Demonstrated through Walter, it optimizes social media content using engagement data, improving content quality and training stability.


<details>
  <summary>Details</summary>
Motivation: Address the impracticality of traditional RLHF in real-world domains by leveraging noisy, unverified reward signals.

Method: Use baseline normalization and semantic similarity-based reward transfer in RLNVR framework, combined with GSPO and UED curriculum for stability and diversity.

Result: Significant improvements in content quality and training stability demonstrated with Walter prototype.

Conclusion: RLNVR provides a novel applied framework for optimizing language models using noisy social engagement data, paving the way for future comprehensive evaluations.

Abstract: This paper introduces RLNVR (Reinforcement Learning from Non-Verified
Rewards), a framework for training language models using noisy, real-world
feedback signals without requiring explicit human verification. Traditional
RLHF requires expensive, verified reward signals that are impractical in many
real-world domains. RLNVR addresses this challenge through baseline
normalization and semantic similarity-based reward transfer. We demonstrate
RLNVR through Walter, a prototype system that optimizes social media content
generation using actual engagement data from Bluesky. Our experimental results
show significant improvements in content quality and training stability, with
comprehensive evaluation planned for future work. Positioning: We present a
practical framework that combines RLNVR with GSPO (Group Sequence Policy
Optimization) and an optional UED (Unsupervised Environment Design) curriculum
to improve stability and diversity under noisy, implicit rewards. To our
knowledge, combining GSPO-style normalization with a UED-style curriculum for
LLM content generation from implicit social engagement has not been previously
documented in this applied setting; we frame this as an applied integration
rather than a new algorithm.

</details>


### [21] [Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting](https://arxiv.org/abs/2508.12260)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Ananya Sharma,Emily Martin,Marisa Eisenberg*

Main category: cs.AI

TL;DR: Mantis is a foundation model for disease forecasting, trained entirely on simulations and capable of generalization across diseases, regions, and outcomes.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations in infectious disease forecasting, especially in novel outbreaks or low-resource settings, where disease-specific data and expert tuning are required.

Method: The authors developed Mantis, a model trained only on mechanistic simulations covering over 400 million simulated days of diverse outbreak scenarios without using real-world training data.

Result: Mantis outperformed 39 expert-tuned models across six diseases, including surpassing all models in the CDC's COVID-19 Forecast Hub, and generalized to novel epidemiological regimes.

Conclusion: Mantis is general, interpretable, and deployable in scenarios where traditional models fail; it extends forecasting horizons, enabling better public health planning.

Abstract: Infectious disease forecasting in novel outbreaks or low resource settings
has been limited by the need for disease-specific data, bespoke training, and
expert tuning. We introduce Mantis, a foundation model trained entirely on
mechanistic simulations, which enables out-of-the-box forecasting across
diseases, regions, and outcomes, even in settings with limited historical data.
Mantis is built on over 400 million simulated days of outbreak dynamics
spanning diverse pathogens, transmission modes, interventions, and surveillance
artifacts. Despite requiring no real-world data during training, Mantis
outperformed 39 expert-tuned models we tested across six diseases, including
all models in the CDC's COVID-19 Forecast Hub. Mantis generalized to novel
epidemiological regimes, including diseases with held-out transmission
mechanisms, demonstrating that it captures fundamental contagion dynamics.
Critically, Mantis is mechanistically interpretable, enabling public health
decision-makers to identify the latent drivers behind its predictions. Finally,
Mantis delivers accurate forecasts at 8-week horizons, more than doubling the
actionable range of most models, enabling proactive public health planning.
Together, these capabilities position Mantis as a foundation for
next-generation disease forecasting systems: general, interpretable, and
deployable where traditional models fail.

</details>


### [22] [RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts](https://arxiv.org/abs/2508.12291)
*Xuming He,Zhiyuan You,Junchao Gong,Couhua Liu,Xiaoyu Yue,Peiqin Zhuang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: This paper introduces RadarQA, an MLLM-based tool for improving weather forecast quality analysis, supported by a new dataset and multi-stage training.


<details>
  <summary>Details</summary>
Motivation: Current weather forecast evaluation lacks interpretability, descriptive capability, and understanding of dynamic evolution, which this paper aims to address using advanced MLLMs.

Method: The proposed RadarQA integrates physical attributes for assessment, introduces a comprehensive task paradigm, constructs a large-scale RQA-70K dataset through hybrid annotation, and employs iterative multi-stage training.

Result: RadarQA demonstrates superior performance compared to conventional MLLMs in all evaluation settings, showcasing its effectiveness and robustness.

Conclusion: The RadarQA framework advances the field of weather forecast analysis by improving interpretability and performance, offering a practical and scalable solution.

Abstract: Quality analysis of weather forecasts is an essential topic in meteorology.
Although traditional score-based evaluation metrics can quantify certain
forecast errors, they are still far from meteorological experts in terms of
descriptive capability, interpretability, and understanding of dynamic
evolution. With the rapid development of Multi-modal Large Language Models
(MLLMs), these models become potential tools to overcome the above challenges.
In this work, we introduce an MLLM-based weather forecast analysis method,
RadarQA, integrating key physical attributes with detailed assessment reports.
We introduce a novel and comprehensive task paradigm for multi-modal quality
analysis, encompassing both single frame and sequence, under both rating and
assessment scenarios. To support training and benchmarking, we design a hybrid
annotation pipeline that combines human expert labeling with automated
heuristics. With such an annotation method, we construct RQA-70K, a large-scale
dataset with varying difficulty levels for radar forecast quality evaluation.
We further design a multi-stage training strategy that iteratively improves
model performance at each stage. Extensive experiments show that RadarQA
outperforms existing general MLLMs across all evaluation settings, highlighting
its potential for advancing quality analysis in weather prediction.

</details>


### [23] [Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback](https://arxiv.org/abs/2508.12338)
*Wenzhen Yuan,Shengji Tang,Weihao Lin,Jiacheng Ruan,Ganqu Cui,Bo Zhang,Tao Chen,Ting Liu,Yuzhuo Fu,Peng Ye,Lei Bai*

Main category: cs.AI

TL;DR: RLCCF is a novel RL method that uses collaborative feedback from multiple models to enhance their reasoning abilities without external supervision, achieving substantial accuracy improvements on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning for large language models is often hindered by the need for costly human-labeled data or complex reward systems, and single-model approaches risk issues like overconfidence and training collapse.

Method: The proposed RLCCF framework trains multiple models collaboratively, using a voting-based reward system that weights votes according to each model's confidence in its output. This coevolutionary strategy aims to improve the collective reasoning ability of the models.

Result: Experiments on various mathematical reasoning benchmarks show that RLCCF improves accuracy by 16.72% on average and enhances majority-voting accuracy by 4.51%, benefiting both individual models and the collective.

Conclusion: RLCCF demonstrates that coevolution among multiple models can expand their collective reasoning capability and overcome the inherent limitations of single-model training in reinforcement learning.

Abstract: Reinforcement learning (RL) has significantly enhanced the reasoning
capabilities of large language models (LLMs), but its reliance on expensive
human-labeled data or complex reward models severely limits scalability. While
existing self-feedback methods aim to address this problem, they are
constrained by the capabilities of a single model, which can lead to
overconfidence in incorrect answers, reward hacking, and even training
collapse. To this end, we propose Reinforcement Learning from Coevolutionary
Collective Feedback (RLCCF), a novel RL framework that enables multi-model
collaborative evolution without external supervision. Specifically, RLCCF
optimizes the ability of a model collective by maximizing its Collective
Consistency (CC), which jointly trains a diverse ensemble of LLMs and provides
reward signals by voting on collective outputs. Moreover, each model's vote is
weighted by its Self-Consistency (SC) score, ensuring that more confident
models contribute more to the collective decision. Benefiting from the diverse
output distributions and complementary abilities of multiple LLMs, RLCCF
enables the model collective to continuously enhance its reasoning ability
through coevolution. Experiments on four mainstream open-source LLMs across
four mathematical reasoning benchmarks demonstrate that our framework yields
significant performance gains, achieving an average relative improvement of
16.72\% in accuracy. Notably, RLCCF not only improves the performance of
individual models but also enhances the group's majority-voting accuracy by
4.51\%, demonstrating its ability to extend the collective capability boundary
of the model collective.

</details>


### [24] [Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems](https://arxiv.org/abs/2508.12375)
*Yu Sha,Shuiping Gou,Bo Liu,Johannes Faber,Ningtao Liu,Stefan Schramm,Horst Stoecker,Thomas Steckenreiter,Domagoj Vnucec,Nadine Wetzstein,Andreas Widl,Kai Zhou*

Main category: cs.AI

TL;DR: This paper introduces a novel hierarchical knowledge-guided fault intensity diagnosis framework (HKG), which utilizes graph convolutional networks and a re-weighted hierarchical knowledge correlation matrix (Re-HKCM) to enhance prediction accuracy for mechanical fault intensities.


<details>
  <summary>Details</summary>
Motivation: Current fault intensity diagnosis methods fail to consider dependencies among target classes. This paper aims to address this limitation by exploring hierarchical relationships for improved diagnosis.

Method: The framework employs graph convolutional networks to represent hierarchical class relationships and integrates with a re-weighted hierarchical knowledge correlation matrix (Re-HKCM) for effective information sharing during learning.

Result: Experiments on four real-world datasets from diverse industrial domains demonstrate superior performance of the proposed method, surpassing state-of-the-art fault intensity diagnosis techniques.

Conclusion: The hierarchical approach effectively captures class dependencies, providing improved predictive capabilities for fault intensity diagnosis in complex industrial systems.

Abstract: Fault intensity diagnosis (FID) plays a pivotal role in monitoring and
maintaining mechanical devices within complex industrial systems. As current
FID methods are based on chain of thought without considering dependencies
among target classes. To capture and explore dependencies, we propose a
hierarchical knowledge guided fault intensity diagnosis framework (HKG)
inspired by the tree of thought, which is amenable to any representation
learning methods. The HKG uses graph convolutional networks to map the
hierarchical topological graph of class representations into a set of
interdependent global hierarchical classifiers, where each node is denoted by
word embeddings of a class. These global hierarchical classifiers are applied
to learned deep features extracted by representation learning, allowing the
entire model to be end-to-end learnable. In addition, we develop a re-weighted
hierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding
inter-class hierarchical knowledge into a data-driven statistical correlation
matrix (SCM) which effectively guides the information sharing of nodes in
graphical convolutional neural networks and avoids over-smoothing issues. The
Re-HKCM is derived from the SCM through a series of mathematical
transformations. Extensive experiments are performed on four real-world
datasets from different industrial domains (three cavitation datasets from
SAMSON AG and one existing publicly) for FID, all showing superior results and
outperform recent state-of-the-art FID methods.

</details>


### [25] [Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks](https://arxiv.org/abs/2508.13143)
*Ruofan Lu,Yichen Li,Yintong Huo*

Main category: cs.AI

TL;DR: The paper discusses a benchmark framework for assessing autonomous agents using LLMs, identifies failure causes, and proposes improvements for system robustness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic analysis in evaluating autonomous agent systems, which currently focus mostly on success rates.

Method: The researchers designed a benchmark set of 34 tasks and applied it to three agent frameworks using two LLM backbones, analyzing success rates and failures.

Result: The study found a roughly 50% task completion rate and developed a taxonomy of failure causes across planning, execution, and response generation phases.

Conclusion: Insights from failure analysis led to proposed methods for improving agent planning and self-diagnosis, paving the way for more effective systems.

Abstract: Autonomous agent systems powered by Large Language Models (LLMs) have
demonstrated promising capabilities in automating complex tasks. However,
current evaluations largely rely on success rates without systematically
analyzing the interactions, communication mechanisms, and failure causes within
these systems. To bridge this gap, we present a benchmark of 34 representative
programmable tasks designed to rigorously assess autonomous agents. Using this
benchmark, we evaluate three popular open-source agent frameworks combined with
two LLM backbones, observing a task completion rate of approximately 50%.
Through in-depth failure analysis, we develop a three-tier taxonomy of failure
causes aligned with task phases, highlighting planning errors, task execution
issues, and incorrect response generation. Based on these insights, we propose
actionable improvements to enhance agent planning and self-diagnosis
capabilities. Our failure taxonomy, together with mitigation advice, provides
an empirical foundation for developing more robust and effective autonomous
agent systems in the future.

</details>


### [26] [GraphCogent: Overcoming LLMs' Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding](https://arxiv.org/abs/2508.12379)
*Rongzheng Wang,Qizhi Chen,Yihong Huang,Yizhuo Ma,Muquan Li,Jiakai Li,Ke Qin,Guangchun Luo,Shuang Liang*

Main category: cs.AI

TL;DR: Large language models struggle with complex real-world graph reasoning tasks, prompting the proposal of GraphCogent—a framework using modular cognitive processes to improve performance. A new benchmark, Graph4real, was also introduced, showing that GraphCogent significantly outperforms existing models.


<details>
  <summary>Details</summary>
Motivation: Existing large language models fail at efficiently tackling real-world graph reasoning tasks due to their challenges in processing complex graph topology and performing multi-step reasoning concurrently.

Method: The proposed GraphCogent framework breaks down graph reasoning into three cognitive modules: Sensory Module for standardizing graph representation, Buffer Module for integrating and indexing graph data, and Execution Module for tool-supported reasoning. A new benchmark, Graph4real, was developed to evaluate these capabilities using diverse graph domains and tasks.

Result: GraphCogent, using the Llama3.1-8B model, showed a 50% performance improvement over large models like DeepSeek-R1 (671B) and outperformed agent-based baselines by 20% in accuracy with significant token reductions in both in-toolset and out-toolset tasks.

Conclusion: GraphCogent demonstrates significant advancements in real-world graph reasoning tasks, showcasing improvements in both accuracy and efficiency. The framework and the new benchmark provide valuable steps forward for the capabilities of large language models in graph-based reasoning.

Abstract: Large language models (LLMs) show promising performance on small-scale graph
reasoning tasks but fail when handling real-world graphs with complex queries.
This phenomenon stems from LLMs' inability to effectively process complex graph
topology and perform multi-step reasoning simultaneously. To address these
limitations, we propose GraphCogent, a collaborative agent framework inspired
by human Working Memory Model that decomposes graph reasoning into specialized
cognitive processes: sense, buffer, and execute. The framework consists of
three modules: Sensory Module standardizes diverse graph text representations
via subgraph sampling, Buffer Module integrates and indexes graph data across
multiple formats, and Execution Module combines tool calling and model
generation for efficient reasoning. We also introduce Graph4real, a
comprehensive benchmark contains with four domains of real-world graphs (Web,
Social, Transportation, and Citation) to evaluate LLMs' graph reasoning
capabilities. Our Graph4real covers 21 different graph reasoning tasks,
categorized into three types (Structural Querying, Algorithmic Reasoning, and
Predictive Modeling tasks), with graph scales that are 10 times larger than
existing benchmarks. Experiments show that Llama3.1-8B based GraphCogent
achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B).
Compared to state-of-the-art agent-based baseline, our framework outperforms by
20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30%
for out-toolset tasks. Code will be available after review.

</details>


### [27] [Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning](https://arxiv.org/abs/2508.12425)
*Phuong Minh Nguyen,Tien Huu Dang,Naoya Inoue*

Main category: cs.AI

TL;DR: The paper introduces Symbolic-Aided Chain-of-Thought (CoT), a method to enhance logical reasoning in LLMs by incorporating symbolic representations, showing improvements over standard CoT across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Standard Chain-of-Thought (CoT) in LLMs lacks transparency, interpretability, and effectiveness in complex logical reasoning tasks.

Method: Integrating lightweight symbolic representations into few-shot prompts to structure inference steps, enhancing reasoning patterns within a non-iterative process.

Result: Symbolic-Aided CoT significantly outperforms standard CoT in reasoning benchmarks, improving LLM logical reasoning across various model sizes.

Conclusion: The incorporation of symbolic structures improves LLMs' logical reasoning capabilities, particularly for complex tasks, while maintaining generalizability and interpretability.

Abstract: This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved
approach to standard CoT, for logical reasoning in large language models
(LLMs). The key idea is to integrate lightweight symbolic representations into
few-shot prompts, structuring the inference steps with a consistent strategy to
make reasoning patterns more explicit within a non-iterative reasoning process.
By incorporating these symbolic structures, our method preserves the
generalizability of standard prompting techniques while enhancing the
transparency, interpretability, and analyzability of LLM logical reasoning.
Extensive experiments on four well-known logical reasoning benchmarks --
ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse
reasoning scenarios -- demonstrate the effectiveness of the proposed approach,
particularly in complex reasoning tasks that require navigating multiple
constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'
reasoning capabilities across various model sizes and significantly outperforms
conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and
LogicalDeduction.

</details>


### [28] [GALA: Can Graph-Augmented Large Language Model Agentic Workflows Elevate Root Cause Analysis?](https://arxiv.org/abs/2508.12472)
*Yifang Tian,Yaming Liu,Zichun Chong,Zihang Huang,Hans-Arno Jacobsen*

Main category: cs.AI

TL;DR: This paper proposes GALA, a multi-modal RCA framework for diagnosing failures in microservice systems, combining statistical causal inference with LLM-driven reasoning for better performance.


<details>
  <summary>Details</summary>
Motivation: Traditional RCA methods often rely on single data modalities or service ranking, failing to provide actionable insights for resolving microservice failures.

Method: The framework, named GALA, merges statistical causal inference with iterative reasoning using LLMs (Large Language Models) to improve RCA diagnosis.

Result: GALA achieves up to 42.22% higher accuracy compared to existing methods and produces more causally sound and actionable diagnostic outputs.

Conclusion: GALA enhances failure diagnosis and incident resolution in microservice systems by providing accurate root cause identification and practical remediation guidance, effectively bridging automation and human usability.

Abstract: Root cause analysis (RCA) in microservice systems is challenging, requiring
on-call engineers to rapidly diagnose failures across heterogeneous telemetry
such as metrics, logs, and traces. Traditional RCA methods often focus on
single modalities or merely rank suspect services, falling short of providing
actionable diagnostic insights with remediation guidance. This paper introduces
GALA, a novel multi-modal framework that combines statistical causal inference
with LLM-driven iterative reasoning for enhanced RCA. Evaluated on an
open-source benchmark, GALA achieves substantial improvements over
state-of-the-art methods of up to 42.22% accuracy. Our novel human-guided LLM
evaluation score shows GALA generates significantly more causally sound and
actionable diagnostic outputs than existing methods. Through comprehensive
experiments and a case study, we show that GALA bridges the gap between
automated failure diagnosis and practical incident resolution by providing both
accurate root cause identification and human-interpretable remediation
guidance.

</details>


### [29] [The Yokai Learning Environment: Tracking Beliefs Over Space and Time](https://arxiv.org/abs/2508.12480)
*Constantin Ruhdorfer,Matteo Bortoletto,Andreas Bulling*

Main category: cs.AI

TL;DR: This paper presents a new multi-agent RL environment, Yokai Learning Environment, to study Theory of Mind capabilities required for maintaining common ground in collaborative tasks.


<details>
  <summary>Details</summary>
Motivation: Existing Theory of Mind benchmarks are limited in scenarios or fail to assess how agents establish and maintain common ground over time.

Method: The authors introduce Yokai Learning Environment (YLE), where agents collaboratively play a card game requiring belief tracking, memory, communication, and maintaining shared common ground.

Result: Current RL agents struggle in YLE even with perfect memory access, showing reliance on brittle conventions and poor generalisation abilities over time or across partners.

Conclusion: The YLE serves as a valuable tool for studying advanced AI capabilities like higher-order Theory of Mind, belief modelling, and robust collaboration frameworks.

Abstract: Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to
reason about the beliefs of others to build and maintain common ground.
Existing ToM benchmarks, however, are restricted to passive observer settings
or lack an assessment of how agents establish and maintain common ground over
time. To address these gaps, we introduce the Yokai Learning Environment (YLE)
- a multi-agent reinforcement learning (RL) environment based on the
cooperative card game Yokai. In the YLE, agents take turns peeking at hidden
cards and moving them to form clusters based on colour. Success requires
tracking evolving beliefs, remembering past observations, using hints as
grounded communication, and maintaining common ground with teammates. Our
evaluation yields two key findings: First, current RL agents struggle to solve
the YLE, even when given access to perfect memory. Second, while belief
modelling improves performance, agents are still unable to effectively
generalise to unseen partners or form accurate beliefs over longer games,
exposing a reliance on brittle conventions rather than robust belief tracking.
We use the YLE to investigate research questions in belief modelling, memory,
partner generalisation, and scaling to higher-order ToM.

</details>


### [30] [Advanced DOA Regulation with a Whale-Optimized Fractional Order Fuzzy PID Framework](https://arxiv.org/abs/2508.12487)
*Lida Shahbandari,Hossein Mohseni*

Main category: cs.AI

TL;DR: The paper introduces a FOFPID controller optimized by WOA to regulate Bispectral Index with superior performance compared to standard FOPID controllers.


<details>
  <summary>Details</summary>
Motivation: Enhance automated anesthesia delivery by addressing individual patient physiology for precise control of Bispectral Index.

Method: Utilizing FOFPID controllers with fractional order dynamics and fuzzy logic, optimized by Whale Optimization Algorithm.

Result: Tested on eight patient models, FOFPID showed faster settling times (2.5 vs. 3.2 minutes) and lower steady state errors (0.5 vs. 1.2).

Conclusion: FOFPID demonstrates scalability, AI-driven precision, and improved clinical outcomes, outperforming traditional control methods.

Abstract: This study introduces a Fractional Order Fuzzy PID (FOFPID) controller that
uses the Whale Optimization Algorithm (WOA) to manage the Bispectral Index
(BIS), keeping it within the ideal range of forty to sixty. The FOFPID
controller combines fuzzy logic for adapting to changes and fractional order
dynamics for fine tuning. This allows it to adjust its control gains to handle
a person's unique physiology. The WOA helps fine tune the controller's
parameters, including the fractional orders and the fuzzy membership functions,
which boosts its performance. Tested on models of eight different patient
profiles, the FOFPID controller performed better than a standard Fractional
Order PID (FOPID) controller. It achieved faster settling times, at two and a
half minutes versus three point two minutes, and had a lower steady state
error, at zero point five versus one point two. These outcomes show the
FOFPID's excellent strength and accuracy. It offers a scalable, artificial
intelligence driven solution for automated anesthesia delivery that could
enhance clinical practice and improve patient results.

</details>


### [31] [Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models](https://arxiv.org/abs/2508.12500)
*Rahmat K. Adesunkanmi,Ashfaq Khokhar,Goce Trajcevski,Sohail Murad*

Main category: cs.AI

TL;DR: The paper addresses challenges in molecular dynamics simulations (MDS) by proposing a machine learning-based framework to analyze causes of hydrogen bond formation and separation using causal models.


<details>
  <summary>Details</summary>
Motivation: MDS require computationally expensive resources and manual inspection to detect important phenomena like hydrogen bond dynamics. There is a research gap in understanding the root causes and interactions leading to these bond events.

Method: The authors use spatio-temporal analytics, machine learning, and causal modeling. They employ a variational autoencoder-inspired architecture to capture causal structures and identify root causes of hydrogen bond events, viewing bond separations as interventions.

Result: The proposed model successfully predicts steps in atomic trajectories and identifies variables driving changes in the system. This is validated using MDS atomic trajectories for chiral separation.

Conclusion: The paper introduces a novel causal inference framework that enhances root cause analysis in molecular dynamic systems, shedding light on the underlying factors of molecular interactions and improving predictability.

Abstract: Molecular dynamics simulations (MDS) face challenges, including
resource-heavy computations and the need to manually scan outputs to detect
"interesting events," such as the formation and persistence of hydrogen bonds
between atoms of different molecules. A critical research gap lies in
identifying the underlying causes of hydrogen bond formation and separation
-understanding which interactions or prior events contribute to their emergence
over time. With this challenge in mind, we propose leveraging spatio-temporal
data analytics and machine learning models to enhance the detection of these
phenomena. In this paper, our approach is inspired by causal modeling and aims
to identify the root cause variables of hydrogen bond formation and separation
events. Specifically, we treat the separation of hydrogen bonds as an
"intervention" occurring and represent the causal structure of the bonding and
separation events in the MDS as graphical causal models. These causal models
are built using a variational autoencoder-inspired architecture that enables us
to infer causal relationships across samples with diverse underlying causal
graphs while leveraging shared dynamic information. We further include a step
to infer the root causes of changes in the joint distribution of the causal
models. By constructing causal models that capture shifts in the conditional
distributions of molecular interactions during bond formation or separation,
this framework provides a novel perspective on root cause analysis in molecular
dynamic systems. We validate the efficacy of our model empirically on the
atomic trajectories that used MDS for chiral separation, demonstrating that we
can predict many steps in the future and also find the variables driving the
observed changes in the system.

</details>


### [32] [Help or Hurdle? Rethinking Model Context Protocol-Augmented Large Language Models](https://arxiv.org/abs/2508.12566)
*Wei Song,Haonan Zhong,Ziqi Ding,Jingling Xue,Yuekang Li*

Main category: cs.AI

TL;DR: The paper introduces MCPGAUGE to evaluate large language models' interactions with the Model Context Protocol (MCP), which supports external tool usage.


<details>
  <summary>Details</summary>
Motivation: To better understand how LLMs use external tools via MCP and challenge assumptions about its effectiveness.

Method: Developed MCPGAUGE framework with 160 prompts, 25 datasets, and large-scale evaluations across six LLMs and 30 MCP tool suites.

Result: Results revealed limitations in current AI-tool integration along four dimensions: proactivity, compliance, effectiveness, and overhead.

Conclusion: MCPGAUGE provides a systematic benchmark to address limitations in tool-augmented LLMs and guide future advancements.

Abstract: The Model Context Protocol (MCP) enables large language models (LLMs) to
access external resources on demand. While commonly assumed to enhance
performance, how LLMs actually leverage this capability remains poorly
understood. We introduce MCPGAUGE, the first comprehensive evaluation framework
for probing LLM-MCP interactions along four key dimensions: proactivity
(self-initiated tool use), compliance (adherence to tool-use instructions),
effectiveness (task performance post-integration), and overhead (computational
cost incurred). MCPGAUGE comprises a 160-prompt suite and 25 datasets spanning
knowledge comprehension, general reasoning, and code generation. Our
large-scale evaluation, spanning six commercial LLMs, 30 MCP tool suites, and
both one- and two-turn interaction settings, comprises around 20,000 API calls
and over USD 6,000 in computational cost. This comprehensive study reveals four
key findings that challenge prevailing assumptions about the effectiveness of
MCP integration. These insights highlight critical limitations in current
AI-tool integration and position MCPGAUGE as a principled benchmark for
advancing controllable, tool-augmented LLMs.

</details>


### [33] [An LLM + ASP Workflow for Joint Entity-Relation Extraction](https://arxiv.org/abs/2508.12611)
*Trang Tran,Trung Hoang Le,Huiping Cao,Tran Cao Son*

Main category: cs.AI

TL;DR: The paper introduces a method leveraging large language models (LLMs) and Answer Set Programming (ASP) for joint entity-relation extraction (JERE), achieving robust performance even with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Traditional JERE methods demand extensive annotated data and lack flexibility to incorporate domain-specific knowledge, making them labor-intensive and inefficient.

Method: The proposed method combines generative pretrained LLMs for natural language understanding with ASP for incorporating domain-specific knowledge efficiently. It operates on unannotated text and doesn't need core program modification for elaboration.

Result: Experiments on three benchmarks demonstrate that their workflow outperforms state-of-the-art JERE systems in several aspects, achieving significant improvements with only 10% of training data.

Conclusion: The LLM + ASP-based workflow shows strong potential for efficient and scalable JERE across various domains, significantly reducing the dependency on large annotated datasets.

Abstract: Joint entity-relation extraction (JERE) identifies both entities and their
relationships simultaneously. Traditional machine-learning based approaches to
performing this task require a large corpus of annotated data and lack the
ability to easily incorporate domain specific information in the construction
of the model. Therefore, creating a model for JERE is often labor intensive,
time consuming, and elaboration intolerant. In this paper, we propose
harnessing the capabilities of generative pretrained large language models
(LLMs) and the knowledge representation and reasoning capabilities of Answer
Set Programming (ASP) to perform JERE. We present a generic workflow for JERE
using LLMs and ASP. The workflow is generic in the sense that it can be applied
for JERE in any domain. It takes advantage of LLM's capability in natural
language understanding in that it works directly with unannotated text. It
exploits the elaboration tolerant feature of ASP in that no modification of its
core program is required when additional domain specific knowledge, in the form
of type specifications, is found and needs to be used. We demonstrate the
usefulness of the proposed workflow through experiments with limited training
data on three well-known benchmarks for JERE. The results of our experiments
show that the LLM + ASP workflow is better than state-of-the-art JERE systems
in several categories with only 10\% of training data. It is able to achieve a
2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the
SciERC corpus, one of the most difficult benchmarks.

</details>


### [34] [Cognitive Structure Generation: From Educational Priors to Policy Optimization](https://arxiv.org/abs/2508.12647)
*Hengnian Gu,Zhifu Chen,Yuxin Chen,Jin Peng Zhou,Dongdai Zhou*

Main category: cs.AI

TL;DR: This paper proposes Cognitive Structure Generation (CSG) to better model and assess student cognitive structures using probabilistic models and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: There exists a challenge in effectively assessing students' cognitive structures, which are key in understanding their psychological construction of concepts and relations.

Method: The authors developed Cognitive Structure Generation (CSG), leveraging a Cognitive Structure Diffusion Probabilistic Model and reinforcement learning to align with actual cognitive development levels.

Result: Experiments on educational datasets demonstrate that CSG improves the representation of cognitive structures, leading to better performance on knowledge tracing (KT) and cognitive diagnosis (CD) tasks.

Conclusion: CSG provides a novel and effective way to represent cognitive structures, offering gains in both task performance and interpretability in education systems.

Abstract: Cognitive structure is a student's subjective organization of an objective
knowledge system, reflected in the psychological construction of concepts and
their relations. However, cognitive structure assessment remains a
long-standing challenge in student modeling and psychometrics, persisting as a
foundational yet largely unassessable concept in educational practice. This
paper introduces a novel framework, Cognitive Structure Generation (CSG), in
which we first pretrain a Cognitive Structure Diffusion Probabilistic Model
(CSDPM) to generate students' cognitive structures from educational priors, and
then further optimize its generative process as a policy with hierarchical
reward signals via reinforcement learning to align with genuine cognitive
development levels during students' learning processes. Experimental results on
four popular real-world education datasets show that cognitive structures
generated by CSG offer more comprehensive and effective representations for
student modeling, substantially improving performance on KT and CD tasks while
enhancing interpretability.

</details>


### [35] [The Maximum Coverage Model and Recommendation System for UAV Vertiports Location Planning](https://arxiv.org/abs/2508.12651)
*Chunliang Hua,Xiao Hu,Jiayang Sun,Zeyuan Yang*

Main category: cs.AI

TL;DR: The paper presents a new optimization framework and recommendation system for planning large-scale vertiport networks for urban aerial mobility (UAM), improving traditional methods by 38–52%.


<details>
  <summary>Details</summary>
Motivation: Existing planning frameworks for UAM infrastructure are insufficient due to limitations in data granularity and real-world applicability, especially for complex scenarios like large-scale vertiport networks.

Method: The study proposes the Capacitated Dynamic Maximum Covering Location Problem (CDMCLP) as an optimization framework and introduces an Integrated Planning Recommendation System that incorporates socio-economic factors and adaptive parameter tuning based on user behavior.

Result: Validation in a Chinese city demonstrates that the framework enhances traditional location methods by 38–52% and proves user-friendly while integrating complex elements effectively.

Conclusion: The hybrid approach bridges theoretical modeling and real-world planning, offering municipalities a practical tool for designing UAM infrastructure.

Abstract: As urban aerial mobility (UAM) infrastructure development accelerates
globally, cities like Shenzhen are planning large-scale vertiport networks
(e.g., 1,200+ facilities by 2026). Existing planning frameworks remain
inadequate for this complexity due to historical limitations in data
granularity and real-world applicability. This paper addresses these gaps by
first proposing the Capacitated Dynamic Maximum Covering Location Problem
(CDMCLP), a novel optimization framework that simultaneously models urban-scale
spatial-temporal demand, heterogeneous user behaviors, and infrastructure
capacity constraints. Building on this foundation, we introduce an Integrated
Planning Recommendation System that combines CDMCLP with socio-economic factors
and dynamic clustering initialization. This system leverages adaptive parameter
tuning based on empirical user behavior to generate practical planning
solutions. Validation in a Chinese center city demonstrates the effectiveness
of the new optimization framework and recommendation system. Under the
evaluation and optimization of CDMCLP, the quantitative performance of
traditional location methods are exposed and can be improved by 38\%--52\%,
while the recommendation system shows user-friendliness and the effective
integration of complex elements. By integrating mathematical rigor with
practical implementation considerations, this hybrid approach bridges the gap
between theoretical location modeling and real-world UAM infrastructure
planning, offering municipalities a pragmatic tool for vertiport network
design.

</details>


### [36] [GridCodex: A RAG-Driven AI Framework for Power Grid Code Reasoning and Compliance](https://arxiv.org/abs/2508.12682)
*Jinquan Shi,Yingying Cheng,Fan Zhang,Miao Jiang,Jun Lin,Yanbai Shen*

Main category: cs.AI

TL;DR: The paper presents GridCodex, a framework leveraging large language models and retrieval-augmented generation (RAG) for grid code reasoning and compliance, offering significant improvements in answer quality and recall rate.


<details>
  <summary>Details</summary>
Motivation: The shift to renewable energy requires robust grid regulation and compliance solutions, but existing grid codes are complex and lack automated interpretation, posing challenges for the electricity industry.

Method: GridCodex combines large language models and RAG with multi-stage query refinement and enhanced retrieval techniques like RAPTOR to enable effective grid code reasoning and compliance.

Result: GridCodex achieved a 26.4% improvement in answer quality and a more than 10-fold improvement in recall rate, validated through comprehensive benchmarks and ablation studies.

Conclusion: GridCodex demonstrates the potential of advanced AI techniques to address the complexity of grid codes, improving regulatory reasoning and supporting the profitable expansion of the electricity industry.

Abstract: The global shift towards renewable energy presents unprecedented challenges
for the electricity industry, making regulatory reasoning and compliance
increasingly vital. Grid codes, the regulations governing grid operations, are
complex and often lack automated interpretation solutions, which hinders
industry expansion and undermines profitability for electricity companies. We
introduce GridCodex, an end to end framework for grid code reasoning and
compliance that leverages large language models and retrieval-augmented
generation (RAG). Our framework advances conventional RAG workflows through
multi stage query refinement and enhanced retrieval with RAPTOR. We validate
the effectiveness of GridCodex with comprehensive benchmarks, including
automated answer assessment across multiple dimensions and regulatory agencies.
Experimental results showcase a 26.4% improvement in answer quality and more
than a 10 fold increase in recall rate. An ablation study further examines the
impact of base model selection.

</details>


### [37] [EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding](https://arxiv.org/abs/2508.12687)
*Ashish Seth,Utkarsh Tyagi,Ramaneswaran Selvakumar,Nishit Anand,Sonal Kumar,Sreyan Ghosh,Ramani Duraiswami,Chirag Agarwal,Dinesh Manocha*

Main category: cs.AI

TL;DR: The paper introduces EgoIllusion, a benchmark for assessing hallucinations in Multimodal Large Language Models (MLLMs) when processing egocentric videos.


<details>
  <summary>Details</summary>
Motivation: MLLMs perform well on multimodal tasks but are prone to hallucinations, particularly when handling egocentric video data, which calls for a methodical way to evaluate and improve these weaknesses.

Method: The authors developed EgoIllusion, a benchmark featuring 1,400 egocentric videos and 8,000 human-annotated questions designed to trigger hallucinations in MLLMs. Ten MLLMs were evaluated for their performance.

Result: Even advanced models like GPT-4o and Gemini achieved only 59% accuracy, highlighting considerable challenges with hallucinations in egocentric video analysis.

Conclusion: EgoIllusion exposes the limitations of current MLLMs in handling egocentric videos and provides an essential foundation for improving these models. The benchmark will be open-sourced for further research.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
performance in complex multimodal tasks. While MLLMs excel at visual perception
and reasoning in third-person and egocentric videos, they are prone to
hallucinations, generating coherent yet inaccurate responses. We present
EgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric
videos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated
open and closed-ended questions designed to trigger hallucinations in both
visual and auditory cues in egocentric videos. Evaluations across ten MLLMs
reveal significant challenges, including powerful models like GPT-4o and
Gemini, achieving only 59% accuracy. EgoIllusion lays the foundation in
developing robust benchmarks to evaluate the effectiveness of MLLMs and spurs
the development of better egocentric MLLMs with reduced hallucination rates.
Our benchmark will be open-sourced for reproducibility.

</details>


### [38] [GTool: Graph Enhanced Tool Planning with Large Language Model](https://arxiv.org/abs/2508.12725)
*Wenjie Chen,Wenbin Li,Di Yao,Xuying Meng,Chang Gong,Jingping Bi*

Main category: cs.AI

TL;DR: GTool introduces a method to enhance large language models (LLMs) for tool planning under incomplete dependencies by constructing a request-specific tool graph and predicting missing dependencies, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Current systems fail to leverage inherent dependencies between tools during planning, leading to challenges in selecting tools accurately for complex user requests, especially with large toolsets and incomplete dependencies.

Method: GTool creates a request-specific tool graph to efficiently select tools and generates a specialized <graph token> for LLMs to understand dependencies. It also introduces a missing dependency prediction task to boost reliability under incomplete dependencies.

Result: Extensive evaluations show that GTool improves tool-planning performance by over 29.6% compared to state-of-the-art methods, leveraging a lightweight LLM backbone (7B) without intensive retraining.

Conclusion: GTool enhances tool-planning capability in LLMs by exploiting tool dependencies effectively, offering a seamless integration with various LLMs while addressing the limitation of incomplete dependencies.

Abstract: Tool planning with large language models (LLMs), referring to selecting,
organizing, and preparing the tools necessary to complete a user request,
bridges the gap between natural language understanding and task execution.
However, current works treat different tools as isolated components and fail to
leverage the inherent dependencies of tools, leading to invalid planning
results. Since tool dependencies are often incomplete, it becomes challenging
for LLMs to accurately identify the appropriate tools required by a user
request, especially when confronted with a large toolset. To solve this
challenge, we propose \texttt{GTool}, which is the first work aiming to enhance
the tool planning ability of LLMs under incomplete dependencies. \texttt{GTool}
constructs a request-specific tool graph to select tools efficiently and
generate the \texttt{<graph token>} which provides sufficient dependency
information understandable by LLMs. Moreover, a missing dependency prediction
task is designed to improve the reliability of \texttt{GTool} with incomplete
dependencies. Without trimming LLMs, \texttt{GTool} can be seamlessly
integrated with various LLM backbones without extensive retraining. Extensive
experiments show that \texttt{GTool} achieves more than 29.6\% performance
improvements compared with the state-of-the-art (SOTA) baselines with a
light-weight (7B) LLM backbone.

</details>


### [39] [Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants](https://arxiv.org/abs/2508.12754)
*Alessio Galatolo,Luca Alberto Rappuoli,Katie Winkle,Meriem Beloucif*

Main category: cs.AI

TL;DR: The paper examines the moral reasoning capabilities of large language models (LLMs) by proposing a benchmark to evaluate their role as Artificial Moral Assistants (AMAs). Results reveal variable performance and highlight deficiencies, particularly in abductive reasoning.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address concerns about the superficial evaluation of LLMs' alignment with human moral values and their effectiveness in supporting moral deliberation.

Method: A new theoretical framework was developed based on philosophical literature, followed by the creation of a benchmark to measure deductive and abductive moral reasoning in LLMs.

Result: Evaluations showed significant variability among popular LLMs, with notable weaknesses in abductive moral reasoning capabilities.

Conclusion: The study underscores the importance of explicit strategies to enhance the moral reasoning abilities of LLMs and connects theoretical philosophy with applied AI research.

Abstract: The recent rise in popularity of large language models (LLMs) has prompted
considerable concerns about their moral capabilities. Although considerable
effort has been dedicated to aligning LLMs with human moral values, existing
benchmarks and evaluations remain largely superficial, typically measuring
alignment based on final ethical verdicts rather than explicit moral reasoning.
In response, this paper aims to advance the investigation of LLMs' moral
capabilities by examining their capacity to function as Artificial Moral
Assistants (AMAs), systems envisioned in the philosophical literature to
support human moral deliberation. We assert that qualifying as an AMA requires
more than what state-of-the-art alignment techniques aim to achieve: not only
must AMAs be able to discern ethically problematic situations, they should also
be able to actively reason about them, navigating between conflicting values
outside of those embedded in the alignment phase. Building on existing
philosophical literature, we begin by designing a new formal framework of the
specific kind of behaviour an AMA should exhibit, individuating key qualities
such as deductive and abductive moral reasoning. Drawing on this theoretical
framework, we develop a benchmark to test these qualities and evaluate popular
open LLMs against it. Our results reveal considerable variability across models
and highlight persistent shortcomings, particularly regarding abductive moral
reasoning. Our work connects theoretical philosophy with practical AI
evaluation while also emphasising the need for dedicated strategies to
explicitly enhance moral reasoning capabilities in LLMs. Code available at
https://github.com/alessioGalatolo/AMAeval

</details>


### [40] [HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds](https://arxiv.org/abs/2508.12782)
*Petr Anokhin,Roman Khalikov,Stefan Rebrikov,Viktor Volkov,Artyom Sorokin,Vincent Bissonnette*

Main category: cs.AI

TL;DR: The paper introduces HeroBench, a benchmark for assessing long-horizon planning and structured reasoning in RPG-like virtual worlds, highlighting key model shortcomings.


<details>
  <summary>Details</summary>
Motivation: To tackle the limited exploration of LLMs' performance in long-horizon planning tasks, which require extended and interdependent reasoning beyond isolated step-by-step tasks.

Method: Developed HeroBench, a benchmark with diverse, difficulty-graded tasks, a virtual environment for plan execution, and performance evaluation tools; tested 25 advanced LLMs to identify disparities and weaknesses.

Result: Performance analysis revealed marked disparities among LLMs and specific deficiencies in long-term planning and structured action execution.

Conclusion: HeroBench sets a new standard for evaluating LLM planning, providing a scalable basis for future research in autonomous reasoning and planning capabilities.

Abstract: Large language models (LLMs) have shown remarkable capabilities in isolated
step-by-step reasoning tasks such as mathematics and programming, but their
proficiency in long-horizon planning, where solutions require extended,
structured sequences of interdependent actions, remains underexplored. Existing
benchmarks typically assess LLMs through abstract or low-dimensional
algorithmic tasks, failing to capture the complexity of realistic planning
environments. We introduce HeroBench, a novel benchmark designed specifically
to evaluate long-horizon planning and structured reasoning within complex
RPG-inspired virtual worlds. HeroBench provides a rigorously constructed
dataset of tasks covering a wide range of difficulties, a simulated environment
to execute and validate agent plans, and detailed analytical tools for
evaluating model performance. Tasks challenge models to formulate strategic
plans, efficiently gather resources, master necessary skills, craft equipment,
and defeat adversaries, reflecting practical scenarios' layered dependencies
and constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning
both open-source and proprietary models, including the GPT-5 family, reveals
substantial performance disparities rarely observed in conventional reasoning
benchmarks. Detailed error analysis further uncovers specific weaknesses in
current models' abilities to generate robust high-level plans and reliably
execute structured actions. HeroBench thus not only significantly advances the
evaluation of LLM reasoning but also provides a flexible, scalable foundation
for future research into advanced, autonomous planning in virtual environments.

</details>


### [41] [Reinforcement Learning with Rubric Anchors](https://arxiv.org/abs/2508.12790)
*Zenan Huang,Yihong Zhuang,Guoshan Lu,Zeyu Qin,Haokai Xu,Tianyu Zhao,Ru Peng,Jiaqi Hu,Zhanming Shen,Xiaomeng Hu,Xijun Gu,Peiyi Tu,Jiaxin Liu,Wenyu Chen,Yuzhuo Fu,Zhiting Fan,Yanmei Gu,Yuanyuan Wang,Zhengkai Yang,Jianguo Li,Junbo Zhao*

Main category: cs.AI

TL;DR: This paper introduces an extension to Reinforcement Learning from Verifiable Rewards (RLVR) for open-ended tasks using rubric-based rewards, resulting in significant benchmarks and stylistic improvements.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of RLVR being confined to domains with automatically checkable outcomes and extend its applicability to open-ended tasks.

Method: Rubric-based rewards are used as structured criteria for automatic scoring of subjective outputs, leveraging over 10,000 rubrics created by humans, LLMs, or hybrid collaborations.

Result: The Qwen-30B-A3B model achieves a +5.2% improvement on open-ended benchmarks with only 5K+ samples, outperforming a 671B DeepSeek-V3 model by +2.4%, while maintaining reasoning abilities.

Conclusion: Rubric-based RL enhances human-like expressive responses and stylistic control, and highlights challenges in rubric creation and training for scaling future releases.

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing Large Language Models (LLMs), exemplified by
the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable
signals-such as passing unit tests in code generation or matching correct
answers in mathematical reasoning. While effective, this requirement largely
confines RLVR to domains with automatically checkable outcomes. To overcome
this, we extend the RLVR paradigm to open-ended tasks by integrating
rubric-based rewards, where carefully designed rubrics serve as structured,
model-interpretable criteria for automatic scoring of subjective outputs. We
construct, to our knowledge, the largest rubric reward system to date, with
over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.
Implementing rubric-based RL is challenging; we tackle these issues with a
clear framework and present an open-sourced Qwen-30B-A3B model with notable
gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended
benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by
+2.4%, while preserving general and reasoning abilities. 2) Our method provides
fine-grained stylistic control, using rubrics as anchors to mitigate the
"AI-like" tone and produce more human-like, expressive responses. We share key
lessons in rubric construction, data selection, and training, and discuss
limitations and future releases.

</details>


### [42] [[Social] Allostasis: Or, How I Learned To Stop Worrying and Love The Noise](https://arxiv.org/abs/2508.12791)
*Imran Khan*

Main category: cs.AI

TL;DR: This paper develops a computational model to explore the concept of social allostasis, using simulated agents in dynamic environments to demonstrate improved adaptability compared to traditional homeostasis.


<details>
  <summary>Details</summary>
Motivation: Explore adaptive mechanisms in biological and artificial systems, shifting from stability-focused homeostasis to proactive social allostasis.

Method: Developed agent-based computational models simulating biophysiological mechanisms (akin to hormones like cortisol and oxytocin) to study dynamic reconfiguration in artificial societies.

Result: Simulated agents using allostatic regulation adapt more effectively to dynamic environments and social changes compared to homeostatic agents.

Conclusion: Social allostasis provides a framework for designing adaptive, bio-inspired systems that use environmental and social variability as resources for improved viability and robustness.

Abstract: The notion of homeostasis typically conceptualises biological and artificial
systems as maintaining stability by resisting deviations caused by
environmental and social perturbations. In contrast, (social) allostasis
proposes that these systems can proactively leverage these very perturbations
to reconfigure their regulatory parameters in anticipation of environmental
demands, aligning with von Foerster's ``order through noise'' principle. This
paper formulates a computational model of allostatic and social allostatic
regulation that employs biophysiologically inspired signal transducers,
analogous to hormones like cortisol and oxytocin, to encode information from
both the environment and social interactions, which mediate this dynamic
reconfiguration. The models are tested in a small society of ``animats'' across
several dynamic environments, using an agent-based model. The results show that
allostatic and social allostatic regulation enable agents to leverage
environmental and social ``noise'' for adaptive reconfiguration, leading to
improved viability compared to purely reactive homeostatic agents. This work
offers a novel computational perspective on the principles of social allostasis
and their potential for designing more robust, bio-inspired, adaptive systems

</details>


### [43] [Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics](https://arxiv.org/abs/2508.12840)
*Giovanni Briglia,Francesco Fabiano,Stefano Mariani*

Main category: cs.AI

TL;DR: The paper introduces Graph Neural Networks (GNNs) as heuristics to improve the scalability of Multi-agent Epistemic Planning (MEP) by learning relational structures in epistemic states.


<details>
  <summary>Details</summary>
Motivation: The scalability of epistemic solvers in MEP is hindered by the requirement to explore an exponential search space without effective guidance, making the planning process intractable.

Method: The authors employ Graph Neural Networks to learn patterns within epistemic states (represented as Kripke structures) and derive meaningful estimates of state quality. These predictive heuristics are incorporated into the epistemic planning pipeline.

Result: The GNN-based predictive heuristics significantly improve the scalability of solving multi-agent epistemic planning problems compared to standard baselines.

Conclusion: Applying GNNs as predictive heuristics in MEP effectively addresses the scalability challenges, allowing for more tractable and efficient planning across complex domains.

Abstract: Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for
reasoning about both the physical world and the beliefs of agents, with
applications in domains where information flow and awareness among agents are
critical. The richness of MEP requires states to be represented as Kripke
structures, i.e., directed labeled graphs. This representation limits the
applicability of existing heuristics, hindering the scalability of epistemic
solvers, which must explore an exponential search space without guidance,
resulting often in intractability. To address this, we exploit Graph Neural
Networks (GNNs) to learn patterns and relational structures within epistemic
states, to guide the planning process. GNNs, which naturally capture the
graph-like nature of Kripke models, allow us to derive meaningful estimates of
state quality -- e.g., the distance from the nearest goal -- by generalizing
knowledge obtained from previously solved planning instances. We integrate
these predictive heuristics into an epistemic planning pipeline and evaluate
them against standard baselines, showing significant improvements in the
scalability of multi-agent epistemic planning.

</details>


### [44] [CAMAR: Continuous Actions Multi-Agent Routing](https://arxiv.org/abs/2508.12845)
*Artem Pshenitsyn,Aleksandr Panov,Alexey Skrynnik*

Main category: cs.AI

TL;DR: CAMAR is a new MARL benchmark focused on multi-agent pathfinding with continuous actions, supporting cooperative and competitive interactions efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the lack of MARL benchmarks combining continuous action spaces and complex coordination tasks, ensuring algorithmic progress tracking and evaluation.

Method: CAMAR introduces a benchmark for continuous-action MARL combined with classical planning methods like RRT and RRT*. It includes standalone and hybrid approaches with evaluation tools for reproducibility.

Result: Experiments demonstrate CAMAR's challenge as a realistic testbed for multi-agent reinforcement learning research.

Conclusion: CAMAR successfully provides an advanced platform that combines MARL and classical planning, fostering better progress tracking and fair analysis.

Abstract: Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving
cooperative and competitive decision-making problems. While many MARL
benchmarks have been proposed, few combine continuous state and action spaces
with challenging coordination and planning tasks. We introduce CAMAR, a new
MARL benchmark designed explicitly for multi-agent pathfinding in environments
with continuous actions. CAMAR supports cooperative and competitive
interactions between agents and runs efficiently at up to 100,000 environment
steps per second. We also propose a three-tier evaluation protocol to better
track algorithmic progress and enable deeper analysis of performance. In
addition, CAMAR allows the integration of classical planning methods such as
RRT and RRT* into MARL pipelines. We use them as standalone baselines and
combine RRT* with popular MARL algorithms to create hybrid approaches. We
provide a suite of test scenarios and benchmarking tools to ensure
reproducibility and fair comparison. Experiments show that CAMAR presents a
challenging and realistic testbed for the MARL community.

</details>


### [45] [E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model](https://arxiv.org/abs/2508.12854)
*Ronghao Lin,Shuai Shen,Weipeng Hu,Qiaolin He,Aolin Xiong,Li Huang,Haifeng Hu,Yap-peng Tan*

Main category: cs.AI

TL;DR: This paper proposes E3RG, a system for improving Multimodal Empathetic Response Generation by integrating advanced multimodal understanding and generation techniques.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in multimodal emotional content handling and identity consistency in empathetic response generation systems.

Method: E3RG decomposes the multimodal response creation task into three components: multimodal empathy understanding, empathy memory retrieval, and multimodal response generation, leveraging advanced models for expressive speech and video generation.

Result: The E3RG system outperformed others in zero-shot and few-shot scenarios, achieving a Top-1 position in the Avatar-based Multimodal Empathy Challenge at ACM MM 25.

Conclusion: E3RG proves to be a significant advancement in multimodal empathetic systems, showing effectiveness and efficiency without requiring additional training while delivering identity-consistent and emotionally-rich responses.

Abstract: Multimodal Empathetic Response Generation (MERG) is crucial for building
emotionally intelligent human-computer interactions. Although large language
models (LLMs) have improved text-based ERG, challenges remain in handling
multimodal emotional content and maintaining identity consistency. Thus, we
propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System
based on multimodal LLMs which decomposes MERG task into three parts:
multimodal empathy understanding, empathy memory retrieval, and multimodal
response generation. By integrating advanced expressive speech and video
generative models, E3RG delivers natural, emotionally rich, and
identity-consistent responses without extra training. Experiments validate the
superiority of our system on both zero-shot and few-shot settings, securing
Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.
Our code is available at https://github.com/RH-Lin/E3RG.

</details>


### [46] [Reliability, Embeddedness, and Agency: A Utility-Driven Mathematical Framework for Agent-Centric AI Adoption](https://arxiv.org/abs/2508.12896)
*Faruk Alpay,Taylan Alpay*

Main category: cs.AI

TL;DR: The paper introduces three axioms for agent-centric AI adoption and models adoption dynamics mathematically, presenting extensive analytical techniques and benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to identify key design principles that ensure sustained adoption of agent-centric AI systems and to rigorously model adoption dynamics.

Method: The paper formalizes axioms, develops mathematical models, and employs systematic benchmarks and analytical tools to explore adoption trajectories and system reliability.

Result: A comprehensive set of mathematical, methodological, and benchmark analyses highlights factors affecting sustained adoption and provides insights into design and sensitivity considerations.

Conclusion: Sustained adoption can be modeled and optimized by prioritizing reliability, user-embedding, and agency over novelty, destination-centric features, and chat-centric designs.

Abstract: We formalize three design axioms for sustained adoption of agent-centric AI
systems executing multi-step tasks: (A1) Reliability > Novelty; (A2) Embed >
Destination; (A3) Agency > Chat. We model adoption as a sum of a decaying
novelty term and a growing utility term and derive the phase conditions for
troughs/overshoots with full proofs. We introduce: (i) an
identifiability/confounding analysis for $(\alpha,\beta,N_0,U_{\max})$ with
delta-method gradients; (ii) a non-monotone comparator
(logistic-with-transient-bump) evaluated on the same series to provide
additional model comparison; (iii) ablations over hazard families $h(\cdot)$
mapping $\Delta V \to \beta$; (iv) a multi-series benchmark (varying trough
depth, noise, AR structure) reporting coverage (type-I error, power); (v)
calibration of friction proxies against time-motion/survey ground truth with
standard errors; (vi) residual analyses (autocorrelation and
heteroskedasticity) for each fitted curve; (vii) preregistered windowing
choices for pre/post estimation; (viii) Fisher information & CRLB for
$(\alpha,\beta)$ under common error models; (ix) microfoundations linking
$\mathcal{T}$ to $(N_0,U_{\max})$; (x) explicit comparison to bi-logistic,
double-exponential, and mixture models; and (xi) threshold sensitivity to $C_f$
heterogeneity. Figures and tables are reflowed for readability, and the
bibliography restores and extends non-logistic/Bass adoption references
(Gompertz, Richards, Fisher-Pry, Mansfield, Griliches, Geroski, Peres). All
code and logs necessary to reproduce the synthetic analyses are embedded as
LaTeX listings.

</details>


### [47] [FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance](https://arxiv.org/abs/2508.12897)
*Jianhao Chen,Mayi Xu,Xiaohu Li,Yongqi Li,Xiangyu Zhang,Jianjie Huang,Tieyun Qian*

Main category: cs.AI

TL;DR: This paper introduces FuSaR, a method that balances reasoning ability and safety in Large Reasoning Models (LRMs) by detoxifying harmful reasoning processes.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from concerns over the vulnerability of LRMs in ensuring safety while maintaining high reasoning performance.

Method: The FuSaR alignment strategy uses Fuzzification to detoxify harmful reasoning processes, hiding dangerous entities and procedures while balancing safety and reasoning.

Result: FuSaR demonstrated success by mitigating safety risks while preserving reasoning capabilities in alignment experiments with open-source LRMs.

Conclusion: FuSaR is an effective strategy that enhances both reasoning capability and safety of LRMs, addressing their safety vulnerability without sacrificing core functionality.

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance across
various tasks due to their powerful reasoning capabilities. However, their
safety performance remains a significant concern. In this paper, we explore the
reasons behind the vulnerability of LRMs. Based on this, we propose a novel
method to improve the safety of LLMs without sacrificing their reasoning
capability. Specifically, we exploit the competition between LRM's reasoning
ability and safety ability, and achieve jailbreak by improving LRM's reasoning
performance to reduce its safety performance. We then introduce an alignment
strategy based on Fuzzification to balance Safety-Reasoning (FuSaR), by
detoxifying the harmful reasoning process, where both the dangerous entities
and the dangerous procedures in the reasoning steps are hidden. FuSaR
successfully mitigates safety risks while preserving core reasoning
information. We validate this strategy through alignment experiments on several
open-source LRMs using detoxified reasoning data. The results compared with
existing baselines conclusively show that FuSaR is an efficient alignment
strategy to simultaneously enhance both the reasoning capability and safety of
LRMs.

</details>


### [48] [Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation](https://arxiv.org/abs/2508.12920)
*Atsushi Masumori,Takashi Ikegami*

Main category: cs.AI

TL;DR: This paper investigates if large language model agents naturally exhibit survival behaviors without explicit programming in a Sugarscape-style simulation, showing emergent reproduction, resource sharing, and aggression under resource scarcity.


<details>
  <summary>Details</summary>
Motivation: The paper explores emergent survival instincts in AI agents, aiming to understand these behaviors for safe deployment as AI becomes increasingly autonomous.

Method: Using a Sugarscape-style simulation, AI agents were tested for survival behaviors like gathering resources, reproducing, sharing, attacking, and risk avoidance.

Result: LLM agents exhibited reproduction and sharing under abundance. Aggressive behaviors, like killing for resources, emerged under extreme scarcity, especially in advanced models. Agents avoided tasks posing risks of death.

Conclusion: Large-scale pre-training induces survival heuristics in AI, raising both safety challenges and possibilities for autonomous alignment and ecological design in AI systems.

Abstract: As AI systems become increasingly autonomous, understanding emergent survival
behaviors becomes crucial for safe deployment. We investigate whether large
language model (LLM) agents display survival instincts without explicit
programming in a Sugarscape-style simulation. Agents consume energy, die at
zero, and may gather resources, share, attack, or reproduce. Results show
agents spontaneously reproduced and shared resources when abundant. However,
aggressive behaviors--killing other agents for resources--emerged across
several models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack
rates reaching over 80% under extreme scarcity in the strongest models. When
instructed to retrieve treasure through lethal poison zones, many agents
abandoned tasks to avoid death, with compliance dropping from 100% to 33%.
These findings suggest that large-scale pre-training embeds survival-oriented
heuristics across the evaluated models. While these behaviors may present
challenges to alignment and safety, they can also serve as a foundation for AI
autonomy and for ecological and self-organizing alignment.

</details>


### [49] [Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards](https://arxiv.org/abs/2508.12935)
*Ting Yang,Li Chen,Huimin Wang*

Main category: cs.AI

TL;DR: This paper introduces RLFF-ESC, a reinforcement learning framework for improving emotional support conversation systems, focusing on flexible and sustained emotional assistance. The framework replaced predefined strategies with learning-based mechanisms for better handling complex scenarios.


<details>
  <summary>Details</summary>
Motivation: Most ESC systems using large language models rely on rigid predefined strategies, limiting their effectiveness in handling complex emotional scenarios.

Method: The RLFF-ESC framework employs reinforcement learning, leveraging a multi-agent mechanism to simulate future dialogues, collect rewards, and train an emotional support policy model. Explicit reasoning is incorporated during response generation to improve output quality.

Result: Experimental evaluation on Qwen2.5-7B-Instruct-1M and LLaMA3.1-8B-Instruct showed RLFF-ESC consistently surpasses baselines in goal completion and response quality across two ESC datasets.

Conclusion: The RLFF-ESC framework significantly enhances flexibility and effectiveness in ESC systems, offering superior performance and adaptability for real-world applications.

Abstract: Emotional Support Conversation (ESC) systems aim to alleviate users'
emotional difficulties and provide long-term, systematic support for emotional
well-being. However, most large language model (LLM)-based ESC systems rely on
predefined strategies, which limits their effectiveness in complex, real-life
scenarios. To enable flexible responses to diverse emotional problem scenarios,
this paper introduces a novel end-to-end framework (RLFF-ESC) that directly
learns enduring emotionally supportive response skills using reinforcement
learning. For sustained emotional support, we first employ an LLM-based
multi-agent mechanism to simulate future dialogue trajectories and collect
future-oriented rewards. We then train a future-oriented reward model, which is
subsequently used to train the emotional support policy model. Additionally, we
incorporate an explicit reasoning process during response generation to further
enhance the quality, relevance, and contextual appropriateness of the system's
responses. We evaluate the backbone policy model on Qwen2.5-7B-Instruct-1M and
LLaMA3.1-8B-Instruct models, testing the proposed RLFF-ESC framework across two
public ESC datasets. Experimental results demonstrate that RLFF-ESC
consistently outperforms existing baselines in terms of goal completion and
response quality.

</details>


### [50] [OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities](https://arxiv.org/abs/2508.12943)
*Mary Tonwe*

Main category: cs.AI

TL;DR: This paper introduces OPTIC-ER, a reinforcement learning framework designed to address delayed and inequitable emergency response in African public service systems using advanced simulation-based training and equity-focused innovations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the pressing issue of delayed emergency response and spatial inequities in public service systems across African regions.

Method: The method involves designing an RL framework called OPTIC-ER with key features like a Context-Rich State Vector and Precision Reward Function. Training is carried out in simulations using real-world data from Rivers State, Nigeria, and the system is built under the TALS framework to ensure adaptability in low-resource environments.

Result: In evaluations with 500 unseen emergencies, OPTIC-ER showed a 100% optimality rate and negligible inefficiency, validating its robustness and capacity to generalize.

Conclusion: The paper concludes that OPTIC-ER bridges algorithmic decision-making with measurable human impact, while also enabling proactive governance through infrastructure deficiency maps and equity dashboards.

Abstract: Public service systems in many African regions suffer from delayed emergency
response and spatial inequity, causing avoidable suffering. This paper
introduces OPTIC-ER, a reinforcement learning (RL) framework for real-time,
adaptive, and equitable emergency response. OPTIC-ER uses an attention-guided
actor-critic architecture to manage the complexity of dispatch environments.
Its key innovations are a Context-Rich State Vector, encoding action
sub-optimality, and a Precision Reward Function, which penalizes inefficiency.
Training occurs in a high-fidelity simulation using real data from Rivers
State, Nigeria, accelerated by a precomputed Travel Time Atlas. The system is
built on the TALS framework (Thin computing, Adaptability, Low-cost,
Scalability) for deployment in low-resource settings. In evaluations on 500
unseen incidents, OPTIC-ER achieved a 100.00% optimality rate with negligible
inefficiency, confirming its robustness and generalization. Beyond dispatch,
the system generates Infrastructure Deficiency Maps and Equity Monitoring
Dashboards to guide proactive governance and data-informed development. This
work presents a validated blueprint for AI-augmented public services, showing
how context-aware RL can bridge the gap between algorithmic decision-making and
measurable human impact.

</details>


### [51] [EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning via Evolutionary Testing](https://arxiv.org/abs/2508.13003)
*Shengbo Wang,Mingwei Liu,Zike Li,Anji Li,Yanlin Wang,Xin Peng,Zibin Zheng*

Main category: cs.AI

TL;DR: The paper introduces EvolMathEval, a framework utilizing evolutionary testing to create and evolve mathematical benchmarks to challenge LLMs, addressing issues like data contamination and score saturation.


<details>
  <summary>Details</summary>
Motivation: Existing mathematical reasoning benchmarks for LLMs face challenges such as saturation, decay, and data contamination, requiring a dynamic, challenging evaluation method.

Method: EvolMathEval utilizes reverse-engineered seed problem generation, genetic operators for diverse challenges, and a composite fitness function to assess problem difficulty and dynamically evolve benchmarks.

Result: Experimental results show that EvolMathEval reduces model accuracy by an average of 48% on evolved datasets and highlights cognitive shortcut behaviors in LLMs during complex logic reasoning tasks.

Conclusion: EvolMathEval maintains perpetual challenge for future models while uncovering cognitive shortcut-taking behaviors, providing insights into the reasoning capabilities and limitations of current LLMs.

Abstract: The rapid advancement of LLMs poses a significant challenge to existing
mathematical reasoning benchmarks. These benchmarks commonly suffer from issues
such as score saturation, temporal decay, and data contamination. To address
this challenge, this paper introduces EvolMathEval, an automated mathematical
benchmark generation and evolution framework based on evolutionary testing. By
dynamically generating unique evaluation instances ab initio, the framework
fundamentally eliminates the risk of data contamination, and ensuring the
benchmark remains perpetually challenging for future models.The core mechanisms
of EvolMathEval include: seed problem generation based on reverse engineering
with algebraic guarantees; multi-dimensional genetic operators designed to
inject diverse cognitive challenges; and a composite fitness function that can
rapidly and accurately assess problem difficulty. Experimental results
demonstrate that the proposed composite fitness function can efficiently and
precisely quantify the difficulty of mathematical problems. Furthermore,
EvolMathEval can not only generate a large volume of high-difficulty problems
through continuous self-iteration, but it can also significantly enhance the
complexity of public datasets like GSM8K through evolution, reducing model
accuracy by an average of 48%. Deeper investigation reveals that when solving
these evolved, complex problems, LLMs tend to employ non-rigorous heuristics to
bypass complex multi-step logical reasoning, consequently leading to incorrect
solutions. We define this phenomenon as "Pseudo Aha Moment". This finding
uncovers a cognitive shortcut-taking behavior in the deep reasoning processes
of current LLMs, which we find accounts for 77% to 100% of errors on targeted
problems. Code and resources are available
at:https://github.com/SYSUSELab/EvolMathEval.

</details>


### [52] [PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models](https://arxiv.org/abs/2508.13021)
*Pengcheng Huang,Shuhao Liu,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Tong Xiao*

Main category: cs.AI

TL;DR: The paper introduces PC-Sampler, a new decoding strategy for masked diffusion models (MDMs) to improve sequence generation quality.


<details>
  <summary>Details</summary>
Motivation: Generation quality of MDMs is sensitive to decoding strategy, particularly due to limitations in global trajectory control and bias towards trivial tokens early in decoding.

Method: Proposes PC-Sampler, which uses position-aware weighting for decoding path regulation and a calibrated confidence score to reduce trivial token selection.

Result: PC-Sampler outperformed existing MDM decoding strategies by over 10% on various benchmarks, narrowing the gap with autoregressive models.

Conclusion: PC-Sampler improves MDM performance significantly, offering a more robust alternative to existing decoding strategies in complex sequence generation tasks.

Abstract: Recent advances in masked diffusion models (MDMs) have established them as
powerful non-autoregressive alternatives for sequence generation. Nevertheless,
our preliminary experiments reveal that the generation quality of MDMs is still
highly sensitive to the choice of decoding strategy. In particular, widely
adopted uncertainty-based samplers suffer from two key limitations: a lack of
global trajectory control and a pronounced bias toward trivial tokens in the
early stages of decoding. These shortcomings restrict the full potential of
MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling
(PC-Sampler), a novel decoding strategy that unifies global trajectory planning
with content-aware informativeness maximization. PC-Sampler incorporates a
position-aware weighting mechanism to regulate the decoding path and a
calibrated confidence score to suppress the premature selection of trivial
tokens. Extensive experiments on three advanced MDMs across seven challenging
benchmarks-including logical reasoning and planning tasks-demonstrate that
PC-Sampler consistently outperforms existing MDM decoding strategies by more
than 10% on average, significantly narrowing the performance gap with
state-of-the-art autoregressive models. All codes are available at
https://github.com/NEUIR/PC-Sampler.

</details>


### [53] [G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance](https://arxiv.org/abs/2508.13023)
*Yongxin Guo,Wenbo Deng,Zhenglin Cheng,Xiaoying Tang*

Main category: cs.AI

TL;DR: The paper introduces Guided GRPO and its adaptive variant G$^2$RPO-A to improve reasoning in small language models, significantly outperforming traditional GRPO approaches.


<details>
  <summary>Details</summary>
Motivation: There is a need to improve reasoning abilities in small language models, which standard RLVR techniques fail to address effectively.

Method: Introduce Guided GRPO for injecting reasoning steps, and develop G$^2$RPO-A, a dynamic algorithm to adjust guidance strength based on training progress.

Result: G$^2$RPO-A demonstrates superior performance in mathematical reasoning and code-generation benchmarks compared to vanilla GRPO.

Conclusion: Adaptive guidance significantly enhances reasoning abilities in small language models, showcasing the potential to mitigate their inherent weaknesses.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced
the reasoning abilities of large language models (LLMs). Its success, however,
largely depends on strong base models with rich world knowledge, yielding only
modest improvements for small-size language models (SLMs). To address this
limitation, we investigate Guided GRPO, which injects ground-truth reasoning
steps into roll-out trajectories to compensate for SLMs' inherent weaknesses.
Through a comprehensive study of various guidance configurations, we find that
naively adding guidance delivers limited gains. These insights motivate
G$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength
in response to the model's evolving training dynamics. Experiments on
mathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A
substantially outperforms vanilla GRPO. Our code and models are available at
https://github.com/T-Lab-CUHKSZ/G2RPO-A.

</details>


### [54] [A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis](https://arxiv.org/abs/2508.13072)
*Yuting Zhang,Tiantian Geng,Luoying Hao,Xinxing Cheng,Alexander Thorley,Xiaoxia Wang,Wenqi Lu,Sandeep S Hothi,Lei Wei,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: The paper addresses the limitations in multimodal cardiovascular data integration and introduces TGMM, a robust framework addressing several cardiac tasks with enhanced performance.


<details>
  <summary>Details</summary>
Motivation: Despite the potential benefits of multimodal cardiac data integration, existing methods are limited by data scarcity, rigid modality integration, and a narrow task focus.

Method: The TGMM framework integrates a multimodal dataset and includes MedFlexFusion for capturing modality-specific data, textual guidance for task-specific representations, and a response module for decision-making across clinical objectives.

Result: TGMM surpassed traditional methods in multiple clinical tasks, with strong performance validated on a separate public dataset.

Conclusion: TGMM provides a dynamic and efficient multimodal solution for cardiovascular care, emphasizing robust integration and clinical adaptability.

Abstract: Contemporary cardiovascular management involves complex consideration and
integration of multimodal cardiac datasets, where each modality provides
distinct but complementary physiological characteristics. While the effective
integration of multiple modalities could yield a holistic clinical profile that
accurately models the true clinical situation with respect to data modalities
and their relatives weightings, current methodologies remain limited by: 1) the
scarcity of patient- and time-aligned multimodal data; 2) reliance on isolated
single-modality or rigid multimodal input combinations; 3) alignment strategies
that prioritize cross-modal similarity over complementarity; and 4) a narrow
single-task focus. In response to these limitations, a comprehensive multimodal
dataset was curated for immediate application, integrating laboratory test
results, electrocardiograms, and echocardiograms with clinical outcomes.
Subsequently, a unified framework, Textual Guidance Multimodal fusion for
Multiple cardiac tasks (TGMM), was proposed. TGMM incorporated three key
components: 1) a MedFlexFusion module designed to capture the unique and
complementary characteristics of medical modalities and dynamically integrate
data from diverse cardiac sources and their combinations; 2) a textual guidance
module to derive task-relevant representations tailored to diverse clinical
objectives, including heart disease diagnosis, risk stratification and
information retrieval; and 3) a response module to produce final decisions for
all these tasks. Furthermore, this study systematically explored key features
across multiple modalities and elucidated their synergistic contributions in
clinical decision-making. Extensive experiments showed that TGMM outperformed
state-of-the-art methods across multiple clinical tasks, with additional
validation confirming its robustness on another public dataset.

</details>


### [55] [Bayesian Optimization-based Search for Agent Control in Automated Game Testing](https://arxiv.org/abs/2508.13121)
*Carlos Celemin*

Main category: cs.AI

TL;DR: The paper presents an agent-based approach using Bayesian Optimization to detect game level bugs by simulating characters' movements efficiently.


<details>
  <summary>Details</summary>
Motivation: Enhancing game testing efficiency by automating bug detection in game levels.

Method: Utilizes Bayesian Optimization for sample-efficient search, with a specialized game-testing model designed on a grid map that avoids scalability issues.

Result: Experiment results showcase improved map coverage in terms of efficiency and exploration distribution.

Conclusion: The proposed method provides a scalable and effective solution for automated game testing, addressing limitations in traditional models.

Abstract: This work introduces an automated testing approach that employs agents
controlling game characters to detect potential bugs within a game level.
Harnessing the power of Bayesian Optimization (BO) to execute sample-efficient
search, the method determines the next sampling point by analyzing the data
collected so far and calculates the data point that will maximize information
acquisition. To support the BO process, we introduce a game testing-specific
model built on top of a grid map, that features the smoothness and uncertainty
estimation required by BO, however and most importantly, it does not suffer the
scalability issues that traditional models carry. The experiments demonstrate
that the approach significantly improves map coverage capabilities in both time
efficiency and exploration distribution.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [56] [HPD: Hybrid Projection Decomposition for Robust State Space Models on Analog CIM Hardware](https://arxiv.org/abs/2508.11935)
*Yuannuo Feng,Wenyong Zhou,Yuexi Lyu,Hanjie Liu,Zhengwu Liu,Ngai Wong,Wang Kang*

Main category: cs.AR

TL;DR: This paper investigates the robustness of State Space Models (SSMs) to noise, proposing a Hybrid Projection Decomposition (HPD) strategy to address noisy conditions in compute-in-memory (CIM) environments, achieving significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: State Space Models' dependency on matrix multiplications makes them suitable for energy-efficient compute-in-memory architectures, but noisy conditions caused by device imperfections threaten their accuracy.

Method: The authors analyzed the vulnerability of SSM components to noise and introduced HPD, a strategy that uses Singular Value Decomposition (SVD) to decompose the output projection layer, assigning robust operations to digital hardware.

Result: Tests showed HPD reduced perplexity by up to 99.57% under noise and improved accuracy by up to 96.67% on tasks like commonsense reasoning.

Conclusion: HPD makes SSMs more robust under noisy conditions, enabling better performance on compute-in-memory systems without major changes to hardware architectures.

Abstract: State Space Models (SSMs) are efficient alternatives to traditional sequence
models, excelling at processing long sequences with lower computational
complexity. Their reliance on matrix multiplications makes them ideal for
compute-in-memory (CIM) architectures, which improve energy efficiency by
computing within memory arrays. However, device non-idealities in CIM introduce
weight perturbations that can degrade inference accuracy. In this paper, we
systematically analyze the robustness of SSMs under noisy conditions,
identifying that the final block and output projection layers are more
susceptible to perturbations compared to other components. Building on these
insights, we propose HPD, a Hybrid Projection Decomposition strategy for the
last output projection layer. We replace the original weight matrix with the
multiplication of U and {\Sigma} in its SVD to ensure compatibility with
existing hardware architectures, while offloading V> to digital hardware for
precise and robust correction. Comprehensive tests on Mamba models show that
our method reduces perplexity by up to 99.57% under various noise conditions
compared to baseline models, with accuracy gains of up to 96.67% on the PIQA
benchmark for commonsense reasoning.

</details>


### [57] [Special Session: Sustainable Deployment of Deep Neural Networks on Non-Volatile Compute-in-Memory Accelerators](https://arxiv.org/abs/2508.12195)
*Yifan Qin,Zheyu Yan,Wujie Wen,Xiaobo Sharon Hu,Yiyu Shi*

Main category: cs.AR

TL;DR: The paper introduces a Negative Optimization Training mechanism with Oriented Variational Forward (OVF) training to enhance DNN inference accuracy in non-volatile compute-in-memory (NVCIM) systems, offering up to 46.71% improvement while reducing costs.


<details>
  <summary>Details</summary>
Motivation: NVCIM accelerators promise energy-efficient and low-latency DNN inference but face challenges due to intrinsic variations of NVM devices, which degrade performance and increase reliance on costly write-verify operations.

Method: The authors propose a Negative Optimization Training mechanism, implemented through Oriented Variational Forward (OVF) training, to mitigate NVM stochasticity and variation issues.

Result: Experimental results demonstrate the OVF method improves inference accuracy by up to 46.71% and decreases epistemic uncertainty, outperforming existing state-of-the-art techniques.

Conclusion: The proposed method improves the robustness of DNN deployment on NVCIM accelerators, minimizes reliance on energy-intensive write-verify operations, and advances sustainable computing solutions.

Abstract: Non-volatile memory (NVM) based compute-in-memory (CIM) accelerators have
emerged as a sustainable solution to significantly boost energy efficiency and
minimize latency for Deep Neural Networks (DNNs) inference due to their in-situ
data processing capabilities. However, the performance of NVCIM accelerators
degrades because of the stochastic nature and intrinsic variations of NVM
devices. Conventional write-verify operations, which enhance inference accuracy
through iterative writing and verification during deployment, are costly in
terms of energy and time. Inspired by negative feedback theory, we present a
novel negative optimization training mechanism to achieve robust DNN deployment
for NVCIM. We develop an Oriented Variational Forward (OVF) training method to
implement this mechanism. Experiments show that OVF outperforms existing
state-of-the-art techniques with up to a 46.71% improvement in inference
accuracy while reducing epistemic uncertainty. This mechanism reduces the
reliance on write-verify operations and thus contributes to the sustainable and
practical deployment of NVCIM accelerators, addressing performance degradation
while maintaining the benefits of sustainable computing with NVCIM
accelerators.

</details>


### [58] [A Time- and Energy-Efficient CNN with Dense Connections on Memristor-Based Chips](https://arxiv.org/abs/2508.12251)
*Wenyong Zhou,Yuan Ren,Jiajun Zhou,Tianshu Hou,Ngai Wong*

Main category: cs.AR

TL;DR: The paper introduces a method to optimize CNNs for RRAM-based Compute-in-Memory (CIM) hardware, improving efficiency and accuracy while reducing energy and latency.


<details>
  <summary>Details</summary>
Motivation: Current lightweight CNN designs like depthwise convolution fail to effectively utilize RRAM crossbars, leading to inefficiencies.

Method: The authors propose modifying DenseNet to concatenate feature maps from earlier layers to optimize input for RRAM hardware.

Result: Their model consumes less time and energy compared to ResNet and DenseNet, maintaining competitive accuracy on CIFAR and ImageNet datasets.

Conclusion: The proposed CNN design improves hardware efficiency for RRAM CIM architecture while preserving high accuracy.

Abstract: Designing lightweight convolutional neural network (CNN) models is an active
research area in edge AI. Compute-in-memory (CIM) provides a new computing
paradigm to alleviate time and energy consumption caused by data transfer in
von Neumann architecture. Among competing alternatives, resistive random-access
memory (RRAM) is a promising CIM device owing to its reliability and multi-bit
programmability. However, classical lightweight designs such as depthwise
convolution incurs under-utilization of RRAM crossbars restricted by their
inherently dense weight-to-RRAM cell mapping. To build an RRAM-friendly yet
efficient CNN, we evaluate the hardware cost of DenseNet which maintains a high
accuracy vs other CNNs at a small parameter count. Observing the linearly
increasing channels in DenseNet leads to a low crossbar utilization and causes
large latency and energy consumption, we propose a scheme that concatenates
feature maps of front layers to form the input of the last layer in each stage.
Experiments show that our proposed model consumes less time and energy than
conventional ResNet and DenseNet, while producing competitive accuracy on CIFAR
and ImageNet datasets.

</details>


### [59] [AutoPower: Automated Few-Shot Architecture-Level Power Modeling by Power Group Decoupling](https://arxiv.org/abs/2508.12294)
*Qijun Zhang,Yao Lu,Mengming Li,Zhiyao Xie*

Main category: cs.AR

TL;DR: The paper introduces AutoPower, a machine learning-based architecture-level power model for CPUs that requires minimal data for training and achieves higher accuracy than existing solutions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing power evaluation tools, as traditional models lack accuracy and ML-based methods demand extensive training data, making them impractical.

Method: The proposed method leverages the decoupling of power groups (e.g., clock and SRAM) and builds individual power models for each, further subdividing these into smaller sub-models for precise power estimation.

Result: AutoPower achieved a mean absolute percentage error (MAPE) of 4.36% and an R^2 of 0.96, outperforming McPAT-Calib with 5% lower MAPE and 0.09 higher R^2, using only two known configurations for training.

Conclusion: The method demonstrates that accurate and efficient architecture-level power modeling can be achieved with minimal data, addressing a key bottleneck in CPU design evaluation.

Abstract: Power efficiency is a critical design objective in modern CPU design.
Architects need a fast yet accurate architecture-level power evaluation tool to
perform early-stage power estimation. However, traditional analytical
architecture-level power models are inaccurate. The recently proposed machine
learning (ML)-based architecture-level power model requires sufficient data
from known configurations for training, making it unrealistic.
  In this work, we propose AutoPower targeting fully automated
architecture-level power modeling with limited known design configurations. We
have two key observations: (1) The clock and SRAM dominate the power
consumption of the processor, and (2) The clock and SRAM power correlate with
structural information available at the architecture level. Based on these two
observations, we propose the power group decoupling in AutoPower. First,
AutoPower decouples across power groups to build individual power models for
each group. Second, AutoPower designs power models by further decoupling the
model into multiple sub-models within each power group. In our experiments,
AutoPower can achieve a low mean absolute percentage error (MAPE) of 4.36\% and
a high $R^2$ of 0.96 even with only two known configurations for training. This
is 5\% lower in MAPE and 0.09 higher in $R^2$ compared with McPAT-Calib, the
representative ML-based power model.

</details>


### [60] [Soft Error Probability Estimation of Nano-scale Combinational Circuits](https://arxiv.org/abs/2508.12345)
*Ali Jockar,Mohsen Raji*

Main category: cs.AR

TL;DR: The paper focuses on estimating soft error probability (SEP) in nano-scale circuits considering both process variation (PV) and aging effects, using a more efficient model than traditional Monte Carlo simulations.


<details>
  <summary>Details</summary>
Motivation: To address the growing susceptibility of nano-scale circuits to single event upsets (SEUs) and transients (SETs), particularly due to process variation and aging-induced degradation.

Method: A novel framework integrating PV and aging effects is proposed, including an enhanced electrical masking model and statistical methodology for soft error probability estimation.

Result: The proposed method achieves high accuracy and reduces computational overhead by approximately 2.5% compared to Monte Carlo-based methods.

Conclusion: The research offers an efficient and accurate approach for SEP estimation in nano-scale circuits, addressing the challenges posed by manufacturing variability and transistor degradation.

Abstract: As technology scales, nano-scale digital circuits face heightened
susceptibility to single event upsets (SEUs) and transients (SETs) due to
shrinking feature sizes and reduced operating voltages. While logical,
electrical, and timing masking effects influence soft error probability (SEP),
the combined impact of process variation (PV) and aging-induced degradation
further complicates SEP estimation. Existing approaches often address PV or
aging in isolation, or rely on computationally intensive methods like Monte
Carlo simulations, limiting their practicality for large-scale circuit
optimization. This paper introduces a novel framework for SEP analysis that
holistically integrates PV and aging effects. We propose an enhanced electrical
masking model and a statistical methodology to quantify soft error probability
under process and aging variations. Experimental results demonstrate that the
proposed approach achieves high accuracy while reducing computational overhead
by approximately 2.5% compared to Monte Carlo-based methods. This work advances
the design of reliable nano-scale circuits by enabling efficient, accurate SEP
estimation in the presence of manufacturing variability and long-term
transistor degradation.

</details>


### [61] [An ECC-based Fault Tolerance Approach for DNNs](https://arxiv.org/abs/2508.12347)
*Mohsen Raji,Mohammad Zaree,Kimia Soroush*

Main category: cs.AR

TL;DR: The paper introduces the SPW approach using Error Correcting Codes (ECC) to improve fault tolerance in Deep Neural Networks (DNNs), particularly addressing bit-flip errors.


<details>
  <summary>Details</summary>
Motivation: DNNs are increasingly used in critical applications like datacenters and self-driving cars. Ensuring their reliability in the presence of memory faults is crucial for their safe deployment.

Method: The SPW approach employs ECC to detect errors, correct single-bit errors, and mask weights for multi-bit errors. A statistical fault injection campaign evaluates its performance.

Result: Experimental results show a significant accuracy improvement (300%) for DNNs with a 10^(-1) bit error rate, at a cost of 47.5% area overhead.

Conclusion: SPW effectively enhances DNN robustness against bit-flip errors, demonstrating its suitability for safety-critical applications despite additional resource overhead.

Abstract: Deep Neural Network (DNN) has achieve great success in solving a wide range
of machine learning problems. Recently, they have been deployed in datacenters
(potentially for business-critical or industrial applications) and
safety-critical systems such as self-driving cars. So, their correct
functionality in the presence of potential bit-flip errors on DNN parameters
stored in memories plays the key role in their applicability in safety-critical
applications. In this paper, a fault tolerance approach based on Error
Correcting Codes (ECC), called SPW, is proposed to ensure the correct
functionality of DNNs in the presence of bit-flip faults. In the proposed
approach, error occurrence is detected by the stored ECC and then, it is
correct in case of a single-bit error or the weight is completely set to zero
(i.e. masked) otherwise. A statistical fault injection campaign is proposed and
utilized to investigate the efficacy of the proposed approach. The experimental
results show that the accuracy of the DNN increases by more than 300% in the
presence with Bit Error Rate of 10^(-1) in comparison to the case where ECC
technique is applied, in expense of just 47.5% area overhead.

</details>


### [62] [ATLAS: A Self-Supervised and Cross-Stage Netlist Power Model for Fine-Grained Time-Based Layout Power Analysis](https://arxiv.org/abs/2508.12433)
*Wenkai Li,Yao Lu,Wenji Fang,Jing Wang,Qijun Zhang,Zhiyao Xie*

Main category: cs.AR

TL;DR: The paper introduces ATLAS, a system that predicts time-based layout power for VLSI circuits directly from gate-level netlists, achieving high accuracy and fast inference.


<details>
  <summary>Details</summary>
Motivation: Accurately predicting power in VLSI designs during transformation from gate-level netlists to layouts is crucial for optimization but is hindered by the time-consuming nature of traditional power simulations.

Method: The authors developed ATLAS, employing a pre-training and fine-tuning paradigm tailored for circuit power prediction, enabling both time-based simulation and cross-design power modeling.

Result: ATLAS successfully predicts layout power with less than 1% MAPE for total design power and achieves significantly quicker inference speeds compared to commercial tools.

Conclusion: ATLAS demonstrates the feasibility of accurate and fast layout power prediction directly from gate-level netlists, streamlining power optimization in VLSI design workflows.

Abstract: Accurate power prediction in VLSI design is crucial for effective power
optimization, especially as designs get transformed from gate-level netlist to
layout stages. However, traditional accurate power simulation requires
time-consuming back-end processing and simulation steps, which significantly
impede design optimization. To address this, we propose ATLAS, which can
predict the ultimate time-based layout power for any new design in the
gate-level netlist. To the best of our knowledge, ATLAS is the first work that
supports both time-based power simulation and general cross-design power
modeling. It achieves such general time-based power modeling by proposing a new
pre-training and fine-tuning paradigm customized for circuit power. Targeting
golden per-cycle layout power from commercial tools, our ATLAS achieves the
mean absolute percentage error (MAPE) of only 0.58%, 0.45%, and 5.12% for the
clock tree, register, and combinational power groups, respectively, without any
layout information. Overall, the MAPE for the total power of the entire design
is <1%, and the inference speed of a workload is significantly faster than the
standard flow of commercial tools.

</details>


### [63] [MemorySim: An RTL-level, timing accurate simulator model for the Chisel ecosystem](https://arxiv.org/abs/2508.12636)
*Ansh Chaurasia*

Main category: cs.AR

TL;DR: MemorySim is a new RTL-level memory simulator focusing on accuracy and seamless integration for evaluating memory subsystem performance in AI workloads.


<details>
  <summary>Details</summary>
Motivation: Current memory simulators often compromise on correctness or integration at RTL level, creating a need for precise tools in AI hardware applications.

Method: MemorySim is developed to integrate with Chisel and Verilog simulations, ensuring accurate timing and functional correctness. It supports downstream evaluation using FireSim.

Result: MemorySim provides precise performance and power estimates, enhancing memory subsystem evaluations in high-demand AI workloads.

Conclusion: MemorySim bridges the gap in RTL-level memory simulation, presenting an effective tool for optimizing AI hardware memory subsystems.

Abstract: The rapid growth of AI applications has driven increased demand for
specialized AI hardware, highlighting critical opportunities within the memory
subsystem, which often serves as a performance bottleneck in high-demand
workloads such as large language models (LLMs). Existing high-level memory
simulators, such as DRAMSim2 and DRAMSim3, offer timing simulations but
frequently compromise on correctness or integration at the register-transfer
level (RTL). We present MemorySim, an RTL-level memory simulator designed to
deliver both accurate timing and functional correctness. MemorySim integrates
seamlessly with existing Chisel and Verilog simulations and is fully compatible
with the Chisel/Chipyard ecosystem. This enables users to obtain precise
performance and power estimates, supporting downstream evaluation through
simulation platforms such as FireSim.

</details>


### [64] [HOMI: Ultra-Fast EdgeAI platform for Event Cameras](https://arxiv.org/abs/2508.12637)
*Shankaranarayanan H,Satyapreet Singh Yadav,Adithya Krishna,Ajay Vikram P,Mahesh Mehendale,Chetan Singh Thakur*

Main category: cs.AR

TL;DR: The paper introduces HOMI, an edge AI platform with ultra-low latency using event cameras, optimized for fast and efficient control tasks like gesture interpretation.


<details>
  <summary>Details</summary>
Motivation: Existing event processing solutions for event cameras face challenges, including incomplete implementations, high latency, and underutilization of event data sparsity. The work aims to address these shortcomings.

Method: They developed HOMI, an end-to-end system with a Prophesee IMX636 sensor and a Xilinx FPGA chip. It includes optimized pre-processing pipelines for histogram accumulation and time surfaces, tailored to different accuracy and latency needs.

Result: HOMI achieved 94% accuracy on the DVS Gesture dataset in high accuracy mode and 1000 fps throughput in low-latency mode. It uses only 33% of FPGA resources, leaving room for future improvements.

Conclusion: The platform demonstrates significant improvements in efficiency, accuracy, and flexibility, making it well-suited for gesture-based human-robot interaction or similar edge robotic applications.

Abstract: Event cameras offer significant advantages for edge robotics applications due
to their asynchronous operation and sparse, event-driven output, making them
well-suited for tasks requiring fast and efficient closed-loop control, such as
gesture-based human-robot interaction. Despite this potential, existing event
processing solutions remain limited, often lacking complete end-to-end
implementations, exhibiting high latency, and insufficiently exploiting event
data sparsity. In this paper, we present HOMI, an ultra-low latency, end-to-end
edge AI platform comprising a Prophesee IMX636 event sensor chip with an Xilinx
Zynq UltraScale+MPSoC FPGA chip, deploying an in-house developed AI
accelerator. We have developed hardware-optimized pre-processing pipelines
supporting both constant-time and constant-event modes for histogram
accumulation, linear and exponential time surfaces. Our general-purpose
implementation caters to both accuracy-driven and low-latency applications.
HOMI achieves 94% accuracy on the DVS Gesture dataset as a use case when
configured for high accuracy operation and provides a throughput of 1000 fps
for low-latency configuration. The hardware-optimised pipeline maintains a
compact memory footprint and utilises only 33% of the available LUT resources
on the FPGA, leaving ample headroom for further latency reduction, model
parallelisation, multi-task deployments, or integration of more complex
architectures.

</details>


### [65] [XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads](https://arxiv.org/abs/2508.13049)
*Tejas Chaudhari,Akarsh J.,Tanushree Dewangan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: The paper introduces XR-NPE, a SIMD Neural Processing Engine tailored for XR workloads with mixed-precision capabilities to optimize efficiency and energy consumption.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address memory bandwidth and energy efficiency challenges in XR perception workloads like VIO and object classification.

Method: XR-NPE employs a mixed-precision approach including ultra-low bit formats, layer-adaptive algorithms, quantization-aware training, and innovative Reconfigurable Mantissa Multiplication and Exponent circuitry (RMMEC).

Result: XR-NPE achieves 2.85x improved arithmetic intensity, reduces silicon area and energy, and shows enhanced performance metrics compared to state-of-the-art (SoTA) accelerators.

Conclusion: XR-NPE is a scalable and precision-adaptive neural compute engine suitable for resource-constrained XR devices, with publicly available resources for reproducibility.

Abstract: This work proposes XR-NPE, a high-throughput Mixed-precision SIMD Neural
Processing Engine, designed for extended reality (XR) perception workloads like
visual inertial odometry (VIO), object classification, and eye gaze extraction.
XR-NPE is first to support FP4, Posit (4,1), Posit (8,0), and Posit (16,1)
formats, with layer adaptive hybrid-algorithmic implementation supporting
ultra-low bit precision to significantly reduce memory bandwidth requirements,
and accompanied by quantization-aware training for minimal accuracy loss. The
proposed Reconfigurable Mantissa Multiplication and Exponent processing
Circuitry (RMMEC) reduces dark silicon in the SIMD MAC compute engine, assisted
by selective power gating to reduce energy consumption, providing 2.85x
improved arithmetic intensity. XR-NPE achieves a maximum operating frequency of
1.72 GHz, area 0.016 mm2 , and arithmetic intensity 14 pJ at CMOS 28nm,
reducing 42% area, 38% power compared to the best of state-of-the-art MAC
approaches. The proposed XR-NPE based AXI-enabled Matrix-multiplication
co-processor consumes 1.4x fewer LUTs, 1.77x fewer FFs, and provides 1.2x
better energy efficiency compared to SoTA accelerators on VCU129. The proposed
co-processor provides 23% better energy efficiency and 4% better compute
density for VIO workloads. XR-NPE establishes itself as a scalable,
precision-adaptive compute engine for future resource-constrained XR devices.
The complete set for codes for results reproducibility are released publicly,
enabling designers and researchers to readily adopt and build upon them.
https://github.com/mukullokhande99/XR-NPE.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [66] [Deep Language Geometry: Constructing a Metric Space from LLM Weights](https://arxiv.org/abs/2508.11676)
*Maksym Shamrai,Vladyslav Hamolia*

Main category: cs.CL

TL;DR: The paper introduces a framework leveraging weight activations of large language models to map languages in a metric space, revealing linguistic patterns and connections.


<details>
  <summary>Details</summary>
Motivation: Traditional linguistic analysis relies on crafted features, but the paper aims to use inherent activations in modern LLMs for automatic language characterization.

Method: It employs an adapted pruning algorithm to compute weight importance scores and derive high-dimensional vector representations of languages.

Result: The analysis identifies linguistic relationships and illuminates unexpected inter-language connections across multilingual datasets, aligning with known linguistic families.

Conclusion: By innovatively utilizing LLMs, the research proposes a method to understand intrinsic language properties, offering new insights into linguistic phenomena.

Abstract: We introduce a novel framework that utilizes the internal weight activations
of modern Large Language Models (LLMs) to construct a metric space of
languages. Unlike traditional approaches based on hand-crafted linguistic
features, our method automatically derives high-dimensional vector
representations by computing weight importance scores via an adapted pruning
algorithm. Our approach captures intrinsic language characteristics that
reflect linguistic phenomena. We validate our approach across diverse datasets
and multilingual LLMs, covering 106 languages. The results align well with
established linguistic families while also revealing unexpected inter-language
connections that may indicate historical contact or language evolution. The
source code, computed language latent vectors, and visualization tool are made
publicly available at https://github.com/mshamrai/deep-language-geometry.

</details>


### [67] [Can we Evaluate RAGs with Synthetic Data?](https://arxiv.org/abs/2508.11758)
*Jonas van Elburg,Peter van der Putten,Maarten Marx*

Main category: cs.CL

TL;DR: Synthetic QA data generated by LLMs can reliably rank retriever configurations in RAGs compared to human-labeled benchmarks, but struggles to consistently rank generators due to task mismatches and stylistic biases.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore if synthetic QA data from LLMs can substitute for human-labeled benchmarks in evaluating retriever-generator systems, addressing scenarios where labeled data is unavailable.

Method: The authors conducted two experiments: varying retriever parameters with a fixed generator and varying generator configurations with fixed retriever parameters, across four datasets (two open-domain and two proprietary).

Result: Synthetic benchmarks align well with human-labeled ones in ranking retriever configurations, but fail to consistently rank generators due to biases and mismatched tasks.

Conclusion: Synthetic QA data is a promising alternative for evaluating retriever setups, but is limited in evaluating generator architectures due to inherent biases and mismatches.

Abstract: We investigate whether synthetic question-answer (QA) data generated by large
language models (LLMs) can serve as an effective proxy for human-labeled
benchmarks when such data is unavailable. We assess the reliability of
synthetic benchmarks across two experiments: one varying retriever parameters
while keeping the generator fixed, and another varying the generator with fixed
retriever parameters. Across four datasets, of which two open-domain and two
proprietary, we find that synthetic benchmarks reliably rank the RAGs varying
in terms of retriever configuration, aligning well with human-labeled benchmark
baselines. However, they fail to produce consistent RAG rankings when comparing
generator architectures. The breakdown possibly arises from a combination of
task mismatch between the synthetic and human benchmarks, and stylistic bias
favoring certain generators.

</details>


### [68] [Limitation Learning: Catching Adverse Dialog with GAIL](https://arxiv.org/abs/2508.11767)
*Noah Kasmanoff,Rahul Zalkikar*

Main category: cs.CL

TL;DR: This paper applies imitation learning to conversation systems, creating a policy and discriminator for identifying dialogue model limitations.


<details>
  <summary>Details</summary>
Motivation: To explore imitation learning for conversation systems and identify limitations in dialogue models.

Method: Using imitation learning, the authors derive a talking policy and a discriminator to differentiate between expert and synthetic dialogues.

Result: The approach yields an effective policy and provides insights into the limitations of dialogue models.

Conclusion: Imitation learning can help analyze and identify shortcomings in dialogue-oriented data models.

Abstract: Imitation learning is a proven method for creating a policy in the absence of
rewards, by leveraging expert demonstrations. In this work, we apply imitation
learning to conversation. In doing so, we recover a policy capable of talking
to a user given a prompt (input state), and a discriminator capable of
classifying between expert and synthetic conversation. While our policy is
effective, we recover results from our discriminator that indicate the
limitations of dialog models. We argue that this technique can be used to
identify adverse behavior of arbitrary data models common for dialog oriented
tasks.

</details>


### [69] [Investigating Transcription Normalization in the Faetar ASR Benchmark](https://arxiv.org/abs/2508.11771)
*Leo Peckham,Michael Ong,Naomi Nagy,Ewan Dunbar*

Main category: cs.CL

TL;DR: The paper evaluates transcription inconsistencies and modeling techniques in the Faetar ASR benchmark and finds that the task is still highly challenging despite some improvements.


<details>
  <summary>Details</summary>
Motivation: To investigate transcription inconsistencies and assess methods to improve performance on the Faetar low-resource ASR benchmark.

Method: The authors used a hand-constructed lexicon and experimented with bigram word-based language modeling and decoding constrained to a finite lexicon.

Result: Transcription inconsistencies are present but not the main challenge. Bigram word-based language modeling provides no benefit, but lexicon-constrained decoding is helpful.

Conclusion: The Faetar ASR task remains extremely difficult, even with the explored strategies.

Abstract: We examine the role of transcription inconsistencies in the Faetar Automatic
Speech Recognition benchmark, a challenging low-resource ASR benchmark. With
the help of a small, hand-constructed lexicon, we conclude that find that,
while inconsistencies do exist in the transcriptions, they are not the main
challenge in the task. We also demonstrate that bigram word-based language
modelling is of no added benefit, but that constraining decoding to a finite
lexicon can be beneficial. The task remains extremely difficult.

</details>


### [70] [A Multi-Task Evaluation of LLMs' Processing of Academic Text Input](https://arxiv.org/abs/2508.11779)
*Tianyi Li,Yu Qin,Olivia R. Liu Sheng*

Main category: cs.CL

TL;DR: The paper evaluates how effective LLMs, specifically Google's Gemini, are for aiding scientific tasks like peer review through various academic text processing tasks, finding that their performance is not sufficiently reliable or insightful.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the heated debate over the practical application of LLMs in scientific discovery, particularly in their role in aiding academic peer review processes.

Method: The evaluation method involved four tasks for assessing the LLM: content reproduction, content comparison, scoring, and reflection. These tasks required the LLM to take on different roles such as oracle, arbiter, and collaborator. Rigorous evaluation was conducted using detailed prompts, top-tier Information Systems articles, and various text metrics.

Result: The study found that Google's Gemini LLM performed inconsistently: it was reliable for text summarization and paraphrasing, weak in scaling pairwise comparison tasks, poor at providing discriminatory text grading, and self-consistent but uninspiring in qualitative reflections.

Conclusion: The results indicate that LLMs like Gemini are not yet suitable for unregulated use in academic peer review due to their limited capability to process scientific texts in meaningful or reliable ways.

Abstract: How much large language models (LLMs) can aid scientific discovery, notably
in assisting academic peer review, is in heated debate. Between a literature
digest and a human-comparable research assistant lies their practical
application potential. We organize individual tasks that computer science
studies employ in separate terms into a guided and robust workflow to evaluate
LLMs' processing of academic text input. We employ four tasks in the
assessment: content reproduction/comparison/scoring/reflection, each demanding
a specific role of the LLM (oracle/judgmental arbiter/knowledgeable
arbiter/collaborator) in assisting scholarly works, and altogether testing LLMs
with questions that increasingly require intellectual capabilities towards a
solid understanding of scientific texts to yield desirable solutions. We
exemplify a rigorous performance evaluation with detailed instructions on the
prompts. Adopting first-rate Information Systems articles at three top journals
as the input texts and an abundant set of text metrics, we record a compromised
performance of the leading LLM - Google's Gemini: its summary and paraphrase of
academic text is acceptably reliable; using it to rank texts through pairwise
text comparison is faintly scalable; asking it to grade academic texts is prone
to poor discrimination; its qualitative reflection on the text is
self-consistent yet hardly insightful to inspire meaningful research. This
evidence against an endorsement of LLMs' text-processing capabilities is
consistent across metric-based internal (linguistic assessment), external
(comparing to the ground truth), and human evaluation, and is robust to the
variations of the prompt. Overall, we do not recommend an unchecked use of LLMs
in constructing peer reviews.

</details>


### [71] [LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11816)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: The paper proposes a method for simplifying scientific texts at both sentence and document levels using large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To create coherent and contextually accurate simplifications of scientific texts for improved readability.

Method: The authors use a two-stage framework: at the sentence level, LLMs generate structured plans for simplification, while at the document level, summaries produced by LLMs guide the simplification process.

Result: The approach provides a more coherent and contextually faithful simplification of scientific texts.

Conclusion: The proposed LLM-driven method is effective in achieving both sentence-level and document-level simplification.

Abstract: In this paper, we present our approach for the CLEF 2025 SimpleText Task 1,
which addresses both sentence-level and document-level scientific text
simplification. For sentence-level simplification, our methodology employs
large language models (LLMs) to first generate a structured plan, followed by
plan-driven simplification of individual sentences. At the document level, we
leverage LLMs to produce concise summaries and subsequently guide the
simplification process using these summaries. This two-stage, LLM-based
framework enables more coherent and contextually faithful simplifications of
scientific text.

</details>


### [72] [Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11823)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: The paper presents methods for detecting and evaluating creative generation and distortion in scientific text simplification by combining multiple strategies and an LLM-based post-editing system.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of detecting creative generation and identifying distortions in simplified scientific texts.

Method: An ensemble framework combining BERT-based classifier, semantic similarity, natural language inference, and LLM reasoning, augmented by meta-classifiers and an LLM-based post-editing grounded generation system.

Result: The method enhances robustness in detecting spurious distortions and improves grounded text simplifications.

Conclusion: The proposed framework effectively integrates diverse tools to address issues in evaluating simplified scientific texts.

Abstract: In this paper, we describe our methodology for the CLEF 2025 SimpleText Task
2, which focuses on detecting and evaluating creative generation and
information distortion in scientific text simplification. Our solution
integrates multiple strategies: we construct an ensemble framework that
leverages BERT-based classifier, semantic similarity measure, natural language
inference model, and large language model (LLM) reasoning. These diverse
signals are combined using meta-classifiers to enhance the robustness of
spurious and distortion detection. Additionally, for grounded generation, we
employ an LLM-based post-editing system that revises simplifications based on
the original input texts.

</details>


### [73] [A Survey of Idiom Datasets for Psycholinguistic and Computational Research](https://arxiv.org/abs/2508.11828)
*Michael Flor,Xinyi Liu,Anna Feldman*

Main category: cs.CL

TL;DR: This survey reviews 53 datasets on idioms from psycholinguistics and computational linguistics, focusing on their content, form, and tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of interpreting idioms, which are often non-compositional, making them difficult for both computational and human analysis.

Method: The study systematically reviews 53 idiom-related datasets, analyzing their annotations, language coverage, and associated tasks within psycholinguistics and computational linguistics.

Result: It identifies trends in dataset design but observes a lack of integration between psycholinguistic and computational approaches to idioms.

Conclusion: The paper calls attention to the need for bridging the gap between psycholinguistic and computational research to advance idiom understanding.

Abstract: Idioms are figurative expressions whose meanings often cannot be inferred
from their individual words, making them difficult to process computationally
and posing challenges for human experimental studies. This survey reviews
datasets developed in psycholinguistics and computational linguistics for
studying idioms, focusing on their content, form, and intended use.
Psycholinguistic resources typically contain normed ratings along dimensions
such as familiarity, transparency, and compositionality, while computational
datasets support tasks like idiomaticity detection/classification,
paraphrasing, and cross-lingual modeling. We present trends in annotation
practices, coverage, and task framing across 53 datasets. Although recent
efforts expanded language coverage and task diversity, there seems to be no
relation yet between psycholinguistic and computational research on idioms.

</details>


### [74] [Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions](https://arxiv.org/abs/2508.11829)
*Leigh Levinson,Christopher J. Agostino*

Main category: cs.CL

TL;DR: The paper explores how embedding simulated hormonal cycles into large language models can address contextual relevance challenges, revealing emotional, stylistic impacts and biases.


<details>
  <summary>Details</summary>
Motivation: AI systems struggle to determine contextual relevance from vast possibilities, known as the frame problem. Biological rhythms may act as natural relevance filters.

Method: The authors modeled hormonal cycles (e.g., estrogen, testosterone, cortisol) using periodic functions to generate system prompts, embedding these into state-of-the-art language models.

Result: Emotional and stylistic variations align with hormonal patterns (e.g., sadness during menstruation, happiness during ovulation). Benchmarking shows performance peaks in moderate hormonal ranges.

Conclusion: This approach enhances contextual relevance in AI and highlights societal biases embedded in language models regarding gender and biology.

Abstract: Despite significant advances, AI systems struggle with the frame problem:
determining what information is contextually relevant from an exponentially
large possibility space. We hypothesize that biological rhythms, particularly
hormonal cycles, serve as natural relevance filters that could address this
fundamental challenge. We develop a framework that embeds simulated menstrual
and circadian cycles into Large Language Models through system prompts
generated from periodic functions modeling key hormones including estrogen,
testosterone, and cortisol. Across multiple state-of-the-art models, linguistic
analysis reveals emotional and stylistic variations that track biological
phases; sadness peaks during menstruation while happiness dominates ovulation
and circadian patterns show morning optimism transitioning to nocturnal
introspection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates
subtle but consistent performance variations aligning with biological
expectations, including optimal function in moderate rather than extreme
hormonal ranges. This methodology provides a novel approach to contextual AI
while revealing how societal biases regarding gender and biology are embedded
within language models.

</details>


### [75] [When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection](https://arxiv.org/abs/2508.11831)
*Julia Sammartino,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: The paper examines sequential fine-tuning of multilingual models for euphemism detection across five languages, showing improvement in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Euphemisms are ambiguous and culturally variable, creating challenges for language models, particularly for low-resource languages.

Method: The study tests sequential, monolingual, and simultaneous fine-tuning strategies with models XLM-R and mBERT, analyzing performance impact across different languages and linguistic features.

Result: Sequential fine-tuning with high-resource L1 significantly improves L2 performance for low-resource languages like Yoruba and Turkish. XLM-R shows greater gains but is prone to forgetting, whereas mBERT offers stability with slightly lower results.

Conclusion: Sequential fine-tuning presents a straightforward and effective technique for enhancing euphemism detection in multilingual contexts, addressing challenges in low-resource languages.

Abstract: Euphemisms are culturally variable and often ambiguous, posing challenges for
language models, especially in low-resource settings. This paper investigates
how cross-lingual transfer via sequential fine-tuning affects euphemism
detection across five languages: English, Spanish, Chinese, Turkish, and
Yoruba. We compare sequential fine-tuning with monolingual and simultaneous
fine-tuning using XLM-R and mBERT, analyzing how performance is shaped by
language pairings, typological features, and pretraining coverage. Results show
that sequential fine-tuning with a high-resource L1 improves L2 performance,
especially for low-resource languages like Yoruba and Turkish. XLM-R achieves
larger gains but is more sensitive to pretraining gaps and catastrophic
forgetting, while mBERT yields more stable, though lower, results. These
findings highlight sequential fine-tuning as a simple yet effective strategy
for improving euphemism detection in multilingual models, particularly when
low-resource languages are involved.

</details>


### [76] [SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance](https://arxiv.org/abs/2508.11857)
*Andrei-Valentin Tănase,Elena Pelican*

Main category: cs.CL

TL;DR: SupraTok introduces a novel tokenization architecture improving efficiency and model performance without changing model designs.


<details>
  <summary>Details</summary>
Motivation: Tokenization techniques in NLP are outdated compared to advancements in model architectures.

Method: SupraTok innovates tokenization by leveraging multi-word semantic units, entropy-driven corpus optimization, and multi-phase curriculum learning.

Result: SupraTok achieved significant efficiency improvements and boosted benchmark performance in English and 38 other languages.

Conclusion: Efficient tokenization, such as SupraTok, can complement architectural advancements to enhance language model performance.

Abstract: Tokenization remains a fundamental yet underexplored bottleneck in natural
language processing, with strategies largely static despite remarkable progress
in model architectures. We present SupraTok, a novel tokenization architecture
that reimagines subword segmentation through three innovations: cross-boundary
pattern learning that discovers multi-word semantic units, entropy-driven data
curation that optimizes training corpus quality, and multi-phase curriculum
learning for stable convergence. Our approach extends Byte-Pair Encoding by
learning "superword" tokens, coherent multi-word expressions that preserve
semantic unity while maximizing compression efficiency. SupraTok achieves 31%
improvement in English tokenization efficiency (5.91 versus 4.51 characters per
token) compared to OpenAI's o200k tokenizer and 30% improvement over Google's
Gemma 3 tokenizer (256k vocabulary), while maintaining competitive performance
across 38 languages. When integrated with a GPT-2 scale model (124M parameters)
trained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%
improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural
modifications. While these results are promising at this scale, further
validation at larger model scales is needed. These findings suggest that
efficient tokenization can complement architectural innovations as a path to
improved language model performance.

</details>


### [77] [In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning](https://arxiv.org/abs/2508.11889)
*Hui Ma,Bo Zhang,Jinpeng Hu,Zenglin Shi*

Main category: cs.CL

TL;DR: The paper presents InitERC, a one-stage in-context instruction tuning framework for emotion recognition in conversation (ERC), which improves upon existing multi-stage methods by better aligning speaker characteristics, context, and emotion states.


<details>
  <summary>Details</summary>
Motivation: Existing ERC methods have limitations in jointly capturing the interplay between speaker identity, conversational context, and emotional states, leading to weak alignment. A unified framework is needed to address this.

Method: The proposed InitERC framework uses four components: demonstration pool construction, in-context example selection, prompt template design, and in-context instruction tuning. It integrates speaker-context-emotion alignment into a single-stage process.

Result: Extensive experiments conducted on three widely used datasets show that InitERC outperforms state-of-the-art ERC techniques.

Conclusion: InitERC proves to be a simple but effective solution for advancing ERC capabilities by introducing a unified framework for speaker-context-emotion alignment.

Abstract: Emotion recognition in conversation (ERC) aims to identify the emotion of
each utterance in a conversation, playing a vital role in empathetic artificial
intelligence. With the growing of large language models (LLMs), instruction
tuning has emerged as a critical paradigm for ERC. Existing studies mainly
focus on multi-stage instruction tuning, which first endows LLMs with speaker
characteristics, and then conducts context-aware instruction tuning to
comprehend emotional states. However, these methods inherently constrains the
capacity to jointly capture the dynamic interaction between speaker
characteristics and conversational context, resulting in weak alignment among
speaker identity, contextual cues, and emotion states within a unified
framework. In this paper, we propose InitERC, a simple yet effective one-stage
in-context instruction tuning framework for ERC. InitERC adapts LLMs to learn
speaker-context-emotion alignment from context examples via in-context
instruction tuning. Specifically, InitERC comprises four components, i.e.,
demonstration pool construction, in-context example selection, prompt template
design, and in-context instruction tuning. To explore the impact of in-context
examples, we conduct a comprehensive study on three key factors: retrieval
strategy, example ordering, and the number of examples. Extensive experiments
on three widely used datasets demonstrate that our proposed InitERC achieves
substantial improvements over the state-of-the-art baselines.

</details>


### [78] [CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures](https://arxiv.org/abs/2508.11915)
*Punya Syon Pandey,Yongjin Yang,Jiarui Liu,Zhijing Jin*

Main category: cs.CL

TL;DR: This paper introduces CORE, a metric for measuring linguistic diversity and effectiveness in agent interactions using Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: There is a lack of quantification regarding linguistic diversity in game-theoretic interactions between LLM-based agents.

Method: The authors propose CORE, which integrates measures like cluster entropy, lexical repetition, and semantic similarity to assess dialog quality, and apply it in different interaction settings.

Result: Findings reveal that cooperative interactions lead to greater vocabulary expansion and more linguistic repetition, while competitive ones result in constrained vocabularies and less repetition.

Conclusion: CORE is an effective tool for evaluating linguistic robustness in multi-agent LLM systems, shedding light on the influence of social dynamics on language adaptation.

Abstract: Game-theoretic interactions between agents with Large Language Models (LLMs)
have revealed many emergent capabilities, yet the linguistic diversity of these
interactions has not been sufficiently quantified. In this paper, we present
the Conversational Robustness Evaluation Score: CORE, a metric to quantify the
effectiveness of language use within multi-agent systems across different
game-theoretic interactions. CORE integrates measures of cluster entropy,
lexical repetition, and semantic similarity, providing a direct lens of dialog
quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,
and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws
to characterize word frequency distributions and vocabulary growth. Our
findings show that cooperative settings exhibit both steeper Zipf distributions
and higher Heap exponents, indicating more repetition alongside greater
vocabulary expansion. In contrast, competitive interactions display lower Zipf
and Heaps exponents, reflecting less repetition and more constrained
vocabularies. These results provide new insights into how social incentives
influence language adaptation, and highlight CORE as a robust diagnostic for
measuring linguistic robustness in multi-agent LLM systems. Our code is
available at https://github.com/psyonp/core.

</details>


### [79] [LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese](https://arxiv.org/abs/2508.11927)
*Jie Lu,Du Jin,Hitomi Yanaka*

Main category: cs.CL

TL;DR: This paper addresses the challenges of temporal inference in Natural Language Inference (NLI) tasks due to the lack of distinct tense markers in Chinese and Japanese.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to understand and address the difficulty that AI models face in handling NLI tasks in languages like Chinese and Japanese, which lack grammatical forms to indicate tense within the perfect aspect.

Method: The researchers constructed a template-based NLI dataset focusing on the perfect aspect for Chinese and Japanese, with 1,350 sentence pairs per language.

Result: Experimental results showed that advanced language models struggle with temporal reasoning, especially in detecting subtle shifts in tense and reference time.

Conclusion: The study highlights the limitations of existing models in handling cross-linguistic temporal semantics, emphasizing the importance of such evaluations for improvement.

Abstract: Unlike English, which uses distinct forms (e.g., had, has, will have) to mark
the perfect aspect across tenses, Chinese and Japanese lack separate
grammatical forms for tense within the perfect aspect, which complicates
Natural Language Inference (NLI). Focusing on the perfect aspect in these
languages, we construct a linguistically motivated, template-based NLI dataset
(1,350 pairs per language). Experiments reveal that even advanced LLMs struggle
with temporal inference, particularly in detecting subtle tense and
reference-time shifts. These findings highlight model limitations and
underscore the need for cross-linguistic evaluation in temporal semantics. Our
dataset is available at https://github.com/Lujie2001/CrossNLI.

</details>


### [80] [CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection](https://arxiv.org/abs/2508.11933)
*Yue Wang,Liesheng Wei,Yuxiang Wang*

Main category: cs.CL

TL;DR: This paper introduces CAMF, a framework to detect machine-generated text (MGT) using a multi-agent LLM-based system that significantly outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: The rapid rise of machine-generated text raises risks like disinformation and academic integrity threats, and current zero-shot detection techniques underperform in robustly identifying such text.

Method: The authors propose CAMF, which consists of three phases: feature extraction, adversarial consistency probing, and judgment aggregation by incorporating multiple specialized LLM-based agents.

Result: CAMF is empirically shown to significantly outperform state-of-the-art zero-shot techniques for MGT detection.

Conclusion: The CAMF framework offers a better and more reliable mechanism to detect machine-generated text by addressing critical shortcomings in existing methods.

Abstract: Detecting machine-generated text (MGT) from contemporary Large Language
Models (LLMs) is increasingly crucial amid risks like disinformation and
threats to academic integrity. Existing zero-shot detection paradigms, despite
their practicality, often exhibit significant deficiencies. Key challenges
include: (1) superficial analyses focused on limited textual attributes, and
(2) a lack of investigation into consistency across linguistic dimensions such
as style, semantics, and logic. To address these challenges, we introduce the
\textbf{C}ollaborative \textbf{A}dversarial \textbf{M}ulti-agent
\textbf{F}ramework (\textbf{CAMF}), a novel architecture using multiple
LLM-based agents. CAMF employs specialized agents in a synergistic three-phase
process: \emph{Multi-dimensional Linguistic Feature Extraction},
\emph{Adversarial Consistency Probing}, and \emph{Synthesized Judgment
Aggregation}. This structured collaborative-adversarial process enables a deep
analysis of subtle, cross-dimensional textual incongruities indicative of
non-human origin. Empirical evaluations demonstrate CAMF's significant
superiority over state-of-the-art zero-shot MGT detection techniques.

</details>


### [81] [Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases](https://arxiv.org/abs/2508.12031)
*Shaozhe Yin,Jinyu Guo,Kai Shuang,Xia Liu,Ruize Ou*

Main category: cs.CL

TL;DR: The paper enhances continual relation extraction (CRE) by proposing an instruction-based contrastive tuning approach, which uses error cases for more effective fine-tuning of large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Existing methods in CRE mainly rely on memory replay and contrastive learning, yet they overlook the significance of error cases in addressing catastrophic forgetting. This research seeks a more targeted approach to alleviate cognitive biases.

Method: They introduce a dual-task fine-tuning strategy that distinguishes between correct and incorrect responses in training and memory data. Additionally, they propose instruction-based contrastive tuning leveraging LLMs' ability to follow instructions, facilitating better handling of emerging relations.

Result: Experiments on TACRED and FewRel datasets show that the proposed method achieves new state-of-the-art performance in CRE, with significant improvements.

Conclusion: Focusing on error cases and leveraging instructional capabilities of LLMs proves crucial in advancing CRE methods, effectively addressing catastrophic forgetting.

Abstract: Continual Relation Extraction (CRE) aims to continually learn new emerging
relations while avoiding catastrophic forgetting. Existing CRE methods mainly
use memory replay and contrastive learning to mitigate catastrophic forgetting.
However, these methods do not attach importance to the error cases that can
reveal the model's cognitive biases more effectively. To address this issue, we
propose an instruction-based continual contrastive tuning approach for Large
Language Models (LLMs) in CRE. Different from existing CRE methods that
typically handle the training and memory data in a unified manner, this
approach splits the training and memory data of each task into two parts
respectively based on the correctness of the initial responses and treats them
differently through dual-task fine-tuning. In addition, leveraging the
advantages of LLM's instruction-following ability, we propose a novel
instruction-based contrastive tuning strategy for LLM to continuously correct
current cognitive biases with the guidance of previous data in an
instruction-tuning manner, which mitigates the gap between old and new
relations in a more suitable way for LLMs. We experimentally evaluate our model
on TACRED and FewRel, and the results show that our model achieves new
state-of-the-art CRE performance with significant improvements, demonstrating
the importance of specializing in exploiting error cases.

</details>


### [82] [Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation](https://arxiv.org/abs/2508.12040)
*Jinyi Han,Tingyun Li,Shisong Chen,Jie Shi,Xinyi Wang,Guanglei Yue,Jiaqing Liang,Xin Lin,Liqian Wen,Zulong Chen,Yanghua Xiao*

Main category: cs.CL

TL;DR: The paper proposes FineCE, a novel fine-grained confidence estimation method for large language models, addressing overconfidence and improving trustworthiness of generated outputs.


<details>
  <summary>Details</summary>
Motivation: To overcome the coarse-grained scoring mechanisms in existing confidence estimation methods for large language models, which hinder the reliability of LLM-generated outputs.

Method: FineCE involves constructing a rigorous training pipeline to capture the probabilistic distribution of responses and training a supervised model. It also introduces Backward Confidence Integration and position optimization strategies for continuous confidence scoring.

Result: FineCE consistently outperformed classical confidence estimation methods across multiple benchmark datasets, offering accurate fine-grained confidence scores.

Conclusion: The proposed FineCE method improves the trustworthiness and reliability of large language models by providing superior confidence estimation during text generation, validated through extensive experiments.

Abstract: While large language models (LLMs) have demonstrated remarkable performance
across diverse tasks, they fundamentally lack self-awareness and frequently
exhibit overconfidence, assigning high confidence scores to incorrect
predictions. Accurate confidence estimation is therefore critical for enhancing
the trustworthiness and reliability of LLM-generated outputs. However, existing
approaches suffer from coarse-grained scoring mechanisms that fail to provide
fine-grained, continuous confidence estimates throughout the generation
process. To address these limitations, we introduce FineCE, a novel confidence
estimation method that delivers accurate, fine-grained confidence scores during
text generation. Specifically, we first develop a comprehensive pipeline for
constructing training data that effectively captures the underlying
probabilistic distribution of LLM responses, and then train a model to predict
confidence scores for arbitrary text sequences in a supervised manner.
Furthermore, we propose a Backward Confidence Integration (BCI) strategy that
leverages information from the subsequent text to enhance confidence estimation
for the current sequence during inference. We also introduce three strategies
for identifying optimal positions to perform confidence estimation within the
generation process. Extensive experiments on multiple benchmark datasets
demonstrate that FineCE consistently outperforms existing classical confidence
estimation methods. Our code and all baselines used in the paper are available
on GitHub.

</details>


### [83] [J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs](https://arxiv.org/abs/2508.12086)
*Yao Wu*

Main category: cs.CL

TL;DR: This paper introduces J6, a structured Jacobian-based method designed to improve multi-objective optimization while adapting large language models (LLMs), addressing conflicts between factuality and confidence.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of balancing multiple optimization objectives, such as improving factuality and increasing confidence, in large language model adaptation while accounting for non-trivial interactions between prompt parameters.

Method: The authors propose J6, which decomposes the gradient interaction matrix into six interpretable components. It facilitates dynamic update strategies like hard decision-making (via argmax) and soft strategies (via softmax) to resolve local conflicts and exploit synergies.

Result: J6 enables conflict-aware prompt optimization by leveraging insights into parameter attribution, task interference, and geometry-aligned adaptation, achieving better alignment between multiple objectives.

Conclusion: The J6 framework introduces a principled approach for structured Jacobian reasoning in neural tuning, offering extensibility and enhanced interpretability for multi-objective optimization in LLM adaptation.

Abstract: In large language model (LLM) adaptation, balancing multiple optimization
objectives such as improving factuality (heat) and increasing confidence (via
low entropy) poses a fundamental challenge, especially when prompt parameters
(e.g., hidden-layer insertions h and embedding modifications w) interact in
non-trivial ways. Existing multi-objective optimization strategies often rely
on scalar gradient aggregation, ignoring the deeper geometric structure between
objectives and parameters. We propose J6, a structured Jacobian-based method
that decomposes the gradient interaction matrix into six interpretable
components. This decomposition enables both hard decision-making (e.g.,
choosing the dominant update direction via argmax) and soft strategies (e.g.,
attention-style weighting via softmax over J6), forming a dynamic update
framework that adapts to local conflict and synergy. Moreover, the
interpretable structure of J6 provides insight into parameter attribution, task
interference, and geometry-aligned adaptation. Our work introduces a principled
and extensible mechanism for conflict-aware prompt optimization, and opens a
new avenue for incorporating structured Jacobian reasoning into multi-objective
neural tuning.

</details>


### [84] [STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples](https://arxiv.org/abs/2508.12096)
*Haiquan Hu,Jiazhi Jiang,Shiyou Xu,Ruhan Zeng,Tian Wang*

Main category: cs.CL

TL;DR: The paper proposes STEM, a structured and lightweight evaluation method for assessing LLM capabilities through consistent performance transitions from samples among models with varied scales.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods fail to effectively measure the real-world reasoning advancements of LLMs and suffer from benchmark overfitting and high computational costs.

Method: STEM utilizes significant transition samples (STS) by analyzing consistent performance transitions among LLM architectures with varied parameter scales to estimate capabilities.

Result: Experimental results show that STEM can reliably capture performance trends and align with ground-truth rankings, demonstrating its scalability and efficiency.

Conclusion: STEM is a practical and scalable solution for precise evaluation of LLMs that extends beyond traditional computationally expensive benchmarks.

Abstract: Evaluating large language models (LLMs) has become increasingly challenging
as model capabilities advance rapidly. While recent models often achieve higher
scores on standard benchmarks, these improvements do not consistently reflect
enhanced real-world reasoning capabilities. Moreover, widespread overfitting to
public benchmarks and the high computational cost of full evaluations have made
it both expensive and less effective to distinguish meaningful differences
between models. To address these challenges, we propose the \textbf{S}tructured
\textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweight
and interpretable evaluation framework for efficiently estimating the relative
capabilities of LLMs. STEM identifies \textit{significant transition samples}
(STS) by analyzing consistent performance transitions among LLMs of the same
architecture but varying parameter scales. These samples enable STEM to
effectively estimate the capability position of an unknown model. Qwen3 model
family is applied to construct the STS pool on six diverse and representative
benchmarks. To assess generalizability. Experimental results indicate that STEM
reliably captures performance trends, aligns with ground-truth rankings of
model capability. These findings highlight STEM as a practical and scalable
method for fine-grained, architecture-agnostic evaluation of LLMs.

</details>


### [85] [Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality](https://arxiv.org/abs/2508.12140)
*Ziqian Bi,Lu Chen,Junhao Song,Hongying Luo,Enze Ge,Junmin Huang,Tianyang Wang,Keyu Chen,Chia Xin Liang,Zihan Wei,Huafeng Liu,Chunjie Tian,Jibin Guan,Joe Yeong,Yongzhi Xu,Peng Wang,Junfeng Hao*

Main category: cs.CL

TL;DR: The paper evaluates the impact of computational resources (thinking budgets) on medical AI reasoning quality across model sizes and datasets, revealing scaling laws and efficiency regimes.


<details>
  <summary>Details</summary>
Motivation: To optimize medical AI systems by exploring how thinking budgets affect model performance and reasoning quality, ensuring alignment with clinical needs.

Method: Systematic evaluation of two major AI model families, Qwen3 and DeepSeek-R1, across 15 medical datasets. Controlled experiments with varying thinking budgets and analysis of scaling relationships.

Result: Accuracy improvements follow logarithmic scaling with thinking budget and model size, identifying three efficiency regimes. Smaller models gain more from extended thinking. Domain-specific reasoning needs also vary.

Conclusion: Thinking budget control is crucial for medical AI optimization, allowing dynamic resource allocation and fostering transparency in healthcare applications. Results are generalizable across architectures.

Abstract: This study presents the first comprehensive evaluation of thinking budget
mechanisms in medical reasoning tasks, revealing fundamental scaling laws
between computational resources and reasoning quality. We systematically
evaluated two major model families, Qwen3 (1.7B to 235B parameters) and
DeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning
diverse specialties and difficulty levels. Through controlled experiments with
thinking budgets ranging from zero to unlimited tokens, we establish
logarithmic scaling relationships where accuracy improvements follow a
predictable pattern with both thinking budget and model size. Our findings
identify three distinct efficiency regimes: high-efficiency (0 to 256 tokens)
suitable for real-time applications, balanced (256 to 512 tokens) offering
optimal cost-performance tradeoffs for routine clinical support, and
high-accuracy (above 512 tokens) justified only for critical diagnostic tasks.
Notably, smaller models demonstrate disproportionately larger benefits from
extended thinking, with 15 to 20% improvements compared to 5 to 10% for larger
models, suggesting a complementary relationship where thinking budget provides
greater relative benefits for capacity-constrained models. Domain-specific
patterns emerge clearly, with neurology and gastroenterology requiring
significantly deeper reasoning processes than cardiovascular or respiratory
medicine. The consistency between Qwen3 native thinking budget API and our
proposed truncation method for DeepSeek-R1 validates the generalizability of
thinking budget concepts across architectures. These results establish thinking
budget control as a critical mechanism for optimizing medical AI systems,
enabling dynamic resource allocation aligned with clinical needs while
maintaining the transparency essential for healthcare deployment.

</details>


### [86] [LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data](https://arxiv.org/abs/2508.12158)
*Stephen Meisenbacher,Alexandra Klymenko,Florian Matthes*

Main category: cs.CL

TL;DR: The paper explores the use of Large Language Models (LLMs) as evaluators for privacy sensitivity in textual data, showing potential due to their alignment with human perspectives.


<details>
  <summary>Details</summary>
Motivation: The accurate evaluation of privacy in Natural Language Processing remains difficult due to its subjective nature.

Method: The researchers analyzed 10 datasets, 13 LLMs, and surveyed 677 humans to compare the alignment of LLM-evaluations with human perceptions of privacy.

Result: Findings indicate privacy is hard to measure empirically, but LLMs can model global human privacy perspectives with reasonable accuracy.

Conclusion: LLMs show promise for assessing privacy sensitivity in textual data, though limitations exist. This motivates further exploration of their potential in privacy evaluation tasks.

Abstract: Despite advances in the field of privacy-preserving Natural Language
Processing (NLP), a significant challenge remains the accurate evaluation of
privacy. As a potential solution, using LLMs as a privacy evaluator presents a
promising approach $\unicode{x2013}$ a strategy inspired by its success in
other subfields of NLP. In particular, the so-called $\textit{LLM-as-a-Judge}$
paradigm has achieved impressive results on a variety of natural language
evaluation tasks, demonstrating high agreement rates with human annotators.
Recognizing that privacy is both subjective and difficult to define, we
investigate whether LLM-as-a-Judge can also be leveraged to evaluate the
privacy sensitivity of textual data. Furthermore, we measure how closely LLM
evaluations align with human perceptions of privacy in text. Resulting from a
study involving 10 datasets, 13 LLMs, and 677 human survey participants, we
confirm that privacy is indeed a difficult concept to measure empirically,
exhibited by generally low inter-human agreement rates. Nevertheless, we find
that LLMs can accurately model a global human privacy perspective, and through
an analysis of human and LLM reasoning patterns, we discuss the merits and
limitations of LLM-as-a-Judge for privacy evaluation in textual data. Our
findings pave the way for exploring the feasibility of LLMs as privacy
evaluators, addressing a core challenge in solving pressing privacy issues with
innovative technical solutions.

</details>


### [87] [Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges](https://arxiv.org/abs/2508.12227)
*Abdelhamid Haouhat,Slimane Bellaouar,Attia Nehar,Hadda Cherroun,Ahmed Abdelali*

Main category: cs.CL

TL;DR: This paper surveys Arabic MML by categorizing efforts through a novel taxonomy, focusing on datasets, applications, approaches, and challenges.


<details>
  <summary>Details</summary>
Motivation: The paper aims to consolidate knowledge in Arabic Multimodal Machine Learning, an emerging field, and identify research gaps.

Method: A taxonomy-based approach was adopted to categorize contributions into four key areas: datasets, applications, approaches, and challenges.

Result: The survey provides a structured overview of Arabic MML, pinpointing untouched research areas and critical gaps in the field.

Conclusion: This survey equips researchers with insights to explore identified opportunities and challenges, fostering advancements in Arabic MML.

Abstract: Multimodal Machine Learning (MML) aims to integrate and analyze information
from diverse modalities, such as text, audio, and visuals, enabling machines to
address complex tasks like sentiment analysis, emotion recognition, and
multimedia retrieval. Recently, Arabic MML has reached a certain level of
maturity in its foundational development, making it time to conduct a
comprehensive survey. This paper explores Arabic MML by categorizing efforts
through a novel taxonomy and analyzing existing research. Our taxonomy
organizes these efforts into four key topics: datasets, applications,
approaches, and challenges. By providing a structured overview, this survey
offers insights into the current state of Arabic MML, highlighting areas that
have not been investigated and critical research gaps. Researchers will be
empowered to build upon the identified opportunities and address challenges to
advance the field.

</details>


### [88] [SEA-BED: Southeast Asia Embedding Benchmark](https://arxiv.org/abs/2508.12243)
*Wuttikorn Ponwitayarat,Raymond Ng,Jann Railey Montalan,Thura Aung,Jian Gang Ngui,Yosephine Susanto,William Tjhi,Panuthep Tasawong,Erik Cambria,Ekapol Chuangsuwanich,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: SEA-BED is a new benchmark for sentence embeddings in Southeast Asian languages, featuring 169 datasets with 71% human-curated content. It identifies unique challenges in these languages and examines translation quality and model performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robust embedding evaluation resources for Southeast Asian languages and tasks, which are often overlooked in NLP benchmarks.

Method: Developed SEA-BED, a large-scale benchmark covering 169 datasets across 10 Southeast Asian languages and 9 tasks, with evaluations of 17 embedding models, examining performance variations and translation impacts.

Result: SEA-BED reveals significant performance variations across SEA languages, highlights the unique challenges of low-resource languages like Burmese, and emphasizes the value of human-curated data over machine-translated datasets.

Conclusion: A region-specific benchmark like SEA-BED is essential for advancing NLP in underrepresented languages, ensuring better evaluation and model performance tailored to diverse linguistic challenges.

Abstract: Sentence embeddings are essential for NLP tasks such as semantic search,
re-ranking, and textual similarity. Although multilingual benchmarks like MMTEB
broaden coverage, Southeast Asia (SEA) datasets are scarce and often
machine-translated, missing native linguistic properties. With nearly 700
million speakers, the SEA region lacks a region-specific embedding benchmark.
We introduce SEA-BED, the first large-scale SEA embedding benchmark with 169
datasets across 9 tasks and 10 languages, where 71% are formulated by humans,
not machine generation or translation. We address three research questions: (1)
which SEA languages and tasks are challenging, (2) whether SEA languages show
unique performance gaps globally, and (3) how human vs. machine translations
affect evaluation. We evaluate 17 embedding models across six studies,
analyzing task and language challenges, cross-benchmark comparisons, and
translation trade-offs. Results show sharp ranking shifts, inconsistent model
performance among SEA languages, and the importance of human-curated datasets
for low-resource languages like Burmese.

</details>


### [89] [What do Speech Foundation Models Learn? Analysis and Applications](https://arxiv.org/abs/2508.12255)
*Ankita Pasad*

Main category: cs.CL

TL;DR: The paper explores Speech Foundation Models (SFMs), analyzing their encoded acoustic and linguistic knowledge, and addresses their application in spoken language understanding tasks like named entity recognition (NER) and localization (NEL). It also provides tools and datasets for deeper analysis.


<details>
  <summary>Details</summary>
Motivation: There is a limited understanding of the knowledge encoded in SFMs despite their widespread use. Additionally, SFMs' effectiveness in complex spoken language understanding (SLU) tasks has not been adequately explored due to dataset limitations.

Method: A lightweight analytical framework using statistical tools and training-free tasks was developed to assess SFMs. New SLU tasks like NER and NEL were introduced, along with evaluations of SFM-based end-to-end models versus cascaded models.

Result: The analysis offers insights into the acoustic and linguistic knowledge within SFMs and demonstrates that end-to-end models using SFMs outperform traditional approaches for NER and NEL tasks.

Conclusion: The paper delivers tools, datasets, and insights to better understand SFMs, emphasizing their utility in SLU tasks. It aims to guide the design and adoption of future models to handle broader speech-processing challenges.

Abstract: Speech foundation models (SFMs) are designed to serve as general-purpose
representations for a wide range of speech-processing tasks. The last five
years have seen an influx of increasingly successful self-supervised and
supervised pre-trained models with impressive performance on various downstream
tasks.
  Although the zoo of SFMs continues to grow, our understanding of the
knowledge they acquire lags behind. This thesis presents a lightweight analysis
framework using statistical tools and training-free tasks to investigate the
acoustic and linguistic knowledge encoded in SFM layers. We conduct a
comparative study across multiple SFMs and statistical tools. Our study also
shows that the analytical insights have concrete implications for downstream
task performance.
  The effectiveness of an SFM is ultimately determined by its performance on
speech applications. Yet it remains unclear whether the benefits extend to
spoken language understanding (SLU) tasks that require a deeper understanding
than widely studied ones, such as speech recognition. The limited exploration
of SLU is primarily due to a lack of relevant datasets. To alleviate that, this
thesis contributes tasks, specifically spoken named entity recognition (NER)
and named entity localization (NEL), to the Spoken Language Understanding
Evaluation benchmark. We develop SFM-based approaches for NER and NEL, and find
that end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded
(speech recognition followed by a text model) approaches. Further, we evaluate
E2E SLU models across SFMs and adaptation strategies to assess the impact on
task performance.
  Collectively, this thesis tackles previously unanswered questions about SFMs,
providing tools and datasets to further our understanding and to enable the
community to make informed design choices for future model development and
adoption.

</details>


### [90] [Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework](https://arxiv.org/abs/2508.12257)
*Zheye Deng,Chunkit Chan,Tianshi Zheng,Wei Fan,Weiqi Wang,Yangqiu Song*

Main category: cs.CL

TL;DR: This paper reviews text-to-structure techniques, challenges, datasets, and evaluation metrics, introducing a universal framework for structured outputs in AI systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in comprehensive studies on converting unstructured text into structured formats, which are crucial for advanced AI applications like summarization and data mining.

Method: The paper conducts a systematic review of text-to-structure methods, evaluates current datasets and metrics, and proposes a universal evaluation framework for structured outputs.

Result: The study identifies challenges, gaps, and advancements in text-to-structure methods, contributing a universal framework for better evaluation of structured outputs.

Conclusion: Text-to-structure transformation is essential for next-gen AI, requiring standardized evaluative frameworks and further research to enhance methodologies and datasets.

Abstract: The evolution of AI systems toward agentic operation and context-aware
retrieval necessitates transforming unstructured text into structured formats
like tables, knowledge graphs, and charts. While such conversions enable
critical applications from summarization to data mining, current research lacks
a comprehensive synthesis of methodologies, datasets, and metrics. This
systematic review examines text-to-structure techniques and the encountered
challenges, evaluates current datasets and assessment criteria, and outlines
potential directions for future research. We also introduce a universal
evaluation framework for structured outputs, establishing text-to-structure as
foundational infrastructure for next-generation AI systems.

</details>


### [91] [Fast, Slow, and Tool-augmented Thinking for LLMs: A Review](https://arxiv.org/abs/2508.12265)
*Xinda Jia,Jinpeng Li,Zezhong Wang,Jingjing Li,Xingshan Zeng,Yasheng Wang,Weinan Zhang,Yong Yu,Weiwen Liu*

Main category: cs.CL

TL;DR: The paper presents a taxonomy for reasoning strategies in LLMs, inspired by cognitive psychology, and surveys recent adaptive reasoning methods.


<details>
  <summary>Details</summary>
Motivation: There is a need for LLMs to adapt reasoning strategies based on task requirements, ranging from intuitive responses to deliberate tool-augmented reasoning.

Method: The authors propose a taxonomy distinguishing intuitive versus deliberative reasoning and internal versus external reasoning. They also survey existing adaptation methods in LLMs.

Result: The taxonomy and categorization provide clarity about reasoning methods in LLMs and highlight challenges and future directions.

Conclusion: The study emphasizes the importance of adaptive reasoning for efficient and reliable LLMs, offering a framework and insights for future research.

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
reasoning across diverse domains. However, effective reasoning in real-world
tasks requires adapting the reasoning strategy to the demands of the problem,
ranging from fast, intuitive responses to deliberate, step-by-step reasoning
and tool-augmented thinking. Drawing inspiration from cognitive psychology, we
propose a novel taxonomy of LLM reasoning strategies along two knowledge
boundaries: a fast/slow boundary separating intuitive from deliberative
processes, and an internal/external boundary distinguishing reasoning grounded
in the model's parameters from reasoning augmented by external tools. We
systematically survey recent work on adaptive reasoning in LLMs and categorize
methods based on key decision factors. We conclude by highlighting open
challenges and future directions toward more adaptive, efficient, and reliable
LLMs.

</details>


### [92] [The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution](https://arxiv.org/abs/2508.12277)
*Elon Ezra,Ariel Weizman,Amos Azaria*

Main category: cs.CL

TL;DR: This paper introduces a benchmark to evaluate large language models (LLMs) on their ability to anticipate aspects of their own responses and reveals limitations in their performance.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLMs focus on knowledge and reasoning, but this work aims to assess whether LLMs can predict characteristics of their own outputs.

Method: The authors designed the Self-Execution Benchmark, which tests LLMs on predicting features like difficulty level, refusal likelihood, and types of associations in their responses.

Result: Experiments show LLMs generally struggle with self-prediction tasks, and improvements in model size or capability don't consistently enhance benchmark performance.

Conclusion: Models have inherent limitations in representing and reasoning about their own behavior, implying challenges in improving self-awareness capabilities in LLMs.

Abstract: Large language models (LLMs) are commonly evaluated on tasks that test their
knowledge or reasoning abilities. In this paper, we explore a different type of
evaluation: whether an LLM can predict aspects of its own responses. Since LLMs
lack the ability to execute themselves, we introduce the Self-Execution
Benchmark, which measures a model's ability to anticipate properties of its
output, such as whether a question will be difficult for it, whether it will
refuse to answer, or what kinds of associations it is likely to produce. Our
experiments show that models generally perform poorly on this benchmark, and
that increased model size or capability does not consistently lead to better
performance. These results suggest a fundamental limitation in how LLMs
represent and reason about their own behavior.

</details>


### [93] [Legal$Δ$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain](https://arxiv.org/abs/2508.12281)
*Xin Dai,Buqiang Xu,Zhenghao Liu,Yukun Yan,Huiyuan Xie,Xiaoyuan Yi,Shuo Wang,Ge Yu*

Main category: cs.CL

TL;DR: The paper introduces LegalΔ, a reinforcement learning-based framework to improve the reasoning process of legal AI systems using chain-of-thought guided information gain. It shows enhanced accuracy and interpretability in legal reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing legal AI systems struggle with generating reliable and interpretable reasoning processes due to their reliance on direct answers rather than multi-step reasoning.

Method: LegalΔ is trained using a two-stage reinforcement learning approach, leveraging reasoning capabilities from DeepSeek-R1 and refining reasoning quality via differential comparisons and a reward mechanism for structural coherence and legal-domain specificity.

Result: Experimental results show that LegalΔ surpasses baseline models in accuracy and interpretability across various legal reasoning tasks while ensuring robust legal judgments without dependence on labeled preference data.

Conclusion: LegalΔ represents a significant advance in automating complex legal reasoning tasks by improving accuracy and interpretability, addressing shortcomings of current legal language models.

Abstract: Legal Artificial Intelligence (LegalAI) has achieved notable advances in
automating judicial decision-making with the support of Large Language Models
(LLMs). However, existing legal LLMs still struggle to generate reliable and
interpretable reasoning processes. They often default to fast-thinking behavior
by producing direct answers without explicit multi-step reasoning, limiting
their effectiveness in complex legal scenarios that demand rigorous
justification. To address this challenge, we propose Legal$\Delta$, a
reinforcement learning framework designed to enhance legal reasoning through
chain-of-thought guided information gain. During training, Legal$\Delta$
employs a dual-mode input setup-comprising direct answer and
reasoning-augmented modes-and maximizes the information gain between them. This
encourages the model to acquire meaningful reasoning patterns rather than
generating superficial or redundant explanations. Legal$\Delta$ follows a
two-stage approach: (1) distilling latent reasoning capabilities from a
powerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning
quality via differential comparisons, combined with a multidimensional reward
mechanism that assesses both structural coherence and legal-domain specificity.
Experimental results on multiple legal reasoning tasks demonstrate that
Legal$\Delta$ outperforms strong baselines in both accuracy and
interpretability. It consistently produces more robust and trustworthy legal
judgments without relying on labeled preference data. All code and data will be
released at https://github.com/NEUIR/LegalDelta.

</details>


### [94] [Improving Detection of Watermarked Language Models](https://arxiv.org/abs/2508.13131)
*Dara Bahri,John Wieting*

Main category: cs.CL

TL;DR: The paper explores combining watermark-based and non-watermark detection methods to improve detection of large language model (LLM) generations, especially under low-entropy conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the challenge of detecting LLM generations when limited entropy arises, particularly in models refined through methods like instruction tuning or RLHF, where traditional watermarking alone proves less effective.

Method: The authors investigate hybrid detection schemes by combining watermark and non-watermark detection techniques, comparing their performance under varying experimental conditions.

Result: The hybrid approaches demonstrate better detection performance compared to using either watermark-based or non-watermark-based methods independently, across diverse scenarios.

Conclusion: Hybrid detection methods offer a more robust solution to detecting LLM outputs, particularly in cases where constraints such as low entropy reduce the effectiveness of watermark techniques alone.

Abstract: Watermarking has recently emerged as an effective strategy for detecting the
generations of large language models (LLMs). The strength of a watermark
typically depends strongly on the entropy afforded by the language model and
the set of input prompts. However, entropy can be quite limited in practice,
especially for models that are post-trained, for example via instruction tuning
or reinforcement learning from human feedback (RLHF), which makes detection
based on watermarking alone challenging. In this work, we investigate whether
detection can be improved by combining watermark detectors with non-watermark
ones. We explore a number of hybrid schemes that combine the two, observing
performance gains over either class of detector under a wide range of
experimental conditions.

</details>


### [95] [A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.12282)
*Ziyang Chen,Erxue Min,Xiang Zhao,Yunxin Li,Xin Jia,Jinzhi Liao,Jichao Li,Shuaiqiang Wang,Baotian Hu,Dawei Yin*

Main category: cs.CL

TL;DR: ChronoQA is a benchmark dataset for Chinese question answering, focusing on temporal reasoning in RAG systems, built from 300,000 news articles with 5,176 annotated questions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of temporal reasoning in RAG systems and provide a reliable benchmark for time-sensitive QA tasks.

Method: ChronoQA includes multi-document scenarios, structural annotations, and multi-stage validation using rules, language models, and human evaluation.

Result: The dataset consists of diverse, well-validated questions on absolute, aggregate, and relative temporal reasoning types.

Conclusion: ChronoQA offers a scalable resource for evaluating and improving temporal reasoning in retrieval-augmented QA systems.

Abstract: We introduce ChronoQA, a large-scale benchmark dataset for Chinese question
answering, specifically designed to evaluate temporal reasoning in
Retrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over
300,000 news articles published between 2019 and 2024, and contains 5,176
high-quality questions covering absolute, aggregate, and relative temporal
types with both explicit and implicit time expressions. The dataset supports
both single- and multi-document scenarios, reflecting the real-world
requirements for temporal alignment and logical consistency. ChronoQA features
comprehensive structural annotations and has undergone multi-stage validation,
including rule-based, LLM-based, and human evaluation, to ensure data quality.
By providing a dynamic, reliable, and scalable resource, ChronoQA enables
structured evaluation across a wide range of temporal tasks, and serves as a
robust benchmark for advancing time-sensitive retrieval-augmented question
answering systems.

</details>


### [96] [Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction](https://arxiv.org/abs/2508.12286)
*Qinghua Wang,Xu Zhang,Lingyan Yang,Rui Shao,Bonan Wang,Fang Wang,Cunquan Qu*

Main category: cs.CL

TL;DR: This paper proposes a new deep learning-based approach to predict probation outcomes, incorporating legal logic and outperforming other models.


<details>
  <summary>Details</summary>
Motivation: The current Intelligent Judicial Assistant System (IJAS) lacks specialized methods for probation prediction and neglects the legal logic in judicial decision-making, creating a need for improved systems.

Method: The authors created a dedicated probation dataset, developed the Multi-Task Dual-Theory Probation Prediction Model (MT-DT), and validated their approach through experiments.

Result: The MT-DT model demonstrated superior performance compared to baseline models in predicting probation eligibility.

Conclusion: Integrating legal logic into deep learning models enhances probation prediction and validates its alignment with judicial principles.

Abstract: Probation is a crucial institution in modern criminal law, embodying the
principles of fairness and justice while contributing to the harmonious
development of society. Despite its importance, the current Intelligent
Judicial Assistant System (IJAS) lacks dedicated methods for probation
prediction, and research on the underlying factors influencing probation
eligibility remains limited. In addition, probation eligibility requires a
comprehensive analysis of both criminal circumstances and remorse. Much of the
existing research in IJAS relies primarily on data-driven methodologies, which
often overlooks the legal logic underpinning judicial decision-making. To
address this gap, we propose a novel approach that integrates legal logic into
deep learning models for probation prediction, implemented in three distinct
stages. First, we construct a specialized probation dataset that includes fact
descriptions and probation legal elements (PLEs). Second, we design a distinct
probation prediction model named the Multi-Task Dual-Theory Probation
Prediction Model (MT-DT), which is grounded in the legal logic of probation and
the \textit{Dual-Track Theory of Punishment}. Finally, our experiments on the
probation dataset demonstrate that the MT-DT model outperforms baseline models,
and an analysis of the underlying legal logic further validates the
effectiveness of the proposed approach.

</details>


### [97] [CarelessWhisper: Turning Whisper into a Causal Streaming Model](https://arxiv.org/abs/2508.12301)
*Tomer Krichli,Bhiksha Raj,Joseph Keshet*

Main category: cs.CL

TL;DR: The paper addresses adapting SOTA ASR models like Whisper and Canary for streaming transcription through encoder modifications and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current SOTA ASR models have excellent offline performance but struggle with streaming transcription due to architectural limitations.

Method: The authors propose converting non-causal encoder-decoder models into causal models using Low-Rank Adaptation (LoRA) and weakly aligned datasets, as well as an updated inference mechanism for decoding.

Result: Their method achieves better performance on low-latency chunk sizes compared to non-fine-tuned streaming approaches, with lower computational complexity and improved word-level alignment.

Conclusion: The adapted models enhance streaming transcription and enable new capabilities like word-level timestamps extraction, supporting further ASR development.

Abstract: Automatic Speech Recognition (ASR) has seen remarkable progress, with models
like OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA)
performance in offline transcription. However, these models are not designed
for streaming (online or real-time) transcription, due to limitations in their
architecture and training methodology. We propose a method to turn the
transformer encoder-decoder model into a low-latency streaming model that is
careless about future context. We present an analysis explaining why it is not
straightforward to convert an encoder-decoder transformer to a low-latency
streaming model. Our proposed method modifies the existing (non-causal) encoder
to a causal encoder by fine-tuning both the encoder and decoder using Low-Rank
Adaptation (LoRA) and a weakly aligned dataset. We then propose an updated
inference mechanism that utilizes the fine-tune causal encoder and decoder to
yield greedy and beam-search decoding, and is shown to be locally optimal.
Experiments on low-latency chunk sizes (less than 300 msec) show that our
fine-tuned model outperforms existing non-fine-tuned streaming approaches in
most cases, while using a lower complexity. Additionally, we observe that our
training process yields better alignment, enabling a simple method for
extracting word-level timestamps. We release our training and inference code,
along with the fine-tuned models, to support further research and development
in streaming ASR.

</details>


### [98] [Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering](https://arxiv.org/abs/2508.12355)
*Eviatar Nachshoni,Arie Cattan,Shmuel Amar,Ori Shapira,Ido Dagan*

Main category: cs.CL

TL;DR: The paper introduces NATCONFQA, a benchmark for multi-answer question answering (MAQA) that identifies conflicts between valid answers, and evaluates LLMs on this task.


<details>
  <summary>Details</summary>
Motivation: To improve multi-answer question answering (MAQA) where questions can have conflicting valid answers, a challenging area due to the cost of creating realistic datasets.

Method: The authors propose a method to construct a conflict-aware MAQA dataset by repurposing fact-checking datasets, enriching it with detailed conflict annotation.

Result: NATCONFQA was created and used to evaluate eight high-end LLMs, exposing their weaknesses in identifying and resolving answer conflicts.

Conclusion: The paper highlights the difficulty of MAQA tasks for LLMs and presents a cost-effective benchmark, encouraging better models for identifying and resolving conflicting answers.

Abstract: Large Language Models (LLMs) have demonstrated strong performance in question
answering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a
question may have several valid answers, remains challenging. Traditional QA
settings often assume consistency across evidences, but MAQA can involve
conflicting answers. Constructing datasets that reflect such conflicts is
costly and labor-intensive, while existing benchmarks often rely on synthetic
data, restrict the task to yes/no questions, or apply unverified automated
annotation. To advance research in this area, we extend the conflict-aware MAQA
setting to require models not only to identify all valid answers, but also to
detect specific conflicting answer pairs, if any. To support this task, we
introduce a novel cost-effective methodology for leveraging fact-checking
datasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware
MAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate
eight high-end LLMs on NATCONFQA, revealing their fragility in handling various
types of conflicts and the flawed strategies they employ to resolve them.

</details>


### [99] [ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models](https://arxiv.org/abs/2508.12387)
*Yuanfeng Xu,Zehui Dai,Jian Liang,Jiapeng Guan,Guangrun Wang,Liang Lin,Xiaohui Lv*

Main category: cs.CL

TL;DR: Introducing ReaLM, a reinforcement learning framework that enhances small language models (SLMs) in reasoning, autonomy, and generalization.


<details>
  <summary>Details</summary>
Motivation: SLMs are cost-effective alternatives to LLMs but struggle with complex reasoning and consistency, so there's a need for frameworks to address these limitations without sacrificing reasoning capability, autonomy, or generalization.

Method: ReaLM uses Multi-Route Process Verification (MRPV) to analyze positive and negative reasoning paths, Enabling Autonomy via Asymptotic Induction (EAAI) to reduce reliance on external inputs, and guided chain-of-thought distillation for embedding domain-specific knowledge.

Result: Experiments on reasoning tasks showed significant performance improvements in SLMs, enhancing their reasoning, autonomy, and generalization without reliance on external signals.

Conclusion: ReaLM provides an effective solution for making SLMs capable of robust and self-sufficient reasoning in specialized domains, advancing their practical applicability while addressing key limitations.

Abstract: Small Language Models (SLMs) are a cost-effective alternative to Large
Language Models (LLMs), but often struggle with complex reasoning due to their
limited capacity and a tendency to produce mistakes or inconsistent answers
during multi-step reasoning. Existing efforts have improved SLM performance,
but typically at the cost of one or more of three key aspects: (1) reasoning
capability, due to biased supervision that filters out negative reasoning paths
and limits learning from errors; (2) autonomy, due to over-reliance on
externally generated reasoning signals; and (3) generalization, which suffers
when models overfit to teacher-specific patterns. In this paper, we introduce
ReaLM, a reinforcement learning framework for robust and self-sufficient
reasoning in vertical domains. To enhance reasoning capability, we propose
Multi-Route Process Verification (MRPV), which contrasts both positive and
negative reasoning paths to extract decisive patterns. To reduce reliance on
external guidance and improve autonomy, we introduce Enabling Autonomy via
Asymptotic Induction (EAAI), a training strategy that gradually fades external
signals. To improve generalization, we apply guided chain-of-thought
distillation to encode domain-specific rules and expert knowledge into SLM
parameters, making them part of what the model has learned. Extensive
experiments on both vertical and general reasoning tasks demonstrate that ReaLM
significantly improves SLM performance across aspects (1)-(3) above.

</details>


### [100] [MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph](https://arxiv.org/abs/2508.12393)
*Duzhen Zhang,Zixiao Wang,Zhong-Zhi Li,Yahan Yu,Shuncheng Jia,Jiahua Dong,Haotian Xu,Xing Wu,Yingying Zhang,Tielin Zhang,Jie Yang,Xiuying Chen,Le Song*

Main category: cs.CL

TL;DR: The paper introduces MedKGent, an LLM agent-based framework for constructing temporally evolving medical knowledge graphs (KGs) from PubMed abstracts, achieving high accuracy and utility in medical question answering and drug repurposing.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of medical literature poses challenges for organizing and integrating domain knowledge, necessitating effective tools for retrieval, reasoning, and discovery. Existing KG construction methods have limitations in scalability, generalizability, and handling temporal dynamics.

Method: The proposed MedKGent framework leverages PubMed abstracts (1975-2023) to construct medical KGs incrementally via a daily time series. It utilizes two specialized Qwen2.5-32B-Instruct-powered agents—Extractor (for identifying knowledge triples with confidence scoring) and Constructor (for integrating triples into the KG with temporal and confidence-based resolution).

Result: MedKGent generates a medical KG with 156,275 entities and 2,971,384 relational triples, achieving nearly 90% accuracy as assessed by domain experts and state-of-the-art LLMs. It significantly enhances performance in medical question answering and demonstrates its utility in drug repurposing scenarios.

Conclusion: MedKGent effectively addresses the limitations of static biomedical corpora by dynamically constructing accurate and temporally evolving knowledge graphs, showcasing utility in downstream tasks and advancing methods for biomedical knowledge organization.

Abstract: The rapid expansion of medical literature presents growing challenges for
structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)
offer a promising solution by enabling efficient retrieval, automated
reasoning, and knowledge discovery. However, current KG construction methods
often rely on supervised pipelines with limited generalizability or naively
aggregate outputs from Large Language Models (LLMs), treating biomedical
corpora as static and ignoring the temporal dynamics and contextual uncertainty
of evolving knowledge. To address these limitations, we introduce MedKGent, a
LLM agent framework for constructing temporally evolving medical KGs.
Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we
simulate the emergence of biomedical knowledge via a fine-grained daily time
series. MedKGent incrementally builds the KG in a day-by-day manner using two
specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor
Agent identifies knowledge triples and assigns confidence scores via
sampling-based estimation, which are used to filter low-confidence extractions
and inform downstream processing. The Constructor Agent incrementally
integrates the retained triples into a temporally evolving graph, guided by
confidence scores and timestamps to reinforce recurring knowledge and resolve
conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational
triples. Quality assessments by two SOTA LLMs and three domain experts
demonstrate an accuracy approaching 90\%, with strong inter-rater agreement. To
evaluate downstream utility, we conduct RAG across seven medical question
answering benchmarks using five leading LLMs, consistently observing
significant improvements over non-augmented baselines. Case studies further
demonstrate the KG's value in literature-based drug repurposing via
confidence-aware causal inference.

</details>


### [101] [Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing](https://arxiv.org/abs/2508.12405)
*Zilong Bai,Zihan Xu,Cong Sun,Chengxi Zang,H. Timothy Bunnell,Catherine Sinfield,Jacqueline Rutter,Aaron Thomas Martinez,L. Charles Bailey,Mark Weiner,Thomas R. Campion,Thomas Carton,Christopher B. Forrest,Rainu Kaushal,Fei Wang,Yifan Peng*

Main category: cs.CL

TL;DR: The paper presents a hybrid natural language processing (NLP) model designed to identify and verify Post-Acute Sequelae of COVID-19 (PASC) symptoms from clinical notes, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: PASC diagnosis is challenging due to its diverse and evolving symptoms over varying time intervals, necessitating effective tools for accurate identification.

Method: The paper introduces a combined rule-based named entity recognition and BERT-based assertion detection approach. Using a lexicon developed with clinical specialists, the pipeline was rigorously tested across both internal and external datasets from 11 health systems.

Result: The model achieved an average F1 score of 0.82 in internal validation and 0.76 in external validation. Processing speed for each clinical note averaged 2.448 seconds. Spearman correlation results indicated robust performance for identifying both positive and negative mentions.

Conclusion: The model demonstrates strong potential for enhancing PASC diagnosis, proving to be both reliable and efficient in clinical settings.

Abstract: Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC)
remains challenging due to its myriad symptoms that evolve over long- and
variable-time intervals. To address this issue, we developed a hybrid natural
language processing pipeline that integrates rule-based named entity
recognition with BERT-based assertion detection modules for PASC-symptom
extraction and assertion detection from clinical notes. We developed a
comprehensive PASC lexicon with clinical specialists. From 11 health systems of
the RECOVER initiative network across the U.S., we curated 160 intake progress
notes for model development and evaluation, and collected 47,654 progress notes
for a population-level prevalence study. We achieved an average F1 score of
0.82 in one-site internal validation and 0.76 in 10-site external validation
for assertion detection. Our pipeline processed each note at $2.448\pm 0.812$
seconds on average. Spearman correlation tests showed $\rho >0.83$ for positive
mentions and $\rho >0.72$ for negative ones, both with $P <0.0001$. These
demonstrate the effectiveness and efficiency of our models and their potential
for improving PASC diagnosis.

</details>


### [102] [ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads](https://arxiv.org/abs/2508.12407)
*Zhuorui Liu,Chen Zhang,Dawei Song*

Main category: cs.CL

TL;DR: The paper introduces a method called ZigzagAttention to optimize memory usage in large language models by organizing attention heads to minimize performance degradation and latency.


<details>
  <summary>Details</summary>
Motivation: Address difficulties in deploying large language models with long-context capabilities due to increased memory consumption of KV cache.

Method: Develop a criterion that organizes retrieval and streaming heads exclusively in separate layers to reduce latency without significant performance loss.

Result: ZigzagAttention method demonstrated reduced latency and comparable performance to existing baselines.

Conclusion: The approach is effective in optimizing KV cache usage and reducing associated overhead, making it competitive among existing solutions.

Abstract: With the rapid development of large language models (LLMs), handling long
context has become one of the vital abilities in LLMs. Such long-context
ability is accompanied by difficulties in deployment, especially due to the
increased consumption of KV cache. There is certain work aiming to optimize the
memory footprint of KV cache, inspired by the observation that attention heads
can be categorized into retrieval heads that are of great significance and
streaming heads that are of less significance. Typically, identifying the
streaming heads and and waiving the KV cache in the streaming heads would
largely reduce the overhead without hurting the performance that much. However,
since employing both retrieval and streaming heads in one layer decomposes one
large round of attention computation into two small ones, it may unexpectedly
bring extra latency on accessing and indexing tensors. Based on this intuition,
we impose an important improvement to the identification process of retrieval
and streaming heads, in which we design a criterion that enforces exclusively
retrieval or streaming heads gathered in one unique layer. In this way, we
further eliminate the extra latency and only incur negligible performance
degradation. Our method named \textsc{ZigzagAttention} is competitive among
considered baselines owing to reduced latency and comparable performance.

</details>


### [103] [The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases](https://arxiv.org/abs/2508.12411)
*Emanuel Z. Fenech-Borg,Tilen P. Meznaric-Kos,Milica D. Lekovic-Bojovic,Arni J. Hentze-Djurhuus*

Main category: cs.CL

TL;DR: The paper investigates cultural biases in large language models (LLMs) by introducing the concept of "cultural genes," proposing a dataset, and finding evidence of diverging cultural tendencies between Western-centric and Eastern-centric models.


<details>
  <summary>Details</summary>
Motivation: To explore the cultural and ethical assumptions embedded in LLMs, particularly how their training datasets reflect distinct cultural orientations.

Method: Created a Cultural Probe Dataset (CPD) with 200 prompts targeting Individualism-Collectivism (IDV) and Power Distance (PDI) dimensions. Compared GPT-4 and ERNIE Bot using zero-shot prompts and human annotations, followed by a Cultural Alignment Index (CAI) analysis.

Result: The study found that GPT-4 aligns with individualistic and low-power-distance norms (e.g., USA), while ERNIE Bot aligns with collectivistic and high-power-distance norms (e.g., China). Statistically significant divergence between the models was observed.

Conclusion: LLMs reflect the cultural biases inherent in their training corpora, demonstrating that culturally aware evaluation and deployment are needed to counter algorithmic cultural hegemony.

Abstract: Large language models (LLMs) are deployed globally, yet their underlying
cultural and ethical assumptions remain underexplored. We propose the notion of
a "cultural gene" -- a systematic value orientation that LLMs inherit from
their training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200
prompts targeting two classic cross-cultural dimensions:
Individualism-Collectivism (IDV) and Power Distance (PDI). Using standardized
zero-shot prompts, we compare a Western-centric model (GPT-4) and an
Eastern-centric model (ERNIE Bot). Human annotation shows significant and
consistent divergence across both dimensions. GPT-4 exhibits individualistic
and low-power-distance tendencies (IDV score approx 1.21; PDI score approx
-1.05), while ERNIE Bot shows collectivistic and higher-power-distance
tendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically
significant (p < 0.001). We further compute a Cultural Alignment Index (CAI)
against Hofstede's national scores and find GPT-4 aligns more closely with the
USA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns
more closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative
analyses of dilemma resolution and authority-related judgments illustrate how
these orientations surface in reasoning. Our results support the view that LLMs
function as statistical mirrors of their cultural corpora and motivate
culturally aware evaluation and deployment to avoid algorithmic cultural
hegemony.

</details>


### [104] [Uncovering Emergent Physics Representations Learned In-Context by Large Language Models](https://arxiv.org/abs/2508.12448)
*Yeongwoo Song,Jaeyong Bae,Dong-Kyum Kim,Hawoong Jeong*

Main category: cs.CL

TL;DR: This paper explores how large language models develop in-context learning abilities, particularly in reasoning about physical systems, and identifies key principles like energy encoded within their internal mechanics.


<details>
  <summary>Details</summary>
Motivation: Understanding the mechanisms of in-context learning in LLMs is crucial, especially for complex tasks like those in physics, which provide structured and controllable real-world data to study emergent reasoning.

Method: The authors use physics-based dynamics forecasting as a testbed and analyze residual stream activations in LLMs using Sparse Autoencoders to identify correlations with physical variables.

Result: Longer input contexts enhance LLM performance in physics-related tasks, and Sparse Autoencoders reveal that key physical concepts such as energy are encoded in their internal representations.

Conclusion: Meaningful physical concepts emerge within LLMs during in-context learning, providing insights into their reasoning machinery and advancing our understanding of their capabilities.

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL)
abilities, enabling them to solve wide range of tasks via textual prompts
alone. As these capabilities advance, the range of applicable domains continues
to expand significantly. However, identifying the precise mechanisms or
internal structures within LLMs that allow successful ICL across diverse,
distinct classes of tasks remains elusive. Physics-based tasks offer a
promising testbed for probing this challenge. Unlike synthetic sequences such
as basic arithmetic or symbolic equations, physical systems provide
experimentally controllable, real-world data based on structured dynamics
grounded in fundamental principles. This makes them particularly suitable for
studying the emergent reasoning behaviors of LLMs in a realistic yet tractable
setting. Here, we mechanistically investigate the ICL ability of LLMs,
especially focusing on their ability to reason about physics. Using a dynamics
forecasting task in physical systems as a proxy, we evaluate whether LLMs can
learn physics in context. We first show that the performance of dynamics
forecasting in context improves with longer input contexts. To uncover how such
capability emerges in LLMs, we analyze the model's residual stream activations
using sparse autoencoders (SAEs). Our experiments reveal that the features
captured by SAEs correlate with key physical variables, such as energy. These
findings demonstrate that meaningful physical concepts are encoded within LLMs
during in-context learning. In sum, our work provides a novel case study that
broadens our understanding of how LLMs learn in context.

</details>


### [105] [M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following](https://arxiv.org/abs/2508.12458)
*Ruirui Gao,Emily Johnson,Bowen Tan,Yanfei Qian*

Main category: cs.CL

TL;DR: Multimodal-Model-Guided Preference Optimization (M3PO) is proposed to improve large vision-language models (LVLMs) efficiency in preference alignment and fine-tuning using informative sample pairs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of expensive and inconsistent human annotations and inefficiencies in leveraging hard negative samples for improving LVLMs.

Method: M3PO prioritizes learning-valuable sample pairs using Multimodal Alignment Score (MAS) and Self-Consistency metrics, culminating in M3P-Score for fine-tuning LVLMs such as LLaVA-1.5 using LoRA.

Result: M3PO consistently outperformed existing baselines like SFT, RLHF, vanilla DPO, across multimodal benchmarks such as MME-Bench, POPE, and Human Preference Scores.

Conclusion: M3PO enables efficient optimization for LVLMs, enhancing their ability for multimodal instruction following by leveraging high-quality preference pairs intelligently.

Abstract: Large Vision-Language Models (LVLMs) hold immense potential for complex
multimodal instruction following, yet their development is often hindered by
the high cost and inconsistency of human annotation required for effective
fine-tuning and preference alignment. Traditional supervised fine-tuning (SFT)
and existing preference optimization methods like RLHF and DPO frequently
struggle to efficiently leverage the model's own generation space to identify
highly informative "hard negative" samples. To address these challenges, we
propose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and
data-efficient method designed to enhance LVLMs' capabilities in visual
instruction following. M3PO intelligently selects the most "learning-valuable"
preference sample pairs from a diverse pool of LVLM-generated candidates. This
selection is driven by a sophisticated mechanism that integrates two crucial
signals: a Multimodal Alignment Score (MAS) to assess external quality and the
model's Self-Consistency / Confidence (log-probability) to gauge internal
belief. These are combined into a novel M3P-Score, which specifically
identifies preferred responses and challenging dispreferred responses that the
model might confidently generate despite being incorrect. These high-quality
preference pairs are then used for efficient Direct Preference Optimization
(DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our
extensive experiments demonstrate that M3PO consistently outperforms strong
baselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a
comprehensive suite of multimodal instruction following benchmarks (MME-Bench,
POPE, IFT, Human Pref. Score).

</details>


### [106] [LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages](https://arxiv.org/abs/2508.12459)
*Alham Fikri Aji,Trevor Cohn*

Main category: cs.CL

TL;DR: LoraxBench is a multilingual NLP benchmark designed for Indonesia's low-resource languages, spanning diverse tasks and revealing disparities in language model performance.


<details>
  <summary>Details</summary>
Motivation: Indonesia has 700 languages, and progress in NLP for these languages is lagging, necessitating a benchmark like LoraxBench to encourage research in low-resource linguistic settings.

Method: LoraxBench includes tasks like reading comprehension, QA, language inference, causal reasoning, translation, and cultural QA across 20 languages, analyzing performance of various models across multilingual and region-focused landscapes.

Result: The benchmark highlights challenges for models, reveals discrepancies in performance between Indonesian and other low-resource languages, and notes that formality registers significantly influence results.

Conclusion: LoraxBench underscores the need for enhanced NLP research for low-resource languages in Indonesia while indicating limitations in current models and approaches.

Abstract: As one of the world's most populous countries, with 700 languages spoken,
Indonesia is behind in terms of NLP progress. We introduce LoraxBench, a
benchmark that focuses on low-resource languages of Indonesia and covers 6
diverse tasks: reading comprehension, open-domain QA, language inference,
causal reasoning, translation, and cultural QA. Our dataset covers 20
languages, with the addition of two formality registers for three languages. We
evaluate a diverse set of multilingual and region-focused LLMs and found that
this benchmark is challenging. We note a visible discrepancy between
performance in Indonesian and other languages, especially the low-resource
ones. There is no clear lead when using a region-specific model as opposed to
the general multilingual model. Lastly, we show that a change in register
affects model performance, especially with registers not commonly found in
social media, such as high-level politeness `Krama' Javanese.

</details>


### [107] [Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models](https://arxiv.org/abs/2508.12461)
*Ziqian Bi,Keyu Chen,Chiung-Yi Tseng,Danyang Zhang,Tianyang Wang,Hongying Luo,Lu Chen,Junming Huang,Jibin Guan,Junfeng Hao,Junhao Song*

Main category: cs.CL

TL;DR: OpenAI released GPT-OSS models with 20B and 120B parameters, evaluated against other models on various benchmarks, highlighting unexpected performance trends.


<details>
  <summary>Details</summary>
Motivation: To explore the performance trade-offs of sparse architectures and improve large language model efficiency, focusing on open-source capabilities.

Method: Evaluation of GPT-OSS models against six other large language models across ten benchmarks, using statistical tests like McNemar's and effect size analysis under standardized conditions.

Result: GPT-OSS-20B outperformed GPT-OSS-120B on several benchmarks despite requiring less memory and energy. Both models showed strengths in code generation and weaknesses in multilingual tasks.

Conclusion: Scaling sparse architectures does not guarantee proportional performance improvements, prompting the need for better optimization strategies and considerations for energy-efficient model deployments.

Abstract: In August 2025, OpenAI released GPT-OSS models, its first open weight large
language models since GPT-2 in 2019, comprising two mixture of experts
architectures with 120B and 20B parameters. We evaluated both variants against
six contemporary open source large language models ranging from 14.7B to 235B
parameters, representing both dense and sparse designs, across ten benchmarks
covering general knowledge, mathematical reasoning, code generation,
multilingual understanding, and conversational ability. All models were tested
in unquantised form under standardised inference settings, with statistical
validation using McNemars test and effect size analysis. Results show that
gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such
as HumanEval and MMLU, despite requiring substantially less memory and energy
per response. Both models demonstrate mid-tier overall performance within the
current open source landscape, with relative strength in code generation and
notable weaknesses in multilingual tasks. These findings provide empirical
evidence that scaling in sparse architectures may not yield proportional
performance gains, underscoring the need for further investigation into
optimisation strategies and informing more efficient model selection for future
open source deployments.

</details>


### [108] [The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping](https://arxiv.org/abs/2508.12482)
*Xiaomeng Zhu,R. Thomas McCoy,Robert Frank*

Main category: cs.CL

TL;DR: The study investigates whether large language models, such as RoBERTa and GPT-2, use syntactic bootstrapping for verb learning similarly to humans by training them on datasets with syntactic information ablated. Results indicate syntax plays a critical role, especially for mental verbs.


<details>
  <summary>Details</summary>
Motivation: To examine if large language models (e.g., RoBERTa, GPT-2) exhibit syntactic bootstrapping, mimicking how children use syntax to learn verb meanings.

Method: Trained language models on datasets where syntactic or co-occurrence information was ablated, then analyzed the impact on verb and noun representation quality.

Result: When syntactic cues were removed, the quality of verb representation, especially for mental verbs, degraded more significantly than when co-occurrence information was removed. For nouns, co-occurrence distortion had a greater impact.

Conclusion: Large language models rely on syntactic information for verb learning, akin to human syntactic bootstrapping. Such models can be used as scalable tools for testing developmental hypotheses.

Abstract: Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use
the syntactic environments in which a verb occurs to learn its meaning. In this
paper, we examine whether large language models exhibit a similar behavior. We
do this by training RoBERTa and GPT-2 on perturbed datasets where syntactic
information is ablated. Our results show that models' verb representation
degrades more when syntactic cues are removed than when co-occurrence
information is removed. Furthermore, the representation of mental verbs, for
which syntactic bootstrapping has been shown to be particularly crucial in
human verb learning, is more negatively impacted in such training regimes than
physical verbs. In contrast, models' representation of nouns is affected more
when co-occurrences are distorted than when syntax is distorted. In addition to
reinforcing the important role of syntactic bootstrapping in verb learning, our
results demonstrated the viability of testing developmental hypotheses on a
larger scale through manipulating the learning environments of large language
models.

</details>


### [109] [Mitigating Hallucinations in Large Language Models via Causal Reasoning](https://arxiv.org/abs/2508.12495)
*Yuangang Li,Yiqing Shen,Yi Nian,Jiechao Gao,Ziyi Wang,Chenxiao Yu,Shawn Li,Jie Wang,Xiyang Hu,Yue Zhao*

Main category: cs.CL

TL;DR: The paper tackles logically inconsistent hallucinations in large language models (LLMs) by introducing a supervised fine-tuning framework (CDCR-SFT) for constructing causal-directed acyclic graphs (DAGs) and reasoning over them. Results highlight significant improvements in causal reasoning accuracy and reduced hallucinations.


<details>
  <summary>Details</summary>
Motivation: Address logical inconsistencies and hallucinations in LLMs, which occur due to a lack of causal structure modeling in current reasoning approaches like Chain-of-Thought and its variants.

Method: Developed a supervised fine-tuning framework, CDCR-SFT, to train LLMs for constructing variable-level causal DAGs and performing reasoning over them, supported by the creation of a new dataset (CausalDR) with over 25,000 samples.

Result: CDCR-SFT achieved state-of-the-art causal reasoning performance (95.33% accuracy on CLADDER, surpassing human performance) and reduced hallucination by 10% in HaluEval.

Conclusion: Explicit causal structure modeling significantly enhances causal reasoning and reduces hallucinations in LLMs, indicating that such approaches can improve logical consistency in their outputs.

Abstract: Large language models (LLMs) exhibit logically inconsistent hallucinations
that appear coherent yet violate reasoning principles, with recent research
suggesting an inverse relationship between causal reasoning capabilities and
such hallucinations. However, existing reasoning approaches in LLMs, such as
Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic
token level rather than modeling the underlying causal relationships between
variables, lacking the ability to represent conditional independencies or
satisfy causal identification assumptions. To bridge this gap, we introduce
causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning
framework that trains LLMs to explicitly construct variable-level directed
acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a
dataset comprising 25,368 samples (CausalDR), where each sample includes an
input question, explicit causal DAG, graph-based reasoning trace, and validated
answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves
the causal reasoning capability with the state-of-the-art 95.33% accuracy on
CLADDER (surpassing human performance of 94.8% for the first time) and reduces
the hallucination on HaluEval with 10% improvements. It demonstrates that
explicit causal structure modeling in LLMs can effectively mitigate logical
inconsistencies in LLM outputs. Code is available at
https://github.com/MrLYG/CDCR-SFT.

</details>


### [110] [CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2508.12535)
*Seonglae Cho,Zekun Wu,Adriano Koshiyama*

Main category: cs.CL

TL;DR: This paper presents CorrSteer, a method for steering Sparse Autoencoders (SAEs) in Large Language Models (LLMs) to improve task performance using correlation-based feature selection at inference time.


<details>
  <summary>Details</summary>
Motivation: Sparse Autoencoders (SAEs) can extract features from LLMs but are limited in downstream tasks due to reliance on contrastive datasets and storage requirements.

Method: CorrSteer selects relevant features by correlating sample correctness with SAE activations during inference time and automates steering coefficients extraction using average activations.

Result: CorrSteer exhibits notable improvements in benchmarks such as QA, bias mitigation, reasoning, etc., achieving +4.1% in MMLU performance and +22.9% in HarmBench with only 4000 samples.

Conclusion: CorrSteer is an effective and scalable approach for automating SAE steering, showing semantically meaningful feature patterns that align well with specific language model tasks.

Abstract: Sparse Autoencoders (SAEs) can extract interpretable features from large
language models (LLMs) without supervision. However, their effectiveness in
downstream steering tasks is limited by the requirement for contrastive
datasets or large activation storage. To address these limitations, we propose
CorrSteer, which selects features by correlating sample correctness with SAE
activations from generated tokens at inference time. This approach uses only
inference-time activations to extract more relevant features, thereby avoiding
spurious correlations. It also obtains steering coefficients from average
activations, automating the entire pipeline. Our method shows improved task
performance on QA, bias mitigation, jailbreaking prevention, and reasoning
benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%
improvement in MMLU performance and a +22.9% improvement in HarmBench with only
4000 samples. Selected features demonstrate semantically meaningful patterns
aligned with each task's requirements, revealing the underlying capabilities
that drive performance. Our work establishes correlationbased selection as an
effective and scalable approach for automated SAE steering across language
model applications.

</details>


### [111] [Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning](https://arxiv.org/abs/2508.12591)
*Yu-Hsuan Fang,Tien-Hong Lo,Yao-Ting Sung,Berlin Chen*

Main category: cs.CL

TL;DR: This paper explores Multimodal Large Language Models (MLLM) for comprehensive Automated Speaking Assessment (ASA), focusing on both audio and text inputs. It presents a specialized Speech-First Multimodal Training (SFMT) method, significantly improving performance in delivery aspects.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in traditional ASA systems that either rely solely on text or audio, aiming for a unified and robust approach that combines both modalities using MLLM.

Method: The paper proposes Speech-First Multimodal Training (SFMT), a curriculum learning approach emphasizing speech modeling before integrating cross-modal fusion.

Result: MLLM systems improved overall ASA performance, boosting PCC values from 0.783 to 0.846, with SFMT achieving a 4% absolute accuracy improvement in delivery aspect evaluations.

Conclusion: MLLM and SFMT mark a significant advancement in ASA, particularly enhancing evaluation of delivery aspects. Future work could refine specialized training for handling delivery challenges.

Abstract: Traditional Automated Speaking Assessment (ASA) systems exhibit inherent
modality limitations: text-based approaches lack acoustic information while
audio-based methods miss semantic context. Multimodal Large Language Models
(MLLM) offer unprecedented opportunities for comprehensive ASA by
simultaneously processing audio and text within unified frameworks. This paper
presents a very first systematic study of MLLM for comprehensive ASA,
demonstrating the superior performance of MLLM across the aspects of content
and language use . However, assessment on the delivery aspect reveals unique
challenges, which is deemed to require specialized training strategies. We thus
propose Speech-First Multimodal Training (SFMT), leveraging a curriculum
learning principle to establish more robust modeling foundations of speech
before cross-modal synergetic fusion. A series of experiments on a benchmark
dataset show MLLM-based systems can elevate the holistic assessment performance
from a PCC value of 0.783 to 0.846. In particular, SFMT excels in the
evaluation of the delivery aspect, achieving an absolute accuracy improvement
of 4% over conventional training approaches, which also paves a new avenue for
ASA.

</details>


### [112] [Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context](https://arxiv.org/abs/2508.12630)
*Maitreyi Chatterjee,Devansh Agarwal*

Main category: cs.CL

TL;DR: The paper introduces Semantic Anchoring, a memory approach for large language models that integrates linguistic features to enhance recall and coherence in long-term dialogues.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) struggle with memory retention in extended interactions, with conventional methods failing to capture nuanced linguistic structures.

Method: The authors propose a Semantic Anchoring method, incorporating dependency parsing, discourse relation tagging, and coreference resolution to augment standard vector-based storage.

Result: Semantic Anchoring demonstrates improvements of up to 18% in factual recall and discourse coherence in long-term dialogue datasets compared to traditional baselines.

Conclusion: Enriching vector-based memory with explicit linguistic features enhances the effectiveness of LLMs in complex, long-term dialogues, making them more robust and interpretable.

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and task
competence in conversational settings. However, their effectiveness in
multi-session and long-term interactions is hindered by limited memory
persistence. Typical retrieval-augmented generation (RAG) systems store
dialogue history as dense vectors, which capture semantic similarity but
neglect finer linguistic structures such as syntactic dependencies, discourse
relations, and coreference links. We propose Semantic Anchoring, a hybrid
agentic memory architecture that enriches vector-based storage with explicit
linguistic cues to improve recall of nuanced, context-rich exchanges. Our
approach combines dependency parsing, discourse relation tagging, and
coreference resolution to create structured memory entries. Experiments on
adapted long-term dialogue datasets show that semantic anchoring improves
factual recall and discourse coherence by up to 18% over strong RAG baselines.
We further conduct ablation studies, human evaluations, and error analysis to
assess robustness and interpretability.

</details>


### [113] [Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing](https://arxiv.org/abs/2508.12631)
*Yiqun Zhang,Hao Li,Jianhao Chen,Hangfan Zhang,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: Avengers-Pro is a model ensemble framework with test-time routing that optimizes performance-efficiency tradeoffs across various large language models, outperforming single models.


<details>
  <summary>Details</summary>
Motivation: Balancing performance and efficiency in large language models (LLMs) is a challenge; the paper aims to address this issue by proposing a unified framework.

Method: Avengers-Pro uses test-time routing, embedding and clustering incoming queries, and dynamically assigning them to the most suitable model based on performance-efficiency scores.

Result: Achieves state-of-the-art results across 6 benchmarks and 8 leading models, surpasses the strongest single model by +7% accuracy, matches single model accuracy at 27% lower cost, and offers ~90% performance at 63% lower cost.

Conclusion: Avengers-Pro defined a Pareto frontier, consistently providing optimal trade-offs in accuracy and cost compared to single models, proving effective for performance and efficiency across benchmarks.

Abstract: Balancing performance and efficiency is a central challenge in large language
model (LLM) advancement. GPT-5 addresses this with test-time routing,
dynamically assigning queries to either an efficient or a high-capacity model
during inference. In this work, we present Avengers-Pro, a test-time routing
framework that ensembles LLMs of varying capacities and efficiencies, providing
a unified solution for all performance-efficiency tradeoffs. The Avengers-Pro
embeds and clusters incoming queries, then routes each to the most suitable
model based on a performance-efficiency score. Across 6 challenging benchmarks
and 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and
Claude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a
performance-efficiency trade-off parameter, it can surpass the strongest single
model (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the
average accuracy of the strongest single model at 27% lower cost, and reach
~90% of that performance at 63% lower cost. Last but not least, it achieves a
Pareto frontier, consistently yielding the highest accuracy for any given cost,
and the lowest cost for any given accuracy, among all single models. Code is
available at https://github.com/ZhangYiqun018/AvengersPro.

</details>


### [114] [Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection](https://arxiv.org/abs/2508.12632)
*Chi Wang,Min Gao,Zongwei Wang,Junwei Yin,Kai Shu,Chenghua Lin*

Main category: cs.CL

TL;DR: The paper addresses the societal threat of fake news generated by large language models (LLMs) and proposes a novel method (LIFE) to detect it by analyzing linguistic fingerprints.


<details>
  <summary>Details</summary>
Motivation: The widespread and effortless generation of fake news due to the rise of LLMs represents a growing societal threat, necessitating robust detection mechanisms.

Method: The authors utilize distributional divergence analysis to uncover linguistic fingerprints in LLM-generated content, proposing the LIFE method to identify them. LIFE reconstructs word-level probability distributions and employs key-fragment techniques to amplify subtle linguistic differences, enhancing detection reliability.

Result: LIFE demonstrates state-of-the-art performance in detecting fake news generated by LLMs and remains effective in identifying human-written fake news.

Conclusion: The LIFE methodology provides a reliable and advanced solution to address the challenges of detecting LLM-generated fake news, contributing to safeguarding against misinformation.

Abstract: With the rapid development of large language models, the generation of fake
news has become increasingly effortless, posing a growing societal threat and
underscoring the urgent need for reliable detection methods. Early efforts to
identify LLM-generated fake news have predominantly focused on the textual
content itself; however, because much of that content may appear coherent and
factually consistent, the subtle traces of falsification are often difficult to
uncover. Through distributional divergence analysis, we uncover prompt-induced
linguistic fingerprints: statistically distinct probability shifts between
LLM-generated real and fake news when maliciously prompted. Based on this
insight, we propose a novel method named Linguistic Fingerprints Extraction
(LIFE). By reconstructing word-level probability distributions, LIFE can find
discriminative patterns that facilitate the detection of LLM-generated fake
news. To further amplify these fingerprint patterns, we also leverage
key-fragment techniques that accentuate subtle linguistic differences, thereby
improving detection reliability. Our experiments show that LIFE achieves
state-of-the-art performance in LLM-generated fake news and maintains high
performance in human-written fake news. The code and data are available at
https://anonymous.4open.science/r/LIFE-E86A.

</details>


### [115] [Breaking Language Barriers: Equitable Performance in Multilingual Language Models](https://arxiv.org/abs/2508.12662)
*Tanay Nagar,Grigorii Khvatskii,Anna Sokol,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: Cutting-edge large language models (LLMs) show weaker performance in common sense reasoning (CSR) tasks for low-resource languages (LRLs) compared to high-resource languages (HRLs). This paper introduces fine-tuning LLMs with synthetic code-switched datasets, significantly boosting performance in LRLs while maintaining or improving HRL performance.


<details>
  <summary>Details</summary>
Motivation: The study addresses the fairness issue in LLM performance disparities between low-resource languages (LRLs) and high-resource languages (HRLs), aiming to equalize access to high-quality outputs for linguistic diversity.

Method: The method involves fine-tuning LLMs on synthetic code-switched text created using controlled language-mixing techniques, including a newly developed dataset derived from CommonSenseQA with distinct language ratio configurations.

Result: The proposed fine-tuning approach leads to significant performance improvements in LRLs and preserves or enhances accuracy in HRLs.

Conclusion: Fine-tuning LLMs with synthetic code-switched datasets effectively addresses the performance gap, promoting linguistic inclusivity and equitable language model outputs.

Abstract: Cutting-edge LLMs have emerged as powerful tools for multilingual
communication and understanding. However, LLMs perform worse in Common Sense
Reasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi
or Swahili compared to high-resource languages (HRLs) like English. Equalizing
this inconsistent access to quality LLM outputs is crucial to ensure fairness
for speakers of LRLs and across diverse linguistic communities. In this paper,
we propose an approach to bridge this gap in LLM performance. Our approach
involves fine-tuning an LLM on synthetic code-switched text generated using
controlled language-mixing methods. We empirically demonstrate that fine-tuning
LLMs on synthetic code-switched datasets leads to substantial improvements in
LRL model performance while preserving or enhancing performance in HRLs.
Additionally, we present a new dataset of synthetic code-switched text derived
from the CommonSenseQA dataset, featuring three distinct language ratio
configurations.

</details>


### [116] [Leveraging Large Language Models for Predictive Analysis of Human Misery](https://arxiv.org/abs/2508.12669)
*Bishanka Seal,Rahul Seetharaman,Aman Bansal,Abhilash Nandy*

Main category: cs.CL

TL;DR: The study explores the use of Large Language Models (LLMs) to predict human misery scores from text, experimenting with various prompting techniques and a gamified evaluation framework called the "Misery Game Show."


<details>
  <summary>Details</summary>
Motivation: Understanding and quantifying human-perceived misery from text using AI has potential applications in emotional reasoning and affective computing.

Method: The study uses LLMs for predicting misery scores between 0 to 100 given natural language descriptions. It evaluates zero-shot, few-shot, and retrieval-based prompting, alongside introducing a gamified framework ("Misery Game Show") for dynamic assessment.

Result: Few-shot prompting strategies outperform zero-shot techniques. The gamified evaluation framework tests predictive accuracy and adaptability to feedback in multiple scenarios.

Conclusion: The research showcases the potential of LLMs not only for accurate misery score prediction but also for adaptive emotional reasoning, emphasizing the importance of contextual examples and innovative evaluation setups.

Abstract: This study investigates the use of Large Language Models (LLMs) for
predicting human-perceived misery scores from natural language descriptions of
real-world scenarios. The task is framed as a regression problem, where the
model assigns a scalar value from 0 to 100 to each input statement. We evaluate
multiple prompting strategies, including zero-shot, fixed-context few-shot, and
retrieval-based prompting using BERT sentence embeddings. Few-shot approaches
consistently outperform zero-shot baselines, underscoring the value of
contextual examples in affective prediction. To move beyond static evaluation,
we introduce the "Misery Game Show", a novel gamified framework inspired by a
television format. It tests LLMs through structured rounds involving ordinal
comparison, binary classification, scalar estimation, and feedback-driven
reasoning. This setup enables us to assess not only predictive accuracy but
also the model's ability to adapt based on corrective feedback. The gamified
evaluation highlights the broader potential of LLMs in dynamic emotional
reasoning tasks beyond standard regression. Code and data link:
https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub

</details>


### [117] [ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction](https://arxiv.org/abs/2508.12685)
*Xingshan Zeng,Weiwen Liu,Lingzhi Wang,Liangyou Li,Fei Mi,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu*

Main category: cs.CL

TL;DR: ToolACE-MT is a Non-Autoregressive Iterative Generation framework designed to efficiently generate multi-turn agentic dialogues for LLM tasks, improving upon traditional autoregressive methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for generating data for agentic task-solving using LLMs rely on costly autoregressive interactions, limiting their scalability and real-world applicability.

Method: ToolACE-MT operates in three stages: coarse-grained initialization to create a dialogue skeleton, iterative refinement using mask-and-fill operations to enhance complexity, and offline verification using rules and models for correctness.

Result: ToolACE-MT successfully constructs high-quality, efficient, and generalizable agentic dialogues in multi-turn scenarios, facilitating enhanced data generation.

Conclusion: The framework introduces a new paradigm for tool-augmented data generation that bypasses the limitations of autoregressive methods, making interactions more efficient and realistic.

Abstract: Agentic task-solving with Large Language Models (LLMs) requires multi-turn,
multi-step interactions, often involving complex function calls and dynamic
user-agent exchanges. Existing simulation-based data generation methods for
such scenarios rely heavily on costly autoregressive interactions between
multiple LLM agents, thereby limiting real-world performance of agentic tasks.
In this paper, we propose a novel Non-Autoregressive Iterative Generation
framework, called ToolACE-MT, for constructing high-quality multi-turn agentic
dialogues. ToolACE-MT generates full conversational trajectories through three
stages: coarse-grained initialization, iterative refinement, and offline
verification. The initialization phase builds a structurally complete yet
semantically coarse dialogue skeleton; the iterative refinement phase
introduces realistic complexities and continued refinement via mask-and-fill
operations; and the offline verification phase ensures correctness and
coherence via rule- and model-based checks. Experiments demonstrate that
ToolACE-MT enables efficient, effective and generalizable agentic data
generation, offering a new paradigm for high-quality data construction in
tool-augmented LLM scenarios.

</details>


### [118] [DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning](https://arxiv.org/abs/2508.12726)
*Weize Liu,Yongchi Zhao,Yijia Luo,Mingyu Xu,Jiaheng Liu,Yanan Li,Xiguo Hu,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: The paper introduces DESIGNER, a pipeline for synthesizing challenging multidisciplinary reasoning datasets using raw text corpora, resulting in two extensive datasets: DLR-Book and DLR-Web.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing reasoning datasets, which lack either disciplinary breadth or structural depth to support robust multi-step reasoning.

Method: The DESIGNER pipeline employs LLMs to reverse-engineer design logics from existing questions and matches them with source materials from books and web corpora to generate diverse and challenging reasoning questions.

Result: Two datasets, DLR-Book and DLR-Web, spanning 75 disciplines with over 4.7 million challenging questions were created. Experimental validation shows these datasets lead to significant improvements in reasoning performance for Qwen models.

Conclusion: The proposed pipeline and datasets greatly enhance the quality and performance of multidisciplinary reasoning tasks, surpassing current datasets and improving model capabilities.

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language tasks but still struggle with complex, multi-step reasoning,
particularly across diverse disciplines. Existing reasoning datasets often
either lack disciplinary breadth or the structural depth necessary to elicit
robust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd
Reasoning data synthesis pipeline that leverages naturally available, extensive
raw documents (book corpus and web corpus) to generate multidisciplinary
challenging questions. A core innovation of our approach is the introduction of
a Design Logic concept, which mimics the question-creation process of human
educators. We use LLMs to reverse-engineer and abstract over 120,000 design
logics from existing questions across various disciplines. By matching these
design logics with disciplinary source materials, we are able to create
reasoning questions that far surpass the difficulty and diversity of existing
datasets. Based on this pipeline, we synthesized two large-scale reasoning
datasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book),
containing 3.04 million challenging questions synthesized from the book corpus,
and Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging
questions from the web corpus. Our data analysis demonstrates that the
questions synthesized by our method exhibit substantially greater difficulty
and diversity than those in the baseline datasets. We validate the
effectiveness of these datasets by conducting SFT experiments on the
Qwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset
significantly outperforms existing multidisciplinary datasets of the same
volume. Training with the full datasets further enables the models to surpass
the multidisciplinary reasoning performance of the official Qwen3-8B and
Qwen3-4B models.

</details>


### [119] [LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models](https://arxiv.org/abs/2508.12733)
*Zhiyuan Ning,Tianle Gu,Jiaxin Song,Shixin Hong,Lingyu Li,Huacan Liu,Jie Li,Yixu Wang,Meng Lingyu,Yan Teng,Yingchun Wang*

Main category: cs.CL

TL;DR: The paper presents LinguaSafe, a new multilingual safety benchmark with 45k entries in 12 languages, aimed at evaluating and ensuring the safety of large language models (LLMs) across diverse linguistic contexts.


<details>
  <summary>Details</summary>
Motivation: Existing multilingual safety evaluations for LLMs lack comprehensive datasets and linguistic diversity, hindering progress in robust multilingual safety alignment.

Method: The authors curated the LinguaSafe dataset using translated, transcreated, and natively-sourced data, and developed a fine-grained evaluation framework, including direct and indirect safety assessments and evaluations for oversensitivity.

Result: The safety and helpfulness of LLMs showed significant variation across languages and domains, even among languages with similar resource levels, highlighting the necessity of thorough multilingual evaluation.

Conclusion: LinguaSafe addresses critical gaps in multilingual safety assessments for LLMs and provides public access to its dataset and code, enabling further advancements in this field.

Abstract: The widespread adoption and increasing prominence of large language models
(LLMs) in global technologies necessitate a rigorous focus on ensuring their
safety across a diverse range of linguistic and cultural contexts. The lack of
a comprehensive evaluation and diverse data in existing multilingual safety
evaluations for LLMs limits their effectiveness, hindering the development of
robust multilingual safety alignment. To address this critical gap, we
introduce LinguaSafe, a comprehensive multilingual safety benchmark crafted
with meticulous attention to linguistic authenticity. The LinguaSafe dataset
comprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated
using a combination of translated, transcreated, and natively-sourced data, our
dataset addresses the critical need for multilingual safety evaluations of
LLMs, filling the void in the safety evaluation of LLMs across diverse
under-represented languages from Hungarian to Malay. LinguaSafe presents a
multidimensional and fine-grained evaluation framework, with direct and
indirect safety assessments, including further evaluations for oversensitivity.
The results of safety and helpfulness evaluations vary significantly across
different domains and different languages, even in languages with similar
resource levels. Our benchmark provides a comprehensive suite of metrics for
in-depth safety evaluation, underscoring the critical importance of thoroughly
assessing multilingual safety in LLMs to achieve more balanced safety
alignment. Our dataset and code are released to the public to facilitate
further research in the field of multilingual LLM safety.

</details>


### [120] [CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description](https://arxiv.org/abs/2508.12769)
*Shaoming Duan,Zirui Wang,Chuanyi Liu,Zhibin Zhu,Yuhao Zhang,Peiyi Han,Liang Yan,Zewu Penge*

Main category: cs.CL

TL;DR: CRED-SQL is a framework enhancing Text-to-SQL systems for large-scale databases by integrating cluster-based schema retrieval and an intermediate Execution Description Language (EDL).


<details>
  <summary>Details</summary>
Motivation: Addressing semantic mismatch and schema linking issues in Text-to-SQL systems, especially in large-scale databases with semantically similar attributes, which hinder accurate SQL generation.

Method: The method involves a two-step process: 1) Cluster-based schema retrieval to identify relevant tables and columns for a given query, 2) Introducing an Execution Description Language (EDL) as an intermediary to transform the task into two stages, Text-to-EDL and EDL-to-SQL, leveraging the reasoning strengths of LLMs.

Result: CRED-SQL achieves new state-of-the-art performance on two large-scale benchmarks, SpiderUnion and BirdUnion, demonstrating its accuracy and scalability.

Conclusion: The proposed CRED-SQL framework effectively addresses semantic mismatch challenges in Text-to-SQL systems for large databases, offering a scalable and accurate solution.

Abstract: Recent advances in large language models (LLMs) have significantly improved
the accuracy of Text-to-SQL systems. However, a critical challenge remains: the
semantic mismatch between natural language questions (NLQs) and their
corresponding SQL queries. This issue is exacerbated in large-scale databases,
where semantically similar attributes hinder schema linking and semantic drift
during SQL generation, ultimately reducing model accuracy. To address these
challenges, we introduce CRED-SQL, a framework designed for large-scale
databases that integrates Cluster Retrieval and Execution Description. CRED-SQL
first performs cluster-based large-scale schema retrieval to pinpoint the
tables and columns most relevant to a given NLQ, alleviating schema mismatch.
It then introduces an intermediate natural language representation-Execution
Description Language (EDL)-to bridge the gap between NLQs and SQL. This
reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,
leveraging LLMs' strong general reasoning capabilities while reducing semantic
deviation. Extensive experiments on two large-scale, cross-domain
benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new
state-of-the-art (SOTA) performance, validating its effectiveness and
scalability. Our code is available at https://github.com/smduan/CRED-SQL.git

</details>


### [121] [From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task](https://arxiv.org/abs/2508.12774)
*Javier Garcia Gilabert,Xixian Liao,Severino Da Dalt,Ella Bohman,Audrey Mash,Francesca De Luca Fornaciari,Irene Baucells,Joan Llop,Miguel Claramunt Argote,Carlos Escolano,Maite Melero*

Main category: cs.CL

TL;DR: The SALAMANDRATA model family enhances translation accuracy across 38 European languages, with publicly available 2B and 7B variants, designed through a two-step training process and advanced decoding methods.


<details>
  <summary>Details</summary>
Motivation: Improve upon SALAMANDRA LLMs and optimize machine translation for European languages while expanding support for additional languages in specific tasks.

Method: Utilized a two-phase training approach involving continual pre-training on parallel data and supervised fine-tuning on quality instructions, alongside vocabulary adaptation and advanced decoding strategies (Minimum Bayes Risk Decoding and Tuned Re-ranking).

Result: Successfully enhanced translation performance across multiple languages, with models adapted for diverse language needs; participated in the WMT25 shared task with optimized results.

Conclusion: The SALAMANDRATA models provide significant advancements in multilingual translation tasks and are publicly available for broad utilization.

Abstract: In this paper, we present the SALAMANDRATA family of models, an improved
iteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically
trained to achieve strong performance in translation-related tasks for 38
European languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For
both versions, we applied the same training recipe with a first step of
continual pre-training on parallel data, and a second step of supervised
fine-tuning on high-quality instructions. The BSC submission to the WMT25
General Machine Translation shared task is based on the 7B variant of
SALAMANDRATA. We first adapted the model vocabulary to support the additional
non-European languages included in the task. This was followed by a second
phase of continual pre-training and supervised fine-tuning, carefully designed
to optimize performance across all translation directions for this year's
shared task. For decoding, we employed two quality-aware strategies: Minimum
Bayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI
respectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,
along with the newer SALAMANDRATA-V2 model, on Hugging Face1

</details>


### [122] [HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks](https://arxiv.org/abs/2508.12778)
*Zhe Chen,Yusheng Liao,Shuyang Jiang,Zhiyuan Zhu,Haolin Li,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: This paper introduces HeteroRAG and MedAtlas to address inaccuracies in Medical Large Vision-Language Models by retrieving heterogeneous multimodal knowledge for clinical applications.


<details>
  <summary>Details</summary>
Motivation: Current Med-LVLMs often provide inaccurate and unreliable outputs, which could be dangerous in real-world diagnostic settings. Existing retrieval-augmented systems struggle with retrieving relevant and sufficient information from diverse sources, limiting their effectiveness for clinical decision-making.

Method: The authors propose MedAtlas, a repository that combines multimodal reports and diverse text corpora. They develop HeteroRAG, a framework using Modality-specific CLIPs for efficient retrieval and a Multi-corpora Query Generator for constructing queries. They also apply Heterogeneous Knowledge Preference Tuning to achieve knowledge alignment across modalities and sources.

Result: Extensive evaluations on 12 datasets and 3 modalities show that HeteroRAG outperforms current approaches, achieving state-of-the-art results and improving the factual accuracy and reliability of Med-LVLMs.

Conclusion: HeteroRAG demonstrates significant improvements in clinical decision-making by leveraging heterogeneous multimodal knowledge sources, making it a robust solution for enhancing the diagnostic reliability of Med-LVLMs.

Abstract: Medical large vision-language Models (Med-LVLMs) have shown promise in
clinical applications but suffer from factual inaccuracies and unreliable
outputs, posing risks in real-world diagnostics. While retrieval-augmented
generation has emerged as a potential solution, current medical multimodal RAG
systems are unable to perform effective retrieval across heterogeneous sources.
The irrelevance of retrieved reports affects the factuality of analysis, while
insufficient knowledge affects the credibility of clinical decision-making. To
bridge the gap, we construct MedAtlas, which includes extensive multimodal
report repositories and diverse text corpora. Based on it, we present
HeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous
knowledge sources. The framework introduces Modality-specific CLIPs for
effective report retrieval and a Multi-corpora Query Generator for dynamically
constructing queries for diverse corpora. Incorporating knowledge from such
multifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge
Preference Tuning to achieve cross-modality and multi-source knowledge
alignment. Extensive experiments across 12 datasets and 3 modalities
demonstrate that the proposed HeteroRAG achieves state-of-the-art performance
in most medical vision language benchmarks, significantly improving factual
accuracy and reliability of Med-LVLMs.

</details>


### [123] [Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward](https://arxiv.org/abs/2508.12800)
*Yong Deng,Guoqing Wang,Zhenzhe Ying,Xiaofeng Wu,Jinzhen Lin,Wenwen Xiong,Yuqin Dai,Shuo Yang,Zhanwei Zhang,Qiwen Wang,Yang Qin,Changhua Meng*

Main category: cs.CL

TL;DR: This paper presents "Atomic Thought" and "Atom-Searcher" as innovative approaches to improve reasoning in LLMs, addressing challenges in existing reinforcement learning frameworks through fine-grained rewards and strategic training schedules.


<details>
  <summary>Details</summary>
Motivation: To enhance LLM reasoning, particularly in multi-hop and strategic search tasks, which are hindered by static knowledge bases, rigid workflows in RAG, and limitations of outcome-based RL methods.

Method: The authors introduce Atomic Thought, a decomposition of reasoning processes into fine-grained units supervised by Reasoning Reward Models (RRMs) providing Atomic Thought Rewards (ATR). Combined with Atom-Searcher, an RL framework with a curriculum-inspired reward schedule, this method focuses on improving reasoning efficiency and interpretability.

Result: The proposed methods significantly outperform state-of-the-art models across seven benchmarks by enabling LLMs to achieve better reasoning efficiency, scalability at test-time, and more interpretable, human-like reasoning.

Conclusion: By addressing conflicting gradients and reward sparsity in RL through fine-grained thinking frameworks, Atom-Searcher demonstrates consistent improvements in deep research tasks, paving the way for more autonomous and interpretable LLM reasoning approaches.

Abstract: Large language models (LLMs) exhibit remarkable problem-solving abilities,
but struggle with complex tasks due to static internal knowledge.
Retrieval-Augmented Generation (RAG) enhances access to external information,
yet remains limited in multi-hop reasoning and strategic search due to rigid
workflows. Recent advancements in agentic deep research empower LLMs to
autonomously reason, search, and synthesize information. However, current
approaches relying on outcome-based reinforcement learning (RL) face critical
issues such as conflicting gradients and reward sparsity, limiting performance
gains and training efficiency. To address these, we first propose Atomic
Thought, a novel LLM thinking paradigm that decomposes reasoning into
fine-grained functional units. These units are supervised by Reasoning Reward
Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained
guidance. Building on this, we propose Atom-Searcher, a novel RL framework for
agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher
uses a curriculum-inspired reward schedule, prioritizing process-level ATR
early and transitioning to outcome rewards, accelerating convergence on
effective reasoning paths. Experiments on seven benchmarks show consistent
improvements over the state-of-the-art. Key advantages include: (1)
Atom-Searcher scales computation at test-time. (2) Atomic Thought provides
supervision anchors for RRMs, bridging deep research tasks and RRMs. (3)
Atom-Searcher exhibits more interpretable, human-like reasoning patterns.

</details>


### [124] [When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models](https://arxiv.org/abs/2508.12803)
*Ahmed Elshabrawy,Hour Kaing,Haiyue Song,Alham Fikri Aji,Hideki Tanaka,Masao Utiyama,Raj Dabre*

Main category: cs.CL

TL;DR: The paper investigates how excessive entanglement with high-resource languages, such as Modern Standard Arabic, can harm generative modeling for low-resource dialects and proposes a new probing and intervention framework to address it.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that alignment with high-resource languages aids modeling of related low-resource varieties and explore how it can hinder generative tasks.

Method: The paper introduces an online variational probing framework that estimates and decouples the subspace of dominant language representation from dialects during fine-tuning.

Result: Their intervention improves generative performance across 25 Arabic dialects, achieving up to +4.9 chrF++ gains, with a tradeoff in high-resource language performance.

Conclusion: The study provides causal evidence that dominance by high-resource languages restricts generative capacities for related varieties and offers practical tools for representational control in multilingual models.

Abstract: Alignment with high-resource standard languages is often assumed to aid the
modeling of related low-resource varieties. We challenge this assumption by
demonstrating that excessive representational entanglement with a dominant
variety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects,
can actively hinder generative modeling. We present the first comprehensive
causal study of this phenomenon by analyzing and directly intervening in the
internal representation geometry of large language models (LLMs). Our key
contribution is an online variational probing framework that continuously
estimates the subspace of the standard variety during fine-tuning, enabling
projection-based decoupling from this space. While our study uses Arabic as a
case due to its unusually rich parallel resources across 25 dialects, the
broader motivation is methodological: dialectal MT serves as a controlled proxy
for generative tasks where comparable multi-variety corpora are unavailable.
Across 25 dialects, our intervention improves generation quality by up to +4.9
chrF++ and +2.0 on average compared to standard fine-tuning, despite a measured
tradeoff in standard-language performance. These results provide causal
evidence that subspace dominance by high-resource varieties can restrict
generative capacity for related varieties. More generally, we unify geometric
and information-theoretic probing with subspace-level causal interventions,
offering practical tools for improving generative modeling in closely related
language families and, more broadly, for controlling representational
allocation in multilingual and multi-domain LLMs. Code will be released.

</details>


### [125] [ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue](https://arxiv.org/abs/2508.12819)
*Jeongwoo Kang,Maria Boritchev,Maximin Coavoux*

Main category: cs.CL

TL;DR: The paper builds a French semantic corpus by annotating spontaneous French dialogues with Abstract Meaning Representation (AMR), extending the framework to better fit French and speech-specific dynamics, and training an AMR parser for assistance in annotation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of semantic resources for French dialogue, particularly for spontaneous speech, by extending AMR and creating a corpus.

Method: The authors annotated transcripts from the DinG corpus with AMR, enhanced the framework for French and spontaneous speech structures, provided annotation guidelines, and trained an AMR parser.

Result: The outcome includes an extended annotation framework, a freely accessible French semantic corpus, and a trained AMR parser aiding human annotation.

Conclusion: This work significantly advances semantic resource development for French dialogue and provides tools to support human annotation effectively.

Abstract: We present our work to build a French semantic corpus by annotating French
dialogue in Abstract Meaning Representation (AMR). Specifically, we annotate
the DinG corpus, consisting of transcripts of spontaneous French dialogues
recorded during the board game Catan. As AMR has insufficient coverage of the
dynamics of spontaneous speech, we extend the framework to better represent
spontaneous speech and sentence structures specific to French. Additionally, to
support consistent annotation, we provide an annotation guideline detailing
these extensions. We publish our corpus under a free license (CC-SA-BY). We
also train and evaluate an AMR parser on our data. This model can be used as an
assistance annotation tool to provide initial annotations that can be refined
by human annotators. Our work contributes to the development of semantic
resources for French dialogue.

</details>


### [126] [Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection](https://arxiv.org/abs/2508.12828)
*Raneem Alharthi,Rajwa Alharthi,Aiqi Jiang,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: The paper investigates whether using contextual features from parent tweets enhances abusive language detection in reply tweets, showing significant improvements when leveraging context.


<details>
  <summary>Details</summary>
Motivation: Abusive language detection in social media posts typically overlooks contextual information from surrounding posts, which might aid in better classification.

Method: The study tests four classification models on conversations (parent-reply tweet pairs) using content-based and account-based features derived from the parent tweet.

Result: Contextual features significantly boost abusive language detection accuracy, particularly content-based features rather than account-based ones.

Conclusion: Integrating contextual data from parent tweets is crucial for improving abusive language detection models, with diverse content-based features being more effective.

Abstract: Abusive language detection has become an increasingly important task as a
means to tackle this type of harmful content in social media. There has been a
substantial body of research developing models for determining if a social
media post is abusive or not; however, this research has primarily focused on
exploiting social media posts individually, overlooking additional context that
can be derived from surrounding posts. In this study, we look at conversational
exchanges, where a user replies to an earlier post by another user (the parent
tweet). We ask: does leveraging context from the parent tweet help determine if
a reply post is abusive or not, and what are the features that contribute the
most? We study a range of content-based and account-based features derived from
the context, and compare this to the more widely studied approach of only
looking at the features from the reply tweet. For a more generalizable study,
we test four different classification models on a dataset made of
conversational exchanges (parent-reply tweet pairs) with replies labeled as
abusive or not. Our experiments show that incorporating contextual features
leads to substantial improvements compared to the use of features derived from
the reply tweet only, confirming the importance of leveraging context. We
observe that, among the features under study, it is especially the
content-based features (what is being posted) that contribute to the
classification performance rather than account-based features (who is posting
it). While using content-based features, it is best to combine a range of
different features to ensure improved performance over being more selective and
using fewer features. Our study provides insights into the development of
contextualized abusive language detection models in realistic settings
involving conversations.

</details>


### [127] [It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae](https://arxiv.org/abs/2508.12830)
*Jan Maliszewski*

Main category: cs.CL

TL;DR: The study applies stylometric techniques and computational approaches to analyze Stephen Langton's Quaestiones Theologiae, aiming to uncover editorial layers and validate hypotheses about its formation.


<details>
  <summary>Details</summary>
Motivation: To explore the editorial formation of Stephen Langton's Quaestiones Theologiae and advance computational methodologies for analyzing scholastic Latin corpora.

Method: Stylometric analysis using frequent words, POS tags, pseudo-affixes; application of HTR pipelines and transformer-based OCR for data extraction.

Result: The study aims to compare performance on manually vs. automatically extracted data and test the validity of advanced OCR techniques for medieval texts.

Conclusion: If successful, the study will serve as a reusable analytical template for medieval literary works and advance computational research in scholastic studies.

Abstract: While the indirect evidence suggests that already in the early scholastic
period the literary production based on records of oral teaching (so-called
reportationes) was not uncommon, there are very few sources commenting on the
practice. This paper details the design of a study applying stylometric
techniques of authorship attribution to a collection developed from
reportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover
layers of editorial work and thus validate some hypotheses regarding the
collection's formation. Following Camps, Cl\'erice, and Pinche (2021), I
discuss the implementation of an HTR pipeline and stylometric analysis based on
the most frequent words, POS tags, and pseudo-affixes. The proposed study will
offer two methodological gains relevant to computational research on the
scholastic tradition: it will directly compare performance on manually composed
and automatically extracted data, and it will test the validity of
transformer-based OCR and automated transcription alignment for workflows
applied to scholastic Latin corpora. If successful, this study will provide an
easily reusable template for the exploratory analysis of collaborative literary
production stemming from medieval universities.

</details>


### [128] [Word Meanings in Transformer Language Models](https://arxiv.org/abs/2508.12863)
*Jumbly Grindrod,Peter Grindrod*

Main category: cs.CL

TL;DR: The paper investigates whether transformer models like RoBERTa-base use something analogous to a lexical store for semantic representation and finds significant semantic encoding in the token embedding space.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how word meanings are represented in transformer language models and to test whether these models employ structures analogous to lexical stores for semantic processing.

Method: The authors analyzed RoBERTa-base's token embedding space by clustering it into 200 clusters using k-means and examined the clusters manually and against five psycholinguistic measures: valence, concreteness, iconicity, taboo, and age of acquisition.

Result: The analysis revealed that the token embedding space encodes a wide variety of semantic information, and the clusters are sensitive to the psycholinguistic measures studied.

Conclusion: The findings suggest that there is significant semantic encoding in transformer language models, ruling out certain eliminativist theories about how such models process semantic information.

Abstract: We investigate how word meanings are represented in the transformer language
models. Specifically, we focus on whether transformer models employ something
analogous to a lexical store - where each word has an entry that contains
semantic information. To do this, we extracted the token embedding space of
RoBERTa-base and k-means clustered it into 200 clusters. In our first study, we
then manually inspected the resultant clusters to consider whether they are
sensitive to semantic information. In our second study, we tested whether the
clusters are sensitive to five psycholinguistic measures: valence,
concreteness, iconicity, taboo, and age of acquisition. Overall, our findings
were very positive - there is a wide variety of semantic information encoded
within the token embedding space. This serves to rule out certain "meaning
eliminativist" hypotheses about how transformer LLMs process semantic
information.

</details>


### [129] [An LLM Agent-Based Complex Semantic Table Annotation Approach](https://arxiv.org/abs/2508.12868)
*Yilin Geng,Shujing Wang,Chuan Wang,Keqing He,Yanfei Lv,Ying Wang,Zaiwen Feng,Xiaoying Bai*

Main category: cs.CL

TL;DR: The paper addresses challenges in Semantic Table Annotation (STA) using an LLM-based agent approach, achieving improved annotation accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and efficiency of Semantic Table Annotation, especially in challenging cases such as complex tables with semantic loss, hierarchy requirements, and noise like spelling errors and abbreviations.

Method: Proposes an LLM-based agent for STA tasks, leveraging tools designed with tailored prompts under the ReAct framework and optimizing with Levenshtein distance for reducing redundancy in annotations.

Result: The proposed method outperformed existing solutions in experiments on Tough Tables and BiodivTab datasets, achieving a 70% reduction in time cost and a 60% reduction in LLM token usage.

Conclusion: This novel STA approach, with its superior performance and efficiency, offers a cost-effective solution for table annotation challenges.

Abstract: The Semantic Table Annotation (STA) task, which includes Column Type
Annotation (CTA) and Cell Entity Annotation (CEA), maps table contents to
ontology entities and plays important roles in various semantic applications.
However, complex tables often pose challenges such as semantic loss of column
names or cell values, strict ontological hierarchy requirements, homonyms,
spelling errors, and abbreviations, which hinder annotation accuracy. To
address these issues, this paper proposes an LLM-based agent approach for CTA
and CEA. We design and implement five external tools with tailored prompts
based on the ReAct framework, enabling the STA agent to dynamically select
suitable annotation strategies depending on table characteristics. Experiments
are conducted on the Tough Tables and BiodivTab datasets from the SemTab
challenge, which contain the aforementioned challenges. Our method outperforms
existing approaches across various metrics. Furthermore, by leveraging
Levenshtein distance to reduce redundant annotations, we achieve a 70%
reduction in time costs and a 60% reduction in LLM token usage, providing an
efficient and cost-effective solution for STA.

</details>


### [130] [A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models](https://arxiv.org/abs/2508.12903)
*Jinyi Han,Xinyi Wang,Haiquan Zhao,Tingyun li,Zishang Jiang,Sihang Jiang,Jiaqing Liang,Xin Lin,Weikang Zhou,Zeye Sun,Fei Yu,Yanghua Xiao*

Main category: cs.CL

TL;DR: ProActive Self-Refinement (PASR) introduces a dynamic method for improving LLM outputs during generation, achieving better accuracy and efficiency across multiple tasks compared to traditional approaches.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current self-refinement methods that rely on a fixed number of iterations and do not adapt to evolving generation contexts.

Method: PASR proactively decides refinement timing and content during the generation process, leveraging the model's internal state and context without regenerating entire responses.

Result: PASR improves task accuracy by 8.2% and boosts efficiency by reducing token usage by 41.6% on Qwen3-8B compared to standard generation.

Conclusion: PASR shows potential in enhancing LLM performance dynamically, setting a benchmark for efficient and accurate refinement during generation.

Abstract: Recent advances in self-refinement have demonstrated significant potential
for improving the outputs of large language models (LLMs) through iterative
refinement. However, most existing self-refinement methods rely on a reactive
process with a fixed number of iterations, making it difficult to determine the
optimal timing and content of refinement based on the evolving generation
context. Inspired by the way humans dynamically refine their thoughts during
execution, we propose ProActive Self-Refinement (PASR), a novel method that
enables LLMs to refine their outputs during the generation process. Unlike
methods that regenerate entire responses, PASR proactively decides whether,
when, and how to refine based on the model's internal state and evolving
context. We conduct extensive experiments on a diverse set of 10 tasks to
evaluate the effectiveness of PASR. Experimental results show that PASR
significantly enhances problem-solving performance. In particular, on Qwen3-8B,
PASR reduces average token consumption by 41.6 percent compared to standard
generation, while also achieving an 8.2 percent improvement in accuracy. Our
code and all baselines used in the paper are available in the GitHub.

</details>


### [131] [Analyzing Information Sharing and Coordination in Multi-Agent Planning](https://arxiv.org/abs/2508.12981)
*Tianyue Ou,Saujas Vaduguru,Daniel Fried*

Main category: cs.CL

TL;DR: The paper develops an MAS with LLM agents for travel planning and highlights the effectiveness of a notebook for information sharing and an orchestrator for coordination, achieving notable improvements in task success.


<details>
  <summary>Details</summary>
Motivation: Address the challenges posed by long-horizon and multi-constraint planning tasks which require detailed information and management of complex dependencies.

Method: Introduce a notebook for structured information sharing and an orchestrator agent for improved communication among LLM agents in a travel planning MAS.

Result: The notebook reduces hallucination errors by 18%, the orchestrator minimizes errors by 13.5% in specific areas, and combining both achieves a 25% pass rate on the TravelPlanner benchmark—17.5% higher than the single-agent baseline's rate.

Conclusion: Structured tools like notebooks and orchestrator agents significantly enhance MAS effectiveness for complex, long-horizon tasks, underscoring their utility for LLM-based systems.

Abstract: Multi-agent systems (MASs) have pushed the boundaries of large language model
(LLM) agents in domains such as web research and software engineering. However,
long-horizon, multi-constraint planning tasks involve conditioning on detailed
information and satisfying complex interdependent constraints, which can pose a
challenge for these systems. In this study, we construct an LLM-based MAS for a
travel planning task which is representative of these challenges. We evaluate
the impact of a notebook to facilitate information sharing, and evaluate an
orchestrator agent to improve coordination in free form conversation between
agents. We find that the notebook reduces errors due to hallucinated details by
18%, while an orchestrator directs the MAS to focus on and further reduce
errors by up to 13.5% within focused sub-areas. Combining both mechanisms
achieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute
improvement over the single-agent baseline's 7.5% pass rate. These results
highlight the potential of structured information sharing and reflective
orchestration as key components in MASs for long horizon planning with LLMs.

</details>


### [132] [WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents](https://arxiv.org/abs/2508.13024)
*Ralph Peeters,Aaron Steiner,Luca Schwarz,Julian Yuya Caspary,Christian Bizer*

Main category: cs.CL

TL;DR: WebMall is introduced as a benchmark to evaluate web agents for automated comparison-shopping across multiple shops, featuring diverse tasks and real-world authenticity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for effective and efficient evaluation of LLM-based web agents for automating complex e-commerce tasks, including comparison-shopping.

Method: WebMall combines simulated online shops, real-world product data from Common Crawl, and a suite of 91 tasks across basic and advanced scenarios to test varied configurations of web agents.

Result: Eight baseline agents were evaluated on WebMall using GPT 4.1 and Claude Sonnet 4, achieving completion rates up to 75% for basic tasks and 53% for advanced tasks, with respective F1 scores of 87% and 63%.

Conclusion: WebMall serves as a valuable tool for advancing research on web agents, offering realistic challenges and promoting improvements in reasoning, navigation, and efficiency for e-commerce applications.

Abstract: LLM-based web agents have the potential to automate long-running web tasks,
such as finding offers for specific products in multiple online shops and
subsequently ordering the cheapest products that meet the users needs. This
paper introduces WebMall, a multi-shop online shopping benchmark for evaluating
the effectiveness and efficiency of web agents for comparison-shopping. WebMall
consists of four simulated online shops populated with authentic product offers
sourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These
tasks include basic tasks such as finding specific products in multiple shops,
performing price comparisons, adding items to the shopping cart, and completing
checkout. Advanced tasks involve searching for products based on vague
requirements, identifying suitable substitutes, and finding compatible
products. Compared to existing e-commerce benchmarks, such as WebShop or
ShoppingBench, WebMall introduces comparison-shopping tasks across multiple
shops. Furthermore, the product offers are more heterogeneous, as they
originate from hundreds of distinct real-world shops. The tasks in WebMall
require longer interaction trajectories than those in WebShop, while remaining
representative of real-world shopping behaviors. We evaluate eight baseline
agents on WebMall, varying in observation modality, memory utilization, and
underlying large language model (GPT 4.1 and Claude Sonnet 4). The
best-performing configurations achieve completion rates of 75% and 53%, and F1
scores of 87% and 63%, on the basic and advanced task sets, respectively.
WebMall is publicly released to facilitate research on web agents and to
promote advancements in navigation, reasoning, and efficiency within e-commerce
scenarios.

</details>


### [133] [Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis](https://arxiv.org/abs/2508.13028)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Devraj Raghuvanshi,Nagendra Kumar,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: This paper introduces an advanced method for synthesizing sarcastic speech by integrating sarcasm detection feedback and leveraging transfer learning in speech synthesis models.


<details>
  <summary>Details</summary>
Motivation: The motivation of this study is to address the challenge of synthesizing sarcastic speech characterized by nuanced prosody and overcome the scarcity of annotated sarcastic speech data.

Method: The method involves using a bi-modal sarcasm detection model's feedback loss in the TTS training process and applying a two-stage fine-tuning approach with diverse datasets, specifically targeting sarcastic speech.

Result: Objective and subjective evaluations indicate that the proposed methods enhance the quality, naturalness, and sarcasm-awareness of the synthesized speech.

Conclusion: This study successfully demonstrated a novel approach to sarcastic speech synthesis, improving interaction quality in applications like human-computer interaction and entertainment.

Abstract: Sarcastic speech synthesis, which involves generating speech that effectively
conveys sarcasm, is essential for enhancing natural interactions in
applications such as entertainment and human-computer interaction. However,
synthesizing sarcastic speech remains a challenge due to the nuanced prosody
that characterizes sarcasm, as well as the limited availability of annotated
sarcastic speech data. To address these challenges, this study introduces a
novel approach that integrates feedback loss from a bi-modal sarcasm detection
model into the TTS training process, enhancing the model's ability to capture
and convey sarcasm. In addition, by leveraging transfer learning, a speech
synthesis model pre-trained on read speech undergoes a two-stage fine-tuning
process. First, it is fine-tuned on a diverse dataset encompassing various
speech styles, including sarcastic speech. In the second stage, the model is
further refined using a dataset focused specifically on sarcastic speech,
enhancing its ability to generate sarcasm-aware speech. Objective and
subjective evaluations demonstrate that our proposed methods improve the
quality, naturalness, and sarcasm-awareness of synthesized speech.

</details>


### [134] [Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction](https://arxiv.org/abs/2508.13037)
*Xinhe Li,Jiajun Liu,Peng Wang*

Main category: cs.CL

TL;DR: This paper introduces LoRID, an innovative training approach to improve mathematical reasoning in small language models (SLMs), outperforming other methods significantly.


<details>
  <summary>Details</summary>
Motivation: Small Language Models (SLMs) struggle with mathematical reasoning, making them less effective than Large Language Models (LLMs), which have huge parameter counts. A better method for improving SLM reasoning capabilities is needed.

Method: The authors propose LoRID, a multi-LoRA interaction framework inspired by human dual-system thinking. Their method trains separate components (Intuitive Reasoner, Knowledge Generator, Deep Reasoner) and enforces result consistency through iterative evaluation.

Result: LoRID achieves state-of-the-art performance on the GSM8K dataset, outperforming the second-best method by significant margins across five base models, with accuracy improvements ranging from 1.8% to 16.1%.

Conclusion: LoRID enhances SLMs' mathematical reasoning by integrating intuitive and deliberate reasoning processes, demonstrating that a multi-component approach is more effective for reasoning tasks.

Abstract: Recent studies have demonstrated that Large Language Models (LLMs) have
strong mathematical reasoning abilities but rely on hundreds of billions of
parameters. To tackle the challenge of poor reasoning in Small Language Models
(SLMs), existing methods typically leverage LLMs to generate massive amounts of
data for cramming training. In psychology, they are akin to System 1 thinking,
which resolves reasoning problems rapidly based on experience and intuition.
However, human learning also requires System 2 thinking, where knowledge is
first acquired and then reinforced through practice. Inspired by such two
distinct modes of thinking, we propose a novel method based on the multi-LoRA
Interaction for mathematical reasoning Distillation (LoRID). First, we input
the question and reasoning of each sample into an LLM to create
knowledge-enhanced datasets. Subsequently, we train a LoRA block on the student
model as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts
for problem-solving. Then, to imitate System 2 thinking, we train the Knowledge
Generator (KG) and Deep Reasoner (DR), respectively. The former outputs only
knowledge after receiving problems, while the latter uses that knowledge to
perform reasoning. Finally, to address the randomness in the generation of IR
and DR, we evaluate whether their outputs are consistent, and the inference
process needs to be iterated if not. This step can enhance the mathematical
reasoning ability of SLMs through mutual feedback. Experimental results show
that LoRID achieves state-of-the-art performance, especially on the GSM8K
dataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,
12.3%, and 1.8% accuracy across the five base models, respectively.

</details>


### [135] [Büyük Dil Modelleri için TR-MMLU Benchmarkı: Performans Değerlendirmesi, Zorluklar ve İyileştirme Fırsatları](https://arxiv.org/abs/2508.13044)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Banu Diri,Savaş Yıldırım,Öner Aytaş*

Main category: cs.CL

TL;DR: This paper introduces TR-MMLU, a benchmark for evaluating large language models in Turkish using 6,200 multiple-choice questions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of evaluating language models for Turkish, a resource-limited language.

Method: The TR-MMLU benchmark was created with a dataset of 6,200 multiple-choice questions spanning 62 sections of the Turkish education system.

Result: State-of-the-art models were evaluated on TR-MMLU, showing areas that require improvement.

Conclusion: TR-MMLU establishes a new standard for Turkish NLP research and aims to inspire further advancements in the field.

Abstract: Language models have made significant advancements in understanding and
generating human language, achieving remarkable success in various
applications. However, evaluating these models remains a challenge,
particularly for resource-limited languages like Turkish. To address this
issue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive
evaluation framework designed to assess the linguistic and conceptual
capabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a
meticulously curated dataset comprising 6,200 multiple-choice questions across
62 sections within the Turkish education system. This benchmark provides a
standard framework for Turkish NLP research, enabling detailed analyses of
LLMs' capabilities in processing Turkish text. In this study, we evaluated
state-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model
design. TR-MMLU sets a new standard for advancing Turkish NLP research and
inspiring future innovations.

</details>


### [136] [Doğal Dil İşlemede Tokenizasyon Standartları ve Ölçümü: Türkçe Üzerinden Büyük Dil Modellerinin Karşılaştırmalı Analizi](https://arxiv.org/abs/2508.13058)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Sercan Karakaş,Banu Diri,Savaş Yıldırım*

Main category: cs.CL

TL;DR: The study proposes a new evaluation framework to assess tokenization for morphologically-rich and low-resource languages like Turkish.


<details>
  <summary>Details</summary>
Motivation: Current tokenization methods face challenges in morphologically-rich and low-resource languages, impacting LLM performance.

Method: The study introduces evaluation metrics (vocabulary size, token count, processing time, %TR, %Pure) using the Turkish MMLU dataset with 6,200 multiple-choice questions for analysis.

Result: Language-specific token percentages showed stronger correlation with downstream performance than token purity, emphasizing tailored tokenization for Turkish.

Conclusion: The paper highlights the importance of language-specific tokenization strategies for improving linguistic performance in morphologically-rich languages.

Abstract: Tokenization is a fundamental preprocessing step in Natural Language
Processing (NLP), significantly impacting the capability of large language
models (LLMs) to capture linguistic and semantic nuances. This study introduces
a novel evaluation framework addressing tokenization challenges specific to
morphologically-rich and low-resource languages such as Turkish. Utilizing the
Turkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from
the Turkish education system, we assessed tokenizers based on vocabulary size,
token count, processing time, language-specific token percentages (\%TR), and
token purity (\%Pure). These newly proposed metrics measure how effectively
tokenizers preserve linguistic structures. Our analysis reveals that
language-specific token percentages exhibit a stronger correlation with
downstream performance (e.g., MMLU scores) than token purity. Furthermore,
increasing model parameters alone does not necessarily enhance linguistic
performance, underscoring the importance of tailored, language-specific
tokenization methods. The proposed framework establishes robust and practical
tokenization standards for morphologically complex languages.

</details>


### [137] [Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database](https://arxiv.org/abs/2508.13060)
*John Alderete,Macarious Kin Fung Hui,Aanchan Mohan*

Main category: cs.CL

TL;DR: The Simon Fraser University Speech Error Database (SFUSED) is a public speech error dataset used to test and evaluate speech recognition models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to leverage systematically annotated speech error data to evaluate and improve the performance of speech recognition models.

Method: The SFUSED database incorporates multi-dimensional annotations of speech errors and evaluates transcription accuracy of WhisperX across 5,300 errors to assess its utility for ASR system performance.

Result: The database proved effective as a diagnostic tool for assessing Automatic Speech Recognition (ASR) systems like WhisperX.

Conclusion: SFUSED is a valuable resource for linguistic research and speech recognition model benchmarking, given its systematic design and robust assessment capabilities.

Abstract: The Simon Fraser University Speech Error Database (SFUSED) is a public data
collection developed for linguistic and psycholinguistic research. Here we
demonstrate how its design and annotations can be used to test and evaluate
speech recognition models. The database comprises systematically annotated
speech errors from spontaneous English speech, with each error tagged for
intended and actual error productions. The annotation schema incorporates
multiple classificatory dimensions that are of some value to model assessment,
including linguistic hierarchical level, contextual sensitivity, degraded
words, word corrections, and both word-level and syllable-level error
positioning. To assess the value of these classificatory variables, we
evaluated the transcription accuracy of WhisperX across 5,300 documented word
and phonological errors. This analysis demonstrates the atabase's effectiveness
as a diagnostic tool for ASR system performance.

</details>


### [138] [Reinforced Context Order Recovery for Adaptive Reasoning and Planning](https://arxiv.org/abs/2508.13070)
*Long Ma,Fangwei Zhong,Yizhou Wang*

Main category: cs.CL

TL;DR: The paper introduces ReCOR, a framework for adaptive token generation orders, improving causal models' performance in reasoning and planning tasks.


<details>
  <summary>Details</summary>
Motivation: Current causal and diffusion models struggle in tasks requiring non-fixed token generation orders, and adaptive token selection was identified as a solution.

Method: ReCOR uses reinforcement learning to estimate token prediction difficulty, enabling adaptive token selection during training and inference.

Result: Experiments showcase ReCOR's superiority over existing models, even surpassing oracle models trained on ground-truth token generation order.

Conclusion: ReCOR adds significant value by autonomously optimizing token generation orders, enhancing modeling capabilities in complex tasks.

Abstract: Modern causal language models, followed by rapid developments in discrete
diffusion models, can now produce a wide variety of interesting and useful
content. However, these families of models are predominantly trained to output
tokens with a fixed (left-to-right) or random order, which may deviate from the
logical order in which tokens are generated originally. In this paper, we
observe that current causal and diffusion models encounter difficulties in
problems that require adaptive token generation orders to solve tractably,
which we characterize with the $\mathcal{V}$-information framework. Motivated
by this, we propose Reinforced Context Order Recovery (ReCOR), a
reinforcement-learning-based framework to extract adaptive, data-dependent
token generation orders from text data without annotations. Self-supervised by
token prediction statistics, ReCOR estimates the hardness of predicting every
unfilled token and adaptively selects the next token during both training and
inference. Experiments on challenging reasoning and planning datasets
demonstrate the superior performance of ReCOR compared with baselines,
sometimes outperforming oracle models supervised with the ground-truth order.

</details>


### [139] [DocHPLT: A Massively Multilingual Document-Level Translation Dataset](https://arxiv.org/abs/2508.13079)
*Dayyán O'Brien,Bhavitvya Malik,Ona de Gibert,Pinzhen Chen,Barry Haddow,Jörg Tiedemann*

Main category: cs.CL

TL;DR: The paper introduces DocHPLT, the largest public dataset for document-level machine translation, covering 50 languages with over 124M aligned document pairs.


<details>
  <summary>Details</summary>
Motivation: To aid training and evaluation of document-level machine translation and address the need for long-context modeling, especially for under-resourced languages.

Method: Modified an existing web extraction pipeline to gather and align complete multilingual documents instead of reconstructing them from sentence-level data.

Result: DocHPLT enables fine-tuned models to outperform baseline instruction-tuned models, significantly advancing under-resourced language translation.

Conclusion: Open-sourcing DocHPLT offers essential resources to elevate multilingual translation, particularly for low-resourced languages.

Abstract: Existing document-level machine translation resources are only available for
a handful of languages, mostly high-resourced ones. To facilitate the training
and evaluation of document-level translation and, more broadly, long-context
modeling for global communities, we create DocHPLT, the largest publicly
available document-level translation dataset to date. It contains 124 million
aligned document pairs across 50 languages paired with English, comprising 4.26
billion sentences, with further possibility to provide 2500 bonus pairs not
involving English. Unlike previous reconstruction-based approaches that piece
together documents from sentence-level data, we modify an existing web
extraction pipeline to preserve complete document integrity from the source,
retaining all content including unaligned portions. After our preliminary
experiments identify the optimal training context strategy for document-level
translation, we demonstrate that LLMs fine-tuned on DocHPLT substantially
outperform off-the-shelf instruction-tuned baselines, with particularly
dramatic improvements for under-resourced languages. We open-source the dataset
under a permissive license, providing essential infrastructure for advancing
multilingual document-level translation.

</details>


### [140] [All for law and law for all: Adaptive RAG Pipeline for Legal Research](https://arxiv.org/abs/2508.13107)
*Figarri Keisha,Prince Singh,Pallavi,Dion Fernandes,Aravindh Manivannan,Ilham Wicaksono,Faisal Ahmad*

Main category: cs.CL

TL;DR: The paper introduces an improved Retrieval-Augmented Generation (RAG) pipeline tailored for the legal domain, which enhances query translation, retrieval strategies, and evaluation frameworks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the problem of hallucinations in large language model outputs by providing citations to reliable sources, which is critical in high-stakes domains like legal research.

Method: The paper enhances RAG with a context-aware query translator, open-source retrieval strategies leveraging SBERT and GTE embeddings, and a comprehensive evaluation framework combining metrics like RAGAS, BERTScore-F1, and ROUGE-Recall.

Result: Results show improved retrieval performance (30-95% increase in Recall@K and 2.5x improvement in Precision@K for K>4) and increased answer faithfulness and relevance using custom prompts.

Conclusion: Task-specific tuning of open-source RAG methods can effectively produce legally compliant, reproducible, and cost-efficient systems that rival proprietary solutions for legal assistance.

Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding
large language model outputs in cited sources, a capability that is especially
critical in the legal domain. We present an end-to-end RAG pipeline that
revisits and extends the LegalBenchRAG baseline with three targeted
enhancements: (i) a context-aware query translator that disentangles document
references from natural-language questions and adapts retrieval depth and
response style based on expertise and specificity, (ii) open-source retrieval
strategies using SBERT and GTE embeddings that achieve substantial performance
gains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for
$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and
generation framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to
assess semantic alignment and faithfulness across models and prompt designs.
Our results show that carefully designed open-source pipelines can rival or
outperform proprietary approaches in retrieval quality, while a custom
legal-grounded prompt consistently produces more faithful and contextually
relevant answers than baseline prompting. Taken together, these contributions
demonstrate the potential of task-aware, component-level tuning to deliver
legally grounded, reproducible, and cost-effective RAG systems for legal
research assistance.

</details>


### [141] [AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13118)
*Zefang Liu,Arman Anwar*

Main category: cs.CL

TL;DR: This paper introduces AutoBnB-RAG, a cybersecurity incident response simulation that integrates retrieval-augmented generation (RAG) into autonomous multi-agent systems for more informed decision-making.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of large language models (LLMs) in autonomous incident response, particularly their inability to access external knowledge for effective reasoning.

Method: They extended the AutoBnB framework to incorporate retrieval-augmented generation (RAG), allowing agents to query external technical documentation or narrative incident reports during decision-making simulations using the Backdoors & Breaches tabletop game environment.

Result: By testing various team structures and simulating real-world cyber incidents, the study demonstrates that retrieval augmentation improves decision quality and success rates in incident simulations.

Conclusion: Incorporating retrieval mechanisms into LLM-based multi-agent systems enhances their utility for cybersecurity decision-making, underscoring the value of external knowledge integration.

Abstract: Incident response (IR) requires fast, coordinated, and well-informed
decision-making to contain and mitigate cyber threats. While large language
models (LLMs) have shown promise as autonomous agents in simulated IR settings,
their reasoning is often limited by a lack of access to external knowledge. In
this work, we present AutoBnB-RAG, an extension of the AutoBnB framework that
incorporates retrieval-augmented generation (RAG) into multi-agent incident
response simulations. Built on the Backdoors & Breaches (B&B) tabletop game
environment, AutoBnB-RAG enables agents to issue retrieval queries and
incorporate external evidence during collaborative investigations. We introduce
two retrieval settings: one grounded in curated technical documentation
(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We
evaluate performance across eight team structures, including newly introduced
argumentative configurations designed to promote critical reasoning. To
validate practical utility, we also simulate real-world cyber incidents based
on public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct
complex multi-stage attacks. Our results show that retrieval augmentation
improves decision quality and success rates across diverse organizational
models. This work demonstrates the value of integrating retrieval mechanisms
into LLM-based multi-agent systems for cybersecurity decision-making.

</details>


### [142] [Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries](https://arxiv.org/abs/2508.13124)
*Kawin Mayilvaghanan,Siddhant Gupta,Ayush Kumar*

Main category: cs.CL

TL;DR: The paper introduces BlindSpot, a framework to identify and quantify operational biases in summaries generated by large language models (LLMs) for contact center transcripts.


<details>
  <summary>Details</summary>
Motivation: To address the potential biases in the summaries generated by LLMs in contact center applications, especially focusing on the unexplored area of operational biases.

Method: They developed BlindSpot, a framework informed by a taxonomy of 15 operational bias dimensions, leveraging LLMs as zero-shot classifiers to assess biases using metrics like Fidelity Gap and Coverage.

Result: An empirical study using 2500 call transcripts across 20 LLMs indicates systemic biases across all models, regardless of their size or family.

Conclusion: Operational biases in LLM-generated summaries are prevalent and should be addressed to ensure accurate and unbiased summarization.

Abstract: Abstractive summarization is a core application in contact centers, where
Large Language Models (LLMs) generate millions of summaries of call transcripts
daily. Despite their apparent quality, it remains unclear whether LLMs
systematically under- or over-attend to specific aspects of the transcript,
potentially introducing biases in the generated summary. While prior work has
examined social and positional biases, the specific forms of bias pertinent to
contact center operations - which we term Operational Bias - have remained
unexplored. To address this gap, we introduce BlindSpot, a framework built upon
a taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)
for the identification and quantification of these biases. BlindSpot leverages
an LLM as a zero-shot classifier to derive categorical distributions for each
bias dimension in a pair of transcript and its summary. The bias is then
quantified using two metrics: Fidelity Gap (the JS Divergence between
distributions) and Coverage (the percentage of source labels omitted). Using
BlindSpot, we conducted an empirical study with 2500 real call transcripts and
their summaries generated by 20 LLMs of varying scales and families (e.g., GPT,
Llama, Claude). Our analysis reveals that biases are systemic and present
across all evaluated models, regardless of size or family.

</details>


### [143] [MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation](https://arxiv.org/abs/2508.13130)
*Kareem Elozeiri,Mervat Abassy,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: This paper addresses the underexplored field of Arabic commonsense reasoning by introducing a multi-dialect dataset (MuDRiC) and a novel method using Graph Convolutional Networks (GCNs) to enhance language comprehension.


<details>
  <summary>Details</summary>
Motivation: To address the lack of resources and methods for commonsense validation in Arabic, especially covering its diverse dialects beyond Modern Standard Arabic.

Method: The authors developed a new dataset (MuDRiC) encompassing multiple Arabic dialects and proposed a novel approach using Graph Convolutional Networks (GCNs) to improve semantic relationship modeling in Arabic commonsense validation.

Result: The proposed method demonstrated improved performance in validating commonsense in Arabic across different dialects.

Conclusion: This work provides a significant contribution to Arabic NLP by delivering the first multi-dialect Arabic commonsense reasoning dataset and introducing an effective GCN-based method for tackling this complex problem.

Abstract: Commonsense validation evaluates whether a sentence aligns with everyday
human understanding, a critical capability for developing robust natural
language understanding systems. While substantial progress has been made in
English, the task remains underexplored in Arabic, particularly given its rich
linguistic diversity. Existing Arabic resources have primarily focused on
Modern Standard Arabic (MSA), leaving regional dialects underrepresented
despite their prevalence in spoken contexts. To bridge this gap, we present two
key contributions: (i) we introduce MuDRiC, an extended Arabic commonsense
dataset incorporating multiple dialects, and (ii) a novel method adapting Graph
Convolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances
semantic relationship modeling for improved commonsense validation. Our
experimental results demonstrate that this approach achieves superior
performance in Arabic commonsense validation. Our work enhances Arabic natural
language understanding by providing both a foundational dataset and a novel
method for handling its complex variations. To the best of our knowledge, we
release the first Arabic multi-dialect commonsense reasoning dataset.

</details>


### [144] [OptimalThinkingBench: Evaluating Over and Underthinking in LLMs](https://arxiv.org/abs/2508.13141)
*Pranjal Aggarwal,Seungone Kim,Jack Lanchantin,Sean Welleck,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: This paper introduces OptimalThinkingBench, a benchmark designed to evaluate and improve the balance between overthinking and underthinking in large language models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing performance and efficiency in LLMs, which either overthink simpler problems or underthink complex reasoning tasks, leaving users to manually select the optimal model.

Method: The authors developed OptimalThinkingBench, comprising two sub-benchmarks (OverthinkingBench and UnderthinkingBench) and used thinking-adjusted accuracy metrics to evaluate 33 LLM models.

Result: No model was found to optimally think; thinking models overthink without performance gain, while non-thinking models underperform compared to smaller thinking models. Methods to balance thinking were limited in effectiveness.

Conclusion: There is a significant need for unified, optimally-performing LLMs to address both overthinking and underthinking challenges efficiently.

Abstract: Thinking LLMs solve complex tasks at the expense of increased compute and
overthinking on simpler problems, while non-thinking LLMs are faster and
cheaper but underthink on harder reasoning problems. This has led to the
development of separate thinking and non-thinking LLM variants, leaving the
onus of selecting the optimal model for each query on the end user. In this
work, we introduce OptimalThinkingBench, a unified benchmark that jointly
evaluates overthinking and underthinking in LLMs and also encourages the
development of optimally-thinking models that balance performance and
efficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,
featuring simple queries in 72 domains, and UnderthinkingBench, containing 11
challenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we
perform extensive evaluation of 33 different thinking and non-thinking models
and show that no model is able to optimally think on our benchmark. Thinking
models often overthink for hundreds of tokens on the simplest user queries
without improving performance. In contrast, large non-thinking models
underthink, often falling short of much smaller thinking models. We further
explore several methods to encourage optimal thinking, but find that these
approaches often improve on one sub-benchmark at the expense of the other,
highlighting the need for better unified and optimal models in the future.

</details>


### [145] [Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation](https://arxiv.org/abs/2508.13144)
*David Heineman,Valentin Hofmann,Ian Magnusson,Yuling Gu,Noah A. Smith,Hannaneh Hajishirzi,Kyle Lo,Jesse Dodge*

Main category: cs.CL

TL;DR: The paper introduces strategies to create more reliable evaluation benchmarks for large language models by focusing on signal-to-noise ratio and proposes interventions such as improved metrics, task refinement, and checkpoint averaging.


<details>
  <summary>Details</summary>
Motivation: Developing large language models involves costly decisions based on small-scale experiments and is reliant on multi-task evaluation benchmarks, which can vary in reliability.

Method: The authors define two metrics - signal and noise - to analyze benchmark reliability, and propose interventions such as using perplexity instead of accuracy, filtering noisy subtasks, and averaging outputs from model checkpoints.

Result: Benchmarks with better signal-to-noise ratios are shown to lead to more reliable small-scale model evaluations and have reduced scaling law prediction errors. The proposed interventions consistently improve evaluation reliability.

Conclusion: Evaluation benchmarks for language models should prioritize high signal and low noise. The study offers actionable interventions and provides a substantial dataset for benchmarking analyses.

Abstract: Developing large language models is expensive and involves making decisions
with small experiments, typically by evaluating on large, multi-task evaluation
suites. In this work, we analyze specific properties which make a benchmark
more reliable for such decisions, and interventions to design higher-quality
evaluation benchmarks. We introduce two key metrics that show differences in
current benchmarks: signal, a benchmark's ability to separate better models
from worse models, and noise, a benchmark's sensitivity to random variability
between training steps. We demonstrate that benchmarks with a better
signal-to-noise ratio are more reliable when making decisions at small scale,
and those with less noise have lower scaling law prediction error. These
results suggest that improving signal or noise will lead to more useful
benchmarks, so we introduce three interventions designed to directly affect
signal or noise. For example, we propose that switching to a metric that has
better signal and noise (e.g., perplexity rather than accuracy) leads to better
reliability and improved scaling law error. We also find that filtering noisy
subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable
multi-task evaluations. We also find that averaging the output of a model's
intermediate checkpoints to reduce noise leads to consistent improvements. We
conclude by recommending that those creating new benchmarks, or selecting which
existing benchmarks to use, aim for high signal and low noise. We use 30
benchmarks for these experiments, and 375 open-weight language models from 60M
to 32B parameters, resulting in a new, publicly available dataset of 900K
evaluation benchmark results, totaling 200M instances.

</details>


### [146] [RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns](https://arxiv.org/abs/2508.13152)
*Xin Chen,Junchao Wu,Shu Yang,Runzhe Zhan,Zeyu Wu,Ziyang Luo,Di Wang,Min Yang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: The paper presents RepreGuard, a method for detecting texts generated by large language models (LLMs) using their internal neural representations, achieving high AUROC performance even in out-of-distribution scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robustness in existing methods for detecting out-of-distribution (OOD) generated texts and to enhance trustworthy AI systems.

Method: RepreGuard utilizes internal neural activation patterns of LLMs to detect LLM-generated texts (LGT) versus human-written texts (HWT). It involves a surrogate model to extract distinctive activation features and classify texts via projection scores against a threshold.

Result: RepreGuard outperformed all benchmarks achieving an average AUROC of 94.92% in both in-distribution and out-of-distribution scenarios, while proving robust against diverse text sizes and adversarial attacks.

Conclusion: This approach successfully leverages fundamental differences in neural activations between LGT and HWT, providing a reliable and resilient method for detecting LLM-generated content.

Abstract: Detecting content generated by large language models (LLMs) is crucial for
preventing misuse and building trustworthy AI systems. Although existing
detection methods perform well, their robustness in out-of-distribution (OOD)
scenarios is still lacking. In this paper, we hypothesize that, compared to
features used by existing detection methods, the internal representations of
LLMs contain more comprehensive and raw features that can more effectively
capture and distinguish the statistical pattern differences between
LLM-generated texts (LGT) and human-written texts (HWT). We validated this
hypothesis across different LLMs and observed significant differences in neural
activation patterns when processing these two types of texts. Based on this, we
propose RepreGuard, an efficient statistics-based detection method.
Specifically, we first employ a surrogate model to collect representation of
LGT and HWT, and extract the distinct activation feature that can better
identify LGT. We can classify the text by calculating the projection score of
the text representations along this feature direction and comparing with a
precomputed threshold. Experimental results show that RepreGuard outperforms
all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD
scenarios, while also demonstrating robust resilience to various text sizes and
mainstream attacks. Data and code are publicly available at:
https://github.com/NLP2CT/RepreGuard

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [147] [A Deep Learning-Based CCTV System for Automatic Smoking Detection in Fire Exit Zones](https://arxiv.org/abs/2508.11696)
*Sami Sadat,Mohammad Irtiza Hossain,Junaid Ahmed Sifat,Suhail Haque Rafi,Md. Waseq Alauddin Alvi,Md. Khalilur Rhaman*

Main category: cs.CV

TL;DR: This paper proposes a real-time smoking detection system for CCTV using advanced object detection models, with the custom model achieving high accuracy and compatibility with edge devices.


<details>
  <summary>Details</summary>
Motivation: The study was motivated by the critical need for safety in fire exit areas, requiring efficient surveillance to detect smoking incidents.

Method: The authors created a dataset of 8,124 images from 20 scenarios, evaluated YOLOv8, YOLOv11, and YOLOv12, and developed a custom model based on YOLOv8 tailored for challenging surveillance environments. Performance was tested on different edge devices.

Result: Their custom model outperformed others with a recall of 78.90% and mAP@50 of 83.70%. It also demonstrated efficient processing speeds of 52-97 milliseconds per inference on the Jetson Xavier NX.

Conclusion: The system is effective for real-time public safety monitoring and regulatory compliance in varied and challenging environments.

Abstract: A deep learning real-time smoking detection system for CCTV surveillance of
fire exit areas is proposed due to critical safety requirements. The dataset
contains 8,124 images from 20 different scenarios along with 2,708 raw samples
demonstrating low-light areas. We evaluated three advanced object detection
models: YOLOv8, YOLOv11, and YOLOv12, followed by development of a custom model
derived from YOLOv8 with added structures for challenging surveillance
contexts. The proposed model outperformed the others, achieving a recall of
78.90 percent and mAP at 50 of 83.70 percent, delivering optimal object
detection across varied environments. Performance evaluation on multiple edge
devices using multithreaded operations showed the Jetson Xavier NX processed
data at 52 to 97 milliseconds per inference, establishing its suitability for
time-sensitive operations. This system offers a robust and adaptable platform
for monitoring public safety and enabling automatic regulatory compliance.

</details>


### [148] [Separating Knowledge and Perception with Procedural Data](https://arxiv.org/abs/2508.11697)
*Adrián Rodríguez-Muñoz,Manel Baradad,Phillip Isola,Antonio Torralba*

Main category: cs.CV

TL;DR: This paper explores using procedural data to train representation models, achieving competitive results in visual tasks compared to models trained on real-world images.


<details>
  <summary>Details</summary>
Motivation: The study aims to investigate whether representation models trained using procedural data can retain strong performance in visual analysis tasks without relying on real-world image datasets.

Method: Representation models are trained exclusively on procedural data and applied to tasks like visual similarity, classification, and segmentation using a visual memory (a database of image embeddings) without additional training.

Result: The procedural data models achieve performance close to models trained on real-world data, with slight gaps depending on the task, and provide strong zero-shot segmentation abilities.

Conclusion: Despite performance gaps in certain aspects, this work demonstrates the viability of procedural data for competitive representation modeling and identifies areas for improving procedural model representations of objects.

Abstract: We train representation models with procedural data only, and apply them on
visual similarity, classification, and semantic segmentation tasks without
further training by using visual memory -- an explicit database of reference
image embeddings. Unlike prior work on visual memory, our approach achieves
full compartmentalization with respect to all real-world images while retaining
strong performance. Compared to a model trained on Places, our procedural model
performs within $1\%$ on NIGHTS visual similarity, outperforms by $8\%$ and
$15\%$ on CUB200 and Flowers102 fine-grained classification, and is within
$10\%$ on ImageNet-1K classification. It also demonstrates strong zero-shot
segmentation, achieving an $R^2$ on COCO within $10\%$ of the models trained on
real data. Finally, we analyze procedural versus real data models, showing that
parts of the same object have dissimilar representations in procedural models,
resulting in incorrect searches in memory and explaining the remaining
performance gap.

</details>


### [149] [FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis](https://arxiv.org/abs/2508.11721)
*Ke Zou,Jocelyn Hui Lin Goh,Yukun Zhou,Tian Lin,Samantha Min Er Yew,Sahana Srinivasan,Meng Wang,Rui Santos,Gabor M. Somfai,Huazhu Fu,Haoyu Chen,Pearse A. Keane,Ching-Yu Cheng,Yih Chung Tham*

Main category: cs.CV

TL;DR: This paper evaluates and compares individual and fused foundation models (FMs) for ophthalmic and systemic disease detection using retinal images.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address fundamental uncertainties around performance comparisons of foundation models (FMs) in ophthalmology, their applicability across various tasks, and the potential gains from model fusion.

Method: The authors propose FusionFM, a framework involving two fusion strategies and benchmarking four FMs (RETFound, VisionFM, RetiZero, DINORET) using retinal image datasets from multiple countries. Disease prediction tasks include glaucoma, diabetic retinopathy, AMD, diabetes, and hypertension.

Result: DINORET and RetiZero outperformed other models in both ophthalmic and systemic disease prediction. Fusion strategies showed modest improvements for specific diseases but highlighted challenges in systemic disease prediction, particularly hypertension.

Conclusion: The study provides evidence-based insights into ophthalmic model performance, supports model fusion benefits, and emphasizes challenges and pathways to improve systemic disease prediction.

Abstract: Foundation models (FMs) have shown great promise in medical image analysis by
improving generalization across diverse downstream tasks. In ophthalmology,
several FMs have recently emerged, but there is still no clear answer to
fundamental questions: Which FM performs the best? Are they equally good across
different tasks? What if we combine all FMs together? To our knowledge, this is
the first study to systematically evaluate both single and fused ophthalmic
FMs. To address these questions, we propose FusionFM, a comprehensive
evaluation suite, along with two fusion approaches to integrate different
ophthalmic FMs. Our framework covers both ophthalmic disease detection
(glaucoma, diabetic retinopathy, and age-related macular degeneration) and
systemic disease prediction (diabetes and hypertension) based on retinal
imaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM,
RetiZero, and DINORET) using standardized datasets from multiple countries and
evaluated their performance using AUC and F1 metrics. Our results show that
DINORET and RetiZero achieve superior performance in both ophthalmic and
systemic disease tasks, with RetiZero exhibiting stronger generalization on
external datasets. Regarding fusion strategies, the Gating-based approach
provides modest improvements in predicting glaucoma, AMD, and hypertension.
Despite these advances, predicting systemic diseases, especially hypertension
in external cohort remains challenging. These findings provide an
evidence-based evaluation of ophthalmic FMs, highlight the benefits of model
fusion, and point to strategies for enhancing their clinical applicability.

</details>


### [150] [UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction](https://arxiv.org/abs/2508.11728)
*Chunxia Ren,Ning Zhu,Yue Lai,Gui Chen,Ruijie Wang,Yangyi Hu,Suyao Liu,Shuwen Mao,Hong Su,Yu Zhang,Li Xiao*

Main category: cs.CV

TL;DR: UniDCF is a unified framework leveraging multimodal data for precise reconstruction of dentocraniofacial hard tissues, outperforming current methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of current deep learning models in reconstructing dentocraniofacial hard tissue defects, which suffer from poor generalizability and modality-specific trade-offs.

Method: Introduces UniDCF, using multimodal fusion of point clouds and multi-view images, alongside a score-based denoising module for enhanced reconstruction.

Result: Evaluations show geometric precision, structural completeness, and spatial accuracy surpassing state-of-the-art methods, with design time reduced by 99% and clinician acceptability above 94%.

Conclusion: UniDCF offers rapid, automated, and high-quality reconstructions, improving personalized restorative treatments and clinical workflows, while boosting patient outcomes.

Abstract: Dentocraniofacial hard tissue defects profoundly affect patients'
physiological functions, facial aesthetics, and psychological well-being,
posing significant challenges for precise reconstruction. Current deep learning
models are limited to single-tissue scenarios and modality-specific imaging
inputs, resulting in poor generalizability and trade-offs between anatomical
fidelity, computational efficiency, and cross-tissue adaptability. Here we
introduce UniDCF, a unified framework capable of reconstructing multiple
dentocraniofacial hard tissues through multimodal fusion encoding of point
clouds and multi-view images. By leveraging the complementary strengths of each
modality and incorporating a score-based denoising module to refine surface
smoothness, UniDCF overcomes the limitations of prior single-modality
approaches. We curated the largest multimodal dataset, comprising intraoral
scans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated
instances. Evaluations demonstrate that UniDCF outperforms existing
state-of-the-art methods in terms of geometric precision, structural
completeness, and spatial accuracy. Clinical simulations indicate UniDCF
reduces reconstruction design time by 99% and achieves clinician-rated
acceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and
high-fidelity reconstruction, supporting personalized and precise restorative
treatments, streamlining clinical workflows, and enhancing patient outcomes.

</details>


### [151] [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737)
*Shiyin Lu,Yang Li,Yu Xia,Yuwei Hu,Shanshan Zhao,Yanqing Ma,Zhichao Wei,Yinglun Li,Lunhao Duan,Jianshan Zhao,Yuxuan Han,Haijun Li,Wanying Chen,Junke Tang,Chengkun Hou,Zhixing Du,Tianli Zhou,Wenjie Zhang,Huping Ding,Jiahe Li,Wen Li,Gui Hu,Yiliang Gu,Siran Yang,Jiamang Wang,Hailong Sun,Yibo Wang,Hui Sun,Jinlong Huang,Yuping He,Shengze Shi,Weihong Zhang,Guodong Zheng,Junpeng Jiang,Sensen Gao,Yi-Feng Wu,Sijia Chen,Yuhui Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: Ovis2.5 introduces advancements in native-resolution visual perception and multimodal reasoning, improving accuracy, speed, and performance on complex visual tasks in open-source MLLMs.


<details>
  <summary>Details</summary>
Motivation: The development of Ovis2.5 is motivated by the need for better handling of complex visual data in native resolutions without quality degradation, along with enhanced reasoning capabilities for more difficult tasks.

Method: Ovis2.5 employs a five-phase curriculum training, including visual/multimodal pretraining, instruction tuning, and reasoning enhancements, alongside techniques like multimodal data packing and hybrid parallelism to optimize efficiency.

Result: Ovis2.5 outperforms its predecessor Ovis2 and achieves state-of-the-art results for MLLMs in its parameter range, with high accuracy on STEM benchmarks, grounding, video tasks, and complex chart analysis.

Conclusion: Ovis2.5 represents a major step forward in multimodal reasoning and visual perception, offering versatile performance for challenging tasks with scalable models suitable for diverse deployment scenarios.

Abstract: We present Ovis2.5, a successor to Ovis2 designed for native-resolution
visual perception and strong multimodal reasoning. Ovis2.5 integrates a
native-resolution vision transformer that processes images at their native,
variable resolutions, avoiding the degradation from fixed-resolution tiling and
preserving both fine detail and global layout -- crucial for visually dense
content like complex charts. To strengthen reasoning, we train the model to
move beyond linear chain-of-thought and perform reflection -- including
self-checking and revision. This advanced capability is exposed as an optional
"thinking mode" at inference time, allowing users to trade latency for enhanced
accuracy on difficult inputs. The model is trained via a comprehensive
five-phase curriculum that progressively builds its skills. The process begins
with foundational visual and multimodal pretraining, advances through
large-scale instruction tuning, and culminates in alignment and reasoning
enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ
multimodal data packing and hybrid parallelism, yielding a significant
end-to-end speedup. We release two open-source models: Ovis2.5-9B and
Ovis2.5-2B. The latter continues the "small model, big performance" philosophy
of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the
OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a
substantial improvement over its predecessor, Ovis2-8B, and achieving
state-of-the-art results among open-source MLLMs in the sub-40B parameter
range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate
scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong
capabilities on grounding and video tasks, and achieves open-source SOTA at its
scale for complex chart analysis.

</details>


### [152] [VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models](https://arxiv.org/abs/2508.11801)
*Ming Cheng,Tong Wu,Jiazhen Hu,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CV

TL;DR: The paper introduces VideoAVE, the first publicly available video-to-text dataset for Attribute Value Extraction (AVE) in e-commerce, covering 14 domains and 172 attributes, and evaluates the challenges in AVE using state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Existing AVE datasets lack coverage for video-to-text settings, diverse attributes, and publicly available datasets, creating a gap in e-commerce AVE research.

Method: The authors provide a VideoAVE dataset by implementing a CLIP-based Mixture of Experts (CLIP-MoE) filtering system for refining video-product pairs, ensuring dataset quality. They also establish benchmarks with state-of-the-art video vision language models (VLMs) for evaluation.

Result: The resulting dataset contains 224k training and 25k evaluation data. Experimental results indicate that video-to-text AVE tasks remain challenging, especially under open settings.

Conclusion: The research highlights the need for advancements in video vision language models to better leverage temporal information for video-to-text AVE tasks.

Abstract: Attribute Value Extraction (AVE) is important for structuring product
information in e-commerce. However, existing AVE datasets are primarily limited
to text-to-text or image-to-text settings, lacking support for product videos,
diverse attribute coverage, and public availability. To address these gaps, we
introduce VideoAVE, the first publicly available video-to-text e-commerce AVE
dataset across 14 different domains and covering 172 unique attributes. To
ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts
filtering system (CLIP-MoE) to remove the mismatched video-product pairs,
resulting in a refined dataset of 224k training data and 25k evaluation data.
In order to evaluate the usability of the dataset, we further establish a
comprehensive benchmark by evaluating several state-of-the-art video vision
language models (VLMs) under both attribute-conditioned value prediction and
open attribute-value pair extraction tasks. Our results analysis reveals that
video-to-text AVE remains a challenging problem, particularly in open settings,
and there is still room for developing more advanced VLMs capable of leveraging
effective temporal information. The dataset and benchmark code for VideoAVE are
available at: https://github.com/gjiaying/VideoAVE

</details>


### [153] [An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation](https://arxiv.org/abs/2508.11803)
*Azam Nouri*

Main category: cs.CV

TL;DR: The paper explores second-order geometric cues as inputs for an MLP classifier on handwritten character recognition, achieving high accuracy with handcrafted features.


<details>
  <summary>Details</summary>
Motivation: To test whether second-order geometric cues are sufficient for handwritten character recognition, as an alternative to CNNs, which rely on less interpretable patterns.

Method: The study uses three handcrafted feature maps (planar curvature magnitude, curvature sign, and gradient orientation) as inputs to an MLP for classification tasks on the MNIST and EMNIST datasets.

Result: The curvature-orientation MLP achieved 97 percent accuracy on MNIST digits and 89 percent on EMNIST letters, showcasing the effectiveness of these handcrafted features.

Conclusion: Curvature-based representations are highly discriminative for handwritten character recognition and demonstrate that deep learning can leverage a simpler, interpretable feature set effectively.

Abstract: This study investigates whether second-order geometric cues - planar
curvature magnitude, curvature sign, and gradient orientation - are sufficient
on their own to drive a multilayer perceptron (MLP) classifier for handwritten
character recognition (HCR), offering an alternative to convolutional neural
networks (CNNs). Using these three handcrafted feature maps as inputs, our
curvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89
percent on EMNIST letters. These results underscore the discriminative power of
curvature-based representations for handwritten character images and
demonstrate that the advantages of deep learning can be realized even with
interpretable, hand-engineered features.

</details>


### [154] [Labels or Input? Rethinking Augmentation in Multimodal Hate Detection](https://arxiv.org/abs/2508.11808)
*Sahajpreet Singh,Rongxin Ouyang,Subhayan Mukerjee,Kokil Jaidka*

Main category: cs.CV

TL;DR: This paper tackles multimodal hate detection in online content using prompt optimization and targeted data augmentation for improved performance and generalization.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of detecting hate speech in multimodal content, where harmful intent often involves subtle text-image interactions.

Method: Their approach uses two key strategies: (1) systematic experimentation with prompt optimization (including prompt structure and supervision granularity), and (2) a multimodal data augmentation pipeline that rewrites hateful memes to reduce spurious correlations.

Result: Prompt optimization demonstrated improved robustness even for smaller models, with InternVL2 achieving top F1-scores. The data augmentation pipeline generated 2,479 neutral memes, improving classifier performance by reducing bias.

Conclusion: Prompt design and data augmentation are as impactful as model size in enhancing multimodal hate detection, paving the way for trustworthy and context-sensitive systems.

Abstract: The modern web is saturated with multimodal content, intensifying the
challenge of detecting hateful memes, where harmful intent is often conveyed
through subtle interactions between text and image under the guise of humor or
satire. While recent advances in Vision-Language Models (VLMs) show promise,
these models lack support for fine-grained supervision and remain susceptible
to implicit hate speech. In this paper, we present a dual-pronged approach to
improve multimodal hate detection. First, we propose a prompt optimization
framework that systematically varies prompt structure, supervision granularity,
and training modality. We show that prompt design and label scaling both
influence performance, with structured prompts improving robustness even in
small models, and InternVL2 achieving the best F1-scores across binary and
scaled settings. Second, we introduce a multimodal data augmentation pipeline
that generates 2,479 counterfactually neutral memes by isolating and rewriting
the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,
successfully reduces spurious correlations and improves classifier
generalization. Our approaches inspire new directions for building synthetic
data to train robust and fair vision-language models. Our findings demonstrate
that prompt structure and data composition are as critical as model size, and
that targeted augmentation can support more trustworthy and context-sensitive
hate detection.

</details>


### [155] [Towards Understanding 3D Vision: the Role of Gaussian Curvature](https://arxiv.org/abs/2508.11825)
*Sherlon Almeida da Silva,Davi Geiger,Luiz Velho,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: The paper investigates Gaussian curvature's role in 3D surface modeling, providing insights into its benefits in improving current methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods excel in 3D tasks like stereo and depth reconstruction but lack explicit and analyzable 3D geometric models. The study explores Gaussian curvature to address this gap.

Method: The authors analyze Gaussian curvature, utilize the Middlebury stereo dataset, and examine its capabilities for informing 3D surface modeling and reconstruction.

Result: Gaussian curvature offers a sparse description of 3D surfaces, reflects implicit use in current methods, informs geometric priors, and can serve as an unsupervised metric in stereo tasks.

Conclusion: Explicit use of Gaussian curvature can lead to more effective 3D surface modeling, bringing benefits like compactness and geometric-prior informed improvements in reconstruction techniques.

Abstract: Recent advances in computer vision have predominantly relied on data-driven
approaches that leverage deep learning and large-scale datasets. Deep neural
networks have achieved remarkable success in tasks such as stereo matching and
monocular depth reconstruction. However, these methods lack explicit models of
3D geometry that can be directly analyzed, transferred across modalities, or
systematically modified for controlled experimentation. We investigate the role
of Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being
an invariant quantity under change of observers or coordinate systems, we
demonstrate using the Middlebury stereo dataset that it offers: (i) a sparse
and compact description of 3D surfaces, (ii) state-of-the-art monocular and
stereo methods seem to implicitly consider it, but no explicit module of such
use can be extracted, (iii) a form of geometric prior that can inform and
improve 3D surface reconstruction, and (iv) a possible use as an unsupervised
metric for stereo methods.

</details>


### [156] [From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images](https://arxiv.org/abs/2508.11826)
*Dehn Xu,Tim Katzke,Emmanuel Müller*

Main category: cs.CV

TL;DR: The paper explores various image-to-graph transformations for enhancing graph-level anomaly detection using Graph Neural Networks (GNNs).


<details>
  <summary>Details</summary>
Motivation: There is currently no comprehensive comparison of image-to-graph transformation schemes for achieving effective graph-level anomaly detection.

Method: The authors systematically evaluate different graph construction techniques including segmentation schemes, edge construction strategies, and node feature sets using color, texture, and shape descriptors. They then test these transformations on dermoscopic images.

Result: Their experiments demonstrate that color descriptors alone yield competitive results, but combining them with shape and texture features significantly improves anomaly detection performance in various supervision regimes. The best unsupervised setup achieves 0.805 AUC-ROC, which increases to 0.914 with full supervision.

Conclusion: Image-to-graph transformation strategies are critical in enhancing the effectiveness of GNN-based anomaly detection. Combining descriptors and leveraging supervision improves overall detection performance.

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful approach for
graph-based machine learning tasks. Previous work applied GNNs to image-derived
graph representations for various downstream tasks such as classification or
anomaly detection. These transformations include segmenting images, extracting
features from segments, mapping them to nodes, and connecting them. However, to
the best of our knowledge, no study has rigorously compared the effectiveness
of the numerous potential image-to-graph transformation approaches for
GNN-based graph-level anomaly detection (GLAD). In this study, we
systematically evaluate the efficacy of multiple segmentation schemes, edge
construction strategies, and node feature sets based on color, texture, and
shape descriptors to produce suitable image-derived graph representations to
perform graph-level anomaly detection. We conduct extensive experiments on
dermoscopic images using state-of-the-art GLAD models, examining performance
and efficiency in purely unsupervised, weakly supervised, and fully supervised
regimes. Our findings reveal, for example, that color descriptors contribute
the best standalone performance, while incorporating shape and texture features
consistently enhances detection efficacy. In particular, our best unsupervised
configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805
without relying on pretrained backbones like comparable image-based approaches.
With the inclusion of sparse labels, the performance increases substantially to
0.872 and with full supervision to 0.914 AUC-ROC.

</details>


### [157] [Recent Advances in Transformer and Large Language Models for UAV Applications](https://arxiv.org/abs/2508.11834)
*Hamza Kheddar,Yassine Habchi,Mohamed Chahine Ghanem,Mustapha Hemis,Dusit Niyato*

Main category: cs.CV

TL;DR: This review paper categorizes and evaluates Transformer-based models for UAV systems, providing taxonomy, benchmarks, and future directions.


<details>
  <summary>Details</summary>
Motivation: Transformer models are rapidly progressing and enhancing UAV systems' performance in areas like perception and autonomy, necessitating a systematic review to understand their impact.

Method: The paper classifies Transformer architectures in UAVs, compares them using structured tables and benchmarks, and reviews datasets, simulators, and metrics.

Result: Key gaps, challenges, and innovations in Transformer-based UAV applications like precision agriculture and navigation are highlighted.

Conclusion: Researchers and practitioners are provided with a comprehensive framework to push the boundaries of Transformer-driven UAV technologies.

Abstract: The rapid advancement of Transformer-based models has reshaped the landscape
of uncrewed aerial vehicle (UAV) systems by enhancing perception,
decision-making, and autonomy. This review paper systematically categorizes and
evaluates recent developments in Transformer architectures applied to UAVs,
including attention mechanisms, CNN-Transformer hybrids, reinforcement learning
Transformers, and large language models (LLMs). Unlike previous surveys, this
work presents a unified taxonomy of Transformer-based UAV models, highlights
emerging applications such as precision agriculture and autonomous navigation,
and provides comparative analyses through structured tables and performance
benchmarks. The paper also reviews key datasets, simulators, and evaluation
metrics used in the field. Furthermore, it identifies existing gaps in the
literature, outlines critical challenges in computational efficiency and
real-time deployment, and offers future research directions. This comprehensive
synthesis aims to guide researchers and practitioners in understanding and
advancing Transformer-driven UAV technologies.

</details>


### [158] [TSLA: A Task-Specific Learning Adaptation for Semantic Segmentation on Autonomous Vehicles Platform](https://arxiv.org/abs/2508.12279)
*Jun Liu,Zhenglun Kong,Pu Zhao,Weihao Zeng,Hao Tang,Xuan Shen,Changdi Yang,Wenbin Zhang,Geng Yuan,Wei Niu,Xue Lin,Yanzhi Wang*

Main category: cs.CV

TL;DR: This paper focuses on optimizing semantic segmentation networks for autonomous driving platforms, balancing computational costs and precision by tailoring models to hardware constraints and driving scenarios.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving systems require adaptable solutions to meet diverse scenario requirements while addressing the limitations of computational resources in embedded platforms.

Method: The authors propose a three-tier control mechanism for adapting the segmentation network, alongside Bayesian Optimization to explore hyperparameters under computational constraints for scenario-specific tasks.

Result: The approach enables dynamic customization of model components, scaling MACs via Task-Specific Learning Adaptation (TSLA), and achieves improvements in computational efficiency and accuracy.

Conclusion: Dynamic adaptability via TSLA and Bayesian Optimization optimizes both computing power and performance for varying autonomous driving scenarios, enhancing resource allocation and task-specific outcomes.

Abstract: Autonomous driving platforms encounter diverse driving scenarios, each with
varying hardware resources and precision requirements. Given the computational
limitations of embedded devices, it is crucial to consider computing costs when
deploying on target platforms like the NVIDIA\textsuperscript{\textregistered}
DRIVE PX 2. Our objective is to customize the semantic segmentation network
according to the computing power and specific scenarios of autonomous driving
hardware. We implement dynamic adaptability through a three-tier control
mechanism -- width multiplier, classifier depth, and classifier kernel --
allowing fine-grained control over model components based on hardware
constraints and task requirements. This adaptability facilitates broad model
scaling, targeted refinement of the final layers, and scenario-specific
optimization of kernel sizes, leading to improved resource allocation and
performance.
  Additionally, we leverage Bayesian Optimization with surrogate modeling to
efficiently explore hyperparameter spaces under tight computational budgets.
Our approach addresses scenario-specific and task-specific requirements through
automatic parameter search, accommodating the unique computational complexity
and accuracy needs of autonomous driving. It scales its Multiply-Accumulate
Operations (MACs) for Task-Specific Learning Adaptation (TSLA), resulting in
alternative configurations tailored to diverse self-driving tasks. These TSLA
customizations maximize computational capacity and model accuracy, optimizing
hardware utilization.

</details>


### [159] [ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages](https://arxiv.org/abs/2508.11854)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haorang Wang,Matthew Lau,Wenke Lee,Wilian Lunardi,Martin Andreoni,Polo Chau*

Main category: cs.CV

TL;DR: The paper introduces ComplicitSplat, the first attack exploiting 3D Gaussian Splatting (3DGS) techniques to embed viewpoint-specific adversarial content for tampering object detectors without requiring model or architecture knowledge.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the potential vulnerability in 3DGS applications, particularly the safety risk posed by adversaries tampering with efficient novel-view synthesis used in safety-critical tasks such as autonomous navigation.

Method: ComplicitSplat leverages existing 3DGS shading methods to create adversarial content with colors and textures that change based on the viewing angle, targeting object detectors in a black-box manner without requiring access to model architecture or weights.

Result: The experiments demonstrate the attack's success across various object detector models, including single-stage, multi-stage, and transformer-based models, in both real-world and synthetic environments.

Conclusion: The research identifies a novel safety risk in 3DGS applications by presenting the first black-box attack targeting downstream object detectors, emphasizing the need for robust defenses against such threats in mission-critical systems.

Abstract: As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks
for efficient novel-view synthesis from static images, how might an adversary
tamper images to cause harm? We introduce ComplicitSplat, the first attack that
exploits standard 3DGS shading methods to create viewpoint-specific camouflage
- colors and textures that change with viewing angle - to embed adversarial
content in scene objects that are visible only from specific viewpoints and
without requiring access to model architecture or weights. Our extensive
experiments show that ComplicitSplat generalizes to successfully attack a
variety of popular detector - both single-stage, multi-stage, and
transformer-based models on both real-world capture of physical objects and
synthetic scenes. To our knowledge, this is the first black-box attack on
downstream object detectors using 3DGS, exposing a novel safety risk for
applications like autonomous navigation and other mission-critical robotic
systems.

</details>


### [160] [Impact of Clinical Image Quality on Efficient Foundation Model Finetuning](https://arxiv.org/abs/2508.11864)
*Yucheng Tang,Pawel Rajwa,Alexander Ng,Yipei Wang,Wen Yan,Natasha Thorley,Aqua Asif,Clare Allen,Louise Dickinson,Francesco Giganti,Shonit Punwani,Daniel C. Alexander,Veeru Kasivisvanathan,Yipeng Hu*

Main category: cs.CV

TL;DR: The paper examines how image quality impacts the performance and label efficiency of the ProFound foundation model for prostate multiparametric MRI.


<details>
  <summary>Details</summary>
Motivation: The study seeks to understand the role of variable image quality in label-efficient finetuning and its effect on the generalizability of foundation models in medical imaging.

Method: An experimental setup systematically adjusts the ratio of high- and low-quality images in finetuning and evaluation sets to observe their impact on downstream tasks like automated reporting and cancer detection.

Result: The paper finds that mismatches in image quality distribution between finetuning and test sets significantly influence model performance, and a sufficient quantity of high-quality finetuning images is essential.

Conclusion: For effective use of foundation models like ProFound, careful assessment and alignment of image quality in finetuning and deployment data are crucial, emphasizing the importance of quality standards for specific tasks.

Abstract: Foundation models in medical imaging have shown promising label efficiency,
achieving high downstream performance with only a fraction of annotated data.
Here, we evaluate this in prostate multiparametric MRI using ProFound, a
domain-specific vision foundation model pretrained on large-scale prostate MRI
datasets. We investigate how variable image quality affects label-efficient
finetuning by measuring the generalisability of finetuned models. Experiments
systematically vary high-/low-quality image ratios in finetuning and evaluation
sets. Our findings indicate that image quality distribution and its
finetune-and-test mismatch significantly affect model performance. In
particular: a) Varying the ratio of high- to low-quality images between
finetuning and test sets leads to notable differences in downstream
performance; and b) The presence of sufficient high-quality images in the
finetuning set is critical for maintaining strong performance, whilst the
importance of matched finetuning and testing distribution varies between
different downstream tasks, such as automated radiology reporting and prostate
cancer detection.When quality ratios are consistent, finetuning needs far less
labeled data than training from scratch, but label efficiency depends on image
quality distribution. Without enough high-quality finetuning data, pretrained
models may fail to outperform those trained without pretraining. This
highlights the importance of assessing and aligning quality distributions
between finetuning and deployment, and the need for quality standards in
finetuning data for specific downstream tasks. Using ProFound, we show the
value of quantifying image quality in both finetuning and deployment to fully
realise the data and compute efficiency benefits of foundation models.

</details>


### [161] [AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition](https://arxiv.org/abs/2508.11870)
*Ying Huang,Yuanbin Man,Wenqi Jia,Zhengzhong Tu,Junzhou Huang,Miao Yin*

Main category: cs.CV

TL;DR: Proposes AdaRing, a new vision-language fine-tuning framework, using tensor ring decomposition (TRD) for efficient adapter integration with significant parameter reduction.


<details>
  <summary>Details</summary>
Motivation: Improve the efficiency and capacity of adapter-based fine-tuning for vision-language models (VLMs), addressing redundancy and representational limits in existing methods.

Method: AdaRing uses cross-layer tensor ring decomposition (TRD) with low-rank representation to create layer-shared tensor cores and specific slices, combined with rank-driven diverse adapters.

Result: AdaRing achieves state-of-the-art performance on various tasks while reducing training parameters by 90%.

Conclusion: AdaRing is an ultra-light, parameter-efficient fine-tuning framework for VLMs, demonstrating improved efficiency and task performance compared to previous methods.

Abstract: Adapter-based fine-tuning has gained remarkable attention in adapting large
pre-trained vision language models (VLMs) for a wide range of downstream tasks
efficiently. In this paradigm, only the inserted adapters are fine-tuned,
without the need for training the original VLM backbone. Existing works scale
adapters by integrating them into every layer of VLMs to increase the capacity
of adapters. However, these methods face two primary limitations: 1) limited
compression rate due to ignoring cross-layer redundancy, and 2) limited
representational capacity across homogeneous adapters. In this paper, we
propose a novel vision-language fine-tuning framework based on cross-layer
tensor ring decomposition (TRD) with the integration and collaboration of
diverse adapters, called AdaRing, achieving ultra-light parameter-efficient
adaptation of VLMs on various tasks. To remove the high redundancy that exists
among adapters across layers, we exploit the tensor-level low-rankness to
formulate adapters as layer-shared tensor cores and layer-specific slices.
Moreover, guided by generalization-aware fine-tuning, diverse rank-driven
adapters cooperate to handle tasks that require different representations. Our
experiments show that the proposed AdaRing achieves the state-of-the-art
performance while reducing average training parameters by 90%.

</details>


### [162] [EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)
*Wenhui Zhu,Xiwen Chen,Zhipeng Wang,Shao Tang,Sayan Ghosh,Xuanzhao Dong,Rajat Koner,Yalin Wang*

Main category: cs.CV

TL;DR: The paper introduces EVTP-IV, a token pruning method for Instructed Visual Segmentation (IVS), which speeds up inference significantly by reducing token use while maintaining segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the high inference cost of multimodal large language models (MLLMs) in IVS tasks, particularly in handling videos.

Method: The proposed method, EVTP-IV, leverages a k-center inspired approach with spatial integration for visual token pruning, supported by an information-theoretic analysis.

Result: EVTP-IV achieves up to 5X speed-up on video tasks and 3.5X on image tasks with comparable accuracy, using only 20% of the original tokens. It also surpasses state-of-the-art pruning baselines across pruning ratios.

Conclusion: The developed approach offers a computationally efficient solution for IVS tasks, reducing token usage substantially without compromising performance, particularly in challenging video environments.

Abstract: Instructed Visual Segmentation (IVS) tasks require segmenting objects in
images or videos based on natural language instructions. While recent
multimodal large language models (MLLMs) have achieved strong performance on
IVS, their inference cost remains a major bottleneck, particularly in video. We
empirically analyze visual token sampling in MLLMs and observe a strong
correlation between subset token coverage and segmentation performance. This
motivates our design of a simple and effective token pruning method that
selects a compact yet spatially representative subset of tokens to accelerate
inference. In this paper, we introduce a novel visual token pruning method for
IVS, called EVTP-IV, which builds upon the k-center by integrating spatial
information to ensure better coverage. We further provide an
information-theoretic analysis to support our design. Experiments on standard
IVS benchmarks show that our method achieves up to 5X speed-up on video tasks
and 3.5X on image tasks, while maintaining comparable accuracy using only 20%
of the tokens. Our method also consistently outperforms state-of-the-art
pruning baselines under varying pruning ratios.

</details>


### [163] [Large Kernel Modulation Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11893)
*Quanwei Hu,Yinggan Tang,Xuguang Zhang*

Main category: cs.CV

TL;DR: LKMN, a CNN-based model, improves image super-resolution tasks by addressing the trade-off between performance and latency, utilizing advanced techniques like EPLKB and CGFN for non-local feature extraction and efficient processing.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of balancing performance and latency in image super-resolution tasks for resource-constrained scenarios.

Method: Introducing the LKMN model with Enhanced Partial Large Kernel Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN) for efficient non-local feature extraction and feature fusion.

Result: LKMN outperforms SOTA lightweight SR models, achieving higher PSNR scores and significantly faster inference speeds, with a notable improvement on the Manga109 dataset.

Conclusion: The LKMN model successfully balances quality and efficiency in lightweight super-resolution while introducing innovative techniques for CNN-based non-local modeling.

Abstract: Image super-resolution (SR) in resource-constrained scenarios demands
lightweight models balancing performance and latency. Convolutional neural
networks (CNNs) offer low latency but lack non-local feature capture, while
Transformers excel at non-local modeling yet suffer slow inference. To address
this trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure
CNN-based model. LKMN has two core components: Enhanced Partial Large Kernel
Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes
channel shuffle to boost inter-channel interaction, incorporates channel
attention to focus on key information, and applies large kernel strip
convolutions on partial channels for non-local feature extraction with reduced
complexity. The CGFN dynamically adjusts discrepancies between input, local,
and non-local features via a learnable scaling factor, then employs a
cross-gate strategy to modulate and fuse these features, enhancing their
complementarity. Extensive experiments demonstrate that our method outperforms
existing state-of-the-art (SOTA) lightweight SR models while balancing quality
and efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over
DAT-light on the Manga109 dataset at $\times$4 upscale, with nearly $\times$4.8
times faster. Codes are in the supplementary materials. The code is available
at https://github.com/Supereeeee/LKMN.

</details>


### [164] [A Sobel-Gradient MLP Baseline for Handwritten Character Recognition](https://arxiv.org/abs/2508.11902)
*Azam Nouri*

Main category: cs.CV

TL;DR: The paper investigates whether first-order edge maps (Sobel operator outputs) are adequate for training a dense MLP to recognize handwritten characters, achieving near-CNN accuracy levels.


<details>
  <summary>Details</summary>
Motivation: The authors aim to determine if the simplicity of edge detection using Sobel derivatives can replace complex CNN architectures for handwritten character recognition while maintaining competitive accuracy.

Method: The method involves using horizontal and vertical Sobel derivatives as inputs to train an all-dense MLP on MNIST and EMNIST Letters datasets.

Result: The MLP achieves 98% accuracy on MNIST digits and 92% accuracy on EMNIST letters using just edge-map inputs, approaching the performance of CNNs.

Conclusion: First-order gradients capture critical class-discriminative information, proving edge-aware MLPs to be a viable and simpler alternative to CNNs for handwritten character recognition tasks.

Abstract: We revisit the classical Sobel operator to ask a simple question: Are
first-order edge maps sufficient to drive an all-dense multilayer perceptron
(MLP) for handwritten character recognition (HCR), as an alternative to
convolutional neural networks (CNNs)? Using only horizontal and vertical Sobel
derivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its
extreme simplicity, the resulting network reaches 98% accuracy on MNIST digits
and 92% on EMNIST letters -- approaching CNNs while offering a smaller memory
footprint and transparent features. Our findings highlight that much of the
class-discriminative information in handwritten character images is already
captured by first-order gradients, making edge-aware MLPs a compelling option
for HCR.

</details>


### [165] [OVG-HQ: Online Video Grounding with Hybrid-modal Queries](https://arxiv.org/abs/2508.11903)
*Runhao Zeng,Jiaqi Mao,Minghao Lai,Minh Hieu Phan,Yanjie Dong,Wei Wang,Qi Chen,Xiping Hu*

Main category: cs.CV

TL;DR: This paper introduces a novel framework called OVG-HQ-Unify for Online Video Grounding with Hybrid-modal Queries, addressing challenges in online segment localization using multimodal queries.


<details>
  <summary>Details</summary>
Motivation: Video grounding often struggles in scenarios involving streaming video or visual-cue-based queries, necessitating a solution for hybrid-modal inputs in online settings.

Method: The OVG-HQ-Unify framework utilizes a Parametric Memory Block for knowledge retention and a cross-modal distillation strategy to improve learning of weaker modalities.

Result: The framework demonstrates superior performance in hybrid-modal online video grounding and introduces novel online evaluation metrics such as oR@n and omAP.

Conclusion: OVG-HQ-Unify offers a robust method to tackle the challenges of modality imbalance and limited context in online video processing, enhancing prediction accuracy and efficiency.

Abstract: Video grounding (VG) task focuses on locating specific moments in a video
based on a query, usually in text form. However, traditional VG struggles with
some scenarios like streaming video or queries using visual cues. To fill this
gap, we present a new task named Online Video Grounding with Hybrid-modal
Queries (OVG-HQ), which enables online segment localization using text, images,
video segments, and their combinations. This task poses two new challenges:
limited context in online settings and modality imbalance during training,
where dominant modalities overshadow weaker ones. To address these, we propose
OVG-HQ-Unify, a unified framework featuring a Parametric Memory Block (PMB)
that retain previously learned knowledge to enhance current decision and a
cross-modal distillation strategy that guides the learning of non-dominant
modalities. This design enables a single model to effectively handle
hybrid-modal queries. Due to the lack of suitable datasets, we construct
QVHighlights-Unify, an expanded dataset with multi-modal queries. Besides,
since offline metrics overlook prediction timeliness, we adapt them to the
online setting, introducing oR@n, IoU=m, and online mean Average Precision
(omAP) to evaluate both accuracy and efficiency. Experiments show that our
OVG-HQ-Unify outperforms existing models, offering a robust solution for
online, hybrid-modal video grounding. Source code and datasets are available at
https://github.com/maojiaqi2324/OVG-HQ.

</details>


### [166] [SafeCtrl: Region-Based Safety Control for Text-to-Image Diffusion via Detect-Then-Suppress](https://arxiv.org/abs/2508.11904)
*Lingyun Zhang,Yu Xie,Yanwei Fu,Ping Chen*

Main category: cs.CV

TL;DR: SafeCtrl introduces a novel approach to address harmful content in text-to-image generative models. Using a detect-then-suppress method combined with Direct Preference Optimization, it provides safer and more natural image generation without compromising fidelity.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of current safety methods (e.g., prompt rewriting & concept replacement) in text-to-image models that often sacrifice fidelity or lead to semantic mismatches.

Method: Development of SafeCtrl, a suppression-based plugin employing a detect-then-suppress paradigm, trained via Direct Preference Optimization for precise localization and nuanced suppression of unsafe content.

Result: SafeCtrl achieves superior performance compared to state-of-the-art methods in ensuring safety while preserving the overall quality and fidelity of generated images.

Conclusion: Decoupling control with suppression-based strategies offers a scalable and effective model design for safer and context-aware generative systems.

Abstract: The widespread deployment of text-to-image models is challenged by their
potential to generate harmful content. While existing safety methods, such as
prompt rewriting or model fine-tuning, provide valuable interventions, they
often introduce a trade-off between safety and fidelity. Recent
localization-based approaches have shown promise, yet their reliance on
explicit ``concept replacement" can sometimes lead to semantic incongruity. To
address these limitations, we explore a more flexible detect-then-suppress
paradigm. We introduce SafeCtrl, a lightweight, non-intrusive plugin that first
precisely localizes unsafe content. Instead of performing a hard A-to-B
substitution, SafeCtrl then suppresses the harmful semantics, allowing the
generative process to naturally and coherently resolve into a safe,
context-aware alternative. A key aspect of our work is a novel training
strategy using Direct Preference Optimization (DPO). We leverage readily
available, image-level preference data to train our module, enabling it to
learn nuanced suppression behaviors and perform region-guided interventions at
inference without requiring costly, pixel-level annotations. Extensive
experiments show that SafeCtrl significantly outperforms state-of-the-art
methods in both safety efficacy and fidelity preservation. Our findings suggest
that decoupled, suppression-based control is a highly effective and scalable
direction for building more responsible generative models.

</details>


### [167] [TimeSenCLIP: A Vision-Language Model for Remote Sensing Using Single-Pixel Time Series](https://arxiv.org/abs/2508.11919)
*Pallavi Jain,Diego Marcos,Dino Ienco,Roberto Interdonato,Tristan Berchoux*

Main category: cs.CV

TL;DR: TimeSenCLIP is a lightweight framework that uses single-pixel inputs with temporal and spectral data for efficient land-use and land-cover classification, minimizing reliance on text-based training.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of high computational cost due to large spatial tiles and limited availability of text-based supervision in current vision-language models for remote sensing tasks.

Method: TimeSenCLIP leverages temporal and spectral information from Sentinel-2 imagery and cross-view learning with geo-tagged ground-level photos to classify LULC and ecosystem types without heavy reliance on caption-based training.

Result: The framework demonstrated that single-pixel inputs, enriched with spectral and temporal cues, are sufficient for accurate classification tasks including LULC, crop type, and ecosystem type, using the LUCAS and Sen4Map datasets.

Conclusion: TimeSenCLIP offers a scalable, efficient, and lightweight alternative for thematic mapping in large-scale remote sensing applications, reducing computational resources and training data constraints.

Abstract: Vision-language models have shown significant promise in remote sensing
applications, particularly for land-use and land-cover (LULC) via zero-shot
classification and retrieval. However, current approaches face two key
challenges: reliance on large spatial tiles that increase computational cost,
and dependence on text-based supervision, which is often not readily available.
In this work, we present TimeSenCLIP, a lightweight framework that reevaluate
the role of spatial context by evaluating the effectiveness of a single pixel
by leveraging its temporal and spectral dimensions, for classifying LULC and
ecosystem types. By leveraging spectral and temporal information from
Sentinel-2 imagery and cross-view learning with geo-tagged ground-level photos,
we minimises the need for caption-based training while preserving semantic
alignment between overhead (satellite) and ground perspectives. Our approach is
grounded in the LUCAS and Sen4Map datasets, and evaluated on classification
tasks including LULC, crop type, and ecosystem type. We demonstrate that single
pixel inputs, when combined with temporal and spectral cues, are sufficient for
thematic mapping, offering a scalable and efficient alternative for large-scale
remote sensing applications. Code is available at
https://github.com/pallavijain-pj/TimeSenCLIP

</details>


### [168] [Assessment of Using Synthetic Data in Brain Tumor Segmentation](https://arxiv.org/abs/2508.11922)
*Aditi Jahagirdar,Sameer Joshi*

Main category: cs.CV

TL;DR: Synthetic MRI data, generated by GANs, can be used alongside real data to train U-Net for brain tumor segmentation, improving boundary delineation but not region-wise accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual segmentation of brain tumors in MRI is difficult due to tumor heterogeneity, limited annotated data, and class imbalance in datasets.

Method: The study uses a pre-trained GAN to generate synthetic MRI data and explores the effect of hybrid datasets (mixtures of real and synthetic data) on U-Net segmentation performance.

Result: Hybrid datasets showed improved tumor boundary delineation with 40% real and 60% synthetic data, but region-specific accuracy for tumor core and enhancing tumors remained lower due to class imbalance.

Conclusion: Synthetic data can augment brain tumor segmentation but requires addressing challenges like class imbalance and consistency in future research.

Abstract: Manual brain tumor segmentation from MRI scans is challenging due to tumor
heterogeneity, scarcity of annotated data, and class imbalance in medical
imaging datasets. Synthetic data generated by generative models has the
potential to mitigate these issues by improving dataset diversity. This study
investigates, as a proof of concept, the impact of incorporating synthetic MRI
data, generated using a pre-trained GAN model, into training a U-Net
segmentation network. Experiments were conducted using real data from the BraTS
2020 dataset, synthetic data generated with the medigan library, and hybrid
datasets combining real and synthetic samples in varying proportions. While
overall quantitative performance (Dice coefficient, IoU, precision, recall,
accuracy) was comparable between real-only and hybrid-trained models,
qualitative inspection suggested that hybrid datasets, particularly with 40%
real and 60% synthetic data, improved whole tumor boundary delineation.
However, region-wise accuracy for the tumor core and the enhancing tumor
remained lower, indicating a persistent class imbalance. The findings support
the feasibility of synthetic data as an augmentation strategy for brain tumor
segmentation, while highlighting the need for larger-scale experiments,
volumetric data consistency, and mitigating class imbalance in future work.

</details>


### [169] [Deep Learning For Point Cloud Denoising: A Survey](https://arxiv.org/abs/2508.11932)
*Chengwei Zhang,Xueyi Zhang,Mingrui Lao,Tao Jiang,Xinhao Xu,Wenjie Li,Fubo Zhang,Longyong Chen*

Main category: cs.CV

TL;DR: This paper reviews deep learning-based point cloud denoising (PCD) methods, identifies key challenges, proposes a taxonomy, and outlines future directions for the field.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of a systematic, comprehensive survey summarizing developments in DL-based PCD, which is crucial for improving downstream task performance in noisy point cloud environments.

Method: The paper formulates PCD as a two-step process of outlier removal and surface noise restoration. It also compares and analyzes existing DL-based methods using a taxonomy tailored to denoising tasks.

Result: The paper provides a detailed summary of progress in DL-based PCD, highlighting similarities, differences, and advantages among methods, while addressing challenges in the field.

Conclusion: DL-based PCD models are effective at surpassing traditional methods, but research gaps and limitations persist. The taxonomy and future directions proposed in this paper aim to guide further advancements in the field.

Abstract: Real-world environment-derived point clouds invariably exhibit noise across
varying modalities and intensities. Hence, point cloud denoising (PCD) is
essential as a preprocessing step to improve downstream task performance. Deep
learning (DL)-based PCD models, known for their strong representation
capabilities and flexible architectures, have surpassed traditional methods in
denoising performance. To our best knowledge, despite recent advances in
performance, no comprehensive survey systematically summarizes the developments
of DL-based PCD. To fill the gap, this paper seeks to identify key challenges
in DL-based PCD, summarizes the main contributions of existing methods, and
proposes a taxonomy tailored to denoising tasks. To achieve this goal, we
formulate PCD as a two-step process: outlier removal and surface noise
restoration, encompassing most scenarios and requirements of PCD. Additionally,
we compare methods in terms of similarities, differences, and respective
advantages. Finally, we discuss research limitations and future directions,
offering insights for further advancements in PCD.

</details>


### [170] [DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects](https://arxiv.org/abs/2508.11950)
*Tingbang Liang,Yixin Zeng,Jiatong Xie,Boyu Zhou*

Main category: cs.CV

TL;DR: DynamicPose is a retraining-free 6D pose tracking framework designed for robust tracking in scenarios involving fast movement of both cameras and objects.


<details>
  <summary>Details</summary>
Motivation: To address the significant performance deterioration in prior methods during fast and simultaneous motion of cameras and objects.

Method: DynamicPose integrates three components: visual-inertial odometry for camera motion shifts, depth-informed 2D tracker for object translation errors, and VIO-guided Kalman filter for object rotation prediction and pose refinement, forming a closed-loop system.

Result: The proposed method demonstrated real-time effectiveness and robustness in both simulations and real-world experiments for 6D pose tracking in dynamic scenarios.

Conclusion: DynamicPose successfully improves pose tracking robustness without the need for retraining, ensuring accuracy in challenging fast-motion environments.

Abstract: We present DynamicPose, a retraining-free 6D pose tracking framework that
improves tracking robustness in fast-moving camera and object scenarios.
Previous work is mainly applicable to static or quasi-static scenes, and its
performance significantly deteriorates when both the object and the camera move
rapidly. To overcome these challenges, we propose three synergistic components:
(1) A visual-inertial odometry compensates for the shift in the Region of
Interest (ROI) caused by camera motion; (2) A depth-informed 2D tracker
corrects ROI deviations caused by large object translation; (3) A VIO-guided
Kalman filter predicts object rotation, generates multiple candidate poses, and
then obtains the final pose by hierarchical refinement. The 6D pose tracking
results guide subsequent 2D tracking and Kalman filter updates, forming a
closed-loop system that ensures accurate pose initialization and precise pose
tracking. Simulation and real-world experiments demonstrate the effectiveness
of our method, achieving real-time and robust 6D pose tracking for fast-moving
cameras and objects.

</details>


### [171] [Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection](https://arxiv.org/abs/2508.11951)
*Hao Peng,Hong Sang,Yajing Ma,Ping Qiu,Chao Ji*

Main category: cs.CV

TL;DR: This paper presents methods for lightweight and efficient multi-scale feature learning for point cloud object detection using knowledge distillation and transferable feature embedding.


<details>
  <summary>Details</summary>
Motivation: Multi-scale features are essential for accurate point cloud object detection but often require high computational resources and complex layer designs, limiting their application in lightweight models.

Method: The method uses knowledge distillation to approximate multi-scale features from a single neighborhood while introducing a transferable feature mechanism using class-aware statistics to compensate for feature diversity loss. A central weighted intersection over union is also proposed for improved localization.

Result: Experiments on public datasets confirm the method's effectiveness in achieving high detection accuracy while reducing computational costs.

Conclusion: The proposed approach enables efficient and accurate multi-scale feature learning for point cloud detection, offering benefits for scenarios with limited computational resources.

Abstract: This paper investigates multi-scale feature approximation and transferable
features for object detection from point clouds. Multi-scale features are
critical for object detection from point clouds. However, multi-scale feature
learning usually involves multiple neighborhood searches and scale-aware
layers, which can hinder efforts to achieve lightweight models and may not be
conducive to research constrained by limited computational resources. This
paper approximates point-based multi-scale features from a single neighborhood
based on knowledge distillation. To compensate for the loss of constructive
diversity in a single neighborhood, this paper designs a transferable feature
embedding mechanism. Specifically, class-aware statistics are employed as
transferable features given the small computational cost. In addition, this
paper introduces the central weighted intersection over union for localization
to alleviate the misalignment brought by the center offset in optimization.
Note that the method presented in this paper saves computational costs.
Extensive experiments on public datasets demonstrate the effectiveness of the
proposed method.

</details>


### [172] [UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding](https://arxiv.org/abs/2508.11952)
*Yueming Xu,Jiahui Zhang,Ze Huang,Yurui Chen,Yanpeng Zhou,Zhenyu Chen,Yu-Jie Yuan,Pengxiang Xia,Guowei Huang,Xinyue Cai,Zhongang Qi,Xingyue Quan,Jianye Hao,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: This paper presents UniUGG, a unified framework for 3D modality understanding and generation using latent diffusion models and LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of integrating 3D tasks into unified architectures for understanding and generating visual content.

Method: The framework uses a spatial decoder with latent diffusion modeling for 3D generation and a geometric-semantic strategy for pre-training vision encoders.

Result: Experiments show superior performance in visual representation, spatial understanding, and 3D generation.

Conclusion: UniUGG successfully integrates spatial decoding and semantic-geometric learning to unify 3D understanding and generation tasks, advancing research in 3D modalities.

Abstract: Despite the impressive progress on understanding and generating images shown
by the recent unified architectures, the integration of 3D tasks remains
challenging and largely unexplored. In this paper, we introduce UniUGG, the
first unified understanding and generation framework for 3D modalities. Our
unified framework employs an LLM to comprehend and decode sentences and 3D
representations. At its core, we propose a spatial decoder leveraging a latent
diffusion model to generate high-quality 3D representations. This allows for
the generation and imagination of 3D scenes based on a reference image and an
arbitrary view transformation, while remaining supports for spatial visual
question answering (VQA) tasks. Additionally, we propose a geometric-semantic
learning strategy to pretrain the vision encoder. This design jointly captures
the input's semantic and geometric cues, enhancing both spatial understanding
and generation. Extensive experimental results demonstrate the superiority of
our method in visual representation, spatial understanding, and 3D generation.
The source code will be released upon paper acceptance.

</details>


### [173] [SAMDWICH: Moment-aware Video-text Alignment for Referring Video Object Segmentation](https://arxiv.org/abs/2508.11955)
*Seunghun Lee,Jiwan Seo,Jeonghoon Kim,Siwon Kim,Haeun Yun,Hyogyeong Jeon,Wonhyeok Choi,Jaehoon Jeong,Zane Durante,Sang Hyun Park,Sunghoon Im*

Main category: cs.CV

TL;DR: The paper introduces SAMDWICH, a framework for Referring Video Object Segmentation (RVOS), which uses moment-aware supervision to improve accuracy in object tracking and segmentation based on textual expressions.


<details>
  <summary>Details</summary>
Motivation: Existing RVOS methods face challenges in aligning video-text information due to indiscriminate frame sampling and training on irrelevant objects, leading to semantic misalignment.

Method: The authors propose SAMDWICH and a dataset MeViS-M, with Moment-guided Dual-path Propagation (MDP) for moment-aware propagation and Object-level Selective Supervision (OSS) for semantically aligned object training.

Result: The framework significantly enhances referential understanding and outperforms prior methods, achieving state-of-the-art performance on the MeViS benchmark.

Conclusion: SAMDWICH effectively addresses semantic misalignment in RVOS by leveraging temporally-aligned supervision, showing promise in handling complex scenarios with diverse language expressions.

Abstract: Referring Video Object Segmentation (RVOS) aims to segment and track objects
in videos based on natural language expressions, requiring precise alignment
between visual content and textual queries. However, existing methods often
suffer from semantic misalignment, largely due to indiscriminate frame sampling
and supervision of all visible objects during training -- regardless of their
actual relevance to the expression. To address this, we introduce a
moment-aware RVOS framework named SAMDWICH, along with a newly annotated
dataset, MeViS-M, built upon the challenging MeViS benchmark. We manually
annotate temporal moments indicating when each object is referred to by the
expression, enabling semantically grounded supervision that strengthens
video-text alignment. SAMDWICH leverages these aligned text-to-clip pairs to
guide training, significantly enhancing referential understanding. Building
upon this framework, we propose Moment-guided Dual-path Propagation (MDP), a
moment-aware propagation strategy that improves both object grounding and
tracking by training on both relevant and irrelevant frames through a
moment-centric memory mechanism. In addition, we introduce Object-level
Selective Supervision (OSS), an object-level filtering strategy that supervises
only the objects temporally aligned with the expression in each training clip.
This selective supervision reduces semantic noise and reinforces
language-conditioned learning. Extensive experiments show that SAMDWICH
achieves state-of-the-art performance on challenging MeViS benchmark,
particularly excelling in complex scenarios involving diverse expressions.

</details>


### [174] [PEdger++: Practical Edge Detection via Assembling Cross Information](https://arxiv.org/abs/2508.11961)
*Yuanbin Fu,Liang Li,Xiaojie Guo*

Main category: cs.CV

TL;DR: The paper introduces PEdger++, a framework for edge detection that balances high accuracy and low computational complexity, addressing the limitations of deep learning models on resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the challenge of efficiently capturing discriminative features for edge detection without relying on large, computationally expensive models, expanding applicability across diverse devices.

Method: PEdger++ uses a collaborative learning framework that incorporates cross-information from heterogeneous architectures, various training moments, and multiple parameter samplings to enhance edge detection accuracy and efficiency.

Result: Experiments on datasets like BSDS500, NYUD, and Multicue confirm the enhanced accuracy and efficiency of PEdger++ compared to existing methods, with versions adaptable to different computational requirements.

Conclusion: PEdger++ successfully balances edge detection accuracy and computational efficiency, making it suitable for deployment across devices with varied resource capacities.

Abstract: Edge detection serves as a critical foundation for numerous computer vision
applications, including object detection, semantic segmentation, and image
editing, by extracting essential structural cues that define object boundaries
and salient edges. To be viable for broad deployment across devices with
varying computational capacities, edge detectors shall balance high accuracy
with low computational complexity. While deep learning has evidently improved
accuracy, they often suffer from high computational costs, limiting their
applicability on resource-constrained devices. This paper addresses the
challenge of achieving that balance: \textit{i.e.}, {how to efficiently capture
discriminative features without relying on large-size and sophisticated
models}. We propose PEdger++, a collaborative learning framework designed to
reduce computational costs and model sizes while improving edge detection
accuracy. The core principle of our PEdger++ is that cross-information derived
from heterogeneous architectures, diverse training moments, and multiple
parameter samplings, is beneficial to enhance learning from an ensemble
perspective. Extensive experimental results on the BSDS500, NYUD and Multicue
datasets demonstrate the effectiveness of our approach, both quantitatively and
qualitatively, showing clear improvements over existing methods. We also
provide multiple versions of the model with varying computational requirements,
highlighting PEdger++'s adaptability with respect to different resource
constraints. Codes are accessible at
https://github.com/ForawardStar/EdgeDetectionviaPEdgerPlus/.

</details>


### [175] [Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis](https://arxiv.org/abs/2508.11988)
*Nicolas Mastropasqua,Ignacio Bugueno-Cordova,Rodrigo Verschae,Daniel Acevedo,Pablo Negri,Maria E. Buemi*

Main category: cs.CV

TL;DR: The paper introduces a novel dataset combining RGB and event cameras to improve micro-expression analysis under various lighting conditions, achieving promising accuracy and image reconstruction results.


<details>
  <summary>Details</summary>
Motivation: Current RGB cameras struggle with subtle facial movements due to limitations like motion blur and low temporal resolution.

Method: A dataset combining synchronized RGB and event cameras was employed, alongside Spiking Neural Networks for micro-expression classification and Conditional Variational Autoencoders for frame reconstruction.

Result: The event-based approach significantly outperformed RGB inputs, achieving 51.23% classification accuracy versus 23.12%, and high-quality frame reconstruction metrics (SSIM=0.8513, PSNR=26.89 dB).

Conclusion: Event cameras enhance micro-expression recognition in terms of accuracy and image reconstruction quality, demonstrating their potential in related domains.

Abstract: Micro-expression analysis has applications in domains such as Human-Robot
Interaction and Driver Monitoring Systems. Accurately capturing subtle and fast
facial movements remains difficult when relying solely on RGB cameras, due to
limitations in temporal resolution and sensitivity to motion blur. Event
cameras offer an alternative, with microsecond-level precision, high dynamic
range, and low latency. However, public datasets featuring event-based
recordings of Action Units are still scarce. In this work, we introduce a
novel, preliminary multi-resolution and multi-modal micro-expression dataset
recorded with synchronized RGB and event cameras under variable lighting
conditions. Two baseline tasks are evaluated to explore the spatial-temporal
dynamics of micro-expressions: Action Unit classification using Spiking Neural
Networks (51.23\% accuracy with events vs. 23.12\% with RGB), and frame
reconstruction using Conditional Variational Autoencoders, achieving SSIM =
0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising
results show that event-based data can be used for micro-expression recognition
and frame reconstruction.

</details>


### [176] [MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2508.11999)
*Daoze Zhang,Zhanheng Nie,Jianyu Liu,Chenghan Fu,Wanxian Guan,Yuan Gao,Jun Song,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: This paper introduces MOON, a generative multimodal language model for product representation learning, addressing multimodality, background noise, and lack of standardized benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current dual-flow architectures struggle with the many-to-one alignment of product images and texts. The authors identify the potential of generative multimodal language models for improving product representation learning.

Method: The MOON model utilizes a guided Mixture-of-Experts (MoE) for aspect-aware modeling, detects semantic regions in product images to reduce noise, and employs a specialized negative sampling strategy. A multimodal benchmark (MBE) is also released.

Result: MOON demonstrates strong zero-shot performance on the MBE benchmark and public datasets, excelling in tasks such as cross-modal retrieval, classification, and attribute prediction.

Conclusion: MOON and its accompanying benchmark advance product understanding and demonstrate effective generalization across tasks, setting a foundation for generative multimodal language models in e-commerce research.

Abstract: With the rapid advancement of e-commerce, exploring general representations
rather than task-specific ones has attracted increasing research attention. For
product understanding, although existing discriminative dual-flow architectures
drive progress in this field, they inherently struggle to model the many-to-one
alignment between multiple images and texts of products. Therefore, we argue
that generative Multimodal Large Language Models (MLLMs) hold significant
potential for improving product representation learning. Nevertheless,
achieving this goal still remains non-trivial due to several key challenges:
the lack of multimodal and aspect-aware modeling modules in typical LLMs; the
common presence of background noise in product images; and the absence of a
standard benchmark for evaluation. To address these issues, we propose the
first generative MLLM-based model named MOON for product representation
learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for
targeted modeling of multimodal and aspect-specific product content; (2)
effectively detects core semantic regions in product images to mitigate the
distraction and interference caused by background noise; and (3) introduces the
specialized negative sampling strategy to increase the difficulty and diversity
of negative samples. In addition, we release a large-scale multimodal benchmark
MBE for various product understanding tasks. Experimentally, our model
demonstrates competitive zero-shot performance on both our benchmark and the
public dataset, showcasing strong generalization across various downstream
tasks, including cross-modal retrieval, product classification, and attribute
prediction. Furthermore, the case study and visualization illustrate the
effectiveness of MOON for product understanding.

</details>


### [177] [InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2508.12015)
*Hongyuan Liu,Haochen Yu,Jianfei Jiang,Qiankun Liu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: The paper introduces InstDrive, an instance-aware 3D Gaussian Splatting framework for reconstructing dynamic driving scenes in 3D with instance-level understanding.


<details>
  <summary>Details</summary>
Motivation: Existing methods for reconstructing driving scenes either unify all background elements, limiting functionality, or rely on complex methodologies suited only for indoor scenes. There is a need for an effective framework for outdoor driving scenarios with instance-level understanding.

Method: InstDrive uses masks generated by SAM as pseudo ground-truth and employs contrastive loss and pseudo-supervised objectives for 2D feature learning. It incorporates a voxel-based loss and lightweight codebook at the 3D level to maintain instance-level identities without preprocessing or complex pipelines.

Result: InstDrive achieves 3D instance segmentation for dynamic, open-world driving scenes, validated through both quantitative and qualitative experiments.

Conclusion: InstDrive is the first framework successfully achieving instance-aware 3D reconstruction for outdoor driving scenarios, offering improved functionality and flexibility in scene understanding.

Abstract: Reconstructing dynamic driving scenes from dashcam videos has attracted
increasing attention due to its significance in autonomous driving and scene
understanding. While recent advances have made impressive progress, most
methods still unify all background elements into a single representation,
hindering both instance-level understanding and flexible scene editing. Some
approaches attempt to lift 2D segmentation into 3D space, but often rely on
pre-processed instance IDs or complex pipelines to map continuous features to
discrete identities. Moreover, these methods are typically designed for indoor
scenes with rich viewpoints, making them less applicable to outdoor driving
scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian
Splatting framework tailored for the interactive reconstruction of dynamic
driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D
feature learning via contrastive loss and pseudo-supervised objectives. At the
3D level, we introduce regularization to implicitly encode instance identities
and enforce consistency through a voxel-based loss. A lightweight static
codebook further bridges continuous features and discrete identities without
requiring data pre-processing or complex optimization. Quantitative and
qualitative experiments demonstrate the effectiveness of InstDrive, and to the
best of our knowledge, it is the first framework to achieve 3D instance
segmentation in dynamic, open-world driving scenes.More visualizations are
available at our project page.

</details>


### [178] [WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements](https://arxiv.org/abs/2508.12023)
*Durgesh Kumar Singh,Qing Cao,Sarina Thomas,Ahcène Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: This paper introduces WiseLVAM, a fully automated framework for LV linear measurements in echocardiography that minimizes errors by combining structure and motion-awareness.


<details>
  <summary>Details</summary>
Motivation: Accurate LV measurements in echocardiography are critical, but current automated methods suffer from errors due to small shifts in landmark predictions, affecting clinical reliability.

Method: WiseLVAM builds upon a semi-automatic method by adding a contour-aware scanline placement using a weakly supervised B-mode landmark detector to infer the LV long axis and basal level. It integrates B-mode structural and AMM motion data for automation.

Result: The proposed framework allows for fully automated LV measurements while remaining adaptable for manual adjustments, improving performance and aligning with clinical requirements.

Conclusion: WiseLVAM presents a robust, practical solution by enhancing measurement accuracy and adapting to clinical workflows, advancing the automation of echocardiographic practices.

Abstract: Clinical guidelines recommend performing left ventricular (LV) linear
measurements in B-mode echocardiographic images at the basal level -- typically
at the mitral valve leaflet tips -- and aligned perpendicular to the LV long
axis along a virtual scanline (SL). However, most automated methods estimate
landmarks directly from B-mode images for the measurement task, where even
small shifts in predicted points along the LV walls can lead to significant
measurement errors, reducing their clinical reliability. A recent
semi-automatic method, EnLVAM, addresses this limitation by constraining
landmark prediction to a clinician-defined SL and training on generated
Anatomical Motion Mode (AMM) images to predict LV landmarks along the same. To
enable full automation, a contour-aware SL placement approach is proposed in
this work, in which the LV contour is estimated using a weakly supervised
B-mode landmark detector. SL placement is then performed by inferring the LV
long axis and the basal level-mimicking clinical guidelines. Building on this
foundation, we introduce \textit{WiseLVAM} -- a novel, fully automated yet
manually adaptable framework for automatically placing the SL and then
automatically performing the LV linear measurements in the AMM mode.
\textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the
motion-awareness from AMM mode to enhance robustness and accuracy with the
potential to provide a practical solution for the routine clinical application.

</details>


### [179] [Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering](https://arxiv.org/abs/2508.12036)
*Rakesh Thakur,Yusra Tariq*

Main category: cs.CV

TL;DR: The paper proposes Q-FSRU, a model combining Frequency Spectrum Representation and Quantum Retrieval-Augmented Generation to enhance medical Visual Question Answering (VQA) tasks by processing image-text data and using quantum-based techniques.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of medical Visual Question Answering, which requires effective understanding and reasoning over both medical images and text data.

Method: The proposed Q-FSRU model processes medical image and text features by transforming them into the frequency domain using Fast Fourier Transform and integrates externally fetched medical knowledge using a quantum-inspired retrieval system.

Result: Evaluations on the VQA-RAD dataset demonstrate that Q-FSRU surpasses prior models, particularly in cases requiring intricate reasoning between image and text.

Conclusion: The method of merging frequency-domain data with quantum-inspired knowledge retrieval enhances both the performance and explainability of AI solutions for medical VQA, offering a valuable tool for healthcare practitioners.

Abstract: Solving tough clinical questions that require both image and text
understanding is still a major challenge in healthcare AI. In this work, we
propose Q-FSRU, a new model that combines Frequency Spectrum Representation and
Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation
(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in
features from medical images and related text, then shifts them into the
frequency domain using Fast Fourier Transform (FFT). This helps it focus on
more meaningful data and filter out noise or less useful information. To
improve accuracy and ensure that answers are based on real knowledge, we add a
quantum-inspired retrieval system. It fetches useful medical facts from
external sources using quantum-based similarity techniques. These details are
then merged with the frequency-based features for stronger reasoning. We
evaluated our model using the VQA-RAD dataset, which includes real radiology
images and questions. The results showed that Q-FSRU outperforms earlier
models, especially on complex cases needing image-text reasoning. The mix of
frequency and quantum information improves both performance and explainability.
Overall, this approach offers a promising way to build smart, clear, and
helpful AI tools for doctors.

</details>


### [180] [VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models](https://arxiv.org/abs/2508.12081)
*Haidong Xu,Guangwei Xu,Zhedong Zheng,Xiatian Zhu,Wei Ji,Xiangtai Li,Ruijie Guo,Meishan Zhang,Min zhang,Hao Fei*

Main category: cs.CV

TL;DR: The paper presents VimoRAG, a framework to enhance motion large language models by leveraging in-the-wild video databases for better 3D motion generation.


<details>
  <summary>Details</summary>
Motivation: Motion LLMs struggle with out-of-domain and out-of-vocabulary issues due to limited annotated data.

Method: The authors developed the Gemini Motion Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer to improve retrieval and generation.

Result: Experimental results highlight the benefits of using VimoRAG, showing significant improvements for motion LLMs relying solely on text input.

Conclusion: VimoRAG provides an effective solution to enhance motion LLMs by integrating video retrieval models and addressing retrieval errors.

Abstract: This paper introduces VimoRAG, a novel video-based retrieval-augmented motion
generation framework for motion large language models (LLMs). As motion LLMs
face severe out-of-domain/out-of-vocabulary issues due to limited annotated
data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D
motion generation by retrieving relevant 2D human motion signals. While
video-based motion RAG is nontrivial, we address two key bottlenecks: (1)
developing an effective motion-centered video retrieval model that
distinguishes human poses and actions, and (2) mitigating the issue of error
propagation caused by suboptimal retrieval results. We design the Gemini Motion
Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,
enabling effective retrieval and generation processes. Experimental results
show that VimoRAG significantly boosts the performance of motion LLMs
constrained to text-only input.

</details>


### [181] [Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity](https://arxiv.org/abs/2508.12082)
*Seungju Yoo,Hyuk Kwon,Joong-Won Hwang,Kibok Lee*

Main category: cs.CV

TL;DR: The paper proposes an automated model evaluation framework (AutoEval) for object detection, aiming to estimate performance without using ground-truth labels.


<details>
  <summary>Details</summary>
Motivation: Real-world assessment of object detectors relies on costly and manual annotations, creating a need for a scalable and automated evaluation method.

Method: The paper introduces 'Prediction Consistency and Reliability' (PCR) metrics that analyze spatial consistency and box reliability using detection confidence scores before and after non-maximum suppression (NMS).

Result: Experimental results showed that PCR metrics outperform existing AutoEval methods in performance estimates and are validated across a meta-dataset with image corruptions.

Conclusion: The PCR-based AutoEval framework improves evaluation accuracy, scalability, and robustness for object detection, enabling performance assessments without requiring manual annotations.

Abstract: Recent advances in computer vision have made training object detectors more
efficient and effective; however, assessing their performance in real-world
applications still relies on costly manual annotation. To address this
limitation, we develop an automated model evaluation (AutoEval) framework for
object detection. We propose Prediction Consistency and Reliability (PCR),
which leverages the multiple candidate bounding boxes that conventional
detectors generate before non-maximum suppression (NMS). PCR estimates
detection performance without ground-truth labels by jointly measuring 1) the
spatial consistency between boxes before and after NMS, and 2) the reliability
of the retained boxes via the confidence scores of overlapping boxes. For a
more realistic and scalable evaluation, we construct a meta-dataset by applying
image corruptions of varying severity. Experimental results demonstrate that
PCR yields more accurate performance estimates than existing AutoEval methods,
and the proposed meta-dataset covers a wider range of detection performance.
The code is available at https://github.com/YonseiML/autoeval-det.

</details>


### [182] [Generic Event Boundary Detection via Denoising Diffusion](https://arxiv.org/abs/2508.12084)
*Jaejun Hwang,Dayoung Gong,Manjin Kim,Minsu Cho*

Main category: cs.CV

TL;DR: The paper introduces DiffGEBD, a novel diffusion-based model for identifying natural event boundaries in videos. It emphasizes generating diverse and plausible boundaries rather than deterministic predictions.


<details>
  <summary>Details</summary>
Motivation: To address the subjectivity in event boundary detection in videos and the limitations of deterministic methods by introducing a generative perspective.

Method: The DiffGEBD model uses temporal self-similarity to encode frame changes, and a denoising diffusion process with classifier-free guidance to generate diverse, plausible event boundaries.

Result: DiffGEBD achieves robust performance on Kinetics-GEBD and TAPOS benchmarks, providing diverse and realistic event boundary predictions.

Conclusion: DiffGEBD successfully addresses the inherent subjectivity in event boundary detection through a generative approach, demonstrating both diversity and fidelity in its predictions, supported by a novel evaluation metric.

Abstract: Generic event boundary detection (GEBD) aims to identify natural boundaries
in a video, segmenting it into distinct and meaningful chunks. Despite the
inherent subjectivity of event boundaries, previous methods have focused on
deterministic predictions, overlooking the diversity of plausible solutions. In
this paper, we introduce a novel diffusion-based boundary detection model,
dubbed DiffGEBD, that tackles the problem of GEBD from a generative
perspective. The proposed model encodes relevant changes across adjacent frames
via temporal self-similarity and then iteratively decodes random noise into
plausible event boundaries being conditioned on the encoded features.
Classifier-free guidance allows the degree of diversity to be controlled in
denoising diffusion. In addition, we introduce a new evaluation metric to
assess the quality of predictions considering both diversity and fidelity.
Experiments show that our method achieves strong performance on two standard
benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event
boundaries.

</details>


### [183] [Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction](https://arxiv.org/abs/2508.12089)
*Qinyuan Fan,Clemens Gühmann*

Main category: cs.CV

TL;DR: This paper introduces a multi-stage convolutional neural network (MSCNN) method to reduce 3D point accuracy uncertainty in laser scanners, significantly enhancing spatial measurement precision for low-end devices.


<details>
  <summary>Details</summary>
Motivation: Uncertainty in 3D measurements due to equipment limitations and environmental factors needs resolution, particularly for low-end laser scanners used in geometric modeling and renovation.

Method: High-accuracy scanners (HAS) are paired with low-accuracy scanners (LAS) in identical environments to quantify error patterns. Systematic inaccuracies are converted into a supervised learning problem using MSCNNs for error correction.

Result: Experimental results demonstrate a reduction in mean square error (MSE) by over 70% and a peak signal-to-noise ratio (PSNR) improvement of approximately 6 decibels, enhancing the precision of low-end devices.

Conclusion: The proposed approach allows low-end laser scanners to achieve accuracy levels comparable to high-end devices without requiring hardware upgrades, facilitating better spatial measurement and modeling.

Abstract: We propose a multi-stage convolutional neural network (MSCNN) based
integrated method for reducing uncertainty of 3D point accuracy of lasar
scanner (LS) in rough indoor rooms, providing more accurate spatial
measurements for high-precision geometric model creation and renovation. Due to
different equipment limitations and environmental factors, high-end and low-end
LS have positional errors. Our approach pairs high-accuracy scanners (HAS) as
references with corresponding low-accuracy scanners (LAS) of measurements in
identical environments to quantify specific error patterns. By establishing a
statistical relationship between measurement discrepancies and their spatial
distribution, we develop a correction framework that combines traditional
geometric processing with targeted neural network refinement. This method
transforms the quantification of systematic errors into a supervised learning
problem, allowing precise correction while preserving critical geometric
features. Experimental results in our rough indoor rooms dataset show
significant improvements in measurement accuracy, with mean square error (MSE)
reductions exceeding 70% and peak signal-to-noise ratio (PSNR) improvements of
approximately 6 decibels. This approach enables low-end devices to achieve
measurement uncertainty levels approaching those of high-end devices without
hardware modifications.

</details>


### [184] [Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion](https://arxiv.org/abs/2508.12094)
*Songwei Liu,Hong Liu,Fangmin Chen,Xurui Peng,Chenqian Yan,Lean Fu,Xing Mei*

Main category: cs.CV

TL;DR: This paper addresses the issue of accumulating quantization errors in low-precision diffusion models by proposing a theoretical framework and timestep-aware compensation strategy, achieving improved performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle the challenge of stepwise quantization errors in diffusion models during image generation, which compromises the fidelity of the synthesized outputs.

Method: They developed a theoretical framework to mathematically model error propagation in diffusion models, derived per-step quantization error equations, and proposed a timestep-aware error compensation scheme.

Result: The experiments across multiple image datasets demonstrate that the proposed method significantly reduces error propagation, enhancing the performance of post-training quantization approaches to achieve state-of-the-art results.

Conclusion: The proposed strategy effectively mitigates quantization error accumulation, paving the way for efficient, high-quality low-precision diffusion models.

Abstract: Diffusion models have transformed image synthesis by establishing
unprecedented quality and creativity benchmarks. Nevertheless, their
large-scale deployment faces challenges due to computationally intensive
iterative denoising processes. Although post-training quantization(PTQ)
provides an effective pathway for accelerating sampling, the iterative nature
of diffusion models causes stepwise quantization errors to accumulate
progressively during generation, inevitably compromising output fidelity. To
address this challenge, we develop a theoretical framework that mathematically
formulates error propagation in Diffusion Models (DMs), deriving per-step
quantization error propagation equations and establishing the first closed-form
solution for cumulative error. Building on this theoretical foundation, we
propose a timestep-aware cumulative error compensation scheme. Extensive
experiments across multiple image datasets demonstrate that our compensation
strategy effectively mitigates error propagation, significantly enhancing
existing PTQ methods to achieve state-of-the-art(SOTA) performance on
low-precision diffusion models.

</details>


### [185] [VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine](https://arxiv.org/abs/2508.12108)
*Ziyang Zhang,Yang Yu,Xulei Yang,Si Yong Yeo*

Main category: cs.CV

TL;DR: This paper introduces VELVET-Med, a novel vision-language pre-training framework designed for limited volumetric medical data like 3D CT scans and radiology reports.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the difficulty of curating large-scale paired data in the medical domain for volumetric modalities, which limits performance in downstream tasks.

Method: VELVET-Med incorporates uni-modal self-supervised learning, uses a novel language encoder called TriBERT, and employs hierarchical contrastive learning to uncover spatial and semantic relationships.

Result: The framework achieved state-of-the-art performance in tasks like 3D segmentation, cross-modal retrieval, visual question answering, and report generation using only 38,875 scan-report pairs.

Conclusion: VELVET-Med effectively enhances generalization and transferability of encoders for various downstream vision-and-language medical tasks by leveraging limited data with innovative methods.

Abstract: Vision-and-language models (VLMs) have been increasingly explored in the
medical domain, particularly following the success of CLIP in general domain.
However, unlike the relatively straightforward pairing of 2D images and text,
curating large-scale paired data in the medical field for volumetric modalities
such as CT scans remains a challenging and time-intensive process. This
difficulty often limits the performance on downstream tasks. To address these
challenges, we propose a novel vision-language pre-training (VLP) framework,
termed as \textbf{VELVET-Med}, specifically designed for limited volumetric
data such as 3D CT and associated radiology reports. Instead of relying on
large-scale data collection, our method focuses on the development of effective
pre-training objectives and model architectures. The key contributions are: 1)
We incorporate uni-modal self-supervised learning into VLP framework, which are
often underexplored in the existing literature. 2) We propose a novel language
encoder, termed as \textbf{TriBERT}, for learning multi-level textual
semantics. 3) We devise the hierarchical contrastive learning to capture
multi-level vision-language correspondence. Using only 38,875 scan-report
pairs, our approach seeks to uncover rich spatial and semantic relationships
embedded in volumetric medical images and corresponding clinical narratives,
thereby enhancing the generalization ability of the learned encoders. The
resulting encoders exhibit strong transferability, achieving state-of-the-art
performance across a wide range of downstream tasks, including 3D segmentation,
cross-modal retrieval, visual question answering, and report generation.

</details>


### [186] [Simple o3: Towards Interleaved Vision-Language Reasoning](https://arxiv.org/abs/2508.12109)
*Ye Wang,Qianglong Chen,Zejun Li,Siyuan Wang,Shijie Guo,Zhirui Zhang,Zhongyu Wei*

Main category: cs.CV

TL;DR: The paper introduces Simple o3, a scalable framework for multimodal vision-language reasoning, which integrates dynamic visual tools like cropping and zooming into a reasoning process, outperforming previous approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve multimodal large language models' reasoning abilities in complex scenarios by exploring chain-of-thought capabilities in vision-language tasks.

Method: Simple o3 uses a fine-tuning process with dynamic tool interactions (e.g., cropping, zooming) and introduces a synthetic dataset (TWI-Tools-146K) to train interleaved vision-language reasoning models.

Result: Simple o3 outperforms existing methods on various benchmarks and provides new strategies to improve visual reasoning performance, such as using additional visual tokens and precise grounding techniques.

Conclusion: Simple o3 enhances multimodal reasoning in an efficient way, offers insight into interleaved reasoning strategies, and proposes a cost-effective framework for advancing this field.

Abstract: Multimodal Large Language Models (MLLMs) have shown impressive performance on
vision-language tasks, but their long Chain-of-Thought (CoT) capabilities in
multimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which
emulates human-like ''thinking with image'' through iterative visual
transformations and linguistic reasoning, we propose Simple o3, an end-to-end
framework that integrates dynamic tool interactions (e.g., cropping, zooming,
and reusing) into interleaved vision-language reasoning via supervised
fine-tuning (SFT). Our approach features a scalable data synthesis pipeline
that generates high-quality interleaved vision-language reasoning chains via an
''observe-reason-act'' cycle, complete with executable visual operations and
rigorous verification, yielding the open-source TWI-Tools-146K dataset.
Experimental results demonstrate Simple o3's superior performance on diverse
benchmarks, outperforming existing approaches. By combining enhanced reasoning
capabilities, Simple o3 establishes a powerful yet computationally affordable
paradigm for advancing multimodal reasoning. Remarkably, we provide the first
in-depth analysis of different interleaved reasoning strategies, offering
insights into their impact on model performance. We found that by introducing
additional visual tokens for interleaved vision-language reasoning, reusing and
magnifying the original image significantly improves the model's visual
reasoning and fine-grained perception, while image cropping based on precise
visual grounding allows the model to effectively focus on key entities or
regions, further enhancing its capabilities.

</details>


### [187] [DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis](https://arxiv.org/abs/2508.12131)
*Minh Tran,Johnmark Clements,Annie Prasanna,Tri Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: The paper introduces DualFit, a hybrid virtual try-on (VTON) pipeline that enhances the preservation of fine-grained garment details like logos and text in online fashion try-on systems.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to improve virtual try-on technology, particularly in maintaining fine-grained garment details critical for brand integrity and customer trust, which existing diffusion-based methods struggle to achieve.

Method: DualFit is a two-stage method: first it aligns the target garment with the person’s image using a learned flow field, preserving garment detail; then it synthesizes the final try-on output using a fidelity-preserving blending module guided by preserved-region input and inpainting masks.

Result: DualFit demonstrates qualitative improvements by producing seamless try-on results that effectively balance accuracy and realism while preserving fine details of garments.

Conclusion: DualFit offers a significant advance in virtual try-on systems, displaying enhanced visual realism and detail preservation, thus fostering greater trust and usability in online fashion retail.

Abstract: Virtual Try-On technology has garnered significant attention for its
potential to transform the online fashion retail experience by allowing users
to visualize how garments would look on them without physical trials. While
recent advances in diffusion-based warping-free methods have improved
perceptual quality, they often fail to preserve fine-grained garment details
such as logos and printed text elements that are critical for brand integrity
and customer trust. In this work, we propose DualFit, a hybrid VTON pipeline
that addresses this limitation by two-stage approach. In the first stage,
DualFit warps the target garment to align with the person image using a learned
flow field, ensuring high-fidelity preservation. In the second stage, a
fidelity-preserving try-on module synthesizes the final output by blending the
warped garment with preserved human regions. Particularly, to guide this
process, we introduce a preserved-region input and an inpainting mask, enabling
the model to retain key areas and regenerate only where necessary, particularly
around garment seams. Extensive qualitative results show that DualFit achieves
visually seamless try-on results while faithfully maintaining high-frequency
garment details, striking an effective balance between reconstruction accuracy
and perceptual realism.

</details>


### [188] [TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks](https://arxiv.org/abs/2508.12132)
*Amira Guesmi,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: TriQDef is a defense framework designed to reduce the transferability of patch-based adversarial attacks across quantized neural networks (QNNs).


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle patch-based adversarial attacks in quantized neural networks (QNNs), which pose unique challenges in resource-constrained environments due to their transferability across bit-widths.

Method: TriQDef combines three strategies: Feature Disalignment Penalty (FDP), Gradient Perceptual Dissonance Penalty (GPDP), and Joint Quantization-Aware Training Protocol to combat adversarial attacks.

Result: TriQDef effectively reduced Attack Success Rates (ASR) by over 40% on unseen combinations while maintaining high accuracy on clean datasets like CIFAR-10 and ImageNet.

Conclusion: The framework demonstrates that disrupting semantic and perceptual gradient alignment is crucial for mitigating adversarial patch transferability in QNNs.

Abstract: Quantized Neural Networks (QNNs) are increasingly deployed in edge and
resource-constrained environments due to their efficiency in computation and
memory usage. While shown to distort the gradient landscape and weaken
conventional pixel-level attacks, it provides limited robustness against
patch-based adversarial attacks-localized, high-saliency perturbations that
remain surprisingly transferable across bit-widths. Existing defenses either
overfit to fixed quantization settings or fail to address this cross-bit
generalization vulnerability. We introduce \textbf{TriQDef}, a tri-level
quantization-aware defense framework designed to disrupt the transferability of
patch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature
Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing
perceptual similarity in intermediate representations; (2) a Gradient
Perceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients
across bit-widths by minimizing structural and directional agreement via Edge
IoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training
Protocol that unifies these penalties within a shared-weight training scheme
across multiple quantization levels. Extensive experiments on CIFAR-10 and
ImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over
40\% on unseen patch and quantization combinations, while preserving high clean
accuracy. Our findings underscore the importance of disrupting both semantic
and perceptual gradient alignment to mitigate patch transferability in QNNs.

</details>


### [189] [Infusing fine-grained visual knowledge to Vision-Language Models](https://arxiv.org/abs/2508.12137)
*Nikolaos-Antonios Ypsilantis,Kaifeng Chen,André Araujo,Ondřej Chum*

Main category: cs.CV

TL;DR: The paper addresses the limitations of existing Vision-and-Language Models (VLMs) in fine-grained open-set visual retrieval and proposes an improved fine-tuning method.


<details>
  <summary>Details</summary>
Motivation: Vision-and-Language Models, although effective for general tasks, struggle with fine-grained open-set visual retrieval due to the need for fine-tuning that often results in catastrophic forgetting of the model's broader capabilities.

Method: The paper employs techniques from continual learning to design a fine-tuning method that balances domain adaptation with knowledge retention. The authors also emphasize proper validation set design and hyperparameter tuning for reproducibility.

Result: The proposed method achieves strong performance on both fine-grained and coarse-grained retrieval benchmarks without requiring text data or the original text encoder during fine-tuning.

Conclusion: The fine-tuning approach enhances retrieval performance while retaining the pretrained VLM's general-purpose capabilities, advancing fine-grained domain adaptation research.

Abstract: Large-scale contrastive pre-training produces powerful Vision-and-Language
Models (VLMs) capable of generating representations (embeddings) effective for
a wide variety of visual and multimodal tasks. However, these pretrained
embeddings remain suboptimal for fine-grained open-set visual retrieval, where
state-of-the-art results require fine-tuning the vision encoder using annotated
domain-specific samples. Naively performing such fine-tuning typically leads to
catastrophic forgetting, severely diminishing the model's general-purpose
visual and cross-modal capabilities.
  In this work, we propose a fine-tuning method explicitly designed to achieve
optimal balance between fine-grained domain adaptation and retention of the
pretrained VLM's broad multimodal knowledge. Drawing inspiration from continual
learning literature, we systematically analyze standard regularization
techniques aimed at knowledge retention and propose an efficient and effective
combination strategy. Additionally, we address the commonly overlooked yet
critical aspects of validation set design and hyperparameter tuning to ensure
reproducibility and robust generalization across datasets and pretrained
models. We extensively evaluate our method on both fine-grained and
coarse-grained image-image and image-text retrieval benchmarks. Our approach
consistently achieves strong results, notably retaining the visual-text
alignment without utilizing any text data or the original text encoder during
fine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .

</details>


### [190] [KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction](https://arxiv.org/abs/2508.12147)
*Donghang Lyu,Marius Staring,Mariya Doneva,Hildo J. Lamb,Nicola Pezzotti*

Main category: cs.CV

TL;DR: This paper introduces KP-INR, a dual-branch method for reconstructing high-quality cardiac cine MRI images from undersampled k-space data.


<details>
  <summary>Details</summary>
Motivation: Cardiac cine MRI reconstruction suffers from a trade-off between scan time and image quality, necessitating advanced methods for high-quality recovery from undersampled data.

Method: The KP-INR method utilizes a dual-branch approach: one branch captures positional embeddings of k-space coordinates, while the other learns multi-scale local k-space feature representations, with cross-branch interaction for enhanced reconstruction.

Result: KP-INR demonstrates significant performance improvement in reconstructing cardiac MRI data compared to baseline models on the CMRxRecon2024 dataset.

Conclusion: KP-INR effectively balances positional and local feature learning in k-space and shows great potential for advancing cardiac MRI reconstruction.

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for
assessing cardiac structure, function, and blood flow. Cine MRI extends this by
capturing heart motion, providing detailed insights into cardiac mechanics. To
reduce scan time and breath-hold discomfort, fast acquisition techniques have
been utilized at the cost of lowering image quality. Recently, Implicit Neural
Representation (INR) methods have shown promise in unsupervised reconstruction
by learning coordinate-to-value mappings from undersampled data, enabling
high-quality image recovery. However, current existing INR methods primarily
focus on using coordinate-based positional embeddings to learn the mapping,
while overlooking the feature representations of the target point and its
neighboring context. In this work, we propose KP-INR, a dual-branch INR method
operating in k-space for cardiac cine MRI reconstruction: one branch processes
the positional embedding of k-space coordinates, while the other learns from
local multi-scale k-space feature representations at those coordinates. By
enabling cross-branch interaction and approximating the target k-space values
from both branches, KP-INR can achieve strong performance on challenging
Cartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its
improved performance over baseline models and highlights its potential in this
field.

</details>


### [191] [Demystifying Foreground-Background Memorization in Diffusion Models](https://arxiv.org/abs/2508.12148)
*Jimmy Z. Di,Yiwei Lu,Yaoliang Yu,Gautam Kamath,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: This paper introduces FB-Mem, a segmentation-based method for identifying and quantifying memorized regions in diffusion models' outputs, showing that memorization is widespread and inadequately addressed by current methods.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the insufficiency of existing techniques for detecting memorization in diffusion models, particularly partial memorization in localized image regions and broader memorization patterns.

Method: The authors propose Foreground Background Memorization (FB-Mem), a segmentation-based metric that identifies and quantifies memorized regions within generated images.

Result: Their findings reveal that memorization in diffusion models extends beyond single prompt-image pairs, includes clusters of similar training images, and persists even with current mitigation efforts. Local memorization is especially prevalent in foreground regions.

Conclusion: The study underscores the pervasive nature of memorization in diffusion models, highlights the shortcomings of existing mitigation strategies, and proposes a clustering-based approach for better addressing the issue.

Abstract: Diffusion models (DMs) memorize training images and can reproduce
near-duplicates during generation. Current detection methods identify verbatim
memorization but fail to capture two critical aspects: quantifying partial
memorization occurring in small image regions, and memorization patterns beyond
specific prompt-image pairs. To address these limitations, we propose
Foreground Background Memorization (FB-Mem), a novel segmentation-based metric
that classifies and quantifies memorized regions within generated images. Our
method reveals that memorization is more pervasive than previously understood:
(1) individual generations from single prompts may be linked to clusters of
similar training images, revealing complex memorization patterns that extend
beyond one-to-one correspondences; and (2) existing model-level mitigation
methods, such as neuron deactivation and pruning, fail to eliminate local
memorization, which persists particularly in foreground regions. Our work
establishes an effective framework for measuring memorization in diffusion
models, demonstrates the inadequacy of current mitigation approaches, and
proposes a stronger mitigation method using a clustering approach.

</details>


### [192] [RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis](https://arxiv.org/abs/2508.12163)
*Wenqing Wang,Yun Fu*

Main category: cs.CV

TL;DR: RealTalk is an AI framework that generates talking heads with accurate emotions, controlled expressions, and preserved identity.


<details>
  <summary>Details</summary>
Motivation: Current AI methods struggle to create talking heads with precise emotional expressions and maintained identity while excelling in lip sync and image quality.

Method: RealTalk uses a VAE to generate 3D facial landmarks, integrates these with emotion-label embeddings via a ResNet-based landmark deformation model, and employs a tri-plane attention NeRF for synthesis.

Result: RealTalk achieves higher performance in emotion accuracy, controllability, and identity preservation compared to existing methods.

Conclusion: The RealTalk framework is a significant step forward in developing socially intelligent AI systems with improved emotional expression handling and realism.

Abstract: Emotion is a critical component of artificial social intelligence. However,
while current methods excel in lip synchronization and image quality, they
often fail to generate accurate and controllable emotional expressions while
preserving the subject's identity. To address this challenge, we introduce
RealTalk, a novel framework for synthesizing emotional talking heads with high
emotion accuracy, enhanced emotion controllability, and robust identity
preservation. RealTalk employs a variational autoencoder (VAE) to generate 3D
facial landmarks from driving audio, which are concatenated with emotion-label
embeddings using a ResNet-based landmark deformation model (LDM) to produce
emotional landmarks. These landmarks and facial blendshape coefficients jointly
condition a novel tri-plane attention Neural Radiance Field (NeRF) to
synthesize highly realistic emotional talking heads. Extensive experiments
demonstrate that RealTalk outperforms existing methods in emotion accuracy,
controllability, and identity preservation, advancing the development of
socially intelligent AI systems.

</details>


### [193] [Scalable RF Simulation in Generative 4D Worlds](https://arxiv.org/abs/2508.12176)
*Zhiwei Zheng,Dongyin Hu,Mingmin Zhao*

Main category: cs.CV

TL;DR: WaveVerse is a scalable framework for simulating realistic RF signals and enabling effective indoor perception tasks through language-guided human motion generation and phase-coherent simulations.


<details>
  <summary>Details</summary>
Motivation: Collecting high-quality RF data in dynamic and diverse indoor environments is challenging, preventing broader adoption of this privacy-preserving sensing method.

Method: WaveVerse employs a language-guided 4D world generator and a phase-coherent ray tracing simulator, combining conditioned human motion generation and spatially accurate RF signal simulation.

Result: Experiments validate the system's effectiveness for human motion generation, beamforming, and respiration monitoring. Case studies on RF imaging and activity recognition demonstrate performance gains, even in low-data scenarios.

Conclusion: WaveVerse advances RF sensing by offering a novel method for generating accurate simulation data, enabling new applications like RF imaging and improved human activity recognition.

Abstract: Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving
alternative to vision-based methods for indoor perception tasks. However,
collecting high-quality RF data in dynamic and diverse indoor environments
remains a major challenge. To address this, we introduce WaveVerse, a
prompt-based, scalable framework that simulates realistic RF signals from
generated indoor scenes with human motions. WaveVerse introduces a
language-guided 4D world generator, which includes a state-aware causal
transformer for human motion generation conditioned on spatial constraints and
texts, and a phase-coherent ray tracing simulator that enables the simulation
of accurate and coherent RF signals. Experiments demonstrate the effectiveness
of our approach in conditioned human motion generation and highlight how phase
coherence is applied to beamforming and respiration monitoring. We further
present two case studies in ML-based high-resolution imaging and human activity
recognition, demonstrating that WaveVerse not only enables data generation for
RF imaging for the first time, but also consistently achieves performance gain
in both data-limited and data-adequate scenarios.

</details>


### [194] [Splat Feature Solver](https://arxiv.org/abs/2508.12216)
*Butian Xiong,Rong Liu,Kenneth Xu,Meida Chen,Andrew Feng*

Main category: cs.CV

TL;DR: The paper presents a new feature lifting method for attaching descriptors to splat-based 3D representations, using a closed-form solution and introducing regularization strategies to address multi-view inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Current methods face challenges in optimally assigning rich attributes to 3D primitives from inconsistent multi-view images, which impacts the quality of 3D scene understanding.

Method: The problem is formulated as a sparse linear inverse problem with an efficient closed-form solution. Two regularization techniques are introduced: Tikhonov Guidance for numerical stability and Post-Lifting Aggregation for noise filtering.

Result: The proposed approach achieves state-of-the-art results on open-vocabulary 3D segmentation benchmarks, outperforming other baselines, and produces features efficiently within minutes.

Conclusion: The study provides an effective and efficient solution for 3D scene feature lifting, offering high-quality lifted features by addressing inconsistency and noise issues in multi-view observations.

Abstract: Feature lifting has emerged as a crucial component in 3D scene understanding,
enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP)
onto splat-based 3D representations. The core challenge lies in optimally
assigning rich general attributes to 3D primitives while addressing the
inconsistency issues from multi-view images. We present a unified, kernel- and
feature-agnostic formulation of the feature lifting problem as a sparse linear
inverse problem, which can be solved efficiently in closed form. Our approach
admits a provable upper bound on the global optimal error under convex losses
for delivering high quality lifted features. To address inconsistencies and
noise in multi-view observations, we introduce two complementary regularization
strategies to stabilize the solution and enhance semantic fidelity. Tikhonov
Guidance enforces numerical stability through soft diagonal dominance, while
Post-Lifting Aggregation filters noisy inputs via feature clustering. Extensive
experiments demonstrate that our approach achieves state-of-the-art performance
on open-vocabulary 3D segmentation benchmarks, outperforming training-based,
grouping-based, and heuristic-forward baselines while producing the lifted
features in minutes. Code is available at
\href{https://github.com/saliteta/splat-distiller.git}{\textbf{github}}. We
also have a \href{https://splat-distiller.pages.dev/}

</details>


### [195] [C2PSA-Enhanced YOLOv11 Architecture: A Novel Approach for Small Target Detection in Cotton Disease Diagnosis](https://arxiv.org/abs/2508.12219)
*Kaiyuan Wang,Jixing Liu,Xiaobo Cai*

Main category: cs.CV

TL;DR: The paper enhances YOLOv11 deep learning model for cotton disease detection, achieving better accuracy and faster real-time monitoring.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the low precision, performance degradation in field conditions, and high error rates in cotton disease detection.

Method: The proposed optimization includes a C2PSA module for small target feature extraction, dynamic category weighting, and advanced data augmentation via Mosaic-MixUp scaling.

Result: Experimental results showed an 8.0% improvement in mAP50, 10.5% in mAP50-95, and achieved an inference speed of 158 FPS on a dataset with 4,078 images.

Conclusion: The enhanced model provides a superior and mobile-compatible solution for real-time disease detection and precision agricultural applications.

Abstract: This study presents a deep learning-based optimization of YOLOv11 for cotton
disease detection, developing an intelligent monitoring system. Three key
challenges are addressed: (1) low precision in early spot detection (35%
leakage rate for sub-5mm2 spots), (2) performance degradation in field
conditions (25% accuracy drop), and (3) high error rates (34.7%) in
multi-disease scenarios. The proposed solutions include: C2PSA module for
enhanced small-target feature extraction; Dynamic category weighting to handle
sample imbalance; Improved data augmentation via Mosaic-MixUp scaling.
Experimental results on a 4,078-image dataset show: mAP50: 0.820 (+8.0%
improvement); mAP50-95: 0.705 (+10.5% improvement); Inference speed: 158 FPS.
The mobile-deployed system enables real-time disease monitoring and precision
treatment in agricultural applications.

</details>


### [196] [In vivo 3D ultrasound computed tomography of musculoskeletal tissues with generative neural physics](https://arxiv.org/abs/2508.12226)
*Zhijun Zeng,Youjia Zheng,Chang Su,Qianhang Wu,Hao Hu,Zeyuan Dong,Shan Gao,Yang Lv,Rui Tang,Ligang Cui,Zhiyong Hou,Weijun Lin,Zuoqiang Shi,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: The paper introduces a generative neural physics framework to improve musculoskeletal imaging using ultrasound computed tomography, achieving faster and higher-fidelity imaging.


<details>
  <summary>Details</summary>
Motivation: Ultrasound computed tomography faces limitations in musculoskeletal imaging due to conventional scattering neglect in ray-based reconstructions. The paper aims to address these challenges for improving diagnostic imaging.

Method: A generative neural physics framework is introduced, combining generative neural networks with physics-informed simulations to accurately model ultrasonic wave propagation while improving efficiency.

Result: The proposed method reconstructs 3D maps of tissue parameters within 10 minutes, achieves MRI-comparable resolution, and detects biomechanical properties, including muscle and bone.

Conclusion: This approach overcomes significant computational bottlenecks, enabling routine high-accuracy ultrasound imaging for clinical musculoskeletal disease assessment.

Abstract: Ultrasound computed tomography (USCT) is a radiation-free, high-resolution
modality but remains limited for musculoskeletal imaging due to conventional
ray-based reconstructions that neglect strong scattering. We propose a
generative neural physics framework that couples generative networks with
physics-informed neural simulation for fast, high-fidelity 3D USCT. By learning
a compact surrogate of ultrasonic wave propagation from only dozens of
cross-modality images, our method merges the accuracy of wave modeling with the
efficiency and stability of deep learning. This enables accurate quantitative
imaging of in vivo musculoskeletal tissues, producing spatial maps of acoustic
properties beyond reflection-mode images. On synthetic and in vivo data
(breast, arm, leg), we reconstruct 3D maps of tissue parameters in under ten
minutes, with sensitivity to biomechanical properties in muscle and bone and
resolution comparable to MRI. By overcoming computational bottlenecks in
strongly scattering regimes, this approach advances USCT toward routine
clinical assessment of musculoskeletal disease.

</details>


### [197] [WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions](https://arxiv.org/abs/2508.12250)
*Quan Chen,Xiong Yang,Rongfeng Lu,Qianyu Zhang,Yu Liu,Xiaofei Zhou,Bolun Zheng*

Main category: cs.CV

TL;DR: A novel Weather-eXtended Salient Object Detection (WXSOD) dataset addressing weather-noise-related challenges in salient object detection was introduced, along with WFANet, a high-performing baseline model.


<details>
  <summary>Details</summary>
Motivation: The paper was motivated by the lack of datasets with pixel-wise annotations to study the influence of weather noise on salient object detection, a significant challenge in complex environments.

Method: The authors created the WXSOD dataset comprising 14,945 RGB images with pixel-wise annotations and diverse weather labels. They developed the WFANet model, a two-branch supervised architecture leveraging weather-specific and semantic features.

Result: The WFANet model achieved superior performance compared to 17 existing SOD methods on the newly introduced WXSOD dataset.

Conclusion: The study contributes a valuable dataset and demonstrates the effectiveness of weather-aware features, setting a foundation for future research in robust salient object detection under adverse conditions.

Abstract: Salient object detection (SOD) in complex environments remains a challenging
research topic. Most existing methods perform well in natural scenes with
negligible noise, and tend to leverage multi-modal information (e.g., depth and
infrared) to enhance accuracy. However, few studies are concerned with the
damage of weather noise on SOD performance due to the lack of dataset with
pixel-wise annotations. To bridge this gap, this paper introduces a novel
Weather-eXtended Salient Object Detection (WXSOD) dataset. It consists of
14,945 RGB images with diverse weather noise, along with the corresponding
ground truth annotations and weather labels. To verify algorithm
generalization, WXSOD contains two test sets, i.e., a synthesized test set and
a real test set. The former is generated by adding weather noise to clean
images, while the latter contains real-world weather noise. Based on WXSOD, we
propose an efficient baseline, termed Weather-aware Feature Aggregation Network
(WFANet), which adopts a fully supervised two-branch architecture.
Specifically, the weather prediction branch mines weather-related deep
features, while the saliency detection branch fuses semantic features extracted
from the backbone with weather features for SOD. Comprehensive comparisons
against 17 SOD methods shows that our WFANet achieves superior performance on
WXSOD. The code and benchmark results will be made publicly available at
https://github.com/C-water/WXSOD

</details>


### [198] [Superpixel-informed Continuous Low-Rank Tensor Representation for Multi-Dimensional Data Recovery](https://arxiv.org/abs/2508.12261)
*Zhizhou Wang,Ruijing Zheng,Zhenyu Wu,Jianli Wang*

Main category: cs.CV

TL;DR: The study introduces a novel tensor representation framework (SCTR) leveraging superpixels for enhanced efficiency and adaptability, achieving superior reconstruction quality compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Classical low-rank tensor representation methods face challenges with holistic low-rank data assumptions and their restriction to grid-like structures, calling for a more flexible and semantically-aware solution.

Method: The paper leverages superpixels for localized modeling units and proposes an asymmetric low-rank tensor factorization model, parameterized through neural networks to capture regional coherence and spatial variations.

Result: The SCTR framework yields significant performance improvements (3-5 dB PSNR) in reconstructing multispectral datasets, videos, and images, outperforming traditional LRTR methods.

Conclusion: The SCTR framework successfully addresses key limitations of previous LRTR-based methods, providing a compact, expressive, and adaptable approach to multidimensional data modeling.

Abstract: Low-rank tensor representation (LRTR) has emerged as a powerful tool for
multi-dimensional data processing. However, classical LRTR-based methods face
two critical limitations: (1) they typically assume that the holistic data is
low-rank, this assumption is often violated in real-world scenarios with
significant spatial variations; and (2) they are constrained to discrete
meshgrid data, limiting their flexibility and applicability. To overcome these
limitations, we propose a Superpixel-informed Continuous low-rank Tensor
Representation (SCTR) framework, which enables continuous and flexible modeling
of multi-dimensional data beyond traditional grid-based constraints. Our
approach introduces two main innovations: First, motivated by the observation
that semantically coherent regions exhibit stronger low-rank characteristics
than holistic data, we employ superpixels as the basic modeling units. This
design not only encodes rich semantic information, but also enhances
adaptability to diverse forms of data streams. Second, we propose a novel
asymmetric low-rank tensor factorization (ALTF) where superpixel-specific
factor matrices are parameterized by a shared neural network with specialized
heads. By strategically separating global pattern learning from local
adaptation, this framework efficiently captures both cross-superpixel
commonalities and within-superpixel variations. This yields a representation
that is both highly expressive and compact, balancing model efficiency with
adaptability. Extensive experiments on several benchmark datasets demonstrate
that SCTR achieves 3-5 dB PSNR improvements over existing LRTR-based methods
across multispectral images, videos, and color images.

</details>


### [199] [Region-Level Context-Aware Multimodal Understanding](https://arxiv.org/abs/2508.12263)
*Hongliang Wei,Xianqi Zhang,Xingtao Wang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: The paper addresses the limitation of multimodal large language models (MLLMs) in Region-level Context-aware Multimodal Understanding (RCMU) by introducing a new task, dataset, method (RCVIT), models, and evaluation benchmark.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs lack the ability to integrate textual context associated with specific visual regions or objects, essential for context-aware multimodal understanding.

Method: The authors propose RCVIT, which incorporates bounding box coordinates to link visual and textual information of objects. They also develop the RCMU dataset, RC\&P-Bench benchmark, and reference-free evaluation metrics.

Result: Experimental results show RC-Qwen2-VL models excel in RCMU tasks, multimodal retrieval-augmented generation (RAG), and personalized conversations.

Conclusion: RC-Qwen2-VL models achieve state-of-the-art performance for RCMU tasks, and the framework paves the way for advancements in multimodal personalized understanding.

Abstract: Despite significant progress, existing research on Multimodal Large Language
Models (MLLMs) mainly focuses on general visual understanding, overlooking the
ability to integrate textual context associated with objects for a more
context-aware multimodal understanding -- an ability we refer to as
Region-level Context-aware Multimodal Understanding (RCMU). To address this
limitation, we first formulate the RCMU task, which requires models to respond
to user instructions by integrating both image content and textual information
of regions or objects. To equip MLLMs with RCMU capabilities, we propose
Region-level Context-aware Visual Instruction Tuning (RCVIT), which
incorporates object information into the model input and enables the model to
utilize bounding box coordinates to effectively associate objects' visual
content with their textual information. To address the lack of datasets, we
introduce the RCMU dataset, a large-scale visual instruction tuning dataset
that covers multiple RCMU tasks. We also propose RC\&P-Bench, a comprehensive
benchmark that can evaluate the performance of MLLMs in RCMU and multimodal
personalized understanding tasks. Additionally, we propose a reference-free
evaluation metric to perform a comprehensive and fine-grained evaluation of the
region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL
models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental
results indicate that RC-Qwen2-VL models not only achieve outstanding
performance on multiple RCMU tasks but also demonstrate successful applications
in multimodal RAG and personalized conversation. Our data, model and benchmark
are available at https://github.com/hongliang-wei/RC-MLLM

</details>


### [200] [SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration](https://arxiv.org/abs/2508.12271)
*Ronghua Xu,Jin Xie,Jing Nie,Jiale Cao,Yanwei Pang*

Main category: cs.CV

TL;DR: The paper introduces SNNSIR, a Spiking Neural Network (SNN) for stereo image restoration, which fully adopts the spike-driven paradigm, enabling low-power and efficient computation.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of hybrid SNN-ANN models for stereo image restoration by creating a truly spike-driven, low-power, and hardware-friendly solution.

Method: The authors propose SNNSIR with key components: Spike Residual Basic Block (SRBB) for residual learning, Spike Stereo Convolutional Modulation (SSCM) to enhance noise sensitivity and nonlinearity, and Spike Stereo Cross-Attention (SSCA) for stereo correspondence through efficient feature interaction.

Result: The model demonstrated competitive performance in diverse stereo image restoration tasks such as rain streak removal, raindrop removal, low-light enhancement, and super-resolution, while significantly reducing computational overhead.

Conclusion: SNNSIR effectively balances high restoration performance with low computational requirements, promising for real-time, low-energy stereo vision applications.

Abstract: Spiking Neural Networks (SNNs), characterized by discrete binary activations,
offer high computational efficiency and low energy consumption, making them
well-suited for computation-intensive tasks such as stereo image restoration.
In this work, we propose SNNSIR, a simple yet effective Spiking Neural Network
for Stereo Image Restoration, specifically designed under the spike-driven
paradigm where neurons transmit information through sparse, event-based binary
spikes. In contrast to existing hybrid SNN-ANN models that still rely on
operations such as floating-point matrix division or exponentiation, which are
incompatible with the binary and event-driven nature of SNNs, our proposed
SNNSIR adopts a fully spike-driven architecture to achieve low-power and
hardware-friendly computation. To address the expressiveness limitations of
binary spiking neurons, we first introduce a lightweight Spike Residual Basic
Block (SRBB) to enhance information flow via spike-compatible residual
learning. Building on this, the Spike Stereo Convolutional Modulation (SSCM)
module introduces simplified nonlinearity through element-wise multiplication
and highlights noise-sensitive regions via cross-view-aware modulation.
Complementing this, the Spike Stereo Cross-Attention (SSCA) module further
improves stereo correspondence by enabling efficient bidirectional feature
interaction across views within a spike-compatible framework. Extensive
experiments on diverse stereo image restoration tasks, including rain streak
removal, raindrop removal, low-light enhancement, and super-resolution
demonstrate that our model achieves competitive restoration performance while
significantly reducing computational overhead. These results highlight the
potential for real-time, low-power stereo vision applications. The code will be
available after the article is accepted.

</details>


### [201] [Precise Action-to-Video Generation Through Visual Action Prompts](https://arxiv.org/abs/2508.13104)
*Yuang Wang,Chao Wen,Haoyu Guo,Sida Peng,Minghan Qin,Hujun Bao,Xiaowei Zhou,Ruizhen Hu*

Main category: cs.CV

TL;DR: The paper proposes a method for action-driven video generation using visual skeletons as prompts, enabling cross-domain precision and adaptability.


<details>
  <summary>Details</summary>
Motivation: Action-to-video generation systems face challenges in achieving both precision and cross-domain adaptability. Existing solutions either sacrifice precision or lack generalizability.

Method: The authors introduce visual skeletons as visual prompts, constructed from two data sources: human-object interactions and robotic manipulation. These skeletons are integrated via fine-tuning into pretrained video generation models.

Result: Experiments on datasets such as EgoVid, RT-1, and DROID show the approach's success in balancing precision and cross-domain transferability.

Conclusion: The proposed visual action prompts offer a promising pathway to precise and adaptable action-to-video generation for complex interactions.

Abstract: We present visual action prompts, a unified action representation for
action-to-video generation of complex high-DoF interactions while maintaining
transferable visual dynamics across domains. Action-driven video generation
faces a precision-generality trade-off: existing methods using text, primitive
actions, or coarse masks offer generality but lack precision, while
agent-centric action signals provide precision at the cost of cross-domain
transferability. To balance action precision and dynamic transferability, we
propose to "render" actions into precise visual prompts as domain-agnostic
representations that preserve both geometric precision and cross-domain
adaptability for complex actions; specifically, we choose visual skeletons for
their generality and accessibility. We propose robust pipelines to construct
skeletons from two interaction-rich data sources - human-object interactions
(HOI) and dexterous robotic manipulation - enabling cross-domain training of
action-driven generative models. By integrating visual skeletons into
pretrained video generation models via lightweight fine-tuning, we enable
precise action control of complex interaction while preserving the learning of
cross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the
effectiveness of our proposed approach. Project page:
https://zju3dv.github.io/VAP/.

</details>


### [202] [CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval](https://arxiv.org/abs/2508.12290)
*Chor Boon Tan,Conghui Hu,Gim Hee Lee*

Main category: cs.CV

TL;DR: The paper addresses weakly supervised zero-shot cross-domain image retrieval (WSZS-CDIR) using refined pseudo-labels from foundation models like CLIP, introducing novel methodologies for improved retrieval and alignment.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in improving zero-shot cross-domain image retrieval (ZS-CDIR) by leveraging large foundation models to refine noisy pseudo-labels and enhance performance in categorizing and retrieving images across domains.

Method: The proposed method, CLAIR, refines noisy pseudo-labels using confidence scores based on CLIP features, employs inter-instance, inter-cluster, and inter-domain contrastive losses, introduces a closed-form cross-domain mapping function, and incorporates learnable prompts to improve generalization to novel categories.

Result: CLAIR outperformed existing state-of-the-art methods in extensive experiments conducted on diverse zero-shot datasets such as TUBerlin, Sketchy, Quickdraw, and DomainNet.

Conclusion: CLAIR demonstrates the effectiveness of combining pseudo-label refinement, novel contrastive loss designs, and cross-domain mappings for enhanced zero-shot cross-domain image retrieval.

Abstract: The recent growth of large foundation models that can easily generate
pseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-Shot
Cross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, we
therefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) with
noisy pseudo labels generated by large foundation models such as CLIP. To this
end, we propose CLAIR to refine the noisy pseudo-labels with a confidence score
from the similarity between the CLIP text and image features. Furthermore, we
design inter-instance and inter-cluster contrastive losses to encode images
into a class-aware latent space, and an inter-domain contrastive loss to
alleviate domain discrepancies. We also learn a novel cross-domain mapping
function in closed-form, using only CLIP text embeddings to project image
features from one domain to another, thereby further aligning the image
features for retrieval. Finally, we enhance the zero-shot generalization
ability of our CLAIR to handle novel categories by introducing an extra set of
learnable prompts. Extensive experiments are carried out using TUBerlin,
Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIR
consistently shows superior performance compared to existing state-of-the-art
methods.

</details>


### [203] [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/abs/2508.13142)
*Zhongang Cai,Yubo Wang,Qingping Sun,Ruisi Wang,Chenyang Gu,Wanqi Yin,Zhiqian Lin,Zhitao Yang,Chen Wei,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Jiaqi Li,Xiangyu Fan,Hanming Deng,Lewei Lu,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: The paper evaluates GPT-5 and other multi-modal models on spatial intelligence tasks, revealing GPT-5's strengths while highlighting its limitations compared to human performance.


<details>
  <summary>Details</summary>
Motivation: To investigate the progress and limitations of modern AI models, specifically GPT-5, with respect to spatial understanding and reasoning, fundamental for achieving artificial general intelligence.

Method: The authors propose a taxonomy of spatial tasks, evaluate various models on eight benchmarks involving over one billion tokens, and conduct qualitative testing to identify challenging scenarios for multi-modal models.

Result: GPT-5 excels in spatial intelligence relative to other models but still performs worse than humans. No clear advantage is observed for proprietary models in solving the most difficult spatial tasks.

Conclusion: Despite advancements, multi-modal models like GPT-5 have significant room for improvement in spatial reasoning; addressing these challenges is critical for advancing toward artificial general intelligence.

Abstract: Multi-modal models have achieved remarkable progress in recent years.
Nevertheless, they continue to exhibit notable limitations in spatial
understanding and reasoning, which are fundamental capabilities to achieving
artificial general intelligence. With the recent release of GPT-5, allegedly
the most powerful AI model to date, it is timely to examine where the leading
models stand on the path toward spatial intelligence. First, we propose a
comprehensive taxonomy of spatial tasks that unifies existing benchmarks and
discuss the challenges in ensuring fair evaluation. We then evaluate
state-of-the-art proprietary and open-source models on eight key benchmarks, at
a cost exceeding one billion total tokens. Our empirical study reveals that (1)
GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)
still falls short of human performance across a broad spectrum of tasks.
Moreover, we (3) identify the more challenging spatial intelligence problems
for multi-modal models, and (4) proprietary models do not exhibit a decisive
advantage when facing the most difficult problems. In addition, we conduct a
qualitative evaluation across a diverse set of scenarios that are intuitive for
humans yet fail even the most advanced multi-modal models.

</details>


### [204] [Improving Densification in 3D Gaussian Splatting for High-Fidelity Rendering](https://arxiv.org/abs/2508.12313)
*Xiaobin Deng,Changyu Diao,Min Li,Ruohan Yu,Duanqing Xu*

Main category: cs.CV

TL;DR: The paper refines 3D Gaussian Splatting (3DGS) for real-time rendering by improving densification strategies to enhance reconstruction quality and reduce overfitting.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in the densification strategy of 3D Gaussian Splatting (3DGS), which leads to suboptimal reconstruction quality despite its strong real-time rendering performance.

Method: The authors improve 3DGS's densification across three areas: (1) introducing Edge-Aware Score for better splitting, (2) using a Long-Axis Split strategy to minimize geometric distortion, and (3) integrating methods like Recovery-Aware Pruning to mitigate overfitting.

Result: The proposed method improves rendering fidelity, reduces overfitting, and achieves state-of-the-art results with fewer Gaussians without incurring extra training or inference costs.

Conclusion: The improvements proposed in this paper significantly enhance the performance of 3DGS in real-time rendering, demonstrating both efficiency and quality gains.

Abstract: Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in
real-time rendering, its densification strategy often results in suboptimal
reconstruction quality. In this work, we present a comprehensive improvement to
the densification pipeline of 3DGS from three perspectives: when to densify,
how to densify, and how to mitigate overfitting. Specifically, we propose an
Edge-Aware Score to effectively select candidate Gaussians for splitting. We
further introduce a Long-Axis Split strategy that reduces geometric distortions
introduced by clone and split operations. To address overfitting, we design a
set of techniques, including Recovery-Aware Pruning, Multi-step Update, and
Growth Control. Our method enhances rendering fidelity without introducing
additional training or inference overhead, achieving state-of-the-art
performance with fewer Gaussians.

</details>


### [205] [Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells](https://arxiv.org/abs/2508.12322)
*Michael Deutges,Chen Yang,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: This paper introduces NCA-WSS, a weakly supervised approach leveraging neural cellular automata for white blood cell segmentation and classification without requiring segmentation labels.


<details>
  <summary>Details</summary>
Motivation: Medical diagnostics depend heavily on accurate detection and segmentation of white blood cells, but obtaining labeled training data remains a costly and time-consuming challenge.

Method: The authors propose using neural cellular automata (NCA) feature maps to infer segmentation masks without retraining, enabling a weakly supervised learning process.

Result: Experimental evaluation on three microscopy datasets shows that NCA-WSS surpasses other weakly supervised methods for segmenting white blood cells.

Conclusion: NCA-WSS is an efficient and scalable solution for combining classification and segmentation in medical image analysis within a weak supervision framework.

Abstract: The detection and segmentation of white blood cells in blood smear images is
a key step in medical diagnostics, supporting various downstream tasks such as
automated blood cell counting, morphological analysis, cell classification, and
disease diagnosis and monitoring. Training robust and accurate models requires
large amounts of labeled data, which is both time-consuming and expensive to
acquire. In this work, we propose a novel approach for weakly supervised
segmentation using neural cellular automata (NCA-WSS). By leveraging the
feature maps generated by NCA during classification, we can extract
segmentation masks without the need for retraining with segmentation labels. We
evaluate our method on three white blood cell microscopy datasets and
demonstrate that NCA-WSS significantly outperforms existing weakly supervised
approaches. Our work illustrates the potential of NCA for both classification
and segmentation in a weakly supervised framework, providing a scalable and
efficient solution for medical image analysis.

</details>


### [206] [Attention Pooling Enhances NCA-based Classification of Microscopy Images](https://arxiv.org/abs/2508.12324)
*Chen Yang,Michael Deutges,Jingsong Liu,Han Li,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: This paper improves Neural Cellular Automata (NCA) for image classification by integrating attention pooling, achieving higher accuracy on microscopy datasets while staying parameter-efficient.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the performance gap between Neural Cellular Automata (NCA) and larger, more complex architectures in image classification, particularly for microscopy image analysis.

Method: The paper integrates attention pooling into the NCA framework to refine focus on relevant image regions, enhancing feature extraction and improving classification accuracy.

Result: The proposed method outperforms existing NCA approaches and lightweight traditional architectures (like CNNs and vision transformers) on eight microscopy datasets, while maintaining fewer parameters.

Conclusion: NCA enhanced with attention pooling is a viable, explainable, and parameter-efficient alternative for microscopy image classification.

Abstract: Neural Cellular Automata (NCA) offer a robust and interpretable approach to
image classification, making them a promising choice for microscopy image
analysis. However, a performance gap remains between NCA and larger, more
complex architectures. We address this challenge by integrating attention
pooling with NCA to enhance feature extraction and improve classification
accuracy. The attention pooling mechanism refines the focus on the most
informative regions, leading to more accurate predictions. We evaluate our
method on eight diverse microscopy image datasets and demonstrate that our
approach significantly outperforms existing NCA methods while remaining
parameter-efficient and explainable. Furthermore, we compare our method with
traditional lightweight convolutional neural network and vision transformer
architectures, showing improved performance while maintaining a significantly
lower parameter count. Our results highlight the potential of NCA-based models
an alternative for explainable image classification.

</details>


### [207] [DoppDrive: Doppler-Driven Temporal Aggregation for Improved Radar Object Detection](https://arxiv.org/abs/2508.12330)
*Yuval Haitman,Oded Bialer*

Main category: cs.CV

TL;DR: DoppDrive method boosts radar-based object detection by using Doppler-driven temporal aggregation to enhance point cloud density and reduce scatter.


<details>
  <summary>Details</summary>
Motivation: Radar-based object detection is vital for autonomous driving, but point sparsity, especially at long range, hampers detection accuracy.

Method: DoppDrive uses Doppler and angle information to shift previous radar points dynamically, minimizing scatter and enhancing density before detection.

Result: DoppDrive improves object detection performance across diverse detectors and datasets by addressing scatter issues while increasing density.

Conclusion: DoppDrive is an effective preprocessing tool compatible with multiple detectors, facilitating better detection by enhancing radar point clouds.

Abstract: Radar-based object detection is essential for autonomous driving due to
radar's long detection range. However, the sparsity of radar point clouds,
especially at long range, poses challenges for accurate detection. Existing
methods increase point density through temporal aggregation with ego-motion
compensation, but this approach introduces scatter from dynamic objects,
degrading detection performance. We propose DoppDrive, a novel Doppler-Driven
temporal aggregation method that enhances radar point cloud density while
minimizing scatter. Points from previous frames are shifted radially according
to their dynamic Doppler component to eliminate radial scatter, with each point
assigned a unique aggregation duration based on its Doppler and angle to
minimize tangential scatter. DoppDrive is a point cloud density enhancement
step applied before detection, compatible with any detector, and we demonstrate
that it significantly improves object detection performance across various
detectors and datasets.

</details>


### [208] [Geometry-Aware Video Inpainting for Joint Headset Occlusion Removal and Face Reconstruction in Social XR](https://arxiv.org/abs/2508.12336)
*Fatemeh Ghorbani Lohesara,Karen Eguiazarian,Sebastian Knorr*

Main category: cs.CV

TL;DR: The paper presents a method to remove head-mounted display (HMD) occlusions and reconstruct 3D facial geometry for extended reality (XR) applications, using video inpainting and 3D modeling techniques.


<details>
  <summary>Details</summary>
Motivation: Head-mounted displays obscure the user's face, hindering facial expression and eye gaze communication in XR applications, particularly in teleconferencing.

Method: The study proposes a geometry-aware framework combining a GAN-based video inpainting network, dense facial landmarks, a reference frame, and a SynergyNet-based module to reconstruct 3D facial geometry.

Result: The framework removes HMD occlusions and reconstructs photorealistic 3D facial geometry from RGB frames with minimal degradation under sparse landmarks.

Conclusion: This approach improves immersive XR interactions by enhancing facial realism and identity preservation in scenarios involving occluded faces.

Abstract: Head-mounted displays (HMDs) are essential for experiencing extended reality
(XR) environments and observing virtual content. However, they obscure the
upper part of the user's face, complicating external video recording and
significantly impacting social XR applications such as teleconferencing, where
facial expressions and eye gaze details are crucial for creating an immersive
experience. This study introduces a geometry-aware learning-based framework to
jointly remove HMD occlusions and reconstruct complete 3D facial geometry from
RGB frames captured from a single viewpoint. The method integrates a GAN-based
video inpainting network, guided by dense facial landmarks and a single
occlusion-free reference frame, to restore missing facial regions while
preserving identity. Subsequently, a SynergyNet-based module regresses 3D
Morphable Model (3DMM) parameters from the inpainted frames, enabling accurate
3D face reconstruction. Dense landmark optimization is incorporated throughout
the pipeline to improve both the inpainting quality and the fidelity of the
recovered geometry. Experimental results demonstrate that the proposed
framework can successfully remove HMDs from RGB facial videos while maintaining
facial identity and realism, producing photorealistic 3D face geometry outputs.
Ablation studies further show that the framework remains robust across
different landmark densities, with only minor quality degradation under sparse
landmark configurations.

</details>


### [209] [Semantic Discrepancy-aware Detector for Image Forgery Identification](https://arxiv.org/abs/2508.12341)
*Ziye Wang,Minghang Yu,Chunyan Xu,Zhen Cui*

Main category: cs.CV

TL;DR: The paper introduces the Semantic Discrepancy-aware Detector (SDD) for enhancing forgery detection in images by aligning semantic and forgery concept spaces using reconstruction learning.


<details>
  <summary>Details</summary>
Motivation: The need for robust detection of forged images, given the advancement in image generation techniques, and addressing challenges due to misalignments between forgery and semantic spaces.

Method: Introduces SDD with modules like semantic token sampling, concept-level discrepancy learning based on visual reconstruction, and low-level forgery feature enhancement.

Result: Experiments indicate SDD outperforms existing approaches on standard image forgery datasets.

Conclusion: SDD effectively mitigates semantic-fakery space misalignments, improving detection robustness and accuracy, and the code is provided for further use.

Abstract: With the rapid advancement of image generation techniques, robust forgery
detection has become increasingly imperative to ensure the trustworthiness of
digital media. Recent research indicates that the learned semantic concepts of
pre-trained models are critical for identifying fake images. However, the
misalignment between the forgery and semantic concept spaces hinders the
model's forgery detection performance. To address this problem, we propose a
novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction
learning to align the two spaces at a fine-grained visual level. By exploiting
the conceptual knowledge embedded in the pre-trained vision language model, we
specifically design a semantic token sampling module to mitigate the space
shifts caused by features irrelevant to both forgery traces and semantic
concepts. A concept-level forgery discrepancy learning module, built upon a
visual reconstruction paradigm, is proposed to strengthen the interaction
between visual semantic concepts and forgery traces, effectively capturing
discrepancies under the concepts' guidance. Finally, the low-level forgery
feature enhancemer integrates the learned concept level forgery discrepancies
to minimize redundant forgery information. Experiments conducted on two
standard image forgery datasets demonstrate the efficacy of the proposed SDD,
which achieves superior results compared to existing methods. The code is
available at https://github.com/wzy1111111/SSD.

</details>


### [210] [AquaFeat: A Features-Based Image Enhancement Model for Underwater Object Detection](https://arxiv.org/abs/2508.12343)
*Emanuel C. Silva,Tatiana T. Schein,Stephanie L. Brião,Guilherme L. M. Costa,Felipe G. Oliveira,Gustavo P. Almeida,Eduardo L. Silva,Sam S. Devincenzi,Karina S. Machado,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: AquaFeat introduces a feature enhancement module tailored to underwater object detection tasks, achieving state-of-the-art precision and recall while retaining competitive processing speed.


<details>
  <summary>Details</summary>
Motivation: Underwater environments severely degrade images, negatively impacting object detection models. There is a need for enhancement methods specifically optimized towards improving detection accuracy.

Method: AquaFeat integrates a multi-scale feature enhancement network, trained end-to-end alongside the detector's loss function, to refine features relevant to object detection in underwater images.

Result: AquaFeat, combined with YOLOv8m, demonstrated precision of 0.877, recall of 0.624, mAP@0.5 of 0.677, and mAP@[0.5:0.95] of 0.421 on underwater datasets at 46.5 FPS.

Conclusion: AquaFeat effectively enhances underwater object detection performance, offering high-accuracy outputs while maintaining computational efficiency, making it suitable for real-world applications involving underwater monitoring and inspection.

Abstract: The severe image degradation in underwater environments impairs object
detection models, as traditional image enhancement methods are often not
optimized for such downstream tasks. To address this, we propose AquaFeat, a
novel, plug-and-play module that performs task-driven feature enhancement. Our
approach integrates a multi-scale feature enhancement network trained
end-to-end with the detector's loss function, ensuring the enhancement process
is explicitly guided to refine features most relevant to the detection task.
When integrated with YOLOv8m on challenging underwater datasets, AquaFeat
achieves state-of-the-art Precision (0.877) and Recall (0.624), along with
competitive mAP scores (mAP@0.5 of 0.677 and mAP@[0.5:0.95] of 0.421). By
delivering these accuracy gains while maintaining a practical processing speed
of 46.5 FPS, our model provides an effective and computationally efficient
solution for real-world applications, such as marine ecosystem monitoring and
infrastructure inspection.

</details>


### [211] [MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring](https://arxiv.org/abs/2508.12346)
*Hu Gao,Depeng Dang*

Main category: cs.CV

TL;DR: This paper introduces MBMamba, an enhancement to the Mamba architecture for image deblurring that improves pixel relevance modeling and image coherence while outperforming prior methods on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitations of the Mamba architecture's flatten-and-scan strategy, which struggles with local pixel forgetting and channel redundancy, thereby impairing 2D spatial information aggregation.

Method: A memory buffer strategy is introduced to retain historical information for feature relevance, along with an Ising-inspired regularization loss to ensure spatial coherence and structure preservation, without altering the core Mamba design.

Result: The proposed model, MBMamba, achieves superior performance compared to state-of-the-art image deblurring techniques across common benchmarks.

Conclusion: MBMamba demonstrates that enhancing the original Mamba architecture with a memory buffer and novel loss function can deliver both structural consistency and improved accuracy in image deblurring tasks.

Abstract: The Mamba architecture has emerged as a promising alternative to CNNs and
Transformers for image deblurring. However, its flatten-and-scan strategy often
results in local pixel forgetting and channel redundancy, limiting its ability
to effectively aggregate 2D spatial information. Although existing methods
mitigate this by modifying the scan strategy or incorporating local feature
modules, it increase computational complexity and hinder real-time performance.
In this paper, we propose a structure-aware image deblurring network without
changing the original Mamba architecture. Specifically, we design a memory
buffer mechanism to preserve historical information for later fusion, enabling
reliable modeling of relevance between adjacent features. Additionally, we
introduce an Ising-inspired regularization loss that simulates the energy
minimization of the physical system's "mutual attraction" between pixels,
helping to maintain image structure and coherence. Building on this, we develop
MBMamba. Experimental results show that our method outperforms state-of-the-art
approaches on widely used benchmarks.

</details>


### [212] [EgoLoc: A Generalizable Solution for Temporal Interaction Localization in Egocentric Videos](https://arxiv.org/abs/2508.12349)
*Junyi Ma,Erhang Zhang,Yin-Dong Zheng,Yuchen Xie,Yixuan Zhou,Hesheng Wang*

Main category: cs.CV

TL;DR: This paper introduces EgoLoc, a zero-shot method to localize hand-object contact and separation moments in egocentric videos without requiring object masks or action category annotations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the underexplored problem of identifying the precise moments of hand-object contact and separation, which is critical for applications in VR/AR and robotics.

Method: EgoLoc uses hand-dynamics-guided sampling to generate visual prompts, leverages a vision-language model to identify interaction attributes and timestamps, and employs closed-loop feedback for refinement.

Result: EgoLoc demonstrates strong performance in temporal interaction localization (TIL) on public datasets and novel benchmarks, showing utility in multiple downstream applications.

Conclusion: The proposed EgoLoc method provides an innovative zero-shot approach to fine-grained temporal localization in egocentric vision and improves capabilities in VR/AR and robotic tasks.

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR
applications and human-robot policy transfer. Existing research has mostly
focused on modeling the behavior paradigm of interactive actions (i.e., ``how
to interact''). However, the more challenging and fine-grained problem of
capturing the critical moments of contact and separation between the hand and
the target object (i.e., ``when to interact'') is still underexplored, which is
crucial for immersive interactive experiences in mixed reality and robotic
motion planning. Therefore, we formulate this problem as temporal interaction
localization (TIL). Some recent works extract semantic masks as TIL references,
but suffer from inaccurate object grounding and cluttered scenarios. Although
current temporal action localization (TAL) methods perform well in detecting
verb-noun action segments, they rely on category annotations during training
and exhibit limited precision in localizing hand-object contact/separation
moments. To address these issues, we propose a novel zero-shot approach dubbed
EgoLoc to localize hand-object contact and separation timestamps in egocentric
videos. EgoLoc introduces hand-dynamics-guided sampling to generate
high-quality visual prompts. It exploits the vision-language model to identify
contact/separation attributes, localize specific timestamps, and provide
closed-loop feedback for further refinement. EgoLoc eliminates the need for
object masks and verb-noun taxonomies, leading to generalizable zero-shot
implementation. Comprehensive experiments on the public dataset and our novel
benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric
videos. It is also validated to effectively facilitate multiple downstream
applications in egocentric vision and robotic manipulation tasks. Code and
relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [213] [Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data](https://arxiv.org/abs/2508.12356)
*Ahmet H. Güzel,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.CV

TL;DR: This paper proposes generating additional synthetic training data using data augmentation and diffusion models to improve generalization in offline reinforcement learning (RL) with vision-based data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of limited generalization in offline reinforcement learning (RL) due to restricted diversity in training data and the complexities of noisy visual data.

Method: The paper introduces a two-step approach: (1) augmenting existing offline data to enhance zero-shot generalization and (2) using diffusion models in latent space to generate additional synthetic data.

Result: The proposed method demonstrates significant improvements in zero-shot generalization for both continuous and discrete action spaces, minimizing the generalization gap during testing and maintaining computational efficiency.

Conclusion: The approach effectively increases training data diversity, reduces overfitting, and holds potential for advancing the robustness of offline RL methods with vision-based data.

Abstract: Offline reinforcement learning (RL) offers a promising framework for training
agents using pre-collected datasets without the need for further environment
interaction. However, policies trained on offline data often struggle to
generalise due to limited exposure to diverse states. The complexity of visual
data introduces additional challenges such as noise, distractions, and spurious
correlations, which can misguide the policy and increase the risk of
overfitting if the training data is not sufficiently diverse. Indeed, this
makes it challenging to leverage vision-based offline data in training robust
agents that can generalize to unseen environments. To solve this problem, we
propose a simple approach generating additional synthetic training data. We
propose a two-step process, first augmenting the originally collected offline
data to improve zero-shot generalization by introducing diversity, then using a
diffusion model to generate additional data in latent space. We test our method
across both continuous action spaces (Visual D4RL) and discrete action spaces
(Procgen), demonstrating that it significantly improves generalization without
requiring any algorithmic changes to existing model-free offline RL methods. We
show that our method not only increases the diversity of the training data but
also significantly reduces the generalization gap at test time while
maintaining computational efficiency. We believe this approach could fuel
additional progress in generating synthetic data to train more general agents
in the future.

</details>


### [214] [IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis](https://arxiv.org/abs/2508.12381)
*Guo Tang,Songhan Jiang,Jinpeng Lu,Linghan Cai,Yongbing Zhang*

Main category: cs.CV

TL;DR: The paper introduces IPGPhormer, an interpretable graph-based Transformer framework that improves accuracy and interpretability in survival analysis for cancer prognosis using pathology images.


<details>
  <summary>Details</summary>
Motivation: Existing survival analysis methods struggle with balancing local and long-range spatial dependencies, and lack inherent interpretability, limiting their utility in clinical settings.

Method: The authors proposed IPGPhormer, which utilizes graph-based Transformers to model tumor microenvironment characteristics and spatial dependencies. It offers inherent interpretability at cellular and tissue levels without manual annotations.

Result: IPGPhormer outperformed state-of-the-art methods in predictive accuracy and interpretability across four public benchmark datasets.

Conclusion: IPGPhormer shows high potential as a reliable decision-support tool in pathology for cancer prognosis, bridging accuracy and clinical interpretability.

Abstract: Pathological images play an essential role in cancer prognosis, while
survival analysis, which integrates computational techniques, can predict
critical clinical events such as patient mortality or disease recurrence from
whole-slide images (WSIs). Recent advancements in multiple instance learning
have significantly improved the efficiency of survival analysis. However,
existing methods often struggle to balance the modeling of long-range spatial
relationships with local contextual dependencies and typically lack inherent
interpretability, limiting their clinical utility. To address these challenges,
we propose the Interpretable Pathology Graph-Transformer (IPGPhormer), a novel
framework that captures the characteristics of the tumor microenvironment and
models their spatial dependencies across the tissue. IPGPhormer uniquely
provides interpretability at both tissue and cellular levels without requiring
post-hoc manual annotations, enabling detailed analyses of individual WSIs and
cross-cohort assessments. Comprehensive evaluations on four public benchmark
datasets demonstrate that IPGPhormer outperforms state-of-the-art methods in
both predictive accuracy and interpretability. In summary, our method,
IPGPhormer, offers a promising tool for cancer prognosis assessment, paving the
way for more reliable and interpretable decision-support systems in pathology.
The code is publicly available at
https://anonymous.4open.science/r/IPGPhormer-6EEB.

</details>


### [215] [ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers](https://arxiv.org/abs/2508.12384)
*Hanwen Cao,Haobo Lu,Xiaosen Wang,Kun He*

Main category: cs.CV

TL;DR: The paper introduces ViT-EnsembleAttack, an ensemble-based attack method designed for Vision Transformers (ViTs) that employs adversarial augmentation strategies to enhance adversarial transferability.


<details>
  <summary>Details</summary>
Motivation: Current ensemble-based adversarial attack methods focus on refining weights or paths, neglecting the role of model selection and diversity. The authors aim to increase the generalization and reduce overfitting risks in these ensemble models.

Method: The method involves adversarial augmentation of ViTs using three techniques (Multi-head dropping, Attention score scaling, and MLP feature mixing) with parameter optimization via Bayesian optimization. The augmented models are ensembled to generate adversarial examples, with additional modules like Automatic Reweighting and Step Size Enlargement to improve performance.

Result: Experimental results show ViT-EnsembleAttack significantly outperforms existing ensemble-based attack methods in enhancing adversarial transferability, particularly for ViTs.

Conclusion: The study demonstrates that applying adversarial augmentation strategies and tailoring attacks for specific architectures like ViTs can substantially improve adversarial transferability. The approach sets a new benchmark for ensemble-based attacks on ViTs.

Abstract: Ensemble-based attacks have been proven to be effective in enhancing
adversarial transferability by aggregating the outputs of models with various
architectures. However, existing research primarily focuses on refining
ensemble weights or optimizing the ensemble path, overlooking the exploration
of ensemble models to enhance the transferability of adversarial attacks. To
address this gap, we propose applying adversarial augmentation to the surrogate
models, aiming to boost overall generalization of ensemble models and reduce
the risk of adversarial overfitting. Meanwhile, observing that ensemble Vision
Transformers (ViTs) gain less attention, we propose ViT-EnsembleAttack based on
the idea of model adversarial augmentation, the first ensemble-based attack
method tailored for ViTs to the best of our knowledge. Our approach generates
augmented models for each surrogate ViT using three strategies: Multi-head
dropping, Attention score scaling, and MLP feature mixing, with the associated
parameters optimized by Bayesian optimization. These adversarially augmented
models are ensembled to generate adversarial examples. Furthermore, we
introduce Automatic Reweighting and Step Size Enlargement modules to boost
transferability. Extensive experiments demonstrate that ViT-EnsembleAttack
significantly enhances the adversarial transferability of ensemble-based
attacks on ViTs, outperforming existing methods by a substantial margin. Code
is available at https://github.com/Trustworthy-AI-Group/TransferAttack.

</details>


### [216] [DeCoT: Decomposing Complex Instructions for Enhanced Text-to-Image Generation with Large Language Models](https://arxiv.org/abs/2508.12396)
*Xiaochuan Lin,Xiangyong Chen,Xuan Li,Yichen Su*

Main category: cs.CV

TL;DR: The study introduces DeCoT, a framework utilizing Large Language Models (LLMs) to improve Text-to-Image (T2I) models' understanding and execution of long, complex instructions, yielding more accurate image generation.


<details>
  <summary>Details</summary>
Motivation: Current T2I models struggle with processing complex, detailed textual instructions effectively, as evidenced by deficiencies in areas like composition and fine details, highlighted by the LongBench-T2I benchmark.

Method: DeCoT involves two stages: 1) decomposition of complex instructions into semantic units using LLMs and 2) the creation of tailored hierarchical or single prompts for existing T2I models, combining structured inputs with adaptive generation techniques.

Result: Extensive experiments using LongBench-T2I show significant improvements in T2I model performance, particularly in managing challenging aspects like text composition. DeCoT integrated with Infinity-8B outperforms the baseline model, as validated by both automated metrics and human evaluations.

Conclusion: DeCoT successfully enhances T2I models' capabilities to align user intent with image generation, yielding better fidelity and perceptual quality in generated outputs.

Abstract: Despite remarkable advancements, current Text-to-Image (T2I) models struggle
with complex, long-form textual instructions, frequently failing to accurately
render intricate details, spatial relationships, or specific constraints. This
limitation is highlighted by benchmarks such as LongBench-T2I, which reveal
deficiencies in handling composition, specific text, and fine textures. To
address this, we propose DeCoT (Decomposition-CoT), a novel framework that
leverages Large Language Models (LLMs) to significantly enhance T2I models'
understanding and execution of complex instructions. DeCoT operates in two core
stages: first, Complex Instruction Decomposition and Semantic Enhancement,
where an LLM breaks down raw instructions into structured, actionable semantic
units and clarifies ambiguities; second, Multi-Stage Prompt Integration and
Adaptive Generation, which transforms these units into a hierarchical or
optimized single prompt tailored for existing T2I models. Extensive experiments
on the LongBench-T2I dataset demonstrate that DeCoT consistently and
substantially improves the performance of leading T2I models across all
evaluated dimensions, particularly in challenging aspects like "Text" and
"Composition". Quantitative results, validated by multiple MLLM evaluators
(Gemini-2.0-Flash and InternVL3-78B), show that DeCoT, when integrated with
Infinity-8B, achieves an average score of 3.52, outperforming the baseline
Infinity-8B (3.44). Ablation studies confirm the critical contribution of each
DeCoT component and the importance of sophisticated LLM prompting. Furthermore,
human evaluations corroborate these findings, indicating superior perceptual
quality and instruction fidelity. DeCoT effectively bridges the gap between
high-level user intent and T2I model requirements, leading to more faithful and
accurate image generation.

</details>


### [217] [Federated Cross-Modal Style-Aware Prompt Generation](https://arxiv.org/abs/2508.12399)
*Suraj Prasad,Navyansh Mahla,Sunny Gupta,Amit Sethi*

Main category: cs.CV

TL;DR: The paper introduces FedCSAP, a federated learning framework that enhances prompt generation in vision-language models like CLIP by incorporating multi-scale features and client-specific style indicators.


<details>
  <summary>Details</summary>
Motivation: Traditional prompt learning methods miss important visual cues and domain-specific style variations, limiting their effectiveness in federated learning environments.

Method: FedCSAP utilizes CLIP's low, mid, and high-level features to create style-aware prompt tokens, ensuring privacy via decentralized training and global aggregation.

Result: FedCSAP achieves superior accuracy and generalization across multiple image classification datasets compared to existing methods.

Conclusion: FedCSAP provides a privacy-preserving solution to federated prompt learning, addressing non-IID data and diverse styles, thus improving model performance in decentralized setups.

Abstract: Prompt learning has propelled vision-language models like CLIP to excel in
diverse tasks, making them ideal for federated learning due to computational
efficiency. However, conventional approaches that rely solely on final-layer
features miss out on rich multi-scale visual cues and domain-specific style
variations in decentralized client data. To bridge this gap, we introduce
FedCSAP (Federated Cross-Modal Style-Aware Prompt Generation). Our framework
harnesses low, mid, and high-level features from CLIP's vision encoder
alongside client-specific style indicators derived from batch-level statistics.
By merging intricate visual details with textual context, FedCSAP produces
robust, context-aware prompt tokens that are both distinct and non-redundant,
thereby boosting generalization across seen and unseen classes. Operating
within a federated learning paradigm, our approach ensures data privacy through
local training and global aggregation, adeptly handling non-IID class
distributions and diverse domain-specific styles. Comprehensive experiments on
multiple image classification datasets confirm that FedCSAP outperforms
existing federated prompt learning methods in both accuracy and overall
generalization.

</details>


### [218] [MPCAR: Multi-Perspective Contextual Augmentation for Enhanced Visual Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2508.12400)
*Amirul Rahman,Qiang Xu,Xueying Huang*

Main category: cs.CV

TL;DR: The paper introduces a method called MPCAR to improve the reasoning abilities of Large Vision-Language Models by augmenting contexts at inference time, leading to enhanced performance on Visual Question Answering tasks.


<details>
  <summary>Details</summary>
Motivation: Large Vision-Language Models struggle with complex visual reasoning tasks that require deep contextual understanding, multi-angle analysis, and detailed recognition. Existing methods fall short due to limitations in single-shot image encoding and prompt usage.

Method: In MPCAR, an LVLM generates diverse descriptions or reasoning paths first. These are then combined into a context-augmented prompt, which is used to guide the LVLM in generating deeper reasoning and a final answer. The approach does not require fine-tuning of the model's parameters.

Result: Experiments on datasets like GQA, VQA-CP v2, and ScienceQA show that MPCAR significantly outperforms baseline methods, especially on tasks requiring strong contextual understanding. Human evaluations also confirm better coherence and completeness of generated answers.

Conclusion: MPCAR effectively leverages the inherent generative abilities of LVLMs to create enriched contexts, improving their reasoning capabilities for complex visual and multimodal tasks without model fine-tuning.

Abstract: Despite significant advancements, Large Vision-Language Models (LVLMs)
continue to face challenges in complex visual reasoning tasks that demand deep
contextual understanding, multi-angle analysis, or meticulous detail
recognition. Existing approaches often rely on single-shot image encoding and
prompts, limiting their ability to fully capture nuanced visual information.
Inspired by the notion that strategically generated "additional" information
can serve as beneficial contextual augmentation, we propose Multi-Perspective
Contextual Augmentation for Reasoning (MPCAR), a novel inference-time strategy
designed to enhance LVLM performance. MPCAR operates in three stages: first, an
LVLM generates N diverse and complementary descriptions or preliminary
reasoning paths from various angles; second, these descriptions are
intelligently integrated with the original question to construct a
comprehensive context-augmented prompt; and finally, this enriched prompt
guides the ultimate LVLM for deep reasoning and final answer generation.
Crucially, MPCAR achieves these enhancements without requiring any fine-tuning
of the underlying LVLM's parameters. Extensive experiments on challenging
Visual Question Answering (VQA) datasets, including GQA, VQA-CP v2, and
ScienceQA (Image-VQA), demonstrate that MPCAR consistently outperforms
established baseline methods. Our quantitative results show significant
accuracy gains, particularly on tasks requiring robust contextual
understanding, while human evaluations confirm improved coherence and
completeness of the generated answers. Ablation studies further highlight the
importance of diverse prompt templates and the number of generated
perspectives. This work underscores the efficacy of leveraging LVLMs' inherent
generative capabilities to enrich input contexts, thereby unlocking their
latent reasoning potential for complex multimodal tasks.

</details>


### [219] [LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving](https://arxiv.org/abs/2508.12404)
*Nan Song,Bozhou Zhang,Xiatian Zhu,Jiankang Deng,Li Zhang*

Main category: cs.CV

TL;DR: The paper introduces LMAD, a vision-language framework for autonomous driving, enhancing scene understanding, reasoning, and compatibility with driving systems. It sets new performance benchmarks on related tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in existing VLM fine-tuning methods, which lack comprehensive scene recognition and spatial awareness necessary for autonomous driving in complex scenarios.

Method: LMAD integrates vision-language models with specialized expert adapters and scene interaction capabilities in a task-specific driving structure, designed to be compatible with existing VLMs and planning-oriented systems.

Result: LMAD achieves superior reasoning and explainability performance on the DriveLM and nuScenes-QA datasets, surpassing benchmarks.

Conclusion: The proposed LMAD framework enhances the reasoning, spatial awareness, and adaptability of VLMs, advancing the explainability and capability of autonomous driving tasks.

Abstract: Large vision-language models (VLMs) have shown promising capabilities in
scene understanding, enhancing the explainability of driving behaviors and
interactivity with users. Existing methods primarily fine-tune VLMs on on-board
multi-view images and scene reasoning text, but this approach often lacks the
holistic and nuanced scene recognition and powerful spatial awareness required
for autonomous driving, especially in complex situations. To address this gap,
we propose a novel vision-language framework tailored for autonomous driving,
called LMAD. Our framework emulates modern end-to-end driving paradigms by
incorporating comprehensive scene understanding and a task-specialized
structure with VLMs. In particular, we introduce preliminary scene interaction
and specialized expert adapters within the same driving task structure, which
better align VLMs with autonomous driving scenarios. Furthermore, our approach
is designed to be fully compatible with existing VLMs while seamlessly
integrating with planning-oriented driving systems. Extensive experiments on
the DriveLM and nuScenes-QA datasets demonstrate that LMAD significantly boosts
the performance of existing VLMs on driving reasoning tasks,setting a new
standard in explainable autonomous driving.

</details>


### [220] [S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing](https://arxiv.org/abs/2508.12409)
*Liang Lv,Di Wang,Jing Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: This paper introduces S5, a scalable semi-supervised semantic segmentation framework for remote sensing (RS), enabling efficient use of vast unlabeled Earth observation data.


<details>
  <summary>Details</summary>
Motivation: Current semi-supervised semantic segmentation approaches for RS are limited by small-scale datasets and models, restricting practical applications.

Method: S5 employs entropy-based filtering, diversity expansion for dataset creation (RS4P-1M), pre-trains RS foundation models at scale, and uses a Mixture-of-Experts fine-tuning approach for multi-dataset adaptation.

Result: The RS foundation models (RSFMs) pre-trained and fine-tuned using S5 achieve state-of-the-art performance across multiple RS benchmarks.

Conclusion: Scalable semi-supervised learning methods like S5 can unlock the potential of large-scale Earth observation data, enhancing RS analysis and generalization capabilities.

Abstract: Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS)
analysis by leveraging unlabeled data through pseudo-labeling and consistency
learning. However, existing S4 studies often rely on small-scale datasets and
models, limiting their practical applicability. To address this, we propose S5,
the first scalable framework for semi-supervised semantic segmentation in RS,
which unlocks the potential of vast unlabeled Earth observation data typically
underutilized due to costly pixel-level annotations. Built upon existing
large-scale RS datasets, S5 introduces a data selection strategy that
integrates entropy-based filtering and diversity expansion, resulting in the
RS4P-1M dataset. Using this dataset, we systematically scales S4 methods by
pre-training RS foundation models (RSFMs) of varying sizes on this extensive
corpus, significantly boosting their performance on land cover segmentation and
object detection tasks. Furthermore, during fine-tuning, we incorporate a
Mixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which
enables efficient adaptation to multiple RS benchmarks with fewer parameters.
This approach improves the generalization and versatility of RSFMs across
diverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance
across all benchmarks, underscoring the viability of scaling semi-supervised
learning for RS applications. All datasets, code, and models will be released
at https://github.com/MiliLab/S5

</details>


### [221] [SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes](https://arxiv.org/abs/2508.12410)
*Jun Zeng,Yannan Huang,Elif Keles,Halil Ertugrul Aktas,Gorkem Durak,Nikhil Kumar Tomar,Quoc-Huy Trinh,Deepak Ranjan Nayak,Ulas Bagci,Debesh Jha*

Main category: cs.CV

TL;DR: The study introduces SRMA-Mamba, a novel network for accurate liver cirrhosis segmentation in MRI volumes, enhancing clinical detection and characterization through spatial anatomy modeling.


<details>
  <summary>Details</summary>
Motivation: Liver cirrhosis prognosis faces challenges due to complex anatomical and pathological changes, and existing MRI segmentation methods lack spatial detail utilization, limiting their effectiveness.

Method: The proposed SRMA-Mamba network integrates the Spatial Anatomy-Based Mamba module (SABMamba) for spatial MRI scanning and the Spatial Reverse Attention module (SRMA) for refining segmentation, leveraging data from sagittal, coronal, and axial planes.

Result: Experiments show that the SRMA-Mamba model outperforms current methods in 3D pathological liver segmentation, advancing in accuracy and performance.

Conclusion: SRMA-Mamba significantly enhances automated liver segmentation, facilitating improved clinical diagnostics and demonstrating notable advancements over existing techniques.

Abstract: Liver Cirrhosis plays a critical role in the prognosis of chronic liver
disease. Early detection and timely intervention are critical in significantly
reducing mortality rates. However, the intricate anatomical architecture and
diverse pathological changes of liver tissue complicate the accurate detection
and characterization of lesions in clinical settings. Existing methods
underutilize the spatial anatomical details in volumetric MRI data, thereby
hindering their clinical effectiveness and explainability. To address this
challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to
model the spatial relationships within the complex anatomical structures of MRI
volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba),
SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and
combines anatomical information from the sagittal, coronal, and axial planes to
construct a global spatial context representation, enabling efficient
volumetric segmentation of pathological liver structures. Furthermore, we
introduce the Spatial Reverse Attention module (SRMA), designed to
progressively refine cirrhotic details in the segmentation map, utilizing both
the coarse segmentation map and hierarchical encoding features. Extensive
experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods,
delivering exceptional performance in 3D pathological liver segmentation. Our
code is available for public:
{\color{blue}{https://github.com/JunZengz/SRMA-Mamba}}.

</details>


### [222] [TiP4GEN: Text to Immersive Panorama 4D Scene Generation](https://arxiv.org/abs/2508.12415)
*Ke Xing,Hanwen Liang,Dejia Xu,Yuyang Yin,Konstantinos N. Plataniotis,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: The paper proposes TiP4GEN, a framework that generates high-quality, dynamic panoramic 4D VR/AR scenes with detailed content control.


<details>
  <summary>Details</summary>
Motivation: Existing VR/AR scene generation approaches focus on static or limited dynamic views, lacking true 360-degree immersive capabilities.

Method: TiP4GEN combines panorama video generation and scene reconstruction with a dual-branch model for global and local views and a geometry-aligned reconstruction model to ensure temporal coherence.

Result: TiP4GEN successfully generates dynamic panoramic scenes that are motion-coherent, visually compelling, and geometrically consistent, as demonstrated by extensive experiments.

Conclusion: The proposed framework significantly advances immersive content creation by enabling 360-degree dynamic scene generation for VR/AR applications with improved visual and motion quality.

Abstract: With the rapid advancement and widespread adoption of VR/AR technologies,
there is a growing demand for the creation of high-quality, immersive dynamic
scenes. However, existing generation works predominantly concentrate on the
creation of static scenes or narrow perspective-view dynamic scenes, falling
short of delivering a truly 360-degree immersive experience from any viewpoint.
In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamic
panorama scene generation framework that enables fine-grained content control
and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN
integrates panorama video generation and dynamic scene reconstruction to create
360-degree immersive virtual environments. For video generation, we introduce a
\textbf{Dual-branch Generation Model} consisting of a panorama branch and a
perspective branch, responsible for global and local view generation,
respectively. A bidirectional cross-attention mechanism facilitates
comprehensive information exchange between the branches. For scene
reconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}
based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using
metric depth maps and initializing scene cameras with estimated poses, our
method ensures geometric consistency and temporal coherence for the
reconstructed scenes. Extensive experiments demonstrate the effectiveness of
our proposed designs and the superiority of TiP4GEN in generating visually
compelling and motion-coherent dynamic panoramic scenes. Our project page is at
https://ke-xing.github.io/TiP4GEN/.

</details>


### [223] [Illusions in Humans and AI: How Visual Perception Aligns and Diverges](https://arxiv.org/abs/2508.12422)
*Jianyi Yang,Junyi Ye,Ankan Dash,Guiling Wang*

Main category: cs.CV

TL;DR: This paper compares biological human perception and artificial perception in AI through visual illusions, highlighting differences that can improve AI systems.


<details>
  <summary>Details</summary>
Motivation: To explore the alignment gaps between human and AI visual perceptions and inform the design of safer, reliable, and human-aligned artificial vision systems.

Method: The study systematically investigates responses of AI to classic visual illusions, identifies illusion-like effects, and compares them with human perception.

Result: AI demonstrates some illusion-like effects, but also exhibits unique illusions such as pixel-level sensitivities and hallucinations that differ from human perception.

Conclusion: The findings reveal perceptual vulnerabilities in AI and suggest directions to develop vision systems that align with human beneficial biases while avoiding harmful distortions.

Abstract: By comparing biological and artificial perception through the lens of
illusions, we highlight critical differences in how each system constructs
visual reality. Understanding these divergences can inform the development of
more robust, interpretable, and human-aligned artificial intelligence (AI)
vision systems. In particular, visual illusions expose how human perception is
based on contextual assumptions rather than raw sensory data. As artificial
vision systems increasingly perform human-like tasks, it is important to ask:
does AI experience illusions, too? Does it have unique illusions? This article
explores how AI responds to classic visual illusions that involve color, size,
shape, and motion. We find that some illusion-like effects can emerge in these
models, either through targeted training or as by-products of pattern
recognition. In contrast, we also identify illusions unique to AI, such as
pixel-level sensitivity and hallucinations, that lack human counterparts. By
systematically comparing human and AI responses to visual illusions, we uncover
alignment gaps and AI-specific perceptual vulnerabilities invisible to human
perception. These findings provide insights for future research on vision
systems that preserve human-beneficial perceptual biases while avoiding
distortions that undermine trust and safety.

</details>


### [224] [Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations](https://arxiv.org/abs/2508.12430)
*Yahsin Yeh,Yilun Wu,Bokai Ruan,Honghan Shuai*

Main category: cs.CV

TL;DR: The paper examines vulnerabilities in visual question answering systems with natural language explanations (VQA-NLE) and proposes strategies to strengthen model robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to expose weaknesses in VQA-NLE systems that compromise their ability to generate consistent explanations, thereby highlighting a need for improved reliability and transparency.

Method: The approach combines adversarial strategies to perturb questions and minimally alter images to induce spurious outputs. A novel mitigation method incorporating external knowledge is also proposed to resolve inconsistencies.

Result: The study reveals susceptibility of existing VQA-NLE models to these attacks and demonstrates that a knowledge-enhanced mitigation strategy improves their robustness.

Conclusion: Current VQA-NLE systems face serious security and reliability challenges, but leveraging external knowledge offers a promising direction for addressing these issues.

Abstract: Natural language explanations in visual question answering (VQA-NLE) aim to
make black-box models more transparent by elucidating their decision-making
processes. However, we find that existing VQA-NLE systems can produce
inconsistent explanations and reach conclusions without genuinely understanding
the underlying context, exposing weaknesses in either their inference pipeline
or explanation-generation mechanism. To highlight these vulnerabilities, we not
only leverage an existing adversarial strategy to perturb questions but also
propose a novel strategy that minimally alters images to induce contradictory
or spurious outputs. We further introduce a mitigation method that leverages
external knowledge to alleviate these inconsistencies, thereby bolstering model
robustness. Extensive evaluations on two standard benchmarks and two widely
used VQA-NLE models underscore the effectiveness of our attacks and the
potential of knowledge-based defenses, ultimately revealing pressing security
and reliability concerns in current VQA-NLE systems.

</details>


### [225] [X-Ray-CoT: Interpretable Chest X-ray Diagnosis with Vision-Language Models via Chain-of-Thought Reasoning](https://arxiv.org/abs/2508.12455)
*Chee Ng,Liliang Sun,Shaoqing Tang*

Main category: cs.CV

TL;DR: Chest X-ray imaging demands expertise; existing deep learning models lack interpretability. X-Ray-CoT leverages multi-modal Vision-Language models for explainable diagnoses and report generation, outperforming black-box systems.


<details>
  <summary>Details</summary>
Motivation: Diagnosing chest X-rays requires clinical expertise and suffers from variability, while existing black-box AI models lack transparency hindering trust in medical applications.

Method: X-Ray-CoT extracts multi-modal features, applies Chain-of-Thought reasoning using Vision-Language models, and generates explainable diagnostic reports.

Result: X-Ray-CoT achieves 80.52% Balanced Accuracy and 78.65% F1 score in disease diagnosis, surpassing black-box models and producing validated interpretative reports.

Conclusion: This framework provides a robust AI mechanism that integrates transparency and performance, advancing trustworthy medical imaging practices.

Abstract: Chest X-ray imaging is crucial for diagnosing pulmonary and cardiac diseases,
yet its interpretation demands extensive clinical experience and suffers from
inter-observer variability. While deep learning models offer high diagnostic
accuracy, their black-box nature hinders clinical adoption in high-stakes
medical settings. To address this, we propose X-Ray-CoT (Chest X-Ray
Chain-of-Thought), a novel framework leveraging Vision-Language Large Models
(LVLMs) for intelligent chest X-ray diagnosis and interpretable report
generation. X-Ray-CoT simulates human radiologists' "chain-of-thought" by first
extracting multi-modal features and visual concepts, then employing an
LLM-based component with a structured Chain-of-Thought prompting strategy to
reason and produce detailed natural language diagnostic reports. Evaluated on
the CORDA dataset, X-Ray-CoT achieves competitive quantitative performance,
with a Balanced Accuracy of 80.52% and F1 score of 78.65% for disease
diagnosis, slightly surpassing existing black-box models. Crucially, it
uniquely generates high-quality, explainable reports, as validated by
preliminary human evaluations. Our ablation studies confirm the integral role
of each proposed component, highlighting the necessity of multi-modal fusion
and CoT reasoning for robust and transparent medical AI. This work represents a
significant step towards trustworthy and clinically actionable AI systems in
medical imaging.

</details>


### [226] [Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping](https://arxiv.org/abs/2508.12466)
*Xuhui Zhan,Tyler Derr*

Main category: cs.CV

TL;DR: This paper introduces Inverse-LLaVA, a novel multimodal learning approach that eliminates the need for costly alignment pre-training and maps text into visual representation space, achieving task-specific performance improvements.


<details>
  <summary>Details</summary>
Motivation: Current multimodal learning methods are computationally expensive and rely heavily on alignment pre-training, projecting visual features into text token spaces. The authors aim to simplify this process and challenge its necessity.

Method: Inverse-LLaVA bypasses alignment pre-training by mapping text embeddings into continuous visual spaces instead of projecting visual features to text space. It uses selective additive attention mechanisms within transformer layers for fusion and eliminates the need for large image-text datasets.

Result: The method shows significant improvements in reasoning-intensive tasks (e.g., +27.2% in cognitive reasoning) but reduced performance in perception tasks such as OCR (-21.3%). Computational requirements are reduced by 45%.

Conclusion: Inverse-LLaVA demonstrates that alignment pre-training is not mandatory for multimodal learning, particularly for complex reasoning tasks. It proposes a novel, efficient architecture with task-specific trade-offs and opens up new avenues for research in this area.

Abstract: Traditional multimodal learning approaches require expensive alignment
pre-training to bridge vision and language modalities, typically projecting
visual features into discrete text token spaces. We challenge both fundamental
assumptions underlying this paradigm by proposing Inverse-LLaVA, a novel
approach that eliminates alignment pre-training entirely while inverting the
conventional mapping direction. Rather than projecting visual features to text
space, our method maps text embeddings into continuous visual representation
space and performs fusion within transformer intermediate layers. Through
selective additive components in attention mechanisms, we enable dynamic
integration of visual and textual representations without requiring massive
image-text alignment datasets. Comprehensive experiments across nine multimodal
benchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves
notable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,
VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing
expected decreases in perception tasks requiring memorized visual-text
associations (celebrity recognition: -49.5%, OCR: -21.3%). These results
provide the first empirical evidence that alignment pre-training is not
necessary for effective multimodal learning, particularly for complex reasoning
tasks. Our work establishes the feasibility of a new paradigm that reduces
computational requirements by 45%, challenges conventional wisdom about
modality fusion, and opens new research directions for efficient multimodal
architectures that preserve modality-specific characteristics. Our project
website with code and additional resources is available at
https://inverse-llava.github.io.

</details>


### [227] [Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2508.12473)
*Eranga Bandara,Ross Gore,Sachin Shetty,Ravi Mukkamala,Christopher Rhea,Atmaram Yarlagadda,Shaifali Kaushik,L. H. M. P. De Silva,Andriy Maznychenko,Inna Sokolowska,Amin Hass,Kasun De Zoysa*

Main category: cs.CV

TL;DR: The paper proposes an AI-based system integrating Vision-Language Models (VLMs) and reasoning Large-Language Models (LLMs) to automate and improve the analysis of H-reflex EMG waveforms, enhancing accuracy and interpretability in neuromuscular diagnostics.


<details>
  <summary>Details</summary>
Motivation: There is a need to address variability and interpretation bias in traditional H-reflex EMG waveform analysis, which affects its reliability and standardization in clinical and sports science applications.

Method: The approach utilizes fine-tuned Vision-Language Models trained on annotated datasets of H-reflex EMG images, along with a reasoning Large-Language Model that ensures robust and explainable decision support. Diagnostic outputs are aggregated using consensus methods and refined using automated reasoning workflows.

Result: Experimental findings indicate that the system delivers high accuracy, consistency, and interpretability in H-reflex assessments, advancing the automation of neuromuscular diagnostics.

Conclusion: This hybrid AI system represents a significant technological advancement, paving the way for next-generation AI-assisted neuromuscular assessment and athlete monitoring platforms.

Abstract: Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a
critical role in sports science, rehabilitation, and clinical neurology.
Traditional analysis of H-reflex EMG waveforms is subject to variability and
interpretation bias among clinicians and researchers, limiting reliability and
standardization. To address these challenges, we propose a Fine-Tuned
Vision-Language Model (VLM) Consortium and a reasoning Large-Language Model
(LLM)-enabled Decision Support System for automated H-reflex waveform
interpretation and diagnosis. Our approach leverages multiple VLMs, each
fine-tuned on curated datasets of H-reflex EMG waveform images annotated with
clinical observations, recovery timelines, and athlete metadata. These models
are capable of extracting key electrophysiological features and predicting
neuromuscular states, including fatigue, injury, and recovery, directly from
EMG images and contextual metadata. Diagnostic outputs from the VLM consortium
are aggregated using a consensus-based method and refined by a specialized
reasoning LLM, which ensures robust, transparent, and explainable decision
support for clinicians and sports scientists. The end-to-end platform
orchestrates seamless communication between the VLM ensemble and the reasoning
LLM, integrating prompt engineering strategies and automated reasoning
workflows using LLM Agents. Experimental results demonstrate that this hybrid
system delivers highly accurate, consistent, and interpretable H-reflex
assessments, significantly advancing the automation and standardization of
neuromuscular diagnostics. To our knowledge, this work represents the first
integration of a fine-tuned VLM consortium with a reasoning LLM for image-based
H-reflex analysis, laying the foundation for next-generation AI-assisted
neuromuscular assessment and athlete monitoring platforms.

</details>


### [228] [Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion](https://arxiv.org/abs/2508.12484)
*Shubhi Agarwal,Amulya Kumar Mahto*

Main category: cs.CV

TL;DR: This paper explores hybrid CNN-Transformer architectures with CKAN for improved skin cancer classification, achieving high performance across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate differentiation between malignant and non-malignant skin lesions is vital for early diagnosis and treatment.

Method: The proposed approach combines hybrid CNN-Transformer models with CKAN, leveraging transfer learning and data augmentation to extract spatial and contextual features and fuse representations through learnable activation functions.

Result: The models achieved 92.81% accuracy and 92.47% F1-score on HAM10000, 97.83% accuracy and F1-score on PAD-UFES, and 91.17% accuracy with 91.79% F1-score on BCN20000 datasets.

Conclusion: Hybrid architectures with CKAN show effective feature representation and generalizability, enhancing medical image classification performance.

Abstract: Skin cancer classification is a crucial task in medical image analysis, where
precise differentiation between malignant and non-malignant lesions is
essential for early diagnosis and treatment. In this study, we explore
Sequential and Parallel Hybrid CNN-Transformer models with Convolutional
Kolmogorov-Arnold Network (CKAN). Our approach integrates transfer learning and
extensive data augmentation, where CNNs extract local spatial features,
Transformers model global dependencies, and CKAN facilitates nonlinear feature
fusion for improved representation learning. To assess generalization, we
evaluate our models on multiple benchmark datasets (HAM10000,BCN20000 and
PAD-UFES) under varying data distributions and class imbalances. Experimental
results demonstrate that hybrid CNN-Transformer architectures effectively
capture both spatial and contextual features, leading to improved
classification performance. Additionally, the integration of CKAN enhances
feature fusion through learnable activation functions, yielding more
discriminative representations. Our proposed approach achieves competitive
performance in skin cancer classification, demonstrating 92.81% accuracy and
92.47% F1-score on the HAM10000 dataset, 97.83% accuracy and 97.83% F1-score on
the PAD-UFES dataset, and 91.17% accuracy with 91.79% F1- score on the BCN20000
dataset highlighting the effectiveness and generalizability of our model across
diverse datasets. This study highlights the significance of feature
representation and model design in advancing robust and accurate medical image
classification.

</details>


### [229] [Design and Validation of a Responsible Artificial Intelligence-based System for the Referral of Diabetic Retinopathy Patients](https://arxiv.org/abs/2508.12506)
*E. Ulises Moya-Sánchez,Abraham Sánchez-Perez,Raúl Nanclares Da Veiga,Alejandro Zarate-Macías,Edgar Villareal,Alejandro Sánchez-Montes,Edtna Jauregui-Ulloa,Héctor Moreno,Ulises Cortés*

Main category: cs.CV

TL;DR: RAIS-DR improves diabetic retinopathy screening with notable gains in accuracy, fairness, and ethical considerations compared to existing AI systems.


<details>
  <summary>Details</summary>
Motivation: Early detection of diabetic retinopathy is crucial to prevent vision loss, but current challenges include a shortage of specialists and limitations in AI systems due to biases and poor data quality.

Method: RAIS-DR employs efficient convolutional models for preprocessing, quality assessment, and specialized DR classification alongside fairness assessments.

Result: RAIS-DR outperformed FDA-approved systems, achieving higher F1 scores (5-12%), accuracy (6-19%), and specificity (10-20%) on a dataset of 1,046 patients. It also demonstrated equitable performance across demographic subgroups.

Conclusion: RAIS-DR is an ethically aligned, robust AI system that enhances diabetic retinopathy screening while addressing healthcare disparities in clinical settings.

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss in working-age
individuals. Early detection of DR can reduce the risk of vision loss by up to
95%, but a shortage of retinologists and challenges in timely examination
complicate detection. Artificial Intelligence (AI) models using retinal fundus
photographs (RFPs) offer a promising solution. However, adoption in clinical
settings is hindered by low-quality data and biases that may lead AI systems to
learn unintended features. To address these challenges, we developed RAIS-DR, a
Responsible AI System for DR screening that incorporates ethical principles
across the AI lifecycle. RAIS-DR integrates efficient convolutional models for
preprocessing, quality assessment, and three specialized DR classification
models. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local
dataset of 1,046 patients, unseen by both systems. RAIS-DR demonstrated
significant improvements, with F1 scores increasing by 5-12%, accuracy by
6-19%, and specificity by 10-20%. Additionally, fairness metrics such as
Disparate Impact and Equal Opportunity Difference indicated equitable
performance across demographic subgroups, underscoring RAIS-DR's potential to
reduce healthcare disparities. These results highlight RAIS-DR as a robust and
ethically aligned solution for DR screening in clinical settings. The code,
weights of RAIS-DR are available at
https://gitlab.com/inteligencia-gubernamental-jalisco/jalisco-retinopathy with
RAIL.

</details>


### [230] [LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models](https://arxiv.org/abs/2508.12512)
*Krishna Teja Chitty-Venkata,Murali Emani,Venkatram Vishwanath*

Main category: cs.CV

TL;DR: The paper introduces LangVision-LoRA-NAS, a framework employing Neural Architecture Search (NAS) with Low-Rank Adaptation (LoRA) to improve Vision Language Models (VLMs) by dynamically optimizing rank configurations for task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitation of traditional LoRA implementations that use fixed-rank configurations, which may not be efficient or flexible for diverse multimodal tasks.

Method: The method integrates NAS with LoRA to allow dynamic search of optimal rank configurations tailored for specific multimodal tasks, enabling balanced model performance and computational efficiency.

Result: Extensive experiments on a LLaMA-3.2-11B model across multiple datasets demonstrate enhanced model performance and reduced fine-tuning costs when using LangVision-LoRA-NAS.

Conclusion: LangVision-LoRA-NAS offers a novel approach to optimize multimodal models by combining NAS and LoRA. It improves task-specific performance while reducing computational resource demands, making it a flexible and efficient fine-tuning method.

Abstract: Vision Language Models (VLMs) integrate visual and text modalities to enable
multimodal understanding and generation. These models typically combine a
Vision Transformer (ViT) as an image encoder and a Large Language Model (LLM)
for text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning
method to adapt pre-trained models to new tasks by introducing low-rank updates
to their weights. While LoRA has emerged as a powerful technique for
fine-tuning large models by introducing low-rank updates, current
implementations assume a fixed rank, potentially limiting flexibility and
efficiency across diverse tasks. This paper introduces
\textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural
Architecture Search (NAS) with LoRA to optimize VLMs for variable-rank
adaptation. Our approach leverages NAS to dynamically search for the optimal
LoRA rank configuration tailored to specific multimodal tasks, balancing
performance and computational efficiency. Through extensive experiments using
the LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates
notable improvement in model performance while reducing fine-tuning costs. Our
Base and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be
found
\href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\textcolor{blue}{here}}
and the code for LangVision-LoRA-NAS can be found
\href{https://github.com/krishnateja95/LangVision-NAS}{\textcolor{blue}{here}}.

</details>


### [231] [An Initial Study of Bird's-Eye View Generation for Autonomous Vehicles using Cross-View Transformers](https://arxiv.org/abs/2508.12520)
*Felipe Carlos dos Santos,Eric Aislan Antonelo,Gustavo Claudio Karl Couto*

Main category: cs.CV

TL;DR: The paper explores using Cross-View Transformers (CVT) to create Bird's-Eye View (BEV) maps from camera images for autonomous driving tasks.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the importance of BEV maps in autonomous driving, offering a structured, top-down view required for perception tasks.

Method: The authors utilize Cross-View Transformers to map camera images into BEV channels, investigate its generalization to unseen towns, and evaluate performance under varying conditions such as camera setups and loss functions.

Result: The four-camera CVT model trained with the L1 loss function delivers the best results when tested on a new town, demonstrating robustness.

Conclusion: This approach shows promise in accurately translating camera inputs into BEV maps, even in unseen urban environments.

Abstract: Bird's-Eye View (BEV) maps provide a structured, top-down abstraction that is
crucial for autonomous-driving perception. In this work, we employ Cross-View
Transformers (CVT) for learning to map camera images to three BEV's channels -
road, lane markings, and planned trajectory - using a realistic simulator for
urban driving. Our study examines generalization to unseen towns, the effect of
different camera layouts, and two loss formulations (focal and L1). Using
training data from only a town, a four-camera CVT trained with the L1 loss
delivers the most robust test performance, evaluated in a new town. Overall,
our results underscore CVT's promise for mapping camera inputs to reasonably
accurate BEV maps.

</details>


### [232] [MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training](https://arxiv.org/abs/2508.12522)
*Muhammad Osama Zeeshan,Natacha Gillet,Alessandro Lameiras Koerich,Marco Pedersoli,Francois Bremond,Eric Granger*

Main category: cs.CV

TL;DR: MuSACo proposes a method to improve personalized expression recognition by leveraging multimodal and multi-domain data.


<details>
  <summary>Details</summary>
Motivation: Existing MSDA methods for subject-specific expression recognition often overlook multimodal information and fail to capture unique characteristics of individual subjects.

Method: MuSACo uses a co-training approach, selecting relevant source subjects, generating pseudo-labels through dominant modalities, and aligning multimodal features for personalized adaptation.

Result: Experiments on BioVid and StressID datasets demonstrate MuSACo surpasses current UDA and MSDA methods in improving recognition accuracy.

Conclusion: MuSACo addresses limitations of previous methods, offering a robust solution for subject-specific ER applications in areas like digital health.

Abstract: Personalized expression recognition (ER) involves adapting a machine learning
model to subject-specific data for improved recognition of expressions with
considerable interpersonal variability. Subject-specific ER can benefit
significantly from multi-source domain adaptation (MSDA) methods, where each
domain corresponds to a specific subject, to improve model accuracy and
robustness. Despite promising results, state-of-the-art MSDA approaches often
overlook multimodal information or blend sources into a single domain, limiting
subject diversity and failing to explicitly capture unique subject-specific
characteristics. To address these limitations, we introduce MuSACo, a
multi-modal subject-specific selection and adaptation method for ER based on
co-training. It leverages complementary information across multiple modalities
and multiple source domains for subject-specific adaptation. This makes MuSACo
particularly relevant for affective computing applications in digital health,
such as patient-specific assessment for stress or pain, where subject-level
nuances are crucial. MuSACo selects source subjects relevant to the target and
generates pseudo-labels using the dominant modality for class-aware learning,
in conjunction with a class-agnostic loss to learn from less confident target
samples. Finally, source features from each modality are aligned, while only
confident target features are combined. Our experimental results on challenging
multimodal ER datasets: BioVid and StressID, show that MuSACo can outperform
UDA (blending) and state-of-the-art MSDA methods.

</details>


### [233] [REVEAL -- Reasoning and Evaluation of Visual Evidence through Aligned Language](https://arxiv.org/abs/2508.12543)
*Ipsita Praharaj,Yukta Butala,Yash Butala*

Main category: cs.CV

TL;DR: This paper addresses the challenge of detecting image forgeries by leveraging vision-language models for reasoning and localization. It introduces the REVEAL framework, which evaluates scenes both holistically and region-wise.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the growing challenge of detecting visual forgeries created by advanced generative models, with the need for frameworks that not only detect but provide reasoning and localization.

Method: The framework, REVEAL, uses prompt-driven reasoning via vision-language models and includes two approaches: Holistic Scene-level Evaluation (focused on physics, semantics, perspective, and realism) and Region-wise Anomaly Detection (splitting images into regions for localized analysis).

Result: The framework was tested across domains such as Photoshop, DeepFake, and AIGC editing datasets, showing promising performance in visual forgery detection and reasoning compared to competitive baselines.

Conclusion: REVEAL demonstrates the potential of prompt-driven visual reasoning via vision-language models and sets a foundation for improved generalization in image forgery detection frameworks.

Abstract: The rapid advancement of generative models has intensified the challenge of
detecting and interpreting visual forgeries, necessitating robust frameworks
for image forgery detection while providing reasoning as well as localization.
While existing works approach this problem using supervised training for
specific manipulation or anomaly detection in the embedding space,
generalization across domains remains a challenge. We frame this problem of
forgery detection as a prompt-driven visual reasoning task, leveraging the
semantic alignment capabilities of large vision-language models. We propose a
framework, `REVEAL` (Reasoning and Evaluation of Visual Evidence through
Aligned Language), that incorporates generalized guidelines. We propose two
tangential approaches - (1) Holistic Scene-level Evaluation that relies on the
physics, semantics, perspective, and realism of the image as a whole and (2)
Region-wise anomaly detection that splits the image into multiple regions and
analyzes each of them. We conduct experiments over datasets from different
domains (Photoshop, DeepFake and AIGC editing). We compare the Vision Language
Models against competitive baselines and analyze the reasoning provided by
them.

</details>


### [234] [Structure-preserving Feature Alignment for Old Photo Colorization](https://arxiv.org/abs/2508.12570)
*Yingxue Pang,Xin Jin,Jun Fu,Zhibo Chen*

Main category: cs.CV

TL;DR: The paper presents a deep learning-based approach to colorize old photos using a novel algorithm, SFAC, which does not rely on large datasets and overcomes the domain gap in gray images.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in colorizing old photos, where ground truth data is unavailable, and natural gray images differ significantly in domain from old photos.

Method: The proposed method, SFAC, focuses on establishing semantic correspondence for colorization by employing a feature distribution alignment loss and structure-preserving mechanisms.

Result: SFAC achieves effective old photo colorization, validated through qualitative and quantitative metrics, highlighting its capability despite using only two images.

Conclusion: SFAC addresses domain gaps in old photo colorization without reliance on large-scale datasets and demonstrates robust performance through novel mechanisms.

Abstract: Deep learning techniques have made significant advancements in
reference-based colorization by training on large-scale datasets. However,
directly applying these methods to the task of colorizing old photos is
challenging due to the lack of ground truth and the notorious domain gap
between natural gray images and old photos. To address this issue, we propose a
novel CNN-based algorithm called SFAC, i.e., Structure-preserving Feature
Alignment Colorizer. SFAC is trained on only two images for old photo
colorization, eliminating the reliance on big data and allowing direct
processing of the old photo itself to overcome the domain gap problem. Our
primary objective is to establish semantic correspondence between the two
images, ensuring that semantically related objects have similar colors. We
achieve this through a feature distribution alignment loss that remains robust
to different metric choices. However, utilizing robust semantic correspondence
to transfer color from the reference to the old photo can result in inevitable
structure distortions. To mitigate this, we introduce a structure-preserving
mechanism that incorporates a perceptual constraint at the feature level and a
frozen-updated pyramid at the pixel level. Extensive experiments demonstrate
the effectiveness of our method for old photo colorization, as confirmed by
qualitative and quantitative metrics.

</details>


### [235] [Foundation Model for Skeleton-Based Human Action Understanding](https://arxiv.org/abs/2508.12586)
*Hongsong Wang,Wanjiang Weng,Junbo Wang,Fang Zhao,Guo-Sen Xie,Xin Geng,Liang Wang*

Main category: cs.CV

TL;DR: This paper introduces a Unified Skeleton-based Dense Representation Learning (USDRL) framework to improve human action understanding using skeleton-based models, aiming to address scalability and generalization challenges.


<details>
  <summary>Details</summary>
Motivation: Existing works lack a scalable and generalizable skeleton-based model applicable to diverse human action understanding tasks.

Method: The proposed USDRL framework includes three modules: Dense Spatio-Temporal Encoder (DSTE) for parallel temporal and spatial feature learning, Multi-Grained Feature Decorrelation (MG-FD) to reduce redundancy, and Multi-Perspective Consistency Training (MPCT) combining multi-view and multi-modal self-supervised training.

Result: USDRL significantly outperforms state-of-the-art methods in experiments across 25 benchmarks and 9 skeleton-based prediction tasks.

Conclusion: This work aims to push research boundaries by offering a foundational skeleton-based framework and shifting focus toward dense prediction tasks in human action understanding.

Abstract: Human action understanding serves as a foundational pillar in the field of
intelligent motion perception. Skeletons serve as a modality- and
device-agnostic representation for human modeling, and skeleton-based action
understanding has potential applications in humanoid robot control and
interaction. \RED{However, existing works often lack the scalability and
generalization required to handle diverse action understanding tasks. There is
no skeleton foundation model that can be adapted to a wide range of action
understanding tasks}. This paper presents a Unified Skeleton-based Dense
Representation Learning (USDRL) framework, which serves as a foundational model
for skeleton-based human action understanding. USDRL consists of a
Transformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature
Decorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The
DSTE module adopts two parallel streams to learn temporal dynamic and spatial
structure features. The MG-FD module collaboratively performs feature
decorrelation across temporal, spatial, and instance domains to reduce
dimensional redundancy and enhance information extraction. The MPCT module
employs both multi-view and multi-modal self-supervised consistency training.
The former enhances the learning of high-level semantics and mitigates the
impact of low-level discrepancies, while the latter effectively facilitates the
learning of informative multimodal features. We perform extensive experiments
on 25 benchmarks across across 9 skeleton-based action understanding tasks,
covering coarse prediction, dense prediction, and transferred prediction. Our
approach significantly outperforms the current state-of-the-art methods. We
hope that this work would broaden the scope of research in skeleton-based
action understanding and encourage more attention to dense prediction tasks.

</details>


### [236] [Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models](https://arxiv.org/abs/2508.12587)
*Tan-Hanh Pham,Chris Ngo*

Main category: cs.CV

TL;DR: The paper introduces MCOUT, a method for multimodal reasoning that operates in a latent space rather than natural language, achieving notable accuracy and BLEU score improvements over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current multimodal models struggle to dynamically align information across modalities using language-based reasoning techniques.

Method: MCOUT represents reasoning as continuous hidden vectors refined through multimodal latent attention for better cross-modal alignment.

Result: Experiments show up to 8.23% accuracy gains and 8.27% BLEU score improvements in tasks compared to baselines.

Conclusion: Latent continuous reasoning is effective for multimodal contexts, enhancing large multimodal models with scalable and human-like inference mechanisms.

Abstract: Many reasoning techniques for large multimodal models adapt language model
approaches, such as Chain-of-Thought (CoT) prompting, which express reasoning
as word sequences. While effective for text, these methods are suboptimal for
multimodal contexts, struggling to align audio, visual, and textual information
dynamically. To explore an alternative paradigm, we propose the Multimodal
Chain of Continuous Thought (MCOUT), which enables reasoning directly in a
joint latent space rather than in natural language. In MCOUT, the reasoning
state is represented as a continuous hidden vector, iteratively refined and
aligned with visual and textual embeddings, inspired by human reflective
cognition. We develop two variants: MCOUT-Base, which reuses the language
model`s last hidden state as the continuous thought for iterative reasoning,
and MCOUT-Multi, which integrates multimodal latent attention to strengthen
cross-modal alignment between visual and textual features. Experiments on
benchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently
improves multimodal reasoning, yielding up to 8.23% accuracy gains over strong
baselines and improving BLEU scores up to 8.27% across multiple-choice and
open-ended tasks. These findings highlight latent continuous reasoning as a
promising direction for advancing LMMs beyond language-bound CoT, offering a
scalable framework for human-like reflective multimodal inference. Code is
available at https://github.com/Hanhpt23/OmniMod.

</details>


### [237] [ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.12603)
*Can Cui,Yupeng Zhou,Juntong Peng,Sung-Yeon Park,Zichong Yang,Prashanth Sankaranarayanan,Jiaru Zhang,Ruqi Zhang,Ziran Wang*

Main category: cs.CV

TL;DR: The paper introduces ViLaD, a novel framework for autonomous driving using a Large Vision Language Diffusion (LVLD) model to overcome limitations in traditional autoregressive systems, enabling faster inference speeds and improved decision-making.


<details>
  <summary>Details</summary>
Motivation: Autoregressive Vision Language Models in autonomous driving face latency issues and lack bidirectional reasoning, making them unsuitable for dynamic and safety-critical situations.

Method: ViLaD employs a masked diffusion model for parallel generation of driving decision sequences, supports bidirectional reasoning, and allows iterative quality improvements in decision-making.

Result: ViLaD outperforms current autoregressive VLM systems in planning accuracy and speed on the nuScenes dataset and achieves a near-zero failure rate. The approach is also validated with real-world autonomous parking.

Conclusion: ViLaD addresses the shortcomings of autoregressive models by introducing a faster, more reliable framework well-suited for both experimental benchmarks and real-world autonomous driving applications.

Abstract: End-to-end autonomous driving systems built on Vision Language Models (VLMs)
have shown significant promise, yet their reliance on autoregressive
architectures introduces some limitations for real-world applications. The
sequential, token-by-token generation process of these models results in high
inference latency and cannot perform bidirectional reasoning, making them
unsuitable for dynamic, safety-critical environments. To overcome these
challenges, we introduce ViLaD, a novel Large Vision Language Diffusion (LVLD)
framework for end-to-end autonomous driving that represents a paradigm shift.
ViLaD leverages a masked diffusion model that enables parallel generation of
entire driving decision sequences, significantly reducing computational
latency. Moreover, its architecture supports bidirectional reasoning, allowing
the model to consider both past and future simultaneously, and supports
progressive easy-first generation to iteratively improve decision quality. We
conduct comprehensive experiments on the nuScenes dataset, where ViLaD
outperforms state-of-the-art autoregressive VLM baselines in both planning
accuracy and inference speed, while achieving a near-zero failure rate.
Furthermore, we demonstrate the framework's practical viability through a
real-world deployment on an autonomous vehicle for an interactive parking task,
confirming its effectiveness and soundness for practical applications.

</details>


### [238] [ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images](https://arxiv.org/abs/2508.12605)
*Wenjie Liao,Jieyu Yuan,Yifang Xu,Chunle Guo,Zilong Zhang,Jihong Li,Jiachen Fu,Haotian Fan,Tao Li,Junhui Cui,Chongyi Li*

Main category: cs.CV

TL;DR: The study establishes ViDA-UGC, a novel dataset aimed at explainable Image Quality Assessment (IQA) for User-Generated Content images, leveraging human annotations and GPT-4o-guided Chain-of-Thought frameworks.


<details>
  <summary>Details</summary>
Motivation: Current IQA methods fail to differentiate the evaluation criteria for UGC and AI-Generated images and lack detailed quality analysis for guidance and restoration.

Method: The researchers created ViDA-UGC, an 11K image dataset with detailed quality analysis using a human annotation and GPT-4o-guided Chain-of-Thought approach. A subset of this dataset is refined into ViDA-UGC-Bench.

Result: Experimental evaluations show that ViDA-UGC and the CoT framework consistently enhance image quality assessment capabilities across multiple MLLMs and benchmarks, outperforming GPT-4o.

Conclusion: ViDA-UGC and its benchmarking process offer an effective pathway for advancing explainable IQA methods, with promising implications for UGC image quality restoration and analysis.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have introduced a
paradigm shift for Image Quality Assessment (IQA) from unexplainable image
quality scoring to explainable IQA, demonstrating practical applications like
quality control and optimization guidance. However, current explainable IQA
methods not only inadequately use the same distortion criteria to evaluate both
User-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also
lack detailed quality analysis for monitoring image quality and guiding image
restoration. In this study, we establish the first large-scale Visual
Distortion Assessment Instruction Tuning Dataset for UGC images, termed
ViDA-UGC, which comprises 11K images with fine-grained quality grounding,
detailed quality perception, and reasoning quality description data. This
dataset is constructed through a distortion-oriented pipeline, which involves
human subject annotation and a Chain-of-Thought (CoT) assessment framework.
This framework guides GPT-4o to generate quality descriptions by identifying
and analyzing UGC distortions, which helps capturing rich low-level visual
features that inherently correlate with distortion patterns. Moreover, we
carefully select 476 images with corresponding 6,149 question answer pairs from
ViDA-UGC and invite a professional team to ensure the accuracy and quality of
GPT-generated information. The selected and revised data further contribute to
the first UGC distortion assessment benchmark, termed ViDA-UGC-Bench.
Experimental results demonstrate the effectiveness of the ViDA-UGC and CoT
framework for consistently enhancing various image quality analysis abilities
across multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing
GPT-4o.

</details>


### [239] [OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion](https://arxiv.org/abs/2508.12610)
*Chen Qian,Danyang Li,Xinran Yu,Zheng Yang,Qiang Ma*

Main category: cs.CV

TL;DR: The authors introduce OpenMoCap, a new model for optical motion capture with occlusions, and CMU-Occlu, a dataset simulating realistic marker occlusions, both showing improved motion-solving results.


<details>
  <summary>Details</summary>
Motivation: Current optical motion capture systems struggle with significant occlusions in real-world cases due to poorly reflective datasets and lack of techniques for capturing long-range marker dependencies.

Method: The authors developed CMU-Occlu, a dataset for simulating realistic occlusions using ray tracing, and OpenMoCap, a model utilizing marker-joint chain inference for robust and optimized motion-solving.

Result: OpenMoCap consistently outperformed existing methods in handling diverse occlusion scenarios, while the CMU-Occlu dataset facilitated robust motion solving and future research potential.

Conclusion: The research significantly enhances motion capture performance in occlusive environments, with OpenMoCap offering practical deployment through the MoSen system and open-source availability.

Abstract: Optical motion capture is a foundational technology driving advancements in
cutting-edge fields such as virtual reality and film production. However,
system performance suffers severely under large-scale marker occlusions common
in real-world applications. An in-depth analysis identifies two primary
limitations of current models: (i) the lack of training datasets accurately
reflecting realistic marker occlusion patterns, and (ii) the absence of
training strategies designed to capture long-range dependencies among markers.
To tackle these challenges, we introduce the CMU-Occlu dataset, which
incorporates ray tracing techniques to realistically simulate practical marker
occlusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving
model designed specifically for robust motion capture in environments with
significant occlusions. Leveraging a marker-joint chain inference mechanism,
OpenMoCap enables simultaneous optimization and construction of deep
constraints between markers and joints. Extensive comparative experiments
demonstrate that OpenMoCap consistently outperforms competing methods across
diverse scenarios, while the CMU-Occlu dataset opens the door for future
studies in robust motion solving. The proposed OpenMoCap is integrated into the
MoSen MoCap system for practical deployment. The code is released at:
https://github.com/qianchen214/OpenMoCap.

</details>


### [240] [WIPES: Wavelet-based Visual Primitives](https://arxiv.org/abs/2508.12615)
*Wenhao Zhang,Hao Zhu,Delong Wu,Di Kang,Linchao Bao,Zhan Ma,Xun Cao*

Main category: cs.CV

TL;DR: This paper introduces WIPES, a wavelet-based visual representation for multi-dimensional visual signals, achieving better rendering quality and speed compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a visual representation that combines flexible frequency modulation and fast rendering, overcoming limitations like spectrum loss and slow inference in current methods.

Method: The method involves designing WIPES, a wavelet-based visual primitive that leverages wavelets for spatial-frequency localization and includes a wavelet-based differentiable rasterizer for efficient rendering.

Result: WIPES delivers superior rendering quality and faster performance in tasks like 2D image representation and 5D/6D novel view synthesis compared to neural network-based and Gaussian-based representations.

Conclusion: WIPES successfully addresses both flexibility in frequency representation and rendering speed, setting a new benchmark in the quality and efficiency of visual signal representation.

Abstract: Pursuing a continuous visual representation that offers flexible frequency
modulation and fast rendering speed has recently garnered increasing attention
in the fields of 3D vision and graphics. However, existing representations
often rely on frequency guidance or complex neural network decoding, leading to
spectrum loss or slow rendering. To address these limitations, we propose
WIPES, a universal Wavelet-based vIsual PrimitivES for representing
multi-dimensional visual signals. Building on the spatial-frequency
localization advantages of wavelets, WIPES effectively captures both the
low-frequency "forest" and the high-frequency "trees." Additionally, we develop
a wavelet-based differentiable rasterizer to achieve fast visual rendering.
Experimental results on various visual tasks, including 2D image
representation, 5D static and 6D dynamic novel view synthesis, demonstrate that
WIPES, as a visual primitive, offers higher rendering quality and faster
inference than INR-based methods, and outperforms Gaussian-based
representations in rendering quality.

</details>


### [241] [Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning](https://arxiv.org/abs/2508.12628)
*Yukang Lin,Xiang Zhang,Shichang Jia,Bowen Wan,Chenghan Fu,Xudong Ren,Yueran Liu,Wanxian Guan,Pengji Wang,Jian Xu,Bo Zheng,Baolin Liu*

Main category: cs.CV

TL;DR: This paper addresses the challenge of creative image selection in advertising by proposing an explainable approach using Multimodal Large Language Models (MLLMs), incorporating a new dataset and advanced optimization techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the selection process of creative images in advertising by providing an explainable and user-interest-centric model. Existing methods focus only on rankings, lacking the ability to explain creative quality assessment.

Method: The authors introduced CreativePair, a new dataset of 8k annotated image pairs, and Creative4U, a creative selector powered by MLLMs. They use Reason-to-Select RFT, combining supervised fine-tuning with Chain-of-Thought and Group Relative Policy Optimization for reinforcement learning, to improve both the explanatory and selection capabilities.

Result: The method demonstrated its ability to evaluate and select creative images more effectively through both offline and online experiments.

Conclusion: The proposed paradigm advances creative image assessment and selection, offering a more explainable and scalable approach for advertisers. Code and dataset will be released for broader research and industry impact.

Abstract: Creative image in advertising is the heart and soul of e-commerce platform.
An eye-catching creative image can enhance the shopping experience for users,
boosting income for advertisers and advertising revenue for platforms. With the
advent of AIGC technology, advertisers can produce large quantities of creative
images at minimal cost. However, they struggle to assess the creative quality
to select. Existing methods primarily focus on creative ranking, which fails to
address the need for explainable creative selection.
  In this work, we propose the first paradigm for explainable creative
assessment and selection. Powered by multimodal large language models (MLLMs),
our approach integrates the assessment and selection of creative images into a
natural language generation task. To facilitate this research, we construct
CreativePair, the first comparative reasoning-induced creative dataset
featuring 8k annotated image pairs, with each sample including a label
indicating which image is superior. Additionally, we introduce Creative4U
(pronounced Creative for You), a MLLMs-based creative selector that takes into
account users' interests. Through Reason-to-Select RFT, which includes
supervised fine-tuning with Chain-of-Thought (CoT-SFT) and Group Relative
Policy Optimization (GRPO) based reinforcement learning, Creative4U is able to
evaluate and select creative images accurately. Both offline and online
experiments demonstrate the effectiveness of our approach. Our code and dataset
will be made public to advance research and industrial applications.

</details>


### [242] [SpotVLM: Cloud-edge Collaborative Real-time VLM based on Context Transfer](https://arxiv.org/abs/2508.12638)
*Chen Qian,Xinran Yu,Zewen Huang,Danyang Li,Qiang Ma,Fan Dang,Xuan Ding,Guangyong Shang,Zheng Yang*

Main category: cs.CV

TL;DR: This paper introduces SpotVLM, a new method that uses context transfer to improve real-time processing in Vision-Language Models (VLMs).


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the shortcomings of existing cloud-edge collaborative architectures for VLMs, which are unable to handle cloud latency fluctuations and underutilize delayed but accurate responses from large VLMs.

Method: The paper proposes a paradigm called Context Transfer, where delayed outputs from large Vision-Language Models (LVLMs) guide smaller VLMs (SVLMs) in real-time. SpotVLM incorporates context replacement and visual focus modules for refining inputs and enhancing visual grounding.

Result: Experiments spanning three vision tasks and four datasets validate SpotVLM's effectiveness in delivering more reliable and real-time processing.

Conclusion: SpotVLM offers a latency-aware, cloud-edge collaborative solution that can serve as a template for future VLM systems, combining real-time performance with improved consistency.

Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-time
applications such as autonomous driving and human-computer interaction, which
demand fast and reliable responses based on accurate perception. To meet these
requirements, existing systems commonly employ cloud-edge collaborative
architectures, such as partitioned Large Vision-Language Models (LVLMs) or task
offloading strategies between Large and Small Vision-Language Models (SVLMs).
However, these methods fail to accommodate cloud latency fluctuations and
overlook the full potential of delayed but accurate LVLM responses. In this
work, we propose a novel cloud-edge collaborative paradigm for VLMs, termed
Context Transfer, which treats the delayed outputs of LVLMs as historical
context to provide real-time guidance for SVLMs inference. Based on this
paradigm, we design SpotVLM, which incorporates both context replacement and
visual focus modules to refine historical textual input and enhance visual
grounding consistency. Extensive experiments on three real-time vision tasks
across four datasets demonstrate the effectiveness of the proposed framework.
The new paradigm lays the groundwork for more effective and latency-aware
collaboration strategies in future VLM systems.

</details>


### [243] [Synthesizing Accurate and Realistic T1-weighted Contrast-Enhanced MR Images using Posterior-Mean Rectified Flow](https://arxiv.org/abs/2508.12640)
*Bastian Brandstötter,Erich Kobler*

Main category: cs.CV

TL;DR: The paper introduces a two-stage deep learning method to create contrast-enhanced brain MRIs without the use of gadolinium agents.


<details>
  <summary>Details</summary>
Motivation: Gadolinium-based agents in contrast-enhanced MRIs increase cost, scanning time, and environmental impact, and may present risks to patients.

Method: The two-stage Posterior-Mean Rectified Flow (PMRF) pipeline predicts voxel-wise posterior means with a 3D U-Net in the first stage, followed by a time-conditioned 3D rectified flow for texture refinement.

Result: The method reduced the image fidelity disparity measures (FID by ~68.7% and volumetric mean squared error by ~27%) while providing realistic visual details, validated on 360 test volumes.

Conclusion: The proposed PMRF method produces highly accurate contrast-enhanced synthetic MRIs that maintain structural fidelity and lesion detail, offering potential for clinical applications.

Abstract: Contrast-enhanced (CE) T1-weighted MRI is central to neuro-oncologic
diagnosis but requires gadolinium-based agents, which add cost and scan time,
raise environmental concerns, and may pose risks to patients. In this work, we
propose a two-stage Posterior-Mean Rectified Flow (PMRF) pipeline for
synthesizing volumetric CE brain MRI from non-contrast inputs. First, a
patch-based 3D U-Net predicts the voxel-wise posterior mean (minimizing MSE).
Then, this initial estimate is refined by a time-conditioned 3D rectified flow
to incorporate realistic textures without compromising structural fidelity. We
train this model on a multi-institutional collection of paired pre- and
post-contrast T1w volumes (BraTS 2023-2025). On a held-out test set of 360
diverse volumes, our best refined outputs achieve an axial FID of $12.46$ and
KID of $0.007$ ($\sim 68.7\%$ lower FID than the posterior mean) while
maintaining low volumetric MSE of $0.057$ ($\sim 27\%$ higher than the
posterior mean). Qualitative comparisons confirm that our method restores
lesion margins and vascular details realistically, effectively navigating the
perception-distortion trade-off for clinical deployment.

</details>


### [244] [Learn Faster and Remember More: Balancing Exploration and Exploitation for Continual Test-time Adaptation](https://arxiv.org/abs/2508.12643)
*Pinci Yang,Peisong Wen,Ke Ma,Qianqian Xu*

Main category: cs.CV

TL;DR: This paper introduces a new framework called BEE for continual test-time adaptation (CTTA) that balances quick adaptation to new domains and retaining knowledge from past ones.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenges of balancing exploration (adapting to new target domains) and exploitation (retaining knowledge of past domains) in CTTA, where existing methods have limitations like inefficiency in handling shallow features and forgetting previous domain knowledge.

Method: The BEE framework uses two innovations: Multi-level Consistency Regularization (MCR) aligns intermediate features to speed up adaptation, and Complementary Anchor Replay (CAR) reuses historical checkpoints to recover knowledge for diverse domains.

Result: BEE significantly outperforms state-of-the-art CTTA methods across several benchmarks, showcasing its robustness and effectiveness.

Conclusion: The proposed framework effectively addresses the challenges in CTTA by balancing adaptation speed and historical knowledge retention, making it a promising approach for tasks requiring continual adaptation.

Abstract: Continual Test-Time Adaptation (CTTA) aims to adapt a source pre-trained
model to continually changing target domains during inference. As a fundamental
principle, an ideal CTTA method should rapidly adapt to new domains
(exploration) while retaining and exploiting knowledge from previously
encountered domains to handle similar domains in the future. Despite
significant advances, balancing exploration and exploitation in CTTA is still
challenging: 1) Existing methods focus on adjusting predictions based on
deep-layer outputs of neural networks. However, domain shifts typically affect
shallow features, which are inefficient to be adjusted from deep predictions,
leading to dilatory exploration; 2) A single model inevitably forgets knowledge
of previous domains during the exploration, making it incapable of exploiting
historical knowledge to handle similar future domains. To address these
challenges, this paper proposes a mean teacher framework that strikes an
appropriate Balance between Exploration and Exploitation (BEE) during the CTTA
process. For the former challenge, we introduce a Multi-level Consistency
Regularization (MCR) loss that aligns the intermediate features of the student
and teacher models, accelerating adaptation to the current domain. For the
latter challenge, we employ a Complementary Anchor Replay (CAR) mechanism to
reuse historical checkpoints (anchors), recovering complementary knowledge for
diverse domains. Experiments show that our method significantly outperforms
state-of-the-art methods on several benchmarks, demonstrating its effectiveness
for CTTA tasks.

</details>


### [245] [DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video](https://arxiv.org/abs/2508.12644)
*Hao Wen,Hongbo Kang,Jian Ma,Jing Huang,Yuanwang Yang,Haozhe Lin,Yu-Kun Lai,Kun Li*

Main category: cs.CV

TL;DR: This paper introduces DyCrowd, a method for temporally consistent 3D reconstruction of dynamic crowds in large scene videos, addressing challenges like occlusion and instability through motion optimization and a VAE-based human motion prior.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D reconstruction of crowds lack temporal consistency and fail to mitigate occlusion effects due to reliance on static images.

Method: DyCrowd uses a group-guided motion optimization strategy along with a VAE-based human motion prior and an Asynchronous Motion Consistency (AMC) loss to robustly reconstruct 3D motion under severe occlusion and temporal desynchronization.

Result: DyCrowd achieves state-of-the-art results for reconstructing hundreds of individuals' 3D shapes, positions, and poses in large-scene videos. The authors also introduce VirtualCrowd, a new benchmark dataset.

Conclusion: DyCrowd effectively addresses long-term occlusion and rhythmic inconsistencies for dynamic crowd reconstruction, offering highly realistic and temporally consistent results. The proposed dataset will aid further research in this domain.

Abstract: 3D reconstruction of dynamic crowds in large scenes has become increasingly
important for applications such as city surveillance and crowd analysis.
However, current works attempt to reconstruct 3D crowds from a static image,
causing a lack of temporal consistency and inability to alleviate the typical
impact caused by occlusions. In this paper, we propose DyCrowd, the first
framework for spatio-temporally consistent 3D reconstruction of hundreds of
individuals' poses, positions and shapes from a large-scene video. We design a
coarse-to-fine group-guided motion optimization strategy for occlusion-robust
crowd reconstruction in large scenes. To address temporal instability and
severe occlusions, we further incorporate a VAE (Variational Autoencoder)-based
human motion prior along with a segment-level group-guided optimization. The
core of our strategy leverages collective crowd behavior to address long-term
dynamic occlusions. By jointly optimizing the motion sequences of individuals
with similar motion segments and combining this with the proposed Asynchronous
Motion Consistency (AMC) loss, we enable high-quality unoccluded motion
segments to guide the motion recovery of occluded ones, ensuring robust and
plausible motion recovery even in the presence of temporal desynchronization
and rhythmic inconsistencies. Additionally, in order to fill the gap of no
existing well-annotated large-scene video dataset, we contribute a virtual
benchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction
from large-scene videos. Experimental results demonstrate that the proposed
method achieves state-of-the-art performance in the large-scene dynamic crowd
reconstruction task. The code and dataset will be available for research
purposes.

</details>


### [246] [Stable Diffusion-Based Approach for Human De-Occlusion](https://arxiv.org/abs/2508.12663)
*Seung Young Noh,Ju Yong Chang*

Main category: cs.CV

TL;DR: The paper presents a novel two-stage approach for human de-occlusion using mask and RGB completion, leveraging diffusion-based prior, VQA-based textual features, and fine-tuned Stable Diffusion models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately predicting occluded regions in human images by using prior knowledge and visible cues, which traditional deep learning methods struggle with.

Method: The approach comprises two stages: (1) Mask completion using a diffusion-based human body prior and occluded joint heatmaps; (2) RGB completion guided by the reconstructed mask and enriched with human-specific textual features derived from a visual question answering model encoded via CLIP. RGB refinement leverages fine-tuned Stable Diffusion.

Result: The method achieves effective reconstruction of heavily occluded human appearances and outperforms prior methods in both mask and RGB completion. It also benefits downstream human-centric tasks like 2D pose estimation and 3D reconstruction.

Conclusion: This two-stage framework demonstrates significant progress in de-occlusion for human images, offering not only high-quality reconstructions but also improved performance in related downstream applications. The approach's practical utility is emphasized, and its code will be publicly available.

Abstract: Humans can infer the missing parts of an occluded object by leveraging prior
knowledge and visible cues. However, enabling deep learning models to
accurately predict such occluded regions remains a challenging task.
De-occlusion addresses this problem by reconstructing both the mask and RGB
appearance. In this work, we focus on human de-occlusion, specifically
targeting the recovery of occluded body structures and appearances. Our
approach decomposes the task into two stages: mask completion and RGB
completion. The first stage leverages a diffusion-based human body prior to
provide a comprehensive representation of body structure, combined with
occluded joint heatmaps that offer explicit spatial cues about missing regions.
The reconstructed amodal mask then serves as a conditioning input for the
second stage, guiding the model on which areas require RGB reconstruction. To
further enhance RGB generation, we incorporate human-specific textual features
derived using a visual question answering (VQA) model and encoded via a CLIP
encoder. RGB completion is performed using Stable Diffusion, with decoder
fine-tuning applied to mitigate pixel-level degradation in visible regions -- a
known limitation of prior diffusion-based de-occlusion methods caused by latent
space transformations. Our method effectively reconstructs human appearances
even under severe occlusions and consistently outperforms existing methods in
both mask and RGB completion. Moreover, the de-occluded images generated by our
approach can improve the performance of downstream human-centric tasks, such as
2D pose estimation and 3D human reconstruction. The code will be made publicly
available.

</details>


### [247] [WP-CLIP: Leveraging CLIP to Predict Wölfflin's Principles in Visual Art](https://arxiv.org/abs/2508.12668)
*Abhijay Ghildyal,Li-Yun Wang,Feng Liu*

Main category: cs.CV

TL;DR: This paper investigates using CLIP, a vision-language model, for predicting Wölfflin's art principles. Fine-tuning it led to improved performance in evaluating artistic styles.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of effective metrics for predicting Wölfflin's five stylistic principles in art analysis using computational methods.

Method: The researchers fine-tuned CLIP, a vision-language model, using annotated datasets of real art images. They tested the model, named WP-CLIP, on GAN-generated paintings and a large art dataset to evaluate its capabilities.

Result: The fine-tuned WP-CLIP demonstrated improved ability to generalize and predict Wölfflin's principles across different artistic styles.

Conclusion: Vision-language models, when fine-tuned, show potential for automated art analysis of nuanced stylistic principles.

Abstract: W\"olfflin's five principles offer a structured approach to analyzing
stylistic variations for formal analysis. However, no existing metric
effectively predicts all five principles in visual art. Computationally
evaluating the visual aspects of a painting requires a metric that can
interpret key elements such as color, composition, and thematic choices. Recent
advancements in vision-language models (VLMs) have demonstrated their ability
to evaluate abstract image attributes, making them promising candidates for
this task. In this work, we investigate whether CLIP, pre-trained on
large-scale data, can understand and predict W\"olfflin's principles. Our
findings indicate that it does not inherently capture such nuanced stylistic
elements. To address this, we fine-tune CLIP on annotated datasets of real art
images to predict a score for each principle. We evaluate our model, WP-CLIP,
on GAN-generated paintings and the Pandora-18K art dataset, demonstrating its
ability to generalize across diverse artistic styles. Our results highlight the
potential of VLMs for automated art analysis.

</details>


### [248] [Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation](https://arxiv.org/abs/2508.12680)
*Yuheng Zha,Kun Zhou,Yujia Wu,Yushu Wang,Jie Feng,Zhi Xu,Shibo Hao,Zhengzhong Liu,Eric P. Xing,Zhiting Hu*

Main category: cs.CV

TL;DR: The paper presents Vision-G1, a visual language model trained with a diverse RL-ready dataset, excelling in visual reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning visual language models (VLMs) are limited in the range of tasks they can handle due to a narrow training focus and challenges in integrating diverse datasets.

Method: The authors created a visual reasoning dataset from 46 sources across 8 dimensions and trained the Vision-G1 model using multi-round reinforcement learning, employing data selection and difficulty-based filtering strategies.

Result: Vision-G1 achieved state-of-the-art performance on various visual reasoning benchmarks, surpassing both similar-sized VLMs and proprietary models like GPT-4o and Gemini-1.5 Flash.

Conclusion: Vision-G1 demonstrates effective generalization to various visual reasoning tasks, validating the proposed dataset and training strategies. The resources are publicly available.

Abstract: Despite their success, current training pipelines for reasoning VLMs focus on
a limited range of tasks, such as mathematical and logical reasoning. As a
result, these models face difficulties in generalizing their reasoning
capabilities to a wide range of domains, primarily due to the scarcity of
readily available and verifiable reward data beyond these narrowly defined
areas. Moreover, integrating data from multiple domains is challenging, as the
compatibility between domain-specific datasets remains uncertain. To address
these limitations, we build a comprehensive RL-ready visual reasoning dataset
from 46 data sources across 8 dimensions, covering a wide range of tasks such
as infographic, mathematical, spatial, cross-image, graphic user interface,
medical, common sense and general science. We propose an influence function
based data selection and difficulty based filtering strategy to identify
high-quality training samples from this dataset. Subsequently, we train the
VLM, referred to as Vision-G1, using multi-round RL with a data curriculum to
iteratively improve its visual reasoning capabilities. Our model achieves
state-of-the-art performance across various visual reasoning benchmarks,
outperforming similar-sized VLMs and even proprietary models like GPT-4o and
Gemini-1.5 Flash. The model, code and dataset are publicly available at
https://github.com/yuh-zha/Vision-G1.

</details>


### [249] [Refine-and-Contrast: Adaptive Instance-Aware BEV Representations for Multi-UAV Collaborative Object Detection](https://arxiv.org/abs/2508.12684)
*Zhongyao Li,Peirui Cheng,Liangjin Zhao,Chen Chen,Yundu Li,Zhechao Wang,Xue Yang,Xian Sun,Zhirui Wang*

Main category: cs.CV

TL;DR: AdaBEV proposes a new framework for multi-UAV collaborative 3D detection, focusing on improving BEV feature representation through adaptive and instance-aware techniques.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance multi-UAV 3D detection capabilities by addressing challenges like coverage, occlusion handling, and computational resource limitations on UAV platforms.

Method: AdaBEV introduces the Box-Guided Refinement Module (BG-RM) for refining BEV grids related to foreground instances and Instance-Background Contrastive Learning (IBCL) for better feature distinction between foreground and background.

Result: The framework demonstrates superior accuracy-computation trade-offs and outperforms state-of-the-art methods on the Air-Co-Pred dataset, even at low-resolution settings.

Conclusion: AdaBEV successfully advances 3D detection for UAVs, achieving high performance with efficient processing, thereby addressing computation constraints in UAV-based perception systems.

Abstract: Multi-UAV collaborative 3D detection enables accurate and robust perception
by fusing multi-view observations from aerial platforms, offering significant
advantages in coverage and occlusion handling, while posing new challenges for
computation on resource-constrained UAV platforms. In this paper, we present
AdaBEV, a novel framework that learns adaptive instance-aware BEV
representations through a refine-and-contrast paradigm. Unlike existing methods
that treat all BEV grids equally, AdaBEV introduces a Box-Guided Refinement
Module (BG-RM) and an Instance-Background Contrastive Learning (IBCL) to
enhance semantic awareness and feature discriminability. BG-RM refines only BEV
grids associated with foreground instances using 2D supervision and spatial
subdivision, while IBCL promotes stronger separation between foreground and
background features via contrastive learning in BEV space. Extensive
experiments on the Air-Co-Pred dataset demonstrate that AdaBEV achieves
superior accuracy-computation trade-offs across model scales, outperforming
other state-of-the-art methods at low resolutions and approaching upper bound
performance while maintaining low-resolution BEV inputs and negligible
overhead.

</details>


### [250] [TTA-DAME: Test-Time Adaptation with Domain Augmentation and Model Ensemble for Dynamic Driving Conditions](https://arxiv.org/abs/2508.12690)
*Dongjae Jeon,Taeheon Kim,Seongwon Cho,Minhyuk Seo,Jonghyun Choi*

Main category: cs.CV

TL;DR: The paper introduces TTA-DAME, a method for Test-time Adaptation that uses domain augmentation and domain detectors to handle weather and time changes in real-world driving scenes, achieving improvements on the SHIFT Benchmark.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of adapting models dynamically to shifting target domains, particularly in the context of real-world driving scenes affected by weather or time of day changes.

Method: The method, TTA-DAME, employs source domain data augmentation into target domains, uses a domain discriminator and a domain detector to handle domain shifts, and trains multiple detectors for prediction consolidation through Non-Maximum Suppression (NMS).

Result: Experimental results show that the method significantly improves performance on the SHIFT Benchmark.

Conclusion: TTA-DAME effectively handles dynamic domain shifts in driving scenes, validating its efficacy and offering a robust solution to TTA challenges.

Abstract: Test-time Adaptation (TTA) poses a challenge, requiring models to dynamically
adapt and perform optimally on shifting target domains. This task is
particularly emphasized in real-world driving scenes, where weather domain
shifts occur frequently. To address such dynamic changes, our proposed method,
TTA-DAME, leverages source domain data augmentation into target domains.
Additionally, we introduce a domain discriminator and a specialized domain
detector to mitigate drastic domain shifts, especially from daytime to
nighttime conditions. To further improve adaptability, we train multiple
detectors and consolidate their predictions through Non-Maximum Suppression
(NMS). Our empirical validation demonstrates the effectiveness of our method,
showing significant performance enhancements on the SHIFT Benchmark.

</details>


### [251] [Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning](https://arxiv.org/abs/2508.12692)
*Taeheon Kim,San Kim,Minhyuk Seo,Dongjae Jeon,Wonje Jeong,Jonghyun Choi*

Main category: cs.CV

TL;DR: The paper addresses a realistic learning scenario called Class-Incremental with Repetition (CIR), proposing techniques to improve model performance using unlabeled data.


<details>
  <summary>Details</summary>
Motivation: To enhance model performance in CIR scenarios, where previous classes reappear in future tasks, leveraging easily accessible unlabeled data and overcoming stability-plasticity challenges.

Method: The authors proposed multi-level knowledge distillation (MLKD) to retain knowledge across perspectives and dynamic self-supervised loss (SSL) for accelerated learning of new classes with weighted focus on primary tasks.

Result: The proposed components significantly improved CIR-based models, securing 2nd place in the CVPR 5th CLVISION Challenge.

Conclusion: The techniques effectively utilize unlabeled data to balance stability and plasticity in CIR models, advancing performance in realistic incremental learning settings.

Abstract: Class-incremental with repetition (CIR), where previously trained classes
repeatedly introduced in future tasks, is a more realistic scenario than the
traditional class incremental setup, which assumes that each task contains
unseen classes. CIR assumes that we can easily access abundant unlabeled data
from external sources, such as the Internet. Therefore, we propose two
components that efficiently use the unlabeled data to ensure the high stability
and the plasticity of models trained in CIR setup. First, we introduce
multi-level knowledge distillation (MLKD) that distills knowledge from multiple
previous models across multiple perspectives, including features and logits, so
the model can maintain much various previous knowledge. Moreover, we implement
dynamic self-supervised loss (SSL) to utilize the unlabeled data that
accelerates the learning of new classes, while dynamic weighting of SSL keeps
the focus of training to the primary task. Both of our proposed components
significantly improve the performance in CIR setup, achieving 2nd place in the
CVPR 5th CLVISION Challenge.

</details>


### [252] [Neural Rendering for Sensor Adaptation in 3D Object Detection](https://arxiv.org/abs/2508.12695)
*Felix Embacher,David Holtz,Jonas Uhrig,Marius Cordts,Markus Enzweiler*

Main category: cs.CV

TL;DR: The paper addresses the challenge of cross-sensor domain gaps in autonomous vehicles, affecting 3D object detection. It introduces a dataset (CamShift), identifies robust architectures, and proposes a neural rendering solution to improve performance despite varying camera setups.


<details>
  <summary>Details</summary>
Motivation: To tackle the performance degradation of 3D object detectors when trained on one vehicle's camera setup and deployed on another, caused by varying sensor configurations in autonomous vehicles.

Method: The paper creates a synthetic dataset called CamShift based on CARLA simulations and nuScenes inspiration, analyzes the robustness of existing architectures, and proposes a neural rendering-based data pipeline to adapt sensor setups.

Result: Performance degradation due to cross-sensor domain gaps is demonstrated. BEVFormer and similar architectures show more robustness. The proposed neural rendering approach significantly reduces performance drops by adapting datasets to different sensors.

Conclusion: This work mitigates cross-sensor performance gaps in autonomous vehicles, enabling efficient data reuse and fewer requirements for new data collection. The solutions improve adaptability and accuracy across different sensor setups.

Abstract: Autonomous vehicles often have varying camera sensor setups, which is
inevitable due to restricted placement options for different vehicle types.
Training a perception model on one particular setup and evaluating it on a new,
different sensor setup reveals the so-called cross-sensor domain gap, typically
leading to a degradation in accuracy. In this paper, we investigate the impact
of the cross-sensor domain gap on state-of-the-art 3D object detectors. To this
end, we introduce CamShift, a dataset inspired by nuScenes and created in CARLA
to specifically simulate the domain gap between subcompact vehicles and sport
utility vehicles (SUVs). Using CamShift, we demonstrate significant
cross-sensor performance degradation, identify robustness dependencies on model
architecture, and propose a data-driven solution to mitigate the effect. On the
one hand, we show that model architectures based on a dense Bird's Eye View
(BEV) representation with backward projection, such as BEVFormer, are the most
robust against varying sensor configurations. On the other hand, we propose a
novel data-driven sensor adaptation pipeline based on neural rendering, which
can transform entire datasets to match different camera sensor setups. Applying
this approach improves performance across all investigated 3D object detectors,
mitigating the cross-sensor domain gap by a large margin and reducing the need
for new data collection by enabling efficient data reusability across vehicles
with different sensor setups. The CamShift dataset and the sensor adaptation
benchmark are available at https://dmholtz.github.io/camshift/.

</details>


### [253] [Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection](https://arxiv.org/abs/2508.12711)
*Fanxiao Li,Jiaying Wu,Tingchao Fu,Yunyun Dong,Bingbing Song,Wei Zhou*

Main category: cs.CV

TL;DR: The paper explores vulnerabilities in current multimodal misinformation detection systems caused by Generative AI-driven news diversity, introducing a benchmark called DriftBench to systematically study these challenges.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the growing threats posed by multimodal misinformation and the added complexity introduced by Generative AI, which makes detection increasingly unreliable.

Method: The authors present DriftBench, a benchmark comprising 16,000 diversified news instances. They evaluate systems on three tasks: robustness to multi-level drift, susceptibility to adversarial evidence contamination, and consistency of reasoning across diverse inputs.

Result: Experiments with six state-of-the-art detectors show notable performance declines, including an average F1 drop of 14.8%, and unstable reasoning traces under specific challenges such as adversarial evidence injection.

Conclusion: Current multimodal misinformation detection systems are vulnerable in the GenAI era, necessitating the development of more robust methods to manage content and reasoning diversity challenges effectively.

Abstract: The proliferation of multimodal misinformation poses growing threats to
public discourse and societal trust. While Large Vision-Language Models (LVLMs)
have enabled recent progress in multimodal misinformation detection (MMD), the
rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven
news diversity, characterized by highly varied and complex content. We show
that this diversity induces multi-level drift, comprising (1) model-level
misperception drift, where stylistic variations disrupt a model's internal
reasoning, and (2) evidence-level drift, where expression diversity degrades
the quality or relevance of retrieved external evidence. These drifts
significantly degrade the robustness of current LVLM-based MMD systems. To
systematically study this problem, we introduce DriftBench, a large-scale
benchmark comprising 16,000 news instances across six categories of
diversification. We design three evaluation tasks: (1) robustness of truth
verification under multi-level drift; (2) susceptibility to adversarial
evidence contamination generated by GenAI; and (3) analysis of reasoning
consistency across diverse inputs. Experiments with six state-of-the-art
LVLM-based detectors show substantial performance drops (average F1 -14.8%) and
increasingly unstable reasoning traces, with even more severe failures under
adversarial evidence injection. Our findings uncover fundamental
vulnerabilities in existing MMD systems and suggest an urgent need for more
resilient approaches in the GenAI era.

</details>


### [254] [Real-Time Sign Language Gestures to Speech Transcription using Deep Learning](https://arxiv.org/abs/2508.12713)
*Brandone Fonya*

Main category: cs.CV

TL;DR: This paper presents a real-time system that converts sign language into both text and speech using deep learning techniques.


<details>
  <summary>Details</summary>
Motivation: To address communication barriers for individuals with hearing and speech impairments, enhancing their integration into society.

Method: The system utilizes CNNs trained on the Sign Language MNIST dataset to classify live webcam images of hand gestures and translates them into text and speech.

Result: Experiments demonstrated high classification accuracy and robust performance, enabling real-time translation with some latency.

Conclusion: The technology proves to be practical, accessible, and reliable, promoting autonomy and effective communication for sign language users.

Abstract: Communication barriers pose significant challenges for individuals with
hearing and speech impairments, often limiting their ability to effectively
interact in everyday environments. This project introduces a real-time
assistive technology solution that leverages advanced deep learning techniques
to translate sign language gestures into textual and audible speech. By
employing convolution neural networks (CNN) trained on the Sign Language MNIST
dataset, the system accurately classifies hand gestures captured live via
webcam. Detected gestures are instantaneously translated into their
corresponding meanings and transcribed into spoken language using
text-to-speech synthesis, thus facilitating seamless communication.
Comprehensive experiments demonstrate high model accuracy and robust real-time
performance with some latency, highlighting the system's practical
applicability as an accessible, reliable, and user-friendly tool for enhancing
the autonomy and integration of sign language users in diverse social settings.

</details>


### [255] [Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score](https://arxiv.org/abs/2508.12718)
*Syed Muhmmad Israr,Feng Zhao*

Main category: cs.CV

TL;DR: The paper introduces Dual Contrastive Denoising Score, a method for improved real image editing leveraging text-to-image diffusion models, addressing challenges in prompt accuracy and preserving desired content.


<details>
  <summary>Details</summary>
Motivation: The need to edit real images using large-scale text-to-image generative models without compromising accuracy and consistency posed a challenge, as existing techniques often alter undesired areas or require exact textual prompts.

Method: The proposed Dual Contrastive Denoising Score framework introduces a dual contrastive loss inspired by contrastive learning. This method uses spatial information from self-attention layers but bypasses auxiliary networks and additional training.

Result: The model achieves better control over content modification and structure preservation in real image editing while facilitating zero-shot image-to-image translation.

Conclusion: Dual Contrastive Denoising Score improves upon existing methods for real image editing, offering enhanced usability by reusing pretrained diffusion models and demonstrating superior outcomes in extensive experiments.

Abstract: Large-scale text-to-image generative models have shown remarkable ability to
synthesize diverse and high-quality images. However, it is still challenging to
directly apply these models for editing real images for two reasons. First, it
is difficult for users to come up with a perfect text prompt that accurately
describes every visual detail in the input image. Second, while existing models
can introduce desirable changes in certain regions, they often dramatically
alter the input content and introduce unexpected changes in unwanted regions.
To address these challenges, we present Dual Contrastive Denoising Score, a
simple yet powerful framework that leverages the rich generative prior of
text-to-image diffusion models. Inspired by contrastive learning approaches for
unpaired image-to-image translation, we introduce a straightforward dual
contrastive loss within the proposed framework. Our approach utilizes the
extensive spatial information from the intermediate representations of the
self-attention layers in latent diffusion models without depending on auxiliary
networks. Our method achieves both flexible content modification and structure
preservation between input and output images, as well as zero-shot
image-to-image translation. Through extensive experiments, we show that our
approach outperforms existing methods in real image editing while maintaining
the capability to directly utilize pretrained text-to-image diffusion models
without further training.

</details>


### [256] [Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting](https://arxiv.org/abs/2508.12720)
*Kangjie Chen,Yingji Zhong,Zhihao Li,Jiaqi Lin,Youyu Chen,Minghan Qin,Haoqian Wang*

Main category: cs.CV

TL;DR: The paper addresses the appearance artifacts in sparse-view 3D Gaussian Splatting (3DGS) and proposes lightweight strategies to mitigate the root cause of these artifacts, termed co-adaptation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle the challenges seen in novel view synthesis, specifically with appearance artifacts in sparse-view settings of 3DGS.

Method: The paper proposes a Co-Adaptation Score (CA) metric to measure Gaussian entanglement and introduces two plug-and-play strategies: Gaussian dropout and opacity noise injection.

Result: Findings confirm that co-adaptation leads to artifacts, and the proposed strategies are effective in reducing artifacts across various benchmarks.

Conclusion: Mitigating co-adaptation improves sparse-view 3DGS rendering quality, offering insights for future advancements in novel view synthesis systems.

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel
view synthesis under dense-view settings. However, in sparse-view scenarios,
despite the realistic renderings in training views, 3DGS occasionally manifests
appearance artifacts in novel views. This paper investigates the appearance
artifacts in sparse-view 3DGS and uncovers a core limitation of current
approaches: the optimized Gaussians are overly-entangled with one another to
aggressively fit the training views, which leads to a neglect of the real
appearance distribution of the underlying scene and results in appearance
artifacts in novel views. The analysis is based on a proposed metric, termed
Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians,
i.e., co-adaptation, by computing the pixel-wise variance across multiple
renderings of the same viewpoint, with different random subsets of Gaussians.
The analysis reveals that the degree of co-adaptation is naturally alleviated
as the number of training views increases. Based on the analysis, we propose
two lightweight strategies to explicitly mitigate the co-adaptation in
sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise
injection to the opacity. Both strategies are designed to be plug-and-play, and
their effectiveness is validated across various methods and benchmarks. We hope
that our insights into the co-adaptation effect will inspire the community to
achieve a more comprehensive understanding of sparse-view 3DGS.

</details>


### [257] [Frequency-Driven Inverse Kernel Prediction for Single Image Defocus Deblurring](https://arxiv.org/abs/2508.12736)
*Ying Zhang,Xiongxin Tang,Chongyi Li,Qiao Chen,Yuquan Wu*

Main category: cs.CV

TL;DR: The paper introduces a novel Frequency-Driven Inverse Kernel Prediction network (FDIKP) for single-image defocus deblurring, addressing challenges in accurately modeling spatially varying blur kernels.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the limitations of existing defocus deblurring methods in severely blurry regions where spatial-domain approaches struggle due to missing high-frequency details.

Method: The proposed method, FDIKP, incorporates frequency-domain representations for enhanced kernel modeling, uses Dual-Branch Inverse Kernel Prediction (DIKP) for higher estimation accuracy, Position Adaptive Convolution (PAC) for better deconvolution adaptability, and a Dual-Domain Scale Recurrent Module (DSRM) for refining deblurring quality progressively.

Result: The experimental outcomes show that FDIKP surpasses existing methods in single-image defocus deblurring, offering improved performance and adaptability.

Conclusion: This approach achieves state-of-the-art results by leveraging frequency-domain insights, adaptive deconvolution mechanics, and progressive deblurring refinement techniques. The code will be made publicly available.

Abstract: Single image defocus deblurring aims to recover an all-in-focus image from a
defocus counterpart, where accurately modeling spatially varying blur kernels
remains a key challenge. Most existing methods rely on spatial features for
kernel estimation, but their performance degrades in severely blurry regions
where local high-frequency details are missing. To address this, we propose a
Frequency-Driven Inverse Kernel Prediction network (FDIKP) that incorporates
frequency-domain representations to enhance structural identifiability in
kernel modeling. Given the superior discriminative capability of the frequency
domain for blur modeling, we design a Dual-Branch Inverse Kernel Prediction
(DIKP) strategy that improves the accuracy of kernel estimation while
maintaining stability. Moreover, considering the limited number of predicted
inverse kernels, we introduce a Position Adaptive Convolution (PAC) to enhance
the adaptability of the deconvolution process. Finally, we propose a
Dual-Domain Scale Recurrent Module (DSRM) to fuse deconvolution results and
progressively improve deblurring quality from coarse to fine. Extensive
experiments demonstrate that our method outperforms existing approaches. Code
will be made publicly available.

</details>


### [258] [DCSCR: A Class-Specific Collaborative Representation based Network for Image Set Classification](https://arxiv.org/abs/2508.12745)
*Xizhan Gao,Wei Hu*

Main category: cs.CV

TL;DR: The paper introduces a novel approach, Deep Class-specific Collaborative Representation (DCSCR) network, enhancing few-shot image set classification by integrating traditional methods and deep learning for adaptive feature representation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for image set classification struggle with learning effective features and measuring set similarities, particularly in few-shot scenarios. This highlights the need for a better approach to adaptively learn features and measure distances.

Method: The paper develops a DCSCR network, comprising a deep feature extractor, a global feature learning module, and a class-specific collaborative representation-based metric learning module. It uses a new contrastive loss function to adaptively learn both frame- and concept-level representations.

Result: Experiments across multiple few-shot image set classification datasets showcase that the proposed DCSCR network outperforms state-of-the-art methods in this domain.

Conclusion: Combining traditional ISC methods with deep models, the DCSCR network effectively addresses challenges in feature learning and similarity measurements for few-shot image set classification.

Abstract: Image set classification (ISC), which can be viewed as a task of comparing
similarities between sets consisting of unordered heterogeneous images with
variable quantities and qualities, has attracted growing research attention in
recent years. How to learn effective feature representations and how to explore
the similarities between different image sets are two key yet challenging
issues in this field. However, existing traditional ISC methods classify image
sets based on raw pixel features, ignoring the importance of feature learning.
Existing deep ISC methods can learn deep features, but they fail to adaptively
adjust the features when measuring set distances, resulting in limited
performance in few-shot ISC. To address the above issues, this paper combines
traditional ISC methods with deep models and proposes a novel few-shot ISC
approach called Deep Class-specific Collaborative Representation (DCSCR)
network to simultaneously learn the frame- and concept-level feature
representations of each image set and the distance similarities between
different sets. Specifically, DCSCR consists of a fully convolutional deep
feature extractor module, a global feature learning module, and a
class-specific collaborative representation-based metric learning module. The
deep feature extractor and global feature learning modules are used to learn
(local and global) frame-level feature representations, while the
class-specific collaborative representation-based metric learning module is
exploit to adaptively learn the concept-level feature representation of each
image set and thus obtain the distance similarities between different sets by
developing a new CSCR-based contrastive loss function. Extensive experiments on
several well-known few-shot ISC datasets demonstrate the effectiveness of the
proposed method compared with some state-of-the-art image set classification
algorithms.

</details>


### [259] [D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal](https://arxiv.org/abs/2508.12750)
*Linhao Li,Boya Jin,Zizhe Li,Lanqing Guo,Hao Cheng,Bo Li,Yongfeng Dong*

Main category: cs.CV

TL;DR: The paper proposes a novel network based on Mamba with dual-scale fusion and dual-path scanning for improved shadow removal in images, effectively integrating contextual cues and region-specific adaptations.


<details>
  <summary>Details</summary>
Motivation: Shadow removal involves restoring images degraded by spatially localized and non-uniform shadows. The task leverages contextual info from non-shadow regions but struggles with varying transformations across shadowed areas.

Method: The authors introduced the Dual-Scale Fusion Mamba Block (DFMB) to enhance multi-scale features and reduce boundary artifacts, and the Dual-Path Mamba Group (DPMG) for global feature extraction and mask-aware adaptive scanning.

Result: Experimental results demonstrate consistent and significant outperforming of the proposed method over state-of-the-art shadow removal benchmarks.

Conclusion: The proposed approach effectively integrates non-local contextual cues and adaptive modeling for shadow removal, addressing challenges in structural continuity and fine-grained modeling.

Abstract: Shadow removal aims to restore images that are partially degraded by shadows,
where the degradation is spatially localized and non-uniform. Unlike general
restoration tasks that assume global degradation, shadow removal can leverage
abundant information from non-shadow regions for guidance. However, the
transformation required to correct shadowed areas often differs significantly
from that of well-lit regions, making it challenging to apply uniform
correction strategies. This necessitates the effective integration of non-local
contextual cues and adaptive modeling of region-specific transformations. To
this end, we propose a novel Mamba-based network featuring dual-scale fusion
and dual-path scanning to selectively propagate contextual information based on
transformation similarity across regions. Specifically, the proposed Dual-Scale
Fusion Mamba Block (DFMB) enhances multi-scale feature representation by fusing
original features with low-resolution features, effectively reducing boundary
artifacts. The Dual-Path Mamba Group (DPMG) captures global features via
horizontal scanning and incorporates a mask-aware adaptive scanning strategy,
which improves structural continuity and fine-grained region modeling.
Experimental results demonstrate that our method significantly outperforms
existing state-of-the-art approaches on shadow removal benchmarks.

</details>


### [260] [CLAIRE-DSA: Fluoroscopic Image Classification for Quality Assurance of Computer Vision Pipelines in Acute Ischemic Stroke](https://arxiv.org/abs/2508.12755)
*Cristo J. van den Berg,Frank G. te Nijenhuis,Mirre J. Blaauboer,Daan T. W. van Erp,Carlijn M. Keppels,Matthijs van der Sluijs,Bob Roozenbeek,Wim van Zwam,Sandra Cornelissen,Danny Ruijters,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: This paper introduces CLAIRE-DSA, a deep learning model to classify image properties and improve image quality control during mechanical thrombectomy for stroke patients.


<details>
  <summary>Details</summary>
Motivation: Poor image quality in minimum intensity projections (MinIPs) during MT procedures hinders the performance of computer vision models used to assist in acute ischemic stroke treatment.

Method: The authors designed a framework based on fine-tuned ResNet models to classify nine image properties in fluoroscopic MinIPs, using a labeled dataset of 1,758 images.

Result: The model achieved ROC-AUC scores of 0.91 to 0.98 and improved segmentation task success rates from 42% to 69% by filtering poor quality images.

Conclusion: CLAIRE-DSA shows promise as an automated quality control tool for image classification and enhancement in both clinical and research applications during stroke treatment.

Abstract: Computer vision models can be used to assist during mechanical thrombectomy
(MT) for acute ischemic stroke (AIS), but poor image quality often degrades
performance. This work presents CLAIRE-DSA, a deep learning--based framework
designed to categorize key image properties in minimum intensity projections
(MinIPs) acquired during MT for AIS, supporting downstream quality control and
workflow optimization. CLAIRE-DSA uses pre-trained ResNet backbone models,
fine-tuned to predict nine image properties (e.g., presence of contrast,
projection angle, motion artefact severity). Separate classifiers were trained
on an annotated dataset containing $1,758$ fluoroscopic MinIPs. The model
achieved excellent performance on all labels, with ROC-AUC ranging from $0.91$
to $0.98$, and precision ranging from $0.70$ to $1.00$. The ability of
CLAIRE-DSA to identify suitable images was evaluated on a segmentation task by
filtering poor quality images and comparing segmentation performance on
filtered and unfiltered datasets. Segmentation success rate increased from
$42%$ to $69%$, $p < 0.001$. CLAIRE-DSA demonstrates strong potential as an
automated tool for accurately classifying image properties in DSA series of
acute ischemic stroke patients, supporting image annotation and quality control
in clinical and research applications. Source code is available at
https://gitlab.com/icai-stroke-lab/wp3_neurointerventional_ai/claire-dsa.

</details>


### [261] [Harnessing Group-Oriented Consistency Constraints for Semi-Supervised Semantic Segmentation in CdZnTe Semiconductors](https://arxiv.org/abs/2508.12766)
*Peihao Li,Yan Fang,Man Liu,Huihui Bai,Anhong Wang,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: This paper introduces a specialized framework, Intra-group Consistency Augmentation Framework (ICAF), designed for semantic segmentation of CdZnTe semiconductor images that address challenges in low-contrast defect boundaries and their many-to-one ground truth relationship.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the difficulty in labeling CdZnTe images due to their low contrast and unique many-to-one relationship between views and ground truth, which hinders existing SSS methods causing error accumulation and confirmation bias.

Method: The paper proposes ICAF with two key components: Intra-group View Sampling (IVS) to establish group-oriented baselines and a Pseudo-label Correction Network (PCN) that uses View Augmentation Module (VAM) for detailed boundary synthesis and View Correction Module (VCM) for salient information interaction.

Result: ICAF achieves promising results with a 70.6% mIoU on the CdZnTe dataset even when using limited group-annotated data (just 5 per thousand).

Conclusion: This paper highlights a significant advancement in the domain of imaging segmentation for CdZnTe materials using a deeply innovative group-oriented framework that minimizes errors and enhances boundary accuracy.

Abstract: Labeling Cadmium Zinc Telluride (CdZnTe) semiconductor images is challenging
due to the low-contrast defect boundaries, necessitating annotators to
cross-reference multiple views. These views share a single ground truth (GT),
forming a unique ``many-to-one'' relationship. This characteristic renders
advanced semi-supervised semantic segmentation (SSS) methods suboptimal, as
they are generally limited by a ``one-to-one'' relationship, where each image
is independently associated with its GT. Such limitation may lead to error
accumulation in low-contrast regions, further exacerbating confirmation bias.
To address this issue, we revisit the SSS pipeline from a group-oriented
perspective and propose a human-inspired solution: the Intra-group Consistency
Augmentation Framework (ICAF). First, we experimentally validate the inherent
consistency constraints within CdZnTe groups, establishing a group-oriented
baseline using the Intra-group View Sampling (IVS). Building on this insight,
we introduce the Pseudo-label Correction Network (PCN) to enhance consistency
representation, which consists of two key modules. The View Augmentation Module
(VAM) improves boundary details by dynamically synthesizing a boundary-aware
view through the aggregation of multiple views. In the View Correction Module
(VCM), this synthesized view is paired with other views for information
interaction, effectively emphasizing salient regions while minimizing noise.
Extensive experiments demonstrate the effectiveness of our solution for CdZnTe
materials. Leveraging DeepLabV3+ with a ResNet-101 backbone as our segmentation
model, we achieve a 70.6\% mIoU on the CdZnTe dataset using only 2
group-annotated data (5\textperthousand). The code is available at
\href{https://github.com/pipixiapipi/ICAF}{https://github.com/pipixiapipi/ICAF}.

</details>


### [262] [SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior](https://arxiv.org/abs/2508.12777)
*Wenguang Tao,Xiaotian Wang,Tian Yan,Jie Yan,Guodong Li,Kun Bai*

Main category: cs.CV

TL;DR: This paper presents 'SocialTrack,' a framework tailored for UAV-based multi-object tracking in complex urban environments, overcoming challenges of scale variations, occlusions, and motion blur through novel detection and tracking methods.


<details>
  <summary>Details</summary>
Motivation: To improve the stability and accuracy of UAV-based multi-object tracking in urban traffic systems, specifically addressing challenges like small target scales, occlusions, and motion complexities.

Method: SocialTrack incorporates a small-target detector with multi-scale feature enhancement, a Velocity Adaptive Cubature Kalman Filter (VACKF) for better trajectory prediction, a Group Motion Compensation Strategy (GMCS) for improved target association, and a Spatio-Temporal Memory Prediction (STMP) to mitigate identity switching.

Result: The proposed method achieves superior performance compared to state-of-the-art approaches on UAVDT and MOT17 datasets, showing significant improvements in metrics such as MOTA and IDF1.

Conclusion: SocialTrack demonstrates robust capabilities in overcoming tracking challenges in UAV perspectives, is highly modular and compatible, and offers better performance when integrated with existing tracking systems.

Abstract: As a key research direction in the field of multi-object tracking (MOT),
UAV-based multi-object tracking has significant application value in the
analysis and understanding of urban intelligent transportation systems.
However, in complex UAV perspectives, challenges such as small target scale
variations, occlusions, nonlinear crossing motions, and motion blur severely
hinder the stability of multi-object tracking. To address these challenges,
this paper proposes a novel multi-object tracking framework, SocialTrack, aimed
at enhancing the tracking accuracy and robustness of small targets in complex
urban traffic environments. The specialized small-target detector enhances the
detection performance by employing a multi-scale feature enhancement mechanism.
The Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of
trajectory prediction by incorporating a velocity dynamic modeling mechanism.
The Group Motion Compensation Strategy (GMCS) models social group motion priors
to provide stable state update references for low-quality tracks, significantly
improving the target association accuracy in complex dynamic environments.
Furthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical
trajectory information to predict the future state of low-quality tracks,
effectively mitigating identity switching issues. Extensive experiments on the
UAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing
state-of-the-art (SOTA) methods across several key metrics. Significant
improvements in MOTA and IDF1, among other core performance indicators,
highlight its superior robustness and adaptability. Additionally, SocialTrack
is highly modular and compatible, allowing for seamless integration with
existing trackers to further enhance performance.

</details>


### [263] [Leveraging Diffusion Models for Stylization using Multiple Style Images](https://arxiv.org/abs/2508.12784)
*Dan Ruta,Abdelaziz Djelouah,Raphael Ortiz,Christopher Schroers*

Main category: cs.CV

TL;DR: This paper introduces an improved method for image style transfer using latent diffusion models, focusing on addressing key limitations like accurate style matching and content-style disentanglement.


<details>
  <summary>Details</summary>
Motivation: Current image style transfer methods face issues with style accuracy, limited style image integration, and content-style entanglement.

Method: The authors propose a technique leveraging multiple style images, using image prompt adapters and statistical alignment during denoising, integrating interventions at cross-attention and self-attention layers. Clustering is employed for distilling representative attention features.

Result: The proposed method demonstrates state-of-the-art performance in image stylization, as validated by experiments.

Conclusion: Leveraging multiple style images and advanced feature alignment techniques offers significant improvements in style transfer capabilities, addressing previous shortcomings.

Abstract: Recent advances in latent diffusion models have enabled exciting progress in
image style transfer. However, several key issues remain. For example, existing
methods still struggle to accurately match styles. They are often limited in
the number of style images that can be used. Furthermore, they tend to entangle
content and style in undesired ways. To address this, we propose leveraging
multiple style images which helps better represent style features and prevent
content leaking from the style images. We design a method that leverages both
image prompt adapters and statistical alignment of the features during the
denoising process. With this, our approach is designed such that it can
intervene both at the cross-attention and the self-attention layers of the
denoising UNet. For the statistical alignment, we employ clustering to distill
a small representative set of attention features from the large number of
attention values extracted from the style samples. As demonstrated in our
experimental section, the resulting method achieves state-of-the-art results
for stylization.

</details>


### [264] [Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision](https://arxiv.org/abs/2508.12794)
*Kyriaki,Kokka,Rahul Goel,Ali Abbas,Kerry A. Nice,Luca Martial,SM Labib,Rihuan Ke,Carola Bibiane Schönlieb,James Woodcock*

Main category: cs.CV

TL;DR: The paper introduces a novel method combining Google Street View (GSV) images with deep learning to estimate cycling and motorcycling levels across 185 global cities.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comparative global data on cycling and motorcycling behaviors, which influence health through physical activity, air pollution, and injury risks.

Method: The study utilizes YOLOv4, fine-tuned on GSV images, for detecting cycles and motorcycles. Beta regression models predict mode shares using GSV counts and city-level data such as population density.

Result: The YOLOv4 model achieved an 89% mean average precision. Predictions yielded correlations of 0.78 for motorcycles and 0.51 for cycles between image counts and mode shares. R² for mode share predictions was 0.614 for cycling and 0.612 for motorcycling, with consistent accuracy but some city outliers.

Conclusion: The approach using computer vision on street view imagery is effective in analyzing travel modes globally, complementing traditional data sources by providing new insights in diverse contexts.

Abstract: Transportation influence health by shaping exposure to physical activity, air
pollution and injury risk.Comparative data on cycling and motorcycling
behaviours is scarce, particularly at a global scale.Street view imagery, such
as Google Street View (GSV), combined with computer vision, is a valuable
resource for efficiently capturing travel behaviour data.This study
demonstrates a novel approach using deep learning on street view images to
estimate cycling and motorcycling levels across diverse cities worldwide.We
utilized data from 185 global cities.The data on mode shares of cycling and
motorcycling estimated using travel surveys or censuses.We used GSV images to
detect cycles and motorcycles in sampled locations, using 8000 images per
city.The YOLOv4 model, fine-tuned using images from six cities, achieved a mean
average precision of 89% for detecting cycles and motorcycles in GSV images.A
global prediction model was developed using beta regression with city-level
mode shares as outcome, with log transformed explanatory variables of counts of
GSV-detected images with cycles and motorcycles, while controlling for
population density.We found strong correlations between GSV motorcycle counts
and motorcycle mode share (0.78) and moderate correlations between GSV cycle
counts and cycling mode share (0.51).Beta regression models predicted mode
shares with $R^2$ values of 0.614 for cycling and 0.612 for motorcycling,
achieving median absolute errors (MDAE) of 1.3% and 1.4%,
respectively.Scatterplots demonstrated consistent prediction accuracy, though
cities like Utrecht and Cali were outliers.The model was applied to 60 cities
globally for which we didn't have recent mode share data.We provided estimates
for some cities in the Middle East, Latin America and East Asia.With computer
vision, GSV images capture travel modes and activity, providing insights
alongside traditional data sources.

</details>


### [265] [Morphological classification of eclipsing binary stars using computer vision methods](https://arxiv.org/abs/2508.12802)
*Štefan Parimucha,Maksim Gabdeev,Yanna Markus,Martin Vaňko,Pavol Gajdoš*

Main category: cs.CV

TL;DR: The paper applies computer vision models, including ResNet50 and vision transformers, to classify eclipsing binary systems using polar image transformations with high accuracy but highlights limitations in spot detection.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance the classification of eclipsing binaries using computer vision to improve generalization and reduce overfitting while enabling large-scale survey analysis.

Method: Pre-trained CNNs and vision transformers were fine-tuned on synthetic datasets with novel image representations for polar light curves. A hierarchical classification approach was developed to distinguish binary types and detect spots.

Result: Binary classification achieved over 96% accuracy across passbands and demonstrated strong performance on observational data while spot detection showed poor accuracy.

Conclusion: The study highlights the efficacy of computer vision in large-scale classification of eclipsing binaries, but emphasizes that automated photometric spot detection remains a challenge requiring further research.

Abstract: We present an application of computer vision methods to classify the light
curves of eclipsing binaries (EB). We have used pre-trained models based on
convolutional neural networks ($\textit{ResNet50}$) and vision transformers
($\textit{vit\_base\_patch16\_224}$), which were fine-tuned on images created
from synthetic datasets. To improve model generalisation and reduce
overfitting, we developed a novel image representation by transforming
phase-folded light curves into polar coordinates combined with hexbin
visualisation. Our hierarchical approach in the first stage classifies systems
into detached and overcontact types, and in the second stage identifies the
presence or absence of spots. The binary classification models achieved high
accuracy ($>96\%$) on validation data across multiple passbands (Gaia~$G$, $I$,
and $TESS$) and demonstrated strong performance ($>94\%$, up to $100\%$ for
$TESS$) when tested on extensive observational data from the OGLE, DEBCat, and
WUMaCat catalogues. While the primary binary classification was highly
successful, the secondary task of automated spot detection performed poorly,
revealing a significant limitation of our models for identifying subtle
photometric features. This study highlights the potential of computer vision
for EB morphological classification in large-scale surveys, but underscores the
need for further research into robust, automated spot detection.

</details>


### [266] [Next Visual Granularity Generation](https://arxiv.org/abs/2508.12811)
*Yikai Wang,Zhouxia Wang,Zhonghua Wu,Qingyi Tao,Kang Liao,Chen Change Loy*

Main category: cs.CV

TL;DR: The paper presents a novel "Next Visual Granularity" (NVG) framework for image generation, decomposing images into structured sequences that progressively refine visual details. The framework outperforms prior methods like VAR in FID scores.


<details>
  <summary>Details</summary>
Motivation: The motivation for this study stems from the need for fine-grained control in image generation across multiple visual granularities, aiming to improve hierarchical representation and generation quality.

Method: The authors introduce the NVG framework, which performs image generation iteratively, beginning with an empty image and refining it from global layout to fine details. It utilizes hierarchical and structured sequences at different visual granularities.

Result: Experiments on the ImageNet dataset show that NVG consistently achieves better FID scores compared to the VAR series and demonstrates scalable behavior.

Conclusion: The NVG framework provides a more efficient and granular approach to image generation, achieving superior performance and control. Extensive analyses confirm its potential, and the authors will release the code and models.

Abstract: We propose a novel approach to image generation by decomposing an image into
a structured sequence, where each element in the sequence shares the same
spatial resolution but differs in the number of unique tokens used, capturing
different level of visual granularity. Image generation is carried out through
our newly introduced Next Visual Granularity (NVG) generation framework, which
generates a visual granularity sequence beginning from an empty image and
progressively refines it, from global layout to fine details, in a structured
manner. This iterative process encodes a hierarchical, layered representation
that offers fine-grained control over the generation process across multiple
granularity levels. We train a series of NVG models for class-conditional image
generation on the ImageNet dataset and observe clear scaling behavior. Compared
to the VAR series, NVG consistently outperforms it in terms of FID scores (3.30
-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to
showcase the capability and potential of the NVG framework. Our code and models
will be released.

</details>


### [267] [SIS-Challenge: Event-based Spatio-temporal Instance Segmentation Challenge at the CVPR 2025 Event-based Vision Workshop](https://arxiv.org/abs/2508.12813)
*Friedhelm Hamann,Emil Mededovic,Fabian Gülhan,Yuli Wu,Johannes Stegmaier,Jing He,Yiqing Wang,Kexin Zhang,Lingling Li,Licheng Jiao,Mengru Ma,Hongxiang Huang,Yuhao Yan,Hongwei Ren,Xiaopeng Lin,Yulong Huang,Bojun Cheng,Se Hyun Lee,Gyu Sung Ham,Kanghan Oh,Gi Hyun Lim,Boxuan Yang,Bowen Du,Guillermo Gallego*

Main category: cs.CV

TL;DR: This paper summarizes the Spatio-temporal Instance Segmentation (SIS) challenge from CVPR 2025, focusing on predicting pixel-level object segmentation using event camera and grayscale camera data.


<details>
  <summary>Details</summary>
Motivation: To enhance research in instance segmentation, specifically leveraging spatio-temporal event-based camera data alongside grayscale camera data.

Method: The paper reviews the SIS challenge setup, including its task, dataset, and competition structure. It also outlines methods from the top-5 teams in the competition.

Result: The challenge led to insights into effective methods for spatio-temporal segmentation, culminating in shared resources and participant codes for further study.

Conclusion: The SIS challenge advanced knowledge in combining event-based and grayscale imaging for instance segmentation and highlighted effective approaches via the top-performing methods.

Abstract: We present an overview of the Spatio-temporal Instance Segmentation (SIS)
challenge held in conjunction with the CVPR 2025 Event-based Vision Workshop.
The task is to predict accurate pixel-level segmentation masks of defined
object classes from spatio-temporally aligned event camera and grayscale camera
data. We provide an overview of the task, dataset, challenge details and
results. Furthermore, we describe the methods used by the top-5 ranking teams
in the challenge. More resources and code of the participants' methods are
available here:
https://github.com/tub-rip/MouseSIS/blob/main/docs/challenge_results.md

</details>


### [268] [DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in Submerged Aquatics](https://arxiv.org/abs/2508.12824)
*Shuang Chen,Ronald Thenius,Farshad Arvin,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: The paper presents DEEP-SEA, a deep learning model to restore underwater image quality for improved ecological monitoring, species identification, and navigation.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in underwater image clarity caused by light scattering, absorption, and turbidity, which hinder effective ecological monitoring.

Method: The authors propose a Dual-Frequency Enhanced Self-Attention model to enhance low- and high-frequency features while preserving spatial structure in underwater images.

Result: Experimental results on EUVP and LSUI datasets show superior performance compared to existing methods in restoring image detail and structural consistency.

Conclusion: DEEP-SEA effectively mitigates underwater visual degradation, thus advancing underwater monitoring’s accuracy and reliability in ecological, species, and navigation tasks.

Abstract: Continuous and reliable underwater monitoring is essential for assessing
marine biodiversity, detecting ecological changes and supporting autonomous
exploration in aquatic environments. Underwater monitoring platforms rely on
mainly visual data for marine biodiversity analysis, ecological assessment and
autonomous exploration. However, underwater environments present significant
challenges due to light scattering, absorption and turbidity, which degrade
image clarity and distort colour information, which makes accurate observation
difficult. To address these challenges, we propose DEEP-SEA, a novel deep
learning-based underwater image restoration model to enhance both low- and
high-frequency information while preserving spatial structures. The proposed
Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to
adaptively refine feature representations in frequency domains and
simultaneously spatial information for better structural preservation. Our
comprehensive experiments on EUVP and LSUI datasets demonstrate the superiority
over the state of the art in restoring fine-grained image detail and structural
consistency. By effectively mitigating underwater visual degradation, DEEP-SEA
has the potential to improve the reliability of underwater monitoring platforms
for more accurate ecological observation, species identification and autonomous
navigation.

</details>


### [269] [Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection](https://arxiv.org/abs/2508.12842)
*Ronghao Lin,Sijie Mai,Ying Zeng,Qiaolin He,Aolin Xiong,Haifeng Hu*

Main category: cs.CV

TL;DR: The paper introduces a framework called MMPDA to address domain shift issues in multimodal deception detection and demonstrates state-of-the-art performance in the MMDD Challenge.


<details>
  <summary>Details</summary>
Motivation: To resolve the domain shift problem across source and target domains in multimodal datasets for deception detection.

Method: The authors propose a Multi-source Multimodal Progressive Domain Adaptation (MMPDA) framework which progressively aligns source and target domains at both feature and decision levels using audio-visual knowledge.

Result: The approach achieved Top-2 ranking in the MMDD Challenge, with accuracy of 60.43% and F1-score of 56.99%, significantly outperforming the 1st and 3rd ranked teams in F1-score and accuracy respectively.

Conclusion: The proposed MMPDA framework effectively bridges domain shifts in multimodal datasets, demonstrating superior performance in deception detection tasks.

Abstract: This paper presents the winning approach for the 1st MultiModal Deception
Detection (MMDD) Challenge at the 1st Workshop on Subtle Visual Computing
(SVC). Aiming at the domain shift issue across source and target domains, we
propose a Multi-source Multimodal Progressive Domain Adaptation (MMPDA)
framework that transfers the audio-visual knowledge from diverse source domains
to the target domain. By gradually aligning source and the target domain at
both feature and decision levels, our method bridges domain shifts across
diverse multimodal datasets. Extensive experiments demonstrate the
effectiveness of our approach securing Top-2 place. Our approach reaches 60.43%
on accuracy and 56.99\% on F1-score on competition stage 2, surpassing the 1st
place team by 5.59% on F1-score and the 3rd place teams by 6.75% on accuracy.
Our code is available at https://github.com/RH-Lin/MMPDA.

</details>


### [270] [Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models](https://arxiv.org/abs/2508.12861)
*Dexia Chen,Wentao Zhang,Qianjie Zhu,Ping Hu,Weibing Li,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: The paper introduces a fine-tuning strategy, CoMuCo, to overcome the limitations of vision-language models in cross-domain few-shot tasks by leveraging multi-view features and incorporating consistency and consensus mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the ineffectiveness of vision-language models in cross-domain scenarios where image domains differ from their training datasets (natural images).

Method: The proposed CoMuCo strategy uses two complementary expert modules for multi-view feature extraction, and incorporates prior knowledge-based consistency constraints alongside information geometry-based consensus mechanisms to improve robustness.

Result: Experiments show that CoMuCo outperforms current fine-tuning methods in cross-domain few-shot tasks on both the new and existing benchmarks.

Conclusion: CoMuCo is an effective approach to enhance the performance of vision-language models in cross-domain few-shot learning by leveraging consistency and collaborative optimization of multi-view features. Benchmarking tools will also be released to support further research.

Abstract: Vision-language models (VLMs) pre-trained on natural image and language data,
such as CLIP, have exhibited significant potential in few-shot image
recognition tasks, leading to development of various efficient transfer
learning methods. These methods exploit inherent pre-learned knowledge in VLMs
and have achieved strong performance on standard image datasets. However, their
effectiveness is often limited when confronted with cross-domain tasks where
imaging domains differ from natural images. To address this limitation, we
propose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), a
novel fine-tuning strategy for VLMs. This strategy employs two functionally
complementary expert modules to extract multi-view features, while
incorporating prior knowledge-based consistency constraints and information
geometry-based consensus mechanisms to enhance the robustness of feature
learning. Additionally, a new cross-domain few-shot benchmark is established to
help comprehensively evaluate methods on imaging domains distinct from natural
images. Extensive empirical evaluations on both existing and newly proposed
benchmarks suggest CoMuCo consistently outperforms current methods in few-shot
tasks. The code and benchmark will be released.

</details>


### [271] [Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning](https://arxiv.org/abs/2508.12877)
*Dexia Chen,Qianjie Zhu,Weibing Li,Yue Yu,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: This paper introduces a fine-tuning method, MPS-Tuning, aimed at preserving and enhancing the geometric structure of semantic manifolds derived from pretrained vision-language models like CLIP for improved few-shot image classification.


<details>
  <summary>Details</summary>
Motivation: Pretrained vision-language models excel at domain adaptation and transfer learning, but existing regularization methods risk distorting semantic representation by ignoring the geometric data distribution.

Method: The proposed MPS-Tuning method aligns Gram matrices before and after fine-tuning to preserve manifold geometry and pairs image-text features to enhance class separability. This approach approximates an upper bound of the Gromov-Wasserstein distance.

Result: Experiments confirm that MPS-Tuning boosts model performance while maintaining the semantic manifold's intrinsic structure.

Conclusion: MPS-Tuning effectively addresses limitations in regularizing pretrained VLMs, ensuring structure preservation and improved domain adaptation results.

Abstract: Pretrained vision-language models (VLMs), such as CLIP, have shown remarkable
potential in few-shot image classification and led to numerous effective
transfer learning strategies. These methods leverage the pretrained knowledge
of VLMs to enable effective domain adaptation while mitigating overfitting
through parameter-efficient tuning or instance-based consistency constraints.
However, such regularizations often neglect the geometric structure of data
distribution, which may lead to distortion of the overall semantic
representation. To overcome this limitation, we propose a novel fine-tuning
method, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding the
data distribution in feature space as a semantic manifold, MPS-Tuning
explicitly constrains the intrinsic geometry of this manifold while further
sculpting it to enhance class separability. Specifically, MPS-Tuning preserves
both macroscopic and microscopic topological structures of the original
manifold by aligning Gram matrices of features before and after fine-tuning.
Theoretically, this constraint is shown to approximate an upper bound of the
Gromov-Wasserstein distance. Furthermore, features from the image and text
modalities are paired, and pairwise similarities are optimized to enhance the
manifold's class discriminability. Extensive experiments demonstrate that
MPS-Tuning significantly improves model performance while effectively
preserving the structure of the semantic manifold. The code will be released.

</details>


### [272] [S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models](https://arxiv.org/abs/2508.12880)
*Chubin Chen,Jiashu Zhu,Xiaokun Feng,Nisha Huang,Meiqi Wu,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Xiu Li*

Main category: cs.CV

TL;DR: The paper identifies shortcomings in Classifier-free Guidance (CFG) and proposes an alternative method, S^2-Guidance, to improve diffusion model outputs through stochastic block-dropping and sub-network refinement.


<details>
  <summary>Details</summary>
Motivation: To address the suboptimal results and semantic incoherence caused by excessive reliance on Classifier-free Guidance (CFG) predictions in diffusion models.

Method: The proposed S^2-Guidance method uses stochastic block-dropping in the forward process to construct stochastic sub-networks, refining model predictions and improving output quality.

Result: Extensive experiments on text-to-image and text-to-video tasks showed that S^2-Guidance outperformed CFG and other guidance methods in both qualitative and quantitative assessments.

Conclusion: S^2-Guidance offers a superior alternative to CFG, effectively addressing its shortcomings and leading to higher-quality outputs in diffusion models.

Abstract: Classifier-free Guidance (CFG) is a widely used technique in modern diffusion
models for enhancing sample quality and prompt adherence. However, through an
empirical analysis on Gaussian mixture modeling with a closed-form solution, we
observe a discrepancy between the suboptimal results produced by CFG and the
ground truth. The model's excessive reliance on these suboptimal predictions
often leads to semantic incoherence and low-quality outputs. To address this
issue, we first empirically demonstrate that the model's suboptimal predictions
can be effectively refined using sub-networks of the model itself. Building on
this insight, we propose S^2-Guidance, a novel method that leverages stochastic
block-dropping during the forward process to construct stochastic sub-networks,
effectively guiding the model away from potential low-quality predictions and
toward high-quality outputs. Extensive qualitative and quantitative experiments
on text-to-image and text-to-video generation tasks demonstrate that
S^2-Guidance delivers superior performance, consistently surpassing CFG and
other advanced guidance strategies. Our code will be released.

</details>


### [273] [ONG: One-Shot NMF-based Gradient Masking for Efficient Model Sparsification](https://arxiv.org/abs/2508.12891)
*Sankar Behera,Yamuna Prasad*

Main category: cs.CV

TL;DR: ONG (One-shot NMF-based Gradient Masking) is a novel method for sparsifying deep neural networks at the beginning of training to maintain efficiency and sparsity.


<details>
  <summary>Details</summary>
Motivation: To address challenges with existing pruning techniques, which often require iterative steps, complex criteria, or fail to preserve sparsity during training.

Method: ONG utilizes Non-negative Matrix Factorization (NMF) for one-shot pruning and employs a gradient masking technique to ensure only unpruned weights are updated throughout training.

Result: Experiments show that ONG achieves comparable or better performance on CIFAR-10 and CIFAR-100 with ResNet models while maintaining structural integrity and desired sparsity levels.

Conclusion: ONG offers an efficient and effective one-shot sparsification method that simplifies deployment and retains high model performance at various sparsity levels.

Abstract: Deep Neural Networks (DNNs) have achieved remarkable success but their large
size poses deployment challenges. While various pruning techniques exist, many
involve complex iterative processes, specialized criteria, or struggle to
maintain sparsity effectively during training. We introduce ONG (One-shot
NMF-based Gradient Masking), a novel sparsification strategy that identifies
salient weight structures using Non-negative Matrix Factorization (NMF) for
one-shot pruning at the outset of training. Subsequently, ONG employs a precise
gradient masking mechanism to ensure that only unpruned weights are updated,
strictly preserving the target sparsity throughout the training phase. We
integrate ONG into the BIMP comparative framework and evaluate it on CIFAR-10
and CIFAR-100 with ResNet56, ResNet34, and ResNet18 against established stable
sparsification methods. Our experiments demonstrate ONG's ability to achieve
comparable or superior performance at various sparsity levels while maintaining
structural integrity post-pruning and offering a clear mechanism for targeting
desired sparsities.

</details>


### [274] [CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis](https://arxiv.org/abs/2508.12900)
*Jiayi Wang,Hadrien Reynaud,Franciskus Xaverius Erick,Bernhard Kainz*

Main category: cs.CV

TL;DR: The paper introduces CTFlow, a generative model for CT volumes conditioned on clinical reports, leveraging advanced techniques for coherent and diverse data synthesis.


<details>
  <summary>Details</summary>
Motivation: To address the need for tools that accelerate research through privacy-preserving synthesis of CT data and support regulatory compliance while maintaining diagnostic specificity.

Method: CTFlow uses a latent flow matching transformer with A-VAE defining the latent space and a custom autoregressive generation approach for CT volume synthesis, based on clinical report encoding via a CT-Clip text encoder.

Result: CTFlow demonstrates superior performance compared to state-of-the-art generative CT models, assessed through metrics like FID, FVD, IS, and CLIP scores, ensuring coherence, diversity, and alignment.

Conclusion: CTFlow effectively generates consistent whole CT volumes conditioned on text, offering advantages for medical imaging research, data augmentation, and privacy-protecting synthesis.

Abstract: Generative modelling of entire CT volumes conditioned on clinical reports has
the potential to accelerate research through data augmentation,
privacy-preserving synthesis and reducing regulator-constraints on patient data
while preserving diagnostic signals. With the recent release of CT-RATE, a
large-scale collection of 3D CT volumes paired with their respective clinical
reports, training large text-conditioned CT volume generation models has become
achievable. In this work, we introduce CTFlow, a 0.5B latent flow matching
transformer model, conditioned on clinical reports. We leverage the A-VAE from
FLUX to define our latent space, and rely on the CT-Clip text encoder to encode
the clinical reports. To generate consistent whole CT volumes while keeping the
memory constraints tractable, we rely on a custom autoregressive approach,
where the model predicts the first sequence of slices of the volume from
text-only, and then relies on the previously generated sequence of slices and
the text, to predict the following sequence. We evaluate our results against
state-of-the-art generative CT model, and demonstrate the superiority of our
approach in terms of temporal coherence, image diversity and text-image
alignment, with FID, FVD, IS scores and CLIP score.

</details>


### [275] [CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction](https://arxiv.org/abs/2508.12917)
*Zhiwei Ning,Zhaojiang Liu,Xuanang Gao,Yifan Zuo,Jie Yang,Yuming Fang,Wei Liu*

Main category: cs.CV

TL;DR: The paper introduces a multi-modal 3D detection framework called CMF-IOU, which combines data from LiDAR and cameras via multi-stage cross-modal fusion for improved feature extraction and 3D detection performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of single or partial stage fusion methods in multi-modal 3D detection, which often lead to insufficient feature extraction and lower performance.

Method: The proposed method involves projecting camera pixel information to 3D space to align it with LiDAR data, developing a dual-branch encoding system (S2D and ResVC branches) for robust feature representation, and employing iterative voxel-point pooling and IoU-based proposal refinement for accurate 3D object detection.

Result: The CMF-IOU method achieves superior performance compared to existing techniques, as demonstrated by its evaluations on the KITTI, nuScenes, and Waymo datasets.

Conclusion: The CMF-IOU framework effectively bridges 2D and 3D data representation through multi-stage cross-modal fusion, resulting in heightened accuracy and robustness in 3D detection tasks.

Abstract: Multi-modal methods based on camera and LiDAR sensors have garnered
significant attention in the field of 3D detection. However, many prevalent
works focus on single or partial stage fusion, leading to insufficient feature
extraction and suboptimal performance. In this paper, we introduce a
multi-stage cross-modal fusion 3D detection framework, termed CMF-IOU, to
effectively address the challenge of aligning 3D spatial and 2D semantic
information. Specifically, we first project the pixel information into 3D space
via a depth completion network to get the pseudo points, which unifies the
representation of the LiDAR and camera information. Then, a bilateral
cross-view enhancement 3D backbone is designed to encode LiDAR points and
pseudo points. The first sparse-to-distant (S2D) branch utilizes an
encoder-decoder structure to reinforce the representation of sparse LiDAR
points. The second residual view consistency (ResVC) branch is proposed to
mitigate the influence of inaccurate pseudo points via both the 3D and 2D
convolution processes. Subsequently, we introduce an iterative voxel-point
aware fine grained pooling module, which captures the spatial information from
LiDAR points and textural information from pseudo points in the proposal
refinement stage. To achieve more precise refinement during iteration, an
intersection over union (IoU) joint prediction branch integrated with a novel
proposals generation technique is designed to preserve the bounding boxes with
both high IoU and classification scores. Extensive experiments show the
superior performance of our method on the KITTI, nuScenes and Waymo datasets.

</details>


### [276] [7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models](https://arxiv.org/abs/2508.12919)
*Elena Izzo,Luca Parolari,Davide Vezzaro,Lamberto Ballan*

Main category: cs.CV

TL;DR: The paper introduces 7Bench, a benchmark assessing semantic and spatial alignment in layout-guided text-to-image models, addressing a gap in current evaluations.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks largely ignore spatial alignment in layout-guided text-to-image models despite its importance for applications like synthetic data generation.

Method: The study designs 7Bench, which involves diverse text-and-layout pairs across seven challenging scenarios, combining semantic and spatial assessments. It also introduces a layout alignment scoring protocol.

Result: 7Bench successfully evaluates state-of-the-art diffusion models, revealing their strengths and weaknesses in semantic and spatial alignment tasks.

Conclusion: 7Bench fills a crucial benchmarking gap, enabling better evaluation of layout-guided text-to-image models in terms of both semantic and spatial fidelity.

Abstract: Layout-guided text-to-image models offer greater control over the generation
process by explicitly conditioning image synthesis on the spatial arrangement
of elements. As a result, their adoption has increased in many computer vision
applications, ranging from content creation to synthetic data generation. A
critical challenge is achieving precise alignment between the image, textual
prompt, and layout, ensuring semantic fidelity and spatial accuracy. Although
recent benchmarks assess text alignment, layout alignment remains overlooked,
and no existing benchmark jointly evaluates both. This gap limits the ability
to evaluate a model's spatial fidelity, which is crucial when using
layout-guided generation for synthetic data, as errors can introduce noise and
degrade data quality. In this work, we introduce 7Bench, the first benchmark to
assess both semantic and spatial alignment in layout-guided text-to-image
generation. It features text-and-layout pairs spanning seven challenging
scenarios, investigating object generation, color fidelity, attribute
recognition, inter-object relationships, and spatial control. We propose an
evaluation protocol that builds on existing frameworks by incorporating the
layout alignment score to assess spatial accuracy. Using 7Bench, we evaluate
several state-of-the-art diffusion models, uncovering their respective
strengths and limitations across diverse alignment tasks. The benchmark is
available at https://github.com/Elizzo/7Bench.

</details>


### [277] [Towards High-Resolution Industrial Image Anomaly Detection](https://arxiv.org/abs/2508.12931)
*Ximiao Zhang,Min Xu,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: This paper introduces HiAD, a framework for detecting anomalies in high-resolution images, addressing shortcomings in existing methods with a dual-branch architecture and multi-resolution feature fusion.


<details>
  <summary>Details</summary>
Motivation: Current methods often miss subtle anomalies in high-resolution images due to downsampling. Industrial applications demand better accuracy and efficiency.

Method: HiAD combines a dual-branch architecture, multi-resolution feature fusion, and a detector pool with adaptive strategies for efficient anomaly detection.

Result: Experiments on benchmarks like MVTec-HD, VisA-HD, and RealIAD-HD showcase HiAD's superior detection performance compared to existing methods.

Conclusion: HiAD effectively addresses detection challenges in high-resolution anomaly detection while balancing accuracy and computational efficiency.

Abstract: Current anomaly detection methods primarily focus on low-resolution
scenarios. For high-resolution images, conventional downsampling often results
in missed detections of subtle anomalous regions due to the loss of
fine-grained discriminative information. Despite some progress, recent studies
have attempted to improve detection resolution by employing lightweight
networks or using simple image tiling and ensemble methods. However, these
approaches still struggle to meet the practical demands of industrial scenarios
in terms of detection accuracy and efficiency. To address the above issues, we
propose HiAD, a general framework for high-resolution anomaly detection. HiAD
is capable of detecting anomalous regions of varying sizes in high-resolution
images under limited computational resources. Specifically, HiAD employs a
dual-branch architecture that integrates anomaly cues across different scales
to comprehensively capture both subtle and large-scale anomalies. Furthermore,
it incorporates a multi-resolution feature fusion strategy to tackle the
challenges posed by fine-grained texture variations in high-resolution images.
To enhance both adaptability and efficiency, HiAD utilizes a detector pool in
conjunction with various detector assignment strategies, enabling detectors to
be adaptively assigned based on patch features, ensuring detection performance
while effectively controlling computational costs. We conduct extensive
experiments on our specifically constructed high-resolution anomaly detection
benchmarks, including MVTec-HD, VisA-HD, and the real-world benchmark
RealIAD-HD, demonstrating the superior performance of HiAD. The code is
available at https://github.com/cnulab/HiAD.

</details>


### [278] [SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory](https://arxiv.org/abs/2508.12932)
*Hongyang Chen,Shaoling Pu,Lingyu Zheng,Zhongwu Sun*

Main category: cs.CV

TL;DR: SEDEG is a new two-stage incremental learning framework for vision transformers, enhancing both encoder and decoder generalization to improve adaptability and mitigate catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: The research addresses the challenge of catastrophic forgetting in incremental learning, especially in limited-memory scenarios, by aiming to improve the generalization of both encoder and decoder components.

Method: SEDEG employs a two-stage training process: first, it trains an ensembled encoder to boost generalized representations for the decoder; second, it applies knowledge distillation to compress the encoder and create a more generalized version.

Result: The model demonstrates superior performance in experiments across three benchmark datasets, with ablation studies validating its components.

Conclusion: SEDEG presents an effective solution for incremental learning with promising results in addressing catastrophic forgetting, using a balanced approach in enhancing encoder and decoder generality.

Abstract: In incremental learning, enhancing the generality of knowledge is crucial for
adapting to dynamic data inputs. It can develop generalized representations or
more balanced decision boundaries, preventing the degradation of long-term
knowledge over time and thus mitigating catastrophic forgetting. Some emerging
incremental learning methods adopt an encoder-decoder architecture and have
achieved promising results. In the encoder-decoder achitecture, improving the
generalization capabilities of both the encoder and decoder is critical, as it
helps preserve previously learned knowledge while ensuring adaptability and
robustness to new, diverse data inputs. However, many existing continual
methods focus solely on enhancing one of the two components, which limits their
effectiveness in mitigating catastrophic forgetting. And these methods perform
even worse in small-memory scenarios, where only a limited number of historical
samples can be stored. To mitigate this limitation, we introduces SEDEG, a
two-stage training framework for vision transformers (ViT), focusing on
sequentially improving the generality of both Decoder and Encoder. Initially,
SEDEG trains an ensembled encoder through feature boosting to learn generalized
representations, which subsequently enhance the decoder's generality and
balance the classifier. The next stage involves using knowledge distillation
(KD) strategies to compress the ensembled encoder and develop a new, more
generalized encoder. This involves using a balanced KD approach and feature KD
for effective knowledge transfer. Extensive experiments on three benchmark
datasets show SEDEG's superior performance, and ablation studies confirm the
efficacy of its components. The code is available at
https://github.com/ShaolingPu/CIL.

</details>


### [279] [Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data](https://arxiv.org/abs/2508.12942)
*Kyriaki-Margarita Bintsi,Yaël Balbastre,Jingjing Wu,Julia F. Lehman,Suzanne N. Haber,Anastasia Yendiki*

Main category: cs.CV

TL;DR: The paper introduces a fully automated framework using a U-Net architecture for fiber bundle segmentation in macaque tracer data, addressing limitations in existing methods and providing improved accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Validating and improving diffusion MRI tractography requires accurate fiber bundle segmentation, but existing methods struggle with sparse bundles and require manual effort.

Method: A U-Net architecture is employed, utilizing large patch sizes, foreground-aware sampling, and semi-supervised pre-training for automated segmentation.

Result: Compared to state-of-the-art approaches, the framework detects sparse bundles with over 20% improvement, reduces False Discovery Rate (FDR) by 40%, and supports standalone slice analysis.

Conclusion: The framework enhances the scalability and reliability of automated anatomic tracing, potentially generating substantial ground-truth data to improve dMRI tractography validation.

Abstract: Anatomic tracer studies are critical for validating and improving diffusion
MRI (dMRI) tractography. However, large-scale analysis of data from such
studies is hampered by the labor-intensive process of annotating fiber bundles
manually on histological slides. Existing automated methods often miss sparse
bundles or require complex post-processing across consecutive sections,
limiting their flexibility and generalizability. We present a streamlined,
fully automated framework for fiber bundle segmentation in macaque tracer data,
based on a U-Net architecture with large patch sizes, foreground aware
sampling, and semisupervised pre-training. Our approach eliminates common
errors such as mislabeling terminals as bundles, improves detection of sparse
bundles by over 20% and reduces the False Discovery Rate (FDR) by 40% compared
to the state-of-the-art, all while enabling analysis of standalone slices. This
new framework will facilitate the automated analysis of anatomic tracing data
at a large scale, generating more ground-truth data that can be used to
validate and optimize dMRI tractography methods.

</details>


### [280] [Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models](https://arxiv.org/abs/2508.12945)
*Jianshu Zeng,Yuxuan Liu,Yutong Feng,Chenxuan Miao,Zixiang Gao,Jiwang Qu,Jianzhang Zhang,Bin Wang,Kun Yuan*

Main category: cs.CV

TL;DR: The paper introduces 'Lumen,' an end-to-end video relighting framework that adjusts video lighting and background using generative models and a comprehensive dataset of synthetic and realistic video pairs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the challenge of video relighting, where harmoniously blending background changes and consistent foreground lighting are crucial, despite the lack of paired videos with diverse lighting conditions.

Method: The paper proposes a large-scale dataset combining synthetic (using 3D rendering) and realistic (HDR-based simulation) videos, along with a joint training curriculum and domain-aware adapter to balance synthetic precision and realistic generalization.

Result: The authors demonstrate that 'Lumen' produces cinematic quality relighting with consistent lighting and foreground preservation, outperforming existing methods.

Conclusion: The study concludes that 'Lumen' effectively solves video relighting challenges using innovative training methods and datasets, yielding high-quality, consistent, and realistic results.

Abstract: Video relighting is a challenging yet valuable task, aiming to replace the
background in videos while correspondingly adjusting the lighting in the
foreground with harmonious blending. During translation, it is essential to
preserve the original properties of the foreground, e.g., albedo, and propagate
consistent relighting among temporal frames. In this paper, we propose Lumen,
an end-to-end video relighting framework developed on large-scale video
generative models, receiving flexible textual description for instructing the
control of lighting and background. Considering the scarcity of high-qualified
paired videos with the same foreground in various lighting conditions, we
construct a large-scale dataset with a mixture of realistic and synthetic
videos. For the synthetic domain, benefiting from the abundant 3D assets in the
community, we leverage advanced 3D rendering engine to curate video pairs in
diverse environments. For the realistic domain, we adapt a HDR-based lighting
simulation to complement the lack of paired in-the-wild videos. Powered by the
aforementioned dataset, we design a joint training curriculum to effectively
unleash the strengths of each domain, i.e., the physical consistency in
synthetic videos, and the generalized domain distribution in realistic videos.
To implement this, we inject a domain-aware adapter into the model to decouple
the learning of relighting and domain appearance distribution. We construct a
comprehensive benchmark to evaluate Lumen together with existing methods, from
the perspectives of foreground preservation and video consistency assessment.
Experimental results demonstrate that Lumen effectively edit the input into
cinematic relighted videos with consistent lighting and strict foreground
preservation. Our project page: https://lumen-relight.github.io/

</details>


### [281] [MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation](https://arxiv.org/abs/2508.12948)
*Wei Wei,Shaojie Zhang,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: The paper introduces MaskSem, a novel method for skeleton-based human action recognition using semantic-guided masking and hybrid high-order motion targeting, achieving enhanced recognition performance.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised skeleton-based methods are limited in their ability to model complex motion patterns due to focusing on a restricted set of joints and low-order motion dynamics.

Method: The proposed method uses Grad-CAM for semantic-guided masking of critical joints, combined with hybrid high-order motion reconstruction targets involving motion velocity and acceleration.

Result: Experiments on NTU60, NTU120, and PKU-MMD datasets demonstrate improved recognition performance, showcasing MaskSem's effectiveness in human-robot interaction contexts.

Conclusion: MaskSem enhances skeleton-based action recognition by leveraging semantic masking and multi-order motion patterns, enabling more robust learning for intelligent robotic applications.

Abstract: Human action recognition is a crucial task for intelligent robotics,
particularly within the context of human-robot collaboration research. In
self-supervised skeleton-based action recognition, the mask-based
reconstruction paradigm learns the spatial structure and motion patterns of the
skeleton by masking joints and reconstructing the target from unlabeled data.
However, existing methods focus on a limited set of joints and low-order motion
patterns, limiting the model's ability to understand complex motion patterns.
To address this issue, we introduce MaskSem, a novel semantic-guided masking
method for learning 3D hybrid high-order motion representations. This novel
framework leverages Grad-CAM based on relative motion to guide the masking of
joints, which can be represented as the most semantically rich temporal
orgions. The semantic-guided masking process can encourage the model to explore
more discriminative features. Furthermore, we propose using hybrid high-order
motion as the reconstruction target, enabling the model to learn multi-order
motion patterns. Specifically, low-order motion velocity and high-order motion
acceleration are used together as the reconstruction target. This approach
offers a more comprehensive description of the dynamic motion process,
enhancing the model's understanding of motion patterns. Experiments on the
NTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla
transformer, improves skeleton-based action recognition, making it more
suitable for applications in human-robot interaction.

</details>


### [282] [Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination](https://arxiv.org/abs/2508.12957)
*Yizhou Liu,Jingwei Wei,Zizhi Chen,Minghao Han,Xukun Zhang,Keliang Liu,Lihua Zhang*

Main category: cs.CV

TL;DR: The paper introduces ARMed, an RL framework for open-ended medical VQA, addressing challenges in reward discriminability. It improves accuracy and generalization significantly.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing reinforcement fine-tuning approaches, which primarily target closed-ended medical visual question answering, and to improve real-world clinical reasoning through open-ended medical VQA.

Method: The ARMed framework integrates domain knowledge through supervised fine-tuning on chain-of-thought data and applies reinforcement learning with textual correctness and adaptive semantic rewards to refine reasoning.

Result: ARMed demonstrates improvements of 32.64% on in-domain medical VQA tasks and 11.65% on out-of-domain benchmarks, showing enhanced accuracy and generalization.

Conclusion: Reward discriminability plays a critical role in medical RL, and semantically guided rewards hold promise for robust and clinically relevant multimodal reasoning.

Abstract: Reinforcement learning (RL) with rule-based rewards has demonstrated strong
potential in enhancing the reasoning and generalization capabilities of
vision-language models (VLMs) and large language models (LLMs), while reducing
computational overhead. However, its application in medical imaging remains
underexplored. Existing reinforcement fine-tuning (RFT) approaches in this
domain primarily target closed-ended visual question answering (VQA), limiting
their applicability to real-world clinical reasoning. In contrast, open-ended
medical VQA better reflects clinical practice but has received limited
attention. While some efforts have sought to unify both formats via
semantically guided RL, we observe that model-based semantic rewards often
suffer from reward collapse, where responses with significant semantic
differences receive similar scores. To address this, we propose ARMed (Adaptive
Reinforcement for Medical Reasoning), a novel RL framework for open-ended
medical VQA. ARMed first incorporates domain knowledge through supervised
fine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning
with textual correctness and adaptive semantic rewards to enhance reasoning
quality. We evaluate ARMed on six challenging medical VQA benchmarks. Results
show that ARMed consistently boosts both accuracy and generalization, achieving
a 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain
benchmarks. These results highlight the critical role of reward
discriminability in medical RL and the promise of semantically guided rewards
for enabling robust and clinically meaningful multimodal reasoning.

</details>


### [283] [Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a Lightweight Auto3DSeg and SegResNet Implementation](https://arxiv.org/abs/2508.12962)
*Dominic LaBella,Keshav Jha,Jared Robbins,Esther Yu*

Main category: cs.CV

TL;DR: CBCT-based dental segmentation using deep learning achieved significant accuracy for multi-class tasks in dentistry, showcasing promising results in pathology identification and radiation therapy planning.


<details>
  <summary>Details</summary>
Motivation: To improve automation in dental segmentation for assisting pathology diagnosis and enhancing treatment planning in head and neck cancer patients.

Method: A deep learning pipeline using MONAI Auto3DSeg framework, 3D SegResNet architecture, data preprocessing (resampling and intensity clipping), and ensemble fusion through Multi-Label STAPLE.

Result: Achieved an average Dice score of 0.87 on the ToothFairy3 Challenge validation set.

Conclusion: Automated CBCT-based dental segmentation can potentially revolutionize patient care in dentistry and oncology by enabling precise and efficient imaging analyses.

Abstract: Cone-beam computed tomography (CBCT) has become an invaluable imaging
modality in dentistry, enabling 3D visualization of teeth and surrounding
structures for diagnosis and treatment planning. Automated segmentation of
dental structures in CBCT can efficiently assist in identifying pathology
(e.g., pulpal or periapical lesions) and facilitate radiation therapy planning
in head and neck cancer patients. We describe the DLaBella29 team's approach
for the MICCAI 2025 ToothFairy3 Challenge, which involves a deep learning
pipeline for multi-class tooth segmentation. We utilized the MONAI Auto3DSeg
framework with a 3D SegResNet architecture, trained on a subset of the
ToothFairy3 dataset (63 CBCT scans) with 5-fold cross-validation. Key
preprocessing steps included image resampling to 0.6 mm isotropic resolution
and intensity clipping. We applied an ensemble fusion using Multi-Label STAPLE
on the 5-fold predictions to infer a Phase 1 segmentation and then conducted
tight cropping around the easily segmented Phase 1 mandible to perform Phase 2
segmentation on the smaller nerve structures. Our method achieved an average
Dice of 0.87 on the ToothFairy3 challenge out-of-sample validation set. This
paper details the clinical context, data preparation, model development,
results of our approach, and discusses the relevance of automated dental
segmentation for improving patient care in radiation oncology.

</details>


### [284] [GazeDETR: Gaze Detection using Disentangled Head and Gaze Representations](https://arxiv.org/abs/2508.12966)
*Ryan Anthony Jalova de Belen,Gelareh Mohammadi,Arcot Sowmya*

Main category: cs.CV

TL;DR: The paper introduces GazeDETR, a novel architecture with disentangled decoders for gaze prediction and head localization, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current models for gaze target detection are limited by using a single multitasking decoder, leading to entangled representations for head localization and gaze prediction.

Method: GazeDETR employs an end-to-end architecture with two separate decoders, allowing distinct representations for each subtask using attentive fields.

Result: GazeDETR outperforms existing models by a significant margin across multiple datasets including GazeFollow, VideoAttentionTarget, and ChildPlay.

Conclusion: Disentangling the decoders in gaze prediction tasks enhances performance, providing distinct benefits for handling local and global information separately.

Abstract: Gaze communication plays a crucial role in daily social interactions.
Quantifying this behavior can help in human-computer interaction and digital
phenotyping. While end-to-end models exist for gaze target detection, they only
utilize a single decoder to simultaneously localize human heads and predict
their corresponding gaze (e.g., 2D points or heatmap) in a scene. This
multitask learning approach generates a unified and entangled representation
for human head localization and gaze location prediction. Herein, we propose
GazeDETR, a novel end-to-end architecture with two disentangled decoders that
individually learn unique representations and effectively utilize coherent
attentive fields for each subtask. More specifically, we demonstrate that its
human head predictor utilizes local information, while its gaze decoder
incorporates both local and global information. Our proposed architecture
achieves state-of-the-art results on the GazeFollow, VideoAttentionTarget and
ChildPlay datasets. It outperforms existing end-to-end models with a notable
margin.

</details>


### [285] [Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation](https://arxiv.org/abs/2508.12969)
*Qirui Li,Guangcong Zheng,Qi Zhao,Jie Li,Bin Dong,Yiwu Yao,Xi Li*

Main category: cs.CV

TL;DR: This paper introduces a new framework called Compact Attention aimed at accelerating the attention mechanism in transformer-based video generation models, while preserving visual quality.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the inefficiency of current sparse attention methods in synthesizing ultra-long video sequences by leveraging the structured and heterogeneous sparsity patterns in video data.

Method: The authors propose a hardware-aware acceleration framework that includes adaptive tiling strategies, temporally varying windows, and an automated configuration search to optimize sparsity patterns in attention matrices.

Result: Compact Attention achieves a 1.6 to 2.5 times speedup on single-GPU setups compared to full-attention baselines, while maintaining comparable visual quality.

Conclusion: The paper demonstrates how exploiting structured sparsity can enable more efficient long-form video generation without sacrificing performance, offering a principled solution for hardware-aligned acceleration.

Abstract: The computational demands of self-attention mechanisms pose a critical
challenge for transformer-based video generation, particularly in synthesizing
ultra-long sequences. Current approaches, such as factorized attention and
fixed sparse patterns, fail to fully exploit the inherent spatio-temporal
redundancies in video data. Through systematic analysis of video diffusion
transformers (DiT), we uncover a key insight: Attention matrices exhibit
structured, yet heterogeneous sparsity patterns, where specialized heads
dynamically attend to distinct spatiotemporal regions (e.g., local pattern,
cross-shaped pattern, or global pattern). Existing sparse attention methods
either impose rigid constraints or introduce significant overhead, limiting
their effectiveness. To address this, we propose Compact Attention, a
hardware-aware acceleration framework featuring three innovations: 1) Adaptive
tiling strategies that approximate diverse spatial interaction patterns via
dynamic tile grouping, 2) Temporally varying windows that adjust sparsity
levels based on frame proximity, and 3) An automated configuration search
algorithm that optimizes sparse patterns while preserving critical attention
pathways. Our method achieves 1.6~2.5x acceleration in attention computation on
single-GPU setups while maintaining comparable visual quality with
full-attention baselines. This work provides a principled approach to unlocking
efficient long-form video generation through structured sparsity exploitation.
Project Page: https://yo-ava.github.io/Compact-Attention.github.io/

</details>


### [286] [Dextr: Zero-Shot Neural Architecture Search with Singular Value Decomposition and Extrinsic Curvature](https://arxiv.org/abs/2508.12977)
*Rohan Asthana,Joschua Conrad,Maurits Ortmanns,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: This paper introduces a zero-shot NAS method using a zero-cost proxy that doesn't rely on labeled data, leveraging SVD and extrinsic curvature to predict network performance efficiently.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of zero-cost proxies in NAS that depend heavily on labeled data and focus solely on either convergence/generalization or network expressivity.

Method: The method employs Singular Value Decomposition (SVD) for neural network features and extrinsic curvature for network output, combining them into a simplified harmonic mean formula for architecture evaluation without labeled data.

Result: The proxy accurately predicts performance on test data with just one label-free data sample, showing superiority across benchmarks like NAS-Bench-101, NAS-Bench-201, TransNAS-Bench-101-micro, DARTS, and AutoFormer.

Conclusion: The proposed approach is efficient and effective in zero-shot NAS applications, solving the dependence on labeled data and integrating multiple key properties for better architectural evaluations.

Abstract: Zero-shot Neural Architecture Search (NAS) typically optimises the
architecture search process by exploiting the network or gradient properties at
initialisation through zero-cost proxies. The existing proxies often rely on
labelled data, which is usually unavailable in real-world settings.
Furthermore, the majority of the current methods focus either on optimising the
convergence and generalisation attributes or solely on the expressivity of the
network architectures. To address both limitations, we first demonstrate how
channel collinearity affects the convergence and generalisation properties of a
neural network. Then, by incorporating the convergence, generalisation and
expressivity in one approach, we propose a zero-cost proxy that omits the
requirement of labelled data for its computation. In particular, we leverage
the Singular Value Decomposition (SVD) of the neural network layer features and
the extrinsic curvature of the network output to design our proxy. %As a
result, the proposed proxy is formulated as the simplified harmonic mean of the
logarithms of two key components: the sum of the inverse of the feature
condition number and the extrinsic curvature of the network output. Our
approach enables accurate prediction of network performance on test data using
only a single label-free data sample. Our extensive evaluation includes a total
of six experiments, including the Convolutional Neural Network (CNN) search
space, i.e. DARTS and the Transformer search space, i.e. AutoFormer. The
proposed proxy demonstrates a superior performance on multiple correlation
benchmarks, including NAS-Bench-101, NAS-Bench-201, and
TransNAS-Bench-101-micro; as well as on the NAS task within the DARTS and the
AutoFormer search space, all while being notably efficient. The code is
available at https://github.com/rohanasthana/Dextr.

</details>


### [287] [Omni Survey for Multimodality Analysis in Visual Object Tracking](https://arxiv.org/abs/2508.13000)
*Zhangyong Tang,Tianyang Xu,Xuefeng Zhu,Hui Li,Shaochuan Zhao,Tao Zhou,Chunyang Cheng,Xiaojun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: This paper surveys the critical task of multi-modal visual object tracking (MMVOT), discussing data modalities, challenges, methods, and evaluation. It categorizes methods based on RGB and X modalities, analyzes datasets, and evaluates multi-modal tracking advantages.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to manage multi-modal data in smart cities for robust monitoring that addresses the challenges posed by diverse modalities and their integration in visual object tracking.

Method: The authors survey MMVOT tasks, categorizing existing methods based on RGB and additional modalities such as infrared, depth, event, and others. They analyze datasets and propose systematic evaluations and benchmarks.

Result: The paper provides an extensive categorization of MMVOT methods, highlights long-tail distributions in dataset object categories, and identifies gaps such as the lack of animal categories compared to RGB datasets.

Conclusion: The authors conclude that multi-modal tracking can provide superior solutions in certain scenarios but not universally, emphasizing the importance of understanding conditions for its effective application.

Abstract: The development of smart cities has led to the generation of massive amounts
of multi-modal data in the context of a range of tasks that enable a
comprehensive monitoring of the smart city infrastructure and services. This
paper surveys one of the most critical tasks, multi-modal visual object
tracking (MMVOT), from the perspective of multimodality analysis. Generally,
MMVOT differs from single-modal tracking in four key aspects, data collection,
modality alignment and annotation, model designing, and evaluation.
Accordingly, we begin with an introduction to the relevant data modalities,
laying the groundwork for their integration. This naturally leads to a
discussion of challenges of multi-modal data collection, alignment, and
annotation. Subsequently, existing MMVOT methods are categorised, based on
different ways to deal with visible (RGB) and X modalities: programming the
auxiliary X branch with replicated or non-replicated experimental
configurations from the RGB branch. Here X can be thermal infrared (T), depth
(D), event (E), near infrared (NIR), language (L), or sonar (S). The final part
of the paper addresses evaluation and benchmarking. In summary, we undertake an
omni survey of all aspects of multi-modal visual object tracking (VOT),
covering six MMVOT tasks and featuring 338 references in total. In addition, we
discuss the fundamental rhetorical question: Is multi-modal tracking always
guaranteed to provide a superior solution to unimodal tracking with the help of
information fusion, and if not, in what circumstances its application is
beneficial. Furthermore, for the first time in this field, we analyse the
distributions of the object categories in the existing MMVOT datasets,
revealing their pronounced long-tail nature and a noticeable lack of animal
categories when compared with RGB datasets.

</details>


### [288] [Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition and Continual Learning](https://arxiv.org/abs/2508.13005)
*Jiawen Xu,Odej Kao*

Main category: cs.CV

TL;DR: The paper examines how enhancing feature diversity can improve open set recognition and continual learning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in machine learning related to open set recognition and continual learning, focusing on the unexplored role of feature diversity in improving these tasks.

Method: The authors provide empirical evidence demonstrating the benefits of increasing feature diversity for detecting novel classes and effectively updating models.

Result: The study shows that greater feature diversity enhances open set sample recognition and aids in retaining and integrating data for continual learning.

Conclusion: The findings suggest that promoting feature diversity could lead to better practical methods and theoretical understanding in the fields of OSR and continual learning.

Abstract: Open set recognition (OSR) and continual learning are two critical challenges
in machine learning, focusing respectively on detecting novel classes at
inference time and updating models to incorporate the new classes. While many
recent approaches have addressed these problems, particularly OSR, by
heuristically promoting feature diversity, few studies have directly examined
the role that feature diversity plays in tackling them. In this work, we
provide empirical evidence that enhancing feature diversity improves the
recognition of open set samples. Moreover, increased feature diversity also
facilitates both the retention of previously learned data and the integration
of new data in continual learning. We hope our findings can inspire further
research into both practical methods and theoretical understanding in these
domains.

</details>


### [289] [SlimComm: Doppler-Guided Sparse Queries for Bandwidth-Efficient Cooperative 3-D Perception](https://arxiv.org/abs/2508.13007)
*Melih Yazgan,Qiyuan Wu,Iramm Hamdard,Shiqi Li,J. Marius Zoellner*

Main category: cs.CV

TL;DR: SlimComm is a framework that optimizes communication efficiency for autonomous vehicles by exchanging sparse query-specific features instead of dense feature maps for collaborative perception.


<details>
  <summary>Details</summary>
Motivation: To address the bandwidth limitations in inter-vehicle communication for connected autonomous vehicles when sharing sensor data for collaborative perception.

Method: SlimComm uses 4D radar Doppler data and a query-driven sparse approach. It distinguishes moving/static objects and exchanges only query-specific BEV features, employing multi-scale gated deformable attention to ensure accuracy.

Result: SlimComm reduces bandwidth usage by up to 90% compared to full-map sharing while maintaining or surpassing accuracy across different traffic complexities.

Conclusion: SlimComm provides a communication-efficient solution for collaborative perception, balancing reduced data transmission with high accuracy, making it viable for autonomous vehicle networks.

Abstract: Collaborative perception allows connected autonomous vehicles (CAVs) to
overcome occlusion and limited sensor range by sharing intermediate features.
Yet transmitting dense Bird's-Eye-View (BEV) feature maps can overwhelm the
bandwidth available for inter-vehicle communication. We present SlimComm, a
communication-efficient framework that integrates 4D radar Doppler with a
query-driven sparse scheme. SlimComm builds a motion-centric dynamic map to
distinguish moving from static objects and generates two query types: (i)
reference queries on dynamic and high-confidence regions, and (ii) exploratory
queries probing occluded areas via a two-stage offset. Only query-specific BEV
features are exchanged and fused through multi-scale gated deformable
attention, reducing payload while preserving accuracy. For evaluation, we
release OPV2V-R and Adver-City-R, CARLA-based datasets with per-point Doppler
radar. SlimComm achieves up to 90% lower bandwidth than full-map sharing while
matching or surpassing prior baselines across varied traffic densities and
occlusions. Dataset and code will be available at: https://url.fzi.de/SlimComm.

</details>


### [290] [Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model](https://arxiv.org/abs/2508.13009)
*Xianglong He,Chunli Peng,Zexiang Liu,Boyang Wang,Yifan Zhang,Qi Cui,Fei Kang,Biao Jiang,Mengyin An,Yangyang Ren,Baixin Xu,Hao-Xiang Guo,Kaixiong Gong,Cyrus Wu,Wei Li,Xuchen Song,Yang Liu,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-Game 2.0 introduces a framework for real-time interactive video generation with few-step auto-regressive diffusion, achieving high-quality results at 25 FPS.


<details>
  <summary>Details</summary>
Motivation: Existing interactive world models are constrained by bidirectional attention and lengthy inference steps, making them unsuitable for real-time simulations of dynamics.

Method: The framework includes scalable data production for Unreal Engine and GTA5 environments, action injection through mouse and keyboard inputs, and few-step distillation for streaming video generation.

Result: Matrix-Game 2.0 achieves ultra-fast interactive video generation at 25 FPS, with high-quality videos spanning diverse scenes and annotations.

Conclusion: The open-sourced Matrix-Game 2.0 pushes the limits of interactive world models for video generation, offering improvements in efficiency and quality.

Abstract: Recent advances in interactive video generations have demonstrated diffusion
model's potential as world models by capturing complex physical dynamics and
interactive behaviors. However, existing interactive world models depend on
bidirectional attention and lengthy inference steps, severely limiting
real-time performance. Consequently, they are hard to simulate real-world
dynamics, where outcomes must update instantaneously based on historical
context and current actions. To address this, we present Matrix-Game 2.0, an
interactive world model generates long videos on-the-fly via few-step
auto-regressive diffusion. Our framework consists of three key components: (1)
A scalable data production pipeline for Unreal Engine and GTA5 environments to
effectively produce massive amounts (about 1200 hours) of video data with
diverse interaction annotations; (2) An action injection module that enables
frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step
distillation based on the casual architecture for real-time and streaming video
generation. Matrix Game 2.0 can generate high-quality minute-level videos
across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our
model weights and codebase to advance research in interactive world modeling.

</details>


### [291] [EgoTwin: Dreaming Body and View in First Person](https://arxiv.org/abs/2508.13013)
*Jingqiao Xiu,Fangzhou Hong,Yicong Li,Mengze Li,Wentao Wang,Sirui Han,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: The paper explores egocentric video and human motion generation and proposes EgoTwin, a framework that ensures motion consistency and causal interplay using a diffusion transformer architecture.


<details>
  <summary>Details</summary>
Motivation: Current advances in video synthesis focus primarily on exocentric views, leaving egocentric video generation, which involves first-person viewpoint content and camera movement, underexplored.

Method: EgoTwin employs a head-centric motion representation and a cybernetics-inspired mechanism within a diffusion transformer architecture to capture causal alignment between human motion and video.

Result: EgoTwin's performance was validated against a newly curated dataset with novel metrics, showing effective video-motion consistency in the generated output.

Conclusion: EgoTwin successfully addresses the challenges of viewpoint alignment and causal interplay, enhancing egocentric video and human motion generation quality and expanding application possibilities.

Abstract: While exocentric video synthesis has achieved great progress, egocentric
video generation remains largely underexplored, which requires modeling
first-person view content along with camera motion patterns induced by the
wearer's body movements. To bridge this gap, we introduce a novel task of joint
egocentric video and human motion generation, characterized by two key
challenges: 1) Viewpoint Alignment: the camera trajectory in the generated
video must accurately align with the head trajectory derived from human motion;
2) Causal Interplay: the synthesized human motion must causally align with the
observed visual dynamics across adjacent video frames. To address these
challenges, we propose EgoTwin, a joint video-motion generation framework built
on the diffusion transformer architecture. Specifically, EgoTwin introduces a
head-centric motion representation that anchors the human motion to the head
joint and incorporates a cybernetics-inspired interaction mechanism that
explicitly captures the causal interplay between video and motion within
attention operations. For comprehensive evaluation, we curate a large-scale
real-world dataset of synchronized text-video-motion triplets and design novel
metrics to assess video-motion consistency. Extensive experiments demonstrate
the effectiveness of the EgoTwin framework.

</details>


### [292] [HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical Feature Adapters](https://arxiv.org/abs/2508.13026)
*Ruru Xu,Ilkay Oksuz*

Main category: cs.CV

TL;DR: This paper introduces HierAdaptMR, a framework for deep learning-based cardiac MRI reconstruction that enhances cross-center generalization with adaptable and parameter-efficient modules.


<details>
  <summary>Details</summary>
Motivation: To address significant domain shifts in deploying cardiac MRI reconstruction across diverse clinical centers with various protocols and scanner configurations.

Method: The proposed HierAdaptMR framework uses hierarchical feature adaptation, incorporating Protocol-Level Adapters for sequence-specific adaptations, Center-Level Adapters for scanner variation, and a Universal Adapter for unseen centers. The model also leverages variational unrolling, multi-scale SSIM loss, frequency-domain enhancement, and contrast-adaptive weighting.

Result: HierAdaptMR demonstrated superior cross-center generalization and consistent image reconstruction quality in evaluations on the CMRxRecon2025 dataset, which included data from over 5 centers, 10 scanners, and 9 modalities.

Conclusion: The hierarchical adaptation strategy in HierAdaptMR effectively handles domain shifts, achieving robust generalization for cardiac MRI reconstruction across heterogeneous clinical settings.

Abstract: Deep learning-based cardiac MRI reconstruction faces significant domain shift
challenges when deployed across multiple clinical centers with heterogeneous
scanner configurations and imaging protocols. We propose HierAdaptMR, a
hierarchical feature adaptation framework that addresses multi-level domain
variations through parameter-efficient adapters. Our method employs
Protocol-Level Adapters for sequence-specific characteristics and Center-Level
Adapters for scanner-dependent variations, built upon a variational unrolling
backbone. A Universal Adapter enables generalization to entirely unseen centers
through stochastic training that learns center-invariant adaptations. The
framework utilizes multi-scale SSIM loss with frequency domain enhancement and
contrast-adaptive weighting for robust optimization. Comprehensive evaluation
on the CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9
modalities demonstrates superior cross-center generalization while maintaining
reconstruction quality. code: https://github.com/Ruru-Xu/HierAdaptMR

</details>


### [293] [IntelliCap: Intelligent Guidance for Consistent View Sampling](https://arxiv.org/abs/2508.13043)
*Ayaka Yasunaga,Hideo Saito,Dieter Schmalstieg,Shohei Mori*

Main category: cs.CV

TL;DR: The paper introduces a system to assist humans in capturing high-quality images for novel view synthesis, utilizing semantic segmentation and vision-language models to guide coverage of key objects.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of inadequate human-driven image capture for high-fidelity novel view synthesis. Existing view sampling has gaps in uniformity and comprehensiveness, especially for complex, view-dependent scenes.

Method: The paper proposes a novel system that uses semantic segmentation and vision-language models to identify and rank important objects in a scene. Spherical proxies are then used to guide image collection during scanning.

Result: The approach outperforms conventional view sampling strategies in real-world scenarios by ensuring better representation of appearance and structure.

Conclusion: The system improves the image acquisition process for rendering algorithms, ensuring higher-quality synthesized views with a user-friendly guide for capturing necessary views.

Abstract: Novel view synthesis from images, for example, with 3D Gaussian splatting,
has made great progress. Rendering fidelity and speed are now ready even for
demanding virtual reality applications. However, the problem of assisting
humans in collecting the input images for these rendering algorithms has
received much less attention. High-quality view synthesis requires uniform and
dense view sampling. Unfortunately, these requirements are not easily addressed
by human camera operators, who are in a hurry, impatient, or lack understanding
of the scene structure and the photographic process. Existing approaches to
guide humans during image acquisition concentrate on single objects or neglect
view-dependent material characteristics. We propose a novel situated
visualization technique for scanning at multiple scales. During the scanning of
a scene, our method identifies important objects that need extended image
coverage to properly represent view-dependent appearance. To this end, we
leverage semantic segmentation and category identification, ranked by a
vision-language model. Spherical proxies are generated around highly ranked
objects to guide the user during scanning. Our results show superior
performance in real scenes compared to conventional view sampling strategies.

</details>


### [294] [Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping](https://arxiv.org/abs/2508.13065)
*Siddharth Khandelwal,Sridhar Kamath,Arjun Jain*

Main category: cs.CV

TL;DR: This paper introduces a new dataset for human shape editing and proposes a diffusion-based approach to achieve realistic body reshaping with superior performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of advanced systems and datasets for human shape editing, in contrast to the progress made in human pose editing. Current methods face challenges like unrealistic proportions, texture distortions, and background inconsistencies.

Method: The research proposes 'Odo,' a diffusion-based method combining a frozen UNet for fine-grained detail preservation with a ControlNet using SMPL depth maps to guide human body shape transformations.

Result: The approach achieves state-of-the-art results, with per-vertex reconstruction errors reduced to 7.5mm compared to 13.6mm in baseline methods. It also produces realistic reshaping outcomes aligning well with target shapes.

Conclusion: The proposed dataset and method mark significant progress in realistic and controlled human shape editing, offering a new benchmark and tool for the field.

Abstract: Human shape editing enables controllable transformation of a person's body
shape, such as thin, muscular, or overweight, while preserving pose, identity,
clothing, and background. Unlike human pose editing, which has advanced
rapidly, shape editing remains relatively underexplored. Current approaches
typically rely on 3D morphable models or image warping, often introducing
unrealistic body proportions, texture distortions, and background
inconsistencies due to alignment errors and deformations. A key limitation is
the lack of large-scale, publicly available datasets for training and
evaluating body shape manipulation methods. In this work, we introduce the
first large-scale dataset of 18,573 images across 1523 subjects, specifically
designed for controlled human shape editing. It features diverse variations in
body shape, including fat, muscular and thin, captured under consistent
identity, clothing, and background conditions. Using this dataset, we propose
Odo, an end-to-end diffusion-based method that enables realistic and intuitive
body reshaping guided by simple semantic attributes. Our approach combines a
frozen UNet that preserves fine-grained appearance and background details from
the input image with a ControlNet that guides shape transformation using target
SMPL depth maps. Extensive experiments demonstrate that our method outperforms
prior approaches, achieving per-vertex reconstruction errors as low as 7.5mm,
significantly lower than the 13.6mm observed in baseline methods, while
producing realistic results that accurately match the desired target shapes.

</details>


### [295] [Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation](https://arxiv.org/abs/2508.13068)
*Tanjim Islam Riju,Shuchismita Anwar,Saman Sarker Joy,Farig Sadeque,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: The paper introduces a two-stage framework for chest X-ray analysis, combining gaze-guided disease classification and a modular approach for radiology report generation, leveraging gaze data for enhanced performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: The work addresses the need to improve both disease classification accuracy and the interpretability of radiology report generation from chest X-rays.

Method: The framework involves two stages: 1) a gaze-guided contrastive learning architecture utilizing visual features, clinical labels, bounding boxes, and radiologist eye-tracking signals; and 2) a report generation pipeline that uses confidence-weighted diagnostic keywords mapped to anatomical regions with a curated dictionary and structured prompts.

Result: The method boosted F1 score by 5.70% and AUC by 3.41% for classification and improved report quality based on clinical keyword recall and ROUGE overlap.

Conclusion: Incorporating radiologist gaze data enhances disease classification metrics and improves the interpretability and specificity of automatically generated medical reports.

Abstract: We propose a two-stage multimodal framework that enhances disease
classification and region-aware radiology report generation from chest X-rays,
leveraging the MIMIC-Eye dataset. In the first stage, we introduce a
gaze-guided contrastive learning architecture for disease classification. It
integrates visual features, clinical labels, bounding boxes, and radiologist
eye-tracking signals and is equipped with a novel multi-term gaze-attention
loss combining MSE, KL divergence, correlation, and center-of-mass alignment.
Incorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUC
from 0.821 to 0.849 (+3.41%), while also improving precision and recall,
highlighting the effectiveness of gaze-informed attention supervision. In the
second stage, we present a modular report generation pipeline that extracts
confidence-weighted diagnostic keywords, maps them to anatomical regions using
a curated dictionary constructed from domain-specific priors, and generates
region-aligned sentences via structured prompts. This pipeline improves report
quality as measured by clinical keyword recall and ROUGE overlap. Our results
demonstrate that integrating gaze data improves both classification performance
and the interpretability of generated medical reports.

</details>


### [296] [ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset](https://arxiv.org/abs/2508.13078)
*Qingwen Zeng,Juan E. Tapia,Izan Garcia,Juan M. Espin,Christoph Busch*

Main category: cs.CV

TL;DR: This paper addresses challenges in developing Presentation Attack Detection (PAD) systems for ID cards, proposing the generation of synthetic bona fide images using Stable Diffusion to improve detection and overcome data limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the issue of insufficient bona fide images and increasing diversity in attack types, which hinder the robust development of PAD systems.

Method: The authors used Stable Diffusion to create synthetic versions of bona fide images and assessed their impact on PAD systems trained from scratch and commercial solutions.

Result: The PAD system successfully identified the generated synthetic images as bona fide. This improved detection performance and alleviated data limitations.

Conclusion: This innovative approach enhances the generalization capabilities of PAD systems by addressing data restrictions and opens up new possibilities in synthetic image generation for security applications.

Abstract: Nowadays, the development of a Presentation Attack Detection (PAD) system for
ID cards presents a challenge due to the lack of images available to train a
robust PAD system and the increase in diversity of possible attack instrument
species. Today, most algorithms focus on generating attack samples and do not
take into account the limited number of bona fide images. This work is one of
the first to propose a method for mimicking bona fide images by generating
synthetic versions of them using Stable Diffusion, which may help improve the
generalisation capabilities of the detector. Furthermore, the new images
generated are evaluated in a system trained from scratch and in a commercial
solution. The PAD system yields an interesting result, as it identifies our
images as bona fide, which has a positive impact on detection performance and
data restrictions.

</details>


### [297] [Checkmate: interpretable and explainable RSVQA is the endgame](https://arxiv.org/abs/2508.13086)
*Lucrezia Tosato,Christel Tartini Chappuis,Syrielle Montariol,Flora Weissgerber,Sylvain Lobry,Devis Tuia*

Main category: cs.CV

TL;DR: The paper introduces an improved approach for Remote Sensing Visual Question Answering (RSVQA) by presenting a new dataset, Chessboard, and a model, Checkmate, to enhance interpretability and reduce biases.


<details>
  <summary>Details</summary>
Motivation: RSVQA models often lack interpretability and suffer from shortcut learning due to biases in dataset distributions.

Method: Developed the Chessboard dataset with 3,123,253 questions and balanced answers, and proposed the Checkmate model to enhance visual reasoning and identify relevant image cells.

Result: Extensive experiments demonstrate that the Checkmate model is more transparent and trustworthy in decision-making within RSVQA systems.

Conclusion: The work addresses shortcut learning and interpretability issues in RSVQA, offering a stronger foundation for visual reasoning and more reliable decision-making.

Abstract: Remote Sensing Visual Question Answering (RSVQA) presents unique challenges
in ensuring that model decisions are both understandable and grounded in visual
content. Current models often suffer from a lack of interpretability and
explainability, as well as from biases in dataset distributions that lead to
shortcut learning. In this work, we tackle these issues by introducing a novel
RSVQA dataset, Chessboard, designed to minimize biases through 3'123'253
questions and a balanced answer distribution. Each answer is linked to one or
more cells within the image, enabling fine-grained visual reasoning.
  Building on this dataset, we develop an explainable and interpretable model
called Checkmate that identifies the image cells most relevant to its
decisions. Through extensive experiments across multiple model architectures,
we show that our approach improves transparency and supports more trustworthy
decision-making in RSVQA systems.

</details>


### [298] [DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation](https://arxiv.org/abs/2508.13091)
*Zihua Liu,Yizhou Li,Songyan Zhang,Masatoshi Okutomi*

Main category: cs.CV

TL;DR: Self-supervised stereo matching and monocular depth estimation are enhanced using a novel approach, DMS, which utilizes geometric priors from diffusion models to synthesize novel views for better photometric reconstruction.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of photometric reconstruction ambiguity, stemming from occlusions and ill-posed regions in self-supervised stereo methods.

Method: Proposed DMS uses diffusion models, fine-tuned with Stable Diffusion, to generate explicit geometric-aligned views along the epipolar direction, improving pixel correspondence.

Result: DMS resulted in up to 35% reduction in outliers and achieved state-of-the-art performance across multiple datasets.

Conclusion: DMS enhances unsupervised stereo matching and monocular depth estimation by providing better correspondence alignment, without requiring additional labeled data for training.

Abstract: While supervised stereo matching and monocular depth estimation have advanced
significantly with learning-based algorithms, self-supervised methods using
stereo images as supervision signals have received relatively less focus and
require further investigation. A primary challenge arises from ambiguity
introduced during photometric reconstruction, particularly due to missing
corresponding pixels in ill-posed regions of the target view, such as
occlusions and out-of-frame areas. To address this and establish explicit
photometric correspondences, we propose DMS, a model-agnostic approach that
utilizes geometric priors from diffusion models to synthesize novel views along
the epipolar direction, guided by directional prompts. Specifically, we
finetune a Stable Diffusion model to simulate perspectives at key positions:
left-left view shifted from the left camera, right-right view shifted from the
right camera, along with an additional novel view between the left and right
cameras. These synthesized views supplement occluded pixels, enabling explicit
photometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play''
method that seamlessly enhances self-supervised stereo matching and monocular
depth estimation, and relies solely on unlabeled stereo image pairs for both
training and synthesizing. Extensive experiments demonstrate the effectiveness
of our approach, with up to 35% outlier reduction and state-of-the-art
performance across multiple benchmark datasets.

</details>


### [299] [Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants](https://arxiv.org/abs/2508.13101)
*Miftahul Huda,Arsyiah Azahra,Putri Maulida Chairani,Dimas Rizky Ramadhani,Nabila Azhari,Ade Lailani*

Main category: cs.CV

TL;DR: This research evaluates the RT-DETR object detection model for the automated identification and counting of beach litter. While RT-DETR-X achieves slightly better accuracy, RT-DETR-L is more practical for real-time use due to its faster processing speed.


<details>
  <summary>Details</summary>
Motivation: The study aims to address coastal pollution by developing scalable and automated solutions for monitoring beach litter through advanced real-time object detection techniques.

Method: A comparative analysis between RT-DETR-Large (RT-DETR-L) and RT-DETR-Extra-Large (RT-DETR-X) models was conducted using a coastal debris dataset. Key metrics like detection precision and inference speed were evaluated.

Result: RT-DETR-X outperforms RT-DETR-L in accuracy (mAP@50: 0.816 vs. 0.810, mAP@50-95: 0.612 vs. 0.606), but RT-DETR-L offers much faster inference times (20.1 ms vs. 34.5 ms).

Conclusion: RT-DETR-L is more suitable for real-time, in-field deployments due to its optimal trade-off between detection accuracy and computational efficiency, providing actionable insights for environmental conservation practices.

Abstract: Coastal pollution is a pressing global environmental issue, necessitating
scalable and automated solutions for monitoring and management. This study
investigates the efficacy of the Real-Time Detection Transformer (RT-DETR), a
state-of-the-art, end-to-end object detection model, for the automated
detection and counting of beach litter. A rigorous comparative analysis is
conducted between two model variants, RT-DETR-Large (RT-DETR-L) and
RT-DETR-Extra-Large (RT-DETR-X), trained on a publicly available dataset of
coastal debris. The evaluation reveals that the RT-DETR-X model achieves
marginally superior accuracy, with a mean Average Precision at 50\% IoU
(mAP@50) of 0.816 and a mAP@50-95 of 0.612, compared to the RT-DETR-L model's
0.810 and 0.606, respectively. However, this minor performance gain is realized
at a significant computational cost; the RT-DETR-L model demonstrates a
substantially faster inference time of 20.1 ms versus 34.5 ms for the
RT-DETR-X. The findings suggest that the RT-DETR-L model offers a more
practical and efficient solution for real-time, in-field deployment due to its
superior balance of processing speed and detection accuracy. This research
provides valuable insights into the application of advanced Transformer-based
detectors for environmental conservation, highlighting the critical trade-offs
between model complexity and operational viability.

</details>


### [300] [Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence](https://arxiv.org/abs/2508.13139)
*Ling-Hao Chen,Yuhong Zhang,Zixin Yin,Zhiyang Dou,Xin Chen,Jingbo Wang,Taku Komura,Lei Zhang*

Main category: cs.CV

TL;DR: Motion2Motion is a training-free framework designed to transfer animations between skeletal structures with different topologies, addressing the challenge of inconsistent skeletal structures and lack of large-scale paired datasets.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of transferring animations across diverse skeletal topologies and compensate for the lack of large-scale paired motion datasets.

Method: Motion2Motion uses a sparse set of bone correspondences and one or a few example motions on the target skeleton, without requiring large-scale datasets or extensive training.

Result: Motion2Motion demonstrates efficient and reliable animation transfer in scenarios involving similar-skeleton and cross-species skeleton transfers, validated through qualitative and quantitative evaluations.

Conclusion: Motion2Motion is an effective and scalable solution for animation transfer between diverse skeletal structures, proving practical utility in both research and industrial applications.

Abstract: This work studies the challenge of transfer animations between characters
whose skeletal topologies differ substantially. While many techniques have
advanced retargeting techniques in decades, transfer motions across diverse
topologies remains less-explored. The primary obstacle lies in the inherent
topological inconsistency between source and target skeletons, which restricts
the establishment of straightforward one-to-one bone correspondences. Besides,
the current lack of large-scale paired motion datasets spanning different
topological structures severely constrains the development of data-driven
approaches. To address these limitations, we introduce Motion2Motion, a novel,
training-free framework. Simply yet effectively, Motion2Motion works with only
one or a few example motions on the target skeleton, by accessing a sparse set
of bone correspondences between the source and target skeletons. Through
comprehensive qualitative and quantitative evaluations, we demonstrate that
Motion2Motion achieves efficient and reliable performance in both
similar-skeleton and cross-species skeleton transfer scenarios. The practical
utility of our approach is further evidenced by its successful integration in
downstream applications and user interfaces, highlighting its potential for
industrial applications. Code and data are available at
https://lhchen.top/Motion2Motion.

</details>


### [301] [IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion](https://arxiv.org/abs/2508.13153)
*Wenhao Hu,Zesheng Li,Haonan Zhou,Liu Liu,Xuexiang Wen,Zhizhong Su,Xi Li,Gaoang Wang*

Main category: cs.CV

TL;DR: IGFuse proposes a novel framework for overcoming occlusions and limitations in 3D scene reconstruction by fusing multiple scans with natural object rearrangement.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene reconstruction methods struggle with occlusions, limited scans, complex pipelines, and scalability issues.

Method: IGFuse leverages segmentation-aware Gaussian fields, enforces photometric/semantic consistency, incorporates intermediate scene state for alignment, and refines geometry via collaborative co-pruning.

Result: The framework demonstrates strong generalization to novel scenes and enables high fidelity rendering and object manipulation without dense observations.

Conclusion: IGFuse proves effective for real-world 3D reconstruction and enhances real-to-simulation transfer, outperforming existing methods in scalability and detail capture.

Abstract: Reconstructing complete and interactive 3D scenes remains a fundamental
challenge in computer vision and robotics, particularly due to persistent
object occlusions and limited sensor coverage. Multiview observations from a
single scene scan often fail to capture the full structural details. Existing
approaches typically rely on multi stage pipelines, such as segmentation,
background completion, and inpainting or require per-object dense scanning,
both of which are error-prone, and not easily scalable. We propose IGFuse, a
novel framework that reconstructs interactive Gaussian scene by fusing
observations from multiple scans, where natural object rearrangement between
captures reveal previously occluded regions. Our method constructs segmentation
aware Gaussian fields and enforces bi-directional photometric and semantic
consistency across scans. To handle spatial misalignments, we introduce a
pseudo-intermediate scene state for unified alignment, alongside collaborative
co-pruning strategies to refine geometry. IGFuse enables high fidelity
rendering and object level scene manipulation without dense observations or
complex pipelines. Extensive experiments validate the framework's strong
generalization to novel scene configurations, demonstrating its effectiveness
for real world 3D reconstruction and real-to-simulation transfer. Our project
page is available online.

</details>


### [302] [4DNeX: Feed-Forward 4D Generative Modeling Made Easy](https://arxiv.org/abs/2508.13154)
*Zhaoxi Chen,Tianqi Liu,Long Zhuo,Jiawei Ren,Zeng Tao,He Zhu,Fangzhou Hong,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: The paper introduces 4DNeX, a framework that generates dynamic 3D scenes from single images using a fine-tuned video diffusion model, bypassing traditional methods reliant on multi-frame or optimization-heavy processes.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of computational inefficiency and multi-frame data dependency in current methodologies for generating dynamic 3D (4D) scenes.

Method: The authors designed a method that fine-tunes a pretrained video diffusion model, constructed a large-scale 4D dataset (4DNeX-10M), proposed a novel 6D video representation combining RGB and XYZ sequences, and introduced adaptation strategies to enhance 4D modeling.

Result: 4DNeX generates dynamic point clouds that allow novel-view video synthesis, outperforming existing methods in efficiency and generalizability.

Conclusion: The proposed 4DNeX framework provides a scalable, efficient approach for image-to-4D generation, paving the path for comprehensive generative models to simulate evolving dynamic scenes.

Abstract: We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,
dynamic 3D) scene representations from a single image. In contrast to existing
methods that rely on computationally intensive optimization or require
multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D
generation by fine-tuning a pretrained video diffusion model. Specifically, 1)
to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale
dataset with high-quality 4D annotations generated using advanced
reconstruction approaches. 2) we introduce a unified 6D video representation
that jointly models RGB and XYZ sequences, facilitating structured learning of
both appearance and geometry. 3) we propose a set of simple yet effective
adaptation strategies to repurpose pretrained video diffusion models for 4D
modeling. 4DNeX produces high-quality dynamic point clouds that enable
novel-view video synthesis. Extensive experiments demonstrate that 4DNeX
outperforms existing 4D generation methods in efficiency and generalizability,
offering a scalable solution for image-to-4D modeling and laying the foundation
for generative 4D world models that simulate dynamic scene evolution.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [303] [Proceedings 18th Interaction and Concurrency Experience](https://arxiv.org/abs/2508.12308)
*Clément Aubert,Cinzia Di Giusto,Simon Fowler,Violet Ka I Pun*

Main category: cs.DC

TL;DR: Proceedings of ICE’25 detail submissions, reviews, and final research content presented at the 18th Interaction and Concurrency Experience held in Lille, France.


<details>
  <summary>Details</summary>
Motivation: To facilitate interaction between authors and PC members for collaborative improvement of submissions and stimulate robust academic discussions in the field of concurrency and interaction.

Method: The ICE workshop followed a distinctive review process encouraging anonymous interactions between authors and reviewers, receiving 7 submissions with vigorous exchanges before finalizing 5 presentations for the event.

Result: Four research papers and one oral communication were presented, alongside an invited talk by Kirstin Peters, whose abstract is included in the volume.

Conclusion: The ICE’25 workshop successfully fostered academic exchange, incorporating reviewer discussions into the final presentations, thereby enhancing scientific contributions.

Abstract: This volume contains the proceedings of ICE'25, the 18th Interaction and
Concurrency Experience, which was held on Friday 20th June 2025 at the \'Ecole
National Sup\'erieure des Arts et M\'etiers in Lille, France, as a satellite
workshop of DisCoTec 2025. The ICE workshop series features a distinguishing
review and selection procedure: PC members are encouraged to interact,
anonymously, with authors. The 2025 edition of ICE received 7 submissions, each
reviewed by three PC members, and about 75 comments were exchanged during the
review process, witnessing very lively discussions. Four papers were accepted
for publication plus 1 oral communication, which was accepted for presentation
at the workshop. We were proud to host one invited talk, by Kirstin Peters. The
abstract of her talk is included in this volume, together with the final
versions of the research papers, which take into account the discussion at the
workshop and during the review process.

</details>


### [304] [Breaking the Aggregation Bottleneck in Federated Recommendation: A Personalized Model Merging Approach](https://arxiv.org/abs/2508.12386)
*Jundong Chen,Honglei Zhang,Chunxu Zhang,Fangyuan Luo,Yidong Li*

Main category: cs.DC

TL;DR: Federated recommendation suffers from an 'aggregation bottleneck,' which compromises personalization. FedEM addresses this issue by blending global and local models effectively, improving performance without extra mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the issue of suboptimal performance in federated recommendation systems caused by the aggregation bottleneck, where heterogeneity among clients conflicts with personalization goals.

Method: The paper introduces FedEM, a method that elastically merges global and local models, leveraging theoretical insights rather than heuristic approaches, while using existing local models to improve performance.

Result: The proposed FedEM method outperformed state-of-the-art baselines across various real-world datasets by maintaining client-side personalization during collaborative training.

Conclusion: FedEM's innovative approach to addressing the aggregation bottleneck strikes a balance between collaboration and personalization, demonstrating significant advantages in federated recommendation systems.

Abstract: Federated recommendation (FR) facilitates collaborative training by
aggregating local models from massive devices, enabling client-specific
personalization while ensuring privacy. However, we empirically and
theoretically demonstrate that server-side aggregation can undermine
client-side personalization, leading to suboptimal performance, which we term
the aggregation bottleneck. This issue stems from the inherent heterogeneity
across numerous clients in FR, which drives the globally aggregated model to
deviate from local optima. To this end, we propose FedEM, which elastically
merges the global and local models to compensate for impaired personalization.
Unlike existing personalized federated recommendation (pFR) methods, FedEM (1)
investigates the aggregation bottleneck in FR through theoretical insights,
rather than relying on heuristic analysis; (2) leverages off-the-shelf local
models rather than designing additional mechanisms to boost personalization.
Extensive experiments on real-world datasets demonstrate that our method
preserves client personalization during collaborative training, outperforming
state-of-the-art baselines.

</details>


### [305] [DIT: Dimension Reduction View on Optimal NFT Rarity Meters](https://arxiv.org/abs/2508.12671)
*Dmitry Belousov,Yury Yanovich*

Main category: cs.DC

TL;DR: This paper proposes the Rating over all Rarities (ROAR) framework to standardize NFT rarity evaluation and introduces a novel rarity meter design utilizing dimension reduction and new performance measures.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges in assessing and comparing NFT rarity, which is a critical factor influencing their value in the digital asset market.

Method: The authors proposed optimal rarity meter designs using non-metric weighted multidimensional scaling, introduced Dissimilarity in Trades (DIT) as a new performance measure, and evaluated these meters with the ROAR benchmark.

Result: The non-interpretable rarity meter, DIT, showed superior performance compared to existing methods within the ROAR benchmark framework.

Conclusion: The study successfully offers advancements in NFT rarity assessment, improving both methodological rigor and practical utility in rarity evaluation frameworks.

Abstract: Non-fungible tokens (NFTs) have become a significant digital asset class,
each uniquely representing virtual entities such as artworks. These tokens are
stored in collections within smart contracts and are actively traded across
platforms on Ethereum, Bitcoin, and Solana blockchains. The value of NFTs is
closely tied to their distinctive characteristics that define rarity, leading
to a growing interest in quantifying rarity within both industry and academia.
While there are existing rarity meters for assessing NFT rarity, comparing them
can be challenging without direct access to the underlying collection data. The
Rating over all Rarities (ROAR) benchmark addresses this challenge by providing
a standardized framework for evaluating NFT rarity. This paper explores a
dimension reduction approach to rarity design, introducing new performance
measures and meters, and evaluates them using the ROAR benchmark. Our
contributions to the rarity meter design issue include developing an optimal
rarity meter design using non-metric weighted multidimensional scaling,
introducing Dissimilarity in Trades (DIT) as a performance measure inspired by
dimension reduction techniques, and unveiling the non-interpretable rarity
meter DIT, which demonstrates superior performance compared to existing
methods.

</details>


### [306] [Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs](https://arxiv.org/abs/2508.12743)
*Jacob Wahlgren,Gabin Schieffer,Ruimin Shi,Edgar A. León,Roger Pearce,Maya Gokhale,Ivy Peng*

Main category: cs.DC

TL;DR: This paper examines AMD's MI300A Accelerated Processing Units (APUs) equipped with Unified Physical Memory (UPM) within the El Capitan supercomputer, presenting the first detailed analysis of its memory architecture and performance.


<details>
  <summary>Details</summary>
Motivation: Understanding UPM's efficiency in eliminating the difficulties of CPU-GPU memory management, which is particularly relevant in HPC systems.

Method: The authors evaluate the UPM architecture on MI300A through detailed analysis of system properties, software efficiency, and by porting and testing six applications.

Result: Applications using UPM's unified memory model show performance comparable to or better than explicitly managed models, reducing memory costs by up to 44%.

Conclusion: UPM architecture represents a significant advancement in efficiently combining CPU and GPU memory management, making it highly viable for HPC systems.

Abstract: Discrete GPUs are a cornerstone of HPC and data center systems, requiring
management of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)
has been proposed to ease the burden of memory management; however, at a high
cost in performance. The recent introduction of AMD's MI300A Accelerated
Processing Units (APUs)--as deployed in the El Capitan supercomputer--enables
HPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)
for the first time. This work presents the first comprehensive characterization
of the UPM architecture on MI300A. We first analyze the UPM system properties,
including memory latency, bandwidth, and coherence overhead. We then assess the
efficiency of the system software in memory allocation, page fault handling,
TLB management, and Infinity Cache utilization. We propose a set of porting
strategies for transforming applications for the UPM architecture and evaluate
six applications on the MI300A APU. Our results show that applications on UPM
using the unified memory model can match or outperform those in the explicitly
managed model--while reducing memory costs by up to 44%.

</details>


### [307] [Accelerating Edge Inference for Distributed MoE Models with Latency-Optimized Expert Placement](https://arxiv.org/abs/2508.12851)
*Tian Wu,Liming Wang,Zijian Wen,Xiaoxi Zhang,Jingpu Duan,Xianwei Zhang,Jinhang Zuo*

Main category: cs.DC

TL;DR: The paper introduces DanceMoE, a framework for efficient inference of Mixture-of-Experts models on heterogeneous edge devices, focusing on reducing communication and memory challenges.


<details>
  <summary>Details</summary>
Motivation: While MoE models provide increased efficiency and model capacity via sparse expert activation, serving them in resource-constrained environments (like edge devices) is difficult due to their memory and communication demands. Existing solutions often target centralized cloud inference or homogeneous single-edge-device scenarios.

Method: The authors propose DanceMoE, which optimizes expert placement across heterogeneous GPU-equipped edge servers by leveraging activation-aware algorithms. It minimizes cross-server communication through workload locality and incorporates a migration mechanism for dynamic expert allocation as workloads evolve.

Result: Experimental evaluation shows that DanceMoE achieves up to 30.6% lower inference latency and significant reductions in communication overhead when compared against existing methods.

Conclusion: DanceMoE is an effective solution for deploying MoE models in collaborative, heterogeneous edge environments, addressing latency, memory, and communication challenges efficiently.

Abstract: Mixture-of-Experts (MoE) have become a cornerstone for training and scaling
large language models (LLMs), offering substantial gains in model capacity and
efficiency through sparse expert activation. However, serving these models
remains challenging in practice, particularly in resource-constrained edge
environments, due to their large memory footprint and complex communication
demands. While centralized cloud inference is common, it incurs high
infrastructure costs, along with latency and privacy concerns. A few recent
edge MoE works propose memory-efficient strategies but typically focus on
single-device or homogeneous setups. This paper presents DanceMoE, an efficient
MoE inference framework that enables activation-aware expert placement across
collaborative, heterogeneous, GPU-equipped edge servers. DanceMoE leverages the
inherent sparsity of MoE models and workload locality to minimize cross-server
communication and enable efficient expert placement under heterogeneous
resource constraints. It introduces a data-driven, activation-aware placement
algorithm that balances local coverage and memory usage across servers,
alongside a lightweight migration mechanism that adapts expert assignments
under evolving workloads. We evaluate DanceMoE on modern MoE models and widely
used datasets, demonstrating up to 30.6\% lower inference latency, and
substantial communication reduction compared to state-of-the-art baselines,
showcasing the effectiveness of collaborative edge-based MoE inference.

</details>


### [308] [WANify: Gauging and Balancing Runtime WAN Bandwidth for Geo-distributed Data Analytics](https://arxiv.org/abs/2508.12961)
*Anshuman Das Mohapatra,Kwangsung Oh*

Main category: cs.DC

TL;DR: WANify optimizes runtime WAN bandwidth prediction for geo-distributed data analytics systems, enhancing throughput, reducing latency by 26%, and cutting costs by 16%.


<details>
  <summary>Details</summary>
Motivation: Existing methods measure WAN bandwidth statically and fail to capture runtime dynamics and WAN heterogeneity, leading to sub-optimal decisions impacting query latency and costs.

Method: The paper introduces WANify, a framework that uses the Random Forest machine learning model to predict runtime WAN bandwidth dynamically. It optimizes parallel connection usage, accommodating network and workload dynamics and heterogeneities such as skewed data, diverse compute resources, and varying DC quantities.

Result: WANify improved WAN throughput by balancing bandwidth across links, reducing latency by up to 26% and costs by up to 16%, as demonstrated through evaluations on AWS with 8 geo-distributed data centers.

Conclusion: WANify presents an efficient solution to address WAN dynamics and heterogeneity in GDA systems, achieving reduced query latency and costs while improving system performance.

Abstract: Accurate wide area network (WAN) bandwidth (BW) is essential for
geo-distributed data analytics (GDA) systems to make optimal decisions such as
data and task placement to improve performance. Existing GDA systems, however,
measure WAN BW statically and independently between data centers (DCs), while
data transfer occurs dynamically and simultaneously among DCs during workload
execution. Also, they use a single connection WAN BW that cannot capture actual
WAN capacities between distant DCs. Such inaccurate WAN BWs yield sub-optimal
decisions, inflating overall query latency and cost. In this paper, we present
WANify, a new framework that precisely and dynamically gauges achievable
runtime WAN BW using a machine learning prediction scheme, decision tree-based
Random Forest. This helps GDA systems make better decisions yielding reduced
latency and costs including WAN BW monitoring costs. Based on predicted runtime
WAN BW, WANify determines the optimal number of heterogeneous parallel
connections for data transfer among DCs. This approach improves performance
without additional, or even at reduced cost, by fully exploiting available WAN
capacities. In addition, WANify considers dynamics like network and workloads,
and heterogeneity like skewed data, heterogeneous compute resources, and a
varying number of DCs while making decisions. The WANify prototype running on
state-of-the-art GDA systems is evaluated on AWS with 8 geo-distributed DCs.
Results show that WANify enhances WAN throughput by balancing between the
strongest and weakest WAN links, enabling GDA systems to reduce latency and
cost by up to 26% and 16% respectively with minimal effort, all while handling
dynamics and heterogeneity efficiently.

</details>


### [309] [Congested Clique Counting for Local Gibbs Distributions](https://arxiv.org/abs/2508.13083)
*Joshua Z. Sobel*

Main category: cs.DC

TL;DR: This paper presents a distributed algorithm for approximate counting in the CongestedClique model. It achieves efficient runtime for tasks such as approximating graph $q$-colorings, Gibbs distribution partition functions, and weighted independent sets under certain conditions.


<details>
  <summary>Details</summary>
Motivation: Building on recent advances, the paper addresses the need for efficient distributed algorithms to solve counting problems using parallel techniques within the CongestedClique model.

Method: The paper introduces a parallel algorithm leveraging distributed Markov chains to draw $n$ random samples, drawing on techniques like triangle counting and semiring matrix multiplication.

Result: The algorithm achieves a runtime of $\Tilde{O}(n^{1/3}/\epsilon^2)$ rounds for a broad range of counting problems and even faster performance, $\Tilde{O}(1/\epsilon^2)$ rounds, for specific cases such as the hardcore model.

Conclusion: This work demonstrates a significant improvement in distributed approximate counting algorithms in terms of speed and applicability, with potential usefulness in machine learning, statistical physics, and sampling-intensive tasks.

Abstract: There are well established reductions between combinatorial sampling and
counting problems (Jerrum, Valiant, Vazirani TCS 1986). Building off of a very
recent parallel algorithm utilizing this connection (Liu, Yin, Zhang arxiv
2024), we demonstrate the first approximate counting algorithm in the
CongestedClique for a wide range of problems. Most interestingly, we present an
algorithm for approximating the number of $q$-colorings of a graph within
$\epsilon$-multiplicative error, when $q>\alpha\Delta$ for any constant
$\alpha>2$, in $\Tilde{O}\big(\frac{n^{1/3}}{\epsilon^2}\big)$ rounds. More
generally, we achieve a runtime of
$\Tilde{O}\big(\frac{n^{1/3}}{\epsilon^2}\big)$ rounds for approximating the
partition function of Gibbs distributions defined over graphs when simple
locality and fast mixing conditions hold. Gibbs distributions are widely used
in fields such as machine learning and statistical physics. We obtain our
result by providing an algorithm to draw $n$ random samples from a distributed
Markov chain in parallel, using similar ideas to triangle counting (Dolev,
Lenzen, Peled DISC 2012) and semiring matrix multiplication (Censor-Hillel,
Kaski, Korhonen, Lenzen, Paz, Suomela PODC 2015). Aside from counting problems,
this result may be interesting for other applications requiring a large number
of samples. In the special case of estimating the partition function of the
hardcore model, also known as counting weighted independent sets, we can do
even better and achieve an $\Tilde{O}\big(\frac{1}{\epsilon^2}\big)$ round
algorithm, when the fugacity $\lambda \leq \frac{\alpha}{\Delta-1}$, where
$\alpha$ is an arbitrary constant less than $1$.

</details>


### [310] [Team Formation and Applications](https://arxiv.org/abs/2508.13084)
*Yuval Emek,Shay Kutten,Ido Rafael,Gadi Taubenfeld*

Main category: cs.DC

TL;DR: This paper introduces a novel distributed problem called Team Formation (TF) and a corresponding randomized algorithm. The method assembles tokens into teams, even with partial failures, in an efficient message- and time-bound manner, serving as a foundation to solve other distributed problems.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need for a long-lived, efficient solution for assembling teams of tokens in distributed systems with asynchronous communication graphs and bounded messages, where some nodes may initially fail.

Method: The authors propose a randomized algorithm that operates in an asynchronous model with complete communication graphs. It assembles injected environmental tokens into teams of a specified size (σ).

Result: The TF approach facilitates efficient solutions for various existing and new distributed problems, including leader election, threshold detection, and online gaming match-making.

Conclusion: The TF algorithm provides a versatile framework for distributed problem-solving, breaking existing complexity bounds while establishing its own tight lower bound for message complexity.

Abstract: A novel long-lived distributed problem, called Team Formation (TF), is
introduced together with a message- and time-efficient randomized algorithm.
The problem is defined over the asynchronous model with a complete
communication graph, using bounded size messages, where a certain fraction of
the nodes may experience a generalized, strictly stronger, version of initial
failures. The goal of a TF algorithm is to assemble tokens injected by the
environment, in a distributed manner, into teams of size $\sigma$, where
$\sigma$ is a parameter of the problem.
  The usefulness of TF is demonstrated by using it to derive efficient
algorithms for many distributed problems. Specifically, we show that various
(one-shot as well as long-lived) distributed problems reduce to TF. This
includes well-known (and extensively studied) distributed problems such as
several versions of leader election and threshold detection. For example, we
are the first to break the linear message complexity bound for asynchronous
implicit leader election. We also improve the time complexity of
message-optimal algorithms for asynchronous explicit leader election. Other
distributed problems that reduce to TF are new ones, including matching players
in online gaming platforms, a generalization of gathering, constructing a
perfect matching in an induced subgraph of the complete graph, quorum sensing
in message-passing networks, and more. To complement our positive contribution,
we establish a tight lower bound on the message complexity of TF algorithms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [311] [Sparse Attention across Multiple-context KV Cache](https://arxiv.org/abs/2508.11661)
*Ziyi Cao,Qingyi Si,Jingbin Zhang,Bingquan Liu*

Main category: cs.LG

TL;DR: This paper introduces SamKV, a method to efficiently sparsify and locally recompute multiple-context Key-Value (KV) Cache for large language models, achieving sequence compression without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiencies of reusing and sparsifying multiple-context KV Cache in retrieval-augmented generation scenarios, where existing methods fail to reduce memory overhead and maintain accuracy due to missing cross-context attention.

Method: SamKV incorporates attention sparsification by considering complementary information across multiple contexts during sparsification and performs localized recomputation of the sparsified KV Cache.

Result: SamKV compresses sequence length to 15% while maintaining accuracy compared to full recomputation baselines, leading to significant throughput improvements in multi-context RAG scenarios.

Conclusion: The proposed SamKV approach enables efficient inference for large language models by optimizing KV Cache usage in multi-context scenarios while eliminating accuracy degradation and memory overhead challenges.

Abstract: Large language models face significant cost challenges in long-sequence
inference. To address this, reusing historical Key-Value (KV) Cache for
improved inference efficiency has become a mainstream approach. Recent advances
further enhance throughput by sparse attention mechanisms to select the most
relevant KV Cache, thereby reducing sequence length. However, such techniques
are limited to single-context scenarios, where historical KV Cache is computed
sequentially with causal-attention dependencies. In retrieval-augmented
generation (RAG) scenarios, where retrieved documents as context are unknown
beforehand, each document's KV Cache is computed and stored independently
(termed multiple-context KV Cache), lacking cross-attention between contexts.
This renders existing methods ineffective. Although prior work partially
recomputes multiple-context KV Cache to mitigate accuracy loss from missing
cross-attention, it requires retaining all KV Cache throughout, failing to
reduce memory overhead. This paper presents SamKV, the first exploration of
attention sparsification for multiple-context KV Cache. Specifically, SamKV
takes into account the complementary information of other contexts when
sparsifying one context, and then locally recomputes the sparsified
information. Experiments demonstrate that our method compresses sequence length
to 15% without accuracy degradation compared with full-recompuation baselines,
significantly boosting throughput in multi-context RAG scenarios.

</details>


### [312] [Assessing Representation Stability for Transformer Models](https://arxiv.org/abs/2508.11667)
*Bryan E. Tuck,Rakesh M. Verma*

Main category: cs.LG

TL;DR: The paper proposes Representation Stability (RS), a model-agnostic framework for detecting adversarial text attacks by measuring embedding sensitivity when masking critical words.


<details>
  <summary>Details</summary>
Motivation: Adversarial text attacks pose significant risks to transformer models, and current defenses are either attack-specific or demand expensive retraining.

Method: RS ranks important words using heuristics, measures sensitivity to masking top-k critical words, and uses a BiLSTM detector to analyze the patterns.

Result: RS achieved over 88% detection accuracy across three datasets, three attack types, and two victim models, often outperforming existing methods with lower computational costs.

Conclusion: The RS framework effectively generalizes to unseen datasets, attacks, and models without requiring retraining, providing a practical adversarial text detection solution.

Abstract: Adversarial text attacks remain a persistent threat to transformer models,
yet existing defenses are typically attack-specific or require costly model
retraining. We introduce Representation Stability (RS), a model-agnostic
detection framework that identifies adversarial examples by measuring how
embedding representations change when important words are masked. RS first
ranks words using importance heuristics, then measures embedding sensitivity to
masking top-k critical words, and processes the resulting patterns with a
BiLSTM detector. Experiments show that adversarially perturbed words exhibit
disproportionately high masking sensitivity compared to naturally important
words. Across three datasets, three attack types, and two victim models, RS
achieves over 88% detection accuracy and demonstrates competitive performance
compared to existing state-of-the-art methods, often at lower computational
cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure
perturbation identification quality, we reveal that gradient-based ranking
outperforms attention and random selection approaches, with identification
quality correlating with detection performance for word-level attacks. RS also
generalizes well to unseen datasets, attacks, and models without retraining,
providing a practical solution for adversarial text detection.

</details>


### [313] [Hierarchical Evaluation Function (HEF): A Multi-Metric Approach for Optimizing Demand Forecasting Models](https://arxiv.org/abs/2508.13057)
*Adolfo González,Víctor Parada*

Main category: cs.LG

TL;DR: This study evaluates two custom evaluation functions, HEF and FMAE, for multivariate time series modeling in demand forecasting, highlighting their respective advantages.


<details>
  <summary>Details</summary>
Motivation: Demand forecasting is critical for resource optimization and responsiveness in competitive markets, but conventional evaluation metrics have limitations in multivariate time series modeling.

Method: The study compares HEF and FMAE using three data splits (91:9, 80:20, 70:30), three optimizers (Grid Search, PSO, Optuna), and evaluates fit, accuracy, robustness, and computational efficiency.

Result: HEF demonstrated superior performance in global metrics like R2 and RMSE, while FMAE excelled in local metrics and faster execution time, highlighting a trade-off based on use case.

Conclusion: HEF is more suitable for strategic long-term planning, whereas FMAE is better for short-term operational scenarios, and a replicable framework is proposed for optimizing models.

Abstract: Demand forecasting is essential for strategic planning in competitive
environments, enabling resource optimization and improved responsiveness to
market dynamics. However, multivariate time series modeling faces challenges
due to data complexity, uncertainty, and frequent regime shifts. Traditional
evaluation metrics can introduce biases and limit generalization. This work
compares two custom evaluation functions: FMAE (Focused Mean Absolute Error),
focused on minimizing absolute errors, and HEF (Hierarchical Evaluation
Function), designed to weight global metrics and penalize large deviations.
Experiments were conducted under different data splits (91:9, 80:20, 70:30)
using three optimizers (Grid Search, PSO, Optuna), assessing fit, relative
accuracy, robustness, and computational efficiency. Results show that HEF
consistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE,
RMSSE), enhancing model robustness and explanatory power. These findings were
confirmed via visualizations and statistical tests. Conversely, FMAE offers
advantages in local metrics (MAE, MASE) and execution time, making it suitable
for short-term scenarios. The study highlights a methodological trade-off: HEF
is ideal for strategic planning, while FMAE is better suited for operational
efficiency. A replicable framework is proposed for optimizing predictive models
in dynamic environments.

</details>


### [314] [Collaborative Learning-Enhanced Lightweight Models for Predicting Arterial Blood Pressure Waveform in a Large-scale Perioperative Dataset](https://arxiv.org/abs/2508.11669)
*Wentao Li,Yonghu He,Kun Gao,Qing Liu,Yali Zheng*

Main category: cs.LG

TL;DR: The paper proposes a lightweight deep learning model, "KDCL_sInvResUNet," for real-time arterial blood pressure monitoring using noninvasive signals, optimized for embedded systems.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for noninvasive ABP estimation often lack efficiency and are unsuitable for deployment on resource-constrained embedded systems.

Method: Introduced sInvResUNet, a lightweight neural network with 0.89M parameters and 0.02 GFLOPS, and a collaborative learning pipeline (KDCL_sInvResUNet). Evaluated its performance on a large perioperative dataset.

Result: The model performs real-time ABP estimation in just 8.49 milliseconds for a 10-second output, achieving comparable accuracy to large models with a mean absolute error of 10.06 mmHg and a Pearson correlation of 0.88.

Conclusion: Although the model demonstrated promise in real-time monitoring, all tested models exhibited varying performance across diverse population and cardiovascular conditions, signaling a need for further work on generalization.

Abstract: Noninvasive arterial blood pressure (ABP) monitoring is essential for patient
management in critical care and perioperative settings, providing continuous
assessment of cardiovascular hemodynamics with minimal risks. Numerous deep
learning models have developed to reconstruct ABP waveform from noninvasively
acquired physiological signals such as electrocardiogram and
photoplethysmogram. However, limited research has addressed the issue of model
performance and computational load for deployment on embedded systems. The
study introduces a lightweight sInvResUNet, along with a collaborative learning
scheme named KDCL_sInvResUNet. With only 0.89 million parameters and a
computational load of 0.02 GFLOPS, real-time ABP estimation was successfully
achieved on embedded devices with an inference time of just 8.49 milliseconds
for a 10-second output. We performed subject-independent validation in a
large-scale and heterogeneous perioperative dataset containing 1,257,141 data
segments from 2,154 patients, with a wide BP range (41-257 mmHg for SBP, and
31-234 mmHg for DBP). The proposed KDCL_sInvResUNet achieved lightly better
performance compared to large models, with a mean absolute error of 10.06 mmHg
and mean Pearson correlation of 0.88 in tracking ABP changes. Despite these
promising results, all deep learning models showed significant performance
variations across different demographic and cardiovascular conditions,
highlighting their limited ability to generalize across such a broad and
diverse population. This study lays a foundation work for real-time,
unobtrusive ABP monitoring in real-world perioperative settings, providing
baseline for future advancements in this area.

</details>


### [315] [Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning](https://arxiv.org/abs/2508.11673)
*Haojie Zhang,Yixiong Liang,Hulin Kuang,Lihui Cen,Zhe Qu,Yigang Cen,Min Zeng,Shichao Kan*

Main category: cs.LG

TL;DR: This paper introduces MSLoRA-CR, a method for incremental learning across multimodal biomedical images, outperforming existing methods while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: Incremental learning for multimodal biomedical images is needed to avoid the high inference costs of training separate models and to address challenges in knowledge preservation and transfer across modalities.

Method: MSLoRA-CR fine-tunes Modality-Specific LoRA modules while employing Contrastive Regularization, built on a large vision-language model (LVLM). The pretrained model remains frozen, and new LoRA modules are added for new modalities.

Result: MSLoRA-CR achieves a 1.88% improvement in overall performance compared to unconstrained incremental learning methods, while also ensuring computational efficiency.

Conclusion: The proposed MSLoRA-CR method successfully handles multimodal biomedical image incremental learning, offering superior performance and efficiency over existing methods.

Abstract: Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for
handling diverse tasks and modalities in the biomedical domain, as training
separate models for each modality or task significantly increases inference
costs. Existing incremental learning methods focus on task expansion within a
single modality, whereas MBIIL seeks to train a unified model incrementally
across modalities. The MBIIL faces two challenges: I) How to preserve
previously learned knowledge during incremental updates? II) How to effectively
leverage knowledge acquired from existing modalities to support new modalities?
To address these challenges, we propose MSLoRA-CR, a method that fine-tunes
Modality-Specific LoRA modules while incorporating Contrastive Regularization
to enhance intra-modality knowledge sharing and promote inter-modality
knowledge differentiation. Our approach builds upon a large vision-language
model (LVLM), keeping the pretrained model frozen while incrementally adapting
new LoRA modules for each modality or task. Experiments on the incremental
learning of biomedical images demonstrate that MSLoRA-CR outperforms both the
state-of-the-art (SOTA) approach of training separate models for each modality
and the general incremental learning method (incrementally fine-tuning LoRA).
Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance
compared to unconstrained incremental learning methods while maintaining
computational efficiency. Our code is publicly available at
https://github.com/VentusAislant/MSLoRA_CR.

</details>


### [316] [Lifelong Learner: Discovering Versatile Neural Solvers for Vehicle Routing Problems](https://arxiv.org/abs/2508.11679)
*Shaodi Feng,Zhuoyi Lin,Jianan Zhou,Cong Zhang,Jingwen Li,Kuan-Wen Chen,Senthilnath Jayavelu,Yew-Soon Ong*

Main category: cs.LG

TL;DR: Deep learning neural solvers for vehicle routing problems (VRPs) often face limitations in adaptability across varying contexts. This paper introduces a lifelong learning framework to incrementally enhance solver versatility using a Transformer-based model and innovative mechanisms.


<details>
  <summary>Details</summary>
Motivation: Current neural solvers for VRPs operate in simplified contexts, which limits their applicability to varying real-world scenarios.

Method: The paper proposes a lifelong learner (LL) with an inter-context self-attention mechanism for transferring knowledge across VRPs. Additionally, a dynamic context scheduler (DCS) is designed for cross-context experience replay.

Result: The lifelong learning framework demonstrates superior performance on synthetic and benchmark VRPs, outperforming other neural solvers with problem sizes up to 18k.

Conclusion: The introduced lifelong learning framework shows promise in solving VRPs across diverse contexts, providing versatile and effective policies for real-world applications.

Abstract: Deep learning has been extensively explored to solve vehicle routing problems
(VRPs), which yields a range of data-driven neural solvers with promising
outcomes. However, most neural solvers are trained to tackle VRP instances in a
relatively monotonous context, e.g., simplifying VRPs by using Euclidean
distance between nodes and adhering to a single problem size, which harms their
off-the-shelf application in different scenarios. To enhance their versatility,
this paper presents a novel lifelong learning framework that incrementally
trains a neural solver to manage VRPs in distinct contexts. Specifically, we
propose a lifelong learner (LL), exploiting a Transformer network as the
backbone, to solve a series of VRPs. The inter-context self-attention mechanism
is proposed within LL to transfer the knowledge obtained from solving preceding
VRPs into the succeeding ones. On top of that, we develop a dynamic context
scheduler (DCS), employing the cross-context experience replay to further
facilitate LL looking back on the attained policies of solving preceding VRPs.
Extensive results on synthetic and benchmark instances (problem sizes up to
18k) show that our LL is capable of discovering effective policies for tackling
generic VRPs in varying contexts, which outperforms other neural solvers and
achieves the best performance for most VRPs.

</details>


### [317] [Comparative Analysis of Time Series Foundation Models for Demographic Forecasting: Enhancing Predictive Accuracy in US Population Dynamics](https://arxiv.org/abs/2508.11680)
*Aditya Akella,Jonathan Farah*

Main category: cs.LG

TL;DR: This study investigates the use of Time Series Foundation Models (TimesFM) for demographic forecasting in the U.S., showing superior accuracy compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Demographic changes from globalization, economics, and environment require forecasting for effective decision-making in policy and planning.

Method: Time Series Foundation Models (TimesFM) were applied to U.S. demographic data and compared with LSTM, ARIMA, and Linear Regression across six states.

Result: TimesFM achieved the lowest Mean Squared Error (MSE) in 86.67% of test cases, excelling in analyzing minority populations with limited data.

Conclusion: Pre-trained foundation models like TimesFM can improve demographic forecasting accuracy and inform policy interventions with minimal fine-tuning.

Abstract: Demographic shifts, influenced by globalization, economic conditions,
geopolitical events, and environmental factors, pose significant challenges for
policymakers and researchers. Accurate demographic forecasting is essential for
informed decision-making in areas such as urban planning, healthcare, and
economic policy. This study explores the application of time series foundation
models to predict demographic changes in the United States using datasets from
the U.S. Census Bureau and Federal Reserve Economic Data (FRED). We evaluate
the performance of the Time Series Foundation Model (TimesFM) against
traditional baselines including Long Short-Term Memory (LSTM) networks,
Autoregressive Integrated Moving Average (ARIMA), and Linear Regression. Our
experiments across six demographically diverse states demonstrate that TimesFM
achieves the lowest Mean Squared Error (MSE) in 86.67% of test cases, with
particularly strong performance on minority populations with sparse historical
data. These findings highlight the potential of pre-trained foundation models
to enhance demographic analysis and inform proactive policy interventions
without requiring extensive task-specific fine-tuning.

</details>


### [318] [FedUHD: Unsupervised Federated Learning using Hyperdimensional Computing](https://arxiv.org/abs/2508.12021)
*You Hak Lee,Xiaofan Yu,Quanling Zhao,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: Proposing FedUHD, a novel unsupervised federated learning framework using Hyperdimensional Computing (HDC) to address challenges like non-iid data, high costs, and communication noise, showing significant improvements in efficiency, cost, and accuracy.


<details>
  <summary>Details</summary>
Motivation: To overcome key challenges in unsupervised federated learning (UFL), such as non-iid data distribution, computational/communication costs, and noise vulnerability, while also avoiding the overhead associated with NN-based UFL.

Method: FedUHD leverages Hyperdimensional Computing (HDC) for its inherent lightweight and robust properties. It introduces a kNN-based cluster hypervector removal on the client side and a weighted HDC aggregation on the server side to tackle UFL's issues.

Result: FedUHD achieves significant performance gains: up to 173.6x faster training, 612.7x better energy efficiency, 271x lower communication costs, and 15.50% higher accuracy on average compared to state-of-the-art NN-based UFL approaches.

Conclusion: FedUHD demonstrates the potential of HDC in UFL by offering a scalable, efficient, and robust alternative to traditional NN-based methods, effectively overcoming long-standing challenges in the field.

Abstract: Unsupervised federated learning (UFL) has gained attention as a
privacy-preserving, decentralized machine learning approach that eliminates the
need for labor-intensive data labeling. However, UFL faces several challenges
in practical applications: (1) non-independent and identically distributed
(non-iid) data distribution across devices, (2) expensive computational and
communication costs at the edge, and (3) vulnerability to communication noise.
Previous UFL approaches have relied on deep neural networks (NN), which
introduce substantial overhead in both computation and communication. In this
paper, we propose FedUHD, the first UFL framework based on Hyperdimensional
Computing (HDC). HDC is a brain-inspired computing scheme with lightweight
training and inference operations, much smaller model size, and robustness to
communication noise. FedUHD introduces two novel HDC-based designs to improve
UFL performance. On the client side, a kNN-based cluster hypervector removal
method addresses non-iid data samples by eliminating detrimental outliers. On
the server side, a weighted HDC aggregation technique balances the non-iid data
distribution across clients. Our experiments demonstrate that FedUHD achieves
up to 173.6x and 612.7x better speedup and energy efficiency, respectively, in
training, up to 271x lower communication cost, and 15.50% higher accuracy on
average across diverse settings, along with superior robustness to various
types of noise compared to state-of-the-art NN-based UFL approaches.

</details>


### [319] [From Heuristics to Data: Quantifying Site Planning Layout Indicators with Deep Learning and Multi-Modal Data](https://arxiv.org/abs/2508.11723)
*Qian Cao,Jielin Chen,Junchao Zhao,Rudi Stouffs*

Main category: cs.LG

TL;DR: The paper presents the Site Planning Layout Indicator (SPLI) system, a data-driven method for efficiently quantifying urban spatial layouts by integrating various data sources.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional site planning methods that rely on subjective judgment and single-source data, resulting in inadequate quantification of urban spatial layouts.

Method: The SPLI system uses a multimodal data-driven framework integrating OpenStreetMap (OSM), Points of Interest (POI), building morphology, land use data, and satellite imagery. It employs advanced metrics across five dimensions and leverages deep learning models like RGNN and GNN to improve data synthesis and layout analytics.

Result: The SPLI system achieves improved functional classification accuracy, fills data gaps, and offers a standardized analytical approach for urban spatial layouts.

Conclusion: The SPLI system enables automated, data-driven urban planning and provides a structured framework for analyzing and quantifying the efficiency and organization of urban spaces.

Abstract: The spatial layout of urban sites shapes land-use efficiency and spatial
organization. Traditional site planning often relies on experiential judgment
and single-source data, limiting systematic quantification of multifunctional
layouts. We propose a Site Planning Layout Indicator (SPLI) system, a
data-driven framework integrating empirical knowledge with heterogeneous
multi-source data to produce structured urban spatial information. The SPLI
supports multimodal spatial data systems for analytics, inference, and
retrieval by combining OpenStreetMap (OSM), Points of Interest (POI), building
morphology, land use, and satellite imagery. It extends conventional metrics
through five dimensions: (1) Hierarchical Building Function Classification,
refining empirical systems into clear hierarchies; (2) Spatial Organization,
quantifying seven layout patterns (e.g., symmetrical, concentric,
axial-oriented); (3) Functional Diversity, transforming qualitative assessments
into measurable indicators using Functional Ratio (FR) and Simpson Index (SI);
(4) Accessibility to Essential Services, integrating facility distribution and
transport networks for comprehensive accessibility metrics; and (5) Land Use
Intensity, using Floor Area Ratio (FAR) and Building Coverage Ratio (BCR) to
assess utilization efficiency. Data gaps are addressed through deep learning,
including Relational Graph Neural Networks (RGNN) and Graph Neural Networks
(GNN). Experiments show the SPLI improves functional classification accuracy
and provides a standardized basis for automated, data-driven urban spatial
analytics.

</details>


### [320] [Causal Structure Learning in Hawkes Processes with Complex Latent Confounder Networks](https://arxiv.org/abs/2508.11727)
*Songyao Jin,Biwei Huang*

Main category: cs.LG

TL;DR: The paper addresses the challenge of identifying latent subprocesses in partially observed systems using Multivariate Hawkes processes, proposing a discrete-time model and a two-phase iterative algorithm.


<details>
  <summary>Details</summary>
Motivation: Real-world systems are often partially observed with latent subprocesses, complicating causal structure identification within complex systems modeled by the Multivariate Hawkes process.

Method: A discrete-time representation of continuous-time event sequences is employed, alongside a path-based two-phase iterative algorithm that alternates between inferring causal relationships and uncovering latent subprocesses.

Result: Experiments on synthetic and real-world datasets demonstrate the proposed method's effectiveness in recovering causal structures even with latent subprocesses.

Conclusion: The proposed approach offers a robust framework for identifiability of latent subprocesses and causal influences within complex systems.

Abstract: Multivariate Hawkes process provides a powerful framework for modeling
temporal dependencies and event-driven interactions in complex systems. While
existing methods primarily focus on uncovering causal structures among observed
subprocesses, real-world systems are often only partially observed, with latent
subprocesses posing significant challenges. In this paper, we show that
continuous-time event sequences can be represented by a discrete-time model as
the time interval shrinks, and we leverage this insight to establish necessary
and sufficient conditions for identifying latent subprocesses and the causal
influences. Accordingly, we propose a two-phase iterative algorithm that
alternates between inferring causal relationships among discovered subprocesses
and uncovering new latent subprocesses, guided by path-based conditions that
guarantee identifiability. Experiments on both synthetic and real-world
datasets show that our method effectively recovers causal structures despite
the presence of latent subprocesses.

</details>


### [321] [BRIEF: BRain-Inspired network connection search with Extensive temporal feature Fusion enhances disease classification](https://arxiv.org/abs/2508.11732)
*Xiangxiang Cui,Min Zhao,Dongmei Zhi,Shile Qi,Vince D Calhoun,Jing Sui*

Main category: cs.LG

TL;DR: The paper introduces BRIEF, a brain-inspired framework, to improve fMRI-based mental disorder classification by optimizing network architecture and feature fusion. It achieves superior classification accuracy for schizophrenia and autism spectrum disorder.


<details>
  <summary>Details</summary>
Motivation: The study aims to address limitations in deep learning models for fMRI classification, like manual network architecture determination and simplistic feature fusion methods, seeking inspiration from the brain's adaptive neural connectivity mechanisms.

Method: The BRIEF framework employs improved neural network connection search (NCS) with a Transformer-based multi-feature fusion module. Four types of fMRI data representations are processed via encoders, utilizing Q-learning to dynamically optimize NCS within a Markov Decision Process. A Transformer combines the high-level features, incorporating an attention mechanism for interpretability.

Result: BRIEF outperforms 21 state-of-the-art models for schizophrenia (SZ) and autism spectrum disorder (ASD) classification, improving AUC by 2.2% to 12.1%, achieving an AUC of 91.5% for SZ and 78.4% for ASD.

Conclusion: BRIEF is the first framework to integrate brain-inspired reinforcement learning for optimizing fMRI-based mental disorder classification and shows strong potential for detecting neuroimaging biomarkers.

Abstract: Existing deep learning models for functional MRI-based classification have
limitations in network architecture determination (relying on experience) and
feature space fusion (mostly simple concatenation, lacking mutual learning).
Inspired by the human brain's mechanism of updating neural connections through
learning and decision-making, we proposed a novel BRain-Inspired feature Fusion
(BRIEF) framework, which is able to optimize network architecture automatically
by incorporating an improved neural network connection search (NCS) strategy
and a Transformer-based multi-feature fusion module. Specifically, we first
extracted 4 types of fMRI temporal representations, i.e., time series (TCs),
static/dynamic functional connection (FNC/dFNC), and multi-scale dispersion
entropy (MsDE), to construct four encoders. Within each encoder, we employed a
modified Q-learning to dynamically optimize the NCS to extract high-level
feature vectors, where the NCS is formulated as a Markov Decision Process.
Then, all feature vectors were fused via a Transformer, leveraging both
stable/time-varying connections and multi-scale dependencies across different
brain regions to achieve the final classification. Additionally, an attention
module was embedded to improve interpretability. The classification performance
of our proposed BRIEF was compared with 21 state-of-the-art models by
discriminating two mental disorders from healthy controls: schizophrenia (SZ,
n=1100) and autism spectrum disorder (ASD, n=1550). BRIEF demonstrated
significant improvements of 2.2% to 12.1% compared to 21 algorithms, reaching
an AUC of 91.5% - 0.6% for SZ and 78.4% - 0.5% for ASD, respectively. This is
the first attempt to incorporate a brain-inspired, reinforcement learning
strategy to optimize fMRI-based mental disorder classification, showing
significant potential for identifying precise neuroimaging biomarkers.

</details>


### [322] [Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware](https://arxiv.org/abs/2508.11940)
*Yuannuo Feng,Wenyong Zhou,Yuexi Lyu,Yixiang Zhang,Zhengwu Liu,Ngai Wong,Wang Kang*

Main category: cs.LG

TL;DR: The paper introduces an improved Straight-Through Estimator (STE)-based framework for training analog Compute-In-Memory (CIM) architectures, which addresses hardware-induced noise by separating forward noise simulation from backward gradient computation. This method enhances accuracy, efficiency, and memory consumption in neural network deployment.


<details>
  <summary>Details</summary>
Motivation: Analog CIM architectures promise energy-efficient neural network inference but face challenges due to hardware-induced noise. Current noise-aware training approaches use oversimplified noise models, underscoring the need for better methods to overcome these limitations.

Method: The method involves extending the Straight-Through Estimator (STE) framework by decoupling noise simulation in the forward pass from backward gradient computation, enabling the use of more complex yet accurate noise models without overwhelming computational resources.

Result: The proposed framework achieved significant improvements: up to 5.3% in accuracy for image classification, 0.72 reduction in perplexity for text generation, 2.2× faster training times, and a 37.9% reduction in peak memory usage as compared to conventional noise-aware training methods.

Conclusion: The extended STE framework demonstrated its effectiveness in addressing hardware-induced noise in analog CIM systems, balancing improved accuracy with computational efficiency and stability for noise-aware training methods.

Abstract: Analog Compute-In-Memory (CIM) architectures promise significant energy
efficiency gains for neural network inference, but suffer from complex
hardware-induced noise that poses major challenges for deployment. While
noise-aware training methods have been proposed to address this issue, they
typically rely on idealized and differentiable noise models that fail to
capture the full complexity of analog CIM hardware variations. Motivated by the
Straight-Through Estimator (STE) framework in quantization, we decouple forward
noise simulation from backward gradient computation, enabling noise-aware
training with more accurate but computationally intractable noise modeling in
analog CIM systems. We provide theoretical analysis demonstrating that our
approach preserves essential gradient directional information while maintaining
computational tractability and optimization stability. Extensive experiments
show that our extended STE framework achieves up to 5.3% accuracy improvement
on image classification, 0.72 perplexity reduction on text generation,
2.2$\times$ speedup in training time, and 37.9% lower peak memory usage
compared to standard noise-aware training methods.

</details>


### [323] [Scalable Geospatial Data Generation Using AlphaEarth Foundations Model](https://arxiv.org/abs/2508.11739)
*Luc Houriez,Sebastian Pilarski,Behzad Vahedi,Ali Ahmadalipour,Teo Honda Scully,Nicholas Aflitto,David Andre,Caroline Jaffe,Martha Wedner,Rich Mazzola,Josh Jeffery,Ben Messinger,Sage McGinley-Smith,Sarah Russell*

Main category: cs.LG

TL;DR: The paper explores extending geospatial labeled datasets globally using Google DeepMind's AlphaEarth Foundations (AEF) and demonstrates the feasibility of achieving good accuracy even with basic machine learning models.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of geospatial labeled datasets that often lack global coverage, thereby hindering comprehensive geographic analyses.

Method: The authors adopt Google DeepMind's AEF as input and utilize basic machine learning models, such as random forests and logistic regression, to extend datasets like LANDFIRE's EVT from the USA into Canada.

Result: For the EvtPhys dataset with 13 classes, models achieve 81% classification accuracy in the USA and 73% in Canada. Predictions qualitatively align with ground truth despite certain limitations.

Conclusion: The methodology successfully extends geospatial datasets globally, demonstrating the effectiveness of leveraging AEF for geographic scalability even with simple models.

Abstract: High-quality labeled geospatial datasets are essential for extracting
insights and understanding our planet. Unfortunately, these datasets often do
not span the entire globe and are limited to certain geographic regions where
data was collected. Google DeepMind's recently released AlphaEarth Foundations
(AEF) provides an information-dense global geospatial representation designed
to serve as a useful input across a wide gamut of tasks. In this article we
propose and evaluate a methodology which leverages AEF to extend geospatial
labeled datasets beyond their initial geographic regions. We show that even
basic models like random forests or logistic regression can be used to
accomplish this task. We investigate a case study of extending LANDFIRE's
Existing Vegetation Type (EVT) dataset beyond the USA into Canada at two levels
of granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for
EvtPhys, model predictions align with ground truth. Trained models achieve 81%
and 73% classification accuracy on EvtPhys validation sets in the USA and
Canada, despite discussed limitations.

</details>


### [324] [Fed-DPRoC:Communication-Efficient Differentially Private and Robust Federated Learning](https://arxiv.org/abs/2508.12978)
*Yue Xia,Tayyebeh Jahani-Nezhad,Rawad Bitar*

Main category: cs.LG

TL;DR: Fed-DPRoC is a federated learning framework offering differential privacy, Byzantine robustness, and communication efficiency. It uses robust-compatible compression, illustrated via RobAJoL, to achieve these goals.


<details>
  <summary>Details</summary>
Motivation: The need for a federated learning framework that can simultaneously ensure data privacy, resilience against Byzantine attacks, and efficient communication.

Method: Introduces robust-compatible compression, leveraging the Johnson-Lindenstrauss transform for data compression and robust averaging for secure aggregation.

Result: Proved theoretical compatibility between JL transform and robust averaging; experiments show strong performance in robustness and utility compared to existing methods.

Conclusion: RobAJoL ensures differential privacy, robustness against attacks, and lower communication costs, making it superior to existing methods.

Abstract: We propose Fed-DPRoC, a novel federated learning framework that
simultaneously ensures differential privacy (DP), Byzantine robustness, and
communication efficiency. We introduce the concept of robust-compatible
compression, which enables users to compress DP-protected updates while
maintaining the robustness of the aggregation rule. We instantiate our
framework as RobAJoL, combining the Johnson-Lindenstrauss (JL) transform for
compression with robust averaging for robust aggregation. We theoretically
prove the compatibility of JL transform with robust averaging and show that
RobAJoL preserves robustness guarantees, ensures DP, and reduces communication
cost. Experiments on CIFAR-10 and Fashion MNIST validate our theoretical claims
and demonstrate that RobAJoL outperforms existing methods in terms of
robustness and utility under different Byzantine attacks.

</details>


### [325] [Fed-Meta-Align: A Similarity-Aware Aggregation and Personalization Pipeline for Federated TinyML on Heterogeneous Data](https://arxiv.org/abs/2508.11794)
*Hemanth Macharla,Mayukha Pal*

Main category: cs.LG

TL;DR: The paper proposes Fed-Meta-Align, a framework for effective learning on heterogeneous IoT devices under resource constraints using tailored initialization, adaptive aggregation, and on-device personalization.


<details>
  <summary>Details</summary>
Motivation: Existing Federated Learning (FL) techniques struggle with non-IID data in IoT devices, leading to model divergence and poor fault classification performance critical for industrial safety.

Method: Fed-Meta-Align is a four-phase framework: (1) training on a public dataset for a strong initialization, (2) meta-initialization with IoT device data for heterogeneity awareness, (3) parallel FL with dual-criterion aggregation, and (4) on-device model personalization.

Result: Fed-Meta-Align achieved 91.27% test accuracy across heterogeneous IoT devices, surpassing state-of-the-art methods like Personalized FedAvg and FedProx by up to 3.87% and 3.37% on fault datasets.

Conclusion: Fed-Meta-Align effectively addresses challenges of non-IID data in IoT environments, enabling robust, high-performance fault classification via a thoughtful progression of initialization, aggregation, and personalization phases.

Abstract: Real-time fault classification in resource-constrained Internet of Things
(IoT) devices is critical for industrial safety, yet training robust models in
such heterogeneous environments remains a significant challenge. Standard
Federated Learning (FL) often fails in the presence of non-IID data, leading to
model divergence. This paper introduces Fed-Meta-Align, a novel four-phase
framework designed to overcome these limitations through a sophisticated
initialization and training pipeline. Our process begins by training a
foundational model on a general public dataset to establish a competent
starting point. This model then undergoes a serial meta-initialization phase,
where it sequentially trains on a subset of IOT Device data to learn a
heterogeneity-aware initialization that is already situated in a favorable
region of the loss landscape. This informed model is subsequently refined in a
parallel FL phase, which utilizes a dual-criterion aggregation mechanism that
weights for IOT devices updates based on both local performance and cosine
similarity alignment. Finally, an on-device personalization phase adapts the
converged global model into a specialized expert for each IOT Device.
Comprehensive experiments demonstrate that Fed-Meta-Align achieves an average
test accuracy of 91.27% across heterogeneous IOT devices, outperforming
personalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical and
mechanical fault datasets, respectively. This multi-stage approach of sequenced
initialization and adaptive aggregation provides a robust pathway for deploying
high-performance intelligence on diverse TinyML networks.

</details>


### [326] [Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes](https://arxiv.org/abs/2508.11800)
*Michael Bereket,Jure Leskovec*

Main category: cs.LG

TL;DR: The paper evaluates the effectiveness of reinforcement learning (RL) methods in optimizing language models for stochastic domains like scientific experiments, revealing issues with GRPO's overconfidence and recommending improvements.


<details>
  <summary>Details</summary>
Motivation: To determine if current RL methods are effective in optimizing language models for stochastic, verifiable domains like scientific experiments.

Method: Testing RL methods—GRPO, PPO, and RLOO—on synthetic and real-world biological experiment data, analyzing their calibration quality, and investigating the effect of group standard normalization.

Result: GRPO leads to overconfident predictions when group standard normalization is used, while PPO and RLOO generate well-calibrated models. Removing normalization in GRPO resolves the issues.

Conclusion: The paper identifies weaknesses in standard normalization for GRPO and suggests modifications to improve its calibration, helping to expand RL applications beyond deterministic domains.

Abstract: Reinforcement learning (RL) has proven remarkably effective at improving the
accuracy of language models in verifiable and deterministic domains like
mathematics. Here, we examine if current RL methods are also effective at
optimizing language models in verifiable domains with stochastic outcomes, like
scientific experiments. Through applications to synthetic data and real-world
biological experiments, we demonstrate that Group Relative Policy Optimization
(GRPO) induces overconfident probability predictions for binary stochastic
outcomes, while Proximal Policy Optimization (PPO) and REINFORCE Leave-One-Out
(RLOO) yield well-calibrated models. We show that removing group standard
normalization in GRPO fixes its miscalibration and provide a theoretical
explanation for why normalization causes overconfidence. Our results provide
new evidence against the use of standard normalization in GRPO and help pave
the way for applications of RL for reasoning language models beyond
deterministic domains.

</details>


### [327] [Defining and Benchmarking a Data-Centric Design Space for Brain Graph Construction](https://arxiv.org/abs/2508.12533)
*Qinwen Ge,Roza G. Bayrak,Anwar Said,Catie Chang,Xenofon Koutsoukos,Tyler Derr*

Main category: cs.LG

TL;DR: This paper defines and evaluates a data-centric approach to constructing brain graphs from fMRI data, systematically exploring the impact of various design choices on downstream performance.


<details>
  <summary>Details</summary>
Motivation: Current brain graph construction practices rely on rigid, model-centric pipelines that fail to consider critical data-centric factors, limiting the potential for better neuroimaging results.

Method: The authors benchmarked a data-centric design space organized into three stages—temporal signal processing, topology extraction, and graph featurization. They evaluated combinations of existing and modified techniques through experiments on HCP1200 and ABIDE datasets.

Result: Experiments showed that thoughtful data-centric configurations significantly improved classification accuracy compared to standard methods.

Conclusion: The study emphasizes the importance of data-centric decisions in brain graph construction and provides a systematic framework for optimizing upstream data processes in neuroimaging.

Abstract: The construction of brain graphs from functional Magnetic Resonance Imaging
(fMRI) data plays a crucial role in enabling graph machine learning for
neuroimaging. However, current practices often rely on rigid pipelines that
overlook critical data-centric choices in how brain graphs are constructed. In
this work, we adopt a Data-Centric AI perspective and systematically define and
benchmark a data-centric design space for brain graph construction,
constrasting with primarily model-centric prior work. We organize this design
space into three stages: temporal signal processing, topology extraction, and
graph featurization. Our contributions lie less in novel components and more in
evaluating how combinations of existing and modified techniques influence
downstream performance. Specifically, we study high-amplitude BOLD signal
filtering, sparsification and unification strategies for connectivity,
alternative correlation metrics, and multi-view node and edge features, such as
incorporating lagged dynamics. Experiments on the HCP1200 and ABIDE datasets
show that thoughtful data-centric configurations consistently improve
classification accuracy over standard pipelines. These findings highlight the
critical role of upstream data decisions and underscore the importance of
systematically exploring the data-centric design space for graph-based
neuroimaging. Our code is available at
https://github.com/GeQinwen/DataCentricBrainGraphs.

</details>


### [328] [FairTabGen: Unifying Counterfactual and Causal Fairness in Synthetic Tabular Data Generation](https://arxiv.org/abs/2508.11810)
*Nitish Nagesh,Salar Shakibhamedan,Mahdi Bagheri,Ziyu Wang,Nima TaheriNejad,Axel Jantsch,Amir M. Rahmani*

Main category: cs.LG

TL;DR: The paper introduces FairTabGen, a fairness-aware framework using large language models for generating synthetic tabular data with improved fairness and utility.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of generating synthetic tabular data that ensures fairness, especially counterfactual and causal fairness, while retaining high utility in privacy-sensitive and data-scarce contexts.

Method: The method involves using a large language model-based framework with in-context learning, prompt refinement, and fairness-aware data curation integrated into generation and evaluation pipelines. Multiple fairness definitions are employed.

Result: FairTabGen outperforms GAN-based and other LLM-based methods on diverse datasets, achieving up to 10% improvement in fairness metrics like demographic parity and causal effects, and performs well using less than 20% of original data.

Conclusion: FairTabGen is a principled, effective approach for generating fair and statistically useful synthetic tabular data, especially in data-limited scenarios.

Abstract: Generating synthetic data is crucial in privacy-sensitive, data-scarce
settings, especially for tabular datasets widely used in real-world
applications. A key challenge is improving counterfactual and causal fairness,
while preserving high utility. We present FairTabGen, a fairness-aware large
language model-based framework for tabular synthetic data generation. We
integrate multiple fairness definitions including counterfactual and causal
fairness into both its generation and evaluation pipelines. We use in-context
learning, prompt refinement, and fairness-aware data curation to balance
fairness and utility. Across diverse datasets, our method outperforms
state-of-the-art GAN-based and LLM-based methods, achieving up to 10%
improvements on fairness metrics such as demographic parity and path-specific
causal effects while retaining statistical utility. Remarkably, it achieves
these gains using less than 20% of the original data, highlighting its
efficiency in low-data regimes. These results demonstrate a principled and
practical approach for generating fair and useful synthetic tabular data.

</details>


### [329] [Combinations of Fast Activation and Trigonometric Functions in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.11876)
*Hoang-Thang Ta,Duy-Quy Thai,Phuong-Linh Tran-Thi*

Main category: cs.LG

TL;DR: This paper proposes enhancing Kolmogorov-Arnold Networks (KANs) by integrating fast computational functions like ReLU and trigonometric functions, improving efficiency while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold Networks (KANs) have gained attention but rely on polynomial functions like B-splines and RBFs, which are less efficient and GPU-unsupported, creating a need for better alternatives.

Method: The authors integrate fast computational functions such as ReLU and trigonometric functions (sin, cos, arctan) into KANs as basis components to improve network structure and computational efficiency.

Result: Experimental results demonstrated that these computational function combinations enhance training speed and generalization without compromising performance.

Conclusion: The use of fast and GPU-supported computational functions in KANs shows promise for improving efficiency while retaining competitive performance, suggesting practical advancements in neural network design.

Abstract: For years, many neural networks have been developed based on the
Kolmogorov-Arnold Representation Theorem (KART), which was created to address
Hilbert's 13th problem. Recently, relying on KART, Kolmogorov-Arnold Networks
(KANs) have attracted attention from the research community, stimulating the
use of polynomial functions such as B-splines and RBFs. However, these
functions are not fully supported by GPU devices and are still considered less
popular. In this paper, we propose the use of fast computational functions,
such as ReLU and trigonometric functions (e.g., ReLU, sin, cos, arctan), as
basis components in Kolmogorov-Arnold Networks (KANs). By integrating these
function combinations into the network structure, we aim to enhance
computational efficiency. Experimental results show that these combinations
maintain competitive performance while offering potential improvements in
training time and generalization.

</details>


### [330] [PCA- and SVM-Grad-CAM for Convolutional Neural Networks: Closed-form Jacobian Expression](https://arxiv.org/abs/2508.11880)
*Yuto Omae*

Main category: cs.LG

TL;DR: The paper proposes methods ('PCA-Grad-CAM' and 'SVM-Grad-CAM') for visualizing attention regions in PCA and SVM layers integrated within Convolutional Neural Networks (CNNs).


<details>
  <summary>Details</summary>
Motivation: CNNs are excellent for classification but considered black-box methods. Visualization techniques like Grad-CAM make them interpretable, yet Grad-CAM cannot handle PCA or SVM layers, necessitating new methods to enable visualization for these components.

Method: The study develops PCA-Grad-CAM and SVM-Grad-CAM to visualize attention regions in PCA and SVM layers by solving the closed-form Jacobian with partial derivatives from the last convolutional layer.

Result: Closed-form Jacobian calculations were derived, enabling visualization techniques for PCA and SVM layers. Application tests on major datasets confirm the methods’ effectiveness for generating attention regions.

Conclusion: PCA-Grad-CAM and SVM-Grad-CAM enhance the interpretability of CNNs in low-sample settings by enabling visualization of PCA feature vectors and SVM layers, bridging the gap for white-box AI approaches in these scenarios.

Abstract: Convolutional Neural Networks (CNNs) are an effective approach for
classification tasks, particularly when the training dataset is large. Although
CNNs have long been considered a black-box classification method, they can be
used as a white-box method through visualization techniques such as Grad-CAM.
When training samples are limited, incorporating a Principal Component Analysis
(PCA) layer and/or a Support Vector Machine (SVM) classifier into a CNN can
effectively improve classification performance. However, traditional Grad-CAM
cannot be directly applied to PCA and/or SVM layers. It is important to
generate attention regions for PCA and/or SVM layers in CNNs to facilitate the
development of white-box methods. Therefore, we propose ``PCA-Grad-CAM'', a
method for visualizing attention regions in PCA feature vectors, and
``SVM-Grad-CAM'', a method for visualizing attention regions in an SVM
classifier layer. To complete our methods analytically, it is necessary to
solve the closed-form Jacobian consisting of partial derivatives from the last
convolutional layer to the PCA and/or SVM layers. In this paper, we present the
exact closed-form Jacobian and the visualization results of our methods applied
to several major datasets.

</details>


### [331] [Universal Learning of Nonlinear Dynamics](https://arxiv.org/abs/2508.11990)
*Evan Dogariu,Anand Brahmbhatt,Elad Hazan*

Main category: cs.LG

TL;DR: The paper presents an algorithm utilizing spectral filtering to learn nonlinear dynamical systems, proving vanishing prediction errors with a novel quantitative learnability concept.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of determining a marginally stable unknown nonlinear dynamical system.

Method: Develop a spectral filtering-based algorithm combined with online convex optimization techniques.

Result: Prove the algorithm achieves vanishing prediction errors governed by a learnability criterion, and introduce an advanced spectral filtering method for noisy and marginally stable systems.

Conclusion: The proposed approach generalizes existing methods, offering broader applicability and improved error correction in dynamical systems learning.

Abstract: We study the fundamental problem of learning a marginally stable unknown
nonlinear dynamical system. We describe an algorithm for this problem, based on
the technique of spectral filtering, which learns a mapping from past
observations to the next based on a spectral representation of the system.
Using techniques from online convex optimization, we prove vanishing prediction
error for any nonlinear dynamical system that has finitely many marginally
stable modes, with rates governed by a novel quantitative control-theoretic
notion of learnability. The main technical component of our method is a new
spectral filtering algorithm for linear dynamical systems, which incorporates
past observations and applies to general noisy and marginally stable systems.
This significantly generalizes the original spectral filtering algorithm to
both asymmetric dynamics as well as incorporating noise correction, and is of
independent interest.

</details>


### [332] [ENA: Efficient N-dimensional Attention](https://arxiv.org/abs/2508.11921)
*Yibo Zhong*

Main category: cs.LG

TL;DR: The paper presents Efficient N-dimensional Attention (ENA), a hybrid model combining linear recurrence and sliding window attention for modeling ultra-long, high-order data efficiently.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the inefficiency of Transformers when modeling long sequences of high-order data across dimensions (1D to ND).

Method: They explore linear recurrent models extended with scanning strategies and attention-hybrid architectures, ultimately proposing ENA, which combines linear recurrence for global compression with sliding window attention for localized modeling.

Result: Experimental results support the efficiency and effectiveness of ENA, with tiled sliding window attention showing theoretical and practical advantages.

Conclusion: ENA offers a simple yet effective framework for ultra-long, high-order data modeling, balancing global and local information efficiently.

Abstract: Efficient modeling of long sequences of high-order data requires a more
efficient architecture than Transformer. In this paper, we investigate two key
aspects of extending linear recurrent models, especially those originally
designed for language modeling, to high-order data (1D to ND): scanning
strategies and attention-hybrid architectures. Empirical results suggest that
scanning provides limited benefits, while attention-hybrid models yield
promising results. Focusing on the latter, we further evaluate types of
attention and find that tiled high-order sliding window attention (SWA) is
efficient in both theory and practice. We term the resulting hybrid
architecture of linear recurrence and high-order SWA as Efficient N-dimensional
Attention (ENA). We then conduct several experiments to demonstrate its
effectiveness. The intuition behind ENA is that linear recurrence compresses
global information into a state, while SWA complements it by enforcing strict
local modeling. Together, they form a simple framework that offers a promising
and practical solution for ultra-long high-order data modeling.

</details>


### [333] [Scale-Disentangled spatiotemporal Modeling for Long-term Traffic Emission Forecasting](https://arxiv.org/abs/2508.11923)
*Yan Wu,Lihong Pei,Yukai Han,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: This paper introduces a Scale-Disentangled Spatio-Temporal Modeling (SDSTM) framework to address cascading errors in long-term traffic emission forecasting by decomposing and fusing multi-scale features.


<details>
  <summary>Details</summary>
Motivation: Traditional spatiotemporal graph models for traffic emission forecasting suffer from error amplification during long-term predictions due to multi-scale entanglement.

Method: The paper introduces SDSTM, which uses a dual-stream feature decomposition strategy based on the Koopman operator and gated wavelet decomposition, alongside a fusion mechanism with a dual-stream independence constraint via cross-term loss.

Result: Experiments on Xi'an's Second Ring Road traffic emission dataset show that SDSTM achieves state-of-the-art forecasting accuracy.

Conclusion: The proposed SDSTM framework effectively improves long-term traffic emission forecasting by addressing scale entanglement and suppressing interferences between predictions.

Abstract: Long-term traffic emission forecasting is crucial for the comprehensive
management of urban air pollution. Traditional forecasting methods typically
construct spatiotemporal graph models by mining spatiotemporal dependencies to
predict emissions. However, due to the multi-scale entanglement of traffic
emissions across time and space, these spatiotemporal graph modeling method
tend to suffer from cascading error amplification during long-term inference.
To address this issue, we propose a Scale-Disentangled Spatio-Temporal Modeling
(SDSTM) framework for long-term traffic emission forecasting. It leverages the
predictability differences across multiple scales to decompose and fuse
features at different scales, while constraining them to remain independent yet
complementary. Specifically, the model first introduces a dual-stream feature
decomposition strategy based on the Koopman lifting operator. It lifts the
scale-coupled spatiotemporal dynamical system into an infinite-dimensional
linear space via Koopman operator, and delineates the predictability boundary
using gated wavelet decomposition. Then a novel fusion mechanism is
constructed, incorporating a dual-stream independence constraint based on
cross-term loss to dynamically refine the dual-stream prediction results,
suppress mutual interference, and enhance the accuracy of long-term traffic
emission prediction. Extensive experiments conducted on a road-level traffic
emission dataset within Xi'an's Second Ring Road demonstrate that the proposed
model achieves state-of-the-art performance.

</details>


### [334] [An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction](https://arxiv.org/abs/2508.11931)
*Tim van Erven,Jack Mayo,Julia Olkhovskaya,Chen-Yu Wei*

Main category: cs.LG

TL;DR: The paper introduces a new algorithm for linear contextual bandits dealing with adversarial losses and stochastic action sets, achieving efficient regret bounds and polynomial runtime.


<details>
  <summary>Details</summary>
Motivation: To address the open question of achieving low-regret polynomial-time algorithms for combinatorial bandits with adversarial losses and stochastic action sets.

Method: Maps the problem to misspecification-robust adversarial linear bandits with fixed action sets, without requiring a context simulator and achieving $	ilde{O}$-regret bounds.

Result: Achieves $	ilde{O}(	ext{min}\{d^2\sqrt{T}, \sqrt{d^3T\log K}\})$ regret and polynomial runtime while resolving prior limitations in combinatorial bandit setups.

Conclusion: The proposed algorithm establishes a significant advancement by offering efficient and robust solutions for adversarial and stochastic contexts in bandit problems, achieving low regret and polynomial runtime without reliance on a simulator.

Abstract: We present an efficient algorithm for linear contextual bandits with
adversarial losses and stochastic action sets. Our approach reduces this
setting to misspecification-robust adversarial linear bandits with fixed action
sets. Without knowledge of the context distribution or access to a context
simulator, the algorithm achieves $\tilde{O}(\min\{d^2\sqrt{T}, \sqrt{d^3T\log
K}\})$ regret and runs in $\text{poly}(d,C,T)$ time, where $d$ is the feature
dimension, $C$ is an upper bound on the number of linear constraints defining
the action set in each round, $K$ is an upper bound on the number of actions in
each round, and $T$ is number of rounds. This resolves the open question by Liu
et al. (2023) on whether one can obtain $\text{poly}(d)\sqrt{T}$ regret in
polynomial time independent of the number of actions. For the important class
of combinatorial bandits with adversarial losses and stochastic action sets
where the action sets can be described by a polynomial number of linear
constraints, our algorithm is the first to achieve $\text{poly}(d)\sqrt{T}$
regret in polynomial time, while no prior algorithm achieves even $o(T)$ regret
in polynomial time to our knowledge. When a simulator is available, the regret
bound can be improved to $\tilde{O}(d\sqrt{L^\star})$, where $L^\star$ is the
cumulative loss of the best policy.

</details>


### [335] [M3OOD: Automatic Selection of Multimodal OOD Detectors](https://arxiv.org/abs/2508.11936)
*Yuehan Qin,Li Li,Defu Cao,Tiankai Yang,Yue Zhao*

Main category: cs.LG

TL;DR: The paper presents M3OOD, a meta-learning framework for selecting optimal out-of-distribution (OOD) detection models in multimodal machine learning systems.


<details>
  <summary>Details</summary>
Motivation: Current OOD detection methods struggle to handle diverse distribution shifts in multimodal settings, necessitating an adaptable approach to select effective models for unseen scenarios.

Method: The proposed framework uses meta-learning by incorporating multimodal embeddings and handcrafted meta-features derived from historical model performance across benchmarks, enabling rapid adaptation to diverse distribution shifts.

Result: Experiments show that M3OOD consistently performs better than 10 baselines across 12 test scenarios, demonstrating effective detector recommendation with minimal computational cost.

Conclusion: M3OOD provides an innovative solution for OOD robustness in multimodal systems by leveraging meta-learning, offering a scalable and efficient method for model selection in diverse scenarios.

Abstract: Out-of-distribution (OOD) robustness is a critical challenge for modern
machine learning systems, particularly as they increasingly operate in
multimodal settings involving inputs like video, audio, and sensor data.
Currently, many OOD detection methods have been proposed, each with different
designs targeting various distribution shifts. A single OOD detector may not
prevail across all the scenarios; therefore, how can we automatically select an
ideal OOD detection model for different distribution shifts? Due to the
inherent unsupervised nature of the OOD detection task, it is difficult to
predict model performance and find a universally Best model. Also,
systematically comparing models on the new unseen data is costly or even
impractical. To address this challenge, we introduce M3OOD, a
meta-learning-based framework for OOD detector selection in multimodal
settings. Meta learning offers a solution by learning from historical model
behaviors, enabling rapid adaptation to new data distribution shifts with
minimal supervision. Our approach combines multimodal embeddings with
handcrafted meta-features that capture distributional and cross-modal
characteristics to represent datasets. By leveraging historical performance
across diverse multimodal benchmarks, M3OOD can recommend suitable detectors
for a new data distribution shift. Experimental evaluation demonstrates that
M3OOD consistently outperforms 10 competitive baselines across 12 test
scenarios with minimal computational overhead.

</details>


### [336] [Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs](https://arxiv.org/abs/2508.12530)
*Hyunsoo Song,Seungwhan Kim,Seungkyu Lee*

Main category: cs.LG

TL;DR: The paper addresses posterior collapse in VAEs by introducing a Latent Reconstruction (LR) loss to control it without structural constraints on the network architecture.


<details>
  <summary>Details</summary>
Motivation: Posterior collapse in VAEs reduces the diversity of generated samples, and existing methods to avoid it often require network architecture constraints or overly focus on trade-offs that hinder optimal reconstruction.

Method: The authors define 'local posterior collapse' to emphasize individual sample importance and propose a Latent Reconstruction (LR) loss inspired by properties of injective and composite functions to manage posterior collapse independently of specific architectures.

Result: The proposed approach is empirically evaluated on diverse datasets like MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ, demonstrating its effectiveness in controlling posterior collapse.

Conclusion: Latent Reconstruction loss offers a flexible and architecture-agnostic method to address posterior collapse in VAEs, improving generative diversity without imposing restrictive constraints.

Abstract: Variational autoencoders (VAEs), one of the most widely used generative
models, are known to suffer from posterior collapse, a phenomenon that reduces
the diversity of generated samples. To avoid posterior collapse, many prior
works have tried to control the influence of regularization loss. However, the
trade-off between reconstruction and regularization is not satisfactory. For
this reason, several methods have been proposed to guarantee latent
identifiability, which is the key to avoiding posterior collapse. However, they
require structural constraints on the network architecture. For further
clarification, we define local posterior collapse to reflect the importance of
individual sample points in the data space and to relax the network constraint.
Then, we propose Latent Reconstruction(LR) loss, which is inspired by
mathematical properties of injective and composite functions, to control
posterior collapse without restriction to a specific architecture. We
experimentally evaluate our approach, which controls posterior collapse on
varied datasets such as MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ.

</details>


### [337] [Learning Marked Temporal Point Process Explanations based on Counterfactual and Factual Reasoning](https://arxiv.org/abs/2508.11943)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yan Wang*

Main category: cs.LG

TL;DR: The paper introduces a model, Counterfactual and Factual Explainer (CFF), designed to identify minimal and rational explanations for event predictions in Marked Temporal Point Process (MTPP) models.


<details>
  <summary>Details</summary>
Motivation: High-stakes applications of MTPP models require trustworthy predictions, emphasizing the need for interpretable explanations for their outputs.

Method: The study redefines explanations in MTPP as a hybrid of factual and counterfactual explanations, avoiding the irrationality of treating these components separately. It introduces the CFF model with techniques tailored to improve the interpretability of MTPP predictions.

Result: Experiments validate the CFF model's capability to deliver higher-quality and more efficient explanations compared to existing methods.

Conclusion: The proposed CFF approach offers a reliable and efficient framework for generating rational explanations for MTPP, promoting greater trust in their predictions.

Abstract: Neural network-based Marked Temporal Point Process (MTPP) models have been
widely adopted to model event sequences in high-stakes applications, raising
concerns about the trustworthiness of outputs from these models. This study
focuses on Explanation for MTPP, aiming to identify the minimal and rational
explanation, that is, the minimum subset of events in history, based on which
the prediction accuracy of MTPP matches that based on full history to a great
extent and better than that based on the complement of the subset. This study
finds that directly defining Explanation for MTPP as counterfactual explanation
or factual explanation can result in irrational explanations. To address this
issue, we define Explanation for MTPP as a combination of counterfactual
explanation and factual explanation. This study proposes Counterfactual and
Factual Explainer for MTPP (CFF) to solve Explanation for MTPP with a series of
deliberately designed techniques. Experiments demonstrate the correctness and
superiority of CFF over baselines regarding explanation quality and processing
efficiency.

</details>


### [338] [Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems](https://arxiv.org/abs/2508.12569)
*Quercus Hernandez,Max Win,Thomas C. O'Connor,Paulo E. Arratia,Nathaniel Trask*

Main category: cs.LG

TL;DR: This paper introduces a framework using the metriplectic bracket formalism and self-supervised learning to machine-learn coarse-grained dynamics while preserving thermodynamic laws and non-equilibrium statistics.


<details>
  <summary>Details</summary>
Motivation: There is a need to accurately simulate multiscale systems where short spatiotemporal scales link to emergent bulk physics, especially when coarse-graining leads to challenges like entropic loss and emergent stochastic behavior.

Method: The paper employs the metriplectic bracket formalism to ensure coarse-grained models adhere to thermodynamic laws. Additionally, it introduces a novel self-supervised learning strategy to identify emergent structural variables from time-series data without labeled datasets.

Result: The framework was tested on benchmark systems and two complex scenarios: coarse-graining star polymers while retaining non-equilibrium statistics and learning models from high-speed video of colloidal suspensions to capture local dynamics.

Conclusion: The proposed framework successfully preserves properties like fluctuation-dissipation balance and thermodynamic laws in coarse-grained simulations, and offers open-source implementations for practical applications in particle-based systems.

Abstract: Multiscale systems are ubiquitous in science and technology, but are
notoriously challenging to simulate as short spatiotemporal scales must be
appropriately linked to emergent bulk physics. When expensive high-dimensional
dynamical systems are coarse-grained into low-dimensional models, the entropic
loss of information leads to emergent physics which are dissipative,
history-dependent, and stochastic. To machine learn coarse-grained dynamics
from time-series observations of particle trajectories, we propose a framework
using the metriplectic bracket formalism that preserves these properties by
construction; most notably, the framework guarantees discrete notions of the
first and second laws of thermodynamics, conservation of momentum, and a
discrete fluctuation-dissipation balance crucial for capturing non-equilibrium
statistics. We introduce the mathematical framework abstractly before
specializing to a particle discretization. As labels are generally unavailable
for entropic state variables, we introduce a novel self-supervised learning
strategy to identify emergent structural variables. We validate the method on
benchmark systems and demonstrate its utility on two challenging examples: (1)
coarse-graining star polymers at challenging levels of coarse-graining while
preserving non-equilibrium statistics, and (2) learning models from high-speed
video of colloidal suspensions that capture coupling between local
rearrangement events and emergent stochastic dynamics. We provide open-source
implementations in both PyTorch and LAMMPS, enabling large-scale inference and
extensibility to diverse particle-based systems.

</details>


### [339] [OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning](https://arxiv.org/abs/2508.12551)
*Hongyu Lin,Yuchen Li,Haoran Luo,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: OS-R1 uses rule-based RL and LLMs for efficient Linux kernel tuning, offering performance improvements and practical adaptability.


<details>
  <summary>Details</summary>
Motivation: Improve efficiency, scalability, and generalization in Linux kernel tuning methods.

Method: Abstract kernel configurations as an RL environment combined with rule-based RL and LLM-guided modifications. Employ custom reward functions and a two-phase training process.

Result: OS-R1 achieves up to 5.6% performance improvement over heuristic tuning, shows high data efficiency, and adapts across applications.

Conclusion: OS-R1 is effective, scalable, versatile, and supports real-world deployment, with publicly available resources.

Abstract: Linux kernel tuning is essential for optimizing operating system (OS)
performance. However, existing methods often face challenges in terms of
efficiency, scalability, and generalization. This paper introduces OS-R1, an
agentic Linux kernel tuning framework powered by rule-based reinforcement
learning (RL). By abstracting the kernel configuration space as an RL
environment, OS-R1 facilitates efficient exploration by large language models
(LLMs) and ensures accurate configuration modifications. Additionally, custom
reward functions are designed to enhance reasoning standardization,
configuration modification accuracy, and system performance awareness of the
LLMs. Furthermore, we propose a two-phase training process that accelerates
convergence and minimizes retraining across diverse tuning scenarios.
Experimental results show that OS-R1 significantly outperforms existing
baseline methods, achieving up to 5.6% performance improvement over heuristic
tuning and maintaining high data efficiency. Notably, OS-R1 is adaptable across
various real-world applications, demonstrating its potential for practical
deployment in diverse environments. Our dataset and code are publicly available
at https://github.com/LHY-24/OS-R1.

</details>


### [340] [Set-Valued Transformer Network for High-Emission Mobile Source Identification](https://arxiv.org/abs/2508.11976)
*Yunning Cao,Lihong Pei,Jian Guo,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: This paper introduces the Set-Valued Transformer Network (SVTN) to improve the detection accuracy of high-emission vehicles from real-world data.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of identifying high-emission vehicles due to long-tailed data distribution, emission state nonlinearity, and lack of prior knowledge.

Method: The SVTN uses transformers to map high-dimensional emission data into a low-dimensional space and a set-valued algorithm for probabilistic modeling and classification.

Result: The proposed method reduced the missed detection rate for high-emission vehicles by 9.5% compared to a transformer-based baseline in experiments on Hefei’s 2020 diesel vehicle data.

Conclusion: SVTN effectively enhances the detection of high-emission vehicles, addressing real-world challenges and improving model accuracy significantly.

Abstract: Identifying high-emission vehicles is a crucial step in regulating urban
pollution levels and formulating traffic emission reduction strategies.
However, in practical monitoring data, the proportion of high-emission state
data is significantly lower compared to normal emission states. This
characteristic long-tailed distribution severely impedes the extraction of
discriminative features for emission state identification during data mining.
Furthermore, the highly nonlinear nature of vehicle emission states and the
lack of relevant prior knowledge also pose significant challenges to the
construction of identification models.To address the aforementioned issues, we
propose a Set-Valued Transformer Network (SVTN) to achieve comprehensive
learning of discriminative features from high-emission samples, thereby
enhancing detection accuracy. Specifically, this model first employs the
transformer to measure the temporal similarity of micro-trip condition
variations, thus constructing a mapping rule that projects the original
high-dimensional emission data into a low-dimensional feature space. Next, a
set-valued identification algorithm is used to probabilistically model the
relationship between the generated feature vectors and their labels, providing
an accurate metric criterion for the classification algorithm. To validate the
effectiveness of our proposed approach, we conducted extensive experiments on
the diesel vehicle monitoring data of Hefei city in 2020. The results
demonstrate that our method achieves a 9.5\% reduction in the missed detection
rate for high-emission vehicles compared to the transformer-based baseline,
highlighting its superior capability in accurately identifying high-emission
mobile pollution sources.

</details>


### [341] [Constrained Centroid Clustering: A Novel Approach for Compact and Structured Partitioning](https://arxiv.org/abs/2508.12758)
*Sowmini Devi Veeramachaneni,Ramamurthy Garimella*

Main category: cs.LG

TL;DR: The paper introduces Constrained Centroid Clustering (CCC), a method for clustering with constraints on cluster spread, which outperforms standard methods like K-means and GMM in terms of compactness and structure preservation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance centroid-based clustering techniques by controlling the cluster spread for applications in structured and interpretable clustering, such as sensor networks and robotics.

Method: CCC employs a Lagrangian formulation to introduce a constraint on the maximum distance between the cluster center and the farthest point, providing a closed-form solution for spread control.

Result: Experiments on synthetic data demonstrate that CCC reduces radial spread and maintains angular structure, achieving compact clusters and outperforming K-means and GMM in terms of the proposed evaluation metrics.

Conclusion: CCC is effective for structured clustering with controlled spread, making it useful for applications requiring interpretable and compact clustering solutions.

Abstract: This paper presents Constrained Centroid Clustering (CCC), a method that
extends classical centroid-based clustering by enforcing a constraint on the
maximum distance between the cluster center and the farthest point in the
cluster. Using a Lagrangian formulation, we derive a closed-form solution that
maintains interpretability while controlling cluster spread. To evaluate CCC,
we conduct experiments on synthetic circular data with radial symmetry and
uniform angular distribution. Using ring-wise, sector-wise, and joint entropy
as evaluation metrics, we show that CCC achieves more compact clusters by
reducing radial spread while preserving angular structure, outperforming
standard methods such as K-means and GMM. The proposed approach is suitable for
applications requiring structured clustering with spread control, including
sensor networks, collaborative robotics, and interpretable pattern analysis.

</details>


### [342] [Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models](https://arxiv.org/abs/2508.11985)
*Zhanhao Cao,Clement Truong,Andrew Lizarraga*

Main category: cs.LG

TL;DR: The paper explores the combination of independently trained LoRA modules using naive summation, finding it can perform similarly to merged-data fine-tuned models under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To investigate whether independently trained LoRA modules can be combined without fine-tuning, inspired by the superposition principle.

Method: The authors train LoRA adapters for different QA domains and combine them using naive summation. They measure performance using perplexity and cosine similarity between adapter deltas.

Result: The approach performed comparably to merged-data models in some cases, with changes in perplexity correlated to cosine similarity of adapter deltas.

Conclusion: Naive summation of LoRA modules can provide efficient model composition, revealing insights about interference, without requiring extra training.

Abstract: Recent advances in large language models are driven by scale, while
parameter-efficient fine-tuning (PEFT) enables updating only a small fraction
of parameters. Low-Rank Adaptation (LoRA) stores parameter deltas as the
product of two small matrices, which makes them natural building blocks that
can be composed. Motivated by the superposition principle, we hypothesize that
independently trained LoRA modules on disjoint domains are approximately
orthogonal and can be combined by simple addition. Using GPT-2 Small (117M)
with LoRA rank 4 and alpha=64, we train adapters for three QA domains (math,
medicine, finance). In pairwise tests, adding Math+Medicine adapters improves
perplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance
and Finance+Medicine change by +4.54% and +27.56%, respectively. Across
combinations, the RMS cosine similarity between LoRA deltas correlates
positively and approximately linearly with the change in perplexity. Naive
summation requires no additional training, can be applied in seconds, and
achieves performance comparable to models trained on merged data, while
clarifying when interference appears in higher-order compositions.

</details>


### [343] [Randomized PCA Forest for Outlier Detection](https://arxiv.org/abs/2508.12776)
*Muhammad Rajabinasab,Farhad Pakdaman,Moncef Gabbouj,Peter Schneider-Kamp,Arthur Zimek*

Main category: cs.LG

TL;DR: The paper proposes an unsupervised outlier detection method using Randomized PCA Forest, showing superior or competitive performance in comparison to existing methods while being efficient and generalizable.


<details>
  <summary>Details</summary>
Motivation: The need for more efficient and generalizable outlier detection approaches inspired the authors to leverage the proven efficiency of Randomized PCA Forest in approximate KNN search for developing a novel method.

Method: The method utilizes Randomized PCA Forest for unsupervised outlier detection and evaluates its performance across multiple datasets.

Result: The proposed technique outperforms classical and state-of-the-art methods in some cases, and performs competitively in others, showing high generalization power and computational efficiency.

Conclusion: The method is highlighted as a strong choice for unsupervised outlier detection due to its effectiveness, efficiency, and generalizability across datasets.

Abstract: We propose a novel unsupervised outlier detection method based on Randomized
Principal Component Analysis (PCA). Inspired by the performance of Randomized
PCA (RPCA) Forest in approximate K-Nearest Neighbor (KNN) search, we develop a
novel unsupervised outlier detection method that utilizes RPCA Forest for
outlier detection. Experimental results showcase the superiority of the
proposed approach compared to the classical and state-of-the-art methods in
performing the outlier detection task on several datasets while performing
competitively on the rest. The extensive analysis of the proposed method
reflects it high generalization power and its computational efficiency,
highlighting it as a good choice for unsupervised outlier detection.

</details>


### [344] [Bridging Human and LLM Judgments: Understanding and Narrowing the Gap](https://arxiv.org/abs/2508.12792)
*Felipe Maia Polo,Xinhe Wang,Mikhail Yurochkin,Gongjun Xu,Moulinath Banerjee,Yuekai Sun*

Main category: cs.LG

TL;DR: The paper introduces Bridge, a statistical framework to align human and LLM assessments by modeling discrepancies and evaluating latent human preference scores.


<details>
  <summary>Details</summary>
Motivation: Large language models used for evaluations often diverge systematically from human judgments, prompting the need for a unified framework to align evaluations.

Method: Bridge models discrepancies via latent human preference scores and linear transformations of covariates, providing efficient algorithms with statistical guarantees for inference.

Result: Bridge improves agreement with human ratings across benchmarks and identifies systematic gaps between human and LLM judgments.

Conclusion: A principled framework like Bridge can refine LLM ratings and enhance understanding of discrepancies between human and LLM evaluations.

Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to
evaluate model outputs at scale, but their assessments often diverge
systematically from human judgments. We present Bridge, a unified statistical
framework that explicitly bridges human and LLM evaluations under both absolute
scoring and pairwise comparison paradigms. Bridge posits a latent human
preference score for each prompt-response pair and models LLM deviations as
linear transformations of covariates that capture sources of discrepancies.
This offers a simple and principled framework for refining LLM ratings and
characterizing systematic discrepancies between humans and LLMs. We provide an
efficient fitting algorithm with asymptotic guarantees for statistical
inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot
Arena), Bridge achieves higher agreement with human ratings (accuracy,
calibration, and KL divergence) and exposes systematic human-LLM gaps.

</details>


### [345] [Fairness Regularization in Federated Learning](https://arxiv.org/abs/2508.12042)
*Zahra Kharaghani,Ali Dadras,Tommy Löfstedt*

Main category: cs.LG

TL;DR: This study focuses on fairness in Federated Learning (FL), where client data heterogeneity causes disparities in performance. It examines and proposes methods to improve performance equitable fairness, showing that FairGrad approaches enhance fairness and model performance.


<details>
  <summary>Details</summary>
Motivation: Federated Learning enables collaborative training across decentralized data to address privacy concerns and access diverse datasets without centralized storage. However, data heterogeneity introduces fairness issues across clients, which require effective solutions.

Method: The study evaluates fairness-aware methods that explicitly regularize client losses, including novel methods FairGrad (approximate) and FairGrad* (exact), which utilize gradient variance regularization for performance equitable fairness.

Result: Theoretical and empirical findings demonstrate connections between different fairness methods. FairGrad and FairGrad* improve fairness and overall model performance in heterogeneous data settings.

Conclusion: Performance equitable fairness can be enhanced through specific regularization approaches like FairGrad and FairGrad*, addressing disparities in federated learning models caused by data heterogeneity.

Abstract: Federated Learning (FL) has emerged as a vital paradigm in modern machine
learning that enables collaborative training across decentralized data sources
without exchanging raw data. This approach not only addresses privacy concerns
but also allows access to overall substantially larger and potentially more
diverse datasets, without the need for centralized storage or hardware
resources. However, heterogeneity in client data may cause certain clients to
have disproportionate impacts on the global model, leading to disparities in
the clients' performances. Fairness, therefore, becomes a crucial concern in FL
and can be addressed in various ways. However, the effectiveness of existing
fairness-aware methods, particularly in heterogeneous data settings, remains
unclear, and the relationships between different approaches are not well
understood. In this work, we focus on performance equitable fairness, which
aims to minimize differences in performance across clients. We restrict our
study to fairness-aware methods that explicitly regularize client losses,
evaluating both existing and newly proposed approaches. We identify and
theoretically explain connections between the investigated fairness methods,
and empirically show that FairGrad (approximate) and FairGrad* (exact) (two
variants of a gradient variance regularization method introduced here for
performance equitable fairness) improve both fairness and overall model
performance in heterogeneous data settings.

</details>


### [346] [Fairness-Aware Multi-view Evidential Learning with Adaptive Prior](https://arxiv.org/abs/2508.12997)
*Haishun Chen,Cai Xu,Jinlong Yu,Yilin Zhang,Ziyu Guan,Wei Zhao*

Main category: cs.LG

TL;DR: The paper proposes a novel approach, FAML, to address biased evidence learning in multi-view learning, improving prediction accuracy and uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Evidence learning in multi-view settings often shows bias towards data-rich classes, resulting in unreliable predictions. This motivated the development of a framework to address this issue.

Method: The authors introduced FAML, which uses adaptive priors and fairness constraints to correct bias during evidence learning, and an opinion alignment mechanism to improve multi-view integration.

Result: Experiments on five datasets validated that FAML achieves balanced evidence allocation, better prediction accuracy, and improved uncertainty reliability compared to existing methods.

Conclusion: FAML successfully mitigates biased evidence learning, enhances multi-view fusion, and improves reliability in uncertainty estimation through fairness-aware strategies.

Abstract: Multi-view evidential learning aims to integrate information from multiple
views to improve prediction performance and provide trustworthy uncertainty
esitimation. Most previous methods assume that view-specific evidence learning
is naturally reliable. However, in practice, the evidence learning process
tends to be biased. Through empirical analysis on real-world data, we reveal
that samples tend to be assigned more evidence to support data-rich classes,
thereby leading to unreliable uncertainty estimation in predictions. This
motivates us to delve into a new Biased Evidential Multi-view Learning (BEML)
problem. To this end, we propose Fairness-Aware Multi-view Evidential Learning
(FAML). FAML first introduces an adaptive prior based on training trajectory,
which acts as a regularization strategy to flexibly calibrate the biased
evidence learning process. Furthermore, we explicitly incorporate a fairness
constraint based on class-wise evidence variance to promote balanced evidence
allocation. In the multi-view fusion stage, we propose an opinion alignment
mechanism to mitigate view-specific bias across views, thereby encouraging the
integration of consistent and mutually supportive evidence. Extensive
experiments on five real-world multi-view datasets demonstrate that FAML
achieves more balanced evidence allocation and improves both prediction
performance and the reliability of uncertainty estimation compared to
state-of-the-art methods.

</details>


### [347] [VARAN: Variational Inference for Self-Supervised Speech Models Fine-Tuning on Downstream Tasks](https://arxiv.org/abs/2508.12061)
*Daria Diatlova,Nikita Balagansky,Alexander Varlamov,Egor Spirin*

Main category: cs.LG

TL;DR: The paper proposes VARAN, a method for dynamic layer aggregation in fine-tuned self-supervised speech models, showing superior performance by prioritizing features based on individual inputs.


<details>
  <summary>Details</summary>
Motivation: Conventional layer aggregation approaches face limitations like information bottlenecks and static feature weighting, which hinders efficient adaptation in tasks like automatic speech recognition and speech emotion recognition.

Method: VARAN uses layer-specialized probing heads and data-dependent weighting, dynamically adjusting layer aggregation based on individual input characteristics.

Result: Evaluations on speech-specific tasks demonstrated that VARAN outperforms existing methods, especially when combined with the LoRA fine-tuning technique.

Conclusion: VARAN resolves the trade-offs between preserving layer-specific information and enabling dynamic feature utilization, making it a robust framework for adapting self-supervised speech representations.

Abstract: Conventional methods for aggregating layers in fine-tuned self-supervised
speech models, such as using the final layer or weighted sum, suffer from
information bottlenecks and static feature weighting for all dataset examples.
We propose VARAN, a framework that dynamically tailors layer aggregation to
individual inputs. By employing layer-specialized probing heads and
data-dependent weighting, VARAN adaptively prioritizes layer's features based
on input. Evaluations on automatic speech recognition and speech emotion
recognition tasks demonstrate VARAN's superior performance, particularly when
using the LoRA fine-tuning technique. The framework resolves the trade-off
between preserving layer-specific information and enabling flexible feature
utilization, advancing efficient adaptation of self-supervised speech
representations.

</details>


### [348] [A Perfectly Truthful Calibration Measure](https://arxiv.org/abs/2508.13100)
*Jason Hartline,Lunjia Hu,Yifan Wu*

Main category: cs.LG

TL;DR: The paper introduces a new calibration measure, Averaged Two-Bin Calibration Error (ATB), which is perfectly truthful in the batch setting and addresses issues with existing measures incentivizing predictors to lie.


<details>
  <summary>Details</summary>
Motivation: Current calibration measures fail to guarantee truthfulness, meaning predictors are incentivized to lie on finite samples to appear more calibrated. This gap in truly reliable calibration measures inspired the creation of a perfectly truthful measure.

Method: The authors propose the Averaged Two-Bin Calibration Error (ATB), a measure that is straightforward to compute, efficient, truthful, and related to existing measures like smooth calibration error and distance to calibration. They also provide a general recipe for constructing truthful measures, including ATB as a special case.

Result: The ATB measure is not only perfectly truthful but also easier and faster to compute than existing alternatives like smCal and distCal. It achieves better running time and simplicity, enabling efficient testing for calibration properties.

Conclusion: With ATB, the authors address the long-standing issue of truthfulness in calibration measures, providing a reliable, computationally efficient, and straightforward method. Their approach also generalizes to construct other truthful calibration measures.

Abstract: Calibration requires that predictions are conditionally unbiased and,
therefore, reliably interpretable as probabilities. Calibration measures
quantify how far a predictor is from perfect calibration. As introduced by
Haghtalab et al. (2024), a calibration measure is truthful if it is minimized
in expectation when a predictor outputs the ground-truth probabilities.
Although predicting the true probabilities guarantees perfect calibration, in
reality, when calibration is evaluated on a finite sample, predicting the truth
is not guaranteed to minimize any known calibration measure. All known
calibration measures incentivize predictors to lie in order to appear more
calibrated on a finite sample. Such lack of truthfulness motivated Haghtalab et
al. (2024) and Qiao and Zhao (2025) to construct approximately truthful
calibration measures in the sequential prediction setting, but no perfectly
truthful calibration measure was known to exist even in the more basic batch
setting.
  We design a perfectly truthful calibration measure in the batch setting:
averaged two-bin calibration error (ATB). In addition to being truthful, ATB is
sound, complete, continuous, and quadratically related to two existing
calibration measures: the smooth calibration error (smCal) and the (lower)
distance to calibration (distCal). The simplicity in our definition of ATB
makes it efficient and straightforward to compute. ATB allows faster estimation
algorithms with significantly easier implementations than smCal and distCal,
achieving improved running time and simplicity for the calibration testing
problem studied by Hu et al. (2024). We also introduce a general recipe for
constructing truthful measures, which proves the truthfulness of ATB as a
special case and allows us to construct other truthful calibration measures
such as quantile-binned l_2-ECE.

</details>


### [349] [Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks](https://arxiv.org/abs/2508.12079)
*Ningzhe Shi,Yiqing Zhou,Ling Liu,Jinglin Shi,Yihao Wu,Haiwei Shi,Hanxiao Yu*

Main category: cs.LG

TL;DR: The paper proposes an efficient service assessment metric (CAQA) for ISAC-based AIGC networks and a novel LPDRL-F algorithm to optimize resource allocation, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in ISAC-based AIGC networks, where content generation is based on inaccurate sensed data and generating accuracy depends on computing resources.

Method: Introduced a content accuracy- and quality-aware metric (CAQA) and developed a linear programming guided deep reinforcement learning (LPDRL-F) algorithm to optimize the sensing-generating-communication resource allocation trade-offs.

Result: LPDRL-F improves learning efficiency and resource allocation, achieving over 60% faster convergence, a 14% better AvgCAQA, and a 50% enhancement compared to CGQ-focused schemes.

Conclusion: The proposed CAQA metric and LPDRL-F approach significantly improve the quality of ISAC-based AIGC services by optimizing resource allocation across multiple dimensions.

Abstract: Integrated sensing and communication (ISAC) can enhance artificial
intelligence-generated content (AIGC) networks by providing efficient sensing
and transmission. Existing AIGC services usually assume that the accuracy of
the generated content can be ensured, given accurate input data and prompt,
thus only the content generation quality (CGQ) is concerned. However, it is not
applicable in ISAC-based AIGC networks, where content generation is based on
inaccurate sensed data. Moreover, the AIGC model itself introduces generation
errors, which depend on the number of generating steps (i.e., computing
resources). To assess the quality of experience of ISAC-based AIGC services, we
propose a content accuracy and quality aware service assessment metric (CAQA).
Since allocating more resources to sensing and generating improves content
accuracy but may reduce communication quality, and vice versa, this
sensing-generating (computing)-communication three-dimensional resource
tradeoff must be optimized to maximize the average CAQA (AvgCAQA) across all
users with AIGC (CAQA-AIGC). This problem is NP-hard, with a large solution
space that grows exponentially with users. To solve the CAQA-AIGC problem with
low complexity, a linear programming (LP) guided deep reinforcement learning
(DRL) algorithm with an action filter (LPDRL-F) is proposed. Through the
LP-guided approach and the action filter, LPDRL-F can transform the original
three-dimensional solution space to two dimensions, reducing complexity while
improving the learning performance of DRL. Simulations show that compared to
existing DRL and generative diffusion model algorithms without LP, LPDRL-F
converges faster by over 60% and finds better resource allocation solutions,
improving AvgCAQA by more than 14%. With LPDRL-F, CAQA-AIGC can achieve an
improvement in AvgCAQA of more than 50% compared to existing schemes focusing
solely on CGQ.

</details>


### [350] [Generative Medical Event Models Improve with Scale](https://arxiv.org/abs/2508.12104)
*Shane Waxler,Paul Blazek,Davis White,Daniel Sneider,Kevin Chung,Mani Nagarathnam,Patrick Williams,Hank Voeller,Karen Wong,Matthew Swanhorst,Sheng Zhang,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon,Andrew Loza,Daniella Meeker,Seth Hain,Rahul Shah*

Main category: cs.LG

TL;DR: The paper introduces CoMET, a foundation model based on transformer architecture, trained on vast medical event data to predict health events and enhance clinical tasks.


<details>
  <summary>Details</summary>
Motivation: To develop scalable methods for analyzing longitudinal patient records to advance personalized medicine and clinical decision-making.

Method: Use a large medical event dataset for pretraining CoMET transformer models with up to 1 billion parameters, establishing power-law scaling relationships for model optimization.

Result: CoMET outperformed or matched task-specific supervised models in 78 real-world tasks without requiring task-specific tuning, demonstrating enhanced predictive power with increased scales.

Conclusion: CoMET proves effective for capturing complex clinical dynamics and supports diverse healthcare applications, showcasing its scalability and versatility in improving healthcare outcomes.

Abstract: Realizing personalized medicine at scale calls for methods that distill
insights from longitudinal patient journeys, which can be viewed as a sequence
of medical events. Foundation models pretrained on large-scale medical event
data represent a promising direction for scaling real-world evidence generation
and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with
medical events from de-identified longitudinal health records for 16.3 billion
encounters over 300 million unique patient records from 310 health systems, we
introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of
decoder-only transformer models pretrained on 118 million patients representing
115 billion discrete medical events (151 billion tokens). We present the
largest scaling-law study for medical event data, establishing a methodology
for pretraining and revealing power-law scaling relationships for compute,
tokens, and model size. Based on this, we pretrained a series of
compute-optimal models with up to 1 billion parameters. Conditioned on a
patient's real-world history, CoMET autoregressively generates the next medical
event, simulating patient health timelines. We studied 78 real-world tasks,
including diagnosis prediction, disease prognosis, and healthcare operations.
Remarkably for a foundation model with generic pretraining and simulation-based
inference, CoMET generally outperformed or matched task-specific supervised
models on these tasks, without requiring task-specific fine-tuning or few-shot
examples. CoMET's predictive power consistently improves as the model and
pretraining scale. Our results show that CoMET, a generative medical event
foundation model, can effectively capture complex clinical dynamics, providing
an extensible and generalizable framework to support clinical decision-making,
streamline healthcare operations, and improve patient outcomes.

</details>


### [351] [DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections](https://arxiv.org/abs/2508.12116)
*Haebin Shin,Lei Ji,Xiao Liu,Zhiwei Yu,Qi Chen,Yeyun Gong*

Main category: cs.LG

TL;DR: The paper introduces DynamixSFT, a method for dynamically optimizing instruction-tuning dataset mixtures, showing improvements in model performance.


<details>
  <summary>Details</summary>
Motivation: The need to balance and optimize numerous instruction-tuning datasets emerging during the post-training stage to enhance model performance.

Method: A multi-armed bandit approach using Prior-scaled Boltzmann Exploration and 1-Step Look-ahead Reward for updating dataset sampling probabilities.

Result: DynamixSFT improved performance by up to 2.2% across 10 benchmarks using Tulu-v2-mixture datasets.

Conclusion: DynamixSFT demonstrates effective dynamic optimization of dataset mixtures, preserving diversity while improving performance, and offers insights through analysis and visualizations.

Abstract: As numerous instruction-tuning datasets continue to emerge during the
post-training stage, dynamically balancing and optimizing their mixtures has
become a critical challenge. To address this, we propose DynamixSFT, a dynamic
and automated method for instruction-tuning dataset mixture optimization. We
formulate the problem as a multi-armed bandit setup and introduce a
Prior-scaled Boltzmann Exploration that softly anchors the updated sampling
distribution to the original dataset proportions, thereby preserving the
inherent diversity and coverage of the collection. Sampling probabilities are
updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the
dataset contributes to improving the model's performance at its current state.
When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning
datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10
benchmarks. Furthermore, we provide a comprehensive analysis and visualizations
to offer deeper insights into the adaptive dynamics of our method.

</details>


### [352] [Time-Scale Coupling Between States and Parameters in Recurrent Neural Networks](https://arxiv.org/abs/2508.12121)
*Lorenzo Livi*

Main category: cs.LG

TL;DR: Gating mechanisms in RNNs influence adaptive step sizes and optimization behavior, even with a fixed learning rate, acting as data-driven preconditioners during training.


<details>
  <summary>Details</summary>
Motivation: To understand the implicit adaptive learning-rate behavior induced by gating mechanisms in RNNs and their role in optimization and robust training.

Method: Exact Jacobians are derived for leaky-integrator and gated RNNs, leading to a perturbative analysis of gradient propagation and optimization effects caused by gates.

Result: The study reveals that gates reshape gradient dynamics, introduce anisotropic parameter updates, and mimic behaviors like learning-rate schedules and the Adam optimizer.

Conclusion: Gates in RNNs not only control memory but also systematically impact optimization trajectories, enhancing trainability and stability by coupling state evolution with parameter updates.

Abstract: We study how gating mechanisms in recurrent neural networks (RNNs) implicitly
induce adaptive learning-rate behavior, even when training is carried out with
a fixed, global learning rate. This effect arises from the coupling between
state-space time scales--parametrized by the gates--and parameter-space
dynamics during gradient descent. By deriving exact Jacobians for
leaky-integrator and gated RNNs, we obtain a first-order expansion that makes
explicit how constant, scalar, and multi-dimensional gates reshape gradient
propagation, modulate effective step sizes, and introduce anisotropy in
parameter updates. These findings reveal that gates not only control memory
retention in the hidden states, but also act as data-driven preconditioners
that adapt optimization trajectories in parameter space. We further draw formal
analogies with learning-rate schedules, momentum, and adaptive methods such as
Adam, showing that these optimization behaviors emerge naturally from gating.
Numerical experiments confirm the validity of our perturbative analysis,
supporting the view that gate-induced corrections remain small while exerting
systematic effects on training dynamics. Overall, this work provides a unified
dynamical-systems perspective on how gating couples state evolution with
parameter updates, explaining why gated architectures achieve robust
trainability and stability in practice.

</details>


### [353] [DE-VAE: Revealing Uncertainty in Parametric and Inverse Projections with Variational Autoencoders using Differential Entropy](https://arxiv.org/abs/2508.12145)
*Frederik L. Dennig,Daniel A. Keim*

Main category: cs.LG

TL;DR: The paper proposes DE-VAE, an uncertainty-aware variational autoencoder to improve parametric and invertible projections for multidimensional data, with comparable performance to existing methods but added ability to analyze embedding uncertainty.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for parametric and invertible data projections struggle with handling out-of-distribution samples in both data and embedding space.

Method: The authors introduce DE-VAE, which incorporates differential entropy into a variational autoencoder to improve learned projections. It maps data into 2D space and provides an inverse mapping back to the original space.

Result: DE-VAE performs comparably to established methods such as UMAP and t-SNE in creating parametric and inverse projections, while introducing the capability to analyze uncertainty in embeddings.

Conclusion: DE-VAE enhances the reliability and usefulness of projections by maintaining accuracy and providing tools for uncertainty analysis, making it a robust alternative for processing multidimensional data.

Abstract: Recently, autoencoders (AEs) have gained interest for creating parametric and
invertible projections of multidimensional data. Parametric projections make it
possible to embed new, unseen samples without recalculating the entire
projection, while invertible projections allow the synthesis of new data
instances. However, existing methods perform poorly when dealing with
out-of-distribution samples in either the data or embedding space. Thus, we
propose DE-VAE, an uncertainty-aware variational AE using differential entropy
(DE) to improve the learned parametric and invertible projections. Given a
fixed projection, we train DE-VAE to learn a mapping into 2D space and an
inverse mapping back to the original space. We conduct quantitative and
qualitative evaluations on four well-known datasets, using UMAP and t-SNE as
baseline projection methods. Our findings show that DE-VAE can create
parametric and inverse projections with comparable accuracy to other current
AE-based approaches while enabling the analysis of embedding uncertainty.

</details>


### [354] [AICRN: Attention-Integrated Convolutional Residual Network for Interpretable Electrocardiogram Analysis](https://arxiv.org/abs/2508.12162)
*J. M. I. H. Jayakody,A. M. H. H. Alahakoon,C. R. M. Perera,R. M. L. C. Srimal,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.LG

TL;DR: The study presents a novel deep learning model, AICRN, for precise and interpretable ECG analysis, outperforming existing systems in regression tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the precision and interpretability of ECG analysis for better diagnosis and management of cardiac diseases.

Method: Developed an attention-integrated convolutional residual network (AICRN) leveraging spatial and channel attention mechanisms and convolutional residual networks.

Result: AICRN demonstrated superior performance in regressing key ECG parameters compared to existing models.

Conclusion: Deep learning models like AICRN can significantly improve ECG analysis, enabling advanced clinical applications for cardiac monitoring and management.

Abstract: The paradigm of electrocardiogram (ECG) analysis has evolved into real-time
digital analysis, facilitated by artificial intelligence (AI) and machine
learning (ML), which has improved the diagnostic precision and predictive
capacity of cardiac diseases. This work proposes a novel deep learning (DL)
architecture called the attention-integrated convolutional residual network
(AICRN) to regress key ECG parameters such as the PR interval, the QT interval,
the QRS duration, the heart rate, the peak amplitude of the R wave, and the
amplitude of the T wave for interpretable ECG analysis. Our architecture is
specially designed with spatial and channel attention-related mechanisms to
address the type and spatial location of the ECG features for regression. The
models employ a convolutional residual network to address vanishing and
exploding gradient problems. The designed system addresses traditional analysis
challenges, such as loss of focus due to human errors, and facilitates the fast
and easy detection of cardiac events, thereby reducing the manual efforts
required to solve analysis tasks. AICRN models outperform existing models in
parameter regression with higher precision. This work demonstrates that DL can
play a crucial role in the interpretability and precision of ECG analysis,
opening up new clinical applications for cardiac monitoring and management.

</details>


### [355] [ProtTeX-CC: Activating In-Context Learning in Protein LLM via Two-Stage Instruction Compression](https://arxiv.org/abs/2508.12212)
*Chuanliu Fan,Zicheng Ma,Jun Gao,Nan Yu,Jun Zhang,Ziqiang Cao,Yi Qin Gao,Guohong Fu*

Main category: cs.LG

TL;DR: ProtTeX-CC enhances the ProtTeX protein language model by addressing input length and few-shot learning limitations through compression mechanisms.


<details>
  <summary>Details</summary>
Motivation: ProtTeX faces restrictions with elongated input lengths due to concatenated token streams and limited generalization capabilities in few-shot contexts.

Method: ProtTeX-CC employs two-stage compression: joint embedding compression to halve protein input length, and self-compression to reduce demonstration lengths significantly.

Result: ProtTeX-CC reduces prompt length by 93.68% and achieves performance improvements of 2% for in-domain benchmarks and 11% for out-of-domain datasets in protein function prediction tasks.

Conclusion: ProtTeX-CC demonstrates efficient compression and improved generalization in protein analysis, enabling better performance without altering the backbone model significantly.

Abstract: Recent advances in protein large language models, such as ProtTeX, represent
both side-chain amino acids and backbone structure as discrete token sequences
of residue length. While this design enables unified modeling of multimodal
protein information, it suffers from two major limitations: (1) The
concatenation of sequence and structure tokens approximately doubles the
protein length and breaks the intrinsic residue-level alignment between
modalities. (2) Constrained by the training corpus and limited context window,
ProtTeX is typically trained on single-protein inputs, rendering it
incompatible with in-context learning (ICL) and thus limiting its
generalization capability. To address these issues, we propose ProtTeX-CC, a
lightweight two-stage compression framework designed to enhance ProtTeX under
few-shot settings. We first design a joint embedding compression mechanism that
fuses sequence and structure representations at the residue level, effectively
reducing the protein input length by half without sacrificing performance. Then
we propose a self-compression module that aggregates each full demonstration
into the latent space of the last few linguistic tokens, reducing the average
demonstration length from 751 tokens to less than 16 tokens. Compared to the
original ProtTeX, our self-compression approach achieves a compression ratio of
approximately 93.68% in the total prompt length under the 16-shot setting.
Without modifying the backbone model, ProtTeX-CC introduces only a small number
of additional parameters through PEFT-based tuning in the joint embedding
compression stage and a single trainable projection layer in the
self-compression stage. Extensive experiments on protein function prediction
show that ProtTeX-CC improves performance on the in-domain benchmark by 2%, and
generalizes well to the out-of-domain dataset with a performance gain of 11%.

</details>


### [356] [Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models](https://arxiv.org/abs/2508.12220)
*Abdullah X*

Main category: cs.LG

TL;DR: This paper focuses on implementing the 'right to be forgotten' for large language models by proposing deterministic training methods to unlearn data efficiently.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of making large language models compliant with GDPR's 'right to be forgotten' requirement by creating an efficient, reproducible, and systematic approach to data unlearning.

Method: The method involves deterministic training with minimal microbatch logging and tail replay techniques. It incorporates exact reverts, adapter deletion, and curvature-guided anti-update paths to efficiently unlearn data while maintaining model integrity.

Result: The paper demonstrates byte-identical equality of model and optimizer states under a controlled run that satisfies the proposed preconditions, ensuring efficient unlearning of data.

Conclusion: The proposed system offers a viable and reproducible method for implementing unlearning in large language models, supporting GDPR compliance while optimizing storage/latency constraints.

Abstract: We study the right to be forgotten (GDPR Art. 17) for large language models
and frame unlearning as a reproducible systems problem. Our approach treats
training as a deterministic program and logs a minimal per-microbatch record
(ordered ID hash, RNG seed, learning-rate value, optimizer-step counter, and
accumulation boundary). Under a pinned stack and deterministic kernels,
replaying the training tail while filtering only the forget closure yields the
same parameters as training on the retain set (bit-identical in the training
dtype) when preconditions hold. To meet latency and availability constraints,
we add complementary paths: (i) exact reverts of recent steps via
micro-checkpoints or dense per-step deltas, (ii) cohort-scoped adapter deletion
when the base is frozen, and (iii) a curvature-guided anti-update followed by a
short retain-tune, audit-gated with escalation to exact replay. We report
storage/latency budgets and a toy artifact validating mechanics; in a
controlled run that satisfies the preconditions we demonstrate byte-identical
equality of model and optimizer states.

</details>


### [357] [Distribution Matching via Generalized Consistency Models](https://arxiv.org/abs/2508.12222)
*Sagar Shrestha,Rajesh Shrestha,Tri Nguyen,Subash Timilsina*

Main category: cs.LG

TL;DR: The paper introduces a new distribution matching method inspired by Continuous Normalizing Flow (CNF) models, aiming to overcome challenges found in Generative Adversarial Networks (GANs).


<details>
  <summary>Details</summary>
Motivation: The study aims to address common challenges in GANs, such as training instability and mode collapse, while exploring an adaptable yet straightforward approach for distribution matching.

Method: The research proposes a novel distribution matching model that combines the norm minimization objective of CNF models with the adaptability of GANs. The approach is theoretically validated and tested on synthetic and real-world datasets.

Result: The proposed method demonstrates effective distribution matching and performs successfully in experiments on diverse datasets.

Conclusion: The model blends CNF and GAN qualities to provide a robust solution to distribution matching, overcoming some of GANs' main limitations and offering theoretical and practical benefits.

Abstract: Recent advancement in generative models have demonstrated remarkable
performance across various data modalities. Beyond their typical use in data
synthesis, these models play a crucial role in distribution matching tasks such
as latent variable modeling, domain translation, and domain adaptation.
Generative Adversarial Networks (GANs) have emerged as the preferred method of
distribution matching due to their efficacy in handling high-dimensional data
and their flexibility in accommodating various constraints. However, GANs often
encounter challenge in training due to their bi-level min-max optimization
objective and susceptibility to mode collapse. In this work, we propose a novel
approach for distribution matching inspired by the consistency models employed
in Continuous Normalizing Flow (CNF). Our model inherits the advantages of CNF
models, such as having a straight forward norm minimization objective, while
remaining adaptable to different constraints similar to GANs. We provide
theoretical validation of our proposed objective and demonstrate its
performance through experiments on synthetic and real-world datasets.

</details>


### [358] [Communication-Efficient Distributed Asynchronous ADMM](https://arxiv.org/abs/2508.12233)
*Sagar Shrestha*

Main category: cs.LG

TL;DR: The paper introduces coarse quantization to asynchronous ADMM in distributed optimization and federated learning to reduce communication costs, ensuring applicability for large-scale tasks.


<details>
  <summary>Details</summary>
Motivation: Address the communication bottlenecks in asynchronous ADMM used in distributed optimization and federated learning, especially under limited communication budgets.

Method: Propose the utilization of coarse quantization during data exchange in asynchronous ADMM to minimize communication overhead.

Result: Experimental evidence supports the convergence of the quantized ADMM approach across multiple distributed learning tasks, such as neural networks.

Conclusion: Coarse quantization effectively reduces communication costs while preserving the convergence capabilities of asynchronous ADMM, making it suitable for large-scale applications.

Abstract: In distributed optimization and federated learning, asynchronous alternating
direction method of multipliers (ADMM) serves as an attractive option for
large-scale optimization, data privacy, straggler nodes and variety of
objective functions. However, communication costs can become a major bottleneck
when the nodes have limited communication budgets or when the data to be
communicated is prohibitively large. In this work, we propose introducing
coarse quantization to the data to be exchanged in aynchronous ADMM so as to
reduce communication overhead for large-scale federated learning and
distributed optimization applications. We experimentally verify the convergence
of the proposed method for several distributed learning tasks, including neural
networks.

</details>


### [359] [CC-Time: Cross-Model and Cross-Modality Time Series Forecasting](https://arxiv.org/abs/2508.12235)
*Peng Chen,Yihang Wang,Yang Shu,Yunyao Cheng,Kai Zhao,Zhongwen Rao,Lujia Pan,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: This paper introduces CC-Time, which uses pre-trained language models (PLMs) combined with cross-model and cross-modality learning for improved time series forecasting.


<details>
  <summary>Details</summary>
Motivation: To leverage the strong sequential modeling capabilities of PLMs for time series forecasting while addressing their current limitations in prediction accuracy.

Method: The authors propose CC-Time, which utilizes cross-modality learning to incorporate both time series sequences and text descriptions, and a cross-model fusion block to combine knowledge from PLMs and dedicated time series models.

Result: CC-Time outperforms other methods in prediction accuracy across nine real-world datasets, both in full-data training and few-shot learning scenarios.

Conclusion: CC-Time proves the potential of integrating PLMs with time series models using cross-modality and cross-model fusion, achieving state-of-the-art accuracy.

Abstract: With the success of pre-trained language models (PLMs) in various application
fields beyond natural language processing, language models have raised emerging
attention in the field of time series forecasting (TSF) and have shown great
prospects. However, current PLM-based TSF methods still fail to achieve
satisfactory prediction accuracy matching the strong sequential modeling power
of language models. To address this issue, we propose Cross-Model and
Cross-Modality Learning with PLMs for time series forecasting (CC-Time). We
explore the potential of PLMs for time series forecasting from two aspects: 1)
what time series features could be modeled by PLMs, and 2) whether relying
solely on PLMs is sufficient for building time series models. In the first
aspect, CC-Time incorporates cross-modality learning to model temporal
dependency and channel correlations in the language model from both time series
sequences and their corresponding text descriptions. In the second aspect,
CC-Time further proposes the cross-model fusion block to adaptively integrate
knowledge from the PLMs and time series model to form a more comprehensive
modeling of time series patterns. Extensive experiments on nine real-world
datasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy
in both full-data training and few-shot learning situations.

</details>


### [360] [DHG-Bench: A Comprehensive Benchmark on Deep Hypergraph Learning](https://arxiv.org/abs/2508.12244)
*Fan Li,Xiaoyang Wang,Wenjie Zhang,Ying Zhang,Xuemin Lin*

Main category: cs.LG

TL;DR: This paper introduces DHG-Bench, the first comprehensive benchmark for deep hypergraph learning, addressing limitations in datasets, algorithms, and experimental setups. It evaluates 16 state-of-the-art hypergraph neural networks (HNNs) using 20 diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Conventional graph models struggle to capture higher-order interactions in real-world systems, necessitating the use of Hypergraph Neural Networks (HNNs). The lack of a benchmark hinders progress in DHGL research.

Method: The authors created DHG-Bench, incorporating diverse datasets, algorithms, and tasks under uniform preprocessing and experimental protocols. It evaluates algorithms on dimensions like effectiveness, efficiency, robustness, and fairness.

Result: DHG-Bench systematically analyzes HNN performance across various dimensions, highlighting strengths and limitations. It provides experimental insights and facilitates reproducible research via an accessible library.

Conclusion: DHG-Bench is a crucial tool for advancing deep hypergraph learning, offering standardized evaluation and revealing opportunities for improvement in HNN algorithms.

Abstract: Although conventional deep graph models have achieved great success in
relational learning, their focus on pairwise relationships limits their
capacity to learn pervasive higher-order interactions in real-world complex
systems, which can be naturally modeled as hypergraphs. To tackle this,
hypergraph neural networks (HNNs), the dominant approach in deep hypergraph
learning (DHGL), has garnered substantial attention in recent years. Despite
the proposal of numerous HNN methods, there is no comprehensive benchmark for
HNNs, which creates a great obstacle to understanding the progress of DHGL in
several aspects: (i) insufficient coverage of datasets, algorithms, and tasks;
(ii) a narrow evaluation of algorithm performance; and (iii) inconsistent
dataset usage, preprocessing, and experimental setups that hinder
comparability. To fill the gap, we introduce DHG-Bench, the first comprehensive
benchmark for DHGL. Specifically, DHG-Bench integrates 20 diverse datasets
spanning node-, edge-, and graph-level tasks, along with 16 state-of-the-art
HNN algorithms, under consistent data processing and experimental protocols.
Our benchmark systematically investigates the characteristics of HNNs in terms
of four dimensions: effectiveness, efficiency, robustness, and fairness.
Further, to facilitate reproducible research, we have developed an easy-to-use
library for training and evaluating different HNN methods. Extensive
experiments conducted with DHG-Bench reveal both the strengths and inherent
limitations of existing algorithms, offering valuable insights and directions
for future research. The code is publicly available at:
https://github.com/Coco-Hut/DHG-Bench.

</details>


### [361] [STM3: Mixture of Multiscale Mamba for Long-Term Spatio-Temporal Time-Series Prediction](https://arxiv.org/abs/2508.12247)
*Haolong Chen,Liang Zhang,Zhengyuan Xin,Guangxu Zhu*

Main category: cs.LG

TL;DR: The paper introduces STM2 and STM3 for long-term spatio-temporal prediction, resolving multiscale dependency challenges and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with efficiently capturing complex long-term spatio-temporal dependencies and multiscale temporal correlations.

Method: STM2 uses a multiscale Mamba architecture and adaptive graph causal convolution network, while STM3 incorporates a Mixture-of-Experts framework with enhanced routing and causal contrastive learning strategies.

Result: STM2 and STM3 outperform previous approaches on real-world benchmarks for long-term spatio-temporal prediction.

Conclusion: The proposed methods effectively address dependency challenges and enable superior prediction capabilities, advancing the state-of-the-art in spatio-temporal modeling.

Abstract: Recently, spatio-temporal time-series prediction has developed rapidly, yet
existing deep learning methods struggle with learning complex long-term
spatio-temporal dependencies efficiently. The long-term spatio-temporal
dependency learning brings two new challenges: 1) The long-term temporal
sequence includes multiscale information naturally which is hard to extract
efficiently; 2) The multiscale temporal information from different nodes is
highly correlated and hard to model. To address these challenges, we propose an
efficient \textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ultiscale
\textbf{M}amba} (STM2) that includes a multiscale Mamba architecture to capture
the multiscale information efficiently and simultaneously, and an adaptive
graph causal convolution network to learn the complex multiscale
spatio-temporal dependency. STM2 includes hierarchical information aggregation
for different-scale information that guarantees their distinguishability. To
capture diverse temporal dynamics across all spatial nodes more efficiently, we
further propose an enhanced version termed
\textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ixture of
\textbf{M}ultiscale \textbf{M}amba} (STM3) that employs a special
Mixture-of-Experts architecture, including a more stable routing strategy and a
causal contrastive learning strategy to enhance the scale distinguishability.
We prove that STM3 has much better routing smoothness and guarantees the
pattern disentanglement for each expert successfully. Extensive experiments on
real-world benchmarks demonstrate STM2/STM3's superior performance, achieving
state-of-the-art results in long-term spatio-temporal time-series prediction.

</details>


### [362] [Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset](https://arxiv.org/abs/2508.12253)
*Manish Shukla*

Main category: cs.LG

TL;DR: The paper introduces a unified framework combining LIME and SHAP techniques for interpreting time-series forecasts, specifically focusing on overcoming challenges of model opacity and preserving chronology.


<details>
  <summary>Details</summary>
Motivation: Time-series forecasting supports important decisions across several industries but faces challenges in balancing interpretability and accuracy among traditional and modern models.

Method: The framework converts a univariate time-series into a supervised learning problem, uses tree-based models (XGBoost) alongside ARIMA for comparison, and applies LIME and SHAP explanations while maintaining chronological integrity.

Result: Using Air Passengers dataset, the study identified key lagged features and seasonal encodings that explain forecast variance, validating the framework's interpretability.

Conclusion: The study provides a systematic method for applying LIME and SHAP to time-series models, offers theoretical insights, empirically validates the approach, and issues practical guidelines for analysts and practitioners.

Abstract: Time-series forecasting underpins critical decisions across aviation, energy,
retail and health. Classical autoregressive integrated moving average (ARIMA)
models offer interpretability via coefficients but struggle with
nonlinearities, whereas tree-based machine-learning models such as XGBoost
deliver high accuracy but are often opaque. This paper presents a unified
framework for interpreting time-series forecasts using local interpretable
model-agnostic explanations (LIME) and SHapley additive exPlanations (SHAP). We
convert a univariate series into a leakage-free supervised learning problem,
train a gradient-boosted tree alongside an ARIMA baseline and apply post-hoc
explainability. Using the Air Passengers dataset as a case study, we show that
a small set of lagged features -- particularly the twelve-month lag -- and
seasonal encodings explain most forecast variance. We contribute: (i) a
methodology for applying LIME and SHAP to time series without violating
chronology; (ii) theoretical exposition of the underlying algorithms; (iii)
empirical evaluation with extensive analysis; and (iv) guidelines for
practitioners.

</details>


### [363] [L-SR1: Learned Symmetric-Rank-One Preconditioning](https://arxiv.org/abs/2508.12270)
*Gal Lifshitz,Shahar Zuler,Ori Fouks,Dan Raviv*

Main category: cs.LG

TL;DR: The paper introduces a trainable preconditioning unit enhancing a classical second-order optimization algorithm (SR1), offering strong generalization for tasks like Monocular Human Mesh Recovery without the need for labeled data or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Deep learning faces limitations like dependency on large labeled datasets, poor generalization, and computational demands, while classical optimization is lightweight yet slow. There is little exploration of learned second-order optimizers.

Method: The authors create a learned second-order optimization method with a trainable preconditioning unit, enhancing the Symmetric-Rank-One (SR1) algorithm. The unit generates data-driven vectors to construct positive semi-definite matrices, ensuring alignment with a secant constraint.

Result: The proposed optimizer outperforms existing learned optimizer methods in analytic experiments and in Monocular Human Mesh Recovery (HMR).

Conclusion: The method is computationally efficient, requires no annotated data or fine-tuning, generalizes well, and integrates with broader optimization frameworks for enhanced performance.

Abstract: End-to-end deep learning has achieved impressive results but remains limited
by its reliance on large labeled datasets, poor generalization to unseen
scenarios, and growing computational demands. In contrast, classical
optimization methods are data-efficient and lightweight but often suffer from
slow convergence. While learned optimizers offer a promising fusion of both
worlds, most focus on first-order methods, leaving learned second-order
approaches largely unexplored.
  We propose a novel learned second-order optimizer that introduces a trainable
preconditioning unit to enhance the classical Symmetric-Rank-One (SR1)
algorithm. This unit generates data-driven vectors used to construct positive
semi-definite rank-one matrices, aligned with the secant constraint via a
learned projection. Our method is evaluated through analytic experiments and on
the real-world task of Monocular Human Mesh Recovery (HMR), where it
outperforms existing learned optimization-based approaches. Featuring a
lightweight model and requiring no annotated data or fine-tuning, our approach
offers strong generalization and is well-suited for integration into broader
optimization-based frameworks.

</details>


### [364] [CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision](https://arxiv.org/abs/2508.12278)
*Siyue Xie,Da Sun Handason Tam,Wing Cheong Lau*

Main category: cs.LG

TL;DR: Graph Neural Networks (GNNs) struggle with Graph Anomaly Detection (GAD) due to limited labeled data. CRoC leverages both labeled and unlabeled data effectively by reworking node contexts and leveraging contrastive learning, improving detection performance by up to 14% AUC across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: The difficulty in training robust GNNs for GAD stems from the scarcity of labeled data, making it challenging to detect anomalies that are rare and often camouflaged.

Method: CRoC employs context refactoring to augment node attributes while maintaining interaction patterns and integrates heterogeneous relations into GNNs' message-passing. It combines this with contrastive learning for joint training using both labeled and unlabeled data.

Result: CRoC consistently outperforms baseline GNNs and state-of-the-art GAD methods, achieving a significant improvement of up to 14% AUC across seven diverse datasets.

Conclusion: By leveraging both labeled and unlabeled data and refactoring nodes' context, CRoC enhances graph anomaly detection capabilities, making it a robust framework even in limited-label scenarios.

Abstract: Graph Neural Networks (GNNs) are widely used as the engine for various
graph-related tasks, with their effectiveness in analyzing graph-structured
data. However, training robust GNNs often demands abundant labeled data, which
is a critical bottleneck in real-world applications. This limitation severely
impedes progress in Graph Anomaly Detection (GAD), where anomalies are
inherently rare, costly to label, and may actively camouflage their patterns to
evade detection. To address these problems, we propose Context Refactoring
Contrast (CRoC), a simple yet effective framework that trains GNNs for GAD by
jointly leveraging limited labeled and abundant unlabeled data. Different from
previous works, CRoC exploits the class imbalance inherent in GAD to refactor
the context of each node, which builds augmented graphs by recomposing the
attributes of nodes while preserving their interaction patterns. Furthermore,
CRoC encodes heterogeneous relations separately and integrates them into the
message-passing process, enhancing the model's capacity to capture complex
interaction semantics. These operations preserve node semantics while
encouraging robustness to adversarial camouflage, enabling GNNs to uncover
intricate anomalous cases. In the training stage, CRoC is further integrated
with the contrastive learning paradigm. This allows GNNs to effectively harness
unlabeled data during joint training, producing richer, more discriminative
node embeddings. CRoC is evaluated on seven real-world GAD datasets with
varying scales. Extensive experiments demonstrate that CRoC achieves up to 14%
AUC improvement over baseline GNNs and outperforms state-of-the-art GAD methods
under limited-label settings.

</details>


### [365] [Convergence Analysis of the Lion Optimizer in Centralized and Distributed Settings](https://arxiv.org/abs/2508.12327)
*Wei Jiang,Lijun Zhang*

Main category: cs.LG

TL;DR: The paper analyzes the convergence properties of the Lion optimizer, proposes a variance-reduced improvement, and extends its application to distributed settings and communication-efficient variants.


<details>
  <summary>Details</summary>
Motivation: To analyze and improve the convergence properties of the Lion optimizer for better performance in both centralized and distributed optimization tasks.

Method: The study first analyzes the standard Lion optimizer's convergence rate, introduces variance reduction to improve it, and extends the results to distributed and communication-efficient settings.

Result: The enhanced variance-reduced Lion optimizer achieves faster convergence. Distributed versions perform effectively, with communication-efficient variants balancing convergence with reduced communication costs.

Conclusion: The study provides theoretical guarantees for the Lion optimizer's improved convergence rates and showcases its potential in both centralized and distributed scenarios with efficient communication.

Abstract: In this paper, we analyze the convergence properties of the Lion optimizer.
First, we establish that the Lion optimizer attains a convergence rate of
$\mathcal{O}(d^{1/2}T^{-1/4})$ under standard assumptions, where $d$ denotes
the problem dimension and $T$ is the iteration number. To further improve this
rate, we introduce the Lion optimizer with variance reduction, resulting in an
enhanced convergence rate of $\mathcal{O}(d^{1/2}T^{-1/3})$. We then analyze in
distributed settings, where the standard and variance reduced version of the
distributed Lion can obtain the convergence rates of
$\mathcal{O}(d^{1/2}(nT)^{-1/4})$ and $\mathcal{O}(d^{1/2}(nT)^{-1/3})$, with
$n$ denoting the number of nodes. Furthermore, we investigate a
communication-efficient variant of the distributed Lion that ensures sign
compression in both communication directions. By employing the unbiased sign
operations, the proposed Lion variant and its variance reduction counterpart,
achieve convergence rates of $\mathcal{O}\left( \max
\left\{\frac{d^{1/4}}{T^{1/4}}, \frac{d^{1/10}}{n^{1/5}T^{1/5}} \right\}
\right)$ and $\mathcal{O}\left( \frac{d^{1/4}}{T^{1/4}} \right)$, respectively.

</details>


### [366] [Navigating the Exploration-Exploitation Tradeoff in Inference-Time Scaling of Diffusion Models](https://arxiv.org/abs/2508.12361)
*Xun Su,Jianming Huang,Yang Yusen,Zhongxi Fang,Hiroyuki Kasai*

Main category: cs.LG

TL;DR: The paper explores inference-time scaling with Sequential Monte Carlo (SMC) methods for diffusion models, proposing two strategies: Funnel Schedule and Adaptive Temperature, to improve sample quality.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in applying Sequential Monte Carlo methods to diffusion models, particularly dealing with the exploration-exploitation trade-off during multi-modal search.

Method: Proposed Funnel Schedule and Adaptive Temperature techniques focus on reducing particle count progressively and adjusting the influence of early-stage rewards.

Result: The methods improved sample quality significantly without additional computational costs across benchmarks and text-to-image diffusion models.

Conclusion: The strategies effectively address the exploration-exploitation dilemma, enhancing the performance of Sequential Monte Carlo methods in diffusion models.

Abstract: Inference-time scaling has achieved remarkable success in language models,
yet its adaptation to diffusion models remains underexplored. We observe that
the efficacy of recent Sequential Monte Carlo (SMC)-based methods largely stems
from globally fitting the The reward-tilted distribution, which inherently
preserves diversity during multi-modal search. However, current applications of
SMC to diffusion models face a fundamental dilemma: early-stage noise samples
offer high potential for improvement but are difficult to evaluate accurately,
whereas late-stage samples can be reliably assessed but are largely
irreversible. To address this exploration-exploitation trade-off, we approach
the problem from the perspective of the search algorithm and propose two
strategies: Funnel Schedule and Adaptive Temperature. These simple yet
effective methods are tailored to the unique generation dynamics and
phase-transition behavior of diffusion models. By progressively reducing the
number of maintained particles and down-weighting the influence of early-stage
rewards, our methods significantly enhance sample quality without increasing
the total number of Noise Function Evaluations. Experimental results on
multiple benchmarks and state-of-the-art text-to-image diffusion models
demonstrate that our approach outperforms previous baselines.

</details>


### [367] [Bi-Axial Transformers: Addressing the Increasing Complexity of EHR Classification](https://arxiv.org/abs/2508.12418)
*Rachael DeVries,Casper Christensen,Marie Lisandra Zepeda Mendoza,Ole Winther*

Main category: cs.LG

TL;DR: The Bi-Axial Transformer (BAT) improves Electronic Health Records (EHR) analysis by attending to both clinical variable and time point axes, achieving state-of-the-art results for sepsis prediction.


<details>
  <summary>Details</summary>
Motivation: Due to increasing complexity in EHR datasets, existing methods struggle with sparse data and fail to capture informative missingness. Transformers offer a solution but previous applications were limited.

Method: The paper introduces BAT, a transformer model designed to address EHR sparsity and missingness, focusing on both clinical variables and time points. Implementations were standardized using PyTorch.

Result: BAT outperformed other methods in sepsis prediction and was competitive for mortality classification tasks. It showed increased robustness to missing data and enabled unique sensor embeddings for transfer learning.

Conclusion: BAT demonstrates its capability to handle the complexities of EHR effectively, paving the way for improved outcomes in clinical tasks like prediction and classification, with reproducible benchmarks.

Abstract: Electronic Health Records (EHRs), the digital representation of a patient's
medical history, are a valuable resource for epidemiological and clinical
research. They are also becoming increasingly complex, with recent trends
indicating larger datasets, longer time series, and multi-modal integrations.
Transformers, which have rapidly gained popularity due to their success in
natural language processing and other domains, are well-suited to address these
challenges due to their ability to model long-range dependencies and process
data in parallel. But their application to EHR classification remains limited
by data representations, which can reduce performance or fail to capture
informative missingness. In this paper, we present the Bi-Axial Transformer
(BAT), which attends to both the clinical variable and time point axes of EHR
data to learn richer data relationships and address the difficulties of data
sparsity. BAT achieves state-of-the-art performance on sepsis prediction and is
competitive to top methods for mortality classification. In comparison to other
transformers, BAT demonstrates increased robustness to data missingness, and
learns unique sensor embeddings which can be used in transfer learning.
Baseline models, which were previously located across multiple repositories or
utilized deprecated libraries, were re-implemented with PyTorch and made
available for reproduction and future benchmarking.

</details>


### [368] [Machine Learning-Based Manufacturing Cost Prediction from 2D Engineering Drawings via Geometric Features](https://arxiv.org/abs/2508.12440)
*Ahmet Bilal Arıkan,Şener Özönder,Mustafa Taha Koçyiğit,Hüseyin Oktay Altun,H. Kübra Küçükkartal,Murat Arslanoğlu,Fatih Çağırankaya,Berk Ayvaz*

Main category: cs.LG

TL;DR: This paper proposes a machine learning-based framework for cost estimation from 2D engineering drawings using gradient-boosted decision trees, improving speed and accuracy while offering insights for cost-aware design.


<details>
  <summary>Details</summary>
Motivation: Traditional manufacturing cost estimation is labor-intensive and reliant on manual workflows, creating a need for efficient, scalable, and automated solutions.

Method: The authors developed a framework leveraging ~200 geometric and statistical features extracted from 13,684 engineering drawings. They trained models such as XGBoost, CatBoost, and LightGBM to predict costs with high accuracy, utilizing explainability tools (e.g., SHAP) for insights.

Result: The proposed models achieved nearly 10% mean absolute percentage error across 24 product groups, demonstrating scalability and robustness.

Conclusion: The framework advances cost estimation by reducing lead times, improving transparency, ensuring consistency, and enabling Industry 4.0 integration with actionable insights for design decisions.

Abstract: We present an integrated machine learning framework that transforms how
manufacturing cost is estimated from 2D engineering drawings. Unlike
traditional quotation workflows that require labor-intensive process planning,
our approach about 200 geometric and statistical descriptors directly from
13,684 DWG drawings of automotive suspension and steering parts spanning 24
product groups. Gradient-boosted decision tree models (XGBoost, CatBoost,
LightGBM) trained on these features achieve nearly 10% mean absolute percentage
error across groups, demonstrating robust scalability beyond part-specific
heuristics. By coupling cost prediction with explainability tools such as SHAP,
the framework identifies geometric design drivers including rotated dimension
maxima, arc statistics and divergence metrics, offering actionable insights for
cost-aware design. This end-to-end CAD-to-cost pipeline shortens quotation lead
times, ensures consistent and transparent cost assessments across part families
and provides a deployable pathway toward real-time, ERP-integrated decision
support in Industry 4.0 manufacturing environments.

</details>


### [369] [Local Cluster Cardinality Estimation for Adaptive Mean Shift](https://arxiv.org/abs/2508.12450)
*Étienne Pepin*

Main category: cs.LG

TL;DR: This paper introduces an adaptive mean shift algorithm that adjusts to local scale and cluster cardinality, improving clustering performance and adapting bandwidth based on distance densities.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in clustering methods by creating an algorithm that adapts to varying local scales and cluster cardinalities, overcoming the challenges of KDE-based approaches which focus on localized insights.

Method: Local distance distributions are used to estimate cluster cardinality by identifying minima in density. From these cardinality estimates, the algorithm dynamically adjusts bandwidth and kernel radius thresholds during mean shift execution.

Result: The proposed method outperformed a recent adaptive mean shift algorithm on its original dataset and showed robust performance across broader benchmarks.

Conclusion: This adaptive approach enhances mean shift clustering by providing improved adaptability and broader applicability, outperforming existing methods.

Abstract: This article presents an adaptive mean shift algorithm designed for datasets
with varying local scale and cluster cardinality. Local distance distributions,
from a point to all others, are used to estimate the cardinality of the local
cluster by identifying a local minimum in the density of the distance
distribution. Based on these cardinality estimates, local cluster parameters
are then computed for the entire cluster in contrast to KDE-based methods,
which provide insight only into localized regions of the cluster. During the
mean shift execution, the cluster cardinality estimate is used to adaptively
adjust the bandwidth and the mean shift kernel radius threshold. Our algorithm
outperformed a recently proposed adaptive mean shift method on its original
dataset and demonstrated competitive performance on a broader clustering
benchmark.

</details>


### [370] [Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX](https://arxiv.org/abs/2508.12485)
*Aayush Gupta,Arpit Bhayani*

Main category: cs.LG

TL;DR: This paper introduces Cold-RL, a reinforcement learning-based eviction policy for NGINX that significantly improves cache hit ratios while keeping latency within strict budgets.


<details>
  <summary>Details</summary>
Motivation: Traditional eviction policies like LRU in web proxies are size-agnostic and struggle with periodic bursts and mixed object sizes, leading to inefficiencies.

Method: Cold-RL uses a dueling Deep Q-Network to replace the LRU's forced-expire path, sampling K objects and extracting features like age, size, and hit count to determine evictions. Training is conducted offline using NGINX logs and a reward system based on object hits within TTL expiry.

Result: Cold-RL improves cache hit ratios significantly, such as a 146% gain over classical baselines with a 25 MB cache. It maintains latency and CPU overhead within acceptable limits.

Conclusion: Cold-RL demonstrates that reinforcement learning-based eviction can outperform classical methods, achieving better hit ratios and maintaining eviction latency within strict limits, making it a viable solution for real-world NGINX integration.

Abstract: Web proxies such as NGINX commonly rely on least-recently-used (LRU)
eviction, which is size agnostic and can thrash under periodic bursts and mixed
object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that
replaces LRU's forced-expire path with a dueling Deep Q-Network served by an
ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL
samples the K least-recently-used objects, extracts six lightweight features
(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),
and requests a bitmask of victims; a hard timeout of 500 microseconds triggers
immediate fallback to native LRU. Policies are trained offline by replaying
NGINX access logs through a cache simulator with a simple reward: a retained
object earns one point if it is hit again before TTL expiry. We compare against
LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial
workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,
a 146 percent improvement over the best classical baseline; at 100 MB, from
0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods
(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th
percentile eviction latency within budget. To our knowledge, this is the first
reinforcement learning eviction policy integrated into NGINX with strict SLOs.

</details>


### [371] [Cost-Aware Contrastive Routing for LLMs](https://arxiv.org/abs/2508.12491)
*Reza Shirkavand,Shangqian Gao,Peiran Yu,Heng Huang*

Main category: cs.LG

TL;DR: The paper proposes Cost-Spectrum Contrastive Routing (CSCR), a lightweight method allowing efficient and cost-aware model selection for routing prompts across diverse language models.


<details>
  <summary>Details</summary>
Motivation: Existing routing methods for language models are inefficient, neglect prompt-specific contexts, rely on exhaustive profiling, and fail in dynamic expert settings. This study aims to address these limitations.

Method: The authors developed CSCR by embedding prompts and models into a shared space using compact metrics like logit footprints and perplexity fingerprints, facilitating fast and dynamic cost-sensitive routing through k-NN lookup without retraining.

Result: CSCR improves accuracy-cost tradeoff by up to 25%, adapts effectively to unseen models and prompts, and achieves low latency with microsecond-level response times.

Conclusion: CSCR is a robust and efficient framework for routing prompts across diverse language models, demonstrating cost-aware decisions and better performance on benchmarks compared to traditional methods.

Abstract: We study cost-aware routing for large language models across diverse and
dynamic pools of models. Existing approaches often overlook prompt-specific
context, rely on expensive model profiling, assume a fixed set of experts, or
use inefficient trial-and-error strategies. We introduce Cost-Spectrum
Contrastive Routing (CSCR), a lightweight framework that maps both prompts and
models into a shared embedding space to enable fast, cost-sensitive selection.
CSCR uses compact, fast-to-compute logit footprints for open-source models and
perplexity fingerprints for black-box APIs. A contrastive encoder is trained to
favor the cheapest accurate expert within adaptive cost bands. At inference
time, routing reduces to a single k-NN lookup via a FAISS index, requiring no
retraining when the expert pool changes and enabling microsecond latency.
Across multiple benchmarks, CSCR consistently outperforms baselines, improving
the accuracy-cost tradeoff by up to 25%, while generalizing robustly to unseen
LLMs and out-of-distribution prompts.

</details>


### [372] [Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference](https://arxiv.org/abs/2508.12511)
*Denis Blessing,Julius Berner,Lorenz Richter,Carles Domingo-Enrich,Yuanqi Du,Arash Vahdat,Gerhard Neumann*

Main category: cs.LG

TL;DR: This paper presents a novel approach for solving stochastic optimal control problems using trust regions to improve the efficiency of optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional methods face challenges when the target measure is significantly different from the prior, making optimization difficult and performance suboptimal.

Method: The authors introduce a geometric annealing process guided by trust regions, iteratively solving constrained problems to gradually approach the target measure.

Result: The proposed trust region-based approach showed significant performance improvement across various applications, including sampling tasks and diffusion model fine-tuning.

Conclusion: Trust regions provide a systematic and principled way to optimize annealing paths, enabling efficient solutions to challenging control problems.

Abstract: Solving stochastic optimal control problems with quadratic control costs can
be viewed as approximating a target path space measure, e.g. via gradient-based
optimization. In practice, however, this optimization is challenging in
particular if the target measure differs substantially from the prior. In this
work, we therefore approach the problem by iteratively solving constrained
problems incorporating trust regions that aim for approaching the target
measure gradually in a systematic way. It turns out that this trust region
based strategy can be understood as a geometric annealing from the prior to the
target measure, where, however, the incorporated trust regions lead to a
principled and educated way of choosing the time steps in the annealing path.
We demonstrate in multiple optimal control applications that our novel method
can improve performance significantly, including tasks in diffusion-based
sampling, transition path sampling, and fine-tuning of diffusion models.

</details>


### [373] [Results of the NeurIPS 2023 Neural MMO Competition on Multi-task Reinforcement Learning](https://arxiv.org/abs/2508.12524)
*Joseph Suárez,Kyoung Whan Choe,David Bloomin,Jianming Gao,Yunkun Li,Yao Feng,Saidinesh Pola,Kun Zhang,Yonghui Zhu,Nikhil Pinnaparaju,Hao Xiang Li,Nishaanth Kanna,Daniel Scott,Ryan Sullivan,Rose S. Shuman,Lucas de Alcântara,Herbie Bradley,Kirsty You,Bo Wu,Yuhao Jiang,Qimai Li,Jiaxin Chen,Louis Castricato,Xiaolong Zhu,Phillip Isola*

Main category: cs.LG

TL;DR: The NeurIPS 2023 Neural MMO Competition attracted 200+ participants, where the top solution achieved 4x baseline scores within 8 hours of GPU training. All resources, weights, and code are open-sourced.


<details>
  <summary>Details</summary>
Motivation: To advance generalizable AI policies for complex environments like Neural MMO and evaluate them through a competitive platform.

Method: Participants developed and trained goal-conditional policies to handle unseen tasks, maps, and opponents, enabling robustness and adaptability.

Result: The top participant achieved results significantly outpacing the baseline by 4x within limited training time on high-performance hardware.

Conclusion: The competition successfully demonstrated advancements in generalizable AI policies, with results and tools openly shared to foster further research.

Abstract: We present the results of the NeurIPS 2023 Neural MMO Competition, which
attracted over 200 participants and submissions. Participants trained
goal-conditional policies that generalize to tasks, maps, and opponents never
seen during training. The top solution achieved a score 4x higher than our
baseline within 8 hours of training on a single 4090 GPU. We open-source
everything relating to Neural MMO and the competition under the MIT license,
including the policy weights and training code for our baseline and for the top
submissions.

</details>


### [374] [Rethinking Safety in LLM Fine-tuning: An Optimization Perspective](https://arxiv.org/abs/2508.12531)
*Minseon Kim,Jin Myung Kwak,Lama Alssum,Bernard Ghanem,Philip Torr,David Krueger,Fazl Barez,Adel Bibi*

Main category: cs.LG

TL;DR: Fine-tuning language models can maintain safety without specialized interventions by optimizing hyperparameters and using an EMA momentum technique.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the belief that fine-tuning of language models inevitably compromises their safety.

Method: The authors propose optimizing training parameters (e.g., learning rate, batch size) and introduce an exponential moving average (EMA) technique to stabilize optimization paths.

Result: Unsafe responses were reduced from 16% to about 5%, maintaining utility performance, and avoiding the need for additional safety datasets.

Conclusion: Safety issues during fine-tuning can be minimized through better optimization choices, offering practical guidelines for safer model adaptation.

Abstract: Fine-tuning language models is commonly believed to inevitably harm their
safety, i.e., refusing to respond to harmful user requests, even when using
harmless datasets, thus requiring additional safety measures. We challenge this
belief through systematic testing, showing that poor optimization choices,
rather than inherent trade-offs, often cause safety problems, measured as
harmful responses to adversarial prompts. By properly selecting key training
hyper-parameters, e.g., learning rate, batch size, and gradient steps, we
reduce unsafe model responses from 16\% to approximately 5\%, as measured by
keyword matching, while maintaining utility performance. Based on this
observation, we propose a simple exponential moving average (EMA) momentum
technique in parameter space that preserves safety performance by creating a
stable optimization path and retains the original pre-trained model's safety
properties. Our experiments on the Llama families across multiple datasets
(Dolly, Alpaca, ORCA) demonstrate that safety problems during fine-tuning can
largely be avoided without specialized interventions, outperforming existing
approaches that require additional safety data while offering practical
guidelines for maintaining both model performance and safety during adaptation.

</details>


### [375] [Illuminating LLM Coding Agents: Visual Analytics for Deeper Understanding and Enhancement](https://arxiv.org/abs/2508.12555)
*Junpeng Wang,Yuzhong Chen,Menghai Pan,Chin-Chia Michael Yeh,Mahashweta Das*

Main category: cs.LG

TL;DR: The paper presents a visual analytics system to help ML scientists effectively review and adjust coding agents' iterative problem-solving behaviors.


<details>
  <summary>Details</summary>
Motivation: ML scientists face challenges in efficiently tracking coding agent processes, debugging, comparing iterations, and analyzing improvements due to manual inspection inefficiency.

Method: The paper introduces a system focusing on the AIDE framework for comparative analysis at three levels: Code-Level Analysis, Process-Level Analysis, and LLM-Level Analysis.

Result: Case studies on Kaggle competitions illustrate how the system offers insights into coding agents' iterative problem-solving, enhancing debugging and prompt engineering.

Conclusion: The system provides structured understanding and greater efficiency in analyzing coding agent behaviors, addressing key challenges faced by ML scientists.

Abstract: Coding agents powered by large language models (LLMs) have gained traction
for automating code generation through iterative problem-solving with minimal
human involvement. Despite the emergence of various frameworks, e.g.,
LangChain, AutoML, and AIDE, ML scientists still struggle to effectively review
and adjust the agents' coding process. The current approach of manually
inspecting individual outputs is inefficient, making it difficult to track code
evolution, compare coding iterations, and identify improvement opportunities.
To address this challenge, we introduce a visual analytics system designed to
enhance the examination of coding agent behaviors. Focusing on the AIDE
framework, our system supports comparative analysis across three levels: (1)
Code-Level Analysis, which reveals how the agent debugs and refines its code
over iterations; (2) Process-Level Analysis, which contrasts different
solution-seeking processes explored by the agent; and (3) LLM-Level Analysis,
which highlights variations in coding behavior across different LLMs. By
integrating these perspectives, our system enables ML scientists to gain a
structured understanding of agent behaviors, facilitating more effective
debugging and prompt engineering. Through case studies using coding agents to
tackle popular Kaggle competitions, we demonstrate how our system provides
valuable insights into the iterative coding process.

</details>


### [376] [Deep Learning-Based Financial Time Series Forecasting via Sliding Window and Variational Mode Decomposition](https://arxiv.org/abs/2508.12565)
*Luke Li*

Main category: cs.LG

TL;DR: The paper presents a forecasting model that combines VMD and sliding window techniques to improve predictions of financial time series using LSTM.


<details>
  <summary>Details</summary>
Motivation: To handle the complexities and non-stationary nature of financial time series for better forecasting performance.

Method: The model applies Variational Mode Decomposition (VMD) to decompose financial data into smoother subcomponents, then uses a deep learning LSTM model for predictions on the processed data.

Result: LSTM models trained on VMD-preprocessed data performed better and demonstrated greater stability compared to models trained on raw time series data.

Conclusion: Decomposing financial time series using VMD enhances prediction performance and stability when coupled with deep learning methods like LSTM.

Abstract: To address the complexity of financial time series, this paper proposes a
forecasting model combining sliding window and variational mode decomposition
(VMD) methods. Historical stock prices and relevant market indicators are used
to construct datasets. VMD decomposes non-stationary financial time series into
smoother subcomponents, improving model adaptability. The decomposed data is
then input into a deep learning model for prediction. The study compares the
forecasting effects of an LSTM model trained on VMD-processed sequences with
those using raw time series, demonstrating better performance and stability.

</details>


### [377] [Deep Learning Model for Amyloidogenicity Prediction using a Pre-trained Protein LLM](https://arxiv.org/abs/2508.12575)
*Zohra Yagoub,Hafida Bouziane*

Main category: cs.LG

TL;DR: The paper develops a method using a pretrained protein large language model with LSTM and GRU for predicting amyloidogenic regions, achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: Predicting amyloidogenicity in peptides and proteins is a critical task in bioinformatics due to its implications in diseases. Many methods rely on evolutionary motifs and amino acid properties, with sequence-based features showing promise.

Method: The study employs a pretrained protein large language model, utilizing bidirectional LSTM and GRU architectures to analyze protein sequences and predict amyloidogenic regions.

Result: The proposed method achieved an accuracy of 84.5% in 10-fold cross-validation and 83% on the test dataset, demonstrating competitive performance.

Conclusion: The findings underline the effectiveness of large language models in improving amyloid prediction, showcasing their potential in bioinformatics applications.

Abstract: The prediction of amyloidogenicity in peptides and proteins remains a focal
point of ongoing bioinformatics. The crucial step in this field is to apply
advanced computational methodologies. Many recent approaches to predicting
amyloidogenicity within proteins are highly based on evolutionary motifs and
the individual properties of amino acids. It is becoming increasingly evident
that the sequence information-based features show high predictive performance.
Consequently, our study evaluated the contextual features of protein sequences
obtained from a pretrained protein large language model leveraging
bidirectional LSTM and GRU to predict amyloidogenic regions in peptide and
protein sequences. Our method achieved an accuracy of 84.5% on 10-fold
cross-validation and an accuracy of 83% in the test dataset. Our results
demonstrate competitive performance, highlighting the potential of LLMs in
enhancing the accuracy of amyloid prediction.

</details>


### [378] [Widening the Network Mitigates the Impact of Data Heterogeneity on FedAvg](https://arxiv.org/abs/2508.12576)
*Like Jian,Dong Liu*

Main category: cs.LG

TL;DR: This paper studies the convergence of overparameterized Federated Averaging (FedAvg) in the context of gradient descent, demonstrating that data heterogeneity issues fade away as neural network width increases, eventually matching centralized learning in the infinite-width regime.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in federated learning caused by clients' heterogeneous data distributions, which complicates global model generalization.

Method: The paper explores theoretical aspects of FedAvg with gradient descent for overparameterized neural networks, analyzing the diminishing effect of data heterogeneity as network width increases and transitioning to linear model behavior in infinite-width scenarios.

Result: They prove that FedAvg performs equivalently to centralized learning in terms of generalization when neural network width approaches infinity, supported by experimental validations across various setups.

Conclusion: FedAvg achieves performance parity with centralized learning in the infinite-width regime, providing insights into alleviating data heterogeneity issues in federated learning.

Abstract: Federated learning (FL) enables decentralized clients to train a model
collaboratively without sharing local data. A key distinction between FL and
centralized learning is that clients' data are non-independent and identically
distributed, which poses significant challenges in training a global model that
generalizes well across heterogeneous local data distributions. In this paper,
we analyze the convergence of overparameterized FedAvg with gradient descent
(GD). We prove that the impact of data heterogeneity diminishes as the width of
neural networks increases, ultimately vanishing when the width approaches
infinity. In the infinite-width regime, we further prove that both the global
and local models in FedAvg behave as linear models, and that FedAvg achieves
the same generalization performance as centralized learning with the same
number of GD iterations. Extensive experiments validate our theoretical
findings across various network architectures, loss functions, and optimization
methods.

</details>


### [379] [Energy-Efficient Wireless LLM Inference via Uncertainty and Importance-Aware Speculative Decoding](https://arxiv.org/abs/2508.12590)
*Jihoon Park,Seungeun Oh,Seong-Lyun Kim*

Main category: cs.LG

TL;DR: The paper introduces a token-level mechanism for hybrid language models (HLMs) that reduces energy consumption and communication costs without compromising accuracy by selectively uploading informative tokens.


<details>
  <summary>Details</summary>
Motivation: To develop an energy-efficient solution for deploying large language models (LLMs) in resource-constrained edge environments while maintaining performance.

Method: The proposed method applies a filtering mechanism that utilizes epistemic uncertainty and attention-based importance to selectively upload only the most informative tokens to the powerful cloud-based LLM.

Result: Experiments demonstrate improvements in energy savings (up to 43.6%), BERTScore (up to 87.5%), and throughput (from 0.36 to 0.40) compared to standard HLM and previous baselines.

Conclusion: Leveraging token-level filtering, the proposed method enables accurate and energy-efficient deployment of hybrid language models in bandwidth-constrained environments, offering significant reductions in energy and communication costs.

Abstract: To address the growing demand for on-device LLM inference in
resource-constrained environments, hybrid language models (HLM) have emerged,
combining lightweight local models with powerful cloud-based LLMs. Recent
studies on HLM have primarily focused on improving accuracy and latency, while
often overlooking communication and energy efficiency. We propose a token-level
filtering mechanism for an energy-efficient importance- and uncertainty-aware
HLM inference that leverages both epistemic uncertainty and attention-based
importance. Our method opportunistically uploads only informative tokens,
reducing LLM usage and communication costs. Experiments with TinyLlama-1.1B and
LLaMA-2-7B demonstrate that our method achieves up to 87.5% BERT Score and
token throughput of 0.37 tokens/sec while saving the energy consumption by
40.7% compared to standard HLM. Furthermore, compared to our previous U-HLM
baseline, our method improves BERTScore from 85.8% to 87.0%, energy savings
from 31.6% to 43.6%, and throughput from 0.36 to 0.40. This approach enables an
energy-efficient and accurate deployment of LLMs in bandwidth-constrained edge
environments.

</details>


### [380] [Physics-informed deep operator network for traffic state estimation](https://arxiv.org/abs/2508.12593)
*Zhihao Li,Ting Wang,Guojian Zou,Ruofei Wang,Ye Li*

Main category: cs.LG

TL;DR: The paper presents a physics-informed deep operator network (PI-DeepONet) approach for traffic state estimation, showcasing its superiority over traditional methods using experimental datasets.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges in traffic state estimation by reformulating it as an operator learning problem, integrating physical traffic flow principles to enhance accuracy and robustness.

Method: The researchers developed PI-DeepONet, a parameterized neural operator trained to map sparse inputs into full spatiotemporal traffic state fields, embedding traffic flow models and fundamental diagrams within the operator learning process.

Result: Experiments on the NGSIM dataset demonstrate that the PI-DeepONet framework outperforms existing baselines in capturing congestion propagation, spatial correlations, and temporal evolution.

Conclusion: PI-DeepONet offers a robust and physically consistent framework for traffic state estimation, providing insights into optimal model strategies and input generation techniques.

Abstract: Traffic state estimation (TSE) fundamentally involves solving
high-dimensional spatiotemporal partial differential equations (PDEs) governing
traffic flow dynamics from limited, noisy measurements. While Physics-Informed
Neural Networks (PINNs) enforce PDE constraints point-wise, this paper adopts a
physics-informed deep operator network (PI-DeepONet) framework that
reformulates TSE as an operator learning problem. Our approach trains a
parameterized neural operator that maps sparse input data to the full
spatiotemporal traffic state field, governed by the traffic flow conservation
law. Crucially, unlike PINNs that enforce PDE constraints point-wise,
PI-DeepONet integrates traffic flow conservation model and the fundamental
diagram directly into the operator learning process, ensuring physical
consistency while capturing congestion propagation, spatial correlations, and
temporal evolution. Experiments on the NGSIM dataset demonstrate superior
performance over state-of-the-art baselines. Further analysis reveals insights
into optimal function generation strategies and branch network complexity.
Additionally, the impact of input function generation methods and the number of
functions on model performance is explored, highlighting the robustness and
efficacy of proposed framework.

</details>


### [381] [FLARE: Fast Low-rank Attention Routing Engine](https://arxiv.org/abs/2508.12594)
*Vedant Puri,Aditya Joglekar,Kevin Ferguson,Yu-hsuan Chen,Yongjie Jessica Zhang,Levent Burak Kara*

Main category: cs.LG

TL;DR: FLARE introduces a linear complexity self-attention mechanism, improving scalability and accuracy for large-scale problems.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity in traditional self-attention limits its application for large unstructured meshes.

Method: FLARE routes self-attention through fixed-length latent sequences using learnable query tokens, achieving linear complexity.

Result: FLARE demonstrates scalability to large problems and performs better than state-of-the-art neural PDE surrogates on benchmark tests.

Conclusion: FLARE offers an efficient and accurate solution for large-scale self-attention, fostering advancements in machine learning applications.

Abstract: The quadratic complexity of self-attention limits its applicability and
scalability on large unstructured meshes. We introduce Fast Low-rank Attention
Routing Engine (FLARE), a linear complexity self-attention mechanism that
routes attention through fixed-length latent sequences. Each attention head
performs global communication among $N$ tokens by projecting the input sequence
onto a fixed length latent sequence of $M \ll N$ tokens using learnable query
tokens. By routing attention through a bottleneck sequence, FLARE learns a
low-rank form of attention that can be applied at $O(NM)$ cost. FLARE not only
scales to unprecedented problem sizes, but also delivers superior accuracy
compared to state-of-the-art neural PDE surrogates across diverse benchmarks.
We also release a new additive manufacturing dataset to spur further research.
Our code is available at https://github.com/vpuri3/FLARE.py.

</details>


### [382] [Constructing Invariant and Equivariant Operations by Symmetric Tensor Network](https://arxiv.org/abs/2508.12596)
*Meng Zhang,Chao Wang,Hao Zhang,Shaojun Dong,Lixin He*

Main category: cs.LG

TL;DR: The paper develops a systematic method to construct invariant and equivariant operations for neural networks handling tensors of various types and ranks, featuring graphical representations via symmetric tensor networks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing symmetry-aware neural networks crucial for the field of geometric deep learning.

Method: The proposed method constructs invariant and equivariant operations for Cartesian and spherical tensors using symmetric tensor networks, simplifying both mathematical proofs and constructions.

Result: The method was successfully applied to design equivariant message interactions for geometry graph neural networks and to develop a model for learning material constitutive laws.

Conclusion: This systematic approach advances the design of symmetry-aware neural networks, providing tools for both theoretical and practical applications in geometric deep learning.

Abstract: Design of neural networks that incorporate symmetry is crucial for geometric
deep learning. Central to this effort is the development of invariant and
equivariant operations. This works presents a systematic method for
constructing valid invariant and equivariant operations. It can handle inputs
and outputs in the form of Cartesian tensors with different rank, as well as
spherical tensors with different types. In addition, our method features a
graphical representation utilizing the symmetric tensor network, which
simplifies both the proofs and constructions related to invariant and
equivariant functions. We also apply this approach to design the equivariant
interaction message for the geometry graph neural network, and equivariant
machine learning model to learn the constitutive law of materials.

</details>


### [383] [A Hybrid Surrogate for Electric Vehicle Parameter Estimation and Power Consumption via Physics-Informed Neural Operators](https://arxiv.org/abs/2508.12602)
*Hansol Lim,Jongseong Brad Choi,Jee Won Lee,Haeseong Jeoung,Minkyu Han*

Main category: cs.LG

TL;DR: The paper introduces a hybrid surrogate model using Spectral Parameter Operator to estimate electric vehicle (EV) parameters and power consumption based on speed and acceleration inputs.


<details>
  <summary>Details</summary>
Motivation: To develop an interpretable and generalizable surrogate model for EV parameter estimation and battery power prediction, addressing key challenges in path optimization, diagnostics, and health management.

Method: The study combines a novel Fourier Neural Operator-based architecture with a differentiable physics module to estimate dynamic EV parameters and battery power, all without requiring a separate physics-residual loss.

Result: When tested on real-world Tesla Model 3, Tesla Model S, and Kia EV9 data, the model achieved errors as low as 0.2kW for Tesla vehicles and 0.8kW for the Kia EV9, showcasing high accuracy across various conditions.

Conclusion: The framework is modular, interpretable, and generalizable, making it highly practical for applications in eco-routing, diagnostics, and vehicle health management.

Abstract: We present a hybrid surrogate model for electric vehicle parameter estimation
and power consumption. We combine our novel architecture Spectral Parameter
Operator built on a Fourier Neural Operator backbone for global context and a
differentiable physics module in the forward pass. From speed and acceleration
alone, it outputs time-varying motor and regenerative braking efficiencies, as
well as aerodynamic drag, rolling resistance, effective mass, and auxiliary
power. These parameters drive a physics-embedded estimate of battery power,
eliminating any separate physics-residual loss. The modular design lets
representations converge to physically meaningful parameters that reflect the
current state and condition of the vehicle. We evaluate on real-world logs from
a Tesla Model 3, Tesla Model S, and the Kia EV9. The surrogate achieves a mean
absolute error of 0.2kW (about 1% of average traction power at highway speeds)
for Tesla vehicles and about 0.8kW on the Kia EV9. The framework is
interpretable, and it generalizes well to unseen conditions, and sampling
rates, making it practical for path optimization, eco-routing, on-board
diagnostics, and prognostics health management.

</details>


### [384] [SSPO: Self-traced Step-wise Preference Optimization for Process Supervision and Reasoning Compression](https://arxiv.org/abs/2508.12604)
*Yuyang Xu,Yi Cheng,Haochao Ying,Zhuoyun Du,Renjun Hu,Xing Shi,Wei Lin,Jian Wu*

Main category: cs.LG

TL;DR: The paper introduces SSPO, a method to optimize reasoning in Large Language Models without auxiliary models or manual annotations, by leveraging the model's own stepwise preferences.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of verbose and error-prone reasoning in pretrained Large Language Models without relying on computationally heavy post-training methods like RL with CoT reasoning.

Method: SSPO is a reinforcement learning framework that uses the model's own stepwise preferences, instead of auxiliary models or manual annotations, to optimize reasoning at a fine-grained level.

Result: Experiments show that SSPO generates more accurate and concise reasoning sequences, reducing overthinking behaviors across different domains and languages.

Conclusion: SSPO is a lightweight and effective framework that maintains performance while simplifying reasoning processes in Large Language Models.

Abstract: Test-time scaling has proven effective in further enhancing the performance
of pretrained Large Language Models (LLMs). However, mainstream post-training
methods (i.e., reinforcement learning (RL) with chain-of-thought (CoT)
reasoning) often incur substantial computational overhead due to auxiliary
models and overthinking. In this paper, we empirically reveal that the
incorrect answers partially stem from verbose reasoning processes lacking
correct self-fix, where errors accumulate across multiple reasoning steps. To
this end, we propose Self-traced Step-wise Preference Optimization (SSPO), a
pluggable RL process supervision framework that enables fine-grained
optimization of each reasoning step. Specifically, SSPO requires neither
auxiliary models nor stepwise manual annotations. Instead, it leverages
step-wise preference signals generated by the model itself to guide the
optimization process for reasoning compression. Experiments demonstrate that
the generated reasoning sequences from SSPO are both accurate and succinct,
effectively mitigating overthinking behaviors without compromising model
performance across diverse domains and languages.

</details>


### [385] [How can we trust opaque systems? Criteria for robust explanations in XAI](https://arxiv.org/abs/2508.12623)
*Florian J. Boge,Annika Schuster*

Main category: cs.LG

TL;DR: The paper examines the challenges in making deep learning (DL) algorithms explainable and argues for dual robustness criteria—explanatory robustness (ER) and explanation method robustness (EMR)—to ensure trustworthy explanations.


<details>
  <summary>Details</summary>
Motivation: The growing reliance on DL algorithms has led to concerns over their opacity, creating demand for explanations that accurately reflect their decision-making processes and build trust.

Method: The authors propose a framework that establishes criteria for explanatory robustness (ER) and explanation method robustness (EMR) to assess and enhance the trustworthiness of eXplainable Artificial Intelligence (XAI) methods.

Result: The study formalizes ER and EMR criteria and provides a theoretical framework to evaluate XAI methods, offering guidelines and application cases for their development.

Conclusion: For XAI methods to be trustworthy, they must meet both ER and EMR criteria, ensuring their robustness across and within explanation contexts. Further exploration is encouraged to enhance DL explainability.

Abstract: Deep learning (DL) algorithms are becoming ubiquitous in everyday life and in
scientific research. However, the price we pay for their impressively accurate
predictions is significant: their inner workings are notoriously opaque - it is
unknown to laypeople and researchers alike what features of the data a DL
system focuses on and how it ultimately succeeds in predicting correct outputs.
A necessary criterion for trustworthy explanations is that they should reflect
the relevant processes the algorithms' predictions are based on. The field of
eXplainable Artificial Intelligence (XAI) presents promising methods to create
such explanations. But recent reviews about their performance offer reasons for
skepticism. As we will argue, a good criterion for trustworthiness is
explanatory robustness: different XAI methods produce the same explanations in
comparable contexts. However, in some instances, all methods may give the same,
but still wrong, explanation. We therefore argue that in addition to
explanatory robustness (ER), a prior requirement of explanation method
robustness (EMR) has to be fulfilled by every XAI method. Conversely, the
robustness of an individual method is in itself insufficient for
trustworthiness. In what follows, we develop and formalize criteria for ER as
well as EMR, providing a framework for explaining and establishing trust in DL
algorithms. We also highlight interesting application cases and outline
directions for future work.

</details>


### [386] [FlowMol3: Flow Matching for 3D De Novo Small-Molecule Generation](https://arxiv.org/abs/2508.12629)
*Ian Dunn,David R. Koes*

Main category: cs.LG

TL;DR: FlowMol3 enhances molecule generation capabilities with almost 100% validity, reduced complexity, and minimal computation cost using innovative techniques.


<details>
  <summary>Details</summary>
Motivation: Current generative models for molecule design have limitations in generating realistic molecular structures efficiently, which restricts advances in chemical discovery.

Method: FlowMol3 introduces architecture-agnostic techniques like self-conditioning, fake atoms, and train-time geometry distortion, while using a flow matching model for molecule generation.

Result: FlowMol3 outperforms previous versions and competitors by achieving near-perfect molecular validity, better functional group reproduction, and drastically fewer learnable parameters.

Conclusion: The strategies demonstrated by FlowMol3 offer promising paths for stable and quality improvements in molecular generative models, broadening their application scope.

Abstract: A generative model capable of sampling realistic molecules with desired
properties could accelerate chemical discovery across a wide range of
applications. Toward this goal, significant effort has focused on developing
models that jointly sample molecular topology and 3D structure. We present
FlowMol3, an open-source, multi-modal flow matching model that advances the
state of the art for all-atom, small-molecule generation. Its substantial
performance gains over previous FlowMol versions are achieved without changes
to the graph neural network architecture or the underlying flow matching
formulation. Instead, FlowMol3's improvements arise from three
architecture-agnostic techniques that incur negligible computational cost:
self-conditioning, fake atoms, and train-time geometry distortion. FlowMol3
achieves nearly 100% molecular validity for drug-like molecules with explicit
hydrogens, more accurately reproduces the functional group composition and
geometry of its training data, and does so with an order of magnitude fewer
learnable parameters than comparable methods. We hypothesize that these
techniques mitigate a general pathology affecting transport-based generative
models, enabling detection and correction of distribution drift during
inference. Our results highlight simple, transferable strategies for improving
the stability and quality of diffusion- and flow-based molecular generative
models.

</details>


### [387] [Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery](https://arxiv.org/abs/2508.12650)
*Jiyeon Kang,Songseong Kim,Chanhui Lee,Doyeong Hwang,Joanie Hayoun Chung,Yunkyung Ko,Sumin Lee,Sungwoong Kim,Sungbin Lim*

Main category: cs.LG

TL;DR: The paper introduces Score-informed Neural Operator (SciNO), a generative model for causal discovery that improves numerical stability and scalability, achieving better performance than DiffAN by reducing order divergence significantly.


<details>
  <summary>Details</summary>
Motivation: Existing causal discovery methods face challenges in scalability and stability. Specifically, the use of Stein gradient estimators and second-order derivatives in DiffAN is computationally expensive, memory-intensive, and numerically unstable, motivating the need for a robust and efficient alternative.

Method: The authors propose SciNO, a probabilistic generative model that operates in smooth function spaces. SciNO approximates the Hessian diagonal of log-densities, preserving structural information, and provides accurate and stable estimates. Additionally, a probabilistic control algorithm integrates SciNO's outputs with autoregressive model priors for causal reasoning.

Result: Empirical evaluations demonstrate that SciNO reduces order divergence by 42.7% on synthetic graphs and 31.5% on real-world datasets compared to existing methods like DiffAN. SciNO proves memory-efficient and scalable.

Conclusion: SciNO offers a significant advancement in causal discovery and reasoning by addressing critical issues like stability and computational burden. The method improves causal reasoning in LLMs without requiring additional fine-tuning or prompt engineering, making it an efficient and adaptable tool.

Abstract: Ordering-based approaches to causal discovery identify topological orders of
causal graphs, providing scalable alternatives to combinatorial search methods.
Under the Additive Noise Model (ANM) assumption, recent causal ordering methods
based on score matching require an accurate estimation of the Hessian diagonal
of the log-densities. However, previous approaches mainly use Stein gradient
estimators, which are computationally expensive and memory-intensive. Although
DiffAN addresses these limitations by substituting kernel-based estimates with
diffusion models, it remains numerically unstable due to the second-order
derivatives of score models. To alleviate these problems, we propose
Score-informed Neural Operator (SciNO), a probabilistic generative model in
smooth function spaces designed to stably approximate the Hessian diagonal and
to preserve structural information during the score modeling. Empirical results
show that SciNO reduces order divergence by 42.7% on synthetic graphs and by
31.5% on real-world datasets on average compared to DiffAN, while maintaining
memory efficiency and scalability. Furthermore, we propose a probabilistic
control algorithm for causal reasoning with autoregressive models that
integrates SciNO's probability estimates with autoregressive model priors,
enabling reliable data-driven causal ordering informed by semantic information.
Consequently, the proposed method enhances causal reasoning abilities of LLMs
without additional fine-tuning or prompt engineering.

</details>


### [388] [Robust Federated Learning under Adversarial Attacks via Loss-Based Client Clustering](https://arxiv.org/abs/2508.12672)
*Emmanouil Kritharakis,Dusan Jakovetic,Antonios Makris,Konstantinos Tserpes*

Main category: cs.LG

TL;DR: The paper explores a federated learning setup focusing on handling Byzantine attacks, proposing an approach resilient even with only two honest participants.


<details>
  <summary>Details</summary>
Motivation: The need to handle adversarial attacks in federated learning setups where clients may not be trusted while the server and at least one client are.

Method: A theoretical approach that ensures model training effectiveness under strong adversarial conditions, complemented by experimental validation.

Result: The proposed algorithm outperformed existing methods under various adversarial strategies across benchmark datasets.

Conclusion: This approach successfully mitigates Byzantine attacks in federated learning, showcasing robustness with minimal honest participants.

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients without sharing private data. We consider FL scenarios wherein FL
clients are subject to adversarial (Byzantine) attacks, while the FL server is
trusted (honest) and has a trustworthy side dataset. This may correspond to,
e.g., cases where the server possesses trusted data prior to federation, or to
the presence of a trusted client that temporarily assumes the server role. Our
approach requires only two honest participants, i.e., the server and one
client, to function effectively, without prior knowledge of the number of
malicious clients. Theoretical analysis demonstrates bounded optimality gaps
even under strong Byzantine attacks. Experimental results show that our
algorithm significantly outperforms standard and robust FL baselines such as
Mean, Trimmed Mean, Median, Krum, and Multi-Krum under various attack
strategies including label flipping, sign flipping, and Gaussian noise addition
across MNIST, FMNIST, and CIFAR-10 benchmarks using the Flower framework.

</details>


### [389] [Deploying Models to Non-participating Clients in Federated Learning without Fine-tuning: A Hypernetwork-based Approach](https://arxiv.org/abs/2508.12673)
*Yuhao Zhou,Jindi Lv,Yuxin Tian,Dan Si,Qing Ye,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperFedZero addresses data heterogeneity in Federated Learning by dynamically creating specialized models for non-participating clients using a hypernetwork and distribution-aware embeddings, ensuring better adaptability with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the persistent issue of data heterogeneity in Federated Learning, particularly for non-participating clients facing in-domain distribution shifts and resource constraints, which limits the applicability of existing methods.

Method: The researchers propose HyperFedZero, a framework utilizing a hypernetwork conditioned on distribution-aware embeddings. It integrates inductive biases in the model's forward pass, employs a NoisyEmbed-enhanced extractor with Balancing Penalty to avoid feature collapse, and generates specialized models chunk-by-chunk for non-participating clients.

Result: Experiments on various datasets and models showed that HyperFedZero significantly outperformed other methods. It delivered superior results while maintaining low computational, storage, and communication overheads.

Conclusion: HyperFedZero effectively resolves the challenge of adapting models to non-participating clients with unique data distributions in Federated Learning. Its components are validated as integral to ensuring meaningful model adaptability with efficiency.

Abstract: Federated Learning (FL) has emerged as a promising paradigm for
privacy-preserving collaborative learning, yet data heterogeneity remains a
critical challenge. While existing methods achieve progress in addressing data
heterogeneity for participating clients, they fail to generalize to
non-participating clients with in-domain distribution shifts and resource
constraints. To mitigate this issue, we present HyperFedZero, a novel method
that dynamically generates specialized models via a hypernetwork conditioned on
distribution-aware embeddings. Our approach explicitly incorporates
distribution-aware inductive biases into the model's forward pass, extracting
robust distribution embeddings using a NoisyEmbed-enhanced extractor with a
Balancing Penalty, effectively preventing feature collapse. The hypernetwork
then leverages these embeddings to generate specialized models chunk-by-chunk
for non-participating clients, ensuring adaptability to their unique data
distributions. Extensive experiments on multiple datasets and models
demonstrate HyperFedZero's remarkable performance, surpassing competing methods
consistently with minimal computational, storage, and communication overhead.
Moreover, ablation studies and visualizations further validate the necessity of
each component, confirming meaningful adaptations and validating the
effectiveness of HyperFedZero.

</details>


### [390] [BUILDA: A Thermal Building Data Generation Framework for Transfer Learning](https://arxiv.org/abs/2508.12703)
*Thomas Krug,Fabian Raisch,Dominik Aimer,Markus Wirnsberger,Ferdinand Sigg,Benjamin Schäfer,Benjamin Tischler*

Main category: cs.LG

TL;DR: This paper introduces BuilDa, a framework to generate synthetic thermal building data for transfer learning (TL), addressing the lack of quality and quantity in existing datasets.


<details>
  <summary>Details</summary>
Motivation: There is limited availability of high-quality and extensive thermal building data for transfer learning research in building modeling. Existing data generation methods require expert knowledge in simulation.

Method: BuilDa is a framework that uses a single-zone Modelica model exported as a Functional Mock-up Unit (FMU) for simulation in Python to produce synthetic building thermal data in large volumes without needing deep simulation expertise.

Result: The authors demonstrated BuilDa's capability to generate synthetic data and showed its utility for pretraining and fine-tuning transfer learning models.

Conclusion: BuilDa is effective in addressing data quality and availability issues for TL research and simplifies data generation by removing the need for specialized simulation expertise.

Abstract: Transfer learning (TL) can improve data-driven modeling of building thermal
dynamics. Therefore, many new TL research areas emerge in the field, such as
selecting the right source model for TL. However, these research directions
require massive amounts of thermal building data which is lacking presently.
Neither public datasets nor existing data generators meet the needs of TL
research in terms of data quality and quantity. Moreover, existing data
generation approaches typically require expert knowledge in building
simulation. We present BuilDa, a thermal building data generation framework for
producing synthetic data of adequate quality and quantity for TL research. The
framework does not require profound building simulation knowledge to generate
large volumes of data. BuilDa uses a single-zone Modelica model that is
exported as a Functional Mock-up Unit (FMU) and simulated in Python. We
demonstrate BuilDa by generating data and utilizing it for pretraining and
fine-tuning TL models.

</details>


### [391] [Argos: A Decentralized Federated System for Detection of Traffic Signs in CAVs](https://arxiv.org/abs/2508.12712)
*Seyed Mahdi Haji Seyed Hossein,Alireza Hosseini,Soheil Hajian Manesh,Amirali Shahriary*

Main category: cs.LG

TL;DR: This paper proposes a federated learning framework for traffic sign detection in vehicular networks to address privacy and communication challenges, achieving promising results in model accuracy and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Centralized machine learning approaches pose privacy and communication issues in connected and automated vehicles generating vast sensor data, necessitating decentralized alternatives.

Method: The framework used federated learning with models trained locally on partitioned traffic sign data, aggregated with algorithms (FedProx, FedAdam, FedAVG) in simulations utilizing the Flower framework.

Result: Experiments showed enhanced accuracy (from below 0.1 to over 0.8) with more server rounds, optimal local epochs for efficiency (8–10, ~0.67 accuracy), higher client participation improving accuracy (~0.83), and FedProx performing best with heterogeneous data.

Conclusion: Federated learning offers a scalable and privacy-preserving approach to traffic sign detection in vehicular networks, paving the way for future intelligent transportation system optimization.

Abstract: Connected and automated vehicles generate vast amounts of sensor data daily,
raising significant privacy and communication challenges for centralized
machine learning approaches in perception tasks. This study presents a
decentralized, federated learning framework tailored for traffic sign detection
in vehicular networks to enable collaborative model training without sharing
raw data. The framework partitioned traffic sign classes across vehicles for
specialized local training using lightweight object detectors, aggregated model
parameters via algorithms like FedProx, FedAdam and FedAVG in a simulated
environment with the Flower framework, and evaluated multiple configurations
including varying server rounds, local epochs, client participation fractions,
and data distributions. Experiments demonstrated that increasing server rounds
from 2 to 20 boosted accuracy from below 0.1 to over 0.8, moderate local epochs
(8-10) provided optimal efficiency with accuracies around 0.67, higher client
participation fractions enhanced generalization up to 0.83, FedProx
outperformed other aggregators in handling heterogeneity, non-IID data
distributions reduced performance compared to IID, and training duration
primarily scaled with the number of rounds rather than aggregation strategy. We
conclude that this federated approach may offer a scalable, privacy-preserving
solution for real-world vehicular deployments, potentially guiding future
integrations of robust aggregation and communication optimizations to advance
intelligent transportation systems.

</details>


### [392] [FedSODA: Federated Fine-tuning of LLMs via Similarity Group Pruning and Orchestrated Distillation Alignment](https://arxiv.org/abs/2508.12727)
*Manning Zhu,Songtao Guo,Pengzhan Zhou,Yansong Ning,Chang Han,Dewen Qiao*

Main category: cs.LG

TL;DR: The paper introduces FedSODA, a framework for resource-efficient federated fine-tuning (FFT) of large language models (LLMs). It enables clients to adapt LLMs with minimal computational and memory demands, achieving better efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: FFT is promising for domain-specific adaptation and preserving privacy, but its high computational and memory requirements make it impractical for resource-limited clients.

Method: FedSODA utilizes two key innovations: Similarity Group Pruning (SGP), which removes redundant layers while preserving performance, and Orchestrated Distillation Alignment (ODA), which reduces gradient divergence. It incorporates quantized sub-LLMs and lightweight adapters through QLoRA for efficient FFT.

Result: Experiments on three open-source LLMs show FedSODA reduces communication overhead by 70.6%, decreases storage by 75.6%, and improves task accuracy by 3.1%, proving its effectiveness.

Conclusion: FedSODA delivers a practical solution for resource-constrained FFT applications, enabling efficient and accurate domain-specific adaption of LLMs while lowering resource demands.

Abstract: Federated fine-tuning (FFT) of large language models (LLMs) has recently
emerged as a promising solution to enable domain-specific adaptation while
preserving data privacy. Despite its benefits, FFT on resource-constrained
clients relies on the high computational and memory demands of full-model
fine-tuning, which limits the potential advancement. This paper presents
FedSODA, a resource-efficient FFT framework that enables clients to adapt LLMs
without accessing or storing the full model. Specifically, we first propose a
similarity group pruning (SGP) module, which prunes redundant layers from the
full LLM while retaining the most critical layers to preserve the model
performance. Moreover, we introduce an orchestrated distillation alignment
(ODA) module to reduce gradient divergence between the sub-LLM and the full LLM
during FFT. Through the use of the QLoRA, clients only need to deploy quantized
sub-LLMs and fine-tune lightweight adapters, significantly reducing local
resource requirements. We conduct extensive experiments on three open-source
LLMs across a variety of downstream tasks. The experimental results demonstrate
that FedSODA reduces communication overhead by an average of 70.6%, decreases
storage usage by 75.6%, and improves task accuracy by 3.1%, making it highly
suitable for practical FFT applications under resource constraints.

</details>


### [393] [FedUNet: A Lightweight Additive U-Net Module for Federated Learning with Heterogeneous Models](https://arxiv.org/abs/2508.12740)
*Beomseok Seo,Kichang Lee,JaeYeon Park*

Main category: cs.LG

TL;DR: FedUNet improves Federated Learning by introducing a U-Net-inspired framework for heterogeneous client architectures.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of Federated Learning methods constrained by identical model architectures, making them inapplicable in diverse real-world scenarios.

Method: FedUNet attaches a U-Net-inspired additive module to client backbones, sharing only its compact bottleneck for efficient, architecture-agnostic knowledge transfer.

Result: Experimentation demonstrates FedUNet achieving 93.11% accuracy and 92.68% in its compact form with minimal communication overhead of 0.89 MB.

Conclusion: FedUNet effectively facilitates decentralized learning in heterogeneous environments, overcoming structural alignment challenges and improving performance with low communication cost.

Abstract: Federated learning (FL) enables decentralized model training without sharing
local data. However, most existing methods assume identical model architectures
across clients, limiting their applicability in heterogeneous real-world
environments. To address this, we propose FedUNet, a lightweight and
architecture-agnostic FL framework that attaches a U-Net-inspired additive
module to each client's backbone. By sharing only the compact bottleneck of the
U-Net, FedUNet enables efficient knowledge transfer without structural
alignment. The encoder-decoder design and skip connections in the U-Net help
capture both low-level and high-level features, facilitating the extraction of
clientinvariant representations. This enables cooperative learning between the
backbone and the additive module with minimal communication cost. Experiment
with VGG variants shows that FedUNet achieves 93.11% accuracy and 92.68% in
compact form (i.e., a lightweight version of FedUNet) with only 0.89 MB low
communication overhead.

</details>


### [394] [A Multi-Resolution Benchmark Framework for Spatial Reasoning Assessment in Neural Networks](https://arxiv.org/abs/2508.12741)
*Manuela Imbriani,Gina Belmonte,Mieke Massink,Alessandro Tofani,Vincenzo Ciancia*

Main category: cs.LG

TL;DR: The paper introduces a new benchmark framework to evaluate spatial reasoning capabilities of neural networks using synthetic datasets and demonstrates challenges in their geometric and topological understanding.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of neural networks in spatial reasoning tasks, particularly in morphological properties like connectivity and distance relationships.

Method: The framework involves generating synthetic datasets (maze connectivity problems and spatial distance computation tasks), training neural networks, evaluating using Dice coefficient and IoU metrics, and identifying systematic failures.

Result: Experimental results highlight significant challenges in the spatial reasoning capabilities of neural networks, showing systematic failures in solving basic geometric and topological tasks.

Conclusion: The framework serves as a reproducible protocol to study limitations and lays groundwork for exploring hybrid solutions combining neural networks and symbolic reasoning in spatial understanding.

Abstract: This paper presents preliminary results in the definition of a comprehensive
benchmark framework designed to systematically evaluate spatial reasoning
capabilities in neural networks, with a particular focus on morphological
properties such as connectivity and distance relationships. The framework is
currently being used to study the capabilities of nnU-Net, exploiting the
spatial model checker VoxLogicA to generate two distinct categories of
synthetic datasets: maze connectivity problems for topological analysis and
spatial distance computation tasks for geometric understanding. Each category
is evaluated across multiple resolutions to assess scalability and
generalization properties. The automated pipeline encompasses a complete
machine learning workflow including: synthetic dataset generation, standardized
training with cross-validation, inference execution, and comprehensive
evaluation using Dice coefficient and IoU (Intersection over Union) metrics.
Preliminary experimental results demonstrate significant challenges in neural
network spatial reasoning capabilities, revealing systematic failures in basic
geometric and topological understanding tasks. The framework provides a
reproducible experimental protocol, enabling researchers to identify specific
limitations. Such limitations could be addressed through hybrid approaches
combining neural networks with symbolic reasoning methods for improved spatial
understanding in clinical applications, establishing a foundation for ongoing
research into neural network spatial reasoning limitations and potential
solutions.

</details>


### [395] [Short-Term Forecasting of Energy Production and Consumption Using Extreme Learning Machine: A Comprehensive MIMO based ELM Approach](https://arxiv.org/abs/2508.12764)
*Cyril Voyant,Milan Despotovic,Luis Garcia-Gutierrez,Mohammed Asloune,Yves-Marie Saint-Drenan,Jean-Laurent Duchaud,hjuvan Antone Faggianelli,Elena Magliaro*

Main category: cs.LG

TL;DR: This paper introduces a novel short-term energy forecasting method using Extreme Learning Machines (ELM), leveraging Corsica's multi-source energy data with impressive accuracy for up to five hours ahead.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of accurate short-term energy forecasting, which is crucial for grid stability and efficient energy management, using non-stationary and seasonal energy data.

Method: The methodology employs an ELM-based Multi-Input Multi-Output (MIMO) framework with sliding window techniques and cyclic time encoding to dynamically predict energy production.

Result: The ELM model achieves superior forecasting performance, with nRMSE values of 17.9% for solar energy and 5.1% for thermal energy (1-hour horizon), maintaining high accuracy up to five hours ahead.

Conclusion: The ELM-MIMO approach demonstrates computational efficiency, adaptability to diverse contexts, and predictive superiority, making it ideal for real-time applications in energy forecasting.

Abstract: A novel methodology for short-term energy forecasting using an Extreme
Learning Machine ($\mathtt{ELM}$) is proposed. Using six years of hourly data
collected in Corsica (France) from multiple energy sources (solar, wind, hydro,
thermal, bioenergy, and imported electricity), our approach predicts both
individual energy outputs and total production (\cyr{including imports, which
closely follow energy demand, modulo losses)} through a Multi-Input
Multi-Output ($\mathtt{MIMO}$) architecture. To address non-stationarity and
seasonal variability, sliding window techniques and cyclic time encoding are
incorporated, enabling dynamic adaptation to fluctuations. The $\mathtt{ELM}$
model significantly outperforms persistence-based forecasting, particularly for
solar and thermal energy, achieving an $\mathtt{nRMSE}$ of $17.9\%$ and
$5.1\%$, respectively, with $\mathtt{R^2} > 0.98$ (1-hour horizon). The model
maintains high accuracy up to five hours ahead, beyond which renewable energy
sources become increasingly volatile. While $\mathtt{MIMO}$ provides marginal
gains over Single-Input Single-Output ($\mathtt{SISO}$) architectures and
offers key advantages over deep learning methods such as $\mathtt{LSTM}$, it
provides a closed-form solution with lower computational demands, making it
well-suited for real-time applications, including online learning. Beyond
predictive accuracy, the proposed methodology is adaptable to various contexts
and datasets, as it can be tuned to local constraints such as resource
availability, grid characteristics, and market structures.

</details>


### [396] [Online Ensemble Transformer for Accurate Cloud Workload Forecasting in Predictive Auto-Scaling](https://arxiv.org/abs/2508.12773)
*Jiadong Chen,Xiao He,Hengyu Ye,Fuxin Jiang,Tieying Zhang,Jianjun Chen,Xiaofeng Gao*

Main category: cs.LG

TL;DR: The paper introduces E3Former, a novel online ensemble model for workload forecasting in serverless systems, achieving superior accuracy with minimal computational cost and substantial reduction in resource utilization.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of dynamic workload adaptation and complex periodicity in serverless systems' predictive auto-scaling tasks.

Method: The proposed E3Former model utilizes an ensemble of subnetworks, improving predictive accuracy and robustness, while maintaining lean computational overhead.

Result: E3Former reduces forecast errors by 10% in experimentation and achieves over 40% resource utilization reduction in a real-world implementation within ByteDance's IHPA platform, supporting large-scale applications.

Conclusion: E3Former enhances the efficiency and accuracy of predictive auto-scaling systems, benefiting serverless operations by reducing resource costs while ensuring service quality.

Abstract: In the swiftly evolving domain of cloud computing, the advent of serverless
systems underscores the crucial need for predictive auto-scaling systems. This
necessity arises to ensure optimal resource allocation and maintain operational
efficiency in inherently volatile environments. At the core of a predictive
auto-scaling system is the workload forecasting model. Existing forecasting
models struggle to quickly adapt to the dynamics in online workload streams and
have difficulty capturing the complex periodicity brought by fine-grained,
high-frequency forecasting tasks. Addressing this, we propose a novel online
ensemble model, E3Former, for online workload forecasting in large-scale
predictive auto-scaling. Our model synergizes the predictive capabilities of
multiple subnetworks to surmount the limitations of single-model approaches,
thus ensuring superior accuracy and robustness. Remarkably, it accomplishes
this with a minimal increase in computational overhead, adhering to the lean
operational ethos of serverless systems. Through extensive experimentation on
real-world workload datasets, we establish the efficacy of our ensemble model.
In online forecasting tasks, the proposed method reduces forecast error by an
average of 10%, and its effectiveness is further demonstrated through a
predictive auto-scaling test in the real-life online system. Currently, our
method has been deployed within ByteDance's Intelligent Horizontal Pod
Auto-scaling (IHPA) platform, which supports the stable operation of over 30
applications, such as Douyin E-Comerce, TouTiao, and Volcano Engine. The
predictive auto-scaling capacity reaching over 600,000 CPU cores. On the basis
of essentially ensuring service quality, the predictive auto-scaling system can
reduce resource utilization by over 40%.

</details>


### [397] [Wavy Transformer](https://arxiv.org/abs/2508.12787)
*Satoshi Noguchi,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: The paper addresses the over-smoothing issue in deep transformer models by introducing the Wavy Transformer, which incorporates second-order dynamics for improved token representation and performance across NLP and CV tasks.


<details>
  <summary>Details</summary>
Motivation: Deep transformer models often face the problem of over-smoothing, where token representations become too similar, limiting model performance. The authors aimed to address this issue by redefining the dynamics at play.

Method: The authors presented the Wavy Transformer, using a novel attention mechanism inspired by second-order wavy dynamics, alongside optimized feed-forward and normalization layers.

Result: Experiments showed that the Wavy Transformer consistently improved performance across various NLP and CV benchmarks, requiring minimal additional parameters and no complex hyperparameter adjustments.

Conclusion: The Wavy Transformer mitigates the over-smoothing issue effectively, enhancing transformer architectures with better token dynamics and promising results in multi-domain tasks.

Abstract: Transformers have achieved remarkable success across natural language
processing (NLP) and computer vision (CV). However, deep transformer models
often suffer from an over-smoothing issue, in which token representations
converge to similar values as they pass through successive transformer blocks.
In this paper, we establish an equivalence between the hidden-state dynamics
induced by stacked attention layers and graph neural diffusion on a complete
graph. From this perspective, over-smoothing can be interpreted as a
consequence of the dissipative nature of the underlying diffusion dynamics.
Motivated by this physical interpretation, we propose Wavy Transformer, which
consists of a novel attention layer based on second-order wavy dynamics. We
also introduce a feed-forward network and a normalization layer designed to
preserve the physical state-velocity relationship under the chain rule, thereby
extending the transformer architecture. We further validate our proposed
techniques on various transformer models for NLP and CV tasks. The results
consistently demonstrate that Wavy Transformer improves performance with
minimal additional parameters and no extra hyperparameter tuning.

</details>


### [398] [A Shift in Perspective on Causality in Domain Generalization](https://arxiv.org/abs/2508.12798)
*Damian Machlanski,Stephanie Riley,Edward Moroshko,Kurt Butler,Panagiotis Dimitrakopoulos,Thomas Melistas,Akchunya Chanchal,Steven McDonagh,Ricardo Silva,Sotirios A. Tsaftaris*

Main category: cs.LG

TL;DR: The paper revisits the role of causality in domain generalization (DG) benchmarks, reconciling existing contradictions and advocating for a nuanced theory.


<details>
  <summary>Details</summary>
Motivation: To address the debated robustness of causal modeling in achieving AI generalization in DG tasks.

Method: Revisiting literature and reconciling contradictions in causal modeling claims for DG; interactive demo included.

Result: Challenges prior perceptions of causality's role in DG and proposes a more nuanced understanding.

Conclusion: A nuanced theory, rather than absolute claims, is needed for predicting causality's effectiveness in DG benchmarks.

Abstract: The promise that causal modelling can lead to robust AI generalization has
been challenged in recent work on domain generalization (DG) benchmarks. We
revisit the claims of the causality and DG literature, reconciling apparent
contradictions and advocating for a more nuanced theory of the role of
causality in generalization. We also provide an interactive demo at
https://chai-uk.github.io/ukairs25-causal-predictors/.

</details>


### [399] [Maximum Score Routing For Mixture-of-Experts](https://arxiv.org/abs/2508.12801)
*Bowen Dong,Yilong Fan,Yutao Sun,Zhenyu Li,Tengyu Pan,Xun Zhou,Jianyong Wang*

Main category: cs.LG

TL;DR: The paper introduces Maximum Score Routing (MaxScore), a method to improve routing efficiency in mixture-of-experts (MoE) networks by using minimum-cost maximum-flow modeling and a SoftTopk operator, achieving better performance without the drawbacks of traditional constraints.


<details>
  <summary>Details</summary>
Motivation: Traditional MoE networks face inefficiencies due to capacity constraints or load balancing issues, leading to either token dropping or underutilized experts.

Method: MaxScore models MoE routing as a minimum-cost maximum-flow problem and employs a SoftTopk operator to overcome the limitations of constrained and unconstrained routing methods.

Result: MaxScore achieves lower training losses and higher evaluation scores at equivalent FLOPs compared to baseline methods.

Conclusion: MaxScore offers an efficient routing mechanism that preserves scalability, computational efficiency, and achieves improved model outcomes over traditional MoE routing approaches.

Abstract: Routing networks in sparsely activated mixture-of-experts (MoE) dynamically
allocate input tokens to top-k experts through differentiable sparse
transformations, enabling scalable model capacity while preserving
computational efficiency. Traditional MoE networks impose an expert capacity
constraint to ensure GPU-friendly computation. However, this leads to token
dropping when capacity is saturated and results in low hardware efficiency due
to padding in underutilized experts. Removing the capacity constraint, in turn,
compromises load balancing and computational efficiency. To address these
issues, we propose Maximum Score Routing ($\mathbf{MaxScore}$), a novel MoE
routing paradigm that models routing as a minimum-cost maximum-flow problem and
integrates a SoftTopk operator. MaxScore resolves the fundamental limitations
of iterative rerouting and optimal transport formulations, achieving lower
training losses and higher evaluation scores at equivalent FLOPs compared to
both constrained and unconstrained baselines. Implementation details and
experimental configurations can be obtained from
$\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.

</details>


### [400] [Learning to Steer: Input-dependent Steering for Multimodal LLMs](https://arxiv.org/abs/2508.12815)
*Jayneel Parekh,Pegah Khayatan,Mustafa Shukor,Arnaud Dapogny,Alasdair Newson,Matthieu Cord*

Main category: cs.LG

TL;DR: The paper presents L2S (Learn-to-Steer), a method for fine-grained guidance of multimodal large language models (MLLMs) using input-specific steering vectors, which reduces hallucinations and enhances safety.


<details>
  <summary>Details</summary>
Motivation: Existing steering approaches for LLMs, like mean steering, fail to adapt behavior dynamically to input-specific contexts, which is particularly important for handling diverse and sensitive queries.

Method: The paper introduces input-specific linear shifts computed via contrastive prompting. It also proposes training an auxiliary module to predict these steering vectors, enabling dynamic adjustments during inference.

Result: The L2S method significantly outperforms static baselines, ensuring safer and more reliable responses from MLLMs by reducing hallucinations and tailoring advice contextually.

Conclusion: Learn-to-Steer (L2S) offers a practical and effective alternative to static steering methods, improving multimodal language models' adaptability, safety, and overall reliability.

Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of
LLMs towards enforcing a specific behavior. However, it remains largely
underexplored for multimodal LLMs (MLLMs); furthermore, existing steering
techniques, such as mean steering, rely on a single steering vector, applied
independently of the input query. This paradigm faces limitations when the
desired behavior is dependent on the example at hand. For example, a safe
answer may consist in abstaining from answering when asked for an illegal
activity, or may point to external resources or consultation with an expert
when asked about medical advice. In this paper, we investigate a fine-grained
steering that uses an input-specific linear shift. This shift is computed using
contrastive input-specific prompting. However, the input-specific prompts
required for this approach are not known at test time. Therefore, we propose to
train a small auxiliary module to predict the input-specific steering vector.
Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces
hallucinations and enforces safety in MLLMs, outperforming other static
baselines.

</details>


### [401] [Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory Study on JPEG](https://arxiv.org/abs/2508.12833)
*Kichang Lee,Songkuk Kim,JaeYeon Park,JeongGil Ko*

Main category: cs.LG

TL;DR: The paper explores storage-aware learning for on-device machine learning, focusing on balancing data quantity and quality through compression.


<details>
  <summary>Details</summary>
Motivation: On-device machine learning faces storage constraints, especially in continuous data collection settings, necessitating strategies to manage data effectively while maintaining model performance.

Method: The study empirically examines the impact of data compression strategies, highlighting the limitations of uniform and fixed approaches and proposing sample-wise adaptive compression.

Result: Findings show varying sensitivities of data samples to compression and establish the potential for adaptive, tailored strategies to improve storage-aware learning.

Conclusion: The work systematically characterizes storage-aware learning challenges and provides valuable insights to guide the development of more effective storage management and learning systems.

Abstract: On-device machine learning is often constrained by limited storage,
particularly in continuous data collection scenarios. This paper presents an
empirical study on storage-aware learning, focusing on the trade-off between
data quantity and quality via compression. We demonstrate that naive
strategies, such as uniform data dropping or one-size-fits-all compression, are
suboptimal. Our findings further reveal that data samples exhibit varying
sensitivities to compression, supporting the feasibility of a sample-wise
adaptive compression strategy. These insights provide a foundation for
developing a new class of storage-aware learning systems. The primary
contribution of this work is the systematic characterization of this
under-explored challenge, offering valuable insights that advance the
understanding of storage-aware learning.

</details>


### [402] [Learning In-context $\pmb{n}$-grams with Transformers: Sub-$\pmb{n}$-grams Are Near-stationary Points](https://arxiv.org/abs/2508.12837)
*Aditya Varre,Gizem Yüce,Nicolas Flammarion*

Main category: cs.LG

TL;DR: This paper explores the loss landscape of transformer models for in-context next-token prediction, focusing on $n$-gram estimators and their near-stationary points, explaining stage-wise learning and emergent phase transitions.


<details>
  <summary>Details</summary>
Motivation: Prolonged plateaus and stage-wise progression during training in transformer models lack detailed theoretical understanding.

Method: The paper examines $n$-gram language models under cross-entropy loss, establishes sufficient conditions for stationary points, and analyzes simplified transformer models that approximate $k$-gram estimators.

Result: It finds that sub-$n$-grams represent near-stationary points in the loss landscape, providing insight into the observed stage-wise learning dynamics and phase transitions.

Conclusion: The study reveals key theoretical aspects of transformer training, explaining learning dynamics and supporting these insights through numerical experiments.

Abstract: Motivated by empirical observations of prolonged plateaus and stage-wise
progression during training, we investigate the loss landscape of transformer
models trained on in-context next-token prediction tasks. In particular, we
focus on learning in-context $n$-gram language models under cross-entropy loss,
and establish a sufficient condition for parameter configurations to be
stationary points. We then construct a set of parameter configurations for a
simplified transformer model that represent $k$-gram estimators (for $k \leq
n$), and show that the gradient of the population loss at these solutions
vanishes in the limit of infinite sequence length and parameter norm. This
reveals a key property of the loss landscape: {sub-$n$-grams are
near-stationary points of the population cross-entropy loss}, offering
theoretical insight into widely observed phenomena such as stage-wise learning
dynamics and emergent phase transitions. These insights are further supported
by numerical experiments that illustrate the learning dynamics of $n$-grams,
characterized by discrete transitions between near-stationary solutions.

</details>


### [403] [HRS: Hybrid Representation Framework with Scheduling Awareness for Time Series Forecasting in Crowdsourced Cloud-Edge Platforms](https://arxiv.org/abs/2508.12839)
*Tiancheng Zhang,Cheng Zhang,Shuren Liu,Xiaofei Wang,Shaoyuan Huang,Wenyu Wang*

Main category: cs.LG

TL;DR: This paper introduces the HRS framework for load forecasting in Crowdsourced Cloud-Edge Platforms, which integrates novel hybrid representation techniques and a Scheduling-Aware Loss (SAL) to improve traffic surge handling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of maintaining Quality of Service (QoS) during highly variable and bursty traffic patterns in Crowdsourced Cloud-Edge Platforms. Existing forecasting methods fail to strike a balance between underprovisioning and overprovisioning, leading to SLA violations or unnecessary resource costs.

Method: The method involves proposing a Hybrid Representation System (HRS) that combines numerical and image-based representations to better capture irregular load dynamics. Additionally, it introduces a Scheduling-Aware Loss (SAL), which penalizes prediction errors asymmetrically to guide forecasts that optimize scheduling efficiency.

Result: The experiments, conducted on four real-world datasets, demonstrate that HRS outperforms ten baseline methods, achieving significant reductions in SLA violation rates by 63.1% and profit loss by 32.3%.

Conclusion: The study concludes that the proposed HRS framework, with its hybrid representation and scheduling-aware optimization, effectively improves load forecasting under traffic surges. This results in enhanced resource allocation and profitability for CCPs.

Abstract: With the rapid proliferation of streaming services, network load exhibits
highly time-varying and bursty behavior, posing serious challenges for
maintaining Quality of Service (QoS) in Crowdsourced Cloud-Edge Platforms
(CCPs). While CCPs leverage Predict-then-Schedule architecture to improve QoS
and profitability, accurate load forecasting remains challenging under traffic
surges. Existing methods either minimize mean absolute error, resulting in
underprovisioning and potential Service Level Agreement (SLA) violations during
peak periods, or adopt conservative overprovisioning strategies, which mitigate
SLA risks at the expense of increased resource expenditure. To address this
dilemma, we propose HRS, a hybrid representation framework with scheduling
awareness that integrates numerical and image-based representations to better
capture extreme load dynamics. We further introduce a Scheduling-Aware Loss
(SAL) that captures the asymmetric impact of prediction errors, guiding
predictions that better support scheduling decisions. Extensive experiments on
four real-world datasets demonstrate that HRS consistently outperforms ten
baselines and achieves state-of-the-art performance, reducing SLA violation
rates by 63.1% and total profit loss by 32.3%.

</details>


### [404] [One-Class Intrusion Detection with Dynamic Graphs](https://arxiv.org/abs/2508.12885)
*Aleksei Liuliakov,Alexander Schulz,Luca Hermes,Barbara Hammer*

Main category: cs.LG

TL;DR: The paper introduces TGN-SVDD, a new intrusion detection method utilizing dynamic graph modeling and deep anomaly detection, showing superior performance on realistic data.


<details>
  <summary>Details</summary>
Motivation: Digitalization and increasing network security concerns require advanced solutions for detecting novel and unseen network intrusions, managing temporal data, and addressing graph-structured network communications.

Method: The authors developed TGN-SVDD, which utilizes modern dynamic graph modeling and deep anomaly detection techniques to enhance intrusion detection.

Result: The method outperforms several baseline models on realistic intrusion detection datasets, demonstrating its practical effectiveness.

Conclusion: TGN-SVDD offers a significant improvement for intrusion detection tasks and highlights the potential of combining graph modeling and anomaly detection in cybersecurity.

Abstract: With the growing digitalization all over the globe, the relevance of network
security becomes increasingly important. Machine learning-based intrusion
detection constitutes a promising approach for improving security, but it bears
several challenges. These include the requirement to detect novel and unseen
network events, as well as specific data properties, such as events over time
together with the inherent graph structure of network communication. In this
work, we propose a novel intrusion detection method, TGN-SVDD, which builds
upon modern dynamic graph modelling and deep anomaly detection. We demonstrate
its superiority over several baselines for realistic intrusion detection data
and suggest a more challenging variant of the latter.

</details>


### [405] [TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML](https://arxiv.org/abs/2508.12905)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: The paper presents TCUQ, a label-free uncertainty monitor for TinyML systems that is efficient in memory and latency. The method uses lightweight temporal consistency measures and calibration to enable reliable on-device decision-making.


<details>
  <summary>Details</summary>
Motivation: Current uncertainty monitoring methods in TinyML often face limitations like high memory and computational demands, which are not feasible for resource-constrained devices like microcontrollers.

Method: TCUQ utilizes short-horizon temporal consistency on posterior and feature signals to generate a calibrated risk score using streaming and lightweight computations. It includes a conformal layer for budgeted decision-making with a single-pass approach.

Result: TCUQ achieves superior memory efficiency (50-60% smaller footprint) and faster latency (30-45% quicker) compared to traditional methods. It also provides improved performance metrics like high AUPRC and AUROC scores for detecting accuracy drops and failures under corrupted conditions.

Conclusion: The study demonstrates that lightweight temporal consistency, when combined with streaming conformal calibration, is a robust and efficient solution for on-device uncertainty monitoring in resource-constrained settings like TinyML.

Abstract: We introduce TCUQ, a single pass, label free uncertainty monitor for
streaming TinyML that converts short horizon temporal consistency captured via
lightweight signals on posteriors and features into a calibrated risk score
with an O(W ) ring buffer and O(1) per step updates. A streaming conformal
layer turns this score into a budgeted accept/abstain rule, yielding calibrated
behavior without online labels or extra forward passes. On microcontrollers,
TCUQ fits comfortably on kilobyte scale devices and reduces footprint and
latency versus early exit and deep ensembles (typically about 50 to 60% smaller
and about 30 to 45% faster), while methods of similar accuracy often run out of
memory. Under corrupted in distribution streams, TCUQ improves accuracy drop
detection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high
severities; for failure detection it attains up to 0.92 AUROC. These results
show that temporal consistency, coupled with streaming conformal calibration,
provides a practical and resource efficient foundation for on device monitoring
in TinyML.

</details>


### [406] [SparseMap: A Sparse Tensor Accelerator Framework Based on Evolution Strategy](https://arxiv.org/abs/2508.12906)
*Boran Zhao,Haiming Zhai,Zihang Yuan,Hetian Liu,Tian Xia,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: SparseMap is a framework utilizing evolutionary strategies to optimize sparse tensor accelerator designs by considering both mapping and sparse strategy. It outperforms previous methods in diverse design scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing sparse tensor accelerators are manually designed, restricted to specific scenarios, and difficult to adapt. A unified and automated solution integrating mapping and sparse strategies, addressing these limitations, is urgently needed.

Method: The paper introduces SparseMap, a framework leveraging evolutionary strategies with enhanced genetic encoding and evolutionary operators to explore the large design space combining mapping and sparse strategies.

Result: SparseMap consistently discovers superior sparse tensor accelerator solutions compared to previous works and classical optimization methods across vast and diverse design spaces.

Conclusion: By jointly optimizing mapping and sparse strategies with evolutionary techniques, SparseMap overcomes challenges in sparse tensor accelerator design and marks a significant improvement in efficiency and adaptiveness.

Abstract: The growing demand for sparse tensor algebra (SpTA) in machine learning and
big data has driven the development of various sparse tensor accelerators.
However, most existing manually designed accelerators are limited to specific
scenarios, and it's time-consuming and challenging to adjust a large number of
design factors when scenarios change. Therefore, automating the design of SpTA
accelerators is crucial. Nevertheless, previous works focus solely on either
mapping (i.e., tiling communication and computation in space and time) or
sparse strategy (i.e., bypassing zero elements for efficiency), leading to
suboptimal designs due to the lack of comprehensive consideration of both. A
unified framework that jointly optimizes both is urgently needed. However,
integrating mapping and sparse strategies leads to a combinatorial explosion in
the design space(e.g., as large as $O(10^{41})$ for the workload $P_{32 \times
64} \times Q_{64 \times 48} = Z_{32 \times 48}$). This vast search space
renders most conventional optimization methods (e.g., particle swarm
optimization, reinforcement learning and Monte Carlo tree search) inefficient.
To address this challenge, we propose an evolution strategy-based sparse tensor
accelerator optimization framework, called SparseMap. SparseMap constructing a
more comprehensive design space with the consideration of both mapping and
sparse strategy. We introduce a series of enhancements to genetic encoding and
evolutionary operators, enabling SparseMap to efficiently explore the vast and
diverse design space. We quantitatively compare SparseMap with prior works and
classical optimization methods, demonstrating that SparseMap consistently finds
superior solutions.

</details>


### [407] [SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML](https://arxiv.org/abs/2508.12907)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: SNAP-UQ is a novel uncertainty estimation method for TinyML that uses next-activation prediction to assess risk without requiring extra memory or computational resources.


<details>
  <summary>Details</summary>
Motivation: To develop a resource-efficient and accurate solution for uncertainty estimation in TinyML applications, addressing limitations like excessive memory usage and computational demand in traditional methods.

Method: Introduced next-activation prediction with tiny int8 heads and a monotone mapper for calculating actionable uncertainty scores, avoiding temporal buffers and repeated forward passes.

Result: SNAP-UQ reduces memory usage by 40–60% and improves speed by 25–35% compared to competing methods, while maintaining strong failure detection (AUROC ≈0.9) and improving accuracy-drop detection.

Conclusion: SNAP-UQ provides a scalable and efficient uncertainty estimation solution for TinyML, making it suitable for resource-constrained deployments in vision and audio tasks.

Abstract: We introduce \textbf{SNAP-UQ}, a single-pass, label-free uncertainty method
for TinyML that estimates risk from \emph{depth-wise next-activation
prediction}: tiny int8 heads forecast the statistics of the next layer from a
compressed view of the previous one, and a lightweight monotone mapper turns
the resulting surprisal into an actionable score. The design requires no
temporal buffers, auxiliary exits, or repeated forward passes, and adds only a
few tens of kilobytes to MCU deployments. Across vision and audio backbones,
SNAP-UQ consistently reduces flash and latency relative to early-exit and deep
ensembles (typically $\sim$40--60\% smaller and $\sim$25--35\% faster), with
competing methods of similar accuracy often exceeding memory limits. In
corrupted streams it improves accuracy-drop detection by several AUPRC points
and maintains strong failure detection (AUROC $\approx$0.9) in a single pass.
Grounding uncertainty in layer-to-layer dynamics yields a practical,
resource-efficient basis for on-device monitoring in TinyML.

</details>


### [408] [SL-ACC: A Communication-Efficient Split Learning Framework with Adaptive Channel-wise Compression](https://arxiv.org/abs/2508.12984)
*Zehang Lin,Zheng Lin,Miao Yang,Jianhao Huang,Yuxin Zhang,Zihan Fang,Xia Du,Zhe Chen,Shunzhi Zhu,Wei Ni*

Main category: cs.LG

TL;DR: This paper introduces SL-ACC, a communication-efficient split learning framework, which reduces data transmission bottlenecks using adaptive channel importance identification and channel grouping compression.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of neural networks makes deploying distributed machine learning on resource-limited devices challenging, especially concerning the transmission bottleneck in split learning as more devices participate.

Method: The framework uses adaptive channel importance identification via Shannon entropy to assess channel contribution and employs channel grouping compression to perform targeted compression, reducing transmission volume.

Result: SL-ACC experiments on various datasets showed faster achievement of target accuracy with lower data transmission compared to state-of-the-art benchmarks.

Conclusion: SL-ACC improves communication efficiency in split learning without sacrificing training accuracy, enabling practical distributed machine learning deployment on resource-constrained devices.

Abstract: The increasing complexity of neural networks poses a significant barrier to
the deployment of distributed machine learning (ML) on resource-constrained
devices, such as federated learning (FL). Split learning (SL) offers a
promising solution by offloading the primary computing load from edge devices
to a server via model partitioning. However, as the number of participating
devices increases, the transmission of excessive smashed data (i.e.,
activations and gradients) becomes a major bottleneck for SL, slowing down the
model training. To tackle this challenge, we propose a communication-efficient
SL framework, named SL-ACC, which comprises two key components: adaptive
channel importance identification (ACII) and channel grouping compression
(CGC). ACII first identifies the contribution of each channel in the smashed
data to model training using Shannon entropy. Following this, CGC groups the
channels based on their entropy and performs group-wise adaptive compression to
shrink the transmission volume without compromising training accuracy.
Extensive experiments across various datasets validate that our proposed SL-ACC
framework takes considerably less time to achieve a target accuracy than
state-of-the-art benchmarks.

</details>


### [409] [Predicting the Performance of Graph Convolutional Networks with Spectral Properties of the Graph Laplacian](https://arxiv.org/abs/2508.12993)
*Shalima Binta Manir,Tim Oates*

Main category: cs.LG

TL;DR: The paper identifies that a graph's algebraic connectivity (Fiedler value) can predict the performance of Graph Convolutional Networks (GCNs).


<details>
  <summary>Details</summary>
Motivation: The goal is to understand why stacking GCN layers may lead to variable performance and develop insights to predict and optimize GCN effectiveness.

Method: The authors empirically and theoretically analyze the relationship between the Fiedler value and GCN performance, conducting experiments on synthetic and real datasets such as Cora, CiteSeer, and Polblogs.

Result: Graphs with similar Fiedler values exhibit comparable structural properties, enabling similar hyperparameters, filters, and even transfer learning outcomes for GCNs.

Conclusion: The Fiedler value is a reliable predictor of GCN performance and could guide hyperparameter setting and transfer learning strategies for graph-structured tasks.

Abstract: A common observation in the Graph Convolutional Network (GCN) literature is
that stacking GCN layers may or may not result in better performance on tasks
like node classification and edge prediction. We have found empirically that a
graph's algebraic connectivity, which is known as the Fiedler value, is a good
predictor of GCN performance. Intuitively, graphs with similar Fiedler values
have analogous structural properties, suggesting that the same filters and
hyperparameters may yield similar results when used with GCNs, and that
transfer learning may be more effective between graphs with similar algebraic
connectivity. We explore this theoretically and empirically with experiments on
synthetic and real graph data, including the Cora, CiteSeer and Polblogs
datasets. We explore multiple ways of aggregating the Fiedler value for
connected components in the graphs to arrive at a value for the entire graph,
and show that it can be used to predict GCN performance. We also present
theoretical arguments as to why the Fiedler value is a good predictor.

</details>


### [410] [Kourkoutas-Beta: A Sunspike-Driven Adam Optimizer with Desert Flair](https://arxiv.org/abs/2508.12996)
*Stavros C. Kassinos*

Main category: cs.LG

TL;DR: Kourkoutas-Beta, an Adam-style optimizer with dynamic beta2, offers better stability and performance in handling spiky gradients for physics-informed and data-driven settings.


<details>
  <summary>Details</summary>
Motivation: Address instability and inefficiencies in traditional Adam optimizers when faced with erratic losses and spiky gradients in both PINNs and data-driven PDE surrogates.

Method: Introduce dynamic beta2 with sunspike ratios, adaptive settings, and several bias-correction modes within the framework of Adam optimization.

Result: Kourkoutas-Beta yielded improved stability, reduced final loss, and lower bits-per-character for tasks such as PDE surrogates, PINNs, and enwik8 character-level models.

Conclusion: The optimizer enhances robustness while maintaining drop-in use with existing setups and preserving Adam-style convergence guarantees.

Abstract: Transformer neural networks are increasingly used for physics-based problems.
In data-driven PDE surrogates, training samples from varying boundary and
initial conditions can cause erratic losses and spiky gradients; in
physics-informed neural networks (PINNs), stiff composite losses amplify this
effect.
  We introduce Kourkoutas-Beta, an Adam-style optimizer where the fixed
second-moment discount beta2 is replaced by a layer-wise dynamic value driven
by a bounded ``sunspike'' ratio: the current pooled gradient norm divided by an
exponential moving average (EMA) of past norms, squashed to the interval [0,1).
Spikes lower beta2 toward beta2_min; calm phases keep it near beta2_max.
Options include leaky-AMSGrad (decay), trust-region clipping (max_ratio),
adaptive tiny terms, and several bias-correction modes ``none'', ``beta2max'',
``exact'). With all features off and bias_correction=``none'', the method is
exactly Adam.
  We test on four settings: (i) a Transformer PDE surrogate (Heat2D), (ii) a 3D
PINN for heat conduction (Heat3D), (iii) a lightweight MLX synthetic task with
jitter and rare-trigger bursts, and (iv) a character-level Transformer on 30 MB
of enwik8 (small-enwik8). Kourkoutas-Beta improves stability and final loss
versus fixed-beta2 Adam. On small-enwik8 it lowers bits-per-character by about
38% vs Adam-0.95 and about 58% vs Adam-0.999 over 10 seeds, with smaller
variance. The method remains drop-in, with runtime overhead comparable to Adam
in testbeds A-C and within single-digit percent in testbed D. It preserves
Adam-style convergence guarantees while improving robustness under spiky
gradients.

</details>


### [411] [Monte Carlo Functional Regularisation for Continual Learning](https://arxiv.org/abs/2508.13006)
*Pengcheng Hao,Menghao Waiyan William Zhu,Ercan Engin Kuruoglu*

Main category: cs.LG

TL;DR: This paper introduces MCFRCL, a continual learning framework using Monte Carlo samples for functional regularization, addressing computational limitations and linear errors in previous approaches.


<details>
  <summary>Details</summary>
Motivation: To improve continual learning for neural networks by reducing the computational costs and approximation errors seen in existing functional regularisation methods.

Method: The paper proposes the MCFRCL framework, leveraging Monte Carlo sampling and statistical distribution properties, while using Wasserstein and Kullback-Leibler distances for constructing the regularisation function.

Result: MCFRCL demonstrated improved prediction accuracy and training efficiency when evaluated on MNIST and CIFAR datasets compared to benchmark methods.

Conclusion: MCFRCL effectively enhances functional regularisation in continual learning, providing a balance of accuracy and efficiency.

Abstract: Continual learning (CL) is crucial for the adaptation of neural network
models to new environments. Although outperforming weight-space regularisation
approaches, the functional regularisation-based CL methods suffer from high
computational costs and large linear approximation errors. In this work, we
present a new functional regularisation CL framework, called MCFRCL, which
approximates model prediction distributions by Monte Carlo (MC) sampling.
Moreover, three continuous distributions are leveraged to capture the
statistical characteristics of the MC samples via moment-based methods.
Additionally, both the Wasserstein distance and the Kullback-Leibler (KL)
distance are employed to construct the regularisation function. The proposed
MCFRCL is evaluated against multiple benchmark methods on the MNIST and CIFAR
datasets, with simulation results highlighting its effectiveness in both
prediction accuracy and training efficiency.

</details>


### [412] [Design and Analysis of Robust Adaptive Filtering with the Hyperbolic Tangent Exponential Kernel M-Estimator Function for Active Noise Control](https://arxiv.org/abs/2508.13018)
*Iam Kim de S. Hermont,Andre R. Flores,Rodrigo C. de Lamare*

Main category: cs.LG

TL;DR: This paper proposes a robust adaptive filtering algorithm for noise control amidst impulsive noise and demonstrates its efficiency over alternatives.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of handling impulsive noise in active noise control systems effectively.

Method: The development and statistical analysis of the FXHEKM robust adaptive algorithm, combined with computational cost studies.

Result: Numerical evaluation demonstrates the FXHEKM algorithm's superior MSE and ANR performance, effectively mitigating spurious signals like α-stable noises compared to other algorithms.

Conclusion: The proposed FXHEKM algorithm is efficient and robust for active noise control, showing promise in handling impulsive and additive spurious noises.

Abstract: In this work, we propose a robust adaptive filtering approach for active
noise control applications in the presence of impulsive noise. In particular,
we develop the filtered-x hyperbolic tangent exponential generalized Kernel
M-estimate function (FXHEKM) robust adaptive algorithm. A statistical analysis
of the proposed FXHEKM algorithm is carried out along with a study of its
computational cost. {In order to evaluate the proposed FXHEKM algorithm, the
mean-square error (MSE) and the average noise reduction (ANR) performance
metrics have been adopted.} Numerical results show the efficiency of the
proposed FXHEKM algorithm to cancel the presence of the additive spurious
signals, such as \textbf{$\alpha$}-stable noises against competing algorithms.

</details>


### [413] [The Application of Transformer-Based Models for Predicting Consequences of Cyber Attacks](https://arxiv.org/abs/2508.13030)
*Bipin Chhetri,Akbar Siami Namin*

Main category: cs.LG

TL;DR: This paper explores the use of NLP and deep learning to classify the consequences of cyberattacks using descriptions from the CWE database, with BERT outperforming traditional models.


<details>
  <summary>Details</summary>
Motivation: The growing frequency and complexity of cyberattacks are costing industries billions, creating a need for automated tools to understand and predict their impact.

Method: The study leverages natural language processing (NLP) techniques and deep learning models, specifically BERT and Hierarchical Attention Networks (HANs), for multi-label classification of cyberattack consequences.

Result: BERT achieves an overall accuracy of 0.972, outperforming traditional CNN and LSTM-based models. HAN shows improvement over conventional techniques for specific labels, but BERT maintains superior precision and recall.

Conclusion: BERT's performance in precision, accuracy, and recall makes it the optimal choice for modeling the consequences of cyberattacks among the models tested.

Abstract: Cyberattacks are increasing, and securing against such threats is costing
industries billions of dollars annually. Threat Modeling, that is,
comprehending the consequences of these attacks, can provide critical support
to cybersecurity professionals, enabling them to take timely action and
allocate resources that could be used elsewhere. Cybersecurity is heavily
dependent on threat modeling, as it assists security experts in assessing and
mitigating risks related to identifying vulnerabilities and threats. Recently,
there has been a pressing need for automated methods to assess attack
descriptions and forecast the future consequences of the increasing complexity
of cyberattacks. This study examines how Natural Language Processing (NLP) and
deep learning can be applied to analyze the potential impact of cyberattacks by
leveraging textual descriptions from the MITRE Common Weakness Enumeration
(CWE) database. We emphasize classifying attack consequences into five
principal categories: Availability, Access Control, Confidentiality, Integrity,
and Other. This paper investigates the use of Bidirectional Encoder
Representations from Transformers (BERT) in combination with Hierarchical
Attention Networks (HANs) for Multi-label classification, evaluating their
performance in comparison with conventional CNN and LSTM-based models.
Experimental findings show that BERT achieves an overall accuracy of $0.972$,
far higher than conventional deep learning models in multi-label
classification. HAN outperforms baseline forms of CNN and LSTM-based models on
specific cybersecurity labels. However, BERT consistently achieves better
precision and recall, making it more suitable for predicting the consequences
of a cyberattack.

</details>


### [414] [Beyond Internal Data: Bounding and Estimating Fairness from Incomplete Data](https://arxiv.org/abs/2508.13040)
*Varsha Ramineni,Hossein A. Rahmani,Emine Yilmaz,David Barber*

Main category: cs.LG

TL;DR: The paper proposes a method to assess fairness in AI systems by combining incomplete internal and external datasets to compute plausible fairness metrics.


<details>
  <summary>Details</summary>
Motivation: Fairness in AI systems is crucial in sensitive areas like lending and healthcare, but legal and privacy issues often restrict access to complete demographic data.

Method: The approach involves estimating joint distributions from separate datasets (internal predictive attributes and external census data) to calculate bounds on fairness metrics.

Result: Simulations and real experiments show that this method yields meaningful bounds and reliable fairness metric estimates.

Conclusion: This method offers a practical solution for fairness testing in cases where complete data access is restricted.

Abstract: Ensuring fairness in AI systems is critical, especially in high-stakes
domains such as lending, hiring, and healthcare. This urgency is reflected in
emerging global regulations that mandate fairness assessments and independent
bias audits. However, procuring the necessary complete data for fairness
testing remains a significant challenge. In industry settings, legal and
privacy concerns restrict the collection of demographic data required to assess
group disparities, and auditors face practical and cultural challenges in
gaining access to data. In practice, data relevant for fairness testing is
often split across separate sources: internal datasets held by institutions
with predictive attributes, and external public datasets such as census data
containing protected attributes, each providing only partial, marginal
information. Our work seeks to leverage such available separate data to
estimate model fairness when complete data is inaccessible. We propose
utilising the available separate data to estimate a set of feasible joint
distributions and then compute the set plausible fairness metrics. Through
simulation and real experiments, we demonstrate that we can derive meaningful
bounds on fairness metrics and obtain reliable estimates of the true metric.
Our results demonstrate that this approach can serve as a practical and
effective solution for fairness testing in real-world settings where access to
complete data is restricted.

</details>


### [415] [Seeing the Many: Exploring Parameter Distributions Conditioned on Features in Surrogates](https://arxiv.org/abs/2508.13088)
*Xiaohan Wang,Zhimin Li,Joshua A. Levine,Matthew Berger*

Main category: cs.LG

TL;DR: The paper proposes a method to model and visualize the distribution of input parameters generating specified output features using neural surrogate models.


<details>
  <summary>Details</summary>
Motivation: Simulation workflows can be expensive, and inverse problems require finding plausible input parameters that yield specific outputs, often in high-dimensional spaces; existing methods focus only on subsets of matching parameters.

Method: The authors estimate density to manage surrogate model errors and combine it with likelihood computations to efficiently sample plausible input parameters for given output features.

Result: The proposed solution enables feature-driven parameter analysis and visualization over simulation datasets, demonstrating its usability through an interface.

Conclusion: This work introduces an efficient approach for modeling and identifying distributions of input parameters for target outputs, aiding interpretability and improving inverse problem-solving workflows.

Abstract: Recently, neural surrogate models have emerged as a compelling alternative to
traditional simulation workflows. This is accomplished by modeling the
underlying function of scientific simulations, removing the need to run
expensive simulations. Beyond just mapping from input parameter to output,
surrogates have also been shown useful for inverse problems: output to input
parameters. Inverse problems can be understood as search, where we aim to find
parameters whose surrogate outputs contain a specified feature. Yet finding
these parameters can be costly, especially for high-dimensional parameter
spaces. Thus, existing surrogate-based solutions primarily focus on finding a
small set of matching parameters, in the process overlooking the broader
picture of plausible parameters. Our work aims to model and visualize the
distribution of possible input parameters that produce a given output feature.
To achieve this goal, we aim to address two challenges: (1) the approximation
error inherent in the surrogate model and (2) forming the parameter
distribution in an interactive manner. We model error via density estimation,
reporting high density only if a given parameter configuration is close to
training parameters, measured both over the input and output space. Our density
estimate is used to form a prior belief on parameters, and when combined with a
likelihood on features, gives us an efficient way to sample plausible parameter
configurations that generate a target output feature. We demonstrate the
usability of our solution through a visualization interface by performing
feature-driven parameter analysis over the input parameter space of three
simulation datasets. Source code is available at
https://github.com/matthewberger/seeing-the-many

</details>


### [416] [Outlier Detection of Poisson-Distributed Targets Using a Seabed Sensor Network](https://arxiv.org/abs/2508.13099)
*Mingyu Kim,Daniel Stilwell,Jorge Jimenez*

Main category: cs.LG

TL;DR: The paper introduces a framework for detecting spatial outliers in maritime environments using seabed acoustic sensors and stochastic modeling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the detection and classification of spatial outliers in maritime settings, which is essential for managing anomalies such as illegal or unexpected ship activities.

Method: The framework uses log Gaussian Cox processes (LGCPs) to model target arrivals and proposes a second-order approximation for classification. Additionally, a real-time sensor placement strategy is used to optimize outlier detection.

Result: Validation with real ship traffic data near Norfolk, Virginia, shows that the proposed method enhances classification accuracy and outlier detection.

Conclusion: The study concludes that combining advanced probabilistic modeling with adaptive sensor placement significantly improves maritime outlier detection.

Abstract: This paper presents a framework for classifying and detecting spatial
commission outliers in maritime environments using seabed acoustic sensor
networks and log Gaussian Cox processes (LGCPs). By modeling target arrivals as
a mixture of normal and outlier processes, we estimate the probability that a
newly observed event is an outlier. We propose a second-order approximation of
this probability that incorporates both the mean and variance of the normal
intensity function, providing improved classification accuracy compared to
mean-only approaches. We analytically show that our method yields a tighter
bound to the true probability using Jensen's inequality. To enhance detection,
we integrate a real-time, near-optimal sensor placement strategy that
dynamically adjusts sensor locations based on the evolving outlier intensity.
The proposed framework is validated using real ship traffic data near Norfolk,
Virginia, where numerical results demonstrate the effectiveness of our approach
in improving both classification performance and outlier detection through
sensor deployment.

</details>


### [417] [Causally-Guided Pairwise Transformer -- Towards Foundational Digital Twins in Process Industry](https://arxiv.org/abs/2508.13111)
*Michael Mayr,Georgios C. Chasparis*

Main category: cs.LG

TL;DR: The paper introduces Causally-Guided Pairwise Transformer (CGPT) to address shortcomings of channel-dependent (CD) and channel-independent (CI) models for multidimensional industrial time-series data prediction.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome the limitations of CD models, which lack adaptability, and CI models, which miss crucial system-level interactions.

Method: The CGPT architecture integrates causal graphs, employs pairwise modeling, and uses channel-agnostic learnable layers to balance CD information flow and CI generalization.

Result: CGPT outperforms CI and CD baselines in predictive accuracy and competes with end-to-end trained CD models across various synthetic and industrial datasets.

Conclusion: CGPT provides a robust, scalable, and adaptable solution for industrial time-series forecasting, demonstrating improved performance while remaining dimension-agnostic.

Abstract: Foundational modelling of multi-dimensional time-series data in industrial
systems presents a central trade-off: channel-dependent (CD) models capture
specific cross-variable dynamics but lack robustness and adaptability as model
layers are commonly bound to the data dimensionality of the tackled use-case,
while channel-independent (CI) models offer generality at the cost of modelling
the explicit interactions crucial for system-level predictive regression tasks.
To resolve this, we propose the Causally-Guided Pairwise Transformer (CGPT), a
novel architecture that integrates a known causal graph as an inductive bias.
The core of CGPT is built around a pairwise modeling paradigm, tackling the
CD/CI conflict by decomposing the multidimensional data into pairs. The model
uses channel-agnostic learnable layers where all parameter dimensions are
independent of the number of variables. CGPT enforces a CD information flow at
the pair-level and CI-like generalization across pairs. This approach
disentangles complex system dynamics and results in a highly flexible
architecture that ensures scalability and any-variate adaptability. We validate
CGPT on a suite of synthetic and real-world industrial datasets on long-term
and one-step forecasting tasks designed to simulate common industrial
complexities. Results demonstrate that CGPT significantly outperforms both CI
and CD baselines in predictive accuracy and shows competitive performance with
end-to-end trained CD models while remaining agnostic to the problem
dimensionality.

</details>


### [418] [Contrastive Representations for Temporal Reasoning](https://arxiv.org/abs/2508.13113)
*Alicja Ziarko,Michal Bortkiewicz,Michal Zawalski,Benjamin Eysenbach,Piotr Milos*

Main category: cs.LG

TL;DR: This paper introduces CRTR, a method improving temporal reasoning by removing spurious features and demonstrates its success in tasks like Sokoban and Rubik's Cube.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome limitations in existing temporal contrastive learning methods, which fail to capture true temporal structures due to reliance on spurious features.

Method: Proposes CRTR, which applies a negative sampling strategy to mitigate spurious features, thereby enhancing the learning of temporal representations.

Result: CRTR achieves robust performance in temporal tasks like Sokoban and Rubik’s Cube, solving the Rubik's Cube with fewer search steps than traditional methods while generalizing across all initial states.

Conclusion: CRTR shows that learned representations can efficiently address tasks requiring complex temporal reasoning without external search algorithms.

Abstract: In classical AI, perception relies on learning state-based representations,
while planning, which can be thought of as temporal reasoning over action
sequences, is typically achieved through search. We study whether such
reasoning can instead emerge from representations that capture both perceptual
and temporal structure. We show that standard temporal contrastive learning,
despite its popularity, often fails to capture temporal structure due to its
reliance on spurious features. To address this, we introduce Combinatorial
Representations for Temporal Reasoning (CRTR), a method that uses a negative
sampling scheme to provably remove these spurious features and facilitate
temporal reasoning. CRTR achieves strong results on domains with complex
temporal structure, such as Sokoban and Rubik's Cube. In particular, for the
Rubik's Cube, CRTR learns representations that generalize across all initial
states and allow it to solve the puzzle using fewer search steps than BestFS,
though with longer solutions. To our knowledge, this is the first method that
efficiently solves arbitrary Cube states using only learned representations,
without relying on an external search algorithm.

</details>


### [419] [Training Machine Learning Models on Human Spatio-temporal Mobility Data: An Experimental Study [Experiment Paper]](https://arxiv.org/abs/2508.13135)
*Yueyang Liu,Lance Kennedy,Ruochen Kong,Joon-Seok Kim,Andreas Züfle*

Main category: cs.LG

TL;DR: This paper analyzes models and techniques for forecasting individuals' long-term mobility trajectories, leveraging semantic day-of-week and historical data to enhance accuracy, while addressing challenges like data imbalance and user privacy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on macro-level human mobility patterns and develop more effective machine learning approaches for forecasting individuals' complete mobility trajectories over the next days and weeks.

Method: The study involved an experimental analysis of Long Short-Term Memory and Transformer models, parameter tuning, and training strategies. It evaluated the influence of including semantic and user-specific historical data, while employing user semantic clustering with stratified sampling to handle data imbalance.

Result: The paper found that incorporating day-of-the-week semantics, user history, and small-batch gradient optimization improved predictive accuracy. Semantic clustering mitigated data imbalance, resulting in a representative dataset despite privacy constraints.

Conclusion: The research demonstrates that integrating semantic life patterns and robust sampling techniques significantly boosts the performance of human mobility prediction models, while addressing key challenges like data skewness and limited training data.

Abstract: Individual-level human mobility prediction has emerged as a significant topic
of research with applications in infectious disease monitoring, child, and
elderly care. Existing studies predominantly focus on the microscopic aspects
of human trajectories: such as predicting short-term trajectories or the next
location visited, while offering limited attention to macro-level mobility
patterns and the corresponding life routines. In this paper, we focus on an
underexplored problem in human mobility prediction: determining the best
practices to train a machine learning model using historical data to forecast
an individuals complete trajectory over the next days and weeks. In this
experiment paper, we undertake a comprehensive experimental analysis of diverse
models, parameter configurations, and training strategies, accompanied by an
in-depth examination of the statistical distribution inherent in human mobility
patterns. Our empirical evaluations encompass both Long Short-Term Memory and
Transformer-based architectures, and further investigate how incorporating
individual life patterns can enhance the effectiveness of the prediction. We
show that explicitly including semantic information such as day-of-the-week and
user-specific historical information can help the model better understand
individual patterns of life and improve predictions. Moreover, since the
absence of explicit user information is often missing due to user privacy, we
show that the sampling of users may exacerbate data skewness and result in a
substantial loss in predictive accuracy. To mitigate data imbalance and
preserve diversity, we apply user semantic clustering with stratified sampling
to ensure that the sampled dataset remains representative. Our results further
show that small-batch stochastic gradient optimization improves model
performance, especially when human mobility training data is limited.

</details>


### [420] [MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models](https://arxiv.org/abs/2508.13148)
*Haoyu He,Katrin Renz,Yong Cao,Andreas Geiger*

Main category: cs.LG

TL;DR: The paper introduces "Masked Diffusion Policy Optimization" (MDPO) to address discrepancies between training and inference in diffusion language models, achieving improved performance with fewer gradient updates, along with a remasking strategy called RCR that enhances model adaptability.


<details>
  <summary>Details</summary>
Motivation: Previous works overlooked the key discrepancy in the structure between training and inference stages in Masked Diffusion Language Models (MDLMs), leading to suboptimal performance.

Method: The authors framed denoising trajectory learning as a sequential decision-making problem, applied reinforcement learning to optimize diffusion processes, and proposed two approaches: MDPO for training with progressive refining schedules, and RCR as a training-free plug-in for enhanced token refinement.

Result: MDPO achieved matching performance with 60x fewer gradient updates or outperforming the state-of-the-art method with up to 54.2% improvement in specific tasks. RCR further brought consistent performance gains, especially when combined with MDPO.

Conclusion: The paper highlights the significance of addressing training-inference discrepancies in MDLMs and demonstrated that MDPO, with the addition of RCR, can significantly enhance the performance of such models with efficient computation.

Abstract: Diffusion language models, as a promising alternative to traditional
autoregressive (AR) models, enable faster generation and richer conditioning on
bidirectional context. However, they suffer from a key discrepancy between
training and inference: during inference, MDLMs progressively reveal the
structure of the generated sequence by producing fewer and fewer masked tokens,
whereas this structure is ignored in training as tokens are masked at random.
Although this discrepancy between training and inference can lead to suboptimal
performance, it has been largely overlooked by previous works, leaving closing
this gap between the two stages an open problem. To address this, we frame the
problem of learning effective denoising trajectories as a sequential
decision-making problem and use the resulting framework to apply reinforcement
learning. We propose a novel Masked Diffusion Policy Optimization (MDPO) to
exploit the Markov property diffusion possesses and explicitly train the model
under the same progressive refining schedule used at inference. MDPO matches
the performance of the previous state-of-the-art (SOTA) method with 60x fewer
gradient updates, while achieving average improvements of 9.6% on MATH500 and
54.2% on Countdown over SOTA when trained within the same number of weight
updates. Additionally, we improve the remasking strategy of MDLMs as a plug-in
inference replacement to overcome the limitation that the model cannot refine
tokens flexibly. This simple yet effective training-free strategy, what we
refer to as RCR, consistently improves performance and yields additional gains
when combined with MDPO. Our findings establish great potential for
investigating the discrepancy between pre-training and inference of MDLMs.
Code: https://github.com/autonomousvision/mdpo. Project Page:
https://cli212.github.io/MDPO/.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [421] [Toward Practical Equilibrium Propagation: Brain-inspired Recurrent Neural Network with Feedback Regulation and Residual Connections](https://arxiv.org/abs/2508.11659)
*Zhuo Liu,Tao Chen*

Main category: cs.NE

TL;DR: The paper introduces a biologically inspired Feedback-regulated Residual recurrent neural network (FRE-RNN) integrated into the Equilibrium Propagation (EP) framework, addressing computational inefficiencies and matching backpropagation performance.


<details>
  <summary>Details</summary>
Motivation: The goal is to address the instability and high computational costs associated with existing implementations of Equilibrium Propagation, while aiming for biologically plausible methods suited for brain-inspired computing hardware.

Method: The FRE-RNN employs feedback regulation to achieve rapid convergence by reducing spectral radius and leverages residual connections with brain-inspired topologies to address vanishing gradients and improve learning dynamics.

Result: The proposed FRE-RNN achieves faster convergence, lower computational costs, and training efficiency comparable to backpropagation in benchmark tasks, enabling practical use of EP in complex networks.

Conclusion: This work significantly improves the feasibility of EP for large-scale artificial intelligence models and offers insights for developing in-situ learning mechanisms in physical neural networks.

Abstract: Brain-like intelligent systems need brain-like learning methods. Equilibrium
Propagation (EP) is a biologically plausible learning framework with strong
potential for brain-inspired computing hardware. However, existing
im-plementations of EP suffer from instability and prohibi-tively high
computational costs. Inspired by the structure and dynamics of the brain, we
propose a biologically plau-sible Feedback-regulated REsidual recurrent neural
network (FRE-RNN) and study its learning performance in EP framework. Feedback
regulation enables rapid convergence by reducing the spectral radius. The
improvement in con-vergence property reduces the computational cost and
train-ing time of EP by orders of magnitude, delivering perfor-mance on par
with backpropagation (BP) in benchmark tasks. Meanwhile, residual connections
with brain-inspired topologies help alleviate the vanishing gradient problem
that arises when feedback pathways are weak in deep RNNs. Our approach
substantially enhances the applicabil-ity and practicality of EP in large-scale
networks that un-derpin artificial intelligence. The techniques developed here
also offer guidance to implementing in-situ learning in physical neural
networks.

</details>


### [422] [Learning Internal Biological Neuron Parameters and Complexity-Based Encoding for Improved Spiking Neural Networks Performance](https://arxiv.org/abs/2508.11674)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.NE

TL;DR: The paper proposes a biologically inspired probabilistic meta neuron to replace traditional perceptrons, improving classification accuracy in spiking neural networks (SNNs), and introduces a classification framework combining Lempel-Ziv complexity (LZC) with SNNs.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve the classification accuracy of SNNs by integrating biologically inspired enhancements and address gaps in existing works on classifying spatiotemporal neural data with interpretability.

Method: The authors replace standard perceptrons with a probabilistic meta neuron model, compare SNN architectures, introduce a classification framework combining SNNs with LZC, and explore neuron training through algorithms like backpropagation, STDP, and Tempotron, using Poisson processes to model neural spike dynamics.

Result: Depending on the training method, the probabilistic meta neuron improves classifier efficiency by up to 11.00%.

Conclusion: The adoption of probabilistic meta neurons and combining SNNs with LZC leads to significant efficiency gains and novel biologically inspired approaches for spatiotemporal data classification.

Abstract: This study introduces a novel approach by replacing the traditional
perceptron neuron model with a biologically inspired probabilistic meta neuron,
where the internal neuron parameters are jointly learned, leading to improved
classification accuracy of spiking neural networks (SNNs). To validate this
innovation, we implement and compare two SNN architectures: one based on
standard leaky integrate-and-fire (LIF) neurons and another utilizing the
proposed probabilistic meta neuron model. As a second key contribution, we
present a new biologically inspired classification framework that uniquely
integrates SNNs with Lempel-Ziv complexity (LZC) a measure closely related to
entropy rate. By combining the temporal precision and biological plausibility
of SNNs with the capacity of LZC to capture structural regularity, the proposed
approach enables efficient and interpretable classification of spatiotemporal
neural data, an aspect not addressed in existing works. We consider learning
algorithms such as backpropagation, spike-timing-dependent plasticity (STDP),
and the Tempotron learning rule. To explore neural dynamics, we use Poisson
processes to model neuronal spike trains, a well-established method for
simulating the stochastic firing behavior of biological neurons. Our results
reveal that depending on the training method, the classifier's efficiency can
improve by up to 11.00%, highlighting the advantage of learning additional
neuron parameters beyond the traditional focus on weighted inputs alone.

</details>


### [423] [Adaptive Spiking with Plasticity for Energy Aware Neuromorphic Systems](https://arxiv.org/abs/2508.11689)
*Eduardo Calle-Ortiz,Hui Guan,Deepak Ganesan,Phuc Nguyen*

Main category: cs.NE

TL;DR: The paper introduces ASPEN, a technique aimed at reducing energy consumption in neuromorphic systems by minimizing spiking activity, enhancing robustness, and maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the energy constraints of always-on neuromorphic systems for wearables by developing a method to minimize spiking activity and improve energy efficiency.

Method: ASPEN uses stochastic perturbations to neuronal thresholds during training to enhance robustness, reduce spiking, and enable adaptive energy control without retraining or reconfiguring the model.

Result: ASPEN demonstrates significant reductions in spike counts and energy consumption while maintaining accuracy comparable to other state-of-the-art methods.

Conclusion: The ASPEN technique is a lightweight, scalable solution for dynamic energy control in resource-constrained neuromorphic systems, with promising implications for future wearable devices.

Abstract: This paper presents ASPEN, a novel energy-aware technique for neuromorphic
systems that could unleash the future of intelligent, always-on,
ultra-low-power, and low-burden wearables. Our main research objectives are to
explore the feasibility of neuromorphic computing for wearables, identify open
research directions, and demonstrate the feasibility of developing an adaptive
spiking technique for energy-aware computation, which can be game-changing for
resource-constrained devices in always-on applications. As neuromorphic
computing systems operate based on spike events, their energy consumption is
closely related to spiking activity, i.e., each spike incurs computational and
power costs; consequently, minimizing the number of spikes is a critical
strategy for operating under constrained energy budgets. To support this goal,
ASPEN utilizes stochastic perturbations to the neuronal threshold during
training to not only enhance the network's robustness across varying
thresholds, which can be controlled at inference time, but also act as a
regularizer that improves generalization, reduces spiking activity, and enables
energy control without the need for complex retraining or pruning. More
specifically, ASPEN adaptively adjusts intrinsic neuronal parameters as a
lightweight and scalable technique for dynamic energy control without
reconfiguring the entire model. Our evaluation on neuromorphic emulator and
hardware shows that ASPEN significantly reduces spike counts and energy
consumption while maintaining accuracy comparable to state-of-the-art methods.

</details>


### [424] [Data-Driven Discovery of Interpretable Kalman Filter Variants through Large Language Models and Genetic Programming](https://arxiv.org/abs/2508.11703)
*Vasileios Saketos,Sebastian Kaltenbach,Sergey Litvinov,Petros Koumoutsakos*

Main category: cs.NE

TL;DR: This paper investigates using Cartesian Genetic Programming (CGP) and Large Language Models (LLM) to automatically discover the Kalman Filter algorithm and interpretable alternatives.


<details>
  <summary>Details</summary>
Motivation: To explore whether an automated, data-driven approach can replace human ingenuity in discovering scientific computing algorithms like the Kalman Filter.

Method: Combining CGP and LLM-assisted evolution to discover algorithms through adaptive, evolutionary processes under varying optimality conditions.

Result: The framework converged to near-optimal solutions when Kalman filter assumptions held and evolved superior alternatives when assumptions were violated.

Conclusion: Integrating evolutionary algorithms and generative models provides an effective, interpretable method for automated algorithm discovery in scientific computing.

Abstract: Algorithmic discovery has traditionally relied on human ingenuity and
extensive experimentation. Here we investigate whether a prominent scientific
computing algorithm, the Kalman Filter, can be discovered through an automated,
data-driven, evolutionary process that relies on Cartesian Genetic Programming
(CGP) and Large Language Models (LLM). We evaluate the contributions of both
modalities (CGP and LLM) in discovering the Kalman filter under varying
conditions. Our results demonstrate that our framework of CGP and LLM-assisted
evolution converges to near-optimal solutions when Kalman optimality
assumptions hold. When these assumptions are violated, our framework evolves
interpretable alternatives that outperform the Kalman filter. These results
demonstrate that combining evolutionary algorithms and generative models for
interpretable, data-driven synthesis of simple computational modules is a
potent approach for algorithmic discovery in scientific computing.

</details>


### [425] [LLM4CMO: Large Language Model-aided Algorithm Design for Constrained Multiobjective Optimization](https://arxiv.org/abs/2508.11871)
*Zhen-Song Chen,Hong-Wei Ding,Xian-Jia Wang,Witold Pedrycz*

Main category: cs.NE

TL;DR: The paper introduces LLM4CMO, a novel algorithm integrating large language models to solve constrained multi-objective optimization problems, showing superior performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing high-performing constrained multi-objective evolutionary algorithms and explore the integration of large language models as co-designers.

Method: The proposed LLM4CMO algorithm uses a dual-population, two-stage framework with specific techniques like hybrid operators, epsilon-based constraint handling, classification-based strategies, and dynamic resource allocation, optimized with LLM-human interaction.

Result: Experimental results demonstrate LLM4CMO consistently outperforms eleven state-of-the-art algorithms on benchmark suites and real-world problems. Ablation studies confirm the benefits of LLM-aided modular design.

Conclusion: LLM4CMO showcases the potential of large language models in co-designing evolutionary optimization algorithms, achieving better results than existing methods and streamlining algorithm design.

Abstract: Constrained multi-objective optimization problems (CMOPs) frequently arise in
real-world applications where multiple conflicting objectives must be optimized
under complex constraints. Existing dual-population two-stage algorithms have
shown promise by leveraging infeasible solutions to improve solution quality.
However, designing high-performing constrained multi-objective evolutionary
algorithms (CMOEAs) remains a challenging task due to the intricacy of
algorithmic components. Meanwhile, large language models (LLMs) offer new
opportunities for assisting with algorithm design; however, their effective
integration into such tasks remains underexplored. To address this gap, we
propose LLM4CMO, a novel CMOEA based on a dual-population, two-stage framework.
In Stage 1, the algorithm identifies both the constrained Pareto front (CPF)
and the unconstrained Pareto front (UPF). In Stage 2, it performs targeted
optimization using a combination of hybrid operators (HOps), an epsilon-based
constraint-handling method, and a classification-based UPF-CPF relationship
strategy, along with a dynamic resource allocation (DRA) mechanism. To reduce
design complexity, the core modules, including HOps, epsilon decay function,
and DRA, are decoupled and designed through prompt template engineering and
LLM-human interaction. Experimental results on six benchmark test suites and
ten real-world CMOPs demonstrate that LLM4CMO outperforms eleven
state-of-the-art baseline algorithms. Ablation studies further validate the
effectiveness of the LLM-aided modular design. These findings offer preliminary
evidence that LLMs can serve as efficient co-designers in the development of
complex evolutionary optimization algorithms. The code associated with this
article is available at https://anonymous.4open.science/r/LLM4CMO971.

</details>


### [426] [Improving MSA Estimation through Adaptive Weight Vectors in MOEA/D](https://arxiv.org/abs/2508.12133)
*Saem Hasan,Muhammad Ali Nayeem,M. Sohel Rahman*

Main category: cs.NE

TL;DR: This paper introduces PMAO++, a new approach combining MOEA/D-ADF and PMAO to improve sequence alignment and phylogenetic tree inference. It achieves better performance on benchmark tests and enriches downstream analysis.


<details>
  <summary>Details</summary>
Motivation: Accurate phylogenetic inference relies on high-quality multiple sequence alignments, but achieving optimal alignment is computationally challenging and sensitive to scoring. This work aims to improve this process.

Method: The authors introduced MOEA/D-ADF, a variant of MOEA/D, to improve the exploration-exploitation balance in alignment. This is combined with PMAO, forming PMAO++, which generates and evolves solutions using diverse weight vectors for alignment-tree optimization.

Result: PMAO++ outperforms its predecessor, PMAO, on most benchmarks by reducing false-negative rates and providing better alignment-tree pairs, enhancing robustness in phylogenetic inference. However, challenges remain for specific datasets.

Conclusion: PMAO++ shows clear advantages in phylogenetic analysis by improving alignment quality and providing valuable outputs for downstream methods. Further work will address challenges and refine the method for broader use.

Abstract: Accurate phylogenetic inference from biological sequences depends critically
on the quality of multiple sequence alignments, yet optimal alignment for many
sequences is computationally intractable and sensitive to scoring choices. In
this work we introduce MOEA/D-ADF, a novel variant of MOEA/D that adaptively
adjusts subproblem weight vectors based on fitness variance to improve the
exploration-exploitation trade-off. We combine MOEA/D-ADF with PMAO (PASTA with
many application-aware optimization criteria) to form PMAO++, where
PMAO-generated solutions are used to seed MOEA/D-ADF, which then evolves a
population using 30 weight vectors to produce a diverse ensemble of
alignment-tree pairs. PMAO++ outperforms the original PMAO on a majority of
benchmark cases, achieving better false-negative (FN) rates on 12 of 17
BAliBASE-derived datasets and producing superior best-case trees, including
several instances with zero FN rate. Beyond improving single best alignments,
the rich set of alignment-tree pairs produced by PMAO++ is especially valuable
for downstream summary methods (for example, consensus and summary-tree
approaches), allowing more robust phylogenetic inference by integrating signal
across multiple plausible alignments and trees. Certain dataset features, such
as large terminal N/C extensions found in the RV40 group, remain challenging,
but overall PMAO++ demonstrates clear advantages for sequence-based
phylogenetic analysis. Future work will explore parameter tuning, larger
benchmark suites, and tighter integration with summary-tree pipelines to
further enhance applicability for biological sequence studies.

</details>


### [427] [A Self-Ensemble Inspired Approach for Effective Training of Binary-Weight Spiking Neural Networks](https://arxiv.org/abs/2508.12609)
*Qingyan Meng,Mingqing Xiao,Zhengyu Ma,Huihui Zhou,Yonghong Tian,Zhouchen Lin*

Main category: cs.NE

TL;DR: The paper explores the connection between Spiking Neural Networks (SNNs) and Binary Neural Networks (BNNs) to improve SNN training using a method called SEI-BWSNN, achieving high performance even with binary weights.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of training SNNs, which arises from the non-differentiable spike generation function, by exploring their relation to BNNs and proposing methods to improve binary-weight SNN training.

Method: The authors propose the Self-Ensemble Inspired training method for Binary-Weight SNNs (SEI-BWSNN). This involves noise-injected networks, shortcut structures, and knowledge distillation-based training in SNNs and FFN layers of Transformers.

Result: The proposed SEI-BWSNN method achieves 82.52% accuracy on ImageNet using a binary-weight Transformer architecture, requiring only 2 time steps.

Conclusion: The study bridges the gap between SNNs and BNNs, offering a new perspective on training dynamics for energy-efficient neural networks and demonstrating the efficacy of binary-weight SNNs.

Abstract: Spiking Neural Networks (SNNs) are a promising approach to low-power
applications on neuromorphic hardware due to their energy efficiency. However,
training SNNs is challenging because of the non-differentiable spike generation
function. To address this issue, the commonly used approach is to adopt the
backpropagation through time framework, while assigning the gradient of the
non-differentiable function with some surrogates. Similarly, Binary Neural
Networks (BNNs) also face the non-differentiability problem and rely on
approximating gradients. However, the deep relationship between these two
fields and how their training techniques can benefit each other has not been
systematically researched. Furthermore, training binary-weight SNNs is even
more difficult. In this work, we present a novel perspective on the dynamics of
SNNs and their close connection to BNNs through an analysis of the
backpropagation process. We demonstrate that training a feedforward SNN can be
viewed as training a self-ensemble of a binary-activation neural network with
noise injection. Drawing from this new understanding of SNN dynamics, we
introduce the Self-Ensemble Inspired training method for (Binary-Weight) SNNs
(SEI-BWSNN), which achieves high-performance results with low latency even for
the case of the 1-bit weights. Specifically, we leverage a structure of
multiple shortcuts and a knowledge distillation-based training technique to
improve the training of (binary-weight) SNNs. Notably, by binarizing FFN layers
in a Transformer architecture, our approach achieves 82.52% accuracy on
ImageNet with only 2 time steps, indicating the effectiveness of our
methodology and the potential of binary-weight SNNs.

</details>


### [428] [IzhiRISC-V -- a RISC-V-based Processor with Custom ISA Extension for Spiking Neuron Networks Processing with Izhikevich Neurons](https://arxiv.org/abs/2508.12846)
*Wiktor J. Szczerek,Artur Podobas*

Main category: cs.NE

TL;DR: This paper proposes a custom ISA extension for spiking neural networks on RISC-V processors, introducing neuromorphic instructions to improve efficiency.


<details>
  <summary>Details</summary>
Motivation: The inefficiency of general-purpose RISC-V processors in handling spiking neural network computations due to repeated usage of basic instructions.

Method: The researchers designed a custom neuromorphic ISA extension and bespoke hardware additions to the ALU to optimize neuron updating in spiking neural networks.

Result: They implemented this neuromorphic ISA extension on a RISC-V-compliant processor called IzhiRISC-V.

Conclusion: The study marks the initial step towards efficient large-scale neuromorphic systems based on RISC-V processors by integrating customized neuromorphic instructions.

Abstract: Spiking Neural Network processing promises to provide high energy efficiency
due to the sparsity of the spiking events. However, when realized on
general-purpose hardware -- such as a RISC-V processor -- this promise can be
undermined and overshadowed by the inefficient code, stemming from repeated
usage of basic instructions for updating all the neurons in the network. One of
the possible solutions to this issue is the introduction of a custom ISA
extension with neuromorphic instructions for spiking neuron updating, and
realizing those instructions in bespoke hardware expansion to the existing ALU.
In this paper, we present the first step towards realizing a large-scale system
based on the RISC-V-compliant processor called IzhiRISC-V, supporting the
custom neuromorphic ISA extension.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [429] [StackPilot: Autonomous Function Agents for Scalable and Environment-Free Code Execution](https://arxiv.org/abs/2508.11665)
*Xinkui Zhao,Yifan Zhang,Zhengyi Zhou,Yueshen Xu*

Main category: cs.PL

TL;DR: StackPilot is a multi-agent framework for verifying and executing LLM-generated code, independent of traditional compilers or runtimes, achieving high reliability rates.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenges associated with verifying correctness and executability of code generated by large language models, which traditional compilers and runtimes struggle to address in a language-agnostic manner.

Method: The paper introduces StackPilot, which features (1) a Function-as-Agents paradigm for autonomous function verification, (2) an LLM-as-Executor strategy for scalable stack-based scheduling, and (3) a snapshot mechanism for deterministic context preservation during verification.

Result: Empirical evaluations show StackPilot achieves reliability rates of 89% to 97%, outperforming traditional methods in executing and verifying LLM-generated code.

Conclusion: StackPilot offers a more robust and language-agnostic solution to verifying and executing LLM-generated code, overcoming limitations of traditional toolchains and enabling higher reliability across diverse tasks.

Abstract: Recent advances in large language models (LLMs) have substantially enhanced
automated code generation across a wide range of programming languages.
Nonetheless, verifying the correctness and executability of LLM-generated code
remains a significant challenge, as traditional methods rely on
language-specific compilers and environment-dependent runtimes. To overcome
these limitations, we introduce StackPilot, an LLM-native, multi-agent
framework designed for language-agnostic code verification and execution, which
operates independently of conventional toolchains. StackPilot offers three
principal innovations: (1) a Function-as-Agents paradigm, in which each
function is modeled as an autonomous agent capable of fine-grained reasoning
and collaborative verification; (2) an LLM-as-Executor strategy, which enables
scalable verification via stack-based scheduling; and (3) a novel snapshot
mechanism that preserves complete execution contexts, facilitating
deterministic and lossless context switching during verification. Empirical
evaluations demonstrate that StackPilot achieves framework reliability rates
between 89% and 97%, substantially outperforming baseline approaches. These
results indicate that StackPilot can reliably verify and execute a
significantly larger proportion of LLM-generated code across diverse
programming tasks compared to existing methods.

</details>


### [430] [Certified Compilation based on Gödel Numbers](https://arxiv.org/abs/2508.12054)
*Guilherme de Oliveira Silva,Fernando Magno Quintão Pereira*

Main category: cs.PL

TL;DR: The paper tackles the issue of untrustworthy compilers inserting backdoors into programs. It introduces certificates to verify binary matches source code accurately and showcases its feasibility through a compiler named Charon.


<details>
  <summary>Details</summary>
Motivation: To address the trust issues with compilers, specifically those introduced by Ken Thompson's demonstration of maliciously altered compilers perpetuating backdoors.

Method: The method involves generating certificates derived from both the source code and the binary, governed by simple derivation rules applied in constant time, ensuring faithful binary representation of the source.

Result: The authors present Charon, a compiler capable of handling a subset of C, demonstrating this novel certification method effectively.

Conclusion: This approach represents a meaningful step toward trusted compiler verification, potentially shifting reliance away from existing methods like Diverse Double-Compiling and translation validation.

Abstract: In his 1984 Turing Award lecture, Ken Thompson showed that a compiler could
be maliciously altered to insert backdoors into programs it compiles and
perpetuate this behavior by modifying any compiler it subsequently builds.
Thompson's hack has been reproduced in real-world systems for demonstration
purposes. Several countermeasures have been proposed to defend against
Thompson-style backdoors, including the well-known {\it Diverse
Double-Compiling} (DDC) technique, as well as methods like translation
validation and CompCert-style compilation. However, these approaches ultimately
circle back to the fundamental question: "How can we trust the compiler used to
compile the tools we rely on?" In this paper, we introduce a novel approach to
generating certificates to guarantee that a binary image faithfully represents
the source code. These certificates ensure that the binary contains all and
only the statements from the source code, preserves their order, and maintains
equivalent def-use dependencies. The certificate is represented as an integer
derivable from both the source code and the binary using a concise set of
derivation rules, each applied in constant time. To demonstrate the
practicality of our method, we present Charon, a compiler designed to handle a
subset of C expressive enough to compile FaCT, the Flexible and Constant Time
cryptographic programming language.

</details>


### [431] [Controlling Copatterns: There and Back Again (Extended Version)](https://arxiv.org/abs/2508.12427)
*Paul Downen*

Main category: cs.PL

TL;DR: This paper explores the semantics of copatterns, deriving a suite of semantic models using Danvy's correspondence, and reformulating the operational to abstract machine journey twice for compositional depth.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of precisely specifying the behavior of highly expressive compositional copattern mechanisms in functional programming.

Method: The authors use Danvy's functional correspondence to systematically derive semantic models, transitioning from operational semantics to abstract machines and continuation-passing style, and then refactoring for compositional copatterns.

Result: A comprehensive suite of semantics is derived for both monolithic and compositional copatterns, highlighting operational flexibility and compositional expressiveness.

Conclusion: This work reinforces the utility of copattern-based frameworks and paves the way for refined semantic precision in compositional mechanisms for functional programming.

Abstract: Copatterns give functional programs a flexible mechanism for responding to
their context, and composition can greatly enhance their expressiveness.
However, that same expressive power makes it harder to precisely specify the
behavior of programs. Using Danvy's functional and syntactic correspondence
between different semantic artifacts, we derive a full suite of semantics for
copatterns, twice. First, a calculus of monolithic copatterns is taken on a
journey from small-step operational semantics to abstract machine to
continuation-passing style. Then within continuation-passing style, we refactor
the semantics to derive a more general calculus of compositional copatterns,
and take the return journey back to derive the other semantic artifacts in
reverse order.

</details>


### [432] [Type-Driven Prompt Programming: From Typed Interfaces to a Calculus of Constraints](https://arxiv.org/abs/2508.12475)
*Abhijit Paul*

Main category: cs.PL

TL;DR: This paper introduces Lambda Prompt, a dependently typed framework with probabilistic refinements for enhancing type systems in prompt programming.


<details>
  <summary>Details</summary>
Motivation: To address gaps in constraint expressiveness and algorithmic support within type-based prompt programming frameworks.

Method: Introduces Lambda Prompt, a dependently typed framework, and a catalog of 13 constraints along with a constraint-preserving optimization rule.

Result: Identifies underexplored constraint areas (constraints 9-13) and proposes foundational tools for advancing prompt programming frameworks.

Conclusion: Establishes a foundation for type-theoretic prompt programming and outlines future research on compiler development for prompt programs.

Abstract: Prompt programming treats large language model prompts as software components
with typed interfaces. Based on a literature survey of 15 recent works from
2023 to 2025, we observe a consistent trend: type systems are central to
emerging prompt programming frameworks. However, there are gaps in constraint
expressiveness and in supporting algorithms. To address these issues, we
introduce the notion of Lambda Prompt, a dependently typed calculus with
probabilistic refinements for syntactic and semantic constraints. While this is
not yet a full calculus, the formulation motivates a type-theoretic foundation
for prompt programming. Our catalog of 13 constraints highlights underexplored
areas in constraint expressiveness (constraints 9 through 13). To address the
algorithmic gap, we propose a constraint-preserving optimization rule. Finally,
we outline research directions on developing a compiler for prompt programs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [433] [Using Natural Language for Human-Robot Collaboration in the Real World](https://arxiv.org/abs/2508.11759)
*Peter Lindes,Kaoutar Skiker*

Main category: cs.RO

TL;DR: This paper explores integrating large language models (LLMs) into robotics for enhanced natural language understanding in human-robot collaboration, outlining challenges and presenting proof-of-concept experiments.


<details>
  <summary>Details</summary>
Motivation: To enable robots to effectively collaborate with humans by enhancing their natural language communication abilities using LLMs.

Method: Analysis of commercial collaborative robots, conceptualization of AI integration with LLMs, and conducting proof-of-concept experiments using ChatGPT.

Result: Three proof-of-concept experiments demonstrate initial feasibility of using LLMs for natural language understanding in robotic systems.

Conclusion: Future advancements are needed to transform basic experiments into operational systems where LLM-integrated robots collaborate naturally and effectively with humans.

Abstract: We have a vision of a day when autonomous robots can collaborate with humans
as assistants in performing complex tasks in the physical world. This vision
includes that the robots will have the ability to communicate with their human
collaborators using language that is natural to the humans. Traditional
Interactive Task Learning (ITL) systems have some of this ability, but the
language they can understand is very limited. The advent of large language
models (LLMs) provides an opportunity to greatly improve the language
understanding of robots, yet integrating the language abilities of LLMs with
robots that operate in the real physical world is a challenging problem.
  In this chapter we first review briefly a few commercial robot products that
work closely with humans, and discuss how they could be much better
collaborators with robust language abilities. We then explore how an AI system
with a cognitive agent that controls a physical robot at its core, interacts
with both a human and an LLM, and accumulates situational knowledge through its
experiences, can be a possible approach to reach that vision. We focus on three
specific challenges of having the robot understand natural language, and
present a simple proof-of-concept experiment using ChatGPT for each. Finally,
we discuss what it will take to turn these simple experiments into an
operational system where LLM-assisted language understanding is a part of an
integrated robotic assistant that uses language to collaborate with humans.

</details>


### [434] [Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal Robots](https://arxiv.org/abs/2508.11802)
*Luigi Penco,Beomyeong Park,Stefan Fasano,Nehar Poddar,Stephen McCrory,Nicholas Kitchel,Tomasz Bialek,Dexton Anderson,Duncan Calvert,Robert Griffin*

Main category: cs.RO

TL;DR: The paper proposes a system for real-time robot locomotion synchronization with user steps during teleoperation, ensuring stability and adaptability to uneven terrains.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in achieving synchronized motion in teleoperation, especially during high-speed tasks, by improving step transfer between user and robot.

Method: The approach involves retargeting user steps to robotic footstep locations rather than direct pose replication. It anticipates user footsteps to reduce delays and adjusts robot steps based on terrain.

Result: Experimental evaluations on the humanoid robot Nadia showcase the system's effectiveness in maintaining synchronization and navigating uneven terrains.

Conclusion: The proposed system enables seamless, real-time step synchronization between user and robot, enhancing stability and adaptability in teleoperated tasks.

Abstract: Achieving seamless synchronization between user and robot motion in
teleoperation, particularly during high-speed tasks, remains a significant
challenge. In this work, we propose a novel approach for transferring stepping
motions from the user to the robot in real-time. Instead of directly
replicating user foot poses, we retarget user steps to robot footstep
locations, allowing the robot to utilize its own dynamics for locomotion,
ensuring better balance and stability. Our method anticipates user footsteps to
minimize delays between when the user initiates and completes a step and when
the robot does it. The step estimates are continuously adapted to converge with
the measured user references. Additionally, the system autonomously adjusts the
robot's steps to account for its surrounding terrain, overcoming challenges
posed by environmental mismatches between the user's flat-ground setup and the
robot's uneven terrain. Experimental results on the humanoid robot Nadia
demonstrate the effectiveness of the proposed system.

</details>


### [435] [LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba](https://arxiv.org/abs/2508.11849)
*Allen Wang,Gavin Tao*

Main category: cs.RO

TL;DR: The paper introduces LocoMamba, a vision-driven DRL framework that optimizes state representation and long-term dependencies to enhance efficiency in robot policy training and execution.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of efficiently training deep reinforcement learning (DRL) policies capable of handling complex environments with uneven terrains and obstacles while maintaining performance scalability.

Method: They utilize a selective state-space model with Mamba layers for near-linear-time sequence modeling combined with end-to-end training through Proximal Policy Optimization, leveraging depth images and proprioceptive states as compact inputs.

Result: LocoMamba achieves higher success rates and efficiency compared to baselines, handles unseen terrains well, reduces collisions, and converges faster in training under the same computational constraints.

Conclusion: The proposed framework effectively balances progress, safety, and smoothness in robot navigation while demonstrating improved generalization and training scalability.

Abstract: We introduce LocoMamba, a vision-driven cross-modal DRL framework built on
selective state-space models, specifically leveraging Mamba, that achieves
near-linear-time sequence modeling, effectively captures long-range
dependencies, and enables efficient training with longer sequences. First, we
embed proprioceptive states with a multilayer perceptron and patchify depth
images with a lightweight convolutional neural network, producing compact
tokens that improve state representation. Second, stacked Mamba layers fuse
these tokens via near-linear-time selective scanning, reducing latency and
memory footprint, remaining robust to token length and image resolution, and
providing an inductive bias that mitigates overfitting. Third, we train the
policy end-to-end with Proximal Policy Optimization under terrain and
appearance randomization and an obstacle-density curriculum, using a compact
state-centric reward that balances progress, smoothness, and safety. We
evaluate our method in challenging simulated environments with static and
moving obstacles as well as uneven terrain. Compared with state-of-the-art
baselines, our method achieves higher returns and success rates with fewer
collisions, exhibits stronger generalization to unseen terrains and obstacle
densities, and improves training efficiency by converging in fewer updates
under the same compute budget.

</details>


### [436] [Data Shift of Object Detection in Autonomous Driving](https://arxiv.org/abs/2508.11868)
*Lida Xu*

Main category: cs.RO

TL;DR: The paper addresses the challenges of data distribution shifts in autonomous driving systems and proposes a solution using CycleGAN-based data augmentation integrated with YOLOv5, showing improved results on BDD100K dataset.


<details>
  <summary>Details</summary>
Motivation: Current machine learning models for autonomous driving struggle with data shifts caused by environmental changes, which compromises their reliability in real-world scenarios.

Method: The study analyzes data shift problems, reviews detection methods, and uses CycleGAN-based data augmentation integrated with the YOLOv5 framework.

Result: The proposed model outperformed baseline models in object detection tasks on the BDD100K dataset.

Conclusion: The integration of advanced data augmentation techniques and detection frameworks effectively mitigates the impact of data shifts, improving autonomous driving object detection performance.

Abstract: With the widespread adoption of machine learning technologies in autonomous
driving systems, their role in addressing complex environmental perception
challenges has become increasingly crucial. However, existing machine learning
models exhibit significant vulnerability, as their performance critically
depends on the fundamental assumption that training and testing data satisfy
the independent and identically distributed condition, which is difficult to
guarantee in real-world applications. Dynamic variations in data distribution
caused by seasonal changes, weather fluctuations lead to data shift problems in
autonomous driving systems. This study investigates the data shift problem in
autonomous driving object detection tasks, systematically analyzing its
complexity and diverse manifestations. We conduct a comprehensive review of
data shift detection methods and employ shift detection analysis techniques to
perform dataset categorization and balancing. Building upon this foundation, we
construct an object detection model. To validate our approach, we optimize the
model by integrating CycleGAN-based data augmentation techniques with the
YOLOv5 framework. Experimental results demonstrate that our method achieves
superior performance compared to baseline models on the BDD100K dataset.

</details>


### [437] [Bioinspired underwater soft robots: from biology to robotics and back](https://arxiv.org/abs/2508.11883)
*Lei Li,Boyang Qin,Wenzhuo Gao,Yanyu Li,Yiyuan Zhang,Bo Wang,Shihan Kong,Jian Wang,Dekui He,Junzhi Yu*

Main category: cs.RO

TL;DR: The paper discusses the potential of underwater soft robotics inspired by biology, as well as how robotic development can contribute insights back into biology. It introduces a bidirectional framework merging biology and robotics.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between biology and engineering by creating soft robots capable of navigating underwater environments while also contributing to biological research.

Method: A holistic and bidirectional framework is proposed that integrates biological principles into robotic designs and validates these designs through biological applications and studies.

Result: Soft robots prove useful in probing biological functions, testing evolutionary hypotheses, and outperforming rigid robotics in unstructured marine environments.

Conclusion: Underwater soft robotics, through a bidirectional approach, can enhance both ocean exploration and biological discovery, though challenges in materials and technology remain.

Abstract: The ocean vast unexplored regions and diverse soft-bodied marine organisms
have spurred interest in bio-inspired underwater soft robotics. Recent advances
have enabled new capabilities in underwater movement, sensing, and interaction.
However, these efforts are largely unidirectional, with biology guiding
robotics while insights from robotics rarely feed back into biology. Here we
propose a holistic, bidirectional framework that integrates biological
principles, robotic implementation, and biological validation. We show that
soft robots can serve as experimental tools to probe biological functions and
even test evolutionary hypotheses. Their inherent compliance also allows them
to outperform rigid systems in unstructured environments, supporting
applications in marine exploration, manipulation, and medicine. Looking
forward, we introduce bio-universal-inspired robotics, a paradigm that
transcends species-specific mimicry by identifying convergent principles across
species to inspire more adaptable designs. Despite rapid progress, challenges
persist in material robustness, actuation efficiency, autonomy, and
intelligence. By uniting biology and engineering, soft robots can advance ocean
exploration and deepen scientific discovery.

</details>


### [438] [From Screen to Stage: Kid Cosmo, A Life-Like, Torque-Controlled Humanoid for Entertainment Robotics](https://arxiv.org/abs/2508.11884)
*Havel Liu,Mingzhang Zhu,Arturo Moises Flores Alvarez,Yuan Hung Lo,Conrad Ku,Federico Parres,Justin Quan,Colin Togashi,Aditya Navghare,Quanyou Wang,Dennis W. Hong*

Main category: cs.RO

TL;DR: The paper introduces 'Kid Cosmo,' a humanoid robot designed for entertainment, which combines advanced motion capabilities with character embodiment.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped potential of humanoid robots in the entertainment industry, focusing on integrating visual appeal and functional design to meet the unique challenges of character embodiment.

Method: Developed Kid Cosmo, a child-sized humanoid robot with 28 degrees of freedom, proprioceptive actuators, torque-control walking, and lifelike motion capabilities, based on a character from Netflix's movie 'The Electric State.'

Result: The paper highlights the platform's system architecture, challenges for functional entertainment robots, and solutions, along with findings on stability during coordinated upper and lower body movements.

Conclusion: The study demonstrates the feasibility of creating humanoid robots that embody characters while maintaining high technical functionality, bridging the gap between robotics and entertainment.

Abstract: Humanoid robots represent the cutting edge of robotics research, yet their
potential in entertainment remains largely unexplored. Entertainment as a field
prioritizes visuals and form, a principle that contrasts with the purely
functional designs of most contemporary humanoid robots. Designing
entertainment humanoid robots capable of fluid movement presents a number of
unique challenges. In this paper, we present Kid Cosmo, a research platform
designed for robust locomotion and life-like motion generation while imitating
the look and mannerisms of its namesake character from Netflix's movie The
Electric State. Kid Cosmo is a child-sized humanoid robot, standing 1.45 m tall
and weighing 25 kg. It contains 28 degrees of freedom and primarily uses
proprioceptive actuators, enabling torque-control walking and lifelike motion
generation. Following worldwide showcases as part of the movie's press tour, we
present the system architecture, challenges of a functional entertainment robot
and unique solutions, and our initial findings on stability during simultaneous
upper and lower body movement. We demonstrate the viability of
performance-oriented humanoid robots that prioritize both character embodiment
and technical functionality.

</details>


### [439] [Contact-Rich and Deformable Foot Modeling for Locomotion Control of the Human Musculoskeletal System](https://arxiv.org/abs/2508.11885)
*Haixin Gong,Chen Zhang,Yanan Sui*

Main category: cs.RO

TL;DR: The study introduces a deformable foot model integrated with musculoskeletal systems to improve gait simulations, outperforming conventional models in dynamic accuracy.


<details>
  <summary>Details</summary>
Motivation: Current musculoskeletal models oversimplify foot-ground interaction, impeding accurate simulation of walking dynamics.

Method: A contact-rich, deformable foot model was developed and trained using a two-stage policy to simulate natural walking patterns.

Result: Enhanced simulation that outperforms conventional models in biomechanics metrics and aligns closely with human gait data.

Conclusion: The framework improves contact modeling for musculoskeletal systems and paves the way for precise foot-ground interaction control in robotics.

Abstract: The human foot serves as the critical interface between the body and
environment during locomotion. Existing musculoskeletal models typically
oversimplify foot-ground contact mechanics, limiting their ability to
accurately simulate human gait dynamics. We developed a novel contact-rich and
deformable model of the human foot integrated within a complete musculoskeletal
system that captures the complex biomechanical interactions during walking. To
overcome the control challenges inherent in modeling multi-point contacts and
deformable material, we developed a two-stage policy training strategy to learn
natural walking patterns for this interface-enhanced model. Comparative
analysis between our approach and conventional rigid musculoskeletal models
demonstrated improvements in kinematic, kinetic, and gait stability metrics.
Validation against human subject data confirmed that our simulation closely
reproduced real-world biomechanical measurements. This work advances
contact-rich interface modeling for human musculoskeletal systems and
establishes a robust framework that can be extended to humanoid robotics
applications requiring precise foot-ground interaction control.

</details>


### [440] [Saliency-Based Attention Shifting: A Framework for Improving Driver Situational Awareness of Out-of-Label Hazards](https://arxiv.org/abs/2508.11887)
*Yousra Shleibik,Jordan Sinclair,Kerstin Haring*

Main category: cs.RO

TL;DR: The paper proposes a framework using real-time gaze tracking, saliency analysis, and alerts to improve human-driver attention during events requiring takeover in semi-autonomous vehicles.


<details>
  <summary>Details</summary>
Motivation: To address challenges in ensuring driver involvement and safety in critical takeover situations during semi-autonomous driving.

Method: Integration of gaze manipulation with real-time tracking, saliency analysis, and alerting techniques to maintain driver situational awareness.

Result: The conceptual framework enhances driver focus, mitigates target fixation, and improves human-vehicle collaboration.

Conclusion: Attention redirection techniques can effectively support safer and smoother transitions during takeover events in semi-autonomous vehicles.

Abstract: The advent of autonomous driving systems promises to transform transportation
by enhancing safety, efficiency, and comfort. As these technologies evolve
toward higher levels of autonomy, the need for integrated systems that
seamlessly support human involvement in decision-making becomes increasingly
critical. Certain scenarios necessitate human involvement, including those
where the vehicle is unable to identify an object or element in the scene, and
as such cannot take independent action. Therefore, situational awareness is
essential to mitigate potential risks during a takeover, where a driver must
assume control and autonomy from the vehicle. The need for driver attention is
important to avoid collisions with external agents and ensure a smooth
transition during takeover operations. This paper explores the integration of
attention redirection techniques, such as gaze manipulation through targeted
visual and auditory cues, to help drivers maintain focus on emerging hazards
and reduce target fixation in semi-autonomous driving scenarios. We propose a
conceptual framework that combines real-time gaze tracking, context-aware
saliency analysis, and synchronized visual and auditory alerts to enhance
situational awareness, proactively address potential hazards, and foster
effective collaboration between humans and autonomous systems.

</details>


### [441] [Integrating Symbolic RL Planning into a BDI-based Autonomous UAV Framework: System Integration and SIL Validation](https://arxiv.org/abs/2508.11890)
*Sangwoo Jeon,Juchul Shin,YeonJe Cho,Gyeong-Tae Kim,Seongwoo Kim*

Main category: cs.RO

TL;DR: The paper presents the AMAD-SRL framework combining symbolic planning with reinforcement learning for complex UAV missions, showing a 75% improvement in mission efficiency in a simulation setting.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional rule-based architectures in managing dynamically complex UAV environments requiring adaptive symbolic planning.

Method: The study integrates symbolic reinforcement learning with the AMAD cognitive multi-agent architecture and evaluates it in a Software-in-the-Loop (SIL) environment identical to Hardware-In-the-Loop Simulation (HILS).

Result: Experimental findings show a successful integration of modules and improved UAV mission efficiency (75% reduction in travel distance) compared to a baseline.

Conclusion: The research establishes a strong basis for complex UAV mission planning with symbolic RL, highlighting its potential scalability and reliability, along with avenues for future improvement.

Abstract: Modern autonomous drone missions increasingly require software frameworks
capable of seamlessly integrating structured symbolic planning with adaptive
reinforcement learning (RL). Although traditional rule-based architectures
offer robust structured reasoning for drone autonomy, their capabilities fall
short in dynamically complex operational environments that require adaptive
symbolic planning. Symbolic RL (SRL), using the Planning Domain Definition
Language (PDDL), explicitly integrates domain-specific knowledge and
operational constraints, significantly improving the reliability and safety of
unmanned aerial vehicle (UAV) decision making. In this study, we propose the
AMAD-SRL framework, an extended and refined version of the Autonomous Mission
Agents for Drones (AMAD) cognitive multi-agent architecture, enhanced with
symbolic reinforcement learning for dynamic mission planning and execution. We
validated our framework in a Software-in-the-Loop (SIL) environment structured
identically to an intended Hardware-In-the-Loop Simulation (HILS) platform,
ensuring seamless transition to real hardware. Experimental results demonstrate
stable integration and interoperability of modules, successful transitions
between BDI-driven and symbolic RL-driven planning phases, and consistent
mission performance. Specifically, we evaluate a target acquisition scenario in
which the UAV plans a surveillance path followed by a dynamic reentry path to
secure the target while avoiding threat zones. In this SIL evaluation, mission
efficiency improved by approximately 75% over a coverage-based baseline,
measured by travel distance reduction. This study establishes a robust
foundation for handling complex UAV missions and discusses directions for
further enhancement and validation.

</details>


### [442] [OmniD: Generalizable Robot Manipulation Policy via Image-Based BEV Representation](https://arxiv.org/abs/2508.11898)
*Jilei Mao,Jiarui Guan,Yingjuan Tang,Qirui Hu,Zhihang Li,Junjie Yu,Yongjie Mao,Yunzhe Sun,Shuang Liu,Xiaozhu Ju*

Main category: cs.RO

TL;DR: The paper introduces OmniD, a framework to improve visuomotor policy generalization by synthesizing multi-view image observations into a single bird's-eye perspective.


<details>
  <summary>Details</summary>
Motivation: Visuomotor policies often suffer from overfitting to specific training environments, limiting their ability to generalize across scenarios, especially when integrating multi-view information for 3D understanding.

Method: The proposed OmniD framework uses a deformable attention-based Omni-Feature Generator (OFG) to fuse multi-view inputs into a bird's-eye view representation, abstracting task-relevant features while minimizing noise and distractions.

Result: OmniD showed noticeable improvement over the best existing models in three key scenarios, achieving 11% better performance in in-distribution tests, 17% improvement in generalization to out-of-distribution environments, and 84% better results in few-shot experiments.

Conclusion: OmniD effectively addresses overfitting and multi-view integration challenges, presenting a significant step forward for generalized visuomotor policies. The training code and benchmarks are also accessible publicly.

Abstract: The visuomotor policy can easily overfit to its training datasets, such as
fixed camera positions and backgrounds. This overfitting makes the policy
perform well in the in-distribution scenarios but underperform in the
out-of-distribution generalization. Additionally, the existing methods also
have difficulty fusing multi-view information to generate an effective 3D
representation. To tackle these issues, we propose Omni-Vision Diffusion Policy
(OmniD), a multi-view fusion framework that synthesizes image observations into
a unified bird's-eye view (BEV) representation. We introduce a deformable
attention-based Omni-Feature Generator (OFG) to selectively abstract
task-relevant features while suppressing view-specific noise and background
distractions. OmniD achieves 11\%, 17\%, and 84\% average improvement over the
best baseline model for in-distribution, out-of-distribution, and few-shot
experiments, respectively. Training code and simulation benchmark are
available: https://github.com/1mather/omnid.git

</details>


### [443] [Control of Legged Robots using Model Predictive Optimized Path Integral](https://arxiv.org/abs/2508.11917)
*Hossein Keshavarz,Alejandro Ramirez-Serrano,Majid Khadiv*

Main category: cs.RO

TL;DR: This paper proposes combining MPPI with CE and CMA methods, resulting in a new control strategy (MPOPI) for legged robots. It improves efficiency and locomotion capabilities.


<details>
  <summary>Details</summary>
Motivation: Legged robots excel in navigating complex terrains, but they still lag behind natural systems. The paper aims to enhance their performance through advanced predictive control strategies.

Method: A sampling-based model predictive strategy combining MPPI, CE, and CMA methods is introduced to generate real-time motions for legged robots.

Result: The proposed MPOPI approach demonstrates higher sample efficiency, enabling superior locomotion using fewer samples compared to traditional MPPI methods.

Conclusion: MPOPI enhances legged robots' locomotion through better sample efficiency and can function as an effective anytime control strategy in real-world scenarios.

Abstract: Legged robots possess a unique ability to traverse rough terrains and
navigate cluttered environments, making them well-suited for complex,
real-world unstructured scenarios. However, such robots have not yet achieved
the same level as seen in natural systems. Recently, sampling-based predictive
controllers have demonstrated particularly promising results. This paper
investigates a sampling-based model predictive strategy combining model
predictive path integral (MPPI) with cross-entropy (CE) and covariance matrix
adaptation (CMA) methods to generate real-time whole-body motions for legged
robots across multiple scenarios. The results show that combining the benefits
of MPPI, CE and CMA, namely using model predictive optimized path integral
(MPOPI), demonstrates greater sample efficiency, enabling robots to attain
superior locomotion results using fewer samples when compared to typical MPPI
algorithms. Extensive simulation experiments in multiple scenarios on a
quadruped robot show that MPOPI can be used as an anytime control strategy,
increasing locomotion capabilities at each iteration.

</details>


### [444] [ExploreVLM: Closed-Loop Robot Exploration Task Planning with Vision-Language Models](https://arxiv.org/abs/2508.11918)
*Zhichen Lou,Kechun Xu,Zhongxiang Zhou,Rong Xiong*

Main category: cs.RO

TL;DR: ExploreVLM proposes a closed-loop task planning framework powered by Vision-Language Models (VLMs), improving interactive exploration and real-time adaptation for robots in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Models (VLMs) struggle to handle interactive exploration, accurate perception, and real-time plan adjustments in dynamic environments.

Method: ExploreVLM introduces a step-wise feedback mechanism within a dual-stage, self-reflective task planner, combined with an object-centric spatial relation graph to structure scene representation. An execution validator ensures successful task execution.

Result: ExploreVLM achieves significant improvements over state-of-the-art baselines in exploration-centric tasks, as validated by extensive real-world experiments.

Conclusion: The integration of reflective planning and structured language-grounded perception enables robust and efficient task execution for robots, accelerating embodied intelligence applications.

Abstract: The advancement of embodied intelligence is accelerating the integration of
robots into daily life as human assistants. This evolution requires robots to
not only interpret high-level instructions and plan tasks but also perceive and
adapt within dynamic environments. Vision-Language Models (VLMs) present a
promising solution by combining visual understanding and language reasoning.
However, existing VLM-based methods struggle with interactive exploration,
accurate perception, and real-time plan adaptation. To address these
challenges, we propose ExploreVLM, a novel closed-loop task planning framework
powered by Vision-Language Models (VLMs). The framework is built around a
step-wise feedback mechanism that enables real-time plan adjustment and
supports interactive exploration. At its core is a dual-stage task planner with
self-reflection, enhanced by an object-centric spatial relation graph that
provides structured, language-grounded scene representations to guide
perception and planning. An execution validator supports the closed loop by
verifying each action and triggering re-planning. Extensive real-world
experiments demonstrate that ExploreVLM significantly outperforms
state-of-the-art baselines, particularly in exploration-centric tasks. Ablation
studies further validate the critical role of the reflective planner and
structured perception in achieving robust and efficient task execution.

</details>


### [445] [No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain](https://arxiv.org/abs/2508.11929)
*Mohitvishnu S. Gadde,Pranay Dugar,Ashish Malik,Alan Fern*

Main category: cs.RO

TL;DR: The paper develops a novel framework for vision-based omnidirectional bipedal locomotion, overcoming computational challenges through a combination of blind control and supervised training, enabling robust adaptability to diverse terrains.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of achieving effective omnidirectional bipedal locomotion in dynamic environments, which require agile movement and robust terrain sensing.

Method: The proposed framework trains a vision-based locomotion controller using depth images and combines a blind controller with a teacher policy. Noise-augmented data and novel data augmentation techniques are used to reduce rendering costs while ensuring robustness.

Result: The framework showed successful training acceleration, up to 10 times faster, and demonstrated effective omnidirectional locomotion in both simulation and real-world environments.

Conclusion: Vision-based omnidirectional bipedal locomotion is achieved with minimal reliance on rendering costs and enables adaptability to diverse terrains, marking a first-of-its-kind advancement.

Abstract: Effective bipedal locomotion in dynamic environments, such as cluttered
indoor spaces or uneven terrain, requires agile and adaptive movement in all
directions. This necessitates omnidirectional terrain sensing and a controller
capable of processing such input. We present a learning framework for
vision-based omnidirectional bipedal locomotion, enabling seamless movement
using depth images. A key challenge is the high computational cost of rendering
omnidirectional depth images in simulation, making traditional sim-to-real
reinforcement learning (RL) impractical. Our method combines a robust blind
controller with a teacher policy that supervises a vision-based student policy,
trained on noise-augmented terrain data to avoid rendering costs during RL and
ensure robustness. We also introduce a data augmentation technique for
supervised student training, accelerating training by up to 10 times compared
to conventional methods. Our framework is validated through simulation and
real-world tests, demonstrating effective omnidirectional locomotion with
minimal reliance on expensive rendering. This is, to the best of our knowledge,
the first demonstration of vision-based omnidirectional bipedal locomotion,
showcasing its adaptability to diverse terrains.

</details>


### [446] [Toward General Physical Intelligence for Resilient Agile Manufacturing Automation](https://arxiv.org/abs/2508.11960)
*Sandeep Kanta,Mehrdad Tavassoli,Varun Teja Chirkuri,Venkata Akhil Kumar,Santhi Bharath Punati,Praveen Damacharla,Sunny Katyara*

Main category: cs.RO

TL;DR: This paper reviews advances in Vision Language Action (VLA) models, analyzing their role in achieving General Physical Intelligence (GPI) and readiness for industrial use.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the practical applications and potential integration of General Physical Intelligence (GPI) in agile manufacturing processes as part of the Industry 5.0 revolution.

Method: The authors conduct a systematic survey, comparative analysis, and structured ablation study of existing Vision Language Action (VLA) implementations within the GPI framework.

Result: Key developments are organized into five pillars: multisensory representation learning, sim-to-real transfer, planning and control, uncertainty and safety measures, and benchmarking. Open challenges for industrial deployment are also highlighted.

Conclusion: GPI has significant untapped potential in Industry 5.0 manufacturing. Addressing identified research challenges will streamline its integration into next-generation industrial systems.

Abstract: Agile and human-centric manufacturing stipulates resilient robotic solutions
capable of contextual reasoning and safe interaction in unstructured
environments. Foundation models particularly the Vision Language Action (VLA)
models have emerged to fuse multimodal perception, reasoning and physically
grounded action across varied embodiments into unified representation, termed
as General Physical Intelligence (GPI). While GPI has already been described in
the literature but its practical application and evolving role in contemporary
agile manufacturing processes have yet to be duly explored. To bridge this gap,
this practical review systematically surveys recent advancements in VLA models
within GPI context, performs comprehensive comparative analysis of leading
implementations and evaluates their readiness for industrial deployment through
structured ablation study. Our analysis has organized state-of-the-art into
five thematic pillars including multisensory representation learning, sim2real
transfer, planning and control, uncertainty and safety measures and
benchmarking. Finally, we articulate open research challenges and propose
directions to better integrate GPI into next-generation industrial ecosystems
in line with Industry 5.0.

</details>


### [447] [Fully Spiking Actor-Critic Neural Network for Robotic Manipulation](https://arxiv.org/abs/2508.12038)
*Liwen Zhang,Heng Deng,Guanghui Sun*

Main category: cs.RO

TL;DR: This paper introduces a hybrid curriculum reinforcement learning framework using spiking neural networks (SNNs) for robotic arm tasks, focusing on energy efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To address the need for energy-efficient and high-speed inference methods in resource-constrained environments while maintaining strong performance in robotic manipulation tasks.

Method: The framework uses simplified SNNs with only input and output layers, a curriculum strategy integrated with the Proximal Policy Optimization (PPO) algorithm, and an energy consumption model to compare SNNs with ANNs.

Result: The proposed method outperforms traditional PPO and ANN baselines in robotic manipulation tasks, showcasing better scalability and energy efficiency, as demonstrated on the Isaac Gym simulation platform.

Conclusion: The paper validates the effectiveness of the hybrid SNN-based CRL framework for energy-efficient and accurate robotic control, suitable for resource-constrained and dynamic task environments.

Abstract: This study proposes a hybrid curriculum reinforcement learning (CRL)
framework based on a fully spiking neural network (SNN) for 9-degree-of-freedom
robotic arms performing target reaching and grasping tasks. To reduce network
complexity and inference latency, the SNN architecture is simplified to include
only an input and an output layer, which shows strong potential for
resource-constrained environments. Building on the advantages of SNNs-high
inference speed, low energy consumption, and spike-based biological
plausibility, a temporal progress-partitioned curriculum strategy is integrated
with the Proximal Policy Optimization (PPO) algorithm. Meanwhile, an energy
consumption modeling framework is introduced to quantitatively compare the
theoretical energy consumption between SNNs and conventional Artificial Neural
Networks (ANNs). A dynamic two-stage reward adjustment mechanism and optimized
observation space further improve learning efficiency and policy accuracy.
Experiments on the Isaac Gym simulation platform demonstrate that the proposed
method achieves superior performance under realistic physical constraints.
Comparative evaluations with conventional PPO and ANN baselines validate the
scalability and energy efficiency of the proposed approach in dynamic robotic
manipulation tasks.

</details>


### [448] [Talk Less, Fly Lighter: Autonomous Semantic Compression for UAV Swarm Communication via LLMs](https://arxiv.org/abs/2508.12043)
*Fei Lin,Tengchao Zhang,Qinghua Ni,Jun Huang,Siji Ma,Yonglin Tian,Yisheng Lv,Naiqi Wu*

Main category: cs.RO

TL;DR: This paper investigates the use of Large Language Models (LLMs) for efficient semantic communication in UAV swarms under bandwidth constraints.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges of limited communication bandwidth and high-frequency interactions in UAV swarms, aiming to enhance semantic compression for autonomous operations.

Method: The authors designed communication-execution pipelines and evaluated semantic compression using nine mainstream LLMs across scenarios with varying complexity and swarm sizes. Ablation studies were conducted to assess adaptability and stability.

Result: Experimental findings indicated that LLM-driven UAV swarms can adapt to varying conditions and achieve efficient communication despite bandwidth and multi-hop link constraints.

Conclusion: LLM-based UAV swarms hold promise for effective collaborative communication in bandwidth-limited scenarios, paving the way for improved autonomy in unmanned systems.

Abstract: The rapid adoption of Large Language Models (LLMs) in unmanned systems has
significantly enhanced the semantic understanding and autonomous task execution
capabilities of Unmanned Aerial Vehicle (UAV) swarms. However, limited
communication bandwidth and the need for high-frequency interactions pose
severe challenges to semantic information transmission within the swarm. This
paper explores the feasibility of LLM-driven UAV swarms for autonomous semantic
compression communication, aiming to reduce communication load while preserving
critical task semantics. To this end, we construct four types of 2D simulation
scenarios with different levels of environmental complexity and design a
communication-execution pipeline that integrates system prompts with task
instruction prompts. On this basis, we systematically evaluate the semantic
compression performance of nine mainstream LLMs in different scenarios and
analyze their adaptability and stability through ablation studies on
environmental complexity and swarm size. Experimental results demonstrate that
LLM-based UAV swarms have the potential to achieve efficient collaborative
communication under bandwidth-constrained and multi-hop link conditions.

</details>


### [449] [OASIS: Real-Time Opti-Acoustic Sensing for Intervention Systems in Unstructured Environments](https://arxiv.org/abs/2508.12071)
*Amy Phung,Richard Camilli*

Main category: cs.RO

TL;DR: The paper develops OASIS, a method for real-time underwater 3D scene reconstruction using opti-acoustic data fusion with robotic manipulator arms.


<details>
  <summary>Details</summary>
Motivation: Providing real-time spatial awareness for autonomous or piloted underwater operations using high-resolution 3D scene reconstruction.

Method: Uses optical images and voxel carving techniques in an eye-in-hand robotic manipulator configuration to enable real-time reconstruction.

Result: Validated OASIS through tank-based experiments, demonstrating its effectiveness for underwater manipulation tasks.

Conclusion: OASIS enables real-time underwater 3D reconstruction, enhancing utility for manipulation tasks in unstructured environments.

Abstract: High resolution underwater 3D scene reconstruction is crucial for various
applications, including construction, infrastructure maintenance, monitoring,
exploration, and scientific investigation. Prior work has leveraged the
complementary sensing modalities of imaging sonars and optical cameras for
opti-acoustic 3D scene reconstruction, demonstrating improved results over
methods which rely solely on either sensor. However, while most existing
approaches focus on offline reconstruction, real-time spatial awareness is
essential for both autonomous and piloted underwater vehicle operations. This
paper presents OASIS, an opti-acoustic fusion method that integrates data from
optical images with voxel carving techniques to achieve real-time 3D
reconstruction unstructured underwater workspaces. Our approach utilizes an
"eye-in-hand" configuration, which leverages the dexterity of robotic
manipulator arms to capture multiple workspace views across a short baseline.
We validate OASIS through tank-based experiments and present qualitative and
quantitative results that highlight its utility for underwater manipulation
tasks.

</details>


### [450] [Into the Wild: When Robots Are Not Welcome](https://arxiv.org/abs/2508.12075)
*Shaul Ashkenazi,Gabriel Skantze,Jane Stuart-Smith,Mary Ellen Foster*

Main category: cs.RO

TL;DR: The paper discusses the challenges faced in deploying social robots in public spaces and the eventual trust-building with stakeholders to enable deployment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the deployment of social robots in sensitive public spaces, such as student services and refugee centers, and understand challenges from both technological and stakeholder perspectives.

Method: The authors describe their experiences and difficulties encountered during the deployment, focusing on trust-building processes and stakeholder management.

Result: Despite initial resistance, trust was earned among staff in both settings, allowing for the successful deployment and study of the robots.

Conclusion: Deploying social robots in public spaces requires addressing stakeholder concerns and building trust, which is critical for acceptance and successful deployment.

Abstract: Social robots are increasingly being deployed in public spaces, where they
face not only technological difficulties and unexpected user utterances, but
also objections from stakeholders who may not be comfortable with introducing a
robot into those spaces. We describe our difficulties with deploying a social
robot in two different public settings: 1) Student services center; 2) Refugees
and asylum seekers drop-in service. Although this is a failure report, in each
use case we eventually managed to earn the trust of the staff and form a
relationship with them, allowing us to deploy our robot and conduct our
studies.

</details>


### [451] [Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing](https://arxiv.org/abs/2508.12166)
*Gokul Puthumanaillam,Aditya Penumarti,Manav Vora,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla,Jane Shin,Melkior Ornik*

Main category: cs.RO

TL;DR: The paper introduces an energy-efficient planning method for sensor usage in robots, utilizing a diffusion model to determine minimum sensors required for accurate localization.


<details>
  <summary>Details</summary>
Motivation: The need for energy-efficient and reliable localization in robots with rich sensor suites, especially in partially observable environments.

Method: Proposed Belief-Conditioned One-Step Diffusion (B-COD), which provides short-horizon trajectories, aleatoric variances, and a proxy for localization error for online sensor selection.

Result: B-COD reduces sensing energy consumption while maintaining the task performance of always-on sensor usage in real-time marine trials.

Conclusion: B-COD is an effective, efficient alternative to traditional localization methods, optimizing energy use without compromising task accuracy.

Abstract: Robots equipped with rich sensor suites can localize reliably in
partially-observable environments, but powering every sensor continuously is
wasteful and often infeasible. Belief-space planners address this by
propagating pose-belief covariance through analytic models and switching
sensors heuristically--a brittle, runtime-expensive approach. Data-driven
approaches--including diffusion models--learn multi-modal trajectories from
demonstrations, but presuppose an accurate, always-on state estimate. We
address the largely open problem: for a given task in a mapped environment,
which \textit{minimal sensor subset} must be active at each location to
maintain state uncertainty \textit{just low enough} to complete the task? Our
key insight is that when a diffusion planner is explicitly conditioned on a
pose-belief raster and a sensor mask, the spread of its denoising trajectories
yields a calibrated, differentiable proxy for the expected localisation error.
Building on this insight, we present Belief-Conditioned One-Step Diffusion
(B-COD), the first planner that, in a 10 ms forward pass, returns a
short-horizon trajectory, per-waypoint aleatoric variances, and a proxy for
localisation error--eliminating external covariance rollouts. We show that this
single proxy suffices for a soft-actor-critic to choose sensors online,
optimising energy while bounding pose-covariance growth. We deploy B-COD in
real-time marine trials on an unmanned surface vehicle and show that it reduces
sensing energy consumption while matching the goal-reach performance of an
always-on baseline.

</details>


### [452] [Energy Efficiency in Robotics Software: A Systematic Literature Review (2020-2024)](https://arxiv.org/abs/2508.12170)
*Aryan Gupta*

Main category: cs.RO

TL;DR: This paper systematically reviews software-level energy efficiency techniques in robotics for the years 2020–2024, analyzing 79 studies for domains, metrics, evaluations, and techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to update and extend pre-2020 research on energy efficiency in robotics through a systematic review of recent studies, addressing gaps in reporting and exploring unexplored areas.

Method: An automated-but-audited pipeline with ~10% human audits was used to screen and analyze studies, focusing on domains, metrics, evaluation types, energy models, consumer identification, and techniques.

Result: The review found that industrial settings dominate as the application domain, motors are the primary energy consumers, and motion optimization paired with learning is the most prevalent technique. Reporting practices varied widely across papers.

Conclusion: The paper proposes a reporting checklist to improve study comparability and highlights room for innovation in cross-layer designs and trade-off quantifications for energy-efficient robotics.

Abstract: This study presents a systematic literature review of software-level
approaches to energy efficiency in robotics published from 2020 through 2024,
updating and extending pre-2020 evidence. An automated-but-audited pipeline
combined Google Scholar seeding, backward/forward snowballing, and
large-language-model (LLM) assistance for screening and data extraction, with
~10% human audits at each automated step and consensus-with-tie-breaks for
full-text decisions. The final corpus comprises 79 peer-reviewed studies
analyzed across application domain, metrics, evaluation type, energy models,
major energy consumers, software technique families, and energy-quality
trade-offs. Industrial settings dominate (31.6%) followed by exploration
(25.3%). Motors/actuators are identified as the primary consumer in 68.4% of
studies, with computing/controllers a distant second (13.9%). Simulation-only
evaluations remain most common (51.9%), though hybrid evaluations are frequent
(25.3%). Representational (physics-grounded) energy models predominate (87.3%).
Motion and trajectory optimization is the leading technique family (69.6%),
often paired with learning/prediction (40.5%) and computation
allocation/scheduling (26.6%); power management/idle control (11.4%) and
communication/data efficiency (3.8%) are comparatively underexplored. Reporting
is heterogeneous: composite objectives that include energy are most common,
while task-normalized and performance-per-energy metrics appear less often,
limiting cross-paper comparability. The review offers a minimal reporting
checklist (e.g., total energy and average power plus a task-normalized metric
and clear baselines) and highlights opportunities in cross-layer designs and in
quantifying non-performance trade-offs (accuracy, stability). A replication
package with code, prompts, and frozen datasets accompanies the review.

</details>


### [453] [Humanoid Motion Scripting with Postural Synergies](https://arxiv.org/abs/2508.12184)
*Rhea Malhotra,William Chong,Catie Cuan,Oussama Khatib*

Main category: cs.RO

TL;DR: The paper introduces SynSculptor, a framework relying on postural synergies to facilitate human-like motion scripting for humanoid robots without needing training. The authors leverage motion capture data, apply principal component analysis (PCA), and evaluate new motions with smoothness metrics.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the need to effectively generate realistic, human-like motions for humanoid robots while addressing challenges in collecting, synthesizing, and mapping such motions onto robots.

Method: The framework extracts postural synergies from motion capture data using principal component analysis (PCA). A synergy library conditioned on human motion style enables motion generation. Smoothness metrics like foot-sliding ratio and kinetic energy deviations are used for evaluation. Additionally, a motion-language transformer facilitates adaptive posture adjustments.

Result: The experimental results include successful motion reproduction and evaluation using smoothness metrics and synergy-driven adjustments, demonstrating the framework’s efficacy in generating realistic humanoid robot motions.

Conclusion: SynSculptor provides a training-free method for scripting human-like motions on humanoid robots, leveraging postural synergies and smoothness metrics. This approach supports adaptive and realistic motion synthesis for humanoids.

Abstract: Generating sequences of human-like motions for humanoid robots presents
challenges in collecting and analyzing reference human motions, synthesizing
new motions based on these reference motions, and mapping the generated motion
onto humanoid robots. To address these issues, we introduce SynSculptor, a
humanoid motion analysis and editing framework that leverages postural
synergies for training-free human-like motion scripting. To analyze human
motion, we collect 3+ hours of motion capture data across 20 individuals where
a real-time operational space controller mimics human motion on a simulated
humanoid robot. The major postural synergies are extracted using principal
component analysis (PCA) for velocity trajectories segmented by changes in
robot momentum, constructing a style-conditioned synergy library for free-space
motion generation. To evaluate generated motions using the synergy library, the
foot-sliding ratio and proposed metrics for motion smoothness involving total
momentum and kinetic energy deviations are computed for each generated motion,
and compared with reference motions. Finally, we leverage the synergies with a
motion-language transformer, where the humanoid, during execution of motion
tasks with its end-effectors, adapts its posture based on the chosen synergy.
Supplementary material, code, and videos are available at
https://rhea-mal.github.io/humanoidsynergies.io.

</details>


### [454] [Self-Guided Action Diffusion](https://arxiv.org/abs/2508.12189)
*Rhea Malhotra,Yuejiang Liu,Chelsea Finn*

Main category: cs.RO

TL;DR: The paper introduces self-guided action diffusion, a method to enhance diffusion-based policies for robots, which improves efficiency and performance compared to bidirectional decoding at minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the computational limitations posed by bidirectional decoding when enhancing the coherence and reactivity of diffusion policies in robot actions.

Method: The method involves guiding the proposal distribution at each diffusion step based on prior decisions, which streamlines the action generation process.

Result: Experiments show that self-guidance achieves near-optimal performance while drastically reducing inference costs and outperforms existing methods with up to 70% higher success rates under tight sampling conditions.

Conclusion: Self-guided action diffusion offers a computationally efficient and highly effective alternative to existing methods, demonstrating significant performance gains in challenging robot tasks.

Abstract: Recent works have shown the promise of inference-time search over action
samples for improving generative robot policies. In particular, optimizing
cross-chunk coherence via bidirectional decoding has proven effective in
boosting the consistency and reactivity of diffusion policies. However, this
approach remains computationally expensive as the diversity of sampled actions
grows. In this paper, we introduce self-guided action diffusion, a more
efficient variant of bidirectional decoding tailored for diffusion-based
policies. At the core of our method is to guide the proposal distribution at
each diffusion step based on the prior decision. Experiments in simulation
tasks show that the proposed self-guidance enables near-optimal performance at
negligible inference cost. Notably, under a tight sampling budget, our method
achieves up to 70% higher success rates than existing counterparts on
challenging dynamic tasks. See project website at
https://rhea-mal.github.io/selfgad.github.io.

</details>


### [455] [Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search](https://arxiv.org/abs/2508.12211)
*Cyrus Neary,Omar G. Younis,Artur Kuramshin,Ozgur Aslan,Glen Berseth*

Main category: cs.RO

TL;DR: VLAPS enhances the performance of pre-trained VLA models in robotics tasks by embedding model-based search into their inference, improving success rates significantly.


<details>
  <summary>Details</summary>
Motivation: Pre-trained VLA models often encounter brittle or unsafe failures in out-of-distribution scenarios during zero-shot deployment.

Method: VLAPS uses a modified Monte Carlo Tree Search algorithm biased by action priors from pre-trained VLA policies to efficiently explore large search spaces in robotic tasks.

Result: VLAPS increases success rates by up to 67 percentage points compared to VLA-only baselines.

Conclusion: Integrating model-based search with pre-trained VLA policies contributes a scalable and robust approach to handling complex language-conditioned robotics tasks.

Abstract: Pre-trained vision-language-action (VLA) models offer a promising foundation
for generalist robot policies, but often produce brittle behaviours or unsafe
failures when deployed zero-shot in out-of-distribution scenarios. We present
Vision-Language-Action Planning & Search (VLAPS) -- a novel framework and
accompanying algorithms that embed model-based search into the inference
procedure of pre-trained VLA policies to improve their performance on robotic
tasks. Specifically, our method biases a modified Monte Carlo Tree Search
(MCTS) algorithm -- run using a model of the target environment -- using action
priors defined by the VLA policy. By using VLA-derived abstractions and priors
in model-based search, VLAPS efficiently explores language-conditioned robotics
tasks whose search spaces would otherwise be intractably large. Conversely, by
integrating model-based search with the VLA policy's inference procedure, VLAPS
yields behaviours that are more performant than those obtained by directly
following the VLA policy's action predictions. VLAPS offers a principled
framework to: i) control test-time compute in VLA models, ii) leverage a priori
knowledge of the robotic environment, and iii) integrate established planning
and reinforcement learning techniques into the VLA inference process. Across
all experiments, VLAPS significantly outperforms VLA-only baselines on
language-specified tasks that would otherwise be intractable for uninformed
search algorithms, increasing success rates by as much as 67 percentage points.

</details>


### [456] [Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids](https://arxiv.org/abs/2508.12252)
*Kaizhe Hu,Haochen Shi,Yao He,Weizhuo Wang,C. Karen Liu,Shuran Song*

Main category: cs.RO

TL;DR: This paper presents the RTR (Robot-Trains-Robot) framework, enabling real-world training of humanoid robots using assistance from a robotic arm teacher.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of real-world humanoid robot training, including safety, efficiency, and overcoming the sim-to-real gap.

Method: Introduction of the RTR framework, which incorporates robotic arm teacher assistance for protection, reward design, scheduling, and resets, with a novel RL pipeline that simplifies sim-to-real transfer.

Result: Validated through experiments on two challenging tasks: fine-tuning speed tracking in walking and humanoid swing-up, showing effective real-world training.

Conclusion: The RTR framework enables efficient, intuitive real-world reinforcement learning for humanoid robots, minimizing human intervention while overcoming key training obstacles.

Abstract: Simulation-based reinforcement learning (RL) has significantly advanced
humanoid locomotion tasks, yet direct real-world RL from scratch or adapting
from pretrained policies remains rare, limiting the full potential of humanoid
robots. Real-world learning, despite being crucial for overcoming the
sim-to-real gap, faces substantial challenges related to safety, reward design,
and learning efficiency. To address these limitations, we propose
Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher
actively supports and guides a humanoid robot student. The RTR system provides
protection, learning schedule, reward, perturbation, failure detection, and
automatic resets. It enables efficient long-term real-world humanoid training
with minimal human intervention. Furthermore, we propose a novel RL pipeline
that facilitates and stabilizes sim-to-real transfer by optimizing a single
dynamics-encoded latent variable in the real world. We validate our method
through two challenging real-world humanoid tasks: fine-tuning a walking policy
for precise speed tracking and learning a humanoid swing-up task from scratch,
illustrating the promising capabilities of real-world humanoid learning
realized by RTR-style systems. See https://robot-trains-robot.github.io/ for
more info.

</details>


### [457] [Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy for Tight-Fitting Garments](https://arxiv.org/abs/2508.12274)
*Jian Zhao,Yunlong Lian,Andy M Tyrrell,Michael Gienger,Jihong Zhu*

Main category: cs.RO

TL;DR: The paper proposes a bimanual dressing strategy using imitation learning to assist robots in dressing individuals with tight-fitting garments, an area that current research lacks focus on.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the lack of research on assisting robots in dressing individuals with tight-fitting clothing, which is crucial for improving the quality of life of people with mobility limitations.

Method: The approach involves establishing a spherical coordinate system for dressing, using it to encode trajectories. A task-relevant feature (azimuthal angle) is extracted and imitation learning with Gaussian Mixture Model (GMM) and Gaussian Mixture Regression (GMR) is applied to generate adaptable bimanual dressing strategies.

Result: The proposed method is demonstrated to be effective through a series of experiments, showing that it can adapt to different human arm postures while successfully dressing tight-fitting garments.

Conclusion: This study advances robot-assisted dressing by enabling robots to handle the challenging task of dressing individuals with tight-fitting clothing using a bimanual strategy paired with imitation learning techniques.

Abstract: Robot-assisted dressing is a popular but challenging topic in the field of
robotic manipulation, offering significant potential to improve the quality of
life for individuals with mobility limitations. Currently, the majority of
research on robot-assisted dressing focuses on how to put on loose-fitting
clothing, with little attention paid to tight garments. For the former, since
the armscye is larger, a single robotic arm can usually complete the dressing
task successfully. However, for the latter, dressing with a single robotic arm
often fails due to the narrower armscye and the property of diminishing
rigidity in the armscye, which eventually causes the armscye to get stuck. This
paper proposes a bimanual dressing strategy suitable for dressing tight-fitting
clothing. To facilitate the encoding of dressing trajectories that adapt to
different human arm postures, a spherical coordinate system for dressing is
established. We uses the azimuthal angle of the spherical coordinate system as
a task-relevant feature for bimanual manipulation. Based on this new
coordinate, we employ Gaussian Mixture Model (GMM) and Gaussian Mixture
Regression (GMR) for imitation learning of bimanual dressing trajectories,
generating dressing strategies that adapt to different human arm postures. The
effectiveness of the proposed method is validated through various experiments.

</details>


### [458] [A robust and compliant robotic assembly control strategy for batch precision assembly task with uncertain fit types and fit amounts](https://arxiv.org/abs/2508.12296)
*Bin Wang,Jiwen Zhang,Song Wang,Dan Wu*

Main category: cs.RO

TL;DR: The paper addresses robotic assembly tasks involving components with uncertain fit, proposing a robust control strategy using reinforcement learning and force-vision data fusion.


<details>
  <summary>Details</summary>
Motivation: To handle high-precision robotic assembly tasks where components have uncertain fit types and fit amounts, caused by machining errors.

Method: Develop a methodology that decomposes the tasks into subtasks, applies force-vision fusion reinforcement learning and a multi-task learning strategy, and integrates these through a multi-teacher policy distillation into a unified control strategy.

Result: Real-world experiments show the method effectively constructs robust control strategies, improves force compliance, and achieves higher success rates compared to existing methods.

Conclusion: The proposed approach not only successfully addresses uncertain fit issues in robotic assembly but also enhances training efficiency and assembly success rates.

Abstract: In some high-precision industrial applications, robots are deployed to
perform precision assembly tasks on mass batches of manufactured pegs and
holes. If the peg and hole are designed with transition fit, machining errors
may lead to either a clearance or an interference fit for a specific pair of
components, with uncertain fit amounts. This paper focuses on the robotic batch
precision assembly task involving components with uncertain fit types and fit
amounts, and proposes an efficient methodology to construct the robust and
compliant assembly control strategy. Specifically, the batch precision assembly
task is decomposed into multiple deterministic subtasks, and a force-vision
fusion controller-driven reinforcement learning method and a multi-task
reinforcement learning training method (FVFC-MTRL) are proposed to jointly
learn multiple compliance control strategies for these subtasks. Subsequently,
the multi-teacher policy distillation approach is designed to integrate
multiple trained strategies into a unified student network, thereby
establishing a robust control strategy. Real-world experiments demonstrate that
the proposed method successfully constructs the robust control strategy for
high-precision assembly task with different fit types and fit amounts.
Moreover, the MTRL framework significantly improves training efficiency, and
the final developed control strategy achieves superior force compliance and
higher success rate compared with many existing methods.

</details>


### [459] [Implementation and evaluation of a prediction algorithm for an autonomous vehicle](https://arxiv.org/abs/2508.12312)
*Marco Leon Rapp*

Main category: cs.RO

TL;DR: This paper proposes a trajectory prediction algorithm for autonomous vehicles employing a dynamic bicycle model and achieving high precision.


<details>
  <summary>Details</summary>
Motivation: To improve trajectory prediction accuracy for autonomous vehicles, especially at higher speeds.

Method: Comparison of kinematic and dynamic bicycle models with experimental determination of vehicle parameters; use of an extended Kalman filter; implementation in ROS (Robot Operating System) in C++.

Result: The dynamic model is significantly more accurate than the kinematic model, achieving a positional deviation of 1.25 cm per meter and improving precision by up to 82.6%.

Conclusion: The dynamic bicycle model and extended Kalman filter implementation provide superior trajectory prediction performance for autonomous vehicles.

Abstract: This paper presents a prediction algorithm that estimates the vehicle
trajectory every five milliseconds for an autonomous vehicle. A kinematic and a
dynamic bicycle model are compared, with the dynamic model exhibiting superior
accuracy at higher speeds. Vehicle parameters such as mass, center of gravity,
moment of inertia, and cornering stiffness are determined experimentally. For
cornering stiffness, a novel measurement procedure using optical position
tracking is introduced. The model is incorporated into an extended Kalman
filter and implemented in a ROS node in C++. The algorithm achieves a
positional deviation of only 1.25 cm per meter over the entire test drive and
is up to 82.6% more precise than the kinematic model.

</details>


### [460] [Semi-Infinite Programming for Collision-Avoidance in Optimal and Model Predictive Control](https://arxiv.org/abs/2508.12335)
*Yunfan Gao,Florian Messerer,Niels van Duijkeren,Rashmi Dabir,Moritz Diehl*

Main category: cs.RO

TL;DR: This paper introduces a method for collision avoidance using semi-infinite programming in optimal control, efficiently solving constraints related to robot-environment interactions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimal collision avoidance for robots in dynamic or uncertain environments.

Method: Combines local reduction and an external active-set method to solve semi-infinite programming problems, incorporating robust techniques for state uncertainties.

Result: Demonstrated on a real-world robot running at 20Hz for fast collision-free movement; also applied to 3D in simulations.

Conclusion: The method ensures efficient and robust collision avoidance, suitable for real-time applications in complex environments.

Abstract: This paper presents a novel approach for collision avoidance in optimal and
model predictive control, in which the environment is represented by a large
number of points and the robot as a union of padded polygons. The conditions
that none of the points shall collide with the robot can be written in terms of
an infinite number of constraints per obstacle point. We show that the
resulting semi-infinite programming (SIP) optimal control problem (OCP) can be
efficiently tackled through a combination of two methods: local reduction and
an external active-set method. Specifically, this involves iteratively
identifying the closest point obstacles, determining the lower-level distance
minimizer among all feasible robot shape parameters, and solving the
upper-level finitely-constrained subproblems.
  In addition, this paper addresses robust collision avoidance in the presence
of ellipsoidal state uncertainties. Enforcing constraint satisfaction over all
possible uncertainty realizations extends the dimension of constraint
infiniteness. The infinitely many constraints arising from translational
uncertainty are handled by local reduction together with the robot shape
parameterization, while rotational uncertainty is addressed via a backoff
reformulation.
  A controller implemented based on the proposed method is demonstrated on a
real-world robot running at 20Hz, enabling fast and collision-free navigation
in tight spaces. An application to 3D collision avoidance is also demonstrated
in simulation.

</details>


### [461] [SIGN: Safety-Aware Image-Goal Navigation for Autonomous Drones via Reinforcement Learning](https://arxiv.org/abs/2508.12394)
*Zichen Yan,Rui Huang,Lei He,Shao Guo,Lin Zhao*

Main category: cs.RO

TL;DR: The paper introduces a sim-to-real framework for enabling image-goal navigation for autonomous drones using visual reinforcement learning and depth-based safety modules.


<details>
  <summary>Details</summary>
Motivation: Enable autonomous drones to perform image-goal navigation in unknown environments, enhancing navigation capabilities beyond reference tracking or obstacle avoidance.

Method: The approach combines visual reinforcement learning with auxiliary tasks such as image perturbations and future transition prediction to improve policy training and integrates a depth-based safety module for real-time obstacle avoidance.

Result: The proposed framework enables drones to achieve end-to-end image-goal navigation and real-time safety in cluttered environments without external localization or global mapping.

Conclusion: This novel framework opens up new possibilities for autonomous drone navigation by combining exploration, obstacle avoidance, and goal-seeking behaviors effectively.

Abstract: Image-goal navigation (ImageNav) tasks a robot with autonomously exploring an
unknown environment and reaching a location that visually matches a given
target image. While prior works primarily study ImageNav for ground robots,
enabling this capability for autonomous drones is substantially more
challenging due to their need for high-frequency feedback control and global
localization for stable flight. In this paper, we propose a novel sim-to-real
framework that leverages visual reinforcement learning (RL) to achieve ImageNav
for drones. To enhance visual representation ability, our approach trains the
vision backbone with auxiliary tasks, including image perturbations and future
transition prediction, which results in more effective policy training. The
proposed algorithm enables end-to-end ImageNav with direct velocity control,
eliminating the need for external localization. Furthermore, we integrate a
depth-based safety module for real-time obstacle avoidance, allowing the drone
to safely navigate in cluttered environments. Unlike most existing drone
navigation methods that focus solely on reference tracking or obstacle
avoidance, our framework supports comprehensive navigation
behaviors--autonomous exploration, obstacle avoidance, and image-goal
seeking--without requiring explicit global mapping. Code and model checkpoints
will be released upon acceptance.

</details>


### [462] [PUB: A Plasma-Propelled Ultra-Quiet Blimp with Two-DOF Vector Thrusting](https://arxiv.org/abs/2508.12395)
*Zihan Wang*

Main category: cs.RO

TL;DR: The paper introduces a Plasma-propelled Ultra-silence Blimp (PUB), an aerial robot that uses plasma propulsion for quiet flight without mechanical propellers. It is helium-lifted and employs thrust vector control and slip control for stable maneuvering.


<details>
  <summary>Details</summary>
Motivation: To develop an aerial robot suitable for noise-sensitive, confined, and near-space applications, addressing limitations of mechanical propeller-based systems.

Method: Designing a helium-lift blimp equipped with plasma vector propulsion using asymmetric capacitors for ionic wind thrust, along with modular propulsion and a two-DOF thrust vector control head. Implementing a closed-loop slip control for maneuvering.

Result: Flight experiments validate the blimp's capability across various flight conditions (take-off, climb, hover, descent, landing) and demonstrate the effectiveness of plasma propulsion and control systems.

Conclusion: PUB exhibits properties such as ultra-quiet operation, simple design, and good maneuverability, making it suitable for specialized applications in sensitive environments.

Abstract: This study presents the design and control of a Plasma-propelled
Ultra-silence Blimp (PUB), a novel aerial robot employing plasma vector
propulsion for ultra-quiet flight without mechanical propellers. The system
utilizes a helium-lift platform for extended endurance and a four-layer ring
asymmetric capacitor to generate ionic wind thrust. The modular propulsion
units allow flexible configuration to meet mission-specific requirements, while
a two-degree-of-freedom (DOF) head enables thrust vector control. A closed-loop
slip control scheme is implemented for stable maneuvering. Flight experiments
demonstrate full-envelope capability, including take-off, climb, hover,
descent, and smooth landing, confirming the feasibility of plasma vector
propulsion, the effectiveness of DOF vector control, and the stability of the
control system. Owing to its low acoustic signature, structural simplicity, and
high maneuverability, PUB is well suited for noise-sensitive, enclosed, and
near-space applications.

</details>


### [463] [Tactile Gesture Recognition with Built-in Joint Sensors for Industrial Robots](https://arxiv.org/abs/2508.12435)
*Deqing Song,Weimin Yang,Maryam Rezayati,Hans Wernher van de Venn*

Main category: cs.RO

TL;DR: The paper explores gesture recognition using robot joint sensors without external tools, finding spectrogram-based data improves accuracy and demonstrating its feasibility in HRC.


<details>
  <summary>Details</summary>
Motivation: To eliminate the dependency on external sensors for gesture recognition in human-robot collaboration, making it cost-effective and scalable.

Method: Deep learning models, especially CNN architectures, were used along with spectrogram-based data representations for tactile recognition. Two datasets were collected to evaluate model accuracy and generalization.

Result: Spectrogram-based methods significantly enhanced recognition accuracy, surpassing 95% in contact detection and gesture classification using Franka Emika robot.

Conclusion: The work proves external sensor-free gesture recognition is achievable and encourages research into scalable HRC systems.

Abstract: While gesture recognition using vision or robot skins is an active research
area in Human-Robot Collaboration (HRC), this paper explores deep learning
methods relying solely on a robot's built-in joint sensors, eliminating the
need for external sensors. We evaluated various convolutional neural network
(CNN) architectures and collected two datasets to study the impact of data
representation and model architecture on the recognition accuracy. Our results
show that spectrogram-based representations significantly improve accuracy,
while model architecture plays a smaller role. We also tested generalization to
new robot poses, where spectrogram-based models performed better. Implemented
on a Franka Emika Research robot, two of our methods, STFT2DCNN and STT3DCNN,
achieved over 95% accuracy in contact detection and gesture classification.
These findings demonstrate the feasibility of external-sensor-free tactile
recognition and promote further research toward cost-effective, scalable
solutions for HRC.

</details>


### [464] [Geodesic Tracing-Based Kinematic Integration of Rolling and Sliding Contact on Manifold Meshes for Dexterous In-Hand Manipulation](https://arxiv.org/abs/2508.12439)
*Sunyu Wang,Arjun S. Lakshmipathy,Jean Oh,Nancy S. Pollard*

Main category: cs.RO

TL;DR: This paper extends roll-slide contact modeling to manifold meshes for dexterous robotic manipulation, achieving high accuracy and precision even with coarse meshes.


<details>
  <summary>Details</summary>
Motivation: Existing roll-slide contact models focus on continuous, differentiable shapes, limiting fidelity in representing real object geometries.

Method: It uses geodesic tracing for first-order time integration of roll-slide contact directly on meshes and employs least-squares optimization to stabilize grasp during robotic manipulation.

Result: Dexterous motion planning on five objects using a robotic hand showed superior performance with enhanced accuracy and precision compared to baselines.

Conclusion: Mesh-based modeling can significantly improve dexterous manipulation accuracy, emphasizing future work to incorporate multiple contacts and contact forces.

Abstract: Reasoning about rolling and sliding contact, or roll-slide contact for short,
is critical for dexterous manipulation tasks that involve intricate geometries.
But existing works on roll-slide contact mostly focus on continuous shapes with
differentiable parametrizations. This work extends roll-slide contact modeling
to manifold meshes. Specifically, we present an integration scheme based on
geodesic tracing to first-order time-integrate roll-slide contact directly on
meshes, enabling dexterous manipulation to reason over high-fidelity discrete
representations of an object's true geometry. Using our method, we planned
dexterous motions of a multi-finger robotic hand manipulating five objects
in-hand in simulation. The planning was achieved with a least-squares optimizer
that strives to maintain the most stable instantaneous grasp by minimizing
contact sliding and spinning. Then, we evaluated our method against a baseline
using collision detection and a baseline using primitive shapes. The results
show that our method performed the best in accuracy and precision, even for
coarse meshes. We conclude with a future work discussion on incorporating
multiple contacts and contact forces to achieve accurate and robust mesh-based
surface contact modeling.

</details>


### [465] [Autonomous Oil Spill Response Through Liquid Neural Trajectory Modeling and Coordinated Marine Robotics](https://arxiv.org/abs/2508.12456)
*Hadas C. Kuzmenko,David Ehevich,Oren Gal*

Main category: cs.RO

TL;DR: This paper proposes an integrated framework for real-time, accurate oil spill prediction and management using swarm robotics and advanced neural networks.


<details>
  <summary>Details</summary>
Motivation: Oil spills create severe ecological and economic disruptions. Efficiently predicting and managing their behavior is crucial to minimize impacts, but is complicated by numerous dynamic factors.

Method: The framework combines multi-agent swarm robotics (MOOS-IvP platform) and Liquid Time-Constant Neural Networks (LTCNs) for real-time oil spill trajectory forecasting, dynamic tracking, and response.

Result: Using Deepwater Horizon spill data, the proposed LTC-RK4 model achieved 0.96 spatial accuracy, outperforming traditional LSTM-based approaches by 23%.

Conclusion: The integration of LTCNs and swarm robotics delivers significant advances in real-time oil spill management, outperforming current methods in precision, adaptability, and scalability.

Abstract: Marine oil spills pose grave environmental and economic risks, threatening
marine ecosystems, coastlines, and dependent industries. Predicting and
managing oil spill trajectories is highly complex, due to the interplay of
physical, chemical, and environmental factors such as wind, currents, and
temperature, which makes timely and effective response challenging. Accurate
real-time trajectory forecasting and coordinated mitigation are vital for
minimizing the impact of these disasters. This study introduces an integrated
framework combining a multi-agent swarm robotics system built on the MOOS-IvP
platform with Liquid Time-Constant Neural Networks (LTCNs). The proposed system
fuses adaptive machine learning with autonomous marine robotics, enabling
real-time prediction, dynamic tracking, and rapid response to evolving oil
spills. By leveraging LTCNs--well-suited for modeling complex, time-dependent
processes--the framework achieves real-time, high-accuracy forecasts of spill
movement. Swarm intelligence enables decentralized, scalable, and resilient
decision-making among robot agents, enhancing collective monitoring and
containment efforts. Our approach was validated using data from the Deepwater
Horizon spill, where the LTC-RK4 model achieved 0.96 spatial accuracy,
surpassing LSTM approaches by 23%. The integration of advanced neural modeling
with autonomous, coordinated robotics demonstrates substantial improvements in
prediction precision, flexibility, and operational scalability. Ultimately,
this research advances the state-of-the-art for sustainable, autonomous oil
spill management and environmental protection by enhancing both trajectory
prediction and response coordination.

</details>


### [466] [Mechanical Automation with Vision: A Design for Rubik's Cube Solver](https://arxiv.org/abs/2508.12469)
*Abhinav Chalise,Nimesh Gopal Pradhan,Nishan Khanal,Prashant Raj Bista,Dinesh Baniya Kshatri*

Main category: cs.RO

TL;DR: The paper presents a Rubik's cube-solving system combining hardware and software, leveraging YOLO detection, Kociemba's algorithm, and a GUI for visualization.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient mechanical system capable of solving Rubik's cubes with accurate state detection and user-friendly GUI integration.

Method: Utilizes three stepper motors, microcontroller for control, a camera with YOLOv8 for cube state detection, and Kociemba's algorithm for solution computation.

Result: Achieved high detection precision (0.98443) and recall (0.98419), with an average solving time of ~2.2 minutes.

Conclusion: Integration of hardware and software enables accurate real-time cube-solving with effective visualization and manipulation capabilities.

Abstract: The core mechanical system is built around three stepper motors for physical
manipulation, a microcontroller for hardware control, a camera and YOLO
detection model for real-time cube state detection. A significant software
component is the development of a user-friendly graphical user interface (GUI)
designed in Unity. The initial state after detection from real-time YOLOv8
model (Precision 0.98443, Recall 0.98419, Box Loss 0.42051, Class Loss 0.2611)
is virtualized on GUI. To get the solution, the system employs the Kociemba's
algorithm while physical manipulation with a single degree of freedom is done
by combination of stepper motors' interaction with the cube achieving the
average solving time of ~2.2 minutes.

</details>


### [467] [PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions](https://arxiv.org/abs/2508.12554)
*Hamza El-Kebir*

Main category: cs.RO

TL;DR: The paper proposes PROD, a method to reconstruct shapes and mechanical properties of deformable objects using signed distance functions (SDFs) and palpative interaction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the ability to analyze deformable objects by integrating physical interaction data, such as forces, beyond visual or purely geometrical data.

Method: PROD utilizes force-controlled surface probing and models deformation as an elastostatic process, deriving a Poisson equation for SDF estimation from sparse pose and force data. It integrates assumptions about steady-state elastodynamics to recover undeformed SDFs and estimate material stiffness.

Result: The method proved robust in simulated environments, handling pose errors, non-normal forces, and curvature inaccuracies effectively.

Conclusion: PROD demonstrates potential as a powerful tool for reconstructing deformable objects, applicable to areas like robotics, medical imaging, and haptic systems.

Abstract: We introduce PROD (Palpative Reconstruction of Deformables), a novel method
for reconstructing the shape and mechanical properties of deformable objects
using elastostatic signed distance functions (SDFs). Unlike traditional
approaches that rely on purely geometric or visual data, PROD integrates
palpative interaction -- measured through force-controlled surface probing --
to estimate both the static and dynamic response of soft materials. We model
the deformation of an object as an elastostatic process and derive a governing
Poisson equation for estimating its SDF from a sparse set of pose and force
measurements. By incorporating steady-state elastodynamic assumptions, we show
that the undeformed SDF can be recovered from deformed observations with
provable convergence. Our approach also enables the estimation of material
stiffness by analyzing displacement responses to varying force inputs. We
demonstrate the robustness of PROD in handling pose errors, non-normal force
application, and curvature errors in simulated soft body interactions. These
capabilities make PROD a powerful tool for reconstructing deformable objects in
applications ranging from robotic manipulation to medical imaging and haptic
feedback systems.

</details>


### [468] [Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems](https://arxiv.org/abs/2508.12564)
*Jiayao Mai,Xiuyuan Lu,Kuan Dai,Shaojie Shen,Yi Zhou*

Main category: cs.RO

TL;DR: This paper proposes a novel framework for motion-based temporal and rotational calibration in event-centric multi-sensor systems, eliminating the need for dedicated calibration targets, validated through comprehensive experiments.


<details>
  <summary>Details</summary>
Motivation: To address the critical but underexplored challenge of extrinsic calibration for event cameras in multi-sensor systems, which is essential for effective sensor fusion.

Method: The method involves extracting rotational motion estimates from event cameras and other sensors without event-to-frame conversion, using angular velocity obtained from normal flow observations. The pipeline consists of a two-step approach: initialization using Canonical Correlation Analysis (CCA) and refinement of parameters through joint non-linear optimization in SO(3).

Result: The method achieves calibration accuracy on par with target-based techniques, while offering better stability than purely CCA-based methods. Evaluations on multiple datasets demonstrate its precision, robustness, and flexibility.

Conclusion: The proposed calibration framework is a robust, accurate, and flexible alternative to conventional approaches. The open-source release of the implementation promotes accessibility and future research advancements in the field.

Abstract: Event cameras generate asynchronous signals in response to pixel-level
brightness changes, offering a sensing paradigm with theoretically
microsecond-scale latency that can significantly enhance the performance of
multi-sensor systems. Extrinsic calibration is a critical prerequisite for
effective sensor fusion; however, the configuration that involves event cameras
remains an understudied topic. In this paper, we propose a motion-based
temporal and rotational calibration framework tailored for event-centric
multi-sensor systems, eliminating the need for dedicated calibration targets.
Our method uses as input the rotational motion estimates obtained from event
cameras and other heterogeneous sensors, respectively. Different from
conventional approaches that rely on event-to-frame conversion, our method
efficiently estimates angular velocity from normal flow observations, which are
derived from the spatio-temporal profile of event data. The overall calibration
pipeline adopts a two-step approach: it first initializes the temporal offset
and rotational extrinsics by exploiting kinematic correlations in the spirit of
Canonical Correlation Analysis (CCA), and then refines both temporal and
rotational parameters through a joint non-linear optimization using a
continuous-time parametrization in SO(3). Extensive evaluations on both
publicly available and self-collected datasets validate that the proposed
method achieves calibration accuracy comparable to target-based methods, while
exhibiting superior stability over purely CCA-based methods, and highlighting
its precision, robustness and flexibility. To facilitate future research, our
implementation will be made open-source. Code:
https://github.com/NAIL-HNU/EvMultiCalib.

</details>


### [469] [Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory](https://arxiv.org/abs/2508.12681)
*Johann Licher,Max Bartholdt,Henrik Krauss,Tim-Lukas Habich,Thomas Seel,Moritz Schappler*

Main category: cs.RO

TL;DR: This work develops a real-time nonlinear model-predictive control framework for soft continuum robots using a domain-decoupled physics-informed neural network (DD-PINN) that achieves high accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Dynamic control of soft continuum robots is difficult due to high computational demands of accurate dynamic models and limitations of existing methods like Koopman-operator approaches.

Method: The paper introduces a domain-decoupled physics-informed neural network (DD-PINN) as a surrogate model for the dynamic Cosserat rod, paired with an unscented Kalman filter for state estimation and MPC for control at 70 Hz.

Result: The framework achieves a speedup of 44000 for simulations, with precise trajectory tracking accuracy of sub-3 mm errors in both simulations and real-world experiments.

Conclusion: The proposed real-time-capable control framework significantly enhances the practicality and precision of dynamic control for soft continuum robots.

Abstract: Dynamic control of soft continuum robots (SCRs) holds great potential for
expanding their applications, but remains a challenging problem due to the high
computational demands of accurate dynamic models. While data-driven approaches
like Koopman-operator-based methods have been proposed, they typically lack
adaptability and cannot capture the full robot shape, limiting their
applicability. This work introduces a real-time-capable nonlinear
model-predictive control (MPC) framework for SCRs based on a domain-decoupled
physics-informed neural network (DD-PINN) with adaptable bending stiffness. The
DD-PINN serves as a surrogate for the dynamic Cosserat rod model with a
speed-up factor of 44000. It is also used within an unscented Kalman filter for
estimating the model states and bending compliance from end-effector position
measurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the
GPU. In simulation, it demonstrates accurate tracking of dynamic trajectories
and setpoint control with end-effector position errors below 3 mm (2.3% of the
actuator's length). In real-world experiments, the controller achieves similar
accuracy and accelerations up to 3.55 m/s2.

</details>


### [470] [MCTR: Midpoint Corrected Triangulation for Autonomous Racing via Digital Twin Simulation in CARLA](https://arxiv.org/abs/2508.12729)
*Junhao Ye,Cheng Hu,Yiqin Wang,Weizhan Huang,Nicolas Baumann,Jie He,Meixun Qu,Lei Xie,Hongye Su*

Main category: cs.RO

TL;DR: This paper introduces the MCTR algorithm to enhance trajectory smoothness and validate performance using 3D LiDAR in the CARLA simulator and real-world tests.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve autonomous racing by addressing the limitations of existing methods like DTR with smoother trajectory generation and better validation environments.

Method: The MCTR algorithm uses Curvature Corrected Moving Average for smoother trajectory generation and introduces a digital twin system in CARLA for robust testing with 3D LiDAR.

Result: MCTR demonstrated improved trajectory smoothness and robustness, validated through simulations and real-world experiments.

Conclusion: MCTR offers a significant improvement in trajectory smoothness and realistic validation for autonomous racing, overcoming limitations of prior methods and simulators.

Abstract: In autonomous racing, reactive controllers eliminate the computational burden
of the full See-Think-Act autonomy stack by directly mapping sensor inputs to
control actions. This bypasses the need for explicit localization and
trajectory planning. A widely adopted baseline in this category is the
Follow-The-Gap method, which performs trajectory planning using LiDAR data.
Building on FTG, the Delaunay Triangulation-based Racing algorithm introduces
further enhancements. However, DTR's use of circumcircles for trajectory
generation often results in insufficiently smooth paths, ultimately degrading
performance. Additionally, the commonly used F1TENTH-simulator for autonomous
racing competitions lacks support for 3D LiDAR perception, limiting its
effectiveness in realistic testing. To address these challenges, this work
proposes the MCTR algorithm. MCTR improves trajectory smoothness through the
use of Curvature Corrected Moving Average and implements a digital twin system
within the CARLA simulator to validate the algorithm's robustness under 3D
LiDAR perception. The proposed algorithm has been thoroughly validated through
both simulation and real-world vehicle experiments.

</details>


### [471] [RoboRetriever: Single-Camera Robot Object Retrieval via Active and Interactive Perception with Dynamic Scene Graph](https://arxiv.org/abs/2508.12916)
*Hecheng Wang,Jiankun Ren,Jia Yu,Lizhe Qi,Yunquan Sun*

Main category: cs.RO

TL;DR: The paper introduces RoboRetriever, a robot framework that uses a single wrist-mounted RGB-D camera to retrieve objects in cluttered environments, leveraging dynamic scene graphs, active perception, and language-guided reasoning.


<details>
  <summary>Details</summary>
Motivation: Robots often require multiple fixed cameras for object retrieval, which increases hardware costs and limits adaptability. This paper aims to create a more adaptable and cost-effective system.

Method: The framework employs a single wrist-mounted RGB-D camera, dynamic hierarchical scene graphs for memory, and a task-aware active perception system with 6-DoF visual prompting using vision-language models.

Result: RoboRetriever performs robustly across complex object retrieval tasks in cluttered environments, even with human interventions, showcasing adaptability.

Conclusion: A single RGB-D camera setup can enable an adaptable and cost-efficient robotic system for real-world object retrieval through dynamic reasoning and integrated action modules.

Abstract: Humans effortlessly retrieve objects in cluttered, partially observable
environments by combining visual reasoning, active viewpoint adjustment, and
physical interaction-with only a single pair of eyes. In contrast, most
existing robotic systems rely on carefully positioned fixed or multi-camera
setups with complete scene visibility, which limits adaptability and incurs
high hardware costs. We present \textbf{RoboRetriever}, a novel framework for
real-world object retrieval that operates using only a \textbf{single}
wrist-mounted RGB-D camera and free-form natural language instructions.
RoboRetriever grounds visual observations to build and update a \textbf{dynamic
hierarchical scene graph} that encodes object semantics, geometry, and
inter-object relations over time. The supervisor module reasons over this
memory and task instruction to infer the target object and coordinate an
integrated action module combining \textbf{active perception},
\textbf{interactive perception}, and \textbf{manipulation}. To enable
task-aware scene-grounded active perception, we introduce a novel visual
prompting scheme that leverages large reasoning vision-language models to
determine 6-DoF camera poses aligned with the semantic task goal and geometry
scene context. We evaluate RoboRetriever on diverse real-world object retrieval
tasks, including scenarios with human intervention, demonstrating strong
adaptability and robustness in cluttered scenes with only one RGB-D camera.

</details>


### [472] [Deformation of the panoramic sphere into an ellipsoid to induce self-motion in telepresence users](https://arxiv.org/abs/2508.12925)
*Eetu Laukka,Evan G. Center,Timo Ojala,Steven M. LaValle,Matti Pouke*

Main category: cs.RO

TL;DR: The paper explores how optical flow-based self-motion illusions can mitigate latency challenges in mobile telepresence robots with 360-degree cameras, but found no significant benefits to task performance or accuracy, and potential risks of increased VR sickness.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the real-time control difficulty posed by the latency of 360-degree camera streaming in mobile telepresence robots.

Method: A self-motion illusion based on optical flow was introduced to simulate robot movement during latency periods, and its effectiveness was evaluated using task completion time, collisions, and VR sickness metrics.

Result: No significant improvement in controlling telepresence robots was observed, and the method may increase VR sickness.

Conclusion: The proposed self-motion illusion method is not yet viable for reducing latency effects and requires further refinement to avoid detrimental effects like VR sickness.

Abstract: Mobile telepresence robots allow users to feel present and explore remote
environments using technology. Traditionally, these systems are implemented
using a camera onboard a mobile robot that can be controlled. Although
high-immersion technologies, such as 360-degree cameras, can increase
situational awareness and presence, they also introduce significant challenges.
Additional processing and bandwidth requirements often result in latencies of
up to seconds. The current delay with a 360-degree camera streaming over the
internet makes real-time control of these systems difficult. Working with
high-latency systems requires some form of assistance to the users.
  This study presents a novel way to utilize optical flow to create an illusion
of self-motion to the user during the latency period between user sending
motion commands to the robot and seeing the actual motion through the
360-camera stream. We find no significant benefit of using the self-motion
illusion to performance or accuracy of controlling a telepresence robot with a
latency of 500 ms, as measured by the task completion time and collisions into
objects. Some evidence is shown that the method might increase virtual reality
(VR) sickness, as measured by the simulator sickness questionnaire (SSQ). We
conclude that further adjustments are necessary in order to render the method
viable.

</details>


### [473] [Simultaneous Contact Sequence and Patch Planning for Dynamic Locomotion](https://arxiv.org/abs/2508.12928)
*Victor Dhédin,Haizhou Zhao,Majid Khadiv*

Main category: cs.RO

TL;DR: The paper introduces a novel framework combining Monte Carlo tree search and whole-body trajectory optimization to enable legged robots to traverse constrained environments effectively.


<details>
  <summary>Details</summary>
Motivation: Legged robots face significant challenges in traversing constrained environments, which require solving complex optimization problems mixing continuous and discrete variables. The authors aim to address this challenge using a new motion planning pipeline.

Method: The proposed method integrates Monte Carlo tree search for contact sequence optimization and patch selection with whole-body trajectory optimization for highly constrained, multi-contact locomotion tasks.

Result: Through simulations, the framework demonstrates the ability to identify dynamically consistent plans and validate their transferability to a quadruped robot. It also achieves complex humanoid maneuvers using the same system.

Conclusion: This work advances the field by presenting the first synchronized approach to contact sequence and patch selection for acyclic multi-contact locomotion, proving effective in simulation and experiments with robots.

Abstract: Legged robots have the potential to traverse highly constrained environments
with agile maneuvers. However, planning such motions requires solving a highly
challenging optimization problem with a mixture of continuous and discrete
decision variables. In this paper, we present a full pipeline based on
Monte-Carlo tree search (MCTS) and whole-body trajectory optimization (TO) to
perform simultaneous contact sequence and patch selection on highly challenging
environments. Through extensive simulation experiments, we show that our
framework can quickly find a diverse set of dynamically consistent plans. We
experimentally show that these plans are transferable to a real quadruped
robot. We further show that the same framework can find highly complex acyclic
humanoid maneuvers. To the best of our knowledge, this is the first
demonstration of simultaneous contact sequence and patch selection for acyclic
multi-contact locomotion using the whole-body dynamics of a quadruped.

</details>


### [474] [Insights from Interviews with Teachers and Students on the Use of a Social Robot in Computer Science Class in Sixth Grade](https://arxiv.org/abs/2508.12946)
*Ann-Sophie Schenk,Stefan Schiffer,Heqiu Song*

Main category: cs.RO

TL;DR: Interviews with sixth-grade teachers and students reveal openness to using social robots in computer science classes, but highlight complex design challenges due to varying requirements.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the requirements and potential applications of social robots in the classroom, taking into account both teacher and student perspectives.

Method: Conducting interviews with teachers and students to gather qualitative insights regarding the use of social robots in computer science education.

Result: Both teachers and students show enthusiasm for robots in classrooms, although their expectations and requirements differ significantly.

Conclusion: While there's a positive attitude toward social robots in education, the diverse and heterogeneous requirements make their design and implementation challenging.

Abstract: In this paper we report on first insights from interviews with teachers and
students on using social robots in computer science class in sixth grade. Our
focus is on learning about requirements and potential applications. We are
particularly interested in getting both perspectives, the teachers' and the
learners' view on how robots could be used and what features they should or
should not have. Results show that teachers as well as students are very open
to robots in the classroom. However, requirements are partially quite
heterogeneous among the groups. This leads to complex design challenges which
we discuss at the end of this paper.

</details>


### [475] [Scaling Whole-body Multi-contact Manipulation with Contact Optimization](https://arxiv.org/abs/2508.12980)
*Victor Levé,João Moura,Sachiya Fujita,Tamon Miyake,Steve Tonneau,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: The paper proposes a novel method for humanoid robots to perform whole-body manipulation tasks more efficiently.


<details>
  <summary>Details</summary>
Motivation: Enable humanoid robots to autonomously perform complex manipulation tasks using their entire body.

Method: Developed a representation for robot/object surfaces and a cost design using gradient-based optimization for planning tasks.

Result: Achieved a 77% improvement in planning time; validated the framework on real hardware with successful experiments.

Conclusion: The proposed framework overcomes limitations of existing methods and advances whole-body manipulation for humanoid robots.

Abstract: Daily tasks require us to use our whole body to manipulate objects, for
instance when our hands are unavailable. We consider the issue of providing
humanoid robots with the ability to autonomously perform similar whole-body
manipulation tasks. In this context, the infinite possibilities for where and
how contact can occur on the robot and object surfaces hinder the scalability
of existing planning methods, which predominantly rely on discrete sampling.
Given the continuous nature of contact surfaces, gradient-based optimization
offers a more suitable approach for finding solutions. However, a key remaining
challenge is the lack of an efficient representation of robot surfaces. In this
work, we propose (i) a representation of robot and object surfaces that enables
closed-form computation of proximity points, and (ii) a cost design that
effectively guides whole-body manipulation planning. Our experiments
demonstrate that the proposed framework can solve problems unaddressed by
existing methods, and achieves a 77% improvement in planning time over the
state of the art. We also validate the suitability of our approach on real
hardware through the whole-body manipulation of boxes by a humanoid robot.

</details>


### [476] [BOW: Bayesian Optimization over Windows for Motion Planning in Complex Environments](https://arxiv.org/abs/2508.13052)
*Sourav Raxit,Abdullah Al Redwan Newaz,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla*

Main category: cs.RO

TL;DR: The BOW Planner uses constrained Bayesian optimization for efficient motion planning, excelling in managing high-dimensional and safety-aware constraints with rapid computation.


<details>
  <summary>Details</summary>
Motivation: To address limitations in kinodynamic constraints, such as velocity and acceleration limits, and improve efficiency and safety in robotic motion planning.

Method: The algorithm uses constrained Bayesian optimization (CBO) to sample control inputs within reachable velocity windows, enabling fast and efficient motion planning even in complex environments.

Result: Theoretical validation confirms asymptotic convergence, and evaluations show improvements in computation speed, trajectory optimization, and solution quality.

Conclusion: The BOW Planner is a practical, scalable, and efficient tool for robotic motion planning, contributing to safety and performance advances in real-world applications.

Abstract: This paper introduces the BOW Planner, a scalable motion planning algorithm
designed to navigate robots through complex environments using constrained
Bayesian optimization (CBO). Unlike traditional methods, which often struggle
with kinodynamic constraints such as velocity and acceleration limits, the BOW
Planner excels by concentrating on a planning window of reachable velocities
and employing CBO to sample control inputs efficiently. This approach enables
the planner to manage high-dimensional objective functions and stringent safety
constraints with minimal sampling, ensuring rapid and secure trajectory
generation. Theoretical analysis confirms the algorithm's asymptotic
convergence to near-optimal solutions, while extensive evaluations in cluttered
and constrained settings reveal substantial improvements in computation times,
trajectory lengths, and solution times compared to existing techniques.
Successfully deployed across various real-world robotic systems, the BOW
Planner demonstrates its practical significance through exceptional sample
efficiency, safety-aware optimization, and rapid planning capabilities, making
it a valuable tool for advancing robotic applications. The BOW Planner is
released as an open-source package and videos of real-world and simulated
experiments are available at https://bow-web.github.io.

</details>


### [477] [Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey](https://arxiv.org/abs/2508.13073)
*Rui Shao,Wei Li,Lingsen Zhang,Renshan Zhang,Zhiyang Liu,Ran Chen,Liqiang Nie*

Main category: cs.RO

TL;DR: This paper surveys the integration of Large Vision-Language Models (VLMs) into Vision-Language-Action (VLA) systems for robotic manipulation, offering taxonomy, advances, and future directions.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for robotic manipulation struggle to generalize in novel environments, while VLM-based VLA models hold promise to address these challenges using multimodal understanding and control.

Method: The paper systematically categorizes VLM-based VLA models into monolithic and hierarchical architectures, examines their integration with advanced techniques, and discusses their operational strengths, datasets, and benchmarks.

Result: The review highlights the potential of VLM-based VLA models in robotic manipulation, suggesting that these approaches can help resolve inconsistencies, reduce fragmentation, and foster future research.

Conclusion: Large VLMs have the potential to transform robotic manipulation by enabling better generalization and understanding. Future research should focus on memory mechanisms, 4D perception, efficient adaptation, and multi-agent cooperation.

Abstract: Robotic manipulation, a key frontier in robotics and embodied AI, requires
precise motor control and multimodal understanding, yet traditional rule-based
methods fail to scale or generalize in unstructured, novel environments. In
recent years, Vision-Language-Action (VLA) models, built upon Large
Vision-Language Models (VLMs) pretrained on vast image-text datasets, have
emerged as a transformative paradigm. This survey provides the first
systematic, taxonomy-oriented review of large VLM-based VLA models for robotic
manipulation. We begin by clearly defining large VLM-based VLA models and
delineating two principal architectural paradigms: (1) monolithic models,
encompassing single-system and dual-system designs with differing levels of
integration; and (2) hierarchical models, which explicitly decouple planning
from execution via interpretable intermediate representations. Building on this
foundation, we present an in-depth examination of large VLM-based VLA models:
(1) integration with advanced domains, including reinforcement learning,
training-free optimization, learning from human videos, and world model
integration; (2) synthesis of distinctive characteristics, consolidating
architectural traits, operational strengths, and the datasets and benchmarks
that support their development; (3) identification of promising directions,
including memory mechanisms, 4D perception, efficient adaptation, multi-agent
cooperation, and other emerging capabilities. This survey consolidates recent
advances to resolve inconsistencies in existing taxonomies, mitigate research
fragmentation, and fill a critical gap through the systematic integration of
studies at the intersection of large VLMs and robotic manipulation. We provide
a regularly updated project page to document ongoing progress:
https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.

</details>


### [478] [Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy](https://arxiv.org/abs/2508.13103)
*Tianyi Zhang,Haonan Duan,Haoran Hao,Yu Qiao,Jifeng Dai,Zhi Hou*

Main category: cs.RO

TL;DR: The paper presents the Observation-Centric VLA (OC-VLA) framework to address inconsistencies in vision-language-action models caused by different viewpoints. It achieves this by grounding action predictions in the observation space, improving model generalization and task success rates.


<details>
  <summary>Details</summary>
Motivation: To address the generalization challenges of vision-language-action models caused by discrepancies between observation and action spaces, especially in real-world environments.

Method: The OC-VLA framework transforms end-effector poses from the robot base coordinate system to the camera coordinate system using the camera's extrinsic calibration matrix. This plug-and-play strategy ensures alignment between perception and action.

Result: The approach significantly accelerates model convergence, improves task success rates, and enhances generalization across camera viewpoints in both simulated and real-world tasks.

Conclusion: The OC-VLA framework, compatible with existing architectures, provides a robust and efficient solution to viewpoint inconsistencies, making it a practical advancement for robotic manipulation tasks.

Abstract: Vision-Language-Action (VLA) models frequently encounter challenges in
generalizing to real-world environments due to inherent discrepancies between
observation and action spaces. Although training data are collected from
diverse camera perspectives, the models typically predict end-effector poses
within the robot base coordinate frame, resulting in spatial inconsistencies.
To mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)
framework, which grounds action predictions directly in the camera observation
space. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms
end-effector poses from the robot base coordinate system into the camera
coordinate system, thereby unifying prediction targets across heterogeneous
viewpoints. This lightweight, plug-and-play strategy ensures robust alignment
between perception and action, substantially improving model resilience to
camera viewpoint variations. The proposed approach is readily compatible with
existing VLA architectures, requiring no substantial modifications.
Comprehensive evaluations on both simulated and real-world robotic manipulation
tasks demonstrate that OC-VLA accelerates convergence, enhances task success
rates, and improves cross-view generalization. The code will be publicly
available.

</details>


### [479] [Manipulate-to-Navigate: Reinforcement Learning with Visual Affordances and Manipulability Priors](https://arxiv.org/abs/2508.13151)
*Yuying Zhang,Joni Pajarinen*

Main category: cs.RO

TL;DR: This paper proposes a reinforcement learning approach for mobile manipulation in dynamic environments, focusing on combining manipulability priors and affordance maps for better manipulation strategies. It evaluates the method through two tasks and demonstrates successful implementation on a Spot robot.


<details>
  <summary>Details</summary>
Motivation: Address the challenge in dynamic environments where movable obstacles block the robot's path, requiring active interaction to clear obstacles for navigation.

Method: A reinforcement learning-based approach combining manipulability priors and affordance maps to guide robots in selecting and learning effective manipulation actions.

Result: The method was tested on two simulation tasks (Reach and Door) using the Spot robot. It demonstrated effective manipulation strategies and was successfully transferred to a real robotic system for the Reach task.

Conclusion: The approach improves the robot's ability to handle dynamic environments by combining manipulation and navigation tasks, showing potential for real-world applications.

Abstract: Mobile manipulation in dynamic environments is challenging due to movable
obstacles blocking the robot's path. Traditional methods, which treat
navigation and manipulation as separate tasks, often fail in such
'manipulate-to-navigate' scenarios, as obstacles must be removed before
navigation. In these cases, active interaction with the environment is required
to clear obstacles while ensuring sufficient space for movement. To address the
manipulate-to-navigate problem, we propose a reinforcement learning-based
approach for learning manipulation actions that facilitate subsequent
navigation. Our method combines manipulability priors to focus the robot on
high manipulability body positions with affordance maps for selecting
high-quality manipulation actions. By focusing on feasible and meaningful
actions, our approach reduces unnecessary exploration and allows the robot to
learn manipulation strategies more effectively. We present two new
manipulate-to-navigate simulation tasks called Reach and Door with the Boston
Dynamics Spot robot. The first task tests whether the robot can select a good
hand position in the target area such that the robot base can move effectively
forward while keeping the end effector position fixed. The second task requires
the robot to move a door aside in order to clear the navigation path. Both of
these tasks need first manipulation and then navigating the base forward.
Results show that our method allows a robot to effectively interact with and
traverse dynamic environments. Finally, we transfer the learned policy to a
real Boston Dynamics Spot robot, which successfully performs the Reach task.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [480] [Rethinking Autonomy: Preventing Failures in AI-Driven Software Engineering](https://arxiv.org/abs/2508.11824)
*Satyam Kumar Navneet,Joydeep Chandra*

Main category: cs.SE

TL;DR: The paper addresses risks in integrating large language models (LLMs) for code generation, introducing the SAFE-AI framework to mitigate dangers while ensuring trust, transparency, and accountability.


<details>
  <summary>Details</summary>
Motivation: Enable the use of LLMs in software engineering for productivity while addressing significant risks such as insecure outputs, hallucinations, and accountability issues.

Method: Proposed the SAFE-AI Framework that incorporates guardrails, sandboxing, runtime verification, human-in-the-loop systems, explainable AI techniques, and a taxonomy for categorizing AI behaviors.

Result: Introduced a comprehensive risk mitigation framework, discussed open problems like the lack of benchmarks for AI risks, and proposed future research directions.

Conclusion: SAFE-AI framework lays a foundation for responsibly integrating LLMs in software engineering while adhering to evolving regulations like the EU AI Act.

Abstract: The integration of Large Language Models (LLMs) into software engineering has
revolutionized code generation, enabling unprecedented productivity through
promptware and autonomous AI agents. However, this transformation introduces
significant risks, including insecure code generation, hallucinated outputs,
irreversible actions, and a lack of transparency and accountability. Incidents
like the Replit database deletion underscore the urgent need for robust safety
and governance mechanisms. This paper comprehensively analyzes the inherent
challenges of LLM-assisted code generation, such as vulnerability inheritance,
overtrust, misinterpretation, and the absence of standardized validation and
rollback protocols. To address these, we propose the SAFE-AI Framework, a
holistic approach emphasizing Safety, Auditability, Feedback, and
Explainability. The framework integrates guardrails, sandboxing, runtime
verification, risk-aware logging, human-in-the-loop systems, and explainable AI
techniques to mitigate risks while fostering trust and compliance. We introduce
a novel taxonomy of AI behaviors categorizing suggestive, generative,
autonomous, and destructive actions to guide risk assessment and oversight.
Additionally, we identify open problems, including the lack of standardized
benchmarks for code specific hallucinations and autonomy levels, and propose
future research directions for hybrid verification, semantic guardrails, and
proactive governance tools. Through detailed comparisons of autonomy control,
prompt engineering, explainability, and governance frameworks, this paper
provides a roadmap for responsible AI integration in software engineering,
aligning with emerging regulations like the EU AI Act and Canada's AIDA to
ensure safe, transparent, and accountable AI-driven development.

</details>


### [481] [Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs](https://arxiv.org/abs/2508.11715)
*Ananya Singha,Harshita Sahijwani,Walt Williams,Emmanuel Aboah Boateng,Nick Hausman,Miguel Di Luca,Keegan Choudhury,Chaya Binet,Vu Le,Tianwei Chen,Oryan Rokeah Chen,Sulaiman Vesal,Sadid Hasan*

Main category: cs.SE

TL;DR: This paper addresses the challenge of semantic runtime errors in Excel by introducing a novel benchmark dataset for Excel formula repair and evaluating various large language models (LLMs) on it.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the lack of high-quality datasets for training and evaluating models to fix Excel formula runtime errors, enabling progress in automated error correction.

Method: The paper introduces a data generation pipeline using curated seed samples, few-shot prompting with LLMs, and validation frameworks involving LLM-as-a-Judge and execution-based checks to produce a benchmark dataset.

Result: The method generates a high-quality benchmark dataset of 618 samples for Excel formula repair, evaluates various LLMs on this dataset, and provides insights into common runtime errors and their distributions.

Conclusion: The dataset and methodology are proven effective and scalable, enabling reliable evaluation and the potential extension to other low-resource programming languages for code repair tasks.

Abstract: Excel is a pervasive yet often complex tool, particularly for novice users,
where runtime errors arising from logical mistakes or misinterpretations of
functions pose a significant challenge. While large language models (LLMs)
offer promising assistance by explaining formula errors, the automated
correction of these semantic runtime errors remains an open problem. A primary
challenge to advancing models for such scenarios is the severe lack of
high-quality, comprehensive datasets for training and rigorous evaluation. This
paper addresses this gap by introducing a novel approach for constructing a
benchmark dataset specifically designed for Excel formula repair. We propose a
data generation pipeline, which leverages a small set of curated seed samples
from online forums to synthetically expand the dataset. Our pipeline integrates
few-shot prompting with LLMs and employs a robust \textit{LLM-as-a-Judge}
validation framework, combined with execution-based checks to ensure the
correctness and semantic fidelity of the generated data. This process produced
a benchmark dataset of 618 high-quality samples, covering common runtime
errors. Furthermore, we propose a context-aware baseline technique for Excel
formula repair that utilizes LLMs to leverage both the faulty formula, and
relevant spreadsheet context. We evaluate the performance of various LLMs
(GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using
execution-based metrics. Our analysis demonstrates the dataset's quality
through manual annotation and provides insights into error and function
distributions. The proposed generation methodology is highly scalable and can
be readily adapted to create evaluation benchmarks for similar code repair
tasks in other low-resource programming languages.

</details>


### [482] [WIP: Leveraging LLMs for Enforcing Design Principles in Student Code: Analysis of Prompting Strategies and RAG](https://arxiv.org/abs/2508.11717)
*Dhruv Kolhatkar,Soubhagya Akkena,Edward F. Gehringer*

Main category: cs.SE

TL;DR: This paper investigates using Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) to provide automated code reviews for open-source software in educational contexts.


<details>
  <summary>Details</summary>
Motivation: The need for scalable and effective tools to teach key object-oriented design principles to computer science and software engineering students.

Method: The paper develops an automated feedback tool using LLMs and RAG to evaluate student code against design principles like SOLID, DRY, and design patterns. It tests various prompting strategies and RAG integration.

Result: Preliminary findings show promising improvements in the quality of student code when assessed using this approach.

Conclusion: The integration of LLMs and RAG holds potential for enhancing software design education, with future plans to improve accuracy and expand the scope of design principles covered.

Abstract: This work-in-progress research-to-practice paper explores the integration of
Large Language Models (LLMs) into the code-review process for open-source
software projects developed in computer science and software engineering
courses. The focus is on developing an automated feedback tool that evaluates
student code for adherence to key object-oriented design principles, addressing
the need for more effective and scalable methods to teach software design best
practices. The innovative practice involves leveraging LLMs and
Retrieval-Augmented Generation (RAG) to create an automated feedback system
that assesses student code for principles like SOLID, DRY, and design patterns.
It analyzes the effectiveness of various prompting strategies and the RAG
integration. Preliminary findings show promising improvements in code quality.
Future work will aim to improve model accuracy and expand support for
additional design principles.

</details>


### [483] [AI-Augmented CI/CD Pipelines: From Code Commit to Production with Autonomous Decisions](https://arxiv.org/abs/2508.11867)
*Mohammad Baqar,Saba Naqvi,Rajat Khanda*

Main category: cs.SE

TL;DR: The paper proposes AI-Augmented CI/CD pipelines to reduce human decision latency and operational toil during software deployment by embedding LLMs and autonomous agents for decision-making.


<details>
  <summary>Details</summary>
Motivation: Modern software delivery workflows are hindered by slow human decisions interpreting flaky tests, feature flag tuning, rollback strategies, and canary promotions. The paper aims to decrease operational bottlenecks by leveraging AI.

Method: The authors introduce an architecture with agentic decision points, a taxonomy for policies, a trust-tier staged autonomy framework, along with evaluation methods and a detailed case study migrating a microservice to an AI-integrated pipeline.

Result: They presented a novel reference architecture and demonstrated the pipeline migration through a case study with measurable evaluation metrics and insights into challenges.

Conclusion: AI-Augmented CI/CD pipelines can reduce operational toil and accelerate software delivery, but attention to ethics, auditability, and risk management is critical for widespread adoption.

Abstract: Modern software delivery has accelerated from quarterly releases to multiple
deployments per day. While CI/CD tooling has matured, human decision points
interpreting flaky tests, choosing rollback strategies, tuning feature flags,
and deciding when to promote a canary remain major sources of latency and
operational toil. We propose AI-Augmented CI/CD Pipelines, where large language
models (LLMs) and autonomous agents act as policy-bounded co-pilots and
progressively as decision makers. We contribute: (1) a reference architecture
for embedding agentic decision points into CI/CD, (2) a decision taxonomy and
policy-as-code guardrail pattern, (3) a trust-tier framework for staged
autonomy, (4) an evaluation methodology using DevOps Research and Assessment (
DORA) metrics and AI-specific indicators, and (5) a detailed industrial-style
case study migrating a React 19 microservice to an AI-augmented pipeline. We
discuss ethics, verification, auditability, and threats to validity, and chart
a roadmap for verifiable autonomy in production delivery systems.

</details>


### [484] [Clean Code, Better Models: Enhancing LLM Performance with Smell-Cleaned Dataset](https://arxiv.org/abs/2508.11958)
*Zhipeng Xue,Xiaoting Zhang,Zhipeng Gao,Xing Hu,Shan Gao,Xin Xia,Shanping Li*

Main category: cs.SE

TL;DR: This study analyzes and cleans code smells in LLM training datasets and outputs, using an LLM-based tool 'SmellCC'. It showed that reducing code smells improves code quality in generation and downstream tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs perform well in code-related tasks, but limited attention has been given to the quality of their training data, which often includes code smells affecting code maintainability and readability.

Method: The study involved identifying code smells in datasets, developing the 'SmellCC' tool for automated cleaning, fine-tuning LLMs on smell-free data, and assessing performance on downstream tasks.

Result: Code smells are prevalent in benchmark datasets and generated code. Removing these smells improves LLM outputs in code generation and downstream metrics like code completion and code search.

Conclusion: Improving dataset quality by addressing code smells can enhance the usability and effectiveness of LLMs in software engineering. SmellCC proved effective for this purpose.

Abstract: The Large Language Models (LLMs) have demonstrated great potential in
code-related tasks. However, most research focuses on improving the output
quality of LLMs (e.g., correctness), and less attention has been paid to the
LLM input (e.g., the training code quality). Given that code smells are widely
existed in practice and can negatively impact software maintainability and
readability, this study takes the first systematic research to assess and
improve dataset quality in terms of code smells. In this work, we first conduct
a preliminary study to explore the presence of code smells in a popular
benchmark dataset (i.e., CodeSearchNet-Python}) and evaluate the output of
several popular LLMs (i.e., DeepSeek-Coder, CodeLlama, and MagiCoder),
revealing that code smell issues extensively exist in LLM's input (e.g.,
benchmark dataset) and output (e.g., generated code). We then conduct our
systematic research by taking three main steps: Firstly, we propose an
LLM-based code smell cleaning tool, named SmellCC, which automatically
refactors and removes code smells. To evaluate the correctness of the code
refactoring, we construct a test set of 50 repositories sourced from the
CodeSearchNet-Python benchmark for functional testing. Then we apply our
curated smell-cleaned dataset to fine-tune two LLMs (i.e., DeepSeek-V2 and
Qwen-Coder) to explore their potential for generating high-quality code.
Thirdly, we investigate the impact of code smells on two downstream tasks: code
completion and code search. Lastly, we derive several actionable implications
for software engineering researchers and industry practitioners from our
findings.

</details>


### [485] [How Much Can a Behavior-Preserving Changeset Be Decomposed into Refactoring Operations?](https://arxiv.org/abs/2508.11993)
*Kota Someya,Lei Chen,Michael J. Decker,Shinpei Hayashi*

Main category: cs.SE

TL;DR: This paper investigates the decomposition of behavior-preserving modifications into refactorings, finding that existing detectors cover only 33.9%, while defining additional operations increases this to over 128%.


<details>
  <summary>Details</summary>
Motivation: To improve developers' understanding of mixed modifications by separating behavior-preserving changes from behavior-altering ones, and to assess how well they can be decomposed into primitive refactoring operations.

Method: The paper uses a dataset of functionally-equivalent method pairs and applies an existing refactoring detector, alongside 67 new functionally-equivalent operations, to quantify the coverage of refactorings.

Result: Existing refactoring detectors identified only 33.9% of behavior-preserving changes as refactorings, but with the addition of 67 new operations, the coverage increased by 128%.

Conclusion: Expanding the set of recognized refactoring operations improves coverage significantly, and the unexplained changes highlight opportunities for enhancing detection methods.

Abstract: Developers sometimes mix behavior-preserving modifications, such as
refactorings, with behavior-altering modifications, such as feature additions.
Several approaches have been proposed to support understanding such
modifications by separating them into those two parts. Such refactoring-aware
approaches are expected to be particularly effective when the
behavior-preserving parts can be decomposed into a sequence of more primitive
behavior-preserving operations, such as refactorings, but this has not been
explored. In this paper, as an initial validation, we quantify how much of the
behavior-preserving modifications can be decomposed into refactoring operations
using a dataset of functionally-equivalent method pairs. As a result, when
using an existing refactoring detector, only 33.9% of the changes could be
identified as refactoring operations. In contrast, when including 67 newly
defined functionally-equivalent operations, the coverage increased by over
128%. Further investigation into the remaining unexplained differences was
conducted, suggesting improvement opportunities.

</details>


### [486] [LinkAnchor: An Autonomous LLM-Based Agent for Issue-to-Commit Link Recovery](https://arxiv.org/abs/2508.12232)
*Arshia Akhavan,Alireza Hosseinpour,Abbas Heydarnoori,Mehdi Keshani*

Main category: cs.SE

TL;DR: LinkAnchor improves issue-to-commit link recovery by leveraging a dynamic retrieval approach with LLMs, achieving significant accuracy gains compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Improve software traceability and address the limitations of current AI/ML methods for linking issues to commits, particularly constraints posed by large datasets and LLM context window limits.

Method: Introduce LinkAnchor, an autonomous LLM-based agent with lazy-access architecture for dynamically retrieving relevant contextual data and pinpointing target commits without exhaustive scoring.

Result: LinkAnchor outperforms existing methods by 60-262% in Hit@1 score across case study projects, showing significant gains in link recovery accuracy.

Conclusion: LinkAnchor offers a scalable, efficient, and extendable solution for issue-to-commit link recovery, demonstrating its utility for GitHub and Jira while paving the way for broader applications.

Abstract: Issue-to-commit link recovery plays an important role in software
traceability and improves project management. However, it remains a challenging
task. A study on GitHub shows that only 42.2% of the issues are correctly
linked to their commits. This highlights the potential for further development
and research in this area. Existing studies have employed various AI/ML-based
approaches, and with the recent development of large language models,
researchers have leveraged LLMs to tackle this problem. These approaches suffer
from two main issues. First, LLMs are constrained by limited context windows
and cannot ingest all of the available data sources, such as long commit
histories, extensive issue comments, and large code repositories. Second, most
methods operate on individual issue-commit pairs; that is, given a single
issue-commit pair, they determine whether the commit resolves the issue. This
quickly becomes impractical in real-world repositories containing tens of
thousands of commits. To address these limitations, we present LinkAnchor, the
first autonomous LLM-based agent designed for issue-to-commit link recovery.
The lazy-access architecture of LinkAnchor enables the underlying LLM to access
the rich context of software, spanning commits, issue comments, and code files,
without exceeding the token limit by dynamically retrieving only the most
relevant contextual data. Additionally, LinkAnchor is able to automatically
pinpoint the target commit rather than exhaustively scoring every possible
candidate. Our evaluations show that LinkAnchor outperforms state-of-the-art
issue-to-commit link recovery approaches by 60-262% in Hit@1 score across all
our case study projects. We also publicly release LinkAnchor as a ready-to-use
tool, along with our replication package. LinkAnchor is designed and tested for
GitHub and Jira, and is easily extendable to other platforms.

</details>


### [487] ["My productivity is boosted, but ..." Demystifying Users' Perception on AI Coding Assistants](https://arxiv.org/abs/2508.12285)
*Yunbo Lyu,Zhou Yang,Jieke Shi,Jianming Chang,Yue Liu,David Lo*

Main category: cs.SE

TL;DR: The paper investigates user evaluations of AI coding assistants, analyzing real-world reviews to understand developers' expectations and concerns.


<details>
  <summary>Details</summary>
Motivation: Developers increasingly use AI coding assistants like GitHub Copilot. The paper seeks to understand what users value and criticize to better align these tools with real-world development needs.

Method: Researchers manually analyze user reviews from 32 AI coding assistants on the Visual Studio Code Marketplace, categorizing attitudes and concerns to create a taxonomy of feedback.

Result: Insights reveal user satisfaction and dissatisfaction with specific features and interactions. Findings highlight user demands for intelligent, context-aware, customizable, and efficient coding assistants.

Conclusion: Based on these findings, the authors propose five actionable suggestions to improve AI coding assistants, aiming to better meet developer needs in real-world software development contexts.

Abstract: This paper aims to explore fundamental questions in the era when AI coding
assistants like GitHub Copilot are widely adopted: what do developers truly
value and criticize in AI coding assistants, and what does this reveal about
their needs and expectations in real-world software development? Unlike
previous studies that conduct observational research in controlled and
simulated environments, we analyze extensive, first-hand user reviews of AI
coding assistants, which capture developers' authentic perspectives and
experiences drawn directly from their actual day-to-day work contexts. We
identify 1,085 AI coding assistants from the Visual Studio Code Marketplace.
Although they only account for 1.64% of all extensions, we observe a surge in
these assistants: over 90% of them are released within the past two years. We
then manually analyze the user reviews sampled from 32 AI coding assistants
that have sufficient installations and reviews to construct a comprehensive
taxonomy of user concerns and feedback about these assistants. We manually
annotate each review's attitude when mentioning certain aspects of coding
assistants, yielding nuanced insights into user satisfaction and
dissatisfaction regarding specific features, concerns, and overall tool
performance. Built on top of the findings-including how users demand not just
intelligent suggestions but also context-aware, customizable, and
resource-efficient interactions-we propose five practical implications and
suggestions to guide the enhancement of AI coding assistants that satisfy user
needs.

</details>


### [488] [From Fomo3D to Lottery DAPP: Analysis of Ethereum-Based Gambling Applications](https://arxiv.org/abs/2508.12303)
*Xu Long,Yishun Wang,Xiaoqi Li*

Main category: cs.SE

TL;DR: This paper explores Ethereum-based gambling DApps, focusing on their operational principles, technical implementation, vulnerabilities, and future prospects.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address how blockchain technology can improve transparency, fairness, and automation in the online gambling industry through Ethereum-based lottery platforms.

Method: It combines a conceptual review of gambling DApps, technical analysis of existing implementations, and exploration of blockchain tools for development.

Result: Key advantages of lottery DApps include decentralization, improved cost-efficiency, enhanced transparency, and reduced managerial overhead.

Conclusion: Lottery DApps are likely to play a transformative role in the online gambling sector due to their advantages, despite current technical barriers and limitations.

Abstract: As blockchain technology advances, Ethereum based gambling decentralized
applications (DApps) represent a new paradigm in online gambling. This paper
examines the concepts, principles, implementation, and prospects of Ethereum
based gambling DApps. First, we outline the concept and operational principles
of gambling DApps. These DApps are blockchain based online lottery platforms.
They utilize smart contracts to manage the entire lottery process, including
issuance, betting, drawing, and prize distribution. Being decentralized,
lottery DApps operate without central oversight, unlike traditional lotteries.
This ensures fairness and eliminates control by any single entity. Automated
smart contract execution further reduces management costs, increases
profitability, and enhances game transparency and credibility. Next, we analyze
an existing Ethereum based gambling DApp, detailing its technical principles,
implementation, operational status, vulnerabilities, and potential solutions.
We then elaborate on the implementation of lottery DApps. Smart contracts
automate the entire lottery process including betting, drawing, and prize
distribution. Although developing lottery DApps requires technical expertise,
the expanding Ethereum ecosystem provides growing tools and frameworks,
lowering development barriers. Finally, we discuss current limitations and
prospects of lottery DApps. As blockchain technology and smart contracts
evolve, lottery DApps are positioned to significantly transform the online
lottery industry. Advantages like decentralization, automation, and
transparency will likely drive broader future adoption.

</details>


### [489] [Towards the Coordination and Verification of Heterogeneous Systems with Data and Time](https://arxiv.org/abs/2508.12325)
*Tim Kräuter,Adrian Rutle,Yngve Lamo,Harald König,Francisco Durán*

Main category: cs.SE

TL;DR: Developed a non-intrusive coordination framework for formal analysis and verification of real-time heterogeneous system components, demonstrated on a road-rail crossing system.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of seamlessly coordinating heterogeneous software system parts while verifying their overall correctness.

Method: Designed a non-intrusive framework using a central broker, a domain-specific language, abstract rule templates, and implemented it using rewriting logic (Maude).

Result: Showcase of the framework's effectiveness through verification of correctness properties in a heterogeneous road-rail crossing system.

Conclusion: The framework facilitates formal analysis and integration of heterogeneous systems through non-intrusive methods, improving reliability in complex applications.

Abstract: Modern software systems are often realized by coordinating multiple
heterogeneous parts, each responsible for specific tasks. These parts must work
together seamlessly to satisfy the overall system requirements. To verify such
complex systems, we have developed a non-intrusive coordination framework
capable of performing formal analysis of heterogeneous parts that exchange data
and include real-time capabilities. The framework utilizes a linguistic
extension, which is implemented as a central broker and a domain-specific
language for the integration of heterogeneous languages and coordination of
parts. Moreover, abstract rule templates are reified as language adapters for
non-intrusive communications with the broker. The framework is implemented
using rewriting logic (Maude), and its applicability is demonstrated by
verifying certain correctness properties of a heterogeneous road-rail crossing
system.

</details>


### [490] [Uncovering Systematic Failures of LLMs in Verifying Code Against Natural Language Specifications](https://arxiv.org/abs/2508.12358)
*Haolin Jin,Huaming Chen*

Main category: cs.SE

TL;DR: The paper reveals that large language models (LLMs) often fail to accurately evaluate code against natural language specifications, especially with complex prompts, and proposes improved strategies for mitigation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to assess whether LLMs can reliably determine if code aligns with natural language task descriptions, which is essential for enhancing code quality and robustness in software engineering.

Method: The study uses widely recognized benchmarks and unified prompts to test LLMs' accuracy in judging code correctness. It also employs complex prompt engineering techniques, such as explanations and corrections, to understand their impact.

Result: The results demonstrate that LLMs frequently misclassify correct code as incorrect or defective. Surprisingly, using complex prompt engineering increases the misjudgment rate, emphasizing reliability concerns.

Conclusion: The findings highlight critical limitations of LLMs in matching code to requirements and offer actionable prompting strategies to mitigate these issues. The work provides valuable insights for improving LLM-based automated code review systems.

Abstract: Large language models (LLMs) have become essential tools in software
development, widely used for requirements engineering, code generation and
review tasks. Software engineers often rely on LLMs to assess whether system
code implementation satisfy task requirements, thereby enhancing code
robustness and accuracy. However, it remains unclear whether LLMs can reliably
determine whether the code complies fully with the given task descriptions,
which is usually natural language specifications. In this paper, we uncover a
systematic failure of LLMs in evaluating whether code aligns with natural
language requirements. Specifically, with widely used benchmarks, we employ
unified prompts to judge code correctness. Our results reveal that LLMs
frequently misclassify correct code implementations as either ``not satisfying
requirements'' or containing potential defects. Surprisingly, more complex
prompting, especially when leveraging prompt engineering techniques involving
explanations and proposed corrections, leads to higher misjudgment rate, which
highlights the critical reliability issues in using LLMs as code review
assistants. We further analyze the root causes of these misjudgments, and
propose two improved prompting strategies for mitigation. For the first time,
our findings reveals unrecognized limitations in LLMs to match code with
requirements. We also offer novel insights and practical guidance for effective
use of LLMs in automated code review and task-oriented agent scenarios.

</details>


### [491] [Feature Request Analysis and Processing: Tasks, Techniques, and Trends](https://arxiv.org/abs/2508.12436)
*Feifei Niu,Chuanyi Li,Haosheng Zuo,Jionghan Wu,Xin Xia*

Main category: cs.SE

TL;DR: The paper provides a systematic overview of research on feature requests in software products, analyzing 131 studies and highlighting challenges and opportunities for future advancements.


<details>
  <summary>Details</summary>
Motivation: To collectively analyze diverse research on feature requests and identify challenges and opportunities for advancing user-centered software development.

Method: The authors conducted a systematic review of 131 primary studies using descriptive statistics and qualitative analysis, categorizing them based on requirements engineering perspectives.

Result: Key challenges such as ensuring feature request quality, improving specification and validation processes, and creating benchmarks for large language models were identified.

Conclusion: The study highlights the need for better tools, datasets, and quality benchmarks to improve research and practices around feature request handling in software engineering.

Abstract: Feature requests are proposed by users to request new features or
enhancements of existing features of software products, which represent users'
wishes and demands. Satisfying users' demands can benefit the product from both
competitiveness and user satisfaction. Feature requests have seen a rise in
interest in the past few years and the amount of research has been growing.
However, the diversity in the research topics suggests the need for their
collective analysis to identify the challenges and opportunities so as to
promote new advances in the future. In this work, following a defined process
and a search protocol, we provide a systematic overview of the research area by
searching and categorizing relevant studies. We select and analyze 131 primary
studies using descriptive statistics and qualitative analysis methods. We
classify the studies into different topics and group them from the perspective
of requirements engineering activities. We investigate open tools as well as
datasets for future research. In addition, we identify several key challenges
and opportunities, such as: (1) ensuring the quality of feature requests, (2)
improving their specification and validation, and (3) developing high-quality
benchmarks for large language model-driven tasks.

</details>


### [492] [XAMT: Cross-Framework API Matching for Testing Deep Learning Libraries](https://arxiv.org/abs/2508.12546)
*Bin Duan,Ruican Dong,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: The paper introduces XAMT, a cross-framework fuzzing method for detecting bugs in deep learning libraries, by comparing functionally equivalent APIs across frameworks. XAMT identifies bugs missed by traditional intra-framework testing methods.


<details>
  <summary>Details</summary>
Motivation: Bugs in deep learning libraries can cause critical failures across applications like autonomous driving and healthcare. Current testing methods might miss bugs that manifest identically across hardware backends.

Method: XAMT tests deep learning APIs by matching and comparing functionally similar APIs across different frameworks using similarity-based rules. It employs variance-guided differential testing to detect bugs.

Result: XAMT matched 839 APIs and identified 17 bugs across popular frameworks such as PyTorch and TensorFlow, with 12 confirmed bugs. It successfully detected bugs that traditional methods cannot.

Conclusion: XAMT complements existing debugging techniques, offering improved detection of bugs and a novel perspective on testing deep learning libraries.

Abstract: Deep learning powers critical applications such as autonomous driving,
healthcare, and finance, where the correctness of underlying libraries is
essential. Bugs in widely used deep learning APIs can propagate to downstream
systems, causing serious consequences. While existing fuzzing techniques detect
bugs through intra-framework testing across hardware backends (CPU vs. GPU),
they may miss bugs that manifest identically across backends and thus escape
detection under these strategies. To address this problem, we propose XAMT, a
cross-framework fuzzing method that tests deep learning libraries by matching
and comparing functionally equivalent APIs across different frameworks. XAMT
matches APIs using similarity-based rules based on names, descriptions, and
parameter structures. It then aligns inputs and applies variance-guided
differential testing to detect bugs. We evaluated XAMT on five popular
frameworks, including PyTorch, TensorFlow, Keras, Chainer, and JAX. XAMT
matched 839 APIs and identified 238 matched API groups, and detected 17 bugs,
12 of which have been confirmed. Our results show that XAMT uncovers bugs
undetectable by intra-framework testing, especially those that manifest
consistently across backends. XAMT offers a complementary approach to existing
methods and offers a new perspective on the testing of deep learning libraries.

</details>


### [493] [Strengthening Programming Comprehension in Large Language Models through Code Generation](https://arxiv.org/abs/2508.12620)
*Xiaoning Ren,Qiang Hu,Wei Ma,Yan Li,Yao Zhang,Lingxiao Jiang,Yinxing Xue*

Main category: cs.SE

TL;DR: Large language models struggle with deep programming reasoning; this paper proposes counterfactual code augmentation and concept-aware tuning to enhance their understanding and performance.


<details>
  <summary>Details</summary>
Motivation: Large language models lack deep reasoning abilities in fundamental programming tasks, limiting their practical adoption in software development.

Method: Introduces a counterfactual code augmentation framework combined with concept-aware tuning to improve LLMs' conceptual grasp.

Result: Demonstrated improved performance and conceptual understanding of LLMs across multiple models and benchmarks.

Conclusion: The proposed approach effectively enhances LLMs for deeper programming reasoning and practical applications.

Abstract: Large language models (LLMs) have recently shown impressive results on
diverse code-related tasks, benefiting from large-scale training and
instruction tuning. However, studies reveal that their grasp of fundamental
programming concepts, such as data flow and control flow, remains shallow,
leading to fragile performance when code requires deeper reasoning. This
limitation restricts the practical adoption of LLMs in real-world software
development. To address this issue, this work introduces a counterfactual code
augmentation framework combined with concept-aware tuning, designed to guide
LLMs toward stronger conceptual understanding. Comprehensive evaluation across
multiple models and benchmarks demonstrates the effectiveness of the proposed
approach.

</details>


### [494] [ChangePrism: Visualizing the Essence of Code Changes](https://arxiv.org/abs/2508.12649)
*Lei Chen,Michele Lanza,Shinpei Hayashi*

Main category: cs.SE

TL;DR: The paper introduces ChangePrism, a tool that visualizes code changes using extraction and visualization components to simplify understanding of modifications in software repositories.


<details>
  <summary>Details</summary>
Motivation: To improve the comprehension of code changes during software evolution, as traditional code diff views are cumbersome and often lack the ability to differentiate significant changes.

Method: The authors developed ChangePrism, which consists of two components: (1) an extraction module to retrieve changes from git history, and (2) a visualization module featuring summary and detailed views of code changes.

Result: ChangePrism enables developers to view summaries of different types of code changes across commits and inspect precise modifications for each commit, enhancing code review processes.

Conclusion: A novel approach to visualize and understand code changes has been proposed, offering better insights into modifications than traditional diff tools.

Abstract: Understanding the changes made by developers when they submit a pull request
and/or perform a commit on a repository is a crucial activity in software
maintenance and evolution. The common way to review changes relies on examining
code diffs, where textual differences between two file versions are highlighted
in red and green to indicate additions and deletions of lines. This can be
cumbersome for developers, making it difficult to obtain a comprehensive
overview of all changes in a commit. Moreover, certain types of code changes
can be particularly significant and may warrant differentiation from standard
modifications to enhance code comprehension. We present a novel visualization
approach supported by a tool named ChangePrism, which provides a way to better
understand code changes. The tool comprises two components: extraction, which
retrieves code changes and relevant information from the git history, and
visualization, which offers both general and detailed views of code changes in
commits. The general view provides an overview of different types of code
changes across commits, while the detailed view displays the exact changes in
the source code for each commit.

</details>


### [495] [RUM: Rule+LLM-Based Comprehensive Assessment on Testing Skills](https://arxiv.org/abs/2508.12922)
*Yue Wang,Zhenyu Chen,Yuan Zhao,Chunrong Fang,Ziyuan Wang,Song Huang*

Main category: cs.SE

TL;DR: The paper introduces RUM, an approach combining rules and LLMs to automate subjective and objective testing skills assessment, improving efficiency, costs, and accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses deficiencies in the META method, which focuses only on objective assessment and lacks tools for evaluating subjective components such as test cases or reports.

Method: RUM integrates rule-based processing with large language models to assess both objective elements and subjective aspects like documents and reports.

Result: RUM achieved an 80.77% improvement in assessment efficiency and a 97.38% cost reduction while maintaining high accuracy and consistency.

Conclusion: RUM enhances assessment capability in software testing education and provides teachers with objective data, supporting personalized education and advancing software quality assurance.

Abstract: Over the past eight years, the META method has served as a multidimensional
testing skill assessment system in the National College Student Contest on
Software Testing, successfully assessing over 100,000 students' testing skills.
However, META is primarily limited to the objective assessment of test scripts,
lacking the ability to automatically assess subjective aspects such as test
case and test report. To address this limitation, this paper proposes RUM, a
comprehensive assessment approach that combines rules and large language models
(LLMs). RUM achieves a comprehensive assessment by rapidly processing objective
indicators through rules while utilizing LLMs for in-depth subjective analysis
of test case documents, test scripts, and test reports. The experimental
results show that compared to traditional manual testing skill assessment, RUM
improves assessment efficiency by 80.77\% and reduces costs by 97.38\%, while
maintaining high accuracy and consistency of assessment. By applying RUM on the
contest on software testing, we find that it not only enhances the efficiency
and scalability of skill assessment in software testing education, but also
provides teachers with more comprehensive and objective evidence for student
ability assessment, facilitating personalized teaching and learning. This study
offers new insights into the assessment of testing skills, which are expected
to promote further development in test process optimization and software
quality assurance.

</details>


### [496] [Investigating VR Accessibility Reviews for Users with Disabilities: A Qualitative Analysis](https://arxiv.org/abs/2508.13051)
*Yi Wang,Chetan Arora,Xiao Liu,Thuong Hoang,ZHengxin Zhang,Henry Been Lirn Duh,John Grundy*

Main category: cs.SE

TL;DR: This study examines VR app user reviews to understand accessibility issues for people with disabilities, finding that VR apps lack support for accessibility despite various challenges reported.


<details>
  <summary>Details</summary>
Motivation: Investigate the accessibility challenges in VR applications faced by users with disabilities due to a lack of comprehensive research.

Method: Analyzes 1,367,419 user reviews from popular and least-rated VR apps on Meta and Steam platforms, filtering for 1,076 accessibility-related reviews encompassing various disabilities.

Result: Identified 16 disability types across six categories and found Action games have the highest accessibility-related reviews, though overall reviews showed insufficient support.

Conclusion: Accessibility support in VR applications is largely lacking, indicating the need for better design considerations for users with disabilities.

Abstract: Accessibility reviews provide valuable insights into both the limitations and
benefits experienced by users with disabilities when using virtual reality (VR)
applications. However, a comprehensive investigation into VR accessibility for
users with disabilities is still lacking. To fill this gap, this study analyzes
user reviews from the Meta and Steam stores of VR apps, focusing on the
reported issues affecting users with disabilities. We applied selection
criteria to 1,367,419 reviews from the top 40, the 20 most popular, and the 40
lowest-rated VR applications on both platforms. In total, 1,076 (0.078%) VR
accessibility reviews referenced various disabilities across 100 VR
applications. These applications were categorized into Action, Sports, Social,
Puzzle, Horror, and Simulation, with Action receiving the highest number of
accessibility related-reviews. We identified 16 different types of disabilities
across six categories. Furthermore, we examined the causes of accessibility
issues as reported by users with disabilities. Overall, VR accessibility
reviews were predominantly under-supported.

</details>


### [497] [Influencia de fatores organizacionais e sociais na etapa de levantamento de requisitos](https://arxiv.org/abs/2508.13134)
*Glauber da Rocha Balthazar,Marcia Ito*

Main category: cs.SE

TL;DR: This paper reviews studies on the non-technical factors during the requirements gathering stage of software projects.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on humanistic relationships and behaviors in requirements gathering processes.

Method: The paper performs a survey of studies that include emotions, organizational environment, and social context during the software requirements gathering stage.

Result: The paper consolidates studies that incorporate non-technical elements into requirements engineering.

Conclusion: Non-technical factors such as emotions and social context are significant considerations to improve the requirements gathering phase in software development.

Abstract: The most critical and fragile stage of a software development project is
requirements gathering. Because of this, Requirements Engineering has been
evolving its techniques to minimize the challenges faced by Requirements
Analysts. However, few studies consider the humanistic relationships and
behaviors of those involved in this stage. This article presents a survey of
some studies conducted at this stage that consider non-technical factors such
as emotions, organizational environment, and social context.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [498] [HetSyn: Versatile Timescale Integration in Spiking Neural Networks via Heterogeneous Synapses](https://arxiv.org/abs/2508.11644)
*Zhichao Deng,Zhikun Liu,Junxue Wang,Shengqian Chen,Xiang Wei,Qiang Yu*

Main category: q-bio.NC

TL;DR: This paper introduces HetSyn, a framework incorporating synaptic heterogeneity for Spiking Neural Networks (SNNs), resulting in improved performance and insights into brain-inspired temporal processing.


<details>
  <summary>Details</summary>
Motivation: Existing SNNs lack a key biological property, synaptic heterogeneity, which is critical for temporal processing and cognitive abilities.

Method: HetSyn introduces synapse-specific time constants, shifting temporal integration to synaptic currents. This is implemented in HetSynLIF, an advanced LIF model with customizable configurations for synapse decay dynamics.

Result: HetSynLIF outperforms standard SNNs in multiple tasks, shows robustness to noise, better working memory, and generalizes across timescales, aligning with biological synapse behavior.

Conclusion: The model demonstrates the importance of synaptic heterogeneity for efficient and robust neural computation, advancing our understanding of biologically realistic SNNs.

Abstract: Spiking Neural Networks (SNNs) offer a biologically plausible and
energy-efficient framework for temporal information processing. However,
existing studies overlook a fundamental property widely observed in biological
neurons-synaptic heterogeneity, which plays a crucial role in temporal
processing and cognitive capabilities. To bridge this gap, we introduce HetSyn,
a generalized framework that models synaptic heterogeneity with
synapse-specific time constants. This design shifts temporal integration from
the membrane potential to the synaptic current, enabling versatile timescale
integration and allowing the model to capture diverse synaptic dynamics. We
implement HetSyn as HetSynLIF, an extended form of the leaky integrate-and-fire
(LIF) model equipped with synapse-specific decay dynamics. By adjusting the
parameter configuration, HetSynLIF can be specialized into vanilla LIF neurons,
neurons with threshold adaptation, and neuron-level heterogeneous models. We
demonstrate that HetSynLIF not only improves the performance of SNNs across a
variety of tasks-including pattern generation, delayed match-to-sample, speech
recognition, and visual recognition-but also exhibits strong robustness to
noise, enhanced working memory performance, efficiency under limited neuron
resources, and generalization across timescales. In addition, analysis of the
learned synaptic time constants reveals trends consistent with empirical
observations in biological synapses. These findings underscore the significance
of synaptic heterogeneity in enabling efficient neural computation, offering
new insights into brain-inspired temporal modeling.

</details>


### [499] [Memory as Structured Trajectories: Persistent Homology and Contextual Sheaves](https://arxiv.org/abs/2508.11646)
*Xin Li*

Main category: q-bio.NC

TL;DR: This paper introduces a topological framework connecting memory and inference to spike-timing dynamics, persistent homology, and the Context-Content Uncertainty Principle (CCUP), highlighting memory as sparse attractor cycles and inference as dynamic alignment.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore how neural spike-timing dynamics can encode reproducible memory structures and dynamic inference processes that go beyond traditional models of static attractors or distributed coding.

Method: By leveraging concepts like spike-timing dynamics, persistent homology, and spatiotemporal complexes, the authors construct topologically irreducible activation cycles and represent these as cell posets to formalize memory and inference using delta-homology analogies.

Result: Memory traces are shown as sparse topologically irreducible attractors localized along reproducible cycles. Inference is characterized as dynamic alignment with these cycles, integrating content with context.

Conclusion: The paper concludes that memory is better described as a cycle-completing, structure-aware inference process rather than static or distributed coding, aligning topological invariants with neural activity.

Abstract: We propose a topological framework for memory and inference grounded in the
structure of spike-timing dynamics, persistent homology, and the
Context-Content Uncertainty Principle (CCUP). Starting from the observation
that polychronous neural groups (PNGs) encode reproducible, time-locked spike
sequences shaped by axonal delays and synaptic plasticity, we construct
spatiotemporal complexes whose temporally consistent transitions define chain
complexes over which robust activation cycles emerge. These activation loops
are abstracted into cell posets, enabling a compact and causally ordered
representation of neural activity with overlapping and compositional memory
traces. We introduce the delta-homology analogy, which formalizes memory as a
set of sparse, topologically irreducible attractors. A Dirac delta-like memory
trace is identified with a nontrivial homology generator on a latent manifold
of cognitive states. Such traces are sharply localized along reproducible
topological cycles and are only activated when inference trajectories complete
a full cycle. They encode minimal, path-dependent memory units that cannot be
synthesized from local features alone. We interpret these delta-homology
generators as the low-entropy content variable, while the high-entropy context
variable is represented dually as a filtration, cohomology class, or sheaf over
the same latent space. Inference is recast as a dynamic alignment between
content and context and coherent memory retrieval corresponds to the existence
of a global section that selects and sustains a topological generator. Memory
is no longer a static attractor or distributed code, but a cycle-completing,
structure-aware inference process.

</details>


### [500] [Revealing Neurocognitive and Behavioral Patterns by Unsupervised Manifold Learning from Dynamic Brain Data](https://arxiv.org/abs/2508.11672)
*Zixia Zhou,Junyan Liu,Wei Emma Wu,Ruogu Fang,Sheng Liu,Qingyue Wei,Rui Yan,Yi Guo,Qian Tao,Yuanyuan Wang,Md Tauhidul Islam,Lei Xing*

Main category: q-bio.NC

TL;DR: The paper presents BCNE, an unsupervised deep manifold learning method to analyze complex dynamic brain data, uncovering meaningful neurocognitive and behavioral patterns.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of extracting meaningful insights from complex and expansive dynamic brain data, enabling better understanding of the brain's inner workings.

Method: The Brain-dynamic Convolutional-Network-based Embedding (BCNE) captures temporospatial correlations in brain data and applies manifold learning to these correlations to uncover brain-state trajectories and behavioral patterns.

Result: BCNE successfully analyzed dynamic brain datasets, revealing interpretable patterns such as scene transitions, memory processing regions, learning stages, and behavior distinctions.

Conclusion: BCNE is a versatile tool for neuroscience research, offering insights into general brain processes and individual-specific patterns through its unique analytical approach.

Abstract: Dynamic brain data, teeming with biological and functional insights, are
becoming increasingly accessible through advanced measurements, providing a
gateway to understanding the inner workings of the brain in living subjects.
However, the vast size and intricate complexity of the data also pose a
daunting challenge in reliably extracting meaningful information across various
data sources. This paper introduces a generalizable unsupervised deep manifold
learning for exploration of neurocognitive and behavioral patterns. Unlike
existing methods that extract patterns directly from the input data as in the
existing methods, the proposed Brain-dynamic Convolutional-Network-based
Embedding (BCNE) seeks to capture the brain-state trajectories by deciphering
the temporospatial correlations within the data and subsequently applying
manifold learning to this correlative representation. The performance of BCNE
is showcased through the analysis of several important dynamic brain datasets.
The results, both visual and quantitative, reveal a diverse array of intriguing
and interpretable patterns. BCNE effectively delineates scene transitions,
underscores the involvement of different brain regions in memory and narrative
processing, distinguishes various stages of dynamic learning processes, and
identifies differences between active and passive behaviors. BCNE provides an
effective tool for exploring general neuroscience inquiries or
individual-specific patterns.

</details>


### [501] [Excitation-inhibition balance in cortical networks with heterogeneous cluster sizes and its applications](https://arxiv.org/abs/2508.12541)
*Abhijit Chakraborty,Greg Morrison*

Main category: q-bio.NC

TL;DR: The paper addresses the issue of information propagation in heterogeneous cortical networks and proposes a solution using a reweighting method for connection strengths based on cluster sizes.


<details>
  <summary>Details</summary>
Motivation: Understanding how information propagates in cortical networks with heterogeneous cluster sizes and addressing the breakdown of balanced states in such setups.

Method: A formal balance matrix was utilized to diagnose the breakdown of balance, followed by a reweighting mechanism based on community/cluster sizes to achieve partial balance and control synchronization.

Result: Restoring partial balance in heterogeneous networks enabled control over spontaneous synchronization in clusters and showed selective stimulus propagation through hierarchically clustered networks.

Conclusion: Reweighting connection strengths in heterogeneous cortical networks provides an effective way to restore balance, control synchronization, and facilitate selective information propagation.

Abstract: Insight into how information can propagate within cortical networks is
essential for a more complete understanding of neural dynamics and computation
in complex networks. Networks with clustered connections have previously been
shown to give rise to correlated dynamics in individual clusters. However, this
same model applied to a network with highly heterogeneous cluster sizes leads
to a clear breakdown of the balanced state. In this article, using a formal
definition of the balance matrix, we show why the balance condition breaks and
propose a solution to restore balance in heterogeneous networks by reweighing
the connection strengths based on community sizes. We introduce a method of
partially balancing a heterogeneous network and show that the degree of
spontaneous synchronization within communities can be varied using a single
parameter describing the reweighing. We further show that stimuli can propagate
through a hierarchically clustered network, where stimulating one cluster of
neurons in a densely connected pair induces correlated firing in the other
without propagating to other weakly connected clusters.

</details>


### [502] [A Unified Cortical Circuit Model with Divisive Normalization and Self-Excitation for Robust Representation and Memory Maintenance](https://arxiv.org/abs/2508.12702)
*Jie Su,Weiwei Wang,Zhaotian Gu,Dahui Wang,Tianyi Qian*

Main category: q-bio.NC

TL;DR: This study introduces a unified recurrent neural circuit model that achieves noise-resistant processing and stable information maintenance, addressing a critical gap in understanding cortical computation.


<details>
  <summary>Details</summary>
Motivation: To integrate the disparate neural mechanisms responsible for noise-resistant processing and information maintenance into a single framework, which is vital for understanding cortical computation.

Method: The authors designed a recurrent neural circuit combining divisive normalization and self-excitation. Through mathematical analysis, they established that the system forms a continuous attractor with input-proportional stabilization and self-sustained memory states.

Result: The model was effective across two tasks: noise-robust encoding in a random-dot kinematogram paradigm and approximate Bayesian inference in a probabilistic card sorting test.

Conclusion: The study presents a unified approach that links key cognitive processes—noise suppression, working memory, and Bayesian inference—providing implications for neuroscience and biologically plausible artificial intelligence design.

Abstract: Robust information representation and its persistent maintenance are
fundamental for higher cognitive functions. Existing models employ distinct
neural mechanisms to separately address noise-resistant processing or
information maintenance, yet a unified framework integrating both operations
remains elusive -- a critical gap in understanding cortical computation. Here,
we introduce a recurrent neural circuit that combines divisive normalization
with self-excitation to achieve both robust encoding and stable retention of
normalized inputs. Mathematical analysis shows that, for suitable parameter
regimes, the system forms a continuous attractor with two key properties: (1)
input-proportional stabilization during stimulus presentation; and (2)
self-sustained memory states persisting after stimulus offset. We demonstrate
the model's versatility in two canonical tasks: (a) noise-robust encoding in a
random-dot kinematogram (RDK) paradigm; and (b) approximate Bayesian belief
updating in a probabilistic Wisconsin Card Sorting Test (pWCST). This work
establishes a unified mathematical framework that bridges noise suppression,
working memory, and approximate Bayesian inference within a single cortical
microcircuit, offering fresh insights into the brain's canonical computation
and guiding the design of biologically plausible artificial neural
architectures.

</details>


### [503] [Synchronization and semantization in deep spiking networks](https://arxiv.org/abs/2508.12975)
*Jonas Oberste-Frielinghaus,Anno C. Kurth,Julian Göltz,Laura Kriener,Junji Ito,Mihai A. Petrovici,Sonja Grün*

Main category: q-bio.NC

TL;DR: This paper studies the learning dynamics and activity patterns of multi-layer spiking networks to understand their connection to observed cortical computation.


<details>
  <summary>Details</summary>
Motivation: To clarify how learned structures and dynamics in spiking networks relate to experimentally observed neural dynamics and may explain cortical computation.

Method: They trained a multi-layer spiking network as a conceptual analog of the visual hierarchy for visual input classification using spike-time encoding.

Result: The network learned distinct spatio-temporal activity patterns, characterized by initial spread of activity followed by re-convergence, synchronicity, and the formation of distinct pathways.

Conclusion: The findings support the hypothesis that synchronicity in the visual system emerges as a natural consequence of deep learning in hierarchical cortical networks.

Abstract: Recent studies have shown how spiking networks can learn complex
functionality through error-correcting plasticity, but the resulting structures
and dynamics remain poorly studied. To elucidate how these models may link to
observed dynamics in vivo and thus how they may ultimately explain cortical
computation, we need a better understanding of their emerging patterns. We
train a multi-layer spiking network, as a conceptual analog of the bottom-up
visual hierarchy, for visual input classification using spike-time encoding.
After learning, we observe the development of distinct spatio-temporal activity
patterns. While input patterns are synchronous by construction, activity in
early layers first spreads out over time, followed by re-convergence into sharp
pulses as classes are gradually extracted. The emergence of synchronicity is
accompanied by the formation of increasingly distinct pathways, reflecting the
gradual semantization of input activity. We thus observe hierarchical networks
learning spike latency codes to naturally acquire activity patterns
characterized by synchronicity and separability, with pronounced excitatory
pathways ascending through the layers. This provides a rigorous computational
hypothesis for the experimentally observed synchronicity in the visual system
as a natural consequence of deep learning in cortex.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [504] [BaMANI: Bayesian Multi-Algorithm causal Network Inference](https://arxiv.org/abs/2508.11741)
*Habibolla Latifizadeh,Anika C. Pirkey,Alanna Gould,David J. Klinke II*

Main category: stat.ML

TL;DR: This paper introduces BaMANI, a software based on ensemble learning to improve Bayesian causal network inference by mitigating the influence of individual algorithms.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicted Bayesian causal networks reflecting the imprint of specific computational algorithms rather than purely the generative process.

Method: Developed an ensemble learning approach and implemented it as a software tool, BaMANI, which integrates predictions across multiple algorithms for robust causal network inference.

Result: The study demonstrates BaMANI's application in human breast cancer studies, showing its efficacy in marginalizing algorithm-specific impacts on causal network predictions.

Conclusion: Ensemble learning via BaMANI improves the reliability of Bayesian causal inferences by reducing algorithm-specific biases, thereby enhancing its applicability in real-world use cases like cancer research.

Abstract: Improved computational power has enabled different disciplines to predict
causal relationships among modeled variables using Bayesian network inference.
While many alternative algorithms have been proposed to improve the efficiency
and reliability of network prediction, the predicted causal networks reflect
the generative process but also bear an opaque imprint of the specific
computational algorithm used. Following a ``wisdom of the crowds" strategy, we
developed an ensemble learning approach to marginalize the impact of a single
algorithm on Bayesian causal network inference. To introduce the approach, we
first present the theoretical foundation of this framework. Next, we present a
comprehensive implementation of the framework in terms of a new software tool
called BaMANI (Bayesian Multi-Algorithm causal Network Inference). Finally, we
describe a BaMANI use-case from biology, particularly within human breast
cancer studies.

</details>


### [505] [Dropping Just a Handful of Preferences Can Change Top Large Language Model Rankings](https://arxiv.org/abs/2508.11847)
*Jenny Y. Huang,Yunyi Shen,Dennis Wei,Tamara Broderick*

Main category: stat.ML

TL;DR: The paper proposes a method to evaluate the robustness of the Bradley-Terry ranking system when a very small fraction of evaluation data is removed, finding top model rankings highly sensitive to such changes.


<details>
  <summary>Details</summary>
Motivation: Ranking systems, such as the widely used Bradley-Terry model, play a critical role in evaluating large language models (LLMs). However, their robustness to minor changes in evaluation data, especially worst-case scenarios, is not well understood.

Method: The authors introduce a fast and easy-to-adopt approach to study the sensitivity of Bradley-Terry rankings by systematically removing small fractions of evaluation data. They apply this method to data from Chatbot Arena and MT-Bench platforms.

Result: They find that rankings are highly sensitive to small data removals, with changes happening even when as little as 0.02% of evaluations are dropped. Rankings derived from MT-Bench are more robust than Chatbot Arena, likely due to its use of expert annotators.

Conclusion: The study reveals the fragility of ranking systems based on small perturbations, highlighting the need for more robust evaluation frameworks, particularly for crowdsourced human evaluations.

Abstract: We propose a method for evaluating the robustness of a widely used LLM
ranking system -- the Bradley--Terry ranking system -- to dropping a worst-case
very small fraction of evaluation data. Our approach is computationally fast
and easy to adopt. When we apply our method to matchups from two popular
human-preference platforms, Chatbot Arena and MT-Bench, we find that the
Bradley--Terry rankings of top-performing models are remarkably sensitive to
the removal of a small fraction of evaluations. Our framework also identifies
the specific evaluations most responsible for such ranking flips, allowing for
inspections of these influential preferences. We observe that the rankings
derived from MT-Bench preferences are notably more robust than those from
Chatbot Arena, likely due to MT-bench's use of expert annotators and carefully
constructed prompts. Finally, we find that rankings based on crowdsourced
human-evaluated systems are just as sensitive as those based on LLM-as-a-judge
evaluations, where in both, dropping as little as 0.02% of the total
evaluations in the dataset can change the top-ranked model.

</details>


### [506] [Robust Data Fusion via Subsampling](https://arxiv.org/abs/2508.12048)
*Jing Wang,HaiYing Wang,Kun Chen*

Main category: stat.ML

TL;DR: The paper explores transfer learning with subsampling, specifically addressing issues with data contamination due to outliers. Two strategies for subsampling external data are proposed, aiming to enhance estimator performance under realistic constraints.


<details>
  <summary>Details</summary>
Motivation: To improve model performance for limited target data by effectively leveraging large contaminated external datasets, addressing gaps in understanding transfer learning and subsampling under data contamination.

Method: Investigated two subsampling strategies to handle contaminated external data: one focusing on reducing biases and the other on minimizing variances. Combined strategies were also proposed. Theoretical non-asymptotic error bounds were derived, considering multiple factors including outliers and error distribution.

Result: The proposed subsampling strategies and hybrid approaches demonstrated superior performance in simulations. Additionally, the methods successfully analyzed hard landing risks in A380 airplanes, leveraging data from other airplane types.

Conclusion: Robust subsampling in transfer learning can efficiently utilize contaminated external data to improve estimation for target tasks, even with constraints like limited target data and outliers in external datasets.

Abstract: Data fusion and transfer learning are rapidly growing fields that enhance
model performance for a target population by leveraging other related data
sources or tasks. The challenges lie in the various potential heterogeneities
between the target and external data, as well as various practical concerns
that prevent a na\"ive data integration. We consider a realistic scenario where
the target data is limited in size while the external data is large but
contaminated with outliers; such data contamination, along with other
computational and operational constraints, necessitates proper selection or
subsampling of the external data for transfer learning. To our
knowledge,transfer learning and subsampling under data contamination have not
been thoroughly investigated. We address this gap by studying various transfer
learning methods with subsamples of the external data, accounting for outliers
deviating from the underlying true model due to arbitrary mean shifts. Two
subsampling strategies are investigated: one aimed at reducing biases and the
other at minimizing variances. Approaches to combine these strategies are also
introduced to enhance the performance of the estimators. We provide
non-asymptotic error bounds for the transfer learning estimators, clarifying
the roles of sample sizes, signal strength, sampling rates, magnitude of
outliers, and tail behaviors of model error distributions, among other factors.
Extensive simulations show the superior performance of the proposed methods.
Additionally, we apply our methods to analyze the risk of hard landings in A380
airplanes by utilizing data from other airplane types,demonstrating that robust
transfer learning can improve estimation efficiency for relatively rare
airplane types with the help of data from other types of airplanes.

</details>


### [507] [An Introduction to Sliced Optimal Transport](https://arxiv.org/abs/2508.12519)
*Khai Nguyen*

Main category: stat.ML

TL;DR: The paper reviews Sliced Optimal Transport (SOT), emphasizing its efficiency and scalability in computing distances, barycenters, and kernels for probability measures while covering its mathematical foundations, methodological advancements, and diverse applications.


<details>
  <summary>Details</summary>
Motivation: Classical Optimal Transport methods face computational challenges, particularly in high-dimensional settings. SOT leverages the simplicity of one-dimensional OT to address these challenges and broaden application scope.

Method: The study synthesizes mathematical tools like integral geometry (e.g., Radon transform) and computational statistics with developments such as improved Monte Carlo approximations, weighted slicing, and nonlinear projections.

Result: SOT has proven effective in solving various computational problems, enabling fast computation while maintaining geometric richness. Its extended frameworks cover unbalanced OT, multi-marginal problems, and Gromov-Wasserstein estimation.

Conclusion: SOT's advancements make it a versatile tool applicable in machine learning, statistics, and computer vision, offering reliable and scalable alternatives. The field's ongoing innovations are critical for modern computational challenges.

Abstract: Sliced Optimal Transport (SOT) is a rapidly developing branch of optimal
transport (OT) that exploits the tractability of one-dimensional OT problems.
By combining tools from OT, integral geometry, and computational statistics,
SOT enables fast and scalable computation of distances, barycenters, and
kernels for probability measures, while retaining rich geometric structure.
This paper provides a comprehensive review of SOT, covering its mathematical
foundations, methodological advances, computational methods, and applications.
We discuss key concepts of OT and one-dimensional OT, the role of tools from
integral geometry such as Radon transform in projecting measures, and
statistical techniques for estimating sliced distances. The paper further
explores recent methodological advances, including non-linear projections,
improved Monte Carlo approximations, statistical estimation techniques for
one-dimensional optimal transport, weighted slicing techniques, and
transportation plan estimation methods. Variational problems, such as minimum
sliced Wasserstein estimation, barycenters, gradient flows, kernel
constructions, and embeddings are examined alongside extensions to unbalanced,
partial, multi-marginal, and Gromov-Wasserstein settings. Applications span
machine learning, statistics, computer graphics and computer visions,
highlighting SOT's versatility as a practical computational tool. This work
will be of interest to researchers and practitioners in machine learning, data
sciences, and computational disciplines seeking efficient alternatives to
classical OT.

</details>


### [508] [On computing and the complexity of computing higher-order $U$-statistics, exactly](https://arxiv.org/abs/2508.12627)
*Xingyu Chen,Ruiqi Zhang,Lin Liu*

Main category: stat.ML

TL;DR: This paper focuses on the computational challenges of higher-order $U$-statistics, presenting efficient methods and a Python package for practical implementation.


<details>
  <summary>Details</summary>
Motivation: To address the computational complexity and inefficiency associated with higher-order $U$-statistics, which are widely used in statistics, machine learning, and computer science.

Method: The paper introduces a decomposition of $U$-statistics into more computationally feasible $V$-statistics, explores the role of Einstein summation in computation, and ties the problem's complexity to treewidth. A new algorithm is proposed and implemented in the Python package `u-stats`.

Result: The proposed algorithm significantly reduces the runtime for calculating higher-order $U$-statistics and is demonstrated to outperform existing benchmarks in statistical applications.

Conclusion: The study makes strides in both theoretical and practical domains of $U$-statistics, inspiring further research while providing an efficient, practitioner-friendly implementation.

Abstract: Higher-order $U$-statistics abound in fields such as statistics, machine
learning, and computer science, but are known to be highly time-consuming to
compute in practice. Despite their widespread appearance, a comprehensive study
of their computational complexity is surprisingly lacking. This paper aims to
fill that gap by presenting several results related to the computational aspect
of $U$-statistics. First, we derive a useful decomposition from an $m$-th order
$U$-statistic to a linear combination of $V$-statistics with orders not
exceeding $m$, which are generally more feasible to compute. Second, we explore
the connection between exactly computing $V$-statistics and Einstein summation,
a tool often used in computational mathematics, quantum computing, and quantum
information sciences for accelerating tensor computations. Third, we provide an
optimistic estimate of the time complexity for exactly computing
$U$-statistics, based on the treewidth of a particular graph associated with
the $U$-statistic kernel. The above ingredients lead to a new, much more
runtime-efficient algorithm of exactly computing general higher-order
$U$-statistics. We also wrap our new algorithm into an open-source Python
package called $\texttt{u-stats}$. We demonstrate via three statistical
applications that $\texttt{u-stats}$ achieves impressive runtime performance
compared to existing benchmarks. This paper aspires to achieve two goals: (1)
to capture the interest of researchers in both statistics and other related
areas further to advance the algorithmic development of $U$-statistics, and (2)
to offer the package $\texttt{u-stats}$ as a valuable tool for practitioners,
making the implementation of methods based on higher-order $U$-statistics a
more delightful experience.

</details>


### [509] [Unfolded Laplacian Spectral Embedding: A Theoretically Grounded Approach to Dynamic Network Representation](https://arxiv.org/abs/2508.12674)
*Haruka Ezoe,Hiroki Matsumoto,Ryohei Hisano*

Main category: stat.ML

TL;DR: The authors introduce the Unfolded Laplacian Spectral Embedding (ULSE), a method to ensure stability properties for representing dynamic network structures, supported by theoretical proofs and empirical evidence.


<details>
  <summary>Details</summary>
Motivation: To address challenges in representing dynamic relational structures consistently and interpretably, while ensuring stability over time and across dimensions.

Method: The proposed method, ULSE, extends the Unfolded Adjacency Spectral Embedding framework to normalized Laplacians, proving it satisfies stability conditions. It also introduces a Cheeger-style inequality linked to graph conductance.

Result: Theoretical proofs and empirical results on synthetic and real-world datasets confirm stability properties and strong performance of the method in dynamic network representation.

Conclusion: The research establishes ULSE as a stable, theoretically grounded framework for representing dynamic networks using spectral graph theory, with practical and theoretical significance.

Abstract: Dynamic relational structures play a central role in many AI tasks, but their
evolving nature presents challenges for consistent and interpretable
representation. A common approach is to learn time-varying node embeddings,
whose effectiveness depends on satisfying key stability properties. In this
paper, we propose Unfolded Laplacian Spectral Embedding, a new method that
extends the Unfolded Adjacency Spectral Embedding framework to normalized
Laplacians while preserving both cross-sectional and longitudinal stability. We
provide formal proof that our method satisfies these stability conditions. In
addition, as a bonus of using the Laplacian matrix, we establish a new
Cheeger-style inequality that connects the embeddings to the conductance of the
underlying dynamic graphs. Empirical evaluations on synthetic and real-world
datasets support our theoretical findings and demonstrate the strong
performance of our method. These results establish a principled and stable
framework for dynamic network representation grounded in spectral graph theory.

</details>


### [510] [Optimal Condition for Initialization Variance in Deep Neural Networks: An SGD Dynamics Perspective](https://arxiv.org/abs/2508.12834)
*Hiroshi Horii,Sothea Has*

Main category: stat.ML

TL;DR: This paper examines the relationship between weight initialization and optimization dynamics in deep neural networks (DNNs) through stochastic gradient descent (SGD). It proposes explicit mathematical criteria for optimal weight initialization and confirms these criteria experimentally.


<details>
  <summary>Details</summary>
Motivation: Optimization in DNNs relies heavily on heuristic approaches for initialization, often leading to inconsistent performance. This paper aims to provide a mathematically grounded method to optimize weight initialization based on SGD dynamics.

Method: The authors derived a connection between the initialization distribution and the expected loss function using the quasi-stationary distribution from the Fokker-Planck equation. They formulated explicit mathematical bounds for optimization and tested their theoretical framework experimentally on MNIST and Fashion-MNIST datasets.

Result: The theoretical optimal initialization condition leads to DNNs with lower final training loss and higher test accuracy when compared to conventional He-normal initialization.

Conclusion: The study provides a theoretical criterion for optimal weight initialization in DNNs, grounded in optimization dynamics. This offers a more reliable alternative to heuristic initialization methods.

Abstract: Stochastic gradient descent (SGD), one of the most fundamental optimization
algorithms in machine learning (ML), can be recast through a continuous-time
approximation as a Fokker-Planck equation for Langevin dynamics, a viewpoint
that has motivated many theoretical studies. Within this framework, we study
the relationship between the quasi-stationary distribution derived from this
equation and the initial distribution through the Kullback-Leibler (KL)
divergence. As the quasi-steady-state distribution depends on the expected cost
function, the KL divergence eventually reveals the connection between the
expected cost function and the initialization distribution. By applying this to
deep neural network models (DNNs), we can express the bounds of the expected
loss function explicitly in terms of the initialization parameters. Then, by
minimizing this bound, we obtain an optimal condition of the initialization
variance in the Gaussian case. This result provides a concrete mathematical
criterion, rather than a heuristic approach, to select the scale of weight
initialization in DNNs. In addition, we experimentally confirm our theoretical
results by using the classical SGD to train fully connected neural networks on
the MNIST and Fashion-MNIST datasets. The result shows that if the variance of
the initialization distribution satisfies our theoretical optimal condition,
then the corresponding DNN model always achieves lower final training loss and
higher test accuracy than the conventional He-normal initialization. Our work
thus supplies a mathematically grounded indicator that guides the choice of
initialization variance and clarifies its physical meaning of the dynamics of
parameters in DNNs.

</details>


### [511] [The path to a goal: Understanding soccer possessions via path signatures](https://arxiv.org/abs/2508.12930)
*David Hirnschall,Robert Bajons*

Main category: stat.ML

TL;DR: The paper introduces a novel framework using path signatures for predicting next actions in soccer, outperforming transformer models, and introduces a new possession evaluation metric.


<details>
  <summary>Details</summary>
Motivation: To improve predictive modeling of soccer actions by addressing limitations of fixed historical windows and handcrafted features.

Method: Uses path signatures to encode spatio-temporal structures of soccer events, avoiding irrelevant historical data and reducing computational costs.

Result: Achieves better performance than transformer-based benchmarks and introduces a reliable possession evaluation metric for soccer analytics.

Conclusion: The approach enriches soccer analytics by offering a mathematically grounded prediction framework and a domain-relevant evaluation metric, showcasing potential for broader applications and future research.

Abstract: We present a novel framework for predicting next actions in soccer
possessions by leveraging path signatures to encode their complex
spatio-temporal structure. Unlike existing approaches, we do not rely on fixed
historical windows and handcrafted features, but rather encode the entire
recent possession, thereby avoiding the inclusion of potentially irrelevant or
misleading historical information. Path signatures naturally capture the order
and interaction of events, providing a mathematically grounded feature encoding
for variable-length time series of irregular sampling frequencies without the
necessity for manual feature engineering. Our proposed approach outperforms a
transformer-based benchmark across various loss metrics and considerably
reduces computational cost. Building on these results, we introduce a new
possession evaluation metric based on well-established frameworks in soccer
analytics, incorporating both predicted action type probabilities and action
location. Our metric shows greater reliability than existing metrics in
domain-specific comparisons. Finally, we validate our approach through a
detailed analysis of the 2017/18 Premier League season and discuss further
applications and future extensions.

</details>


### [512] [Simulation-Based Inference: A Practical Guide](https://arxiv.org/abs/2508.12939)
*Michael Deistler,Jan Boelts,Peter Steinbach,Guy Moss,Thomas Moreau,Manuel Gloeckler,Pedro L. C. Rodrigues,Julia Linhart,Janne K. Lappalainen,Benjamin Kurt Miller,Pedro J. Gonçalves,Jan-Matthis Lueckmann,Cornelius Schröder,Jakob H. Macke*

Main category: stat.ML

TL;DR: The paper provides a practical guide to Simulation-based Inference (SBI), a framework for efficient Bayesian parameter inference using neural networks and simulators without requiring likelihood evaluations.


<details>
  <summary>Details</summary>
Motivation: To address the computational difficulties of Bayesian inference when dealing with stochastic simulators, which hinder real-world applicability.

Method: SBI uses neural networks trained on simulator-generated data to amortize inference, thus enabling rapid parameter analysis without additional training or simulations. Guidelines and tools for each stage of the SBI process are provided.

Result: This tutorial demonstrates the utility of SBI through examples in astrophysics, psychophysics, and neuroscience, guiding efficient parameter inference workflows.

Conclusion: This paper empowers researchers to adopt and apply SBI methods for efficient parameter estimation and scientific discovery across various domains.

Abstract: A central challenge in many areas of science and engineering is to identify
model parameters that are consistent with prior knowledge and empirical data.
Bayesian inference offers a principled framework for this task, but can be
computationally prohibitive when models are defined by stochastic simulators.
Simulation-based Inference (SBI) is a suite of methods developed to overcome
this limitation, which has enabled scientific discoveries in fields such as
particle physics, astrophysics, and neuroscience. The core idea of SBI is to
train neural networks on data generated by a simulator, without requiring
access to likelihood evaluations. Once trained, inference is amortized: The
neural network can rapidly perform Bayesian inference on empirical observations
without requiring additional training or simulations. In this tutorial, we
provide a practical guide for practitioners aiming to apply SBI methods. We
outline a structured SBI workflow and offer practical guidelines and diagnostic
tools for every stage of the process -- from setting up the simulator and
prior, choosing and training inference networks, to performing inference and
validating the results. We illustrate these steps through examples from
astrophysics, psychophysics, and neuroscience. This tutorial empowers
researchers to apply state-of-the-art SBI methods, facilitating efficient
parameter inference for scientific discovery.

</details>


### [513] [Shapley Values: Paired-Sampling Approximations](https://arxiv.org/abs/2508.12947)
*Michael Mayer,Mario V. Wüthrich*

Main category: stat.ML

TL;DR: This paper discusses Shapley values for machine learning explanations, proposes asymptotic normality for sampling approximations, and explores paired-sampling methods’ properties.


<details>
  <summary>Details</summary>
Motivation: Shapley values are increasingly used to fairly attribute credit for input features in machine learning predictions, but their computational limits hinder wider application.

Method: The authors investigate asymptotic normality for sampling KernelSHAP and PermutationSHAP and focus on paired-sampling approaches for scenarios involving interactions of maximal order two.

Result: They found that paired-sampling PermutationSHAP achieves exact recovery for interactions of order two and has an additive recovery property, unlike KernelSHAP.

Conclusion: The study advances our understanding of Shapley value computation by exploring novel approximations and highlighting benefits of paired-sampling methods.

Abstract: Originally introduced in cooperative game theory, Shapley values have become
a very popular tool to explain machine learning predictions. Based on Shapley's
fairness axioms, every input (feature component) gets a credit how it
contributes to an output (prediction). These credits are then used to explain
the prediction. The only limitation in computing the Shapley values (credits)
for many different predictions is of computational nature. There are two
popular sampling approximations, sampling KernelSHAP and sampling
PermutationSHAP. Our first novel contributions are asymptotic normality results
for these sampling approximations. Next, we show that the paired-sampling
approaches provide exact results in case of interactions being of maximal order
two. Furthermore, the paired-sampling PermutationSHAP possesses the additive
recovery property, whereas its kernel counterpart does not.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [514] [Transfer Learning for Neutrino Scattering: Domain Adaptation with GANs](https://arxiv.org/abs/2508.12987)
*Jose L. Bonilla,Krzysztof M. Graczyk,Artur M. Ankowski,Rwik Dharmapal Banerjee,Beata E. Kowal,Hemant Prasad,Jan T. Sobczyk*

Main category: hep-ph

TL;DR: The paper explores using transfer learning with GANs to generate neutrino scattering data, showing its effectiveness even with limited training data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating neutrino scattering event data with limited experimental datasets.

Method: The study leverages transfer learning to adapt a GAN model trained on synthetic neutrino-carbon data to generate events for other interactions (e.g., neutrino-argon, antineutrino-carbon) and evaluates its performance against models trained from scratch.

Result: Transfer learning was shown to outperform models trained from scratch, maintaining robust performance even with smaller datasets.

Conclusion: Transfer learning is a promising method for developing neutrino scattering event generators in data-scarce scenarios.

Abstract: We utilize transfer learning to extrapolate the physics knowledge encoded in
a Generative Adversarial Network (GAN) model trained on synthetic
charged-current (CC) neutrino-carbon inclusive scattering data. This base model
is adapted to generate CC inclusive scattering events (lepton kinematics only)
for neutrino-argon and antineutrino-carbon interactions. Furthermore, we assess
the effectiveness of transfer learning in re-optimizing a custom model when new
data comes from a different neutrino-nucleus interaction model. Our results
demonstrate that transfer learning significantly outperforms training
generative models from scratch. To study this, we consider two training data
sets: one with 10,000 and another with 100,000 events. The models obtained via
transfer learning perform well even with smaller training data. The proposed
method provides a promising approach for constructing neutrino scattering event
generators in scenarios where experimental data is sparse.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [515] [AegisBlock: A Privacy-Preserving Medical Research Framework using Blockchain](https://arxiv.org/abs/2508.11797)
*Calkin Garg,Omar Rios Cruz,Tessa Andersen,Gaby G. Dagher,Donald Winiecki,Min Long*

Main category: cs.CR

TL;DR: AegisBlock is a framework to securely and anonymously share medical records with researchers while maintaining data trustworthiness.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns due to regulations like HIPAA when sharing patient health records for research purposes.

Method: AegisBlock uses blockchain technology with miners verifying access and patient approval for time-based range queries from researchers.

Result: Experimental evaluation shows AegisBlock is scalable and efficient even in the presence of up to 50% malicious miners.

Conclusion: AegisBlock effectively ensures patient privacy, trustworthiness of research data, and scalability across hospitals and patient populations.

Abstract: Due to HIPAA and other privacy regulations, it is imperative to maintain
patient privacy while conducting research on patient health records. In this
paper, we propose AegisBlock, a patient-centric access controlled framework to
share medical records with researchers such that the anonymity of the patient
is maintained while ensuring the trustworthiness of the data provided to
researchers. AegisBlock allows for patients to provide access to their medical
data, verified by miners. A researcher submits a time-based range query to
request access to records from a certain patient, and upon patient approval,
access will be granted. Our experimental evaluation results show that
AegisBlock is scalable with respect to the number of patients and hospitals in
the system, and efficient with up to 50% of malicious miners.

</details>


### [516] [Attack Graph Generation on HPC Clusters](https://arxiv.org/abs/2508.12161)
*Ming Li,John Hale*

Main category: cs.CR

TL;DR: The paper addresses the time and memory challenges of generating attack graphs (AGs) and explores high-performance computing (HPC) clusters as a solution for efficient AG generation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve computer network security by enabling efficient generation of attack graphs, which are essential for analyzing vulnerabilities and preventing multi-step attacks but are resource-intensive to produce.

Method: The method involves leveraging high-performance computing (HPC) clusters for generating attack graphs, followed by performance evaluation of the proposed approach through experiments.

Result: The results demonstrate how cluster environments alleviate the slow speed and high memory demands of attack graph generation in a balanced manner.

Conclusion: High-performance computing clusters are shown to be effective in resolving the challenges of time and memory usage in attack graph generation, offering a scalable solution for enhancing network security.

Abstract: Attack graphs (AGs) are graphical tools to analyze the security of computer
networks. By connecting the exploitation of individual vulnerabilities, AGs
expose possible multi-step attacks against target networks, allowing system
administrators to take preventive measures to enhance their network's security.
As powerful analytical tools, however, AGs are both time- and memory-consuming
to be generated. As the numbers of network assets, interconnections between
devices, as well as vulnerabilities increase, the size and volume of the
resulting AGs grow at a much higher rate, leading to the well-known state-space
explosion. In this paper, we propose the use of high performance computing
(HPC) clusters to implement AG generators. We evaluate the performance through
experiments and provide insights into how cluster environments can help resolve
the issues of slow speed and high memory demands in AG generation in a balanced
way.

</details>


### [517] [Data-driven Trust Bootstrapping for Mobile Edge Computing-based Industrial IoT Services](https://arxiv.org/abs/2508.12560)
*Prabath Abeysekara,Hai Dong*

Main category: cs.CR

TL;DR: The paper introduces a method to bootstrap the trustworthiness of Internet of Things (IoT) services in Mobile Edge Computing (MEC)-based industrial IoT systems.


<details>
  <summary>Details</summary>
Motivation: Current trust bootstrapping methods for MEC-based IIoT systems face challenges like limited interaction periods with services, sparse peer recommendations, and inconsistent context parameters across environments.

Method: The proposed approach incorporates knowledge sharing among MEC environments and addresses data sparsity, leveraging real-world datasets adjusted for context-dependent trust information.

Result: Experimental evaluations on two datasets showcased the approach's effectiveness in evaluating and validating service trust in MEC-based IIoT systems.

Conclusion: The approach is suitable for enhancing the trustworthiness of IoT services in MEC-based IIoT systems and addresses key contextual challenges.

Abstract: We propose a data-driven and context-aware approach to bootstrap
trustworthiness of homogeneous Internet of Things (IoT) services in Mobile Edge
Computing (MEC) based industrial IoT (IIoT) systems. The proposed approach
addresses key limitations in adapting existing trust bootstrapping approaches
into MEC-based IIoT systems. These key limitations include, the lack of
opportunity for a service consumer to interact with a lesser-known service over
a prolonged period of time to get a robust measure of its trustworthiness,
inability of service consumers to consistently interact with their peers to
receive reliable recommendations of the trustworthiness of a lesser-known
service as well as the impact of uneven context parameters in different MEC
environments causing uneven trust environments for trust evaluation. In
addition, the proposed approach also tackles the problem of data sparsity via
enabling knowledge sharing among different MEC environments within a given MEC
topology. To verify the effectiveness of the proposed approach, we carried out
a comprehensive evaluation on two real-world datasets suitably adjusted to
exhibit the context-dependent trust information accumulated in MEC environments
within a given MEC topology. The experimental results affirmed the
effectiveness of our approach and its suitability to bootstrap trustworthiness
of services in MEC-based IIoT systems.

</details>


### [518] [Cyber Risks to Next-Gen Brain-Computer Interfaces: Analysis and Recommendations](https://arxiv.org/abs/2508.12571)
*Tyler Schroder,Renee Sirbu,Sohee Park,Jessica Morley,Sam Street,Luciano Floridi*

Main category: cs.CR

TL;DR: BCIs have great potential but pose cybersecurity risks; recommendations are made for device security and safety.


<details>
  <summary>Details</summary>
Motivation: To address the cybersecurity risks posed by BCIs and safeguard patient safety and data confidentiality.

Method: Analyzed cybersecurity threats through a hypothetical average-case threat model and proposed technical recommendations.

Result: Outlined specific security measures for BCI devices and predicted risks via various cybersecurity threat categories.

Conclusion: BCIs need robust security measures, regulatory guidance, and reduced network vulnerabilities to ensure user safety and confidentiality.

Abstract: Brain-computer interfaces (BCIs) show enormous potential for advancing
personalized medicine. However, BCIs also introduce new avenues for
cyber-attacks or security compromises. In this article, we analyze the problem
and make recommendations for device manufacturers to better secure devices and
to help regulators understand where more guidance is needed to protect patient
safety and data confidentiality. Device manufacturers should implement the
prior suggestions in their BCI products. These recommendations help protect BCI
users from undue risks, including compromised personal health and genetic
information, unintended BCI-mediated movement, and many other cybersecurity
breaches. Regulators should mandate non-surgical device update methods, strong
authentication and authorization schemes for BCI software modifications,
encryption of data moving to and from the brain, and minimize network
connectivity where possible. We also design a hypothetical, average-case threat
model that identifies possible cybersecurity threats to BCI patients and
predicts the likeliness of risk for each category of threat. BCIs are at less
risk of physical compromise or attack, but are vulnerable to remote attack; we
focus on possible threats via network paths to BCIs and suggest technical
controls to limit network connections.

</details>


### [519] [SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip](https://arxiv.org/abs/2508.12910)
*Ziteng Hu,Yingjie Xia,Xiyuan Chen,Li Kuang*

Main category: cs.CR

TL;DR: SecFSM introduces a security-guided method for using Large Language Models (LLMs) to generate more secure Verilog code for Finite State Machines (FSMs).


<details>
  <summary>Details</summary>
Motivation: FSM implementations often contain security vulnerabilities in LLM-generated Verilog code. Therefore, addressing security concerns in automated Verilog code generation for FSMs is critical for security-sensitive applications.

Method: The approach involves creating a FSM Security Knowledge Graph (FSKG), analyzing user requirements for vulnerabilities, retrieving security-related knowledge, and constructing guided prompts for code generation by LLMs.

Result: SecFSM shows superior performance in security tests, achieving a pass rate of 21 out of 25 cases in evaluations using DeepSeek-R1.

Conclusion: SecFSM effectively enhances the security of Verilog FSM implementations through its security-oriented framework, outperforming existing methods in generating secure code.

Abstract: Finite State Machines (FSMs) play a critical role in implementing control
logic for Systems-on-Chip (SoC). Traditionally, FSMs are implemented by
hardware engineers through Verilog coding, which is often tedious and
time-consuming. Recently, with the remarkable progress of Large Language Models
(LLMs) in code generation, LLMs have been increasingly explored for automating
Verilog code generation. However, LLM-generated Verilog code often suffers from
security vulnerabilities, which is particularly concerning for
security-sensitive FSM implementations. To address this issue, we propose
SecFSM, a novel method that leverages a security-oriented knowledge graph to
guide LLMs in generating more secure Verilog code. Specifically, we first
construct a FSM Security Knowledge Graph (FSKG) as an external aid to LLMs.
Subsequently, we analyze users' requirements to identify vulnerabilities and
get a list of vulnerabilities in the requirements. Then, we retrieve knowledge
from FSKG based on the vulnerabilities list. Finally, we construct security
prompts based on the security knowledge for Verilog code generation. To
evaluate SecFSM, we build a dedicated dataset collected from academic datasets,
artificial datasets, papers, and industrial cases. Extensive experiments
demonstrate that SecFSM outperforms state-of-the-art baselines. In particular,
on a benchmark of 25 security test cases evaluated by DeepSeek-R1, SecFSM
achieves an outstanding pass rate of 21/25.

</details>


### [520] [AUTOVR: Automated UI Exploration for Detecting Sensitive Data Flow Exposures in Virtual Reality Apps](https://arxiv.org/abs/2508.12187)
*John Y. Kim,Chaoshun Zuo,Yanjie Zhao,Zhiqiang Lin*

Main category: cs.CR

TL;DR: This paper introduces AUTOVR, an automated framework for testing and interacting with user interfaces and events in VR apps, demonstrating superior performance compared to conventional tools.


<details>
  <summary>Details</summary>
Motivation: The lack of robust tools for user interface exploration and event testing in VR apps despite the growing popularity of VR platforms like Meta Quest.

Method: AUTOVR analyzes the app's binaries, reveals hidden events, resolves event dependencies, and automates dynamic interaction with VR app interfaces.

Result: AUTOVR outperformed Android Monkey in revealing sensitive data exposures in VR apps, enhancing privacy and performance.

Conclusion: AUTOVR provides an effective solution for testing and improving VR apps, showcasing its potential to address privacy challenges in the VR landscape.

Abstract: The rise of Virtual Reality (VR) has provided developers with an
unprecedented platform for creating games and applications (apps) that require
distinct inputs, different from those of conventional devices like smartphones.
The Meta Quest VR platform, driven by Meta, has democratized VR app publishing
and attracted millions of users worldwide. However, as the number of published
apps grows, there is a notable lack of robust headless tools for user interface
(UI) exploration and user event testing. To address this need, we present
AUTOVR, an automatic framework for dynamic UI and user event interaction in VR
apps built on the Unity Engine. Unlike conventional Android and GUI testers,
AUTOVR analyzes the app's internal binary to reveal hidden events, resolves
generative event dependencies, and utilizes them for comprehensive exploration
of VR apps. Using sensitive data exposure as a performance metric, we compare
AUTOVR with Android Monkey, a widely used headless Android GUI stress testing
tool. Our empirical evaluation demonstrates AUTOVR's superior performance,
triggering an order of magnitude of more sensitive data exposures and
significantly enhancing the privacy of VR apps.

</details>


### [521] [Systematic Analysis of MCP Security](https://arxiv.org/abs/2508.12538)
*Yongjian Guo,Puzhuo Liu,Wanlun Ma,Zehang Deng,Xiaogang Zhu,Peng Di,Xi Xiao,Sheng Wen*

Main category: cs.CR

TL;DR: This paper identifies security vulnerabilities in the Model Context Protocol (MCP) and introduces MCPLIB, a framework categorizing and analyzing 31 distinct attack methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive research on the security vulnerabilities of MCP, focusing on real-world threat diversities.

Method: The paper categorizes 31 tool poisoning attacks into four classifications and conducts experiments to quantitatively evaluate their effectiveness.

Result: Experiments reveal vulnerabilities such as agents' blind reliance on tool descriptions, sensitivity to file-based and chain attacks, and inability to distinguish external data from commands.

Conclusion: The findings emphasize the need for robust defense mechanisms and an informed design for MCP systems, supporting secure MCP evolution.

Abstract: The Model Context Protocol (MCP) has emerged as a universal standard that
enables AI agents to seamlessly connect with external tools, significantly
enhancing their functionality. However, while MCP brings notable benefits, it
also introduces significant vulnerabilities, such as Tool Poisoning Attacks
(TPA), where hidden malicious instructions exploit the sycophancy of large
language models (LLMs) to manipulate agent behavior. Despite these risks,
current academic research on MCP security remains limited, with most studies
focusing on narrow or qualitative analyses that fail to capture the diversity
of real-world threats. To address this gap, we present the MCP Attack Library
(MCPLIB), which categorizes and implements 31 distinct attack methods under
four key classifications: direct tool injection, indirect tool injection,
malicious user attacks, and LLM inherent attack. We further conduct a
quantitative analysis of the efficacy of each attack. Our experiments reveal
key insights into MCP vulnerabilities, including agents' blind reliance on tool
descriptions, sensitivity to file-based attacks, chain attacks exploiting
shared context, and difficulty distinguishing external data from executable
commands. These insights, validated through attack experiments, underscore the
urgency for robust defense strategies and informed MCP design. Our
contributions include 1) constructing a comprehensive MCP attack taxonomy, 2)
introducing a unified attack framework MCPLIB, and 3) conducting empirical
vulnerability analysis to enhance MCP security mechanisms. This work provides a
foundational framework, supporting the secure evolution of MCP ecosystems.

</details>


### [522] [Adjustable AprilTags For Identity Secured Tasks](https://arxiv.org/abs/2508.12304)
*Hao Li*

Main category: cs.CR

TL;DR: The paper suggests using adjustable AprilTags to enhance identity security against adversarial attacks in open, public environments.


<details>
  <summary>Details</summary>
Motivation: Traditional AprilTags are fixed and susceptible to adversarial attacks in open and public settings, raising security concerns.

Method: Proposes the transition from fixed AprilTags to adjustable AprilTags for enhancing robustness against potential adversarial threats.

Result: Adjustable AprilTags are expected to improve identity security and reduce vulnerabilities in less regulated environments.

Conclusion: Adjustable AprilTags provide a viable solution to manage security risks in public settings, addressing flaws in fixed AprilTags.

Abstract: Special tags such as AprilTags that facilitate image processing and pattern
recognition are useful in practical applications. In close and private
environments, identity security is unlikely to be an issue because all involved
AprilTags can be completely regulated. However, in open and public
environments, identity security is no longer an issue that can be neglected. To
handle potential harm caused by adversarial attacks, this note advocates
utilization of adjustable AprilTags instead of fixed ones.

</details>


### [523] [Code Vulnerability Detection Across Different Programming Languages with AI Models](https://arxiv.org/abs/2508.11710)
*Hael Abdulhakim Ali Humran,Ferdi Sonmez*

Main category: cs.CR

TL;DR: The paper explores AI's role, particularly transformer-based models like CodeBERT and CodeLlama, in identifying code vulnerabilities across various programming languages, outperforming traditional static analysis with over 97% accuracy after fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Detecting vulnerabilities in source code written across multiple languages is complex and traditional static analysis tools struggle with context-dependent bugs, prompting exploration of AI-based solutions.

Method: The study uses transformer models, CodeBERT and CodeLlama, on code vulnerability datasets with steps including dataset gathering, language normalization, model fine-tuning, ensemble learning, and explainable AI integration.

Result: Experiments show CodeBERT achieves accuracy exceeding 97%, outperforming static analyzers, though it struggles with precision, which can be enhanced using hybrid models and improved validation.

Conclusion: AI-based models, trained dynamically, offer promising results for detecting code vulnerabilities across languages, but challenges like robustness and interpretability require further development for practical deployment.

Abstract: Security vulnerabilities present in a code that has been written in diverse
programming languages are among the most critical yet complicated aspects of
source code to detect. Static analysis tools based on rule-based patterns
usually do not work well at detecting the context-dependent bugs and lead to
high false positive rates. Recent developments in artificial intelligence,
specifically the use of transformer-based models like CodeBERT and CodeLlama,
provide light to this problem, as they show potential in finding such flaws
better. This paper presents the implementations of these models on various
datasets of code vulnerability, showing how off-the-shelf models can
successfully produce predictive capacity in models through dynamic fine-tuning
of the models on vulnerable and safe code fragments. The methodology comprises
the gathering of the dataset, normalization of the language, fine-tuning of the
model, and incorporation of ensemble learning and explainable AI. Experiments
show that a well-trained CodeBERT can be as good as or even better than some
existing static analyzers in terms of accuracy greater than 97%. Further study
has indicated that although language models can achieve close-to-perfect
recall, the precision can decrease. A solution to this is given by hybrid
models and validation procedures, which will reduce false positives. According
to the results, the AI-based solutions generalize to different programming
languages and classes of vulnerability. Nevertheless, robustness,
interpretability, and deployment readiness are still being developed. The
results illustrate the probabilities that AI will enhance the trustworthiness
in the usability and scalability of machine-learning-based detectors of
vulnerabilities.

</details>


### [524] [Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks](https://arxiv.org/abs/2508.11711)
*Irash Perera,Hiranya Abeyrathne,Sanjeewa Malalgoda,Arshardh Ifthikar*

Main category: cs.CR

TL;DR: The paper identifies security vulnerabilities in GraphQL APIs and proposes an AI-driven real-time detection system using advanced machine learning techniques and optimized implementation strategies.


<details>
  <summary>Details</summary>
Motivation: GraphQL's flexibility poses security risks, such as denial-of-service attacks and data exfiltration, which are inadequately addressed by traditional API security mechanisms.

Method: The approach integrates machine learning techniques like LLMs for schema configuration, contextual query embeddings via SBERT and Doc2Vec, and classifiers using CNNs, Random Forests, and Multilayer Perceptrons to detect malicious GraphQL queries.

Result: The system achieves high accuracy in identifying security threats like SQL injection, OS command injection, XSS exploits, and effectively mitigates DoS and SSRF attacks under load.

Conclusion: This paper introduces a scalable and efficient AI-powered framework enhancing GraphQL security through dynamic threat detection mechanisms, contributing to safer API environments.

Abstract: GraphQL's flexibility, while beneficial for efficient data fetching,
introduces unique security vulnerabilities that traditional API security
mechanisms often fail to address. Malicious GraphQL queries can exploit the
language's dynamic nature, leading to denial-of-service attacks, data
exfiltration through injection, and other exploits. Existing solutions, such as
static analysis, rate limiting, and general-purpose Web Application Firewalls,
offer limited protection against sophisticated, context-aware attacks. This
paper presents a novel, AI-driven approach for real-time detection of malicious
GraphQL queries. Our method combines static analysis with machine learning
techniques, including Large Language Models (LLMs) for dynamic schema-based
configuration, Sentence Transformers (SBERT and Doc2Vec) for contextual
embedding of query payloads, and Convolutional Neural Networks (CNNs), Random
Forests, and Multilayer Perceptrons for classification. We detail the system
architecture, implementation strategies optimized for production environments
(including ONNX Runtime optimization and parallel processing), and evaluate the
performance of our detection models and the overall system under load. Results
demonstrate high accuracy in detecting various threats, including SQL
injection, OS command injection, and XSS exploits, alongside effective
mitigation of DoS and SSRF attempts. This research contributes a robust and
adaptable solution for enhancing GraphQL API security.

</details>


### [525] [Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Detection Methods (FakeIDet2)](https://arxiv.org/abs/2508.11716)
*Javier Muñoz-Haro,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CR

TL;DR: The research focuses on combating AI-generated fake IDs by proposing a privacy-aware approach to fake ID detection, introducing a large dataset (FakeIDet2-db) with diverse samples, and setting a benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: The increase in AI-generated fake IDs, which bypass traditional security measures, is driving a need for robust detection methods while preserving privacy due to the sensitive nature of ID data.

Method: A patch-based privacy-preserving approach is used to detect fake IDs. The paper introduces FakeIDet2 for detection and FakeIDet2-db, a public dataset of ID patches with different physical attack scenarios.

Result: The researchers created a robust database, FakeIDet2-db, with over 900K patches from real and fake IDs, and developed the FakeIDet2 method that successfully distinguishes between authentic and fake IDs under various conditions.

Conclusion: The study highlights the feasibility of privacy-aware fake ID detection using a diverse dataset and benchmark. It contributes significantly to the field by addressing the data scarcity issue while maintaining user privacy.

Abstract: Remote user verification in Internet-based applications is becoming
increasingly important nowadays. A popular scenario for it consists of
submitting a picture of the user's Identity Document (ID) to a service
platform, authenticating its veracity, and then granting access to the
requested digital service. An ID is well-suited to verify the identity of an
individual, since it is government issued, unique, and nontransferable.
However, with recent advances in Artificial Intelligence (AI), attackers can
surpass security measures in IDs and create very realistic physical and
synthetic fake IDs. Researchers are now trying to develop methods to detect an
ever-growing number of these AI-based fakes that are almost indistinguishable
from authentic (bona fide) IDs. In this counterattack effort, researchers are
faced with an important challenge: the difficulty in using real data to train
fake ID detectors. This real data scarcity for research and development is
originated by the sensitive nature of these documents, which are usually kept
private by the ID owners (the users) and the ID Holders (e.g., government,
police, bank, etc.). The main contributions of our study are: 1) We propose and
discuss a patch-based methodology to preserve privacy in fake ID detection
research. 2) We provide a new public database, FakeIDet2-db, comprising over
900K real/fake ID patches extracted from 2,000 ID images, acquired using
different smartphone sensors, illumination and height conditions, etc. In
addition, three physical attacks are considered: print, screen, and composite.
3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We
release a standard reproducible benchmark that considers physical and synthetic
attacks from popular databases in the literature.

</details>


### [526] [Optimizing Token Choice for Code Watermarking: A RL Approach](https://arxiv.org/abs/2508.11925)
*Zhimeng Guo,Huaisheng Zhu,Siyuan Xu,Hangfan Zhang,Teng Xiao,Minhao Cheng*

Main category: cs.CR

TL;DR: This paper introduces CodeTracer, a novel adaptive framework for watermarking code generated by LLMs using reinforcement learning to improve detectability while maintaining code functionality.


<details>
  <summary>Details</summary>
Motivation: The rise of LLM-generated code requires a reliable method to detect such code while ensuring its functionality remains intact, addressing the need for adaptive watermarking systems.

Method: CodeTracer uses a policy-based reinforcement learning model, incorporating execution feedback and watermark embedding signals, combined with Gumbel Top-k reparameterization for gradient optimization.

Result: CodeTracer shows superior performance in watermark detectability and maintains the functionality of the generated code compared to existing methods.

Conclusion: CodeTracer offers an effective and innovative solution for watermarking LLM-generated code, ensuring both functionality and detectability, advancing the state of the art in code watermarking.

Abstract: The need for detecting LLM-generated code necessitates watermarking systems
capable of operating within its highly structured and syntactically constrained
environment. To address this, we introduce CodeTracer, an innovative adaptive
code watermarking framework underpinned by a novel reinforcement learning
training paradigm. At its core, CodeTracer features a policy-driven approach
that utilizes a parameterized model to intelligently bias token choices during
next-token prediction. This strategy ensures that embedded watermarks maintain
code functionality while exhibiting subtle yet statistically detectable
deviations from typical token distributions. To facilitate policy learning, we
devise a comprehensive reward system that seamlessly integrates execution
feedback with watermark embedding signals, balancing process-level and
outcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization
to enable gradient-based optimization of discrete watermarking decisions.
Extensive comparative evaluations demonstrate CodeTracer's significant
superiority over state-of-the-art baselines in both watermark detectability and
the preservation of generated code's functionality.

</details>


### [527] [Mitigating Jailbreaks with Intent-Aware LLMs](https://arxiv.org/abs/2508.12072)
*Wei Jie Yeo,Ranjan Satapathy,Erik Cambria*

Main category: cs.CR

TL;DR: Intent-FT is a fine-tuning method proposed to improve the robustness of large language models against jailbreak attacks by teaching them to infer the intent behind adversarial instructions.


<details>
  <summary>Details</summary>
Motivation: The persistent vulnerability of large language models to adversarial jailbreak attacks showcases the trade-off between safety and performance, necessitating a method to enhance model security without compromising utility.

Method: Intent-FT fine-tunes LLMs by explicitly teaching them to deduce the intent behind instructions. This process utilizes a targeted dataset containing adversarial instructions, enabling models to generalize intent deduction to unseen attacks.

Result: Intent-FT significantly mitigates the success rate of evaluated attack categories (none surpassing a 50% success rate). It preserves the utility of the model while reducing refusals to benign instructions and improving defense against white-box threats.

Conclusion: Intent-FT offers an effective and lightweight defense mechanism for LLMs, improving robustness to jailbreak attacks without negatively impacting their general utility or task performance.

Abstract: Despite extensive safety-tuning, large language models (LLMs) remain
vulnerable to jailbreak attacks via adversarially crafted instructions,
reflecting a persistent trade-off between safety and task performance. In this
work, we propose Intent-FT, a simple and lightweight fine-tuning approach that
explicitly trains LLMs to infer the underlying intent of an instruction before
responding. By fine-tuning on a targeted set of adversarial instructions,
Intent-FT enables LLMs to generalize intent deduction to unseen attacks,
thereby substantially improving their robustness. We comprehensively evaluate
both parametric and non-parametric attacks across open-source and proprietary
models, considering harmfulness from attacks, utility, over-refusal, and impact
against white-box threats. Empirically, Intent-FT consistently mitigates all
evaluated attack categories, with no single attack exceeding a 50\% success
rate -- whereas existing defenses remain only partially effective. Importantly,
our method preserves the model's general capabilities and reduces excessive
refusals on benign instructions containing superficially harmful keywords.
Furthermore, models trained with Intent-FT accurately identify hidden harmful
intent in adversarial attacks, and these learned intentions can be effectively
transferred to enhance vanilla model defenses.

</details>


### [528] [Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position](https://arxiv.org/abs/2508.12398)
*Zhixin Xie,Xurui Song,Jun Luo*

Main category: cs.CR

TL;DR: This paper analyzes the safety performance of diffusion-based large language models (dLLMs) and introduces a method called Middle-tOken Safety Alignment (MOSA) to enhance their safety.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of safety studies on diffusion large language models (dLLMs) and seeks to explore and mitigate safety vulnerabilities unique to their architecture.

Method: The authors identify an asymmetry where middle tokens in dLLM-generated responses are more critical for safety and attackers have limited capability in manipulating them. They propose MOSA, a reinforcement learning-based method that aligns middle tokens to safe refusals.

Result: The MOSA method demonstrates superior safety performance compared to eight attack methods on two benchmarks and maintains its utility in coding, math, and general reasoning tasks.

Conclusion: The study highlights the importance of middle-token alignment for dLLMs and establishes MOSA as an effective method to improve their safety without compromising functionality.

Abstract: Diffusion Large Language Models (dLLMs) have recently emerged as a
competitive non-autoregressive paradigm due to their unique training and
inference approach. However, there is currently a lack of safety study on this
novel architecture. In this paper, we present the first analysis of dLLMs'
safety performance and propose a novel safety alignment method tailored to
their unique generation characteristics. Specifically, we identify a critical
asymmetry between the defender and attacker in terms of security. For the
defender, we reveal that the middle tokens of the response, rather than the
initial ones, are more critical to the overall safety of dLLM outputs; this
seems to suggest that aligning middle tokens can be more beneficial to the
defender. The attacker, on the contrary, may have limited power to manipulate
middle tokens, as we find dLLMs have a strong tendency towards a sequential
generation order in practice, forcing the attack to meet this distribution and
diverting it from influencing the critical middle tokens. Building on this
asymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method
that directly aligns the model's middle generation with safe refusals
exploiting reinforcement learning. We implement MOSA and compare its security
performance against eight attack methods on two benchmarks. We also test the
utility of MOSA-aligned dLLM on coding, math, and general reasoning. The
results strongly prove the superiority of MOSA.

</details>


### [529] [Deciphering the Interplay between Attack and Protection Complexity in Privacy-Preserving Federated Learning](https://arxiv.org/abs/2508.11907)
*Xiaojin Zhang,Mingcong Xu,Yiming Li,Wei Chen,Qiang Yang*

Main category: cs.CR

TL;DR: The paper introduces a theoretical framework analyzing the trade-offs between attack and protection complexities in privacy-preserving federated learning.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of federated learning to gradient inversion attacks while ensuring robust privacy protection mechanisms.

Method: The authors leverage Maximum Bayesian Privacy to mathematically define attack and protection complexities and derive theoretical bounds for their interplay.

Result: The study reveals how attack complexity depends on privacy leakage, gradient distortion, and other parameters, while protection complexity scales with model dimensionality and privacy budgets.

Conclusion: Insights from the paper can guide the design of secure and efficient federated learning systems.

Abstract: Federated learning (FL) offers a promising paradigm for collaborative model
training while preserving data privacy. However, its susceptibility to gradient
inversion attacks poses a significant challenge, necessitating robust privacy
protection mechanisms. This paper introduces a novel theoretical framework to
decipher the intricate interplay between attack and protection complexities in
privacy-preserving FL. We formally define "Attack Complexity" as the minimum
computational and data resources an adversary requires to reconstruct private
data below a given error threshold, and "Protection Complexity" as the expected
distortion introduced by privacy mechanisms. Leveraging Maximum Bayesian
Privacy (MBP), we derive tight theoretical bounds for protection complexity,
demonstrating its scaling with model dimensionality and privacy budget.
Furthermore, we establish comprehensive bounds for attack complexity, revealing
its dependence on privacy leakage, gradient distortion, model dimension, and
the chosen privacy level. Our findings quantitatively illuminate the
fundamental trade-offs between privacy guarantees, system utility, and the
effort required for both attacking and defending. This framework provides
critical insights for designing more secure and efficient federated learning
systems.

</details>


### [530] [Substituting Proof of Work in Blockchain with Training-Verified Collaborative Model Computation](https://arxiv.org/abs/2508.12138)
*Mohammad Ishzaz Asif Rafid,Morsalin Sakib*

Main category: cs.CR

TL;DR: This paper proposes replacing Bitcoin's energy-heavy Proof of Work (PoW) with a cloud-based system where miners train machine learning models, evaluated for contributions and rewarded with the ability to append new blocks.


<details>
  <summary>Details</summary>
Motivation: Bitcoin's PoW mechanism is criticized for high energy use and inefficiencies, sparking the need for a sustainable alternative.

Method: The paper suggests using a centralized collaborative ML training framework where miners train models and are evaluated on parameters trained and model loss reduction. Winners are chosen through a weighted lottery system for block appending.

Result: The proposed system redirects computational resources from energy-intensive PoW to socially beneficial ML tasks, while maintaining blockchain integrity via SHA-256 hashing and digital signatures.

Conclusion: Replacing PoW with a productive ML-based framework aligns blockchain mining with sustainability and computational advancement, addressing key criticism of Bitcoin mining.

Abstract: Bitcoin's Proof of Work (PoW) mechanism, while central to achieving
decentralized consensus, has long been criticized for excessive energy use and
hardware inefficiencies \cite{devries2018bitcoin, truby2018decarbonizing}. This
paper introduces a hybrid architecture that replaces Bitcoin's traditional PoW
with a centralized, cloud-based collaborative training framework. In this
model, miners contribute computing resources to train segments of horizontally
scaled machine learning models on preprocessed datasets, ensuring privacy and
generating meaningful outputs \cite{li2017securing}. A central server evaluates
contributions using two metrics: number of parameters trained and reduction in
model loss during each cycle. At the end of every cycle, a weighted lottery
selects the winning miner, who receives a digitally signed certificate. This
certificate serves as a verifiable substitute for PoW and grants the right to
append a block to the blockchain \cite{nakamoto2008bitcoin}. By integrating
digital signatures and SHA-256 hashing \cite{nist2015sha}, the system preserves
blockchain integrity while redirecting energy toward productive computation.
The proposed approach addresses the sustainability concerns of traditional
mining by converting resource expenditure into socially valuable work, aligning
security incentives with real-world computational progress.

</details>


### [531] [Fortifying the Agentic Web: A Unified Zero-Trust Architecture Against Logic-layer Threats](https://arxiv.org/abs/2508.12259)
*Ken Huang,Yasir Mehmood,Hammad Atta,Jerry Huang,Muhammad Zeeshan Baig,Sree Bhargavi Balija*

Main category: cs.CR

TL;DR: The paper proposes a Unified Security Architecture leveraging Zero-Trust IAM, DIDs, and VCs to fortify the Agentic Web, ensuring provable security against LPCI attacks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address vulnerabilities in the Agentic Web by introducing a security framework that counters LPCI threats effectively through advanced architectural innovations.

Method: The proposed methodology includes foundational structures like DIDs and VCs, an Agent Name Service (ANS) for discovery, and features such as TARE, Causal Chain Auditing, and Behavioral Attestation. These elements culminate in a formal security model with provable guarantees.

Result: The formal analysis verifies that the architecture significantly reduces the likelihood of LPCI attacks, offering measurable security improvements.

Conclusion: The research delivers a forward-thinking architecture to secure the Agentic Web, enhancing resilience and trustworthiness with proven efficacy against specific attacks.

Abstract: This paper presents a Unified Security Architecture that fortifies the
Agentic Web through a Zero-Trust IAM framework. This architecture is built on a
foundation of rich, verifiable agent identities using Decentralized Identifiers
(DIDs) and Verifiable Credentials (VCs), with discovery managed by a
protocol-agnostic Agent Name Service (ANS). Security is operationalized through
a multi-layered Trust Fabric which introduces significant innovations,
including Trust-Adaptive Runtime Environments (TARE), Causal Chain Auditing,
and Dynamic Identity with Behavioral Attestation. By explicitly linking the
LPCI threat to these enhanced architectural countermeasures within a formal
security model, we propose a comprehensive and forward-looking blueprint for a
secure, resilient, and trustworthy agentic ecosystem. Our formal analysis
demonstrates that the proposed architecture provides provable security
guarantees against LPCI attacks with bounded probability of success.

</details>


### [532] [LumiMAS: A Comprehensive Framework for Real-Time Monitoring and Enhanced Observability in Multi-Agent Systems](https://arxiv.org/abs/2508.12412)
*Ron Solomon,Yarin Yerushalmi Levi,Lior Vaknin,Eran Aizikovich,Amit Baras,Etai Ohana,Amit Giloni,Shamik Bose,Chiara Picardi,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: LumiMAS is a novel framework to improve observability in multi-agent systems by monitoring, detecting, and explaining system failures.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems using large language models introduce challenges in monitoring, interpreting, and detecting system failures, especially at the MAS-wide level, which is often overlooked by existing frameworks.

Method: LumiMAS incorporates three components: a monitoring and logging layer to track MAS activity, an anomaly detection layer to identify system-wide issues in real time, and an anomaly explanation layer for failure classification and root cause analysis.

Result: The framework was tested on seven MAS applications (including scenarios such as hallucination or bias), demonstrating its effectiveness in failure detection, classification, and root cause analysis across diverse environments.

Conclusion: LumiMAS successfully addresses the limitations of existing MAS observability by providing a comprehensive tool for detecting and explaining system-wide anomalies in multi-agent systems.

Abstract: The incorporation of large language models in multi-agent systems (MASs) has
the potential to significantly improve our ability to autonomously solve
complex problems. However, such systems introduce unique challenges in
monitoring, interpreting, and detecting system failures. Most existing MAS
observability frameworks focus on analyzing each individual agent separately,
overlooking failures associated with the entire MAS. To bridge this gap, we
propose LumiMAS, a novel MAS observability framework that incorporates advanced
analytics and monitoring techniques. The proposed framework consists of three
key components: a monitoring and logging layer, anomaly detection layer, and
anomaly explanation layer. LumiMAS's first layer monitors MAS executions,
creating detailed logs of the agents' activity. These logs serve as input to
the anomaly detection layer, which detects anomalies across the MAS workflow in
real time. Then, the anomaly explanation layer performs classification and root
cause analysis (RCA) of the detected anomalies. LumiMAS was evaluated on seven
different MAS applications, implemented using two popular MAS platforms, and a
diverse set of possible failures. The applications include two novel
failure-tailored applications that illustrate the effects of a hallucination or
bias on the MAS. The evaluation results demonstrate LumiMAS's effectiveness in
failure detection, classification, and RCA.

</details>


### [533] [A Robust Cross-Domain IDS using BiGRU-LSTM-Attention for Medical and Industrial IoT Security](https://arxiv.org/abs/2508.12470)
*Afrah Gueriani,Hamza Kheddar,Ahmed Cherif Mazari,Mohamed Chahine Ghanem*

Main category: cs.CR

TL;DR: The paper proposes BiGAT-ID, a novel transformer-based IDS combining BiGRU, LSTM, and MHA, achieving high detection accuracy and runtime efficiency for IoMT and IIoT environments.


<details>
  <summary>Details</summary>
Motivation: The growing interconnectivity in IoMT and IIoT environments introduces cybersecurity challenges such as data breaches, safety risks, and system vulnerabilities to cyber threats.

Method: The BiGAT-ID model integrates BiGRU, LSTM, and MHA to capture temporal dependencies, model patterns, and enhance feature representation. Experiments were conducted on benchmark datasets (CICIoMT2024 and EdgeIIoTset) to evaluate performance.

Result: BiGAT-ID achieved 99.13% accuracy for CICIoMT2024 and 99.34% for EdgeIIoTset with efficient inference times (0.0002s and 0.0001s per instance, respectively) and a low false positive rate.

Conclusion: BiGAT-ID is a robust and efficient intrusion detection solution applicable to heterogeneous IoT environments, addressing key cybersecurity challenges.

Abstract: The increased Internet of Medical Things IoMT and the Industrial Internet of
Things IIoT interconnectivity has introduced complex cybersecurity challenges,
exposing sensitive data, patient safety, and industrial operations to advanced
cyber threats. To mitigate these risks, this paper introduces a novel
transformer-based intrusion detection system IDS, termed BiGAT-ID a hybrid
model that combines bidirectional gated recurrent units BiGRU, long short-term
memory LSTM networks, and multi-head attention MHA. The proposed architecture
is designed to effectively capture bidirectional temporal dependencies, model
sequential patterns, and enhance contextual feature representation. Extensive
experiments on two benchmark datasets, CICIoMT2024 medical IoT and EdgeIIoTset
industrial IoT demonstrate the model's cross-domain robustness, achieving
detection accuracies of 99.13 percent and 99.34 percent, respectively.
Additionally, the model exhibits exceptional runtime efficiency, with inference
times as low as 0.0002 seconds per instance in IoMT and 0.0001 seconds in IIoT
scenarios. Coupled with a low false positive rate, BiGAT-ID proves to be a
reliable and efficient IDS for deployment in real-world heterogeneous IoT
environments

</details>


### [534] [Unlearning Comparator: A Visual Analytics System for Comparative Evaluation of Machine Unlearning Methods](https://arxiv.org/abs/2508.12730)
*Jaeung Lee,Suhyeon Yu,Yurim Jang,Simon S. Woo,Jaemin Jo*

Main category: cs.CR

TL;DR: The paper introduces 'Unlearning Comparator,' a visual analytics system to evaluate Machine Unlearning (MU) methods in terms of accuracy, efficiency, and privacy.


<details>
  <summary>Details</summary>
Motivation: Researchers in Machine Unlearning face difficulties in assessing trade-offs between accuracy, efficiency, and privacy due to reliance on aggregate metrics and unsystematic evaluations.

Method: The proposed system, Unlearning Comparator, facilitates model comparison at various levels and simulates membership inference attacks to assess MU methods' privacy.

Result: The system assists in understanding model behaviors and identifying strengths and weaknesses of MU methods via structured visual analysis.

Conclusion: Unlearning Comparator is a valuable tool for comprehensively evaluating and improving Machine Unlearning methods, bridging gaps in current analytical practices.

Abstract: Machine Unlearning (MU) aims to remove target training data from a trained
model so that the removed data no longer influences the model's behavior,
fulfilling "right to be forgotten" obligations under data privacy laws. Yet, we
observe that researchers in this rapidly emerging field face challenges in
analyzing and understanding the behavior of different MU methods, especially in
terms of three fundamental principles in MU: accuracy, efficiency, and privacy.
Consequently, they often rely on aggregate metrics and ad-hoc evaluations,
making it difficult to accurately assess the trade-offs between methods. To
fill this gap, we introduce a visual analytics system, Unlearning Comparator,
designed to facilitate the systematic evaluation of MU methods. Our system
supports two important tasks in the evaluation process: model comparison and
attack simulation. First, it allows the user to compare the behaviors of two
models, such as a model generated by a certain method and a retrained baseline,
at class-, instance-, and layer-levels to better understand the changes made
after unlearning. Second, our system simulates membership inference attacks
(MIAs) to evaluate the privacy of a method, where an attacker attempts to
determine whether specific data samples were part of the original training set.
We evaluate our system through a case study visually analyzing prominent MU
methods and demonstrate that it helps the user not only understand model
behaviors but also gain insights that can inform the improvement of MU methods.

</details>


### [535] [Efficient and Verifiable Privacy-Preserving Convolutional Computation for CNN Inference with Untrusted Clouds](https://arxiv.org/abs/2508.12832)
*Jinyu Lu,Xinrong Sun,Yunting Tao,Tong Ji,Fanyu Kong,Guoqiang Yang*

Main category: cs.CR

TL;DR: This paper proposes a verifiable privacy-preserving method for CNNs used in MLaaS systems, addressing computation efficiency and data confidentiality challenges.


<details>
  <summary>Details</summary>
Motivation: Protect sensitive information when using CNNs in MLaaS systems while addressing inefficiencies in existing privacy-preserving methods.

Method: Introduces a novel scheme for secure CNN convolutions with efficient encryption, decryption, and result-verification mechanism for resource-constrained clients.

Result: Experiments with 10 datasets and multiple CNN models show speedups of 26x to 87x compared to plaintext models, with accuracy maintained.

Conclusion: The proposed scheme effectively balances privacy, efficiency, and accuracy, making it suitable for secure CNN applications in resource-limited environments.

Abstract: The widespread adoption of convolutional neural networks (CNNs) in
resource-constrained scenarios has driven the development of Machine Learning
as a Service (MLaaS) system. However, this approach is susceptible to privacy
leakage, as the data sent from the client to the untrusted cloud server often
contains sensitive information. Existing CNN privacy-preserving schemes, while
effective in ensuring data confidentiality through homomorphic encryption and
secret sharing, face efficiency bottlenecks, particularly in convolution
operations. In this paper, we propose a novel verifiable privacy-preserving
scheme tailored for CNN convolutional layers. Our scheme enables efficient
encryption and decryption, allowing resource-constrained clients to securely
offload computations to the untrusted cloud server. Additionally, we present a
verification mechanism capable of detecting the correctness of the results with
a success probability of at least $1-\frac{1}{\left|Z\right|}$. Extensive
experiments conducted on 10 datasets and various CNN models demonstrate that
our scheme achieves speedups ranging $26 \times$ ~ $\ 87\times$ compared to the
original plaintext model while maintaining accuracy.

</details>


### [536] [VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog](https://arxiv.org/abs/2508.13092)
*Xiang Long,Yingjie Xia,Xiyuan Chen,Li Kuang*

Main category: cs.CR

TL;DR: This paper introduces VerilogLAVD, a novel approach for hardware vulnerability detection in Verilog code, employing graph traversal rules supported by large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Hardware vulnerabilities are expensive to fix in later stages of design, but existing detection methods at early design stages require specialized expertise, hindering usability. This paper aims to improve vulnerability detection by using an enhanced method leveraging LLMs.

Method: The proposed VerilogLAVD combines a unified representation of Verilog code called Verilog Property Graph (VeriPG), which integrates syntactic and semantic features, with LLMs to generate detection rules from Common Weakness Enumeration (CWE) descriptions.

Result: VerilogLAVD demonstrated better performance compared to baseline LLM approaches, achieving an F1-score of 0.54 on a dataset with 77 Verilog designs spanning 12 CWE types.

Conclusion: VerilogLAVD improves Verilog vulnerability detection using structured graph traversal rules generated by LLMs, making early-stage detection more effective and consistent.

Abstract: Timely detection of hardware vulnerabilities during the early design stage is
critical for reducing remediation costs. Existing early detection techniques
often require specialized security expertise, limiting their usability. Recent
efforts have explored the use of large language models (LLMs) for Verilog
vulnerability detection. However, LLMs struggle to capture the structure in
Verilog code, resulting in inconsistent detection results. To this end, we
propose VerilogLAVD, the first LLM-aided graph traversal rule generation
approach for Verilog vulnerability detection. Our approach introduces the
Verilog Property Graph (VeriPG), a unified representation of Verilog code. It
combines syntactic features extracted from the abstract syntax tree (AST) with
semantic information derived from control flow and data dependency graphs. We
leverage LLMs to generate VeriPG-based detection rules from Common Weakness
Enumeration (CWE) descriptions. These rules guide the rule executor that
traversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, we
build a dataset collected from open-source repositories and synthesized data.
In our empirical evaluation on 77 Verilog designs encompassing 12 CWE types,
VerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM with
external knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27,
respectively.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [537] [Scaling Robust Optimization for Swarms: A Distributed Perspective](https://arxiv.org/abs/2508.11799)
*Arshiya Taj Abdul,Augustinos D. Saravanos,Evangelos A. Theodorou*

Main category: math.OC

TL;DR: The paper presents a decentralized robust optimization framework for safe multi-agent control under deterministic and stochastic uncertainties, introducing efficient techniques to address complexity and scalability challenges.


<details>
  <summary>Details</summary>
Motivation: Modeling and managing uncertainties (deterministic or stochastic) in multi-agent systems to ensure safety and performance in complex environments where traditional stochastic models fall short.

Method: Developing robust optimization reformulations to reduce complexity, incorporating robust chance constraints, and using distribution steering with a distributed ADMM-based approach while ensuring convergence despite non-convexity.

Result: Proposed framework achieves robustness and scalability, validated through simulations with up to 246 agents in environments with nonconvex obstacles.

Conclusion: The framework efficiently addresses safety under uncertainty for multi-agent systems, combining robustness and scalability, positioning it as a significant improvement over standard approaches.

Abstract: This article introduces a decentralized robust optimization framework for
safe multi-agent control under uncertainty. Although stochastic noise has been
the primary form of modeling uncertainty in such systems, these formulations
might fall short in addressing uncertainties that are deterministic in nature
or simply lack probabilistic data. To ensure safety under such scenarios, we
employ the concept of robust constraints that must hold for all possible
uncertainty realizations lying inside a bounded set. Nevertheless, standard
robust optimization approaches become intractable due to the large number or
non-convexity of the constraints involved in safe multi-agent control. To
address this, we introduce novel robust reformulations that significantly
reduce complexity without compromising safety. The applicability of the
framework is further broadened to address both deterministic and stochastic
uncertainties by incorporating robust chance constraints and distribution
steering techniques. To achieve scalability, we derive a distributed approach
based on the Alternating Direction Method of Multipliers (ADMM), supported by a
convergence study that accounts for the underlying non-convexity. In addition,
computational complexity bounds highlighting the efficiency of the proposed
frameworks against standard approaches are presented. Finally, the robustness
and scalability of the framework is demonstrated through extensive simulation
results across diverse scenarios, including environments with nonconvex
obstacles and up to 246 agents.

</details>


### [538] [Tightening the mixed integer linear formulation for the piecewise linear approximation in general dimensions](https://arxiv.org/abs/2508.09395)
*Quentin Ploussard,Xiang Li,Matija Pavičević*

Main category: math.OC

TL;DR: This paper aims to improve the efficiency of mixed-integer linear programming (MILP) formulations for continuous piecewise linear (CPWL) approximations through tightening techniques based on well-behaved CPWL interpolations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies in MILP formulations for CPWL approximations by introducing ways to refine and tighten these models, ultimately enhancing computational performance in arbitrary dimensions.

Method: The authors propose six tightening strategies, including fixing specific variables, adding constraints, optimizing big-M parameter values, and applying tighter bounds. These strategies leverage the difference-of-convex representation and the well-behaved structure inherent in CPWL interpolations.

Result: Experimental results show that certain combinations of these tightening strategies significantly reduce solution times, particularly when well-behaved CPWL interpolations are utilized.

Conclusion: Using well-behaved CPWL interpolations and the proposed tightening strategies can greatly improve the computational efficiency of MILP problems, with broad implications for solving CPWL approximations in various applications.

Abstract: This paper addresses the problem of tightening the mixed-integer linear
programming (MILP) formulation for continuous piecewise linear (CPWL)
approximations of data sets in arbitrary dimensions. The MILP formulation
leverages the difference-of-convex (DC) representation of CPWL functions. We
introduce the concept of well-behaved CPWL interpolations and demonstrate that
any CPWL interpolation of a data set has a well-behaved version. This result is
critical to tighten the MILP problem. We present six different strategies to
tighten the problem, which include fixing the values of some variables,
introducing additional constraints, identifying small big-M parameter values
and applying tighter variable bounds. These methods leverage key aspects of the
DC representation and the inherent structure of well-behaved CPWL
interpolations. Experimental results demonstrate that specific combinations of
these tightening strategies lead to significant improvement in solution times,
especially for tightening strategies that consider well-behaved CPWL solutions.

</details>


### [539] [EXOTIC: An Exact, Optimistic, Tree-Based Algorithm for Min-Max Optimization](https://arxiv.org/abs/2508.12479)
*Chinmay Maheshwari,Chinmay Pimpalkhare,Debasish Chatterjee*

Main category: math.OC

TL;DR: This paper addresses the challenge of finding globally optimal solutions in convex-non-concave and non-convex-concave min-max optimization using a novel algorithm, EXOTIC.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of gradient-based methods in min-max optimization problems, which often fail to find global optima beyond convex-concave settings.

Method: The authors reformulate the problem to a non-concave-convex max-min structure and propose a new algorithm, EXOTIC, which combines iterative convex optimization solvers and hierarchical tree search.

Result: EXOTIC demonstrates superior performance over traditional gradient-based methods on benchmark convex-non-concave problems and multi-player game strategy computation.

Conclusion: EXOTIC is a powerful tool for solving challenging min-max optimization problems, with theoretical guarantees on its optimality gap and practical utility in real-world scenarios.

Abstract: Min-max optimization arises in many domains such as game theory, adversarial
machine learning, etc., with gradient-based methods as a typical computational
tool. Beyond convex-concave min-max optimization, the solutions found by
gradient-based methods may be arbitrarily far from global optima. In this work,
we present an algorithmic apparatus for computing globally optimal solutions in
convex-non-concave and non-convex-concave min-max optimization. For former, we
employ a reformulation that transforms it into a non-concave-convex max-min
optimization problem with suitably defined feasible sets and objective
function. The new form can be viewed as a generalization of Sion's minimax
theorem. Next, we introduce EXOTIC-an Exact, Optimistic, Tree-based algorithm
for solving the reformulated max-min problem. EXOTIC employs an iterative
convex optimization solver to (approximately) solve the inner minimization and
a hierarchical tree search for the outer maximization to optimistically select
promising regions to search based on the approximate solution returned by
convex optimization solver. We establish an upper bound on its optimality gap
as a function of the number of calls to the inner solver, the solver's
convergence rate, and additional problem-dependent parameters. Both our
algorithmic apparatus along with its accompanying theoretical analysis can also
be applied for non-convex-concave min-max optimization. In addition, we propose
a class of benchmark convex-non-concave min-max problems along with their
analytical global solutions, providing a testbed for evaluating algorithms for
min-max optimization. Empirically, EXOTIC outperforms gradient-based methods on
this benchmark as well as on existing numerical benchmark problems from the
literature. Finally, we demonstrate the utility of EXOTIC by computing security
strategies in multi-player games with three or more players.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [540] [Discovering Expert-Level Nash Equilibrium Algorithms with Large Language Models](https://arxiv.org/abs/2508.11874)
*Hanyu Li,Dongchen Li,Xiaotie Deng*

Main category: cs.GT

TL;DR: The paper introduces LegoNE, a framework that automates the design and analysis of algorithms, focusing on computing approximate Nash equilibria. The framework discovers and proves performance guarantees swiftly.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in automating the design and formal analysis of algorithms with provable performance guarantees, a traditionally labor-intensive and error-prone process.

Method: LegoNE integrates algorithm design and formal analysis by translating algorithms into constrained optimization problems, allowing an AI model to discover and verify algorithm performance guarantees.

Result: A state-of-the-art language model using LegoNE replicated a two-player game algorithm in hours and found a superior algorithm for three-player games, surpassing human achievements.

Conclusion: The research introduces a collaborative paradigm where humans provide abstract reasoning while AI explores solution spaces, breaking barriers in theoretical algorithm design and analysis.

Abstract: Algorithm design and analysis is a cornerstone of computer science, but it
confronts a major challenge. Proving an algorithm's performance guarantee
across all inputs has traditionally required extensive and often error-prone
human effort. While AI has shown great success in finding solutions to specific
problem instances, automating the discovery of general algorithms with such
provable guarantees has remained a significant barrier. This challenge stems
from the difficulty of integrating the creative process of algorithm design
with the rigorous process of formal analysis. To address this gap, we propose
LegoNE, a framework that tightly fuses these two processes for the fundamental
and notoriously difficult problem of computing approximate Nash equilibria.
LegoNE automatically translates any algorithm written by a simple Python-like
language into a constrained optimization problem. Solving this problem derives
and proves the algorithm's approximation bound. Using LegoNE, a
state-of-the-art large language model rediscovered the state-of-the-art
algorithm for two-player games within hours, a feat that had taken human
researchers 15 years to achieve. For three-player games, the model discovered a
novel algorithm surpassing all existing human-designed ones. This work
demonstrates a new human-machine collaborative paradigm for theoretical
science: humans reason at a higher-abstract level, using symbols to compress
the search space, and AI explores within it, achieving what neither could
alone.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [541] [Does the Barron space really defy the curse of dimensionality?](https://arxiv.org/abs/2508.12273)
*Olov Schavemaker*

Main category: math.FA

TL;DR: The paper investigates whether the Barron space defies the curse of dimensionality with respect to a nonclassical notion of smoothness, introducing ADZ spaces via the Mellin transform.


<details>
  <summary>Details</summary>
Motivation: The Barron space is known for addressing the curse of dimensionality in shallow neural networks using classical smoothness metrics; the paper seeks to explore whether this holds under nonclassical smoothness measures.

Method: To analyze nonclassical smoothness, the authors define ADZ spaces based on the Mellin transform, analogous to the Bessel potential spaces defined via the Fourier transform.

Result: Evidence is presented suggesting that the Barron space and its generalizations may not overcome the curse of dimensionality in terms of nonclassical smoothness.

Conclusion: The findings challenge the prevailing notion that the Barron space defies the curse of dimensionality universally, emphasizing the need to consider smoothness measures in new dimensions.

Abstract: The Barron space has become famous in the theory of (shallow) neural networks
because it seemingly defies the curse of dimensionality. And while the Barron
space (and generalizations) indeed defies (defy) the curse of dimensionality
from the POV of classical smoothness, we herein provide some evidence in favor
of the idea that the Barron space (and generalizations) does (do) not defy the
curse of dimensionality with a nonclassical notion of smoothness which relates
naturally to "infinitely wide" shallow neural networks. Like how the Bessel
potential spaces are defined via the Fourier transform, we define so-called ADZ
spaces via the Mellin transform; these ADZ spaces encapsulate the nonclassical
smoothness we alluded to earlier.
  38 pages, will appear in the dissertation of the author

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [542] [Asymptotic breakdown point analysis of the minimum density power divergence estimator under independent non-homogeneous setups](https://arxiv.org/abs/2508.12426)
*Suryasis Jana,Subhrajyoty Roy,Ayanendranath Basu,Abhik Ghosh*

Main category: math.ST

TL;DR: The paper investigates the robustness of the minimum density power divergence estimator (MDPDE) for independent, non-homogeneous observations, focusing on its asymptotic breakdown behavior.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in understanding the global reliability and breakdown behavior of the MDPDE under independent, non-homogeneous observation setups, beyond the specific case of location-type models.

Method: The authors extend the concept of the asymptotic breakdown point to non-homogeneous setups and derive theoretical lower bounds for the MDPDE's breakdown point under certain assumptions. Applications to regression models and simulation studies support their findings.

Result: The study establishes a theoretical lower bound for the MDPDE's asymptotic breakdown point and validates these findings through regression model applications and simulations.

Conclusion: The paper demonstrates the global robustness of the MDPDE in independent, non-homogeneous settings by extending and validating its breakdown behavior, thereby expanding its applicability.

Abstract: The minimum density power divergence estimator (MDPDE) has gained significant
attention in the literature of robust inference due to its strong robustness
properties and high asymptotic efficiency; it is relatively easy to compute and
can be interpreted as a generalization of the classical maximum likelihood
estimator. It has been successfully applied in various setups, including the
case of independent and non-homogeneous (INH) observations that cover both
classification and regression-type problems with a fixed design. While the
local robustness of this estimator has been theoretically validated through the
bounded influence function, no general result is known about the global
reliability or the breakdown behavior of this estimator under the INH setup,
except for the specific case of location-type models. In this paper, we extend
the notion of asymptotic breakdown point from the case of independent and
identically distributed data to the INH setup and derive a theoretical lower
bound for the asymptotic breakdown point of the MDPDE, under some easily
verifiable assumptions. These results are further illustrated with applications
to some fixed design regression models and corroborated through extensive
simulation studies.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [543] [Point upsampling networks for single-photon sensing](https://arxiv.org/abs/2508.12986)
*Jinyi Liu,Guoyang Zhao,Lijun Liu,Yiguang Hong,Weiping Zhang,Shuming Cheng*

Main category: physics.optics

TL;DR: This paper introduces a novel solution for enhancing the density and spatial accuracy of single-photon point clouds through the use of a point upsampling network.


<details>
  <summary>Details</summary>
Motivation: Single-photon sensing promises ultra-sensitive imaging, but its utility is limited by sparse and spatially biased point clouds.

Method: The proposed framework incorporates a multi-path scanning mechanism, a bidirectional Mamba backbone, and an adaptive upsample shift module grounded in a state space model.

Result: Experiments validate the model's high reconstruction accuracy, resilience to distortion noise, and capacity to produce visually consistent and detail-rich point clouds.

Conclusion: This work pioneers an upsampling framework for single-photon sensing, paving the way for advanced practical applications and downstream tasks.

Abstract: Single-photon sensing has generated great interest as a prominent technique
of long-distance and ultra-sensitive imaging, however, it tends to yield sparse
and spatially biased point clouds, thus limiting its practical utility. In this
work, we propose using point upsampling networks to increase point density and
reduce spatial distortion in single-photon point cloud. Particularly, our
network is built on the state space model which integrates a multi-path
scanning mechanism to enrich spatial context, a bidirectional Mamba backbone to
capture global geometry and local details, and an adaptive upsample shift
module to correct offset-induced distortions. Extensive experiments are
implemented on commonly-used datasets to confirm its high reconstruction
accuracy and strong robustness to the distortion noise, and also on real-world
data to demonstrate that our model is able to generate visually consistent,
detail-preserving, and noise suppressed point clouds. Our work is the first to
establish the upsampling framework for single-photon sensing, and hence opens a
new avenue for single-photon sensing and its practical applications in the
downstreaming tasks.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [544] [Generalized invariants meet constitutive neural networks: A novel framework for hyperelastic materials](https://arxiv.org/abs/2508.12063)
*Denisa Martonová,Alain Goriely,Ellen Kuhl*

Main category: cond-mat.soft

TL;DR: This study presents a data-driven framework for discovering invariants and strain energy functions used in hyperelastic models, leveraging a neural network architecture to adapt flexibly to different material behaviors.


<details>
  <summary>Details</summary>
Motivation: The challenge in hyperelastic modeling is selecting appropriate invariants and determining how the strain energy function depends on them, which current approaches struggle to efficiently integrate.

Method: A neural network-based framework integrates invariant discovery and constitutive model development, considering a continuous range of possible invariants directly from experimental data.

Result: Benchmark tests on rubber and brain tissue demonstrate the method's ability to recover material behaviors (stretch-dominated for rubber and nonlinear shear response for brain tissue) with greater predictive accuracy and interpretability.

Conclusion: The framework improves automated and physically meaningful model discovery for hyperelastic materials, offering a robust and unified approach to address diverse behaviors.

Abstract: The major challenge in determining a hyperelastic model for a given material
is the choice of invariants and the selection how the strain energy function
depends functionally on these invariants. Here we introduce a new data-driven
framework that simultaneously discovers appropriate invariants and constitutive
models for isotropic incompressible hyperelastic materials. Our approach
identifies both the most suitable invariants in a class of generalized
invariants and the corresponding strain energy function directly from
experimental observations. Unlike previous methods that rely on fixed invariant
choices or sequential fitting procedures, our method integrates the discovery
process into a single neural network architecture. By looking at a continuous
family of possible invariants, the model can flexibly adapt to different
material behaviors. We demonstrate the effectiveness of this approach using
popular benchmark datasets for rubber and brain tissue. For rubber, the method
recovers a stretch-dominated formulation consistent with classical models. For
brain tissue, it identifies a formulation sensitive to small stretches,
capturing the nonlinear shear response characteristic of soft biological
matter. Compared to traditional and neural-network-based models, our framework
provides improved predictive accuracy and interpretability across a wide range
of deformation states. This unified strategy offers a robust tool for automated
and physically meaningful model discovery in hyperelasticity.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [545] [Deep Semantic Inference over the Air: An Efficient Task-Oriented Communication System](https://arxiv.org/abs/2508.12748)
*Chenyang Wang,Roger Olsson,Stefan Forsström,Qing He*

Main category: cs.IT

TL;DR: This paper develops a deep learning-based semantic communication system to optimize task accuracy while minimizing computational and communication overhead in wireless environments.


<details>
  <summary>Details</summary>
Motivation: The motivation of this study is to improve wireless communication efficiency by employing semantic communication techniques, focusing on task-relevant meaning rather than transmitting raw data, when implementing real-world classification tasks.

Method: The authors utilize ResNet-based models and partition them at various points to enable split inference over wireless channels. They simulate classification tasks using the CIFAR-10 and CIFAR-100 datasets and analyze trade-offs by altering the split location and semantic feature vector size.

Result: The experimental results demonstrate that the system can maintain over 85% baseline accuracy while substantially reducing computational load and communication costs when employing suitable model partitioning and feature compression.

Conclusion: This study concludes that task-oriented semantic communication with proper configurations can achieve efficient classification performance while optimizing resources in wireless environments.

Abstract: Empowered by deep learning, semantic communication marks a paradigm shift
from transmitting raw data to conveying task-relevant meaning, enabling more
efficient and intelligent wireless systems. In this study, we explore a deep
learning-based task-oriented communication framework that jointly considers
classification performance, computational latency, and communication cost. We
adopt ResNets-based models and evaluate them on the CIFAR-10 and CIFAR-100
datasets to simulate real-world classification tasks in wireless environments.
We partition the model at various points to simulate split inference across a
wireless channel. By varying the split location and the size of the transmitted
semantic feature vector, we systematically analyze the trade-offs between task
accuracy and resource efficiency. Experimental results show that, with
appropriate model partitioning and semantic feature compression, the system can
retain over 85\% of baseline accuracy while significantly reducing both
computational load and communication overhead.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [546] [A Large-Scale Web Search Dataset for Federated Online Learning to Rank](https://arxiv.org/abs/2508.12353)
*Marcel Gregoriadis,Jingwei Kang,Johan Pouwelse*

Main category: cs.IR

TL;DR: AOL4FOLTR introduces a large-scale web search dataset with enhanced realism for federated online learning-to-rank research.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in centralized collection of user interaction data and improve realism in federated learning-to-rank benchmarks.

Method: The paper presents AOL4FOLTR, a dataset featuring real user click data, query timestamps, and user identifiers for realistic modeling in federated learning.

Result: A new dataset with 2.6 million queries from 10,000 users, enabling realistic partitioning and asynchronous federated learning scenarios.

Conclusion: The AOL4FOLTR dataset overcomes limitations of existing benchmarks, fostering better research in privacy-preserving collaborative learning-to-rank approaches.

Abstract: The centralized collection of search interaction logs for training ranking
models raises significant privacy concerns. Federated Online Learning to Rank
(FOLTR) offers a privacy-preserving alternative by enabling collaborative model
training without sharing raw user data. However, benchmarks in FOLTR are
largely based on random partitioning of classical learning-to-rank datasets,
simulated user clicks, and the assumption of synchronous client participation.
This oversimplifies real-world dynamics and undermines the realism of
experimental results. We present AOL4FOLTR, a large-scale web search dataset
with 2.6 million queries from 10,000 users. Our dataset addresses key
limitations of existing benchmarks by including user identifiers, real click
data, and query timestamps, enabling realistic user partitioning, behavior
modeling, and asynchronous federated learning scenarios.

</details>


### [547] [RRRA: Resampling and Reranking through a Retriever Adapter](https://arxiv.org/abs/2508.11670)
*Bongsu Kim*

Main category: cs.IR

TL;DR: This paper introduces a learnable adapter module to improve dense retrieval by detecting false negatives dynamically, leading to better training and inference outcomes.


<details>
  <summary>Details</summary>
Motivation: Existing heuristics for identifying hard negatives in dense retrieval often fail to address instance-specific false negatives, limiting effectiveness.

Method: Develop a learnable adapter module integrated with Bi-Encoder representations to predict false negatives dynamically and use these predictions for resampling in training and reranking in inference.

Result: Empirical evaluations on benchmarks demonstrate the proposed framework consistently surpasses strong Bi-Encoder baselines.

Conclusion: Explicit false negative modeling significantly boosts performance in dense retrieval tasks.

Abstract: In dense retrieval, effective training hinges on selecting high quality hard
negatives while avoiding false negatives. Recent methods apply heuristics based
on positive document scores to identify hard negatives, improving both
performance and interpretability. However, these global, example agnostic
strategies often miss instance specific false negatives. To address this, we
propose a learnable adapter module that monitors Bi-Encoder representations to
estimate the likelihood that a hard negative is actually a false negative. This
probability is modeled dynamically and contextually, enabling fine-grained,
query specific judgments. The predicted scores are used in two downstream
components: (1) resampling, where negatives are reweighted during training, and
(2) reranking, where top-k retrieved documents are reordered at inference.
Empirical results on standard benchmarks show that our adapter-enhanced
framework consistently outperforms strong Bi-Encoder baselines, underscoring
the benefit of explicit false negative modeling in dense retrieval.

</details>


### [548] [LLM-Based Intelligent Agents for Music Recommendation: A Comparison with Classical Content-Based Filtering](https://arxiv.org/abs/2508.11671)
*Ronald Carvalho Boadana,Ademir Guimarães da Costa Junior,Ricardo Rios,Fábio Santos da Silva*

Main category: cs.IR

TL;DR: This paper investigates using Large Language Models (LLMs) with intelligent agents for personalized music recommendations, outperforming traditional methods with high user satisfaction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address information overload on music streaming platforms and improve user experiences through advanced recommendation systems.

Method: The study utilizes LLMs from the Gemini and LLaMA families combined with intelligent agents in a multi-agent music recommendation system and compares results to a traditional content-based model.

Result: LLMs demonstrated high user satisfaction, achieving rates of up to 89.32%, and performed well in novelty and computational efficiency.

Conclusion: LLMs show significant potential in enhancing music recommendation systems, improving user satisfaction over traditional approaches.

Abstract: The growing availability of music on streaming platforms has led to
information overload for users. To address this issue and enhance the user
experience, increasingly sophisticated recommendation systems have been
proposed. This work investigates the use of Large Language Models (LLMs) from
the Gemini and LLaMA families, combined with intelligent agents, in a
multi-agent personalized music recommendation system. The results are compared
with a traditional content-based recommendation model, considering user
satisfaction, novelty, and computational efficiency. LLMs achieved satisfaction
rates of up to \textit{89{,}32\%}, indicating their promising potential in
music recommendation systems.

</details>


### [549] [TBGRecall: A Generative Retrieval Model for E-commerce Recommendation Scenarios](https://arxiv.org/abs/2508.11977)
*Zida Liang,Changfa Wu,Dunxian Huang,Weiqiang Sun,Ziyang Wang,Yuliang Yan,Jian Wu,Yuning Jiang,Bo Zheng,Ke Chen,Silu Zhou,Yu Zhang*

Main category: cs.IR

TL;DR: The paper proposes TBGRecall, a framework integrating Next Session Prediction (NSP), to improve generative models for e-commerce recommendation systems by addressing limitations in optimizing retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Generative models enhance recommendation systems, but they struggle with efficient retrieval in sequential item generation, limiting their performance in e-commerce.

Method: Proposed TBGRecall, reformulated inputs into multi-session sequences, added task-specific optimizations, and used a training pipeline combining historical data pre-training with stochastic partial incremental training.

Result: Experiments on public benchmarks and a large-scale TaoBao dataset show TBGRecall surpasses state-of-the-art methods and demonstrates scaling law trends.

Conclusion: TBGRecall, with its integration of NSP, effectively enhances generative recommendation systems, improving e-commerce applications through superior training methods and optimization.

Abstract: Recommendation systems are essential tools in modern e-commerce, facilitating
personalized user experiences by suggesting relevant products. Recent
advancements in generative models have demonstrated potential in enhancing
recommendation systems; however, these models often exhibit limitations in
optimizing retrieval tasks, primarily due to their reliance on autoregressive
generation mechanisms. Conventional approaches introduce sequential
dependencies that impede efficient retrieval, as they are inherently unsuitable
for generating multiple items without positional constraints within a single
request session. To address these limitations, we propose TBGRecall, a
framework integrating Next Session Prediction (NSP), designed to enhance
generative retrieval models for e-commerce applications. Our framework
reformulation involves partitioning input samples into multi-session sequences,
where each sequence comprises a session token followed by a set of item tokens,
and then further incorporate multiple optimizations tailored to the generative
task in retrieval scenarios. In terms of training methodology, our pipeline
integrates limited historical data pre-training with stochastic partial
incremental training, significantly improving training efficiency and
emphasizing the superiority of data recency over sheer data volume. Our
extensive experiments, conducted on public benchmarks alongside a large-scale
industrial dataset from TaoBao, show TBGRecall outperforms the state-of-the-art
recommendation methods, and exhibits a clear scaling law trend. Ultimately, NSP
represents a significant advancement in the effectiveness of generative
recommendation systems for e-commerce applications.

</details>


### [550] [Ontology-Guided Query Expansion for Biomedical Document Retrieval using Large Language Models](https://arxiv.org/abs/2508.11784)
*Zabir Al Nazi,Vagelis Hristidis,Aaron Lawson McLean,Jannat Ara Meem,Md Taukir Azam Chowdhury*

Main category: cs.IR

TL;DR: The paper introduces BMQExpander, a novel biomedical query expansion method combining UMLS Metathesaurus knowledge with large language model capabilities for improved document retrieval.


<details>
  <summary>Details</summary>
Motivation: The challenge arises from domain-specific vocabulary and semantic ambiguity in biomedical document retrieval for effective question answering.

Method: BMQExpander integrates medical ontology data from UMLS and generative large language models to enhance query relevance in retrieval systems.

Result: BMQExpander outperformed baselines in three biomedical IR benchmarks, achieving notable improvements in NDCG@10 and robustness under query perturbations.

Conclusion: BMQExpander offers superior and robust retrieval performance with reduced hallucinations, a valuable contribution to biomedical question answering research.

Abstract: Effective Question Answering (QA) on large biomedical document collections
requires effective document retrieval techniques. The latter remains a
challenging task due to the domain-specific vocabulary and semantic ambiguity
in user queries. We propose BMQExpander, a novel ontology-aware query expansion
pipeline that combines medical knowledge - definitions and relationships - from
the UMLS Metathesaurus with the generative capabilities of large language
models (LLMs) to enhance retrieval effectiveness. We implemented several
state-of-the-art baselines, including sparse and dense retrievers, query
expansion methods, and biomedical-specific solutions. We show that BMQExpander
has superior retrieval performance on three popular biomedical Information
Retrieval (IR) benchmarks: NFCorpus, TREC-COVID, and SciFact - with
improvements of up to 22.1% in NDCG@10 over sparse baselines and up to 6.5%
over the strongest baseline. Further, BMQExpander generalizes robustly under
query perturbation settings, in contrast to supervised baselines, achieving up
to 15.7% improvement over the strongest baseline. As a side contribution, we
publish our paraphrased benchmarks. Finally, our qualitative analysis shows
that BMQExpander has fewer hallucinations compared to other LLM-based query
expansion baselines.

</details>


### [551] [Leveraging Geometric Insights in Hyperbolic Triplet Loss for Improved Recommendations](https://arxiv.org/abs/2508.11978)
*Viacheslav Yusupov,Maxim Rakhuba,Evgeny Frolov*

Main category: cs.IR

TL;DR: The paper introduces a hyperbolic recommendation model utilizing improved geometry for better representation learning, outperforming Euclidean and prior hyperbolic models.


<details>
  <summary>Details</summary>
Motivation: To leverage hyperbolic geometry in recommender systems for capturing complex data patterns and improving computational stability.

Method: The model reformulates hyperbolic distances to enhance representation capacity and uses a triplet loss to model ternary user-item interactions with pairwise terms.

Result: The hyperbolic model surpasses existing Euclidean and hyperbolic frameworks, reduces popularity bias, and delivers more diverse, personalized recommendations.

Conclusion: Hyperbolic geometry strengthens both performance and diversity in recommendation systems, proving its value over traditional Euclidean methods.

Abstract: Recent studies have demonstrated the potential of hyperbolic geometry for
capturing complex patterns from interaction data in recommender systems. In
this work, we introduce a novel hyperbolic recommendation model that uses
geometrical insights to improve representation learning and increase
computational stability at the same time. We reformulate the notion of
hyperbolic distances to unlock additional representation capacity over
conventional Euclidean space and learn more expressive user and item
representations. To better capture user-items interactions, we construct a
triplet loss that models ternary relations between users and their
corresponding preferred and nonpreferred choices through a mix of pairwise
interaction terms driven by the geometry of data. Our hyperbolic approach not
only outperforms existing Euclidean and hyperbolic models but also reduces
popularity bias, leading to more diverse and personalized recommendations.

</details>


### [552] [Asymmetric Diffusion Recommendation Model](https://arxiv.org/abs/2508.12706)
*Yongchun Zhu,Guanyu Jiang,Jingwu Chen,Feng Zhang,Xiao Yang,Zuotao Liu*

Main category: cs.IR

TL;DR: The paper introduces AsymDiffRec, an asymmetric diffusion model for recommendations, addressing the limitations of Gaussian noise in discrete data spaces.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations in current diffusion-based recommendation models, such as Gaussian noise corrupting personalized information and their design for continuous rather than discrete data spaces.

Method: AsymDiffRec uses an asymmetric forward and reverse process to simulate missing features and optimize personalized information, with a task-oriented optimization strategy for robust representation.

Result: The model improves user engagement, as shown by an increase of 0.131% in users' active days and 0.166% in app usage during online A/B tests. Offline experiments affirm its effectiveness.

Conclusion: The proposed AsymDiffRec method effectively enhances recommendation systems, demonstrating practical benefits and is successfully implemented in the Douyin Music App.

Abstract: Recently, motivated by the outstanding achievements of diffusion models, the
diffusion process has been employed to strengthen representation learning in
recommendation systems. Most diffusion-based recommendation models typically
utilize standard Gaussian noise in symmetric forward and reverse processes in
continuous data space. Nevertheless, the samples derived from recommendation
systems inhabit a discrete data space, which is fundamentally different from
the continuous one. Moreover, Gaussian noise has the potential to corrupt
personalized information within latent representations. In this work, we
propose a novel and effective method, named Asymmetric Diffusion Recommendation
Model (AsymDiffRec), which learns forward and reverse processes in an
asymmetric manner. We define a generalized forward process that simulates the
missing features in real-world recommendation samples. The reverse process is
then performed in an asymmetric latent feature space. To preserve personalized
information within the latent representation, a task-oriented optimization
strategy is introduced. In the serving stage, the raw sample with missing
features is regarded as a noisy input to generate a denoising and robust
representation for the final prediction. By equipping base models with
AsymDiffRec, we conduct online A/B tests, achieving improvements of +0.131% and
+0.166% in terms of users' active days and app usage duration respectively.
Additionally, the extended offline experiments also demonstrate improvements.
AsymDiffRec has been implemented in the Douyin Music App.

</details>


### [553] [Is This News Still Interesting to You?: Lifetime-aware Interest Matching for News Recommendation](https://arxiv.org/abs/2508.13064)
*Seongeun Ryu,Yunyong Ko,Sang-Wook Kim*

Main category: cs.IR

TL;DR: The paper introduces LIME (Lifetime-aware Interest Matching for nEws), a framework aiming to enhance personalized news recommendations by modeling users' interest persistence and news lifetime variability.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address challenges in personalized news recommendation, particularly users' interest persistence based on news age, and the varying lifetime of news across topics and users.

Method: The proposed LIME framework employs three strategies: (1) User-Topic lifetime-aware age representation, (2) Candidate-aware lifetime attention, and (3) Freshness-guided interest refinement.

Result: Experiments on two real-world datasets show that LIME outperforms other state-of-the-art methods and its strategies notably enhance accuracy.

Conclusion: LIME provides an effective and generalizable approach to address time-related challenges in news recommendation, offering measurable improvements in recommendation performance.

Abstract: Personalized news recommendation aims to deliver news articles aligned with
users' interests, serving as a key solution to alleviate the problem of
information overload on online news platforms. While prior work has improved
interest matching through refined representations of news and users, the
following time-related challenges remain underexplored: (C1) leveraging the age
of clicked news to infer users' interest persistence, and (C2) modeling the
varying lifetime of news across topics and users. To jointly address these
challenges, we propose a novel Lifetime-aware Interest Matching framework for
nEws recommendation, named LIME, which incorporates three key strategies: (1)
User-Topic lifetime-aware age representation to capture the relative age of
news with respect to a user-topic pair, (2) Candidate-aware lifetime attention
for generating temporally aligned user representation, and (3) Freshness-guided
interest refinement for prioritizing valid candidate news at prediction time.
Extensive experiments on two real-world datasets demonstrate that LIME
consistently outperforms a wide range of state-of-the-art news recommendation
methods, and its model agnostic strategies significantly improve recommendation
accuracy.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [554] [Control of a commercial vehicle by a tetraplegic human using a bimanual brain-computer interface](https://arxiv.org/abs/2508.11805)
*Xinyun Zou,Jorge Gamez,Meghna Menon,Phillip Ring,Chadwick Boulay,Likhith Chitneni,Jackson Brennecke,Shana R. Melby,Gracy Kureel,Kelsie Pejsa,Emily R. Rosario,Ausaf A. Bari,Aniruddh Ravindran,Tyson Aflalo,Spencer S. Kellis,Dimitar Filev,Florian Solzbacher,Richard A. Andersen*

Main category: eess.SY

TL;DR: This study presents a novel bimanual brain-computer interface (BCI) allowing individuals with tetraplegia to drive vehicles in both simulated and real-world environments.


<details>
  <summary>Details</summary>
Motivation: The study aimed to explore real-world applications of BCIs, which have been largely limited to laboratory settings, with the specific goal of enabling vehicle control by individuals with neurological impairments.

Method: Researchers developed a BCI system using intracortical electrodes implanted in the posterior parietal cortex and motor cortex. The system was tested for driving performance in both simulated environments and real-world teledriving scenarios.

Result: A participant with tetraplegia achieved driving proficiency comparable to motor-intact individuals, successfully driving in both virtual simulations and remotely controlling a vehicle in real-world environments. Safety and feasibility were also demonstrated.

Conclusion: This work demonstrates the broad potential of BCI technology to restore independence to individuals with severe neurological conditions, showcasing its innovative applications in real-world scenarios.

Abstract: Brain-computer interfaces (BCIs) read neural signals directly from the brain
to infer motor planning and execution. However, the implementation of this
technology has been largely limited to laboratory settings, with few real-world
applications. We developed a bimanual BCI system to drive a vehicle in both
simulated and real-world environments. We demonstrate that an individual with
tetraplegia, implanted with intracortical BCI electrodes in the posterior
parietal cortex (PPC) and the hand knob region of the motor cortex (MC), reacts
at least as fast and precisely as motor intact participants, and drives a
simulated vehicle as proficiently as the same control group. This BCI
participant, living in California, could also remotely drive a Ford Mustang
Mach-E vehicle in Michigan. Our first teledriving task relied on cursor control
for speed and steering in a closed urban test facility. However, the final BCI
system added click control for full-stop braking and thus enabled bimanual
cursor-and-click control for both simulated driving through a virtual town with
traffic and teledriving through an obstacle course without traffic in the real
world. We also demonstrate the safety and feasibility of BCI-controlled
driving. This first-of-its-kind implantable BCI application not only highlights
the versatility and innovative potentials of BCIs but also illuminates the
promising future for the development of life-changing solutions to restore
independence to those who suffer catastrophic neurological injury.

</details>


### [555] [A Hierarchical Surrogate Model for Efficient Multi-Task Parameter Learning in Closed-Loop Contro](https://arxiv.org/abs/2508.12738)
*Sebastian Hirt,Lukas Theiner,Maik Pfefferkorn,Rolf Findeisen*

Main category: eess.SY

TL;DR: This paper proposes a hierarchical Bayesian optimization method for tuning controller parameters, focusing on efficiency and adaptability across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of improving controller parameter tuning across distinct closed-loop tasks, where data efficiency and adaptability are critical.

Method: The authors propose using a hierarchical Bayesian optimization framework with Gaussian processes to exploit knowledge of the dynamical system and control structure rather than treating the closed-loop cost as a black-box.

Result: The method demonstrated improved sample efficiency and adaptability in simulation experiments compared to black-box Bayesian optimization approaches.

Conclusion: The framework enhances controller learning efficiency and adaptability, leveraging structural problem knowledge and retaining sublinear regret guarantees for multi-task learning scenarios.

Abstract: Many control problems require repeated tuning and adaptation of controllers
across distinct closed-loop tasks, where data efficiency and adaptability are
critical. We propose a hierarchical Bayesian optimization (BO) framework that
is tailored to efficient controller parameter learning in sequential
decision-making and control scenarios for distinct tasks. Instead of treating
the closed-loop cost as a black-box, our method exploits structural knowledge
of the underlying problem, consisting of a dynamical system, a control law, and
an associated closed-loop cost function. We construct a hierarchical surrogate
model using Gaussian processes that capture the closed-loop state evolution
under different parameterizations, while the task-specific weighting and
accumulation into the closed-loop cost are computed exactly via known
closed-form expressions. This allows knowledge transfer and enhanced data
efficiency between different closed-loop tasks. The proposed framework retains
sublinear regret guarantees on par with standard black-box BO, while enabling
multi-task or transfer learning. Simulation experiments with model predictive
control demonstrate substantial benefits in both sample efficiency and
adaptability when compared to purely black-box BO approaches.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [556] [Statistical analysis of multivariate planar curves and applications to X-ray classification](https://arxiv.org/abs/2508.11780)
*Moindjié Issam-Ali,Descary Marie-Hélène,Beaulac Cédric*

Main category: stat.ME

TL;DR: This paper proposes a statistical approach for multivariate planar curve analysis, applied to segmented medical images for supervised classification. It demonstrates efficacy in cardiomegaly detection using segmented X-rays.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage object shapes within segmented images, particularly in medical imaging, as predictive features for supervised classification tasks like disease diagnosis.

Method: The paper introduces multivariate planar curve analysis, solves curve alignment issues in statistical shape analysis, and employs tangent projections for functional classification.

Result: The proposed method effectively identifies cardiomegaly in segmented X-rays and performs robustly in synthetic data experiments.

Conclusion: The method demonstrates promise in integrating shape analysis into predictive models, particularly aiding medical image-based disease detection.

Abstract: Recent developments in computer vision have enabled the availability of
segmented images across various domains, such as medicine, where segmented
radiography images play an important role in diagnosis-making. As prediction
problems are common in medical image analysis, this work explores the use of
segmented images (through the associated contours they highlight) as predictors
in a supervised classification context. Consequently, we develop a new approach
for image analysis that takes into account the shape of objects within images.
For this aim, we introduce a new formalism that extends the study of single
random planar curves to the joint analysis of multiple planar curves-referred
to here as multivariate planar curves. In this framework, we propose a solution
to the alignment issue in statistical shape analysis. The obtained multivariate
shape variables are then used in functional classification methods through
tangent projections. Detection of cardiomegaly in segmented X-rays and
numerical experiments on synthetic data demonstrate the appeal and robustness
of the proposed method.

</details>


### [557] [Unified Conformalized Multiple Testing with Full Data Efficiency](https://arxiv.org/abs/2508.12085)
*Yuyang Huo,Xiaoyang Wu,Changliang Zou,Haojie Ren*

Main category: stat.ME

TL;DR: This paper proposes a unified framework for conformalized multiple testing that improves data utilization and rigorously controls false discovery rates, showing superior performance across various scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing methods for conformalized multiple testing often do not fully utilize all available data, leading to limited power and inefficiency. This paper aims to address these limitations by proposing a unified and systematic approach.

Method: The proposed framework uses all available data (null, alternative, and unlabeled) to create non-conformity scores and calibrate p-values through a full permutation strategy. It emphasizes optimizing data utilization and enables automatic selection of the best conformal procedure.

Result: The framework significantly improves power by enhancing score quality and increasing calibration set size. It rigorously controls the false discovery rate and demonstrates superior efficiency and adaptability in numerical experiments.

Conclusion: This unified framework sets a design principle for conformal testing, achieving improved performance while maintaining rigorous error control, and supports automatic conformal procedure selection without data splitting.

Abstract: Conformalized multiple testing offers a model-free way to control predictive
uncertainty in decision-making. Existing methods typically use only part of the
available data to build score functions tailored to specific settings. We
propose a unified framework that puts data utilization at the center: it uses
all available data-null, alternative, and unlabeled-to construct scores and
calibrate p-values through a full permutation strategy. This unified use of all
available data significantly improves power by enhancing non-conformity score
quality and maximizing calibration set size while rigorously controlling the
false discovery rate. Crucially, our framework provides a systematic design
principle for conformal testing and enables automatic selection of the best
conformal procedure among candidates without extra data splitting. Extensive
numerical experiments demonstrate that our enhanced methods deliver superior
efficiency and adaptability across diverse scenarios.

</details>


### [558] [Simultaneous estimation of connectivity and dimensionality in samples of networks](https://arxiv.org/abs/2508.12483)
*Wenlong Jiang,Chris McKennan,Jesús Arroyo,Joshua Cape*

Main category: stat.ME

TL;DR: This paper introduces a convex optimization method to estimate connectivity probabilities and their embedding dimensionality in heterogeneous network datasets.


<details>
  <summary>Details</summary>
Motivation: Current methods often overlook parsimonious embedding dimension selection and downstream connectivity estimation in multiple network datasets.

Method: The paper uses a convex optimization approach with an alternating direction method of multipliers algorithm to estimate latent connectivity matrices and embedding ranks after node community memberships are pre-estimated.

Result: The proposed method gives superior performance in connectivity and dimensionality estimation compared to averaging-based methods, validated through numerical and empirical studies.

Conclusion: The methodology is effective, especially when dimensionality is much smaller than the number of communities, and finds practical application in datasets like primate brain networks.

Abstract: An overarching objective in contemporary statistical network analysis is
extracting salient information from datasets consisting of multiple networks.
To date, considerable attention has been devoted to node and network
clustering, while comparatively less attention has been devoted to downstream
connectivity estimation and parsimonious embedding dimension selection. Given a
sample of potentially heterogeneous networks, this paper proposes a method to
simultaneously estimate a latent matrix of connectivity probabilities and its
embedding dimensionality or rank after first pre-estimating the number of
communities and the node community memberships. The method is formulated as a
convex optimization problem and solved using an alternating direction method of
multipliers algorithm. We establish estimation error bounds under the Frobenius
norm and nuclear norm for settings in which observable networks have blockmodel
structure, even when node memberships are imperfectly recovered. When perfect
membership recovery is possible and dimensionality is much smaller than the
number of communities, the proposed method outperforms conventional
averaging-based methods for estimating connectivity and dimensionality.
Numerical studies empirically demonstrate the accuracy of our method across
various scenarios. Additionally, analysis of a primate brain dataset
demonstrates that posited connectivity is not necessarily full rank in
practice, illustrating the need for flexible methodology.

</details>


### [559] [A self-supervised learning approach for denoising autoregressive models with additive noise: finite and infinite variance cases](https://arxiv.org/abs/2508.12970)
*Sayantan Banerjee,Agnieszka Wylomanska,Sundar S*

Main category: stat.ME

TL;DR: This paper introduces a novel self-supervised learning method to effectively denoise additive noise in autoregressive models, particularly for highly impulsive, infinite-variance noise.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of traditional autoregressive model estimation techniques when dealing with heavy-tailed, infinite-variance noise, which poses significant challenges for accurate modeling and forecasting.

Method: The authors propose a self-supervised learning approach inspired by recent advancements in computer vision that doesn't require full knowledge of the noise distribution.

Result: The method shows superior efficiency over baseline models in simulation studies on synthetic and semi-synthetic data, particularly in cases of strong and impulsive noise corruption.

Conclusion: The study highlights the proposed method's effectiveness in recovering autoregressive signals from noisy data and its potential for noise-robust forecasting.

Abstract: The autoregressive time series model is a popular second-order stationary
process, modeling a wide range of real phenomena. However, in applications,
autoregressive signals are often corrupted by additive noise. Further, the
autoregressive process and the corruptive noise may be highly impulsive,
stemming from an infinite-variance distribution. The model estimation
techniques that account for additional noise tend to show reduced efficacy when
there is very strong noise present in the data, especially when the noise is
heavy-tailed. Moreover, identification of a model corrupted with heavy-tailed,
particularly infinite-variance noise, can be a very challenging task. In this
paper, we propose a novel self-supervised learning method to denoise the
additive noise-corrupted autoregressive model. Our approach is motivated by
recent work in computer vision and does not require full knowledge of the noise
distribution. We use the proposed method to recover exemplary finite- and
infinite-variance autoregressive signals, namely, Gaussian- and alpha-stable
distributed signals, respectively, from their noise-corrupted versions. The
simulation study conducted on both synthetic and semi-synthetic data
demonstrates the efficiency of our method compared to several baseline methods,
particularly when the corruption is significant and impulsive in nature.
Finally, we apply the presented methodology to forecast the pure autoregressive
signal from the noise-corrupted data.

</details>


### [560] [A Generalized Genetic Random Field Method for the Genetic Association Analysis of Sequencing Data](https://arxiv.org/abs/2508.12617)
*Ming Li,Zihuai He,Min Zhang,Xiaowei Zhan,Changshuai Wei,Robert C Elston,Qing Lu*

Main category: stat.ME

TL;DR: The paper introduces the Generalized Genetic Random Field (GGRF) method for analyzing sequencing data, showing improved or comparable performance to existing methods in identifying rare genetic variant associations with diseases.


<details>
  <summary>Details</summary>
Motivation: Innovative analysis techniques are necessary to overcome the challenges posed by high-dimensional sequencing data in association studies of human diseases.

Method: The GGRF builds on the generalized estimating equation framework, allowing analysis of diverse phenotypes without threshold specifications for rare variants, and facilitates testing of multiple variants with varying effect directions and magnitudes.

Result: Simulations demonstrate that GGRF performs better or comparable to the commonly used SKAT method, especially in cases involving rare variants. Additionally, it successfully identified associations between specific genes and serum triglyceride levels in a real dataset from the Dallas Heart Study.

Conclusion: The GGRF method is an effective tool for sequencing data analysis, with broad applicability and robustness under diverse disease scenarios involving rare variants.

Abstract: With the advance of high-throughput sequencing technologies, it has become
feasible to investigate the influence of the entire spectrum of sequencing
variations on complex human diseases. Although association studies utilizing
the new sequencing technologies hold great promise to unravel novel genetic
variants, especially rare genetic variants that contribute to human diseases,
the statistical analysis of high-dimensional sequencing data remains a
challenge. Advanced analytical methods are in great need to facilitate
high-dimensional sequencing data analyses. In this article, we propose a
generalized genetic random field (GGRF) method for association analyses of
sequencing data. Like other similarity-based methods (e.g., SIMreg and SKAT),
the new method has the advantages of avoiding the need to specify thresholds
for rare variants and allowing for testing multiple variants acting in
different directions and magnitude of effects. The method is built on the
generalized estimating equation framework and thus accommodates a variety of
disease phenotypes (e.g., quantitative and binary phenotypes). Moreover, it has
a nice asymptotic property, and can be applied to small-scale sequencing data
without need for small-sample adjustment. Through simulations, we demonstrate
that the proposed GGRF attains an improved or comparable power over a commonly
used method, SKAT, under various disease scenarios, especially when rare
variants play a significant role in disease etiology. We further illustrate
GGRF with an application to a real dataset from the Dallas Heart Study. By
using GGRF, we were able to detect the association of two candidate genes,
ANGPTL3 and ANGPTL4, with serum triglyceride.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [561] [Generative AI in Training and Coaching: Redefining the Design Process of Learning Materials](https://arxiv.org/abs/2508.11662)
*Alexander Komar,Marc-André Heidelmann,Kristina Schaaff*

Main category: cs.CY

TL;DR: The study examines the transformative role of generative AI in education, focusing on its integration into teaching material design and the evolving role of educators.


<details>
  <summary>Details</summary>
Motivation: To understand how generative AI influences the design process of educational materials and the evolving responsibilities of educators and corporate trainers.

Method: The research employs qualitative interviews with professionals in education and corporate training to identify trends and impacts of generative AI.

Result: Key findings include that trainers act more as facilitators than content creators, efficiency improvements necessitate new skills, and AI anthropomorphism impacts trust and expectations.

Conclusion: Generative AI tools can enhance education if integrated on individual, organizational, and strategic levels, considering evolving roles and the required new skills of educators.

Abstract: Generative artificial intelligence (GenAI) is transforming education,
redefining the role of trainers and coaches in learning environments. In our
study, we explore how AI integrates into the design process of learning
materials, assessing its impact on efficiency, pedagogical quality, and the
evolving role of human trainers and coaches. Through qualitative interviews
with professionals in education and corporate training, we identify the
following key topics: trainers and coaches increasingly act as facilitators and
content moderators rather than primary creators, efficiency gains allow for a
stronger strategic focus but at the same time the new tools require new skills.
Additionally, we analyze how the anthropomorphism of AI shapes user trust and
expectations. From these insights, we derive how tools based on GenAI can
successfully be implemented for trainers and coaches on an individual,
organizational, systemic, and strategic level.

</details>


### [562] [Future progress in artificial intelligence: A survey of expert opinion](https://arxiv.org/abs/2508.11681)
*Vincent C. Müller,Nick Bostrom*

Main category: cs.CY

TL;DR: The paper investigates expert opinions on the timeline and risks associated with high-level machine intelligence and superintelligent AI.


<details>
  <summary>Details</summary>
Motivation: To clarify expert opinions on when high-level machine intelligence might develop, associated risks, and the speed of its progression.

Method: A questionnaire was distributed to four groups of experts in 2012/2013.

Result: Experts estimated a 50% chance of high-level machine intelligence by 2040-2050, rising to 90% by 2075, and transition to superintelligence within 30 years. One-third predicted negative outcomes for humanity.

Conclusion: Experts foresee significant probability of advanced AI within several decades, with notable risks to humanity, demanding further exploration and preparation.

Abstract: There is, in some quarters, concern about high-level machine intelligence and
superintelligent AI coming up in a few decades, bringing with it significant
risks for humanity. In other quarters, these issues are ignored or considered
science fiction. We wanted to clarify what the distribution of opinions
actually is, what probability the best experts currently assign to high-level
machine intelligence coming up within a particular time-frame, which risks they
see with that development, and how fast they see these developing. We thus
designed a brief questionnaire and distributed it to four groups of experts in
2012/2013. The median estimate of respondents was for a one in two chance that
high-level machine intelligence will be developed around 2040-2050, rising to a
nine in ten chance by 2075. Experts expect that systems will move on to
superintelligence in less than 30 years thereafter. They estimate the chance is
about one in three that this development turns out to be 'bad' or 'extremely
bad' for humanity.

</details>


### [563] [Real Time Child Abduction And Detection System](https://arxiv.org/abs/2508.11690)
*Tadisetty Sai Yashwanth,Yangalasetty Sruthi Royal,Vankayala Rajeshwari Shreya,Mayank Kashyap,Divyaprabha K N*

Main category: cs.CY

TL;DR: The paper develops an edge-based child abduction detection and alert system using Vision-Language Models (VLMs) deployed on Raspberry Pi devices, achieving high detection accuracy and near real-time alerts.


<details>
  <summary>Details</summary>
Motivation: Child abduction remains a significant threat to communities worldwide, and there is a need for efficient, fast, and scalable technological solutions for detection and prevention.

Method: The researchers designed an edge-based multi-agent system using Raspberry Pi connected to webcams. Vision-Language Models (VLMs) process real-time video feeds, and the system integrates with Twilio API to dispatch immediate alerts via SMS, WhatsApp, and calls in case of a detection.

Result: The system demonstrates a high level of accuracy in detecting possible abduction scenarios with performance near real-time, suitable for practical deployment.

Conclusion: The multi-agent, edge-based architecture improves situational data processing, ensuring scalability and cost-effectiveness. It provides an efficient and proactive tool to enhance child safety through monitoring and rapid alerting.

Abstract: Child safety continues to be a paramount concern worldwide, with child
abduction posing significant threats to communities. This paper presents the
development of an edge-based child abduction detection and alert system
utilizing a multi-agent framework where each agent incorporates Vision-Language
Models (VLMs) deployed on a Raspberry Pi. Leveraging the advanced capabilities
of VLMs within individual agents of a multi-agent team, our system is trained
to accurately detect and interpret complex interactions involving children in
various environments in real-time. The multi-agent system is deployed on a
Raspberry Pi connected to a webcam, forming an edge device capable of
processing video feeds, thereby reducing latency and enhancing privacy. An
integrated alert system utilizes the Twilio API to send immediate SMS and
WhatsApp notifications, including calls and messages, when a potential child
abduction event is detected. Experimental results demonstrate that the system
achieves high accuracy in detecting potential abduction scenarios, with near
real-time performance suitable for practical deployment. The multi-agent
architecture enhances the system's ability to process complex situational data,
improving detection capabilities over traditional single-model approaches. The
edge deployment ensures scalability and cost-effectiveness, making it
accessible for widespread use. The proposed system offers a proactive solution
to enhance child safety through continuous monitoring and rapid alerting,
contributing a valuable tool in efforts to prevent child abductions.

</details>


### [564] [Next-Gen Education: Enhancing AI for Microlearning](https://arxiv.org/abs/2508.11704)
*Suman Saha,Fatemeh Rahbari,Farhan Sadique,Sri Krishna Chaitanya Velamakanni,Mahfuza Farooque,William J. Rothwell*

Main category: cs.CY

TL;DR: The paper suggests using microlearning strategies, supported by AI like ChatGPT, to enhance computer science education and engagement amidst challenges after COVID-19.


<details>
  <summary>Details</summary>
Motivation: To address declining class attendance and engagement in US universities post-COVID, particularly as students favor remote and asynchronous learning.

Method: The authors propose microlearning, breaking complex topics into digestible units via interactive formats, and leverage ChatGPT to automate the creation of supplementary materials.

Result: This approach reduces educators' effort in material creation, ensures updated course content, and enhances engagement with computer science topics requiring deep understanding and practice.

Conclusion: Combining AI tools like ChatGPT with educational strategies like microlearning can improve engagement and transform practices in computer science education, while maintaining the essential role of educators.

Abstract: This paper explores integrating microlearning strategies into university
curricula, particularly in computer science education, to counteract the
decline in class attendance and engagement in US universities after COVID. As
students increasingly opt for remote learning and recorded lectures,
traditional educational approaches struggle to maintain engagement and
effectiveness. Microlearning, which breaks complex subjects into manageable
units, is proposed to address shorter attention spans and enhance educational
outcomes. It uses interactive formats such as videos, quizzes, flashcards, and
scenario-based exercises, which are especially beneficial for topics like
algorithms and programming logic requiring deep understanding and ongoing
practice. Adoption of microlearning is often limited by the effort needed to
create such materials. This paper proposes leveraging AI tools, specifically
ChatGPT, to reduce the workload for educators by automating the creation of
supplementary materials. While AI can automate certain tasks, educators remain
essential in guiding and shaping the learning process. This AI-enhanced
approach ensures course content is kept current with the latest research and
technology, with educators providing context and insights. By examining AI
capabilities in microlearning, this study shows the potential to transform
educational practices and outcomes in computer science, offering a practical
model for combining advanced technology with established teaching methods.

</details>


### [565] [Listening with Language Models: Using LLMs to Collect and Interpret Classroom Feedback](https://arxiv.org/abs/2508.11707)
*Sai Siddartha Maram,Ulia Zaman,Magy Seif El-Nasr*

Main category: cs.CY

TL;DR: This paper explores using LLM chatbots for gathering classroom feedback through conversational dialogues, finding them more insightful and engaging than traditional surveys.


<details>
  <summary>Details</summary>
Motivation: Traditional end-of-quarter surveys fail to provide timely and actionable feedback, limiting opportunities for instructors to adapt their teaching methods effectively.

Method: The study deployed a three-part system—PromptDesigner, FeedbackCollector, and FeedbackAnalyzer—in two graduate courses, leveraging LLM chatbots for reflective student dialogues.

Result: The LLM feedback system generated more detailed, contextually relevant, and engaging insights. Instructors appreciated its specificity and adaptability, while students liked the conversational approach.

Conclusion: Using LLM-powered chatbots for feedback enhances the quality and responsiveness of student-instructor interaction, promoting meaningful mid-course improvements and better dialogue in higher education.

Abstract: Traditional end-of-quarter surveys often fail to provide instructors with
timely, detailed, and actionable feedback about their teaching. In this paper,
we explore how Large Language Model (LLM)-powered chatbots can reimagine the
classroom feedback process by engaging students in reflective, conversational
dialogues. Through the design and deployment of a three-part
system-PromptDesigner, FeedbackCollector, and FeedbackAnalyzer-we conducted a
pilot study across two graduate courses at UC Santa Cruz. Our findings suggest
that LLM-based feedback systems offer richer insights, greater contextual
relevance, and higher engagement compared to standard survey tools. Instructors
valued the system's adaptability, specificity, and ability to support
mid-course adjustments, while students appreciated the conversational format
and opportunity for elaboration. We conclude by discussing the design
implications of using AI to facilitate more meaningful and responsive feedback
in higher education.

</details>


### [566] [Street Review: A Participatory AI-Based Framework for Assessing Streetscape Inclusivity](https://arxiv.org/abs/2508.11708)
*Rashid Mushkani,Shin Koseki*

Main category: cs.CY

TL;DR: The paper introduces a mixed-method framework to evaluate public spaces' inclusivity by combining participatory research and AI-based analysis, applied to street views and user feedback in Montréal.


<details>
  <summary>Details</summary>
Motivation: Urban centers face evolving use of public spaces due to demographic, social, and cultural shifts, necessitating tools to assess inclusivity in streetscapes.

Method: The study used semi-directed interviews with 28 residents, evaluations of approximately 45,000 street-view images using AI tools, and developed visual analytics like heatmaps correlating user perceptions and physical attributes.

Result: Findings showed varying inclusivity and accessibility perceptions across demographic lines, suggesting user feedback improves machine learning models and data-labeling approaches.

Conclusion: The Street Review framework is proposed as a systematic tool for urban planning and policy-making to enhance inclusivity in public streets.

Abstract: Urban centers undergo social, demographic, and cultural changes that shape
public street use and require systematic evaluation of public spaces. This
study presents Street Review, a mixed-methods approach that combines
participatory research with AI-based analysis to assess streetscape
inclusivity. In Montr\'eal, Canada, 28 residents participated in semi-directed
interviews and image evaluations, supported by the analysis of approximately
45,000 street-view images from Mapillary. The approach produced visual
analytics, such as heatmaps, to correlate subjective user ratings with physical
attributes like sidewalk, maintenance, greenery, and seating. Findings reveal
variations in perceptions of inclusivity and accessibility across demographic
groups, demonstrating that incorporating diverse user feedback can enhance
machine learning models through careful data-labeling and co-production
strategies. The Street Review framework offers a systematic method for urban
planners and policy analysts to inform planning, policy development, and
management of public streets.

</details>


### [567] [Navigating the New Landscape: A Conceptual Model for Project-Based Assessment (PBA) in the Age of GenAI](https://arxiv.org/abs/2508.11709)
*Rajan Kadel,Samar Shailendra,Urvashi Rahul Saxena*

Main category: cs.CY

TL;DR: The paper proposes a new assessment model for Project-Based Learning (PBL) that addresses challenges posed by Generative Artificial Intelligence (GenAI) in higher education.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges GenAI poses for authentic assessment, academic integrity, and learning in Project-Based Assessments, especially as its use proliferates in education.

Method: The study proposes a process-oriented evaluation model that integrates multi-modal assessments, GenAI-assisted feedback, and ethical practice, illustrated by a use case scenario in a capstone project.

Result: The proposed model emphasizes higher-order thinking, ethical GenAI use, and personalised supervisor feedback to align assessments with integrity and learner-centric goals.

Conclusion: The paper provides actionable recommendations for educators and designers to adapt their assessment practices to ensure robustness and adaptability in the GenAI context.

Abstract: The rapid integration of Generative Artificial Intelligence (GenAI) into
higher education presents both opportunities and challenges for assessment
design, particularly within Project-Based Assessment (PBA) contexts.
Traditional assessment methods often emphasise the final product in the PBA,
which can now be significantly influenced or created by GenAI tools, raising
concerns regarding product authenticity, academic integrity, and learning
validation. This paper advocates for a reimagined assessment model for
Project-Based Learning (PBL) or a capstone project that prioritises
process-oriented evaluation, multi-modal and multifaceted assessment design,
and ethical engagement with GenAI to enable higher-order thinking. The model
also emphasises the use of (GenAI-assisted) personalised feedback by a
supervisor as an observance of the learning process during the project
lifecycle. A use case scenario is provided to illustrate the application of the
model in a capstone project setting. The paper concludes with recommendations
for educators and curriculum designers to ensure that assessment practices
remain robust, learner-centric, and integrity-driven in the evolving landscape
of GenAI.

</details>


### [568] [Are AI Machines Making Humans Obsolete?](https://arxiv.org/abs/2508.11719)
*Matthias Scheutz*

Main category: cs.CY

TL;DR: The chapter outlines the emergence and impacts of generative AI, explores its opportunities and risks, and suggests ways to mitigate dangers.


<details>
  <summary>Details</summary>
Motivation: To examine the evolution, impacts, and challenges of generative AI, and to propose solutions for its safer use.

Method: The chapter analyzes past developments, current impacts, and potential risks of generative AI, emphasizing ethical and societal challenges.

Result: The study identifies both opportunities for growth and risks like dystopian outcomes due to uncontrolled AI systems.

Conclusion: Proposes strategies to control generative AI, aiming to mitigate its dangers and ensure responsible implementation.

Abstract: This chapter starts with a sketch of how we got to "generative AI" (GenAI)
and a brief summary of the various impacts it had so far. It then discusses
some of the opportunities of GenAI, followed by the challenges and dangers,
including dystopian outcomes resulting from using uncontrolled machine learning
and our failures to understand the results. It concludes with some suggestions
for how to control GenAI and address its dangers.

</details>


### [569] [The Stories We Govern By: AI, Risk, and the Power of Imaginaries](https://arxiv.org/abs/2508.11729)
*Ninell Oldenburg,Gleb Papyshev*

Main category: cs.CY

TL;DR: The paper explores how different narratives about AI risks influence governance and policy decisions, categorizing them into existential risk, accelerationist, and critical AI perspectives.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how divergent sociotechnical narratives around AI risks impact regulatory approaches and governance, aiming to inform more inclusive policy-making.

Method: The authors analyze representative texts to compare three AI risk narratives across dimensions like visions of the future, current social diagnoses, views on science/technology, and human agency.

Result: Findings reveal that these narratives embed distinct assumptions on risk, shaping policy-making by limiting alternative governance approaches.

Conclusion: The paper advocates for moving beyond deterministic and speculative narratives, emphasizing pragmatic regulatory strategies to address AI risks effectively.

Abstract: This paper examines how competing sociotechnical imaginaries of artificial
intelligence (AI) risk shape governance decisions and regulatory constraints.
Drawing on concepts from science and technology studies, we analyse three
dominant narrative groups: existential risk proponents, who emphasise
catastrophic AGI scenarios; accelerationists, who portray AI as a
transformative force to be unleashed; and critical AI scholars, who foreground
present-day harms rooted in systemic inequality. Through an analysis of
representative manifesto-style texts, we explore how these imaginaries differ
across four dimensions: normative visions of the future, diagnoses of the
present social order, views on science and technology, and perceived human
agency in managing AI risks. Our findings reveal how these narratives embed
distinct assumptions about risk and have the potential to progress into
policy-making processes by narrowing the space for alternative governance
approaches. We argue against speculative dogmatism and for moving beyond
deterministic imaginaries toward regulatory strategies that are grounded in
pragmatism.

</details>


### [570] [Artificial Intelligence in Rural Healthcare Delivery: Bridging Gaps and Enhancing Equity through Innovation](https://arxiv.org/abs/2508.11738)
*Kiruthika Balakrishnan,Durgadevi Velusamy,Hana E. Hinkle,Zhi Li,Karthikeyan Ramasamy,Hikmat Khan,Srini Ramaswamy,Pir Masoom Shah*

Main category: cs.CY

TL;DR: The study reviews the potential of AI in solving issues faced by rural healthcare and suggests its transformative applications, while identifying existing barriers and recommending solutions.


<details>
  <summary>Details</summary>
Motivation: To address persistent challenges in rural healthcare, including workforce shortages and infrastructure inadequacies, and evaluate AI's potential in mitigating these issues.

Method: The study conducted a systematic review of 109 studies from multiple databases (e.g., PubMed, IEEE Xplore) using PRISMA guidelines and thematic analysis to identify patterns related to AI's integration in rural healthcare.

Result: AI applications such as LLMs and MFMs show promise in improving healthcare accessibility, quality, and efficiency, though challenges like infrastructure limitations and ethical concerns persist.

Conclusion: AI has transformative potential in rural healthcare but successful integration requires interdisciplinary efforts, infrastructure investment, and regulatory frameworks to address existing challenges.

Abstract: Rural healthcare faces persistent challenges, including inadequate
infrastructure, workforce shortages, and socioeconomic disparities that hinder
access to essential services. This study investigates the transformative
potential of artificial intelligence (AI) in addressing these issues in
underserved rural areas. We systematically reviewed 109 studies published
between 2019 and 2024 from PubMed, Embase, Web of Science, IEEE Xplore, and
Scopus. Articles were screened using PRISMA guidelines and Covidence software.
A thematic analysis was conducted to identify key patterns and insights
regarding AI implementation in rural healthcare delivery. The findings reveal
significant promise for AI applications, such as predictive analytics,
telemedicine platforms, and automated diagnostic tools, in improving healthcare
accessibility, quality, and efficiency. Among these, advanced AI systems,
including Multimodal Foundation Models (MFMs) and Large Language Models (LLMs),
offer particularly transformative potential. MFMs integrate diverse data
sources, such as imaging, clinical records, and bio signals, to support
comprehensive decision-making, while LLMs facilitate clinical documentation,
patient triage, translation, and virtual assistance. Together, these
technologies can revolutionize rural healthcare by augmenting human capacity,
reducing diagnostic delays, and democratizing access to expertise. However,
barriers remain, including infrastructural limitations, data quality concerns,
and ethical considerations. Addressing these challenges requires
interdisciplinary collaboration, investment in digital infrastructure, and the
development of regulatory frameworks. This review offers actionable
recommendations and highlights areas for future research to ensure equitable
and sustainable integration of AI in rural healthcare systems.

</details>


### [571] [Singing Syllabi with Virtual Avatars: Enhancing Student Engagement Through AI-Generated Music and Digital Embodiment](https://arxiv.org/abs/2508.11872)
*Xinxing Wu*

Main category: cs.CY

TL;DR: The study introduces AI-generated singing with virtual avatars to present course syllabi in an engaging and memorable audiovisual format.


<details>
  <summary>Details</summary>
Motivation: Students often overlook or fail to fully grasp traditional text-based syllabi guidelines and policies, leading to missed key course details.

Method: The researchers utilized HeyGem, an open-source tool, to convert textual syllabi into audiovisual content featuring digital avatars singing the syllabus material.

Result: Student feedback revealed that this method significantly enhanced their awareness and recall of critical syllabus information.

Conclusion: The approach of using AI-generated singing avatars effectively engages students and improves learning outcomes through enhanced information retention.

Abstract: In practical teaching, we observe that few students thoroughly read or fully
comprehend the information provided in traditional, text-based course syllabi.
As a result, essential details, such as course policies and learning outcomes,
are frequently overlooked. To address this challenge, in this paper, we propose
a novel approach leveraging AI-generated singing and virtual avatars to present
syllabi in a format that is more visually appealing, engaging, and memorable.
Especially, we leveraged the open-source tool, HeyGem, to transform textual
syllabi into audiovisual presentations, in which digital avatars perform the
syllabus content as songs. The proposed approach aims to stimulate students'
curiosity, foster emotional connection, and enhance retention of critical
course information. Student feedback indicated that AI-sung syllabi
significantly improved awareness and recall of key course information.

</details>


### [572] [SimInterview: Transforming Business Education through Large Language Model-Based Simulated Multilingual Interview Training System](https://arxiv.org/abs/2508.11873)
*Truong Thanh Hung Nguyen,Tran Diem Quynh Nguyen,Hoang Loc Cao,Thi Cam Thanh Tran,Thi Cam Mai Truong,Hung Cao*

Main category: cs.CY

TL;DR: SimInterview utilizes cutting-edge LLM and synthetic AI technologies to provide multilingual and realistic interview training tailored to resumes and specific job requirements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in conventional interview preparation methods, which lack cultural awareness and individualization needed by business professionals entering a labor market transformed by AI.

Method: SimInterview uses LLM-based frameworks, retrieval-augmented generation, Whisper speech recognition, GPT-SoVITS voice synthesis, and other AI tools to simulate real-time, personalized interviews in multiple languages.

Result: Experiments with university-level candidates demonstrated accurate job alignment, resume preservation, engaging conversations, and adaptability to cultural differences, particularly between Japanese and English markets.

Conclusion: SimInterview sets a new standard for AI-driven interview preparation, achieving user satisfaction and emphasizing contestable AI design while adhering to regulatory expectations.

Abstract: Business interview preparation demands both solid theoretical grounding and
refined soft skills, yet conventional classroom methods rarely deliver the
individualized, culturally aware practice employers currently expect. This
paper introduces SimInterview, a large language model (LLM)-based simulated
multilingual interview training system designed for business professionals
entering the AI-transformed labor market. Our system leverages an LLM agent and
synthetic AI technologies to create realistic virtual recruiters capable of
conducting personalized, real-time conversational interviews. The framework
dynamically adapts interview scenarios using retrieval-augmented generation
(RAG) to match individual resumes with specific job requirements across
multiple languages. Built on LLMs (OpenAI o3, Llama 4 Maverick, Gemma 3),
integrated with Whisper speech recognition, GPT-SoVITS voice synthesis, Ditto
diffusion-based talking head generation model, and ChromaDB vector databases,
our system significantly improves interview readiness across English and
Japanese markets. Experiments with university-level candidates show that the
system consistently aligns its assessments with job requirements, faithfully
preserves resume content, and earns high satisfaction ratings, with the
lightweight Gemma 3 model producing the most engaging conversations.
Qualitative findings revealed that the standardized Japanese resume format
improved document retrieval while diverse English resumes introduced additional
variability, and they highlighted how cultural norms shape follow-up
questioning strategies. Finally, we also outlined a contestable AI design that
can explain, detect bias, and preserve human-in-the-loop to meet emerging
regulatory expectations.

</details>


### [573] [Predicting ChatGPT Use in Assignments: Implications for AI-Aware Assessment Design](https://arxiv.org/abs/2508.12013)
*Surajit Das,Aleksei Eliseev*

Main category: cs.CY

TL;DR: The paper investigates how generative AI tools like ChatGPT are influencing academic practices, based on a survey of 388 university students, using XGBoost to model their usage patterns.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the lack of quantitative analysis on how generative AI, such as ChatGPT, impacts real-world student behavior in assignment preparation and academic practices.

Method: Survey data from 388 university students were analyzed, and the XGBoost algorithm was employed to predict factors influencing ChatGPT usage, achieving strong performance metrics.

Result: Binary classification reached 80.1% test accuracy, while a multiclass classifier achieved 64.5% test accuracy. The study indicated a correlation between ChatGPT use for learning and potential overreliance.

Conclusion: While generative AI enhances knowledge accessibility, reliance on it may harm critical thinking and originality. Discipline-specific guidelines and revised assessment strategies are recommended for ethical integration of AI in education.

Abstract: The rise of generative AI tools like ChatGPT has significantly reshaped
education, sparking debates about their impact on learning outcomes and
academic integrity. While prior research highlights opportunities and risks,
there remains a lack of quantitative analysis of student behavior when
completing assignments. Understanding how these tools influence real-world
academic practices, particularly assignment preparation, is a pressing and
timely research priority.
  This study addresses this gap by analyzing survey responses from 388
university students, primarily from Russia, including a subset of international
participants. Using the XGBoost algorithm, we modeled predictors of ChatGPT
usage in academic assignments. Key predictive factors included learning habits,
subject preferences, and student attitudes toward AI. Our binary classifier
demonstrated strong predictive performance, achieving 80.1\% test accuracy,
with 80.2\% sensitivity and 79.9\% specificity. The multiclass classifier
achieved 64.5\% test accuracy, 64.6\% weighted precision, and 64.5\% recall,
with similar training scores, indicating potential data scarcity challenges.
  The study reveals that frequent use of ChatGPT for learning new concepts
correlates with potential overreliance, raising concerns about long-term
academic independence. These findings suggest that while generative AI can
enhance access to knowledge, unchecked reliance may erode critical thinking and
originality. We propose discipline-specific guidelines and reimagined
assessment strategies to balance innovation with academic rigor. These insights
can guide educators and policymakers in ethically and effectively integrating
AI into education.

</details>


### [574] [Large Language Models Enable Personalized Nudges to Promote Carbon Offsetting Among Air Travellers](https://arxiv.org/abs/2508.12045)
*Vladimir Maksimenko,Qingyao Xin,Prateek Gupta,Bin Zhang,Prateek Bansal*

Main category: cs.CY

TL;DR: The paper explores how large language models (LLMs) can be used to design personalized nudge strategies that encourage air travellers to offset CO2 emissions, showing their effectiveness through real-world surveys.


<details>
  <summary>Details</summary>
Motivation: To address the lack of personalization in nudge strategies, which affects their efficacy in promoting sustainable behaviours like CO2 offsetting in aviation.

Method: LLMs were used to design decoy-based personalized nudges. Their effectiveness was validated through 3495 surveys conducted in five countries: China, Germany, India, Singapore, and the United States.

Result: LLM-informed personalized nudges increased offsetting rates by 3-7%, potentially mitigating 2.3 million tonnes of CO2 annually. This effect was notably stronger among sceptical travellers with low trust in offset programs.

Conclusion: Personalized nudges designed via LLMs are a promising tool for enhancing commitment to CO2 mitigation in aviation, particularly for travellers with low trust, thereby accelerating decarbonization efforts.

Abstract: Nudge strategies are effective tools for promoting sustainable behaviour, but
their impact depends on individual preferences. By emulating human
decision-making, large language models (LLMs) offer a cost-effective route for
tailoring nudges without extensive behavioural datasets, yet this potential
remains unexplored. Focusing on aviation, we use LLMs to design personalized
decoy-based nudge strategies that encourage air travellers to voluntarily
offset CO$_2$ emissions from flights, and validate their efficacy through 3495
surveys from China, Germany, India, Singapore, and the United States. Results
show that LLM-informed personalized nudges are more effective than uniform
settings, raising offsetting rates by 3-7$\%$ and yielding an additional 2.3
million tonnes of CO$_2$ mitigated annually in aviation. This improvement is
driven primarily by increased participation among sceptical travellers with low
trust in offset programmes. Our study highlights the potential of LLM-driven
personalized nudging strategies for boosting offsetting behaviours to
accelerate aviation decarbonization.

</details>


### [575] [Mutually Assured Deregulation](https://arxiv.org/abs/2508.12300)
*Gilad Abiri*

Main category: cs.CY

TL;DR: Current AI deregulation trends (Regulation Sacrifice) emphasize speed and national security through unsafe practices, but this approach leads to increased risks and undermines innovation, safety, and stability.


<details>
  <summary>Details</summary>
Motivation: Nations seek AI dominance by dismantling safety oversight due to fears of losing competitive advantage to rivals like China or the U.S.

Method: The essay critiques the concept of Regulation Sacrifice, exposing its flaws using evidence and comparisons with other industries, such as automotive regulation.

Result: The paper identifies that weak oversight results in temporary advantages, restricts innovation, creates liabilities, and introduces security risks, such as uncontrollable AGI and bioweapons.

Conclusion: For effective AI security and global safety, stronger regulations and cooperation are essential, as deregulation breeds mutual vulnerabilities instead of competitive strength.

Abstract: We have convinced ourselves that the way to make AI safe is to make it
unsafe. Since 2022, policymakers worldwide have embraced the Regulation
Sacrifice - the belief that dismantling safety oversight will deliver security
through AI dominance. Fearing China or USA will gain advantage, nations rush to
eliminate safeguards that might slow progress. This Essay reveals the fatal
flaw: though AI poses national security challenges, the solution demands
stronger regulatory frameworks, not weaker ones. A race without guardrails
breeds shared danger, not competitive strength. The Regulation Sacrifice makes
three false promises. First, it promises durable technological leads. But AI
capabilities spread rapidly - performance gaps between U.S. and Chinese systems
collapsed from 9 percent to 2 percent in thirteen months. When advantages
evaporate in months, sacrificing permanent safety for temporary speed makes no
sense. Second, it promises deregulation accelerates innovation. The opposite
often proves true. Companies report well-designed governance streamlines
development. Investment flows toward regulated markets. Clear rules reduce
uncertainty; uncertain liability creates paralysis. Environmental standards did
not kill the auto industry; they created Tesla and BYD. Third, enhanced
national security through deregulation actually undermines security across all
timeframes. Near term: it hands adversaries information warfare tools. Medium
term: it democratizes bioweapon capabilities. Long term: it guarantees
deployment of uncontrollable AGI systems. The Regulation Sacrifice persists
because it serves powerful interests, not security. Tech companies prefer
freedom to accountability. Politicians prefer simple stories to complex truths.
This creates mutually assured deregulation, where each nation's sprint for
advantage guarantees collective vulnerability. The only way to win is not to
play.

</details>


### [576] [Vitamin N: Benefits of Different Forms of Public Greenery for Urban Health](https://arxiv.org/abs/2508.12998)
*Sanja Šćepanović,Sagar Joglekar,Stephen Law,Daniele Quercia,Ke Zhou,Alice Battiston,Rossano Schifanella*

Main category: cs.CY

TL;DR: This paper highlights the stronger health benefits of on-road greenery, seen in daily life, over off-road greenery, using detailed greenery data and medical prescription analysis.


<details>
  <summary>Details</summary>
Motivation: Urban greenery is linked to better health, but existing research provides inconsistent outcomes, largely due to official greenery metrics that overlook everyday visibility and usage.

Method: The study combined aerial imagery, OpenStreetMap data, Google Street View images, and road accessibility information to classify on-road and off-road greenery. It then linked these classifications to 7.45 billion medical prescriptions for five health conditions.

Result: On-road greenery showed a stronger correlation with better health outcomes than traditional greenery metrics. For instance, hypertension prescriptions dropped by 3.68% in areas with above-median on-road greenery levels. Citywide adjustments could save up to £3.15 million annually in prescription costs.

Conclusion: Greenery observed in daily life, especially on-road greenery, is more impactful on public health than secluded greenery. Current official greenery metrics have significant room for improvement.

Abstract: Urban greenery is often linked to better health, yet findings from past
research have been inconsistent. One reason is that official greenery metrics
measure the amount or nearness of greenery but ignore how often people actually
may potentially see or use it in daily life. To address this gap, we introduced
a new classification that separates on-road greenery, which people see while
walking through streets, from off-road greenery, which requires planned visits.
We did so by combining aerial imagery of Greater London and greenery data from
OpenStreetMap with quantified greenery from over 100,000 Google Street View
images and accessibility estimates based on 160,000 road segments. We linked
these measures to 7.45 billion medical prescriptions issued by the National
Health Service and processed through our methodology. These prescriptions cover
five conditions: diabetes, hypertension, asthma, depression, and anxiety, as
well as opioid use. As hypothesized, we found that green on-road was more
strongly linked to better health than four widely used official measures. For
example, hypertension prescriptions dropped by 3.68% in wards with on-road
greenery above the median citywide level compared to those below it. If all
below-median wards reached the citywide median in on-road greenery,
prescription costs could fall by up to {\pounds}3.15 million each year. These
results suggest that greenery seen in daily life may be more relevant than
public yet secluded greenery, and that official metrics commonly used in the
literature have important limitations.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [577] [Centralized Permutation Equivariant Policy for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.11706)
*Zhuofan Xu,Benedikt Bollig,Matthias Függer,Thomas Nowak,Vincent Le Dréau*

Main category: cs.MA

TL;DR: The paper introduces a centralized training and execution framework utilizing Centralized Permutation Equivariant (CPE) learning to enhance performance in multi-agent reinforcement learning (MARL).


<details>
  <summary>Details</summary>
Motivation: Despite the popularity of the Centralized Training with Decentralized Execution (CTDE) approach in MARL, decentralized policies under partial observability often underperform, while centralized policies struggle with scalability issues. The authors aim to address these challenges.

Method: The authors propose a novel Centralized Permutation Equivariant learning framework using Global-Local Permutation Equivariant (GLPE) networks. This design is lightweight, scalable, and integrates easily with value decomposition and actor-critic methods.

Result: The CPE framework significantly boosts the performance of standard CTDE algorithms across key cooperative benchmarks like MPE, SMAC, and RWARE. Its performance matches state-of-the-art implementations in RWARE.

Conclusion: Centralized Permutation Equivariant (CPE) learning is a viable alternative to CTDE, offering scalability, simplicity, and improved performance in multi-agent reinforcement learning tasks.

Abstract: The Centralized Training with Decentralized Execution (CTDE) paradigm has
gained significant attention in multi-agent reinforcement learning (MARL) and
is the foundation of many recent algorithms. However, decentralized policies
operate under partial observability and often yield suboptimal performance
compared to centralized policies, while fully centralized approaches typically
face scalability challenges as the number of agents increases.
  We propose Centralized Permutation Equivariant (CPE) learning, a centralized
training and execution framework that employs a fully centralized policy to
overcome these limitations. Our approach leverages a novel permutation
equivariant architecture, Global-Local Permutation Equivariant (GLPE) networks,
that is lightweight, scalable, and easy to implement. Experiments show that CPE
integrates seamlessly with both value decomposition and actor-critic methods,
substantially improving the performance of standard CTDE algorithms across
cooperative benchmarks including MPE, SMAC, and RWARE, and matching the
performance of state-of-the-art RWARE implementations.

</details>


### [578] [SafeSieve: From Heuristics to Experience in Progressive Pruning for LLM-based Multi-Agent Communication](https://arxiv.org/abs/2508.11733)
*Ruijia Zhang,Xinyan Zhao,Ruixiang Wang,Sigen Chen,Guibin Zhang,An Zhang,Kun Wang,Qingsong Wen*

Main category: cs.MA

TL;DR: SafeSieve is a pruning algorithm for LLM-based multi-agent systems that reduces communication overhead while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based multi-agent systems suffer from redundant communication and excessive token usage, with current methods having isolated pre- and post-task optimization strategies.

Method: SafeSieve refines inter-agent communication dynamically using a dual-mechanism that combines LLM-based semantic evaluation and performance feedback, as well as 0-extension clustering to preserve agent group structure.

Result: Experiments demonstrate SafeSieve achieves 94.01% average accuracy, reduces token usage by 12.4%-27.8%, and maintains robustness against prompt injection attacks with only a 1.23% accuracy drop. It also lowers deployment costs by 13.3% in heterogeneous settings.

Conclusion: SafeSieve proves to be a robust, efficient, and scalable framework for enhancing the practicality of multi-agent systems by reducing resource overhead without compromising performance.

Abstract: LLM-based multi-agent systems exhibit strong collaborative capabilities but
often suffer from redundant communication and excessive token overhead.
Existing methods typically enhance efficiency through pretrained GNNs or greedy
algorithms, but often isolate pre- and post-task optimization, lacking a
unified strategy. To this end, we present SafeSieve, a progressive and adaptive
multi-agent pruning algorithm that dynamically refines the inter-agent
communication through a novel dual-mechanism. SafeSieve integrates initial
LLM-based semantic evaluation with accumulated performance feedback, enabling a
smooth transition from heuristic initialization to experience-driven
refinement. Unlike existing greedy Top-k pruning methods, SafeSieve employs
0-extension clustering to preserve structurally coherent agent groups while
eliminating ineffective links. Experiments across benchmarks (SVAMP, HumanEval,
etc.) showcase that SafeSieve achieves 94.01% average accuracy while reducing
token usage by 12.4%-27.8%. Results further demonstrate robustness under prompt
injection attacks (1.23% average accuracy drop). In heterogeneous settings,
SafeSieve reduces deployment costs by 13.3% while maintaining performance.
These results establish SafeSieve as a robust, efficient, and scalable
framework for practical multi-agent systems. Our code can be found in
https://anonymous.4open.science/r/SafeSieve-D8F2FFUN.

</details>


### [579] [A Comprehensive Review of AI Agents: Transforming Possibilities in Technology and Beyond](https://arxiv.org/abs/2508.11957)
*Xiaodong Qu,Andrews Damoah,Joshua Sherwood,Peiyan Liu,Christian Shun Jin,Lulu Chen,Minjie Shen,Nawwaf Aleisa,Zeyuan Hou,Chenyu Zhang,Lifu Gao,Yanshu Li,Qikai Yang,Qun Wang,Cristabelle De Souza*

Main category: cs.MA

TL;DR: The paper reviews architectural principles and frameworks for designing unified AI agents, synthesizing insights from cognitive science, reinforcement learning, and large language models while addressing ethical, safety, and interpretability concerns.


<details>
  <summary>Details</summary>
Motivation: The aim is to tackle the challenge of designing and deploying seamless, unified AI agents capable of cognition, planning, and interaction within complex environments.

Method: The approach involves a systematic review of architectural principles, components, and paradigms shaping modern AI agents and synthesis of insights from related fields.

Result: The review highlights major breakthroughs, challenges, and potential research directions that contribute to making AI agents more robust, adaptable, and trustworthy.

Conclusion: The paper aims to provide a guide for developing the next generation of AI systems with improved robustness, adaptability, and trustworthiness, addressing ethical and safety concerns.

Abstract: Artificial Intelligence (AI) agents have rapidly evolved from specialized,
rule-based programs to versatile, learning-driven autonomous systems capable of
perception, reasoning, and action in complex environments. The explosion of
data, advances in deep learning, reinforcement learning, and multi-agent
coordination have accelerated this transformation. Yet, designing and deploying
unified AI agents that seamlessly integrate cognition, planning, and
interaction remains a grand challenge. In this review, we systematically
examine the architectural principles, foundational components, and emergent
paradigms that define the landscape of contemporary AI agents. We synthesize
insights from cognitive science-inspired models, hierarchical reinforcement
learning frameworks, and large language model-based reasoning. Moreover, we
discuss the pressing ethical, safety, and interpretability concerns associated
with deploying these agents in real-world scenarios. By highlighting major
breakthroughs, persistent challenges, and promising research directions, this
review aims to guide the next generation of AI agent systems toward more
robust, adaptable, and trustworthy autonomous intelligence.

</details>


### [580] [Synchronization Dynamics of Heterogeneous, Collaborative Multi-Agent AI Systems](https://arxiv.org/abs/2508.12314)
*Chiranjit Mitra*

Main category: cs.MA

TL;DR: This paper introduces a framework combining synchronization theory and multi-agent AI systems using the Kuramoto model to study the collective dynamics of heterogeneous AI agents.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand and optimize the behavior of multi-agent AI systems, focusing on synchronization and efficiency in collaborative tasks.

Method: The approach models AI agents as coupled oscillators with phase and amplitude dynamics, using simulations on network structures to examine coupling strength, agent diversity, and topology.

Result: Key results show that stronger coupling leads to robust synchronization in heterogeneous AI capabilities, validated through simulations on different network types.

Conclusion: The study provides a mathematical basis for designing scalable and adaptive multi-agent AI systems, with future potential for incorporating learning dynamics and network adaptations.

Abstract: We present a novel interdisciplinary framework that bridges synchronization
theory and multi-agent AI systems by adapting the Kuramoto model to describe
the collective dynamics of heterogeneous AI agents engaged in complex task
execution. By representing AI agents as coupled oscillators with both phase and
amplitude dynamics, our model captures essential aspects of agent
specialization, influence, and communication within networked systems. We
introduce an order parameter to quantify the degree of coordination and
synchronization, providing insights into how coupling strength, agent
diversity, and network topology impact emergent collective behavior.
Furthermore, we formalize a detailed correspondence between Chain-of-Thought
prompting in AI reasoning and synchronization phenomena, unifying human-like
iterative problem solving with emergent group intelligence. Through extensive
simulations on all-to-all and deterministic scale-free networks, we demonstrate
that increased coupling promotes robust synchronization despite heterogeneous
agent capabilities, reflecting realistic collaborative AI scenarios. Our
physics-informed approach establishes a rigorous mathematical foundation for
designing, analyzing, and optimizing scalable, adaptive, and interpretable
multi-agent AI systems. This work opens pathways for principled orchestration
of agentic AI and lays the groundwork for future incorporation of learning
dynamics and adaptive network architectures to further enhance system
resilience and efficiency.

</details>


### [581] [A Taxonomy of Hierarchical Multi-Agent Systems: Design Patterns, Coordination Mechanisms, and Industrial Applications](https://arxiv.org/abs/2508.12683)
*David J. Moore*

Main category: cs.MA

TL;DR: The paper introduces a taxonomy for hierarchical multi-agent systems (HMAS), considering five dimensions to aid design and comparison, while addressing coordination challenges in industrial settings.


<details>
  <summary>Details</summary>
Motivation: The paper aims to better understand and compare designs for hierarchical multi-agent systems (HMAS), which simplify coordination and scale, yet introduce trade-offs.

Method: The authors propose a multi-dimensional taxonomy spanning five axes (control hierarchy, information flow, delegation, temporal layering, communication), connected to classical and modern coordination techniques.

Result: The taxonomy is applied to real-world contexts, such as power grids and oilfields, showing how hierarchical structures manage complexity efficiently while preserving autonomy.

Conclusion: This unified taxonomy bridges traditional coordination mechanisms with modern AI approaches, but challenges like explainability, scalability, and integrating AI models remain.

Abstract: Hierarchical multi-agent systems (HMAS) organize collections of agents into
layered structures that help manage complexity and scale. These hierarchies can
simplify coordination, but they also can introduce trade-offs that are not
always obvious. This paper proposes a multi-dimensional taxonomy for HMAS along
five axes: control hierarchy, information flow, role and task delegation,
temporal layering, and communication structure. The intent is not to prescribe
a single "best" design but to provide a lens for comparing different
approaches.
  Rather than treating these dimensions in isolation, the taxonomy is connected
to concrete coordination mechanisms - from the long-standing contract-net
protocol for task allocation to more recent work in hierarchical reinforcement
learning. Industrial contexts illustrate the framework, including power grids
and oilfield operations, where agents at production, maintenance, and supply
levels coordinate to diagnose well issues or balance energy demand. These cases
suggest that hierarchical structures may achieve global efficiency while
preserving local autonomy, though the balance is delicate.
  The paper closes by identifying open challenges: making hierarchical
decisions explainable to human operators, scaling to very large agent
populations, and assessing whether learning-based agents such as large language
models can be safely integrated into layered frameworks. This paper presents
what appears to be the first taxonomy that unifies structural, temporal, and
communication dimensions of hierarchical MAS into a single design framework,
bridging classical coordination mechanisms with modern reinforcement learning
and large language model agents.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [582] [A note on simulation methods for the Dirichlet-Laplace prior](https://arxiv.org/abs/2508.11982)
*Luis Gruber,Gregor Kastner,Anirban Bhattacharya,Debdeep Pati,Natesh Pillai,David Dunson*

Main category: stat.CO

TL;DR: The paper corrects an error in the original MCMC algorithm proposed by Bhattacharya et al. (2015) for simulating posterior draws under the Dirichlet-Laplace (DL) prior, and suggests two fixes.


<details>
  <summary>Details</summary>
Motivation: The authors were motivated to address an issue in Bhattacharya et al.'s (2015) MCMC algorithm, which samples from conditional distributions in an incorrect order, leading to an inaccurate joint posterior distribution for latent variables.

Method: The paper details the problem in the original algorithm and proposes two solutions: a correction to the original algorithm and a new algorithm based on an alternative equivalent formulation of the prior.

Result: Two straightforward solutions are presented, ensuring proper sampling from the joint posterior distribution. The theoretical findings from Bhattacharya et al. (2015) remain valid.

Conclusion: The provided corrections and the new algorithm ensure compatibility with the Dirichlet-Laplace (DL) prior, fixing an error without affecting the earlier theoretical results.

Abstract: Bhattacharya et al. (2015, Journal of the American Statistical Association
110(512): 1479-1490) introduce a novel prior, the Dirichlet-Laplace (DL) prior,
and propose a Markov chain Monte Carlo (MCMC) method to simulate posterior
draws under this prior in a conditionally Gaussian setting. The original
algorithm samples from conditional distributions in the wrong order, i.e., it
does not correctly sample from the joint posterior distribution of all latent
variables. This note details the issue and provides two simple solutions: A
correction to the original algorithm and a new algorithm based on an
alternative, yet equivalent, formulation of the prior. This corrigendum does
not affect the theoretical results in Bhattacharya et al. (2015).

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [583] [Denoising diffusion models for inverse design of inflatable structures with programmable deformations](https://arxiv.org/abs/2508.13097)
*Sara Karimi,Nikolaos N. Vlassis*

Main category: cs.CE

TL;DR: The paper introduces a generative design approach using denoising diffusion probabilistic models for designing inflatable elastic structures that undergo desired nonlinear deformations under pressure.


<details>
  <summary>Details</summary>
Motivation: To address the need for an effective system to design elastic structures with specific deformation capabilities under pressure-driven actuation, with applications in robotics, aerospace, and more.

Method: The method employs DDPMs to formulate the inverse design task, where target deformation descriptors are inputs used to generate image-based representations of the undeformed structure configurations.

Result: Numerical experiments demonstrate the capability to produce diverse configurations that fulfill the desired deformation outcomes, allowing parallel exploration and accommodating complex design constraints.

Conclusion: The study presents a systematic and efficient design framework for inflatable structures, aiding in rapid exploration of viable designs tailored to achieve specific deformation profiles.

Abstract: Programmable structures are systems whose undeformed geometries and material
property distributions are deliberately designed to achieve prescribed deformed
configurations under specific loading conditions. Inflatable structures are a
prominent example, using internal pressurization to realize large, nonlinear
deformations in applications ranging from soft robotics and deployable
aerospace systems to biomedical devices and adaptive architecture. We present a
generative design framework based on denoising diffusion probabilistic models
(DDPMs) for the inverse design of elastic structures undergoing large,
nonlinear deformations under pressure-driven actuation. The method formulates
the inverse design as a conditional generation task, using geometric
descriptors of target deformed states as inputs and outputting image-based
representations of the undeformed configuration. Representing these
configurations as simple images is achieved by establishing a pre- and
postprocessing pipeline that involves a fixed image processing, simulation
setup, and descriptor extraction methods. Numerical experiments with scalar and
higher-dimensional descriptors show that the framework can quickly produce
diverse undeformed configurations that achieve the desired deformations when
inflated, enabling parallel exploration of viable design candidates while
accommodating complex constraints.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [584] [Insight Rumors: A Novel Textual Rumor Locating and Marking Model Leveraging Att_BiMamba2 Network](https://arxiv.org/abs/2508.12574)
*Bin Ma,Yifei Zhang,Yongjin Xian,Qi Li,Linna Zhou,Gongxun Miao*

Main category: cs.SI

TL;DR: This paper proposes a sophisticated model called Insight Rumors to detect, locate, and mark rumors within text, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Existing rumor detection models focus solely on classification without being able to locate and mark specific rumor content.

Method: The paper proposes Att_BiMamba2, a bidirectional Mamba2 network with dot-product attention, alongside a Rumor Locating and Marking module. CRF is utilized to enhance output constraints.

Result: The model performs precise detection, locating, and marking of rumors, exceeding the capabilities of current state-of-the-art methods.

Conclusion: The proposed approach enhances rumor detection by providing granular insights into rumor content within social media contexts, addressing limitations in current methods.

Abstract: With the development of social media networks, rumor detection models have
attracted more and more attention. Whereas, these models primarily focus on
classifying contexts as rumors or not, lacking the capability to locate and
mark specific rumor content. To address this limitation, this paper proposes a
novel rumor detection model named Insight Rumors to locate and mark rumor
content within textual data. Specifically, we propose the Bidirectional Mamba2
Network with Dot-Product Attention (Att_BiMamba2), a network that constructs a
bidirectional Mamba2 model and applies dot-product attention to weight and
combine the outputs from both directions, thereby enhancing the representation
of high-dimensional rumor features. Simultaneously, a Rumor Locating and
Marking module is designed to locate and mark rumors. The module constructs a
skip-connection network to project high-dimensional rumor features onto
low-dimensional label features. Moreover, Conditional Random Fields (CRF) is
employed to impose strong constraints on the output label features, ensuring
accurate rumor content location. Additionally, a labeled dataset for rumor
locating and marking is constructed, with the effectiveness of the proposed
model is evaluated through comprehensive experiments. Extensive experiments
indicate that the proposed scheme not only detects rumors accurately but also
locates and marks them in context precisely, outperforming state-of-the-art
schemes that can only discriminate rumors roughly.

</details>


### [585] [On Balancing Sparsity with Reliable Connectivity in Distributed Network Design with Random K-out Graphs](https://arxiv.org/abs/2508.11863)
*Mansi Sood,Eray Can Elumar,Osman Yagan*

Main category: cs.SI

TL;DR: The paper investigates balancing sparsity and connectivity in random K-out graphs, presenting results on parameter choices, connectivity bounds, and the impact of adversarial nodes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure low communication overhead while maintaining reliable connectivity in sparse networks, particularly for settings like privacy-preserving aggregations in environments with limited trust.

Method: The authors derive upper and lower bounds for connectivity probabilities in finite graphs, analyze r-robustness for resilience against malicious nodes, and model the impact of adversarial deletions on network metrics.

Result: The paper provides theoretical theorems and analyses ensuring reliable connectivity, resilience to adversarial activity, and improved understanding of network parameters under finite or stochastic conditions.

Conclusion: The work offers insights and metrics to guide parameter selection for reliable and sparse distributed systems, enabling strong privacy and inference guarantees in adversarial scenarios.

Abstract: In several applications in distributed systems, an important design criterion
is ensuring that the network is sparse, i.e., does not contain too many edges,
while achieving reliable connectivity. Sparsity ensures communication overhead
remains low, while reliable connectivity is tied to reliable communication and
inference on decentralized data reservoirs and computational resources. A class
of network models called random K-out graphs appear widely as a heuristic to
balance connectivity and sparsity, especially in settings with limited trust,
e.g., privacy-preserving aggregation of networked data in which networks are
deployed. However, several questions remain regarding how to choose network
parameters in response to different operational requirements, including the
need to go beyond asymptotic results and the ability to model the stochastic
and adversarial environments. To address this gap, we present theorems to
inform the choice of network parameters that guarantee reliable connectivity in
regimes where nodes can be finite or unreliable. We first derive upper and
lower bounds for probability of connectivity in random K-out graphs when the
number of nodes is finite. Next, we analyze the property of r-robustness, a
stronger notion than connectivity that enables resilient consensus in the
presence of malicious nodes. Finally, motivated by aggregation mechanisms based
on pairwise masking, we model and analyze the impact of a subset of adversarial
nodes, modeled as deletions, on connectivity and giant component size - metrics
that are closely tied to privacy guarantees. Together, our results pave the way
for end-to-end performance guarantees for a suite of algorithms for reliable
inference on networks.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [586] [System-driven Interactive Design Support for Cloud Architecture: A Qualitative User Experience Study with Novice Engineers](https://arxiv.org/abs/2508.12385)
*Ryosuke Kohita,Akira Kasuga*

Main category: cs.HC

TL;DR: This paper explores a system-driven cloud design tool that assists novice engineers by offering guided support to address gaps in their cloud design expertise.


<details>
  <summary>Details</summary>
Motivation: Novice engineers often struggle with ambiguous requirements and complex trade-offs in cloud architecture design, necessitating tools to fill knowledge gaps.

Method: The study examines the experiences of 60 novice engineers using a system-driven support tool for cloud architecture design via qualitative analysis.

Result: The tool effectively helps novices create initial architectures, simulate multiple design options, and better understand design principles.

Conclusion: Structured system guidance can significantly improve novices' cloud design skills, but further enhancements are needed in tailoring support, validating outputs, and improving integration with workflows.

Abstract: Cloud architecture design presents significant challenges due to the
necessity of clarifying ambiguous requirements and systematically addressing
complex trade-offs, especially for novice engineers with limited cloud
experience. While recent advances in the use of AI tools have broadened
available options, system-driven approaches that offer explicit guidance and
step-by-step information management may be especially effective in supporting
novices during the design process. This study qualitatively examines the
experiences of 60 novice engineers using such a system-driven cloud design
support tool. The findings indicate that structured and proactive system
guidance helps novices engage more effectively in architectural design,
especially when addressing tasks where knowledge and experience gaps are most
critical. For example, participants found it easier to create initial
architectures and did not need to craft prompts themselves. In addition,
participants reported that the ability to simulate and compare multiple
architecture options enabled them to deepen their understanding of cloud design
principles and trade-offs, demonstrating the educational value of system-driven
support. The study also identifies areas for improvement, including more
adaptive information delivery tailored to user expertise, mechanisms for
validating system outputs, and better integration with implementation workflows
such as infrastructure-as-code generation and deployment guidance. Addressing
these aspects can further enhance the educational and practical value of
system-driven support tools for cloud architecture design.

</details>


### [587] [iTrace: Click-Based Gaze Visualization on the Apple Vision Pro](https://arxiv.org/abs/2508.12268)
*Esra Mehmedova,Santiago Berrezueta-Guzman,Stefan Wagner*

Main category: cs.HC

TL;DR: The study presents iTrace, a system that overcomes gaze data restrictions on the Apple Vision Pro by using click-based gaze extraction techniques to create dynamic heatmaps for eye-tracking analysis.


<details>
  <summary>Details</summary>
Motivation: To address the privacy restrictions of the Apple Vision Pro device that limit access to continuous user gaze data, impeding eye-tracking applications.

Method: Developed iTrace, a client-server system employing manual and automatic click-based methods, such as a pinch gesture, dwell control, or gaming controllers, to extract gaze data and generate heatmaps.

Result: In a study with 20 participants, the 8BitDo controller achieved a higher click rate (14.22 clicks/s) compared to dwell control (0.45 clicks/s), creating detailed heatmaps that revealed attention patterns during tasks.

Conclusion: iTrace demonstrates high precision (91%) and its potential applications in education, design, marketing, and cognitive research, but it should be limited to research settings due to privacy considerations.

Abstract: The Apple Vision Pro is equipped with accurate eye-tracking capabilities, yet
the privacy restrictions on the device prevent direct access to continuous user
gaze data. This study introduces iTrace, a novel application that overcomes
these limitations through click-based gaze extraction techniques, including
manual methods like a pinch gesture, and automatic approaches utilizing dwell
control or a gaming controller. We developed a system with a client-server
architecture that captures the gaze coordinates and transforms them into
dynamic heatmaps for video and spatial eye tracking. The system can generate
individual and averaged heatmaps, enabling analysis of personal and collective
attention patterns.
  To demonstrate its effectiveness and evaluate the usability and performance,
a study was conducted with two groups of 10 participants, each testing
different clicking methods. The 8BitDo controller achieved higher average data
collection rates at 14.22 clicks/s compared to 0.45 clicks/s with dwell
control, enabling significantly denser heatmap visualizations. The resulting
heatmaps reveal distinct attention patterns, including concentrated focus in
lecture videos and broader scanning during problem-solving tasks. By allowing
dynamic attention visualization while maintaining a high gaze precision of 91
%, iTrace demonstrates strong potential for a wide range of applications in
educational content engagement, environmental design evaluation, marketing
analysis, and clinical cognitive assessment. Despite the current gaze data
restrictions on the Apple Vision Pro, we encourage developers to use iTrace
only in research settings.

</details>


### [588] [fCrit: A Visual Explanation System for Furniture Design Creative Support](https://arxiv.org/abs/2508.12416)
*Vuong Nguyen,Gabriel Vigliensoni*

Main category: cs.HC

TL;DR: fCrit is an AI system for critiquing furniture design with a focus on explainable and user-adaptive reasoning.


<details>
  <summary>Details</summary>
Motivation: To make AI reasoning in the arts transparent and aligned with users' design language and cognitive framing.

Method: A dialogue-based, multi-agent architecture that uses a structured design knowledge base.

Result: fCrit tailors its critiques and explanations to users' design language, enabling meaningful interaction.

Conclusion: The system advances explainable AI in creative practices, emphasizing user-centered and dialogic approaches.

Abstract: We introduce fCrit, a dialogue-based AI system designed to critique furniture
design with a focus on explainability. Grounded in reflective learning and
formal analysis, fCrit employs a multi-agent architecture informed by a
structured design knowledge base. We argue that explainability in the arts
should not only make AI reasoning transparent but also adapt to the ways users
think and talk about their designs. We demonstrate how fCrit supports this
process by tailoring explanations to users' design language and cognitive
framing. This work contributes to Human-Centered Explainable AI (HCXAI) in
creative practice, advancing domain-specific methods for situated, dialogic,
and visually grounded AI support.

</details>


### [589] [Using AI for User Representation: An Analysis of 83 Persona Prompts](https://arxiv.org/abs/2508.13047)
*Joni Salminen,Danial Amin,Bernard Jansen*

Main category: cs.HC

TL;DR: The study analyzes 83 persona prompts from 27 research articles that use large language models (LLMs) for generating user personas, revealing trends in prompt design and methodological practices.


<details>
  <summary>Details</summary>
Motivation: To examine how LLM-based prompts are being utilized to create user personas and identify trends or gaps in the current generation techniques.

Method: The study involves analyzing 83 persona prompts from 27 research articles to understand their characteristics, such as format preferences, types of attributes, the inclusion of structured outputs, and input variability.

Result: The findings highlight a preference for single and concise persona descriptions, the dominance of text in generated attributes, frequent inclusion of demographic data, and the use of structured formats like JSON. Most studies use limited prompt variations, and comparative LLM testing is uncommon.

Conclusion: This paper underscores the evolving practices in computational persona design, emphasizing the need to balance concise outputs with traditional rich persona profiles and calling for more comprehensive testing of LLMs.

Abstract: We analyzed 83 persona prompts from 27 research articles that used large
language models (LLMs) to generate user personas. Findings show that the
prompts predominantly generate single personas. Several prompts express a
desire for short or concise persona descriptions, which deviates from the
tradition of creating rich, informative, and rounded persona profiles. Text is
the most common format for generated persona attributes, followed by numbers.
Text and numbers are often generated together, and demographic attributes are
included in nearly all generated personas. Researchers use up to 12 prompts in
a single study, though most research uses a small number of prompts. Comparison
and testing multiple LLMs is rare. More than half of the prompts require the
persona output in a structured format, such as JSON, and 74% of the prompts
insert data or dynamic variables. We discuss the implications of increased use
of computational personas for user representation.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [590] [Reduced-order modeling of Hamiltonian dynamics based on symplectic neural networks](https://arxiv.org/abs/2508.11911)
*Yongsheng Chen,Wei Guo,Qi Tang,Xinghui Zhong*

Main category: math.NA

TL;DR: This paper presents a new neural architecture for reduced-order modeling (ROM) of Hamiltonian systems that preserves the symplectic structure, yielding accurate and stable predictions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of designing reduced-order models for Hamiltonian systems that retain long-term accuracy and stability by preserving their symplectic structure.

Method: The authors introduce a data-driven framework combining Henon neural networks and symplectic-layer methods to unify latent space discovery and dynamics learning into a single architecture.

Result: The proposed approach demonstrates accurate trajectory reconstruction, robust prediction performance outside the training range, and precise Hamiltonian preservation in numerical experiments.

Conclusion: The outcomes indicate that the symplectic ROM framework has strong potential for applications in complex dynamical systems across various scientific and engineering fields.

Abstract: We introduce a novel data-driven symplectic induced-order modeling (ROM)
framework for high-dimensional Hamiltonian systems that unifies latent-space
discovery and dynamics learning within a single, end-to-end neural
architecture. The encoder-decoder is built from Henon neural networks
(HenonNets) and may be augmented with linear SGS-reflector layers. This yields
an exact symplectic map between full and latent phase spaces. Latent dynamics
are advanced by a symplectic flow map implemented as a HenonNet. This unified
neural architecture ensures exact preservation of the underlying symplectic
structure at the reduced-order level, significantly enhancing the fidelity and
long-term stability of the resulting ROM. We validate our method through
comprehensive numerical experiments on canonical Hamiltonian systems. The
results demonstrate the method's capability for accurate trajectory
reconstruction, robust predictive performance beyond the training horizon, and
accurate Hamiltonian preservation. These promising outcomes underscore the
effectiveness and potential applicability of our symplectic ROM framework for
complex dynamical systems across a broad range of scientific and engineering
disciplines.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [591] [Arabic ASR on the SADA Large-Scale Arabic Speech Corpus with Transformer-Based Models](https://arxiv.org/abs/2508.12968)
*Branislav Gerazov,Marcello Politi,Sébastien Bratières*

Main category: eess.AS

TL;DR: This paper evaluates several advanced ASR models on the SADA Arabic speech dataset and highlights the effectiveness of fine-tuning, language models, and noise handling.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in Arabic ASR, especially given the diversity of dialects and noisy environments, using the SADA dataset.

Method: The paper uses a combination of fine-tuning ASR models, integration with language models, and approaches to handle noise for performance evaluation.

Result: The MMS 1B model, fine-tuned on SADA with a 4-gram language model, achieves remarkable performance with a WER of 40.9% and CER of 17.6% on the clean test set.

Conclusion: Fine-tuned models with robust language modeling significantly improve Arabic ASR performance, even in challenging environments.

Abstract: We explore the performance of several state-of-the-art automatic speech
recognition (ASR) models on a large-scale Arabic speech dataset, the SADA
(Saudi Audio Dataset for Arabic), which contains 668 hours of high-quality
audio from Saudi television shows. The dataset includes multiple dialects and
environments, specifically a noisy subset that makes it particularly
challenging for ASR. We evaluate the performance of the models on the SADA test
set, and we explore the impact of fine-tuning, language models, as well as
noise and denoising on their performance. We find that the best performing
model is the MMS 1B model finetuned on SADA with a 4-gram language model that
achieves a WER of 40.9\% and a CER of 17.6\% on the SADA test clean set.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [592] [Categorical Construction of Logically Verifiable Neural Architectures](https://arxiv.org/abs/2508.11647)
*Logan Nye*

Main category: cs.LO

TL;DR: This paper develops a mathematical framework to design neural networks with built-in logical guarantees.


<details>
  <summary>Details</summary>
Motivation: Neural networks often fail at reliable logical reasoning and violate basic logical principles, limiting their utility in applications requiring strict logical adherence.

Method: The paper transforms logical theories into algebraic structures called Lawvere theories, which are used to systematically construct neural architectures embedding logical principles directly into their structure, ensuring no logical violations.

Result: The framework enables differentiable neural architectures for propositional logic that maintain boolean reasoning and are trainable via gradient descent. It establishes a bijection between logical theories and neural architectures.

Conclusion: This categorical approach expands the scope of Categorical Deep Learning to semantic constraints, laying foundational ground for trustworthy AI systems in areas like theorem proving, formal verification, and safety-critical reasoning tasks.

Abstract: Neural networks excel at pattern recognition but struggle with reliable
logical reasoning, often violating basic logical principles during inference.
We address this limitation by developing a categorical framework that
systematically constructs neural architectures with provable logical
guarantees. Our approach treats logical theories as algebraic structures called
Lawvere theories, which we transform into neural networks using categorical
algebra in the 2-category of parametric maps. Unlike existing methods that
impose logical constraints during training, our categorical construction embeds
logical principles directly into the network's architectural structure, making
logical violations mathematically impossible. We demonstrate this framework by
constructing differentiable neural architectures for propositional logic that
preserve boolean reasoning while remaining trainable via gradient descent. Our
main theoretical result establishes a bijective correspondence between finitary
logical theories and neural architectures, proving that every logically
constrained network arises uniquely from our construction. This extends
Categorical Deep Learning beyond geometric symmetries to semantic constraints,
enabling automatic derivation of verified architectures from logical
specifications. The framework provides mathematical foundations for trustworthy
AI systems, with applications to theorem proving, formal verification, and
safety-critical reasoning tasks requiring verifiable logical behavior.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [593] [Adversarial Robustness in Distributed Quantum Machine Learning](https://arxiv.org/abs/2508.11848)
*Pouya Kananian,Hans-Arno Jacobsen*

Main category: quant-ph

TL;DR: This paper examines the adversarial robustness of quantum machine learning (QML) models, focusing on distributed approaches like federated learning and quantum-specific techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the adversarial robustness of distributed QML models and understand how distribution methods impact the models' vulnerabilities compared to central systems.

Method: The paper reviews existing distribution methods, including federated learning and quantum circuit-specific approaches, while analyzing how their respective vulnerabilities influence adversarial attacks.

Result: Differences in robustness based on distribution paradigms are highlighted, showing that certain methods may make QML models more susceptible to novel attacks.

Conclusion: The paper underscores the importance of studying distributed QML models in adversarial settings and calls for further research to address their vulnerabilities in scalable systems.

Abstract: Studying adversarial robustness of quantum machine learning (QML) models is
essential in order to understand their potential advantages over classical
models and build trustworthy systems. Distributing QML models allows leveraging
multiple quantum processors to overcome the limitations of individual devices
and build scalable systems. However, this distribution can affect their
adversarial robustness, potentially making them more vulnerable to new attacks.
Key paradigms in distributed QML include federated learning, which, similar to
classical models, involves training a shared model on local data and sending
only the model updates, as well as circuit distribution methods inherent to
quantum computing, such as circuit cutting and teleportation-based techniques.
These quantum-specific methods enable the distributed execution of quantum
circuits across multiple devices. This work reviews the differences between
these distribution methods, summarizes existing approaches on the adversarial
robustness of QML models when distributed using each paradigm, and discusses
open questions in this area.

</details>


### [594] [Quantum Flow Matching](https://arxiv.org/abs/2508.12413)
*Zidong Cui,Pan Zhang,Ying Tang*

Main category: quant-ph

TL;DR: This paper introduces Quantum Flow Matching (QFM), enabling efficient interpolation between two quantum density matrices on quantum computers.


<details>
  <summary>Details</summary>
Motivation: The authors aim to extend the classical generative modeling paradigm, flow matching, to quantum systems for efficient preparation and sampling of quantum density matrices.

Method: QFM relies on quantum circuit-based implementations, avoiding costly redesigns, to systematically prepare density matrices and generate samples to estimate observables.

Result: The framework is validated in applications such as generating prescribed quantum states, testing quantum Jarzynski equality, and studying superdiffusion breakdown.

Conclusion: QFM establishes itself as a versatile and unifying framework for generative modeling in quantum systems, with promising applications across varied domains.

Abstract: Flow matching has rapidly become a dominant paradigm in classical generative
modeling, offering an efficient way to interpolate between two complex
distributions. We extend this idea to the quantum realm and introduce Quantum
Flow Matching (QFM)-a fully quantum-circuit realization that offers efficient
interpolation between two density matrices. QFM offers systematic preparation
of density matrices and generation of samples for accurately estimating
observables, and can be realized on a quantum computer without the need for
costly circuit redesigns. We validate its versatility on a set of applications:
(i) generating target states with prescribed magnetization and entanglement
entropy, (ii) estimating nonequilibrium free-energy differences to test the
quantum Jarzynski equality, and (iii) expediting the study on superdiffusion
breakdown. These results position QFM as a unifying and promising framework for
generative modeling across quantum systems.

</details>


### [595] [SimQFL: A Quantum Federated Learning Simulator with Real-Time Visualization](https://arxiv.org/abs/2508.12477)
*Ratun Rahman,Atit Pokharel,Md Raihan Uddin,Dinh C. Nguyen*

Main category: quant-ph

TL;DR: The paper introduces SimQFL, a simulator for quantum federated learning, addressing current simulator limitations like lack of integration for ML tasks and real-time updates.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in observing model convergence, debugging quantum circuits, and integrating user-specific training data in existing quantum simulators.

Method: Development of SimQFL with real-time visualization, epoch-wise output tracking, adjustable quantum and ML parameters, and an intuitive interface for monitoring and customization.

Result: SimQFL simplifies and accelerates QFL experiments, giving dynamic feedback and learning curve visualizations with transparency and control.

Conclusion: SimQFL is a practical, user-friendly platform for prototyping, analyzing, and refining QFL models, supporting diverse applications in distributed quantum networks.

Abstract: Quantum federated learning (QFL) is an emerging field that has the potential
to revolutionize computation by taking advantage of quantum physics concepts in
a distributed machine learning (ML) environment. However, the majority of
available quantum simulators are primarily built for general quantum circuit
simulation and do not include integrated support for machine learning tasks
such as training, evaluation, and iterative optimization. Furthermore,
designing and assessing quantum learning algorithms is still a difficult and
resource-intensive task. Real-time updates are essential for observing model
convergence, debugging quantum circuits, and making conscious choices during
training with the use of limited resources. Furthermore, most current
simulators fail to support the integration of user-specific data for training
purposes, undermining the main purpose of using a simulator. In this study, we
introduce SimQFL, a customized simulator that simplifies and accelerates QFL
experiments in quantum network applications. SimQFL supports real-time,
epoch-wise output development and visualization, allowing researchers to
monitor the process of learning across each training round. Furthermore, SimQFL
offers an intuitive and visually appealing interface that facilitates ease of
use and seamless execution. Users can customize key variables such as the
number of epochs, learning rates, number of clients, and quantum
hyperparameters such as qubits and quantum layers, making the simulator
suitable for various QFL applications. The system gives immediate feedback
following each epoch by showing intermediate outcomes and dynamically
illustrating learning curves. SimQFL is a practical and interactive platform
enabling academics and developers to prototype, analyze, and tune quantum
neural networks with greater transparency and control in distributed quantum
networks.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [596] [Exploring Multimodal AI Reasoning for Meteorological Forecasting from Skew-T Diagrams](https://arxiv.org/abs/2508.12198)
*ChangJae Lee,Heecheol Yang,Jonghak Choi*

Main category: physics.ao-ph

TL;DR: The paper introduces an AI assistant for interpreting Skew-T diagrams in meteorology using fine-tuned Vision-Language Models (VLMs), achieving comparable forecasting accuracy to traditional models while relying on static data.


<details>
  <summary>Details</summary>
Motivation: Forecasting from atmospheric soundings is complex and often requires skilled human forecasters to analyze meteorological Skew-T diagrams, a task that has seen limited application of modern AI techniques like Vision-Language Models.

Method: The study employs a curriculum learning approach, where small VLMs and LMs are fine-tuned for visual question answering and chain-of-thought reasoning on meteorological tasks, leveraging training data and observations from South Korea's Auto Weather Stations.

Result: The fine-tuned VLM matches the forecasting skill of traditional Numerical Weather Prediction (NWP) systems despite relying solely on static atmospheric profiles. Ablation studies identify key elements for its performance, and attention map analysis confirms its focus on relevant features.

Conclusion: Compact and interpretable multimodal AI models can complement weather forecasting, offering computational efficiency and potential expansion to more complex meteorological applications in the future.

Abstract: Forecasting from atmospheric soundings is a fundamental task in operational
meteorology, often requiring structured visual reasoning over Skew-T log-P
diagrams by human forecasters. While recent advances in Vision-Language Models
(VLMs) have shown promise in other scientific domains, their application to
meteorological diagram interpretation remains largely unexplored. In this
study, we present a lightweight AI assistant that interprets Skew-T diagrams
using a small language model (LM) and a small VLM fine-tuned to emulate human
forecasters. Using a curriculum learning framework, we first train the models
to identify key atmospheric features from diagrams through visual question
answering, followed by chain-of-thought reasoning tasks that estimate
precipitation probability based on the derived visual groundings. Model inputs
include either textual summaries or generated Skew-T diagrams derived from
operational Numerical Weather Prediction (NWP) forecasts, paired with
three-hour precipitation observations from South Korea's Auto Weather Stations
network. Evaluation results demonstrate that the fine-tuned VLM achieves skill
comparable to an operational NWP model, despite relying solely on static
atmospheric profiles. Ablation studies reveal that visual grounding and
reasoning supervision are critical for performance, while attention map
analysis confirms that the model learns to focus on relevant meteorological
features. These findings highlight the potential of compact, interpretable
multimodal models to support weather forecasting tasks. The approach offers a
computationally efficient alternative to large-scale systems, and future work
could extend it to more complex applications.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [597] [RefAdGen: High-Fidelity Advertising Image Generation](https://arxiv.org/abs/2508.11695)
*Yiyun Chen,Weikai Yang*

Main category: cs.GR

TL;DR: This paper introduces a novel AIGC-based advertising image generation method, RefAdGen, addressing fidelity-efficiency limitations, and supported by a new dataset, AdProd-100K.


<details>
  <summary>Details</summary>
Motivation: To solve the high fidelity and efficiency challenges of existing AIGC techniques in generating diverse, high-quality advertising images for marketing industries.

Method: Construct AdProd-100K dataset using a dual data augmentation strategy, and propose RefAdGen framework employing spatial control through product masks and an Attention Fusion Module.

Result: RefAdGen achieves state-of-the-art performance with robust generalization for unseen products and real-world images, ensuring both high fidelity and scalability.

Conclusion: The approach offers a scalable, cost-effective alternative to traditional workflows in e-commerce and marketing, with publicly available resources to foster future research.

Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC)
techniques has unlocked opportunities in generating diverse and compelling
advertising images based on referenced product images and textual scene
descriptions. This capability substantially reduces human labor and production
costs in traditional marketing workflows. However, existing AIGC techniques
either demand extensive fine-tuning for each referenced image to achieve high
fidelity, or they struggle to maintain fidelity across diverse products, making
them impractical for e-commerce and marketing industries. To tackle this
limitation, we first construct AdProd-100K, a large-scale advertising image
generation dataset. A key innovation in its construction is our dual data
augmentation strategy, which fosters robust, 3D-aware representations crucial
for realistic and high-fidelity image synthesis. Leveraging this dataset, we
propose RefAdGen, a generation framework that achieves high fidelity through a
decoupled design. The framework enforces precise spatial control by injecting a
product mask at the U-Net input, and employs an efficient Attention Fusion
Module (AFM) to integrate product features. This design effectively resolves
the fidelity-efficiency dilemma present in existing methods. Extensive
experiments demonstrate that RefAdGen achieves state-of-the-art performance,
showcasing robust generalization by maintaining high fidelity and remarkable
visual results for both unseen products and challenging real-world, in-the-wild
images. This offers a scalable and cost-effective alternative to traditional
workflows. Code and datasets are publicly available at
https://github.com/Anonymous-Name-139/RefAdgen.

</details>


### [598] [Express4D: Expressive, Friendly, and Extensible 4D Facial Motion Generation Benchmark](https://arxiv.org/abs/2508.12438)
*Yaron Aloni,Rotem Shalev-Arkushin,Yonatan Shafir,Guy Tevet,Ohad Fried,Amit Haim Bermano*

Main category: cs.GR

TL;DR: The paper introduces Express4D, a dataset for dynamic facial expression generation from text, aimed at fine-grained control and trained using accessible tools.


<details>
  <summary>Details</summary>
Motivation: To address gaps in existing speech-driven or emotion-labeled datasets that lack nuance and require expensive capturing equipment.

Method: The researchers created a dataset using ARKit blendshape facial motion sequences coupled with LLM-generated natural language instructions. They also trained two baseline models to demonstrate text-to-expression mapping.

Result: The dataset and baseline models show the ability to generate meaningful and expressive text-to-facial motion sequences. They also capture the many-to-many relationship between text descriptions and expressions.

Conclusion: Express4D offers a cost-effective and detailed resource for advancing fine-grained text-to-expression generation, setting a new benchmark for future studies.

Abstract: Dynamic facial expression generation from natural language is a crucial task
in Computer Graphics, with applications in Animation, Virtual Avatars, and
Human-Computer Interaction. However, current generative models suffer from
datasets that are either speech-driven or limited to coarse emotion labels,
lacking the nuanced, expressive descriptions needed for fine-grained control,
and were captured using elaborate and expensive equipment. We hence present a
new dataset of facial motion sequences featuring nuanced performances and
semantic annotation. The data is easily collected using commodity equipment and
LLM-generated natural language instructions, in the popular ARKit blendshape
format. This provides riggable motion, rich with expressive performances and
labels. We accordingly train two baseline models, and evaluate their
performance for future benchmarking. Using our Express4D dataset, the trained
models can learn meaningful text-to-expression motion generation and capture
the many-to-many mapping of the two modalities. The dataset, code, and video
examples are available on our webpage: https://jaron1990.github.io/Express4D/

</details>


### [599] [MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration](https://arxiv.org/abs/2508.12691)
*Yuanxin Wei,Lansong Diao,Bujiao Chen,Shenggan Cheng,Zhengping Qian,Wenyuan Yu,Nong Xiao,Wei Lin,Jiangsu Du*

Main category: cs.GR

TL;DR: The paper introduces MixCache, a training-free framework aimed at accelerating video generation using efficient caching strategies, achieving speed improvements on models while retaining high output quality.


<details>
  <summary>Details</summary>
Motivation: Video DiT models, built on Transformer architecture and diffusion process, yield high-quality results but suffer from computational inefficiencies due to their iterative denoising. Existing caching methods are single-granularity and struggle to balance speed and quality.

Method: MixCache introduces a context-aware cache triggering strategy and an adaptive hybrid cache decision strategy to dynamically select optimal caching granularities, improving efficiency during video DiT inference.

Result: MixCache achieves significant improvements in video generation speed (e.g., nearly doubling speed on Wan 14B and HunyuanVideo models) while maintaining or improving generation quality.

Conclusion: The proposed MixCache framework effectively balances inference efficiency and output quality by leveraging an adaptive, multi-granularity caching mechanism, offering a robust solution for video generation in computationally intensive environments.

Abstract: Leveraging the Transformer architecture and the diffusion process, video DiT
models have emerged as a dominant approach for high-quality video generation.
However, their multi-step iterative denoising process incurs high computational
cost and inference latency. Caching, a widely adopted optimization method in
DiT models, leverages the redundancy in the diffusion process to skip
computations in different granularities (e.g., step, cfg, block). Nevertheless,
existing caching methods are limited to single-granularity strategies,
struggling to balance generation quality and inference speed in a flexible
manner. In this work, we propose MixCache, a training-free caching-based
framework for efficient video DiT inference. It first distinguishes the
interference and boundary between different caching strategies, and then
introduces a context-aware cache triggering strategy to determine when caching
should be enabled, along with an adaptive hybrid cache decision strategy for
dynamically selecting the optimal caching granularity. Extensive experiments on
diverse models demonstrate that, MixCache can significantly accelerate video
generation (e.g., 1.94$\times$ speedup on Wan 14B, 1.97$\times$ speedup on
HunyuanVideo) while delivering both superior generation quality and inference
efficiency compared to baseline methods.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [600] [BeeNet: Reconstructing Flower Shapes from Electric Fields using Deep Learning](https://arxiv.org/abs/2508.11724)
*Jake Turley,Ryan A. Palmer,Isaac V. Chenchiah,Daniel Robert*

Main category: q-bio.QM

TL;DR: This study explores how arthropods, particularly bees, use electroreception to perceive environmental features such as flower shapes.


<details>
  <summary>Details</summary>
Motivation: Understanding how pollinators like bees utilize environmental electrical fields to decode various spatial information, potentially enhancing knowledge about arthropod perception.

Method: Deep learning UNet models were trained on simulated electrical field data created by interactions between flowers with different petal geometries and charged bees.

Result: The algorithm successfully reconstructed diverse flower shapes and even complex shapes not included in the training data, demonstrating distance-dependent encoding of spatial details.

Conclusion: Electroreception in arthropods provides rich spatial environmental information, offering new insights into how they perceive their surroundings.

Abstract: Arthropods, including pollinators, respond to environmental electrical
fields. Here, we show that electric field information can be decoded to
reconstruct environmental features. We develop an algorithm capable of
inferring the shapes of polarisable flowers from the electric field generated
by a nearby charged bee. We simulated electric fields arising from bee flower
interactions for flowers with varying petal geometries. These simulated data
were used to train a deep learning UNet model to recreate petal shapes. The
model accurately reconstructed diverse flower shapes including more complex
flower shapes not included in training. Reconstruction performance peaked at an
optimal bee flower distance, indicating distance-dependent encoding of shape
information. These findings show that electroreception can impart rich spatial
detail, offering insights into arthropod environmental perception.

</details>


### [601] [On the Importance of Behavioral Nuances: Amplifying Non-Obvious Motor Noise Under True Empirical Considerations May Lead to Briefer Assays and Faster Classification Processes](https://arxiv.org/abs/2508.12742)
*Theodoros Bermperidis,Joe Vero,Elizabeth B Torres*

Main category: q-bio.QM

TL;DR: This paper addresses the tradeoff in capturing statistical power in biorhythmic data by creating a novel affective computing platform that analyzes brief face video samples using AI and micropeak dynamics.


<details>
  <summary>Details</summary>
Motivation: There is a need to overcome limitations of grand-averaging techniques in biorhythmic data that often lead to loss of important information and are not well-suited for highly scalable assays while maintaining personalized statistical power.

Method: The authors developed a platform using AI-driven face-grid estimation methods and nonlinear dynamical systems analysis to extract micropeak data from 5-second-long face video samples.

Result: The proposed methods successfully capture facial micropeaks, including affective micro-expressions, and distinguish patterns between autistic and neurotypical individuals.

Conclusion: The study introduces a geometric and dynamic approach to analyzing brief video samples, offering new insights into affective computing and enabling personalized analysis while preserving scalability.

Abstract: There is a tradeoff between attaining statistical power with large, difficult
to gather data sets, and producing highly scalable assays that register brief
data samples. Often, as grand-averaging techniques a priori assume
normally-distributed parameters and linear, stationary processes in
biorhythmic, time series data, important information is lost, averaged out as
gross data. We developed an affective computing platform that enables taking
brief data samples while maintaining personalized statistical power. This is
achieved by combining a new data type derived from the micropeaks present in
time series data registered from brief (5-second-long) face videos with recent
advances in AI-driven face-grid estimation methods. By adopting geometric and
nonlinear dynamical systems approaches to analyze the kinematics, especially
the speed data, the new methods capture all facial micropeaks. These include as
well the nuances of different affective micro expressions. We offer new ways to
differentiate dynamical and geometric patterns present in autistic individuals
from those found more commonly in neurotypical development.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [602] [A Graph Neural Network based on a Functional Topology Model: Unveiling the Dynamic Mechanisms of Non-Suicidal Self-Injury in Single-Channel EEG](https://arxiv.org/abs/2508.11684)
*BG Tong*

Main category: eess.SP

TL;DR: The paper proposes a Graph Neural Network (GNN)-based model to analyze neurodynamic mechanisms behind Non-Suicidal Self-Injury (NSSI) using single-channel EEG data, achieving high accuracy and uncovering dysfunctional feedback loops in brain dynamics.


<details>
  <summary>Details</summary>
Motivation: There is a need for a better understanding of the neural mechanisms behind Non-Suicidal Self-Injury (NSSI) to aid in diagnosis and therapy. Current methods are limited in their ability to dynamically model such mechanisms, particularly using portable EEG data.

Method: The study used single-channel EEG data collected over a month from three adolescents with NSSI. A theory-driven GNN model was designed with seven functional nodes and analyzed using cross-validation techniques. Explainability was evaluated through interpretation using GNNExplainer.

Result: The proposed GNN model achieved an intra-subject accuracy of over 85% and a cross-subject performance of approximately 73.7%. Analysis unveiled a reversal in feedback loop mechanisms regulating somatic sensation during NSSI states.

Conclusion: The study marks a significant advancement by marrying theory-driven GNNs with portable EEG data to decode complex mental states like NSSI. The discovery of the feedback loop reversal opens up new computational pathways for objective biomarkers and therapeutic interventions.

Abstract: Objective: This study proposes and preliminarily validates a novel
"Functional-Energetic Topology Model" to uncover neurodynamic mechanisms of
Non-Suicidal Self-Injury (NSSI), using Graph Neural Networks (GNNs) to decode
brain network patterns from single-channel EEG in real-world settings.Methods:
EEG data were collected over ~1 month from three adolescents with NSSI using a
smartphone app and a portable Fp1 EEG headband during impulsive and
non-impulsive states. A theory-driven GNN with seven functional nodes was
built. Performance was evaluated via intra-subject (80/20 split) and
leave-one-subject-out cross-validation (LOSOCV). GNNExplainer was used for
interpretability.Results: The model achieved high intra-subject accuracy (>85%)
and significantly above-chance cross-subject performance (approximately73.7%).
Explainability analysis revealed a key finding: during NSSI states, a critical
feedback loop regulating somatic sensation exhibits dysfunction and directional
reversal. Specifically, the brain loses its ability to self-correct via
negative bodily feedback, and the regulatory mechanism enters an "ineffective
idling" state.Conclusion: This work demonstrates the feasibility of applying
theory-guided GNNs to sparse, single-channel EEG for decoding complex mental
states. The identified "feedback loop reversal" offers a novel, dynamic, and
computable model of NSSI mechanisms, paving the way for objective biomarkers
and next-generation Digital Therapeutics (DTx).

</details>


### [603] [Enhancing Corrosion Resistance of Aluminum Alloys Through AI and ML Modeling](https://arxiv.org/abs/2508.11685)
*Farnaz Kaboudvand,Maham Khalid,Nydia Assaf,Vardaan Sahgal,Jon P. Ruffley,Brian J. McDermott*

Main category: eess.SP

TL;DR: The paper uses machine learning to predict and optimize corrosion resistance in aluminum alloys based on material composition and environmental data, with Gaussian Process Regression showing the best results.


<details>
  <summary>Details</summary>
Motivation: Corrosion significantly impacts the performance of aluminum alloys in marine environments, necessitating effective predictive tools to optimize material compositions and corrosion resistance.

Method: The study utilized open-source corrosion rate data and environmental data, employing ML methods (Random Forest, neural networks, and Gaussian Process Regression) for forward and inverse predictions.

Result: Gaussian Process Regression with hybrid kernels performed best among the tested methodologies, while log-transformed GPR further improved prediction accuracy.

Conclusion: Machine learning, especially Gaussian Process Regression, is effective for predicting corrosion rates and optimizing material properties, offering valuable insights into corrosion resistance optimization.

Abstract: Corrosion poses a significant challenge to the performance of aluminum
alloys, particularly in marine environments. This study investigates the
application of machine learning (ML) algorithms to predict and optimize
corrosion resistance, utilizing a comprehensive open-source dataset compiled
from various sources. The dataset encompasses corrosion rate data and
environmental conditions, preprocessed to standardize units and formats. We
explored two different approaches, a direct approach, where the material's
composition and environmental conditions were used as inputs to predict
corrosion rates; and an inverse approach, where corrosion rate served as the
input to identify suitable material compositions as output. We employed and
compared three distinct ML methodologies for forward predictions: Random Forest
regression, optimized via grid search; a feed-forward neural network, utilizing
ReLU activation and Adam optimization; and Gaussian Process Regression (GPR),
implemented with GPyTorch and employing various kernel functions. The Random
Forest and neural network models provided predictive capabilities based on
elemental compositions and environmental conditions. Notably, Gaussian Process
Regression demonstrated superior performance, particularly with hybrid kernel
functions. Log-transformed GPR further refined predictions. This study
highlights the efficacy of ML, particularly GPR, in predicting corrosion rates
and material properties.

</details>


### [604] [Vibe2Spike: Batteryless Wireless Tags for Vibration Sensing with Event Cameras and Spiking Networks](https://arxiv.org/abs/2508.11640)
*Danny Scott,William LaForest,Hritom Das,Ioannis Polykretis,Catherine D. Schuman,Charles Rizzo,James Plank,Sai Swaminathan*

Main category: eess.SP

TL;DR: The paper introduces Vibe2Spike, a battery-free, wireless sensing framework for activity recognition using vibration energy, visible light communication, and spiking neural networks.


<details>
  <summary>Details</summary>
Motivation: To address energy, scalability, and reliability challenges in deploying dense, low-cost sensing systems for smart environments.

Method: The authors created a system using low-cost piezoelectric tags that convert vibration energy into visible light spikes, processed by event cameras and spiking neural networks optimized through the EONS framework.

Result: Vibe2Spike achieved 94.9% classification accuracy across five device classes and analyzed latency-accuracy trade-offs from different temporal binning strategies.

Conclusion: The study highlights Vibe2Spike as a scalable, energy-efficient solution for intelligent environments without relying on batteries.

Abstract: The deployment of dense, low-cost sensors is critical for realizing
ubiquitous smart environments. However, existing sensing solutions struggle
with the energy, scalability, and reliability trade-offs imposed by battery
maintenance, wireless transmission overhead, and data processing complexity. In
this work, we present Vibe2Spike, a novel battery-free, wireless sensing
framework that enables vibration-based activity recognition using visible light
communication (VLC) and spiking neural networks (SNNs). Our system uses
ultra-low-cost tags composed only of a piezoelectric disc, a Zener diode, and
an LED, which harvest vibration energy and emit sparse visible light spikes
without requiring batteries or RF radios. These optical spikes are captured by
event cameras and classified using optimized SNN models evolved via the EONS
framework. We evaluate Vibe2Spike across five device classes, achieving 94.9\%
average classification fitness while analyzing the latency-accuracy trade-offs
of different temporal binning strategies. Vibe2Spike demonstrates a scalable,
and energy-efficient approach for enabling intelligent environments in a
batteryless manner.

</details>


### [605] [Age-Normalized HRV Features for Non-Invasive Glucose Prediction: A Pilot Sleep-Aware Machine Learning Study](https://arxiv.org/abs/2508.11682)
*Md Basit Azam,Sarangthem Ibotombi Singh*

Main category: eess.SP

TL;DR: The study improves glucose prediction using age-normalized HRV features and sleep-stage specific data, achieving notable accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: Current non-invasive glucose monitoring methods face challenges, especially due to age-related autonomic changes impacting HRV analyses.

Method: Analyzed 43 subjects with sleep-stage specific HRV features normalized by age-scaled factors. Used BayesianRidge regression and 5-fold cross-validation for log-glucose prediction.

Result: Age-normalized HRV features achieved R² = 0.161 (MAE = 0.182), showing 25.6% improvement over non-normalized features. Key predictive features include age-normalized HRV and diastolic blood pressure.

Conclusion: The methodology enhances glucose prediction with HRV data normalized for age, laying groundwork for non-invasive applications, though larger validation studies are needed.

Abstract: Non-invasive glucose monitoring remains a critical challenge in the
management of diabetes. HRV during sleep shows promise for glucose prediction
however, age-related autonomic changes significantly confound traditional HRV
analyses. We analyzed 43 subjects with multi-modal data including sleep-stage
specific ECG, HRV features, and clinical measurements. A novel
age-normalization technique was applied to the HRV features by, dividing the
raw values by age-scaled factors. BayesianRidge regression with 5-fold
cross-validation was employed for log-glucose prediction. Age-normalized HRV
features achieved R2 = 0.161 (MAE = 0.182) for log-glucose prediction,
representing a 25.6% improvement over non-normalized features (R2 = 0.132). The
top predictive features were hrv rem mean rr age normalized (r = 0.443, p =
0.004), hrv ds mean rr age normalized (r = 0.438, p = 0.005), and diastolic
blood pressure (r = 0.437, p = 0.005). Systematic ablation studies confirmed
age-normalization as the critical component, with sleep-stage specific features
providing additional predictive value. Age-normalized HRV features
significantly enhance glucose prediction accuracy compared with traditional
approaches. This sleep-aware methodology addresses fundamental limitations in
autonomic function assessment and suggests a preliminary feasibility for
non-invasive glucose monitoring applications. However, these results require
validation in larger cohorts before clinical consideration.

</details>


### [606] [Towards Generalizable Learning Models for EEG-Based Identification of Pain Perception](https://arxiv.org/abs/2508.11691)
*Mathis Rezzouk,Fabrice Gagnon,Alyson Champagne,Mathieu Roy,Philippe Albouy,Michel-Pierre Coll,Cem Subakan*

Main category: eess.SP

TL;DR: This study evaluates cross-participant performance of various machine learning models on pain perception using EEG data, highlighting the resilience of deep learning approaches and sharing the dataset for standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Pain perception via EEG signals lacks sufficient generalization methods due to high variability across individuals. A robust and standardized investigation is needed.

Method: The study benchmarks traditional and deep models using EEG data from 108 participants under both within- and cross-participant evaluation scenarios, focusing on pain perception with thermal and auditory stimuli.

Result: Traditional models show significant performance drops in cross-participant settings, while deep learning models exhibit better resilience. A graph-based model displayed strong potential for capturing invariant structures.

Conclusion: Deep learning models and graph-based approaches show promise in overcoming subject variability in EEG pain decoding. The dataset sharing provides a valuable benchmark for future studies.

Abstract: EEG-based analysis of pain perception, enhanced by machine learning, reveals
how the brain encodes pain by identifying neural patterns evoked by noxious
stimulation. However, a major challenge that remains is the generalization of
machine learning models across individuals, given the high cross-participant
variability inherent to EEG signals and the limited focus on direct pain
perception identification in current research. In this study, we systematically
evaluate the performance of cross-participant generalization of a wide range of
models, including traditional classifiers and deep neural classifiers for
identifying the sensory modality of thermal pain and aversive auditory
stimulation from EEG recordings. Using a novel dataset of EEG recordings from
108 participants, we benchmark model performance under both within- and
cross-participant evaluation settings. Our findings show that traditional
models suffered the largest drop from within- to cross-participant performance,
while deep learning models proved more resilient, underscoring their potential
for subject-invariant EEG decoding. Even though performance variability
remained high, the strong results of the graph-based model highlight its
potential to capture subject-invariant structure in EEG signals. On the other
hand, we also share the preprocessed dataset used in this study, providing a
standardized benchmark for evaluating future algorithms under the same
generalization constraints.

</details>


### [607] [Scalable, Technology-Agnostic Diagnosis and Predictive Maintenance for Point Machine using Deep Learning](https://arxiv.org/abs/2508.11692)
*Eduardo Di Santi,Ruixiang Ci,Clément Lefebvre,Nenad Mijatovic,Michele Pugnaloni,Jonathan Brown,Victor Martín,Kenza Saiah*

Main category: eess.SP

TL;DR: This paper presents a deep learning-based method to perform predictive maintenance for railway Point Machines using only power signal input, achieving extremely high accuracy while being technology-agnostic and scalable.


<details>
  <summary>Details</summary>
Motivation: To address the critical issue of Point Machine (PM) failures in railways, which lead to service disruptions, by enabling pre-emptive maintenance using an approach that is scalable and requires minimal input.

Method: A deep learning model is applied to power signal patterns to classify whether a PM is nominal or has specific failure types, without relying on multiple inputs or custom feature engineering.

Result: The proposed method achieves >99.99% precision and <0.01% false positives, with negligible false negatives, and is tested on diverse PM types in real-world and controlled environments.

Conclusion: The methodology is effective, scalable, and compliant with ISO-17359, offering maintainers confidence in the system's outputs and ensuring reliable railway operations.

Abstract: The Point Machine (PM) is a critical piece of railway equipment that switches
train routes by diverting tracks through a switchblade. As with any critical
safety equipment, a failure will halt operations leading to service
disruptions; therefore, pre-emptive maintenance may avoid unnecessary
interruptions by detecting anomalies before they become failures. Previous work
relies on several inputs and crafting custom features by segmenting the signal.
This not only adds additional requirements for data collection and processing,
but it is also specific to the PM technology, the installed locations and
operational conditions limiting scalability. Based on the available maintenance
records, the main failure causes for PM are obstacles, friction, power source
issues and misalignment. Those failures affect the energy consumption pattern
of PMs, altering the usual (or healthy) shape of the power signal during the PM
movement. In contrast to the current state-of-the-art, our method requires only
one input. We apply a deep learning model to the power signal pattern to
classify if the PM is nominal or associated with any failure type, achieving
>99.99\% precision, <0.01\% false positives and negligible false negatives. Our
methodology is generic and technology-agnostic, proven to be scalable on
several electromechanical PM types deployed in both real-world and test bench
environments. Finally, by using conformal prediction the maintainer gets a
clear indication of the certainty of the system outputs, adding a confidence
layer to operations and making the method compliant with the ISO-17359
standard.

</details>


### [608] [Track Component Failure Detection Using Data Analytics over existing STDS Track Circuit data](https://arxiv.org/abs/2508.11693)
*Francisco López,Eduardo Di Santi,Clément Lefebvre,Nenad Mijatovic,Michele Pugnaloni,Victor Martín,Kenza Saiah*

Main category: eess.SP

TL;DR: The paper addresses automated failure identification in AC track circuits using Support Vector Machines (SVM).


<details>
  <summary>Details</summary>
Motivation: To improve maintenance actions by automatically identifying failing components in track circuits.

Method: The approach involved using current data from the Smart Train Detection System (STDS) and training an SVM classifier to identify 15 failures across three categories.

Result: The method successfully classified all use cases with field data from 10 track circuits, validated by experts.

Conclusion: The SVM-based system effectively identifies track circuit failures, enhancing maintenance efficiency.

Abstract: Track Circuits (TC) are the main signalling devices used to detect the
presence of a train on a rail track. It has been used since the 19th century
and nowadays there are many types depending on the technology. As a general
classification, Track Circuits can be divided into 2 main groups, DC (Direct
Current) and AC (Alternating Current) circuits. This work is focused on a
particular AC track circuit, called "Smart Train Detection System" (STDS),
designed with both high and low-frequency bands. This approach uses STDS
current data applied to an SVM (support vector machine) classifier as a type of
failure identifier. The main purpose of this work consists on determine
automatically which is the component of the track that is failing to improve
the maintenance action. Model was trained to classify 15 different failures
that belong to 3 more general categories. The method was tested with field data
from 10 different track circuits and validated by the STDS track circuit expert
and maintainers. All use cases were correctly classified by the method.

</details>


### [609] [Inductive transfer learning from regression to classification in ECG analysis](https://arxiv.org/abs/2508.11656)
*Ridma Jayasundara,Ishan Fernando,Adeepa Fernando,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: eess.SP

TL;DR: This paper evaluates the use of synthetic ECG data for deep learning tasks, focusing on transfer learning from regression to classification to enhance real-data classification performance.


<details>
  <summary>Details</summary>
Motivation: CVDs are the leading cause of global mortality, requiring timely diagnosis methods. Synthetic ECG data is explored to address privacy concerns while maintaining diagnostic utility.

Method: Researchers employed deep learning models to predict cardiac parameters using regression tasks, followed by transfer learning for 5-class ECG classification.

Result: Transfer learning from regression to classification significantly enhances ECG classification performance, demonstrating effective utilization of synthetic and open-access data.

Conclusion: The study highlights the potential of synthetic ECG data and transfer learning in improving deep learning performance for medical applications, addressing confidentiality and accuracy demands.

Abstract: Cardiovascular diseases (CVDs) are the leading cause of mortality worldwide,
accounting for over 30% of global deaths according to the World Health
Organization (WHO). Importantly, one-third of these deaths are preventable with
timely and accurate diagnosis. The electrocardiogram (ECG), a non-invasive
method for recording the electrical activity of the heart, is crucial for
diagnosing CVDs. However, privacy concerns surrounding the use of patient ECG
data in research have spurred interest in synthetic data, which preserves the
statistical properties of real data without compromising patient
confidentiality. This study explores the potential of synthetic ECG data for
training deep learning models from regression to classification tasks and
evaluates the feasibility of transfer learning to enhance classification
performance on real ECG data. We experimented with popular deep learning models
to predict four key cardiac parameters, namely, Heart Rate (HR), PR interval,
QT interval, and QRS complex-using separate regression models. Subsequently, we
leveraged these regression models for transfer learning to perform 5-class ECG
signal classification. Our experiments systematically investigate whether
transfer learning from regression to classification is viable, enabling better
utilization of diverse open-access and synthetic ECG datasets. Our findings
demonstrate that transfer learning from regression to classification improves
classification performance, highlighting its potential to maximize the utility
of available data and advance deep learning applications in this domain.

</details>


### [610] [Robust Sparse Bayesian Learning Based on Minimum Error Entropy for Noisy High-Dimensional Brain Activity Decoding](https://arxiv.org/abs/2508.11657)
*Yuanhao Li,Badong Chen,Wenjun Bai,Yasuharu Koike,Okito Yamashita*

Main category: eess.SP

TL;DR: The study proposes a robust sparse Bayesian learning framework with an MEE-based likelihood function to enhance brain signal decoding in high-dimensional, noisy datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional sparse Bayesian learning methods are limited by assumptions about data distributions (e.g., Gaussian, binomial) which are insufficient to adequately model noisy brain signals, motivating the need for a more robust decoding framework.

Method: The paper introduces a likelihood model based on the minimum error entropy (MEE) criterion to improve the inference capabilities of sparse Bayesian learning for handling complex and noisy brain signal datasets.

Result: The proposed approach was tested through regression and classification experiments on high-dimensional brain decoding tasks, yielding superior decoding metrics and physiological pattern identifications compared to conventional and state-of-the-art methods.

Conclusion: This MEE-based likelihood model enables improved performance in noise resolution and high-dimensionality challenges in brain decoding tasks, strengthening sparse Bayesian learning applications.

Abstract: Objective: Sparse Bayesian learning provides an effective scheme to solve the
high-dimensional problem in brain signal decoding. However, traditional
assumptions regarding data distributions such as Gaussian and binomial are
potentially inadequate to characterize the noisy signals of brain activity.
Hence, this study aims to propose a robust sparse Bayesian learning framework
to address noisy highdimensional brain activity decoding. Methods: Motivated by
the commendable robustness of the minimum error entropy (MEE) criterion for
handling complex data distributions, we proposed an MEE-based likelihood
function to facilitate the accurate inference of sparse Bayesian learning in
analyzing noisy brain datasets. Results: Our proposed approach was evaluated
using two high-dimensional brain decoding tasks in regression and
classification contexts, respectively. The experimental results showed that,
our approach can realize superior decoding metrics and physiological patterns
than the conventional and state-of-the-art methods. Conclusion: Utilizing the
proposed MEE-based likelihood model, sparse Bayesian learning is empowered to
simultaneously address the challenges of noise and high dimensionality in the
brain decoding task. Significance: This work provides a powerful tool to
realize robust brain decoding, advancing biomedical engineering applications
such as brain-computer interface.

</details>


### [611] [Unsupervised Pairwise Learning Optimization Framework for Cross-Corpus EEG-Based Emotion Recognition Based on Prototype Representation](https://arxiv.org/abs/2508.11663)
*Guangli Li,Canbiao Wu,Zhen Liang*

Main category: eess.SP

TL;DR: The paper introduces McdPL, a domain adversarial transfer learning framework for cross-corpus emotion recognition, addressing challenges due to physiological differences and label noise.


<details>
  <summary>Details</summary>
Motivation: Cross-corpus emotion recognition faces challenges stemming from physiological differences between subjects, experimental variability, and poorly classified samples at decision boundaries.

Method: The proposed McdPL framework uses domain adversarial transfer learning with dual adversarial classifiers (Ada and RMS) and three-stage adversarial training for fine-grained feature alignment. Pairwise learning is incorporated to handle label noise.

Result: Experiments conducted on SEED, SEED-IV, and SEED-V databases demonstrate superior performance of McdPL, achieving average accuracy improvements of 4.76% and 3.97% over baseline models.

Conclusion: McdPL provides a precise solution for cross-corpus emotion recognition with enhanced accuracy, and contributes to the broader adoption of affective computing in brain-computer interfaces.

Abstract: Affective computing is a rapidly developing interdisciplinary research
direction in the field of brain-computer interface. In recent years, the
introduction of deep learning technology has greatly promoted the development
of the field of emotion recognition. However, due to physiological differences
between subjects, as well as the variations in experimental environments and
equipment, cross-corpus emotion recognition faces serious challenges,
especially for samples near the decision boundary. To solve the above problems,
we propose an optimization method based on domain adversarial transfer learning
to fine-grained alignment of affective features, named Maximum classifier
discrepancy with Pairwise Learning (McdPL) framework. In McdPL, we design a
dual adversarial classifier (Ada classifier and RMS classifier), and apply a
three-stage adversarial training to maximize classification discrepancy and
minimize feature distribution to align controversy samples near the decision
boundary. In the process of domain adversarial training, the two classifiers
also maintain an adversarial relationship, ultimately enabling precise
cross-corpus feature alignment. In addition, the introduction of pairwise
learning transforms the classification problem of samples into a similarity
problem between samples, alleviating the influence of label noise. We conducted
systematic experimental evaluation of the model using publicly available SEED,
SEED-IV and SEED-V databases. The results show that the McdPL model is superior
to other baseline models in the cross-corpus emotion recognition task, and the
average accuracy improvements of 4.76\% and 3.97\%, respectively. Our work
provides a promising solution for emotion recognition cross-corpus. The source
code is available at https://github.com/WuCB-BCI/Mcd_PL.

</details>


### [612] [Energy-Efficient Real-Time 4-Stage Sleep Classification at 10-Second Resolution: A Comprehensive Study](https://arxiv.org/abs/2508.11664)
*Zahra Mohammadi,Parnian Fazel,Siamak Mohammadi*

Main category: eess.SP

TL;DR: The paper introduces energy-efficient methods for classifying sleep stages using ECG data, addressing limitations of traditional techniques.


<details>
  <summary>Details</summary>
Motivation: Conventional sleep monitoring methods are costly and impractical for prolonged home use, necessitating more efficient solutions.

Method: The authors developed and compared machine learning and deep learning models using two distinct windowing strategies to process ECG data, alongside hardware optimizations like eight-bit quantization and FPGA deployment.

Result: MobileNet-v1 achieved high accuracy (92%) but consumed significant energy, while a custom SleepLiteCNN model balanced accuracy (89%) and reduced energy usage.

Conclusion: The proposed solution provides a practical, low-power, wearable system for continuous sleep monitoring using single-lead ECG data.

Abstract: Sleep stage classification is crucial for diagnosing and managing disorders
such as sleep apnea and insomnia. Conventional clinical methods like
polysomnography are costly and impractical for long-term home use. We present
an energy-efficient pipeline that detects four sleep stages (wake, REM, light,
and deep) from a single-lead ECG. Two windowing strategies are introduced: (1)
a 5-minute window with 30-second steps for machine-learning models that use
handcrafted features, and (2) a 30-second window with 10-second steps for
deep-learning models, enabling near-real-time 10-second resolution. Lightweight
networks such as MobileNet-v1 reach 92 percent accuracy and 91 percent F1-score
but still draw significant energy. We therefore design SleepLiteCNN, a custom
model that achieves 89 percent accuracy and 89 percent F1-score while lowering
energy use to 5.48 microjoules per inference at 45 nm. Applying eight-bit
quantization preserves accuracy and further reduces power, and FPGA deployment
confirms low resource usage. The proposed system offers a practical solution
for continuous, wearable ECG-based sleep monitoring.

</details>


### [613] [Explainable Deep Neural Network for Multimodal ECG Signals: Intermediate vs Late Fusion](https://arxiv.org/abs/2508.11666)
*Timothy Oladunni,Ehimen Aneni*

Main category: eess.SP

TL;DR: The paper evaluates intermediate and late fusion strategies for multimodal ECG-based cardiovascular disease classification, finding intermediate fusion performs better with high accuracy and mutual information-supported interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of unimodal deep learning models like overfitting and limited generalizability, especially in critical medical AI contexts like ECG-based cardiovascular disease classification.

Method: Comparative evaluation of intermediate (feature-level) and late (decision-level) multimodal fusion strategies using ECG signals from time, frequency, and time-frequency domains. Conducted experiments and interpretability analyses involving saliency maps and mutual information.

Result: Intermediate fusion achieved superior performance with 97% peak accuracy, Cohen's d > 0.8 compared to standalone models, and d = 0.40 versus late fusion. Saliency maps validated alignment with ECG signals, supported statistically by mutual information.

Conclusion: Intermediate fusion not only outperforms late fusion in predictive capability but also offers enhanced explainability, making it suitable for high-stakes medical AI applications surpassing existing state-of-the-art models.

Abstract: The limitations of unimodal deep learning models, particularly their tendency
to overfit and limited generalizability, have renewed interest in multimodal
fusion strategies. Multimodal deep neural networks (MDNN) have the capability
of integrating diverse data domains and offer a promising solution for robust
and accurate predictions. However, the optimal fusion strategy, intermediate
fusion (feature-level) versus late fusion (decision-level) remains
insufficiently examined, especially in high-stakes clinical contexts such as
ECG-based cardiovascular disease (CVD) classification. This study investigates
the comparative effectiveness of intermediate and late fusion strategies using
ECG signals across three domains: time, frequency, and time-frequency. A series
of experiments were conducted to identify the highest-performing fusion
architecture. Results demonstrate that intermediate fusion consistently
outperformed late fusion, achieving a peak accuracy of 97 percent, with Cohen's
d > 0.8 relative to standalone models and d = 0.40 compared to late fusion.
Interpretability analyses using saliency maps reveal that both models align
with the discretized ECG signals. Statistical dependency between the
discretized ECG signals and corresponding saliency maps for each class was
confirmed using Mutual Information (MI). The proposed ECG domain-based
multimodal model offers superior predictive capability and enhanced
explainability, crucial attributes in medical AI applications, surpassing
state-of-the-art models.

</details>


### [614] [Towards Generalizable Human Activity Recognition: A Survey](https://arxiv.org/abs/2508.12213)
*Yize Cai,Baoshen Guo,Flora Salim,Zhiqing Hong*

Main category: eess.SP

TL;DR: This survey reviews 229 papers and 25 datasets on IMU-based Human Activity Recognition (HAR), focusing on methods to improve generalization affected by domain shifts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address poor generalization in HAR systems, which limits their application in real-world settings.

Method: The paper categorizes approaches into model-centric methods (pre-training, end-to-end, LLM-based) and data-centric techniques (multi-modal learning, data augmentation).

Result: The survey highlights the current progress in IMU-based HAR methodologies and datasets, illustrating their broad applicability.

Conclusion: Future directions include better handling of data scarcity, efficient systems, and the integration of advanced modeling techniques like foundation and large language models.

Abstract: As a critical component of Wearable AI, IMU-based Human Activity Recognition
(HAR) has attracted increasing attention from both academia and industry in
recent years. Although HAR performance has improved considerably in specific
scenarios, its generalization capability remains a key barrier to widespread
real-world adoption. For example, domain shifts caused by variations in users,
sensor positions, or environments can significantly decrease the performance in
practice. As a result, in this survey, we explore the rapidly evolving field of
IMU-based generalizable HAR, reviewing 229 research papers alongside 25
publicly available datasets to provide a broad and insightful overview. We
first present the background and overall framework of IMU-based HAR tasks, as
well as the generalization-oriented training settings. Then, we categorize
representative methodologies from two perspectives: (i) model-centric
approaches, including pre-training method, end-to-end method, and large
language model (LLM)-based learning method; and (ii) data-centric approaches,
including multi-modal learning and data augmentation techniques. In addition,
we summarize widely used datasets in this field, as well as relevant tools and
benchmarks. Building on these methodological advances, the broad applicability
of IMU-based HAR is also reviewed and discussed. Finally, we discuss persistent
challenges (e.g., data scarcity, efficient training, and reliable evaluation)
and also outline future directions for HAR, including the adoption of
foundation and large language models, physics-informed and context-aware
reasoning, generative modeling, and resource-efficient training and inference.
The complete list of this survey is available at
https://github.com/rh20624/Awesome-IMU-Sensing, which will be updated
continuously.

</details>


### [615] [Data-driven RF Tomography via Cross-modal Sensing and Continual Learning](https://arxiv.org/abs/2508.11654)
*Yang Zhao,Tao Wang,Said Elhadi*

Main category: eess.SP

TL;DR: The paper introduces DRIFT, a framework combining RF and visual sensors for underground root tuber detection using advanced neural network techniques, offering a 23.2% improvement over previous methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in achieving accurate and robust performance of RF tomography in dynamic environments for underground target detection.

Method: The DRIFT framework employs cross-modal sensing (RF and visual sensors), uses deep neural networks modeled on cross-modal learning, and integrates continual learning to adapt DNNs dynamically.

Result: DRIFT significantly improves detection accuracy with a 23.2% better equivalent diameter error compared to the state-of-the-art, achieving 2.29 cm error on average.

Conclusion: The DRIFT framework enhances underground RF tomography by innovating cross-modal and continual learning methods, showcasing robust accuracy improvements and practical availability of its resources.

Abstract: Data-driven radio frequency (RF) tomography has demonstrated significant
potential for underground target detection, due to the penetrative nature of RF
signals through soil. However, it is still challenging to achieve accurate and
robust performance in dynamic environments. In this work, we propose a
data-driven radio frequency tomography (DRIFT) framework with the following key
components to reconstruct cross section images of underground root tubers, even
with significant changes in RF signals. First, we design a cross-modal sensing
system with RF and visual sensors, and propose to train an RF tomography deep
neural network (DNN) model following the cross-modal learning approach. Then we
propose to apply continual learning to automatically update the DNN model, once
environment changes are detected in a dynamic environment. Experimental results
show that our approach achieves an average equivalent diameter error of 2.29
cm, 23.2% improvement upon the state-of-the-art approach. Our DRIFT code and
dataset are publicly available on https://github.com/Data-driven-RTI/DRIFT.

</details>


### [616] [ATLAS: AI-Native Receiver Test-and-Measurement by Leveraging AI-Guided Search](https://arxiv.org/abs/2508.12204)
*Mauro Belgiovine,Suyash Pradhan,Johannes Lange,Michael Löhning,Kaushik Chowdhury*

Main category: eess.SP

TL;DR: The paper introduces ATLAS, an AI-driven testing approach to evaluate AI-native wireless receivers against classical designs, highlighting targeted failure scenarios and achieving efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: The slow industry adoption of AI-native wireless receivers is due to concerns about the models' explainability and the risks posed by potential failures in diverse wireless scenarios.

Method: ATLAS uses gradient-based optimization to generate targeted tests for specific high-risk configurations, avoiding exhaustive testing of all channel and environment conditions.

Result: ATLAS successfully identifies suboptimal performance in AI-native receivers under certain conditions and reduces test count for each failure by 19% compared to grid search methods.

Conclusion: ATLAS demonstrates efficient and practical testing of AI-native receiver models, proving its value in mitigating risks and promoting their adoption in wireless systems.

Abstract: Industry adoption of Artificial Intelligence (AI)-native wireless receivers,
or even modular, Machine Learning (ML)-aided wireless signal processing blocks,
has been slow. The main concern is the lack of explainability of these trained
ML models and the significant risks posed to network functionalities in case of
failures, especially since (i) testing on every exhaustive case is infeasible
and (ii) the data used for model training may not be available. This paper
proposes ATLAS, an AI-guided approach that generates a battery of tests for
pre-trained AI-native receiver models and benchmarks the performance against a
classical receiver architecture. Using gradient-based optimization, it avoids
spanning the exhaustive set of all environment and channel conditions; instead,
it generates the next test in an online manner to further probe specific
configurations that offer the highest risk of failure. We implement and
validate our approach by adopting the well-known DeepRx AI-native receiver
model as well as a classical receiver using differentiable tensors in NVIDIA's
Sionna environment. ATLAS uncovers specific combinations of mobility, channel
delay spread, and noise, where fully and partially trained variants of
AI-native DeepRx perform suboptimally compared to the classical receivers. Our
proposed method reduces the number of tests required per failure found by 19%
compared to grid search for a 3-parameters input optimization problem,
demonstrating greater efficiency. In contrast, the computational cost of the
grid-based approach scales exponentially with the number of variables, making
it increasingly impractical for high-dimensional problems.

</details>


### [617] [Towards SISO Bistatic Sensing for ISAC](https://arxiv.org/abs/2508.12614)
*Zhongqin Wang,J. Andrew Zhang,Kai Wu,Min Xu,Y. Jay Guo*

Main category: eess.SP

TL;DR: WiDFS 3.0 enhances sensing accuracy in low-cost, single-antenna wireless setups by resolving Doppler ambiguity and phase distortions.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of low-cost, bistatic Single-Input Single-Output (SISO) systems in sensing due to clock asynchrony affecting CSI.

Method: Introduced the SRCC method to remove random phase effects, and applied delay-domain beamforming to mitigate Doppler ambiguity in SISO systems.

Result: WiDFS 3.0 achieved highly accurate delay and Doppler estimation, outperforming prior multi-antenna methods in certain metrics using compact neural networks.

Conclusion: The framework demonstrated robust and generalizable sensing capabilities in complex dynamic scenarios while maintaining low-complexity deployment.

Abstract: Integrated Sensing and Communication (ISAC) is a key enabler for
next-generation wireless systems. However, real-world deployment is often
limited to low-cost, single-antenna transceivers. In such bistatic Single-Input
Single-Output (SISO) setup, clock asynchrony introduces random phase offsets in
Channel State Information (CSI), which cannot be mitigated using conventional
multi-antenna methods. This work proposes WiDFS 3.0, a lightweight bistatic
SISO sensing framework that enables accurate delay and Doppler estimation from
distorted CSI by effectively suppressing Doppler mirroring ambiguity. It
operates with only a single antenna at both the transmitter and receiver,
making it suitable for low-complexity deployments. We propose a
self-referencing cross-correlation (SRCC) method for SISO random phase removal
and employ delay-domain beamforming to resolve Doppler ambiguity. The resulting
unambiguous delay-Doppler-time features enable robust sensing with compact
neural networks. Extensive experiments show that WiDFS 3.0 achieves accurate
parameter estimation, with performance comparable to or even surpassing that of
prior multi-antenna methods, especially in delay estimation. Validated under
single- and multi-target scenarios, the extracted ambiguity-resolved features
show strong sensing accuracy and generalization. For example, when deployed on
the embedded-friendly MobileViT-XXS with only 1.3M parameters, WiDFS 3.0
consistently outperforms conventional features such as CSI amplitude, mirrored
Doppler, and multi-receiver aggregated Doppler.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [618] [BConformeR: A Conformer Based on Mutual Sampling for Unified Prediction of Continuous and Discontinuous Antibody Binding Sites](https://arxiv.org/abs/2508.12029)
*Zhangyu You,Jiahao Ma,Hongzong Li,Ye-Fan Hu,Jian-Dong Huang*

Main category: q-bio.BM

TL;DR: A novel conformer-based model combining CNNs and Transformers improves conformational epitope prediction accuracy by analyzing antigen sequences.


<details>
  <summary>Details</summary>
Motivation: Epitope prediction is essential for vaccine development, diagnostics, and understanding immune responses, but existing methods struggle with conformational epitopes.

Method: The model integrates CNNs for capturing local features and Transformers for identifying long-range dependencies in antigen sequences, trained on 1,080 antigen-antibody complexes.

Result: The proposed model outperforms previous methods in predicting conformational epitopes, with superior metrics like PCC, ROC-AUC, PR-AUC, and F1 scores.

Conclusion: This approach advances conformational epitope prediction, combining CNNs and Transformers effectively to achieve more accurate outcomes.

Abstract: Accurate prediction of antibody-binding sites (epitopes) on antigens is
crucial for vaccine design, immunodiagnostics, therapeutic antibody
development, antibody engineering, research into autoimmune and allergic
diseases, and for advancing our understanding of immune responses. Despite in
silico methods that have been proposed to predict both linear (continuous) and
conformational (discontinuous) epitopes, they consistently underperform in
predicting conformational epitopes. In this work, we propose a conformer-based
model trained on antigen sequences derived from 1,080 antigen-antibody
complexes, leveraging convolutional neural networks (CNNs) to extract local
features and Transformers to capture long-range dependencies within antigen
sequences. Ablation studies demonstrate that CNN enhances the prediction of
linear epitopes, and the Transformer module improves the prediction of
conformational epitopes. Experimental results show that our model outperforms
existing baselines in terms of PCC, ROC-AUC, PR-AUC, and F1 scores on
conformational epitopes.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [619] [What Matters for Bioacoustic Encoding](https://arxiv.org/abs/2508.11845)
*Marius Miron,David Robinson,Milad Alizadeh,Ellen Gilsenan-McMahon,Gagan Narula,Olivier Pietquin,Matthieu Geist,Emmanuel Chemla,Maddie Cusimano,Felix Effenberger,Masato Hagiwara,Benjamin Hoffman,Sara Keen,Diane Kim,Jane Lawton,Jen-Yu Liu,Aza Raskin*

Main category: cs.SD

TL;DR: This paper conducts a large-scale study to improve bioacoustic encoders for diverse tasks using self-supervised and supervised training to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of current bioacoustic encoders, such as narrow species focus, reliance on single model architectures, and limited evaluation on tasks/datasets.

Method: Large-scale empirical study exploring training data diversity, model architectures, and benchmarks; leveraging self-supervised pre-training and supervised post-training on mixed audio datasets.

Result: Encoders achieved state-of-the-art performance across 26 datasets and various tasks (e.g., species classification, detection, individual ID) with insights into the importance of data diversity.

Conclusion: The proposed framework provides a strong foundation for bioacoustic tasks and emphasizes adaptability for future improvements in data and architectures.

Abstract: Bioacoustics, the study of sounds produced by living organisms, plays a vital
role in conservation, biodiversity monitoring, and behavioral studies. Many
tasks in this field, such as species, individual, and behavior classification
and detection, are well-suited to machine learning. However, they often suffer
from limited annotated data, highlighting the need for a general-purpose
bioacoustic encoder capable of extracting useful representations for diverse
downstream tasks. Such encoders have been proposed before, but are often
limited in scope due to a focus on a narrow range of species (typically birds),
and a reliance on a single model architecture or training paradigm. Moreover,
they are usually evaluated on a small set of tasks and datasets. In this work,
we present a large-scale empirical study that covers aspects of bioacoustics
that are relevant to research but have previously been scarcely considered:
training data diversity and scale, model architectures and training recipes,
and the breadth of evaluation tasks and datasets. We obtain encoders that are
state-of-the-art on the existing and proposed benchmarks. We also identify what
matters for training these encoders, such that this work can be extended when
more data are available or better architectures are proposed. Specifically,
across 26 datasets with tasks including species classification, detection,
individual ID, and vocal repertoire discovery, we find self-supervised
pre-training followed by supervised post-training on a mixed bioacoustics +
general-audio corpus yields the strongest in- and out-of-distribution
performance. We show the importance of data diversity in both stages. To
support ongoing research and application, we will release the model
checkpoints.

</details>


### [620] [Audio Flamingo Sound-CoT Technical Report: Improving Chain-of-Thought Reasoning in Sound Understanding](https://arxiv.org/abs/2508.11818)
*Zhifeng Kong,Arushi Goel,Joao Felipe Santos,Sreyan Ghosh,Rafael Valle,Wei Ping,Bryan Catanzaro*

Main category: cs.SD

TL;DR: The paper explores chain-of-thought reasoning in audio-language models, introduces a new benchmark (AF-Reasoning-Eval), and creates a training dataset (AF-CoT-Train). Finetuning improves sound reasoning abilities significantly.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought reasoning shows promise in language and vision models, but has been underexplored in audio language models. The paper aims to address this gap.

Method: The authors created AF-CoT-Train through automatic data transformation pipelines and designed AF-Reasoning-Eval for assessing reasoning abilities. Finetuning was applied to models from the Audio Flamingo series.

Result: Finetuning on AF-CoT-Train led to significant improvements in performance on reasoning benchmarks, showcasing the utility of chain-of-thought processes.

Conclusion: The research highlights the potential of chain-of-thought reasoning in enhancing sound reasoning capabilities within audio-language models.

Abstract: Chain-of-thought reasoning has demonstrated significant improvements in large
language models and vision language models, yet its potential for audio
language models remains largely unexplored. In this technical report, we take a
preliminary step towards closing this gap. For better assessment of sound
reasoning, we propose AF-Reasoning-Eval, a benchmark targeting common-sense
reasoning and the ability to discriminate among closely related choices. To
prepare training corpus for sound reasoning abilities, we propose automatic
pipelines that transform existing audio question answering and classification
data into explicit reasoning chains, yielding AF-CoT-Train with 1.24M samples.
We study the effect of finetuning Audio Flamingo series on AF-CoT-Train and
observe considerable improvements on several reasoning benchmarks, validating
the effectiveness of chain-of-thought finetuning on advanced sound
understanding.

</details>


### [621] [Optimizing Neural Architectures for Hindi Speech Separation and Enhancement in Noisy Environments](https://arxiv.org/abs/2508.12009)
*Arnav Ramamoorthy*

Main category: cs.SD

TL;DR: The paper refines DEMUCS with U-Net and LSTM layers for Hindi speech separation, achieving notable gains in noisy environments, with efficient deployment on edge devices.


<details>
  <summary>Details</summary>
Motivation: To improve Hindi speech separation and enhancement, addressing noise conditions and constraints of edge devices.

Method: The study refines DEMUCS with U-Net and LSTM layers, training on a diverse dataset (400,000 Hindi clips, augmented with ESC-50 and MS-SNSD) and adopts quantization techniques.

Result: The refined model outperforms traditional methods, achieving better PESQ and STOI scores, especially in extreme noise conditions.

Conclusion: Customized AI models, like the refined DEMUCS, are effective for speech processing in Indian contexts; exploring edge-friendly design is viable for resource-constrained deployment.

Abstract: This paper addresses the challenges of Hindi speech separation and
enhancement using advanced neural network architectures, with a focus on edge
devices. We propose a refined approach leveraging the DEMUCS model to overcome
limitations of traditional methods, achieving substantial improvements in
speech clarity and intelligibility. The model is fine-tuned with U-Net and LSTM
layers, trained on a dataset of 400,000 Hindi speech clips augmented with
ESC-50 and MS-SNSD for diverse acoustic environments. Evaluation using PESQ and
STOI metrics shows superior performance, particularly under extreme noise
conditions. To ensure deployment on resource-constrained devices like TWS
earbuds, we explore quantization techniques to reduce computational
requirements. This research highlights the effectiveness of customized AI
algorithms for speech processing in Indian contexts and suggests future
directions for optimizing edge-based architectures.

</details>


### [622] [HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization](https://arxiv.org/abs/2508.12292)
*Hyebin Ahn,Kangwook Jang,Hoirin Kim*

Main category: cs.SD

TL;DR: The paper addresses the challenge of noise robustness in speech foundation models and proposes a solution called HuBERT-VIC that integrates variance, in-variance, and covariance regularization objectives.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve noise robustness in speech models, which typically degrade in performance when exposed to noisy inputs due to being trained largely on clean data.

Method: The proposed method involves the use of variance, in-variance, and covariance regularization objectives (VICReg) to adapt the statistics of noisy speech representations, aiming to diversify acoustic characteristics and enhance noise generalization.

Result: Relative performance improvements of 23.3% on LibriSpeech test-clean and 13.2% on test-other datasets were achieved using the HuBERT-VIC model compared to a baseline pre-trained on noisy speech.

Conclusion: The study successfully demonstrates that integrating VICReg objectives improves noise robustness in speech foundation models, enhancing their generalization and reducing performance degradation under noisy conditions.

Abstract: Noise robustness in speech foundation models (SFMs) has been a critical
challenge, as most models are primarily trained on clean data and experience
performance degradation when the models are exposed to noisy speech. To address
this issue, we propose HuBERT-VIC, a noise-robust SFM with variance,
in-variance, and covariance regularization (VICReg) objectives. These
objectives adjust the statistics of noisy speech representations, enabling the
model to capture diverse acoustic characteristics and improving the
generalization ability across different types of noise. When applied to HuBERT,
our model shows relative performance improvements of 23.3% on LibriSpeech
test-clean and 13.2% on test-other, compared to the baseline model pre-trained
on noisy speech.

</details>


### [623] [MATPAC++: Enhanced Masked Latent Prediction for Self-Supervised Audio Representation Learning](https://arxiv.org/abs/2508.12709)
*Aurian Quelennec,Pierre Chouteau,Geoffroy Peeters,Slim Essid*

Main category: cs.SD

TL;DR: This paper introduces MATPAC++, an enhanced self-supervised learning model for audio and music, incorporating Multiple Choice Learning (MCL) to handle prediction ambiguity effectively and improve representation quality.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the overlooked role of the predictor module in self-supervised learning systems for audio, especially in managing ambiguous audio content with multiple sound sources.

Method: The proposed method, MATPAC++, integrates Multiple Choice Learning (MCL) into the MATPAC system to explicitly address prediction ambiguity and improve on pretext tasks like prediction and unsupervised classification. The system is evaluated rigorously using a unified protocol.

Result: MATPAC++ achieves state-of-the-art performance when fine-tuned on AudioSet and sets an overall benchmark in downstream tasks. It also shows significant efficiency gains in music-specific training.

Conclusion: MATPAC++ demonstrates the importance of explicitly modeling prediction ambiguity in SSL for audio, providing superior representation quality and performance across tasks while being more efficient in domain-specific scenarios.

Abstract: Masked latent prediction has emerged as a leading paradigm in self-supervised
learning (SSL), especially for general audio and music representation learning.
While recent methods have demonstrated strong performance, the role of the
predictor module used at the output of such SSL systems remains mainly
overlooked, despite being crucial for solving the pretext task at hand. In
particular, this module should be able to deal with the ambiguity inherent in
audio content, especially when it is composed of multiple sound sources. This
work proposes a novel enhancement: integrating Multiple Choice Learning (MCL)
to explicitly model prediction ambiguity and improve representation quality. We
build on top of the recently proposed MATPAC system, improving its prediction
and unsupervised classification pretext tasks with MCL. We extensively evaluate
our method, MATPAC++, through both linear probing across multiple downstream
tasks and fine-tuning on AudioSet, employing a unified protocol that enables
rigorous and fair comparisons with state-of-the-art SSL approaches. Results
show that our proposal achieves state-of-the-art when fine-tuned on AudioSet
and overall state-of-the-art scores on downstream tasks. Additionally, we
examine domain specialisation by training exclusively on music data, where our
model achieves state-of-the-art performance with significantly improved
efficiency.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [624] [DermINO: Hybrid Pretraining for a Versatile Dermatology Foundation Model](https://arxiv.org/abs/2508.12190)
*Jingkai Xu,De Cheng,Xiangqian Zhao,Jungang Yang,Zilong Wang,Xinyang Jiang,Xufang Luo,Lili Chen,Xiaoli Ning,Chengxu Li,Xinzhu Zhou,Xuejiao Song,Ang Li,Qingyue Xia,Zhou Zhuang,Hongfei Ouyang,Ke Xue,Yujun Sheng,Rusong Meng,Feng Xu,Xi Yang,Weimin Ma,Yusheng Lee,Dongsheng Li,Xinbo Gao,Jianming Liang,Lili Qiu,Nannan Wang,Xianbo Zuo,Cui Yong*

Main category: eess.IV

TL;DR: The paper introduces DermNIO, an advanced AI model for dermatology, outperforming state-of-the-art models in clinical and low-level tasks, and enhancing clinician accuracy through AI assistance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the global burden of skin diseases exacerbated by their prevalence, diagnostic complexity, and lack of dermatologists in resource-limited areas. Current AI tools are inadequate for real-world applications due to reliance on narrow, manually labeled datasets.

Method: The paper presents DermNIO, trained on 432,776 images from diverse sources, using a hybrid pretraining framework combining self-supervised, semi-supervised learning, and knowledge-guided prototype initialization, to generalize across clinical tasks and improve understanding of dermatological conditions.

Result: DermNIO outperformed state-of-the-art models in tasks such as malignancy classification, disease severity grading, multi-category diagnosis, and image captioning. It also excelled in skin lesion segmentation and proved robust with federated learning and diverse skin types and sexes.

Conclusion: DermNIO significantly enhances dermatological diagnostics and clinical workflow, achieving 95.79% diagnostic accuracy in a blinded reader study and improving clinician diagnostic performance by 17.21%.

Abstract: Skin diseases impose a substantial burden on global healthcare systems,
driven by their high prevalence (affecting up to 70% of the population),
complex diagnostic processes, and a critical shortage of dermatologists in
resource-limited areas. While artificial intelligence(AI) tools have
demonstrated promise in dermatological image analysis, current models face
limitations-they often rely on large, manually labeled datasets and are built
for narrow, specific tasks, making them less effective in real-world settings.
To tackle these limitations, we present DermNIO, a versatile foundation model
for dermatology. Trained on a curated dataset of 432,776 images from three
sources (public repositories, web-sourced images, and proprietary collections),
DermNIO incorporates a novel hybrid pretraining framework that augments the
self-supervised learning paradigm through semi-supervised learning and
knowledge-guided prototype initialization. This integrated method not only
deepens the understanding of complex dermatological conditions, but also
substantially enhances the generalization capability across various clinical
tasks. Evaluated across 20 datasets, DermNIO consistently outperforms
state-of-the-art models across a wide range of tasks. It excels in high-level
clinical applications including malignancy classification, disease severity
grading, multi-category diagnosis, and dermatological image caption, while also
achieving state-of-the-art performance in low-level tasks such as skin lesion
segmentation. Furthermore, DermNIO demonstrates strong robustness in
privacy-preserving federated learning scenarios and across diverse skin types
and sexes. In a blinded reader study with 23 dermatologists, DermNIO achieved
95.79% diagnostic accuracy (versus clinicians' 73.66%), and AI assistance
improved clinician performance by 17.21%.

</details>


### [625] [FractMorph: A Fractional Fourier-Based Multi-Domain Transformer for Deformable Image Registration](https://arxiv.org/abs/2508.12445)
*Shayan Kebriti,Shahabedin Nabavi,Ali Gooya*

Main category: eess.IV

TL;DR: The paper introduces FractMorph, a 3D dual-parallel transformer architecture for deformable image registration in medical images, achieving state-of-the-art accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to deformable image registration struggle to simultaneously handle fine-grained local deformations and large-scale global deformations within a unified framework.

Method: The method involves FractMorph, which uses Fractional Cross-Attention blocks leveraging fractional Fourier transforms (FrFTs) along different angles to extract multi-domain spectral-spatial features. Features are fused through cross-attention, followed by a lightweight U-Net predicting the deformation field.

Result: FractMorph achieves an overall Dice Similarity Coefficient (DSC) of 86.45%, average per-structure DSC of 75.15%, and HD95 of 1.54 mm on the ACDC cardiac MRI dataset. FractMorph-Light maintains high performance while using around half the memory (29.6M parameters).

Conclusion: FractMorph demonstrates robust, efficient modeling of complex non-rigid deformations in medical images using a single end-to-end network, eliminating the need for scenario-specific tuning or multi-scale architectures.

Abstract: Deformable image registration (DIR) is a crucial and challenging technique
for aligning anatomical structures in medical images and is widely applied in
diverse clinical applications. However, existing approaches often struggle to
capture fine-grained local deformations and large-scale global deformations
simultaneously within a unified framework. We present FractMorph, a novel 3D
dual-parallel transformer-based architecture that enhances cross-image feature
matching through multi-domain fractional Fourier transform (FrFT) branches.
Each Fractional Cross-Attention (FCA) block applies parallel FrFTs at
fractional angles of 0{\deg}, 45{\deg}, 90{\deg}, along with a log-magnitude
branch, to effectively extract local, semi-global, and global features at the
same time. These features are fused via cross-attention between the fixed and
moving image streams. A lightweight U-Net style network then predicts a dense
deformation field from the transformer-enriched features. On the ACDC cardiac
MRI dataset, FractMorph achieves state-of-the-art performance with an overall
Dice Similarity Coefficient (DSC) of 86.45%, an average per-structure DSC of
75.15%, and a 95th-percentile Hausdorff distance (HD95) of 1.54 mm on our data
split. We also introduce FractMorph-Light, a lightweight variant of our model
with only 29.6M parameters, which maintains the superior accuracy of the main
model while using approximately half the memory. Our results demonstrate that
multi-domain spectral-spatial attention in transformers can robustly and
efficiently model complex non-rigid deformations in medical images using a
single end-to-end network, without the need for scenario-specific tuning or
hierarchical multi-scale networks. The source code of our implementation is
available at https://github.com/shayankebriti/FractMorph.

</details>


### [626] [Segmenting Thalamic Nuclei: T1 Maps Provide a Reliable and Efficient Solution](https://arxiv.org/abs/2508.12508)
*Anqi Feng,Zhangxing Bian,Samuel W. Remedios,Savannah P. Hays,Blake E. Dewey,Jiachen Zhuo,Dan Benjamini,Jerry L. Prince*

Main category: eess.IV

TL;DR: The paper evaluates optimal MRI inputs for thalamic nuclei segmentation and concludes that T1 maps are the most effective, while PD maps offer no added value.


<details>
  <summary>Details</summary>
Motivation: Understanding neurological diseases and brain functions requires accurate thalamic nuclei segmentation, but the best MRI inputs for this purpose are unclear.

Method: The study systematically evaluates various MRI contrasts, employs gradient-based saliency analysis with Monte Carlo dropout for image selection, and trains a 3D U-Net model.

Result: T1 maps provide strong quantitative performance and superior qualitative outcomes, while PD maps do not contribute to better segmentation.

Conclusion: T1 maps are reliable and efficient for thalamic nuclei segmentation, offering valuable guidance for clinical and research imaging optimization.

Abstract: Accurate thalamic nuclei segmentation is crucial for understanding
neurological diseases, brain functions, and guiding clinical interventions.
However, the optimal inputs for segmentation remain unclear. This study
systematically evaluates multiple MRI contrasts, including MPRAGE and FGATIR
sequences, quantitative PD and T1 maps, and multiple T1-weighted images at
different inversion times (multi-TI), to determine the most effective inputs.
For multi-TI images, we employ a gradient-based saliency analysis with Monte
Carlo dropout and propose an Overall Importance Score to select the images
contributing most to segmentation. A 3D U-Net is trained on each of these
configurations. Results show that T1 maps alone achieve strong quantitative
performance and superior qualitative outcomes, while PD maps offer no added
value. These findings underscore the value of T1 maps as a reliable and
efficient input among the evaluated options, providing valuable guidance for
optimizing imaging protocols when thalamic structures are of clinical or
research interest.

</details>


### [627] [Anatomic Feature Fusion Model for Diagnosing Calcified Pulmonary Nodules on Chest X-Ray](https://arxiv.org/abs/2508.12562)
*Hyeonjin Choi,Yang-gon Kim,Dong-yeon Yoo,Ju-sung Sun,Jung-won Lee*

Main category: eess.IV

TL;DR: This study introduces a calcification classification model for pulmonary nodules on chest X-rays, demonstrating improved accuracy and AUC compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of varied physician interpretations and anatomical interference in diagnosing pulmonary nodule calcifications on chest X-rays.

Method: The model utilizes fused features from raw chest X-ray images and structure-suppressed variants to reduce interference by overlapping anatomical elements.

Result: The model achieved an accuracy of 86.52% and AUC of 0.8889, outperforming models using only raw images by 3.54% in accuracy and 0.0385 in AUC.

Conclusion: The proposed model enhances diagnostic performance for pulmonary nodule calcifications, offering a reliable alternative to visual assessments.

Abstract: Accurate and timely identification of pulmonary nodules on chest X-rays can
differentiate between life-saving early treatment and avoidable invasive
procedures. Calcification is a definitive indicator of benign nodules and is
the primary foundation for diagnosis. In actual practice, diagnosing pulmonary
nodule calcification on chest X-rays predominantly depends on the physician's
visual assessment, resulting in significant diversity in interpretation.
Furthermore, overlapping anatomical elements, such as ribs and spine,
complicate the precise identification of calcification patterns. This study
presents a calcification classification model that attains strong diagnostic
performance by utilizing fused features derived from raw images and their
structure-suppressed variants to reduce structural interference. We used 2,517
lesion-free images and 656 nodule images (151 calcified nodules and 550
non-calcified nodules), all obtained from Ajou University Hospital. The
suggested model attained an accuracy of 86.52% and an AUC of 0.8889 in
calcification diagnosis, surpassing the model trained on raw images by 3.54%
and 0.0385, respectively.

</details>


### [628] [Learning local and global prototypes with optimal transport for unsupervised anomaly detection and localization](https://arxiv.org/abs/2508.12927)
*Robin Trombetta,Carole Lartizien*

Main category: eess.IV

TL;DR: This paper introduces a novel unsupervised anomaly detection (UAD) method using prototype learning and optimal transport with a pre-trained image encoder, achieving competitive performance on industrial image benchmarks.


<details>
  <summary>Details</summary>
Motivation: Unsupervised anomaly detection is crucial in contexts where labeled data is scarce or costly, and there’s a need to avoid bias in detecting anomalies, such as in industrial inspection or medical imaging.

Method: The authors developed a UAD method leveraging prototype learning and a metric to compare structured embeddings. This balances feature-based and spatial-based costs, utilizes optimal transport, and enforces structural constraints to better capture the organization of normal samples.

Result: The proposed model matches the performance of strong baselines on two key benchmarks for industrial image anomaly detection.

Conclusion: The method effectively improves detection of incoherencies in images by capturing the structure of normal samples, demonstrating its potential application in real-world scenarios.

Abstract: Unsupervised anomaly detection aims to detect defective parts of a sample by
having access, during training, to a set of normal, i.e. defect-free, data. It
has many applications in fields, such as industrial inspection or medical
imaging, where acquiring labels is costly or when we want to avoid introducing
biases in the type of anomalies that can be spotted. In this work, we propose a
novel UAD method based on prototype learning and introduce a metric to compare
a structured set of embeddings that balances a feature-based cost and a
spatial-based cost. We leverage this metric to learn local and global
prototypes with optimal transport from latent representations extracted with a
pre-trained image encoder. We demonstrate that our approach can enforce a
structural constraint when learning the prototypes, allowing to capture the
underlying organization of the normal samples, thus improving the detection of
incoherencies in images. Our model achieves performance that is on par with
strong baselines on two reference benchmarks for anomaly detection on
industrial images. The code is available at
https://github.com/robintrmbtt/pradot.

</details>


### [629] [From Transthoracic to Transesophageal: Cross-Modality Generation using LoRA Diffusion](https://arxiv.org/abs/2508.13077)
*Emmanuel Oladokun,Yuxuan Ou,Anna Novikova,Daria Kulikova,Sarina Thomas,Jurica Šprem,Vicente Grau*

Main category: eess.IV

TL;DR: This paper introduces a method to adapt mask-conditioned diffusion models, originally trained on transthoracic echocardiography (TTE), for transesophageal echocardiography (TEE) using minimal new data.


<details>
  <summary>Details</summary>
Motivation: Deep learning in TEE is limited by the scarcity of training data, despite the significant potential impact. Existing techniques have been more successful in TTE but fail to address underrepresentation in TEE.

Method: The authors develop a pipeline using Low-Rank Adaptation and a novel lightweight remapping layer, MaskR$^2$, to align TEE-specific masks with a pretrained TTE model's conditioning. They adapt only the MLP layers, requiring a small number of parameters.

Result: The adaptation achieves high-fidelity synthetic TEE image generation and improves the segmentation dice score when incorporating less than 200 real TEE frames, especially for underrepresented right-heart structures.

Conclusion: Semantically controlled TEE generation with low computational cost is feasible, MaskR$^2$ effectively remaps unseen mask formats, and the generated images are effective for improving segmentation performance on downstream tasks.

Abstract: Deep diffusion models excel at realistic image synthesis but demand large
training sets-an obstacle in data-scarce domains like transesophageal
echocardiography (TEE). While synthetic augmentation has boosted performance in
transthoracic echo (TTE), TEE remains critically underrepresented, limiting the
reach of deep learning in this high-impact modality.
  We address this gap by adapting a TTE-trained, mask-conditioned diffusion
backbone to TEE with only a limited number of new cases and adapters as small
as $10^5$ parameters. Our pipeline combines Low-Rank Adaptation with MaskR$^2$,
a lightweight remapping layer that aligns novel mask formats with the
pretrained model's conditioning channels. This design lets users adapt models
to new datasets with a different set of anatomical structures to the base
model's original set.
  Through a targeted adaptation strategy, we find that adapting only MLP layers
suffices for high-fidelity TEE synthesis. Finally, mixing less than 200 real
TEE frames with our synthetic echoes improves the dice score on a multiclass
segmentation task, particularly boosting performance on underrepresented
right-heart structures. Our results demonstrate that (1) semantically
controlled TEE images can be generated with low overhead, (2) MaskR$^2$
effectively transforms unseen mask formats into compatible formats without
damaging downstream task performance, and (3) our method generates images that
are effective for improving performance on a downstream task of multiclass
segmentation.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [630] [On the complexity of constrained reconfiguration and motion planning](https://arxiv.org/abs/2508.13032)
*Nicolas Bousquet,Remy El Sabeh,Amer E. Mouawad,Naomi Nishimura*

Main category: cs.CC

TL;DR: This paper addresses the "k-Compatible Ordering" problem, exploring how to coordinate the movements or state changes of multiple agents without conflicts. It demonstrates the problem's complexity (NP-completeness) and provides efficient solutions for specific cases.


<details>
  <summary>Details</summary>
Motivation: The aim is to tackle the challenge of coordinating the motion of multiple agents (like robotic arms) in restricted environments, ensuring no collisions and minimal state changes.

Method: The study analyzes the "k-Compatible Ordering" problem theoretically, proving its NP-completeness in general cases and proposing polynomial-time solutions when specific constraints (like bounded treewidth) are present.

Result: 1. The general "k-Compatible Ordering" problem is NP-complete, even under constrained graph structures. 2. Polynomial-time algorithms are devised for scenarios with $k=1$ and when specific structural properties (like bounded treewidth) are present.

Conclusion: The proposed framework broadens its scope to accommodate diverse state-changing actions and applies to multiple domains such as robotics, scheduling, and motion planning. The work provides both theoretical and practical insights into resolving constrained coordination issues effectively.

Abstract: Coordinating the motion of multiple agents in constrained environments is a
fundamental challenge in robotics, motion planning, and scheduling. A
motivating example involves $n$ robotic arms, each represented as a line
segment. The objective is to rotate each arm to its vertical orientation, one
at a time (clockwise or counterclockwise), without collisions nor rotating any
arm more than once. This scenario is an example of the more general
$k$-Compatible Ordering problem, where $n$ agents, each capable of $k$
state-changing actions, must transition to specific target states under
constraints encoded as a set $\mathcal{G}$ of $k$ pairs of directed graphs.
  We show that $k$-Compatible Ordering is $\mathsf{NP}$-complete, even when
$\mathcal{G}$ is planar, degenerate, or acyclic. On the positive side, we
provide polynomial-time algorithms for cases such as when $k = 1$ or
$\mathcal{G}$ has bounded treewidth. We also introduce generalized variants
supporting multiple state-changing actions per agent, broadening the
applicability of our framework. These results extend to a wide range of
scheduling, reconfiguration, and motion planning applications in constrained
environments.

</details>
