<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 14]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.CL](#cs.CL) [Total: 14]
- [cs.CV](#cs.CV) [Total: 13]
- [cs.DC](#cs.DC) [Total: 17]
- [cs.LG](#cs.LG) [Total: 15]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 14]
- [cs.SE](#cs.SE) [Total: 13]
- [q-bio.NC](#q-bio.NC) [Total: 9]
- [stat.ML](#stat.ML) [Total: 13]
- [cs.ET](#cs.ET) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.LO](#cs.LO) [Total: 2]
- [math.NA](#math.NA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Situation Model of the Transport, Transport Emissions and Meteorological Conditions](https://arxiv.org/abs/2509.10541)
*V. Benes,M. Svitek,A. Michalikova,M. Melicherik*

Main category: cs.AI

TL;DR: The paper addresses urban air pollution from traffic, linking it to weather effects, and uses a predictive fuzzy inference system (FIS) model based on data from Prague.


<details>
  <summary>Details</summary>
Motivation: Urban air pollution remains a key challenge for modern society, and there is a need to reduce traffic-related emissions while considering environmental factors.

Method: The paper develops a predictive model using fuzzy inference systems (FIS), incorporating traffic, meteorological, and emission data from Prague.

Result: The model predicts changes in traffic emissions based on varying meteorological and traffic conditions.

Conclusion: The work highlights the importance of integrating meteorological factors into urban planning for managing traffic emissions and aiding policymakers in creating environmentally friendly urban transport systems.

Abstract: Air pollution in cities and the possibilities of reducing this pollution
represents one of the most important factors that today's society has to deal
with. This paper focuses on a systemic approach to traffic emissions with their
relation to meteorological conditions, analyzing the effect of weather on the
quantity and dispersion of traffic emissions in a city. Using fuzzy inference
systems (FIS) the model for prediction of changes in emissions depending on
various conditions is developed. The proposed model is based on traffic,
meteorology and emission data measured in Prague, Czech Republic. The main
objective of the work is to provide insight into how urban planners and
policymakers can plan and manage urban transport more effectively with
environmental protection in mind.

</details>


### [2] [ZapGPT: Free-form Language Prompting for Simulated Cellular Control](https://arxiv.org/abs/2509.10660)
*Nam H. Le,Patrick Erickson,Yanbo Zhang,Michael Levin,Josh Bongard*

Main category: cs.AI

TL;DR: The paper demonstrates the ability to guide collective behavior of simulated agents with natural language prompts, removing the need for task-specific engineering and achieving generalization to unseen instructions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap where artificial and biological systems struggle to interpret and respond meaningfully to language, which could enable natural control over complex systems.

Method: The proposed approach utilizes two AI models—one transforms language prompts into interventions for cells, and the other evaluates the results. By evolving the first model based on evaluation scores, they remove the need for engineered rewards or task-specific tuning.

Result: The system successfully generalizes to unseen prompts without needing retraining, showcasing the feasibility of free-form language as a control layer.

Conclusion: This method represents a significant step towards AI-biology integration, suggesting future possibilities where natural language guides computational and biological systems.

Abstract: Human language is one of the most expressive tools for conveying intent, yet
most artificial or biological systems lack mechanisms to interpret or respond
meaningfully to it. Bridging this gap could enable more natural forms of
control over complex, decentralized systems. In AI and artificial life, recent
work explores how language can specify high-level goals, but most systems still
depend on engineered rewards, task-specific supervision, or rigid command sets,
limiting generalization to novel instructions. Similar constraints apply in
synthetic biology and bioengineering, where the locus of control is often
genomic rather than environmental perturbation.
  A key open question is whether artificial or biological collectives can be
guided by free-form natural language alone, without task-specific tuning or
carefully designed evaluation metrics. We provide one possible answer here by
showing, for the first time, that simple agents' collective behavior can be
guided by free-form language prompts: one AI model transforms an imperative
prompt into an intervention that is applied to simulated cells; a second AI
model scores how well the prompt describes the resulting cellular dynamics; and
the former AI model is evolved to improve the scores generated by the latter.
  Unlike previous work, our method does not require engineered fitness
functions or domain-specific prompt design. We show that the evolved system
generalizes to unseen prompts without retraining. By treating natural language
as a control layer, the system suggests a future in which spoken or written
prompts could direct computational, robotic, or biological systems to desired
behaviors. This work provides a concrete step toward this vision of AI-biology
partnerships, in which language replaces mathematical objective functions,
fixed rules, and domain-specific programming.

</details>


### [3] [Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration](https://arxiv.org/abs/2509.10704)
*Xingchen Wan,Han Zhou,Ruoxi Sun,Hootan Nakhost,Ke Jiang,Rajarishi Sinha,Sercan Ö. Arık*

Main category: cs.AI

TL;DR: The paper introduces Maestro, a self-evolving system improving text-to-image models by iteratively evolving prompts using critiques from multimodal agents.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models require significant human intervention for prompt engineering, posing usability challenges.

Method: Maestro employs a two-step process: self-critique for identifying image weaknesses through multimodal agents, and self-evolution for prompt iteration guided by human and system validation.

Result: Maestro enhances image quality from initial prompts and outperforms state-of-the-art automated systems in experiments involving complex tasks.

Conclusion: The study provides an effective and interpretable solution to improve text-to-image model outputs by enabling iterative self-improvement of prompts.

Abstract: Text-to-image (T2I) models, while offering immense creative potential, are
highly reliant on human intervention, posing significant usability challenges
that often necessitate manual, iterative prompt engineering over often
underspecified prompts. This paper introduces Maestro, a novel self-evolving
image generation system that enables T2I models to autonomously self-improve
generated images through iterative evolution of prompts, using only an initial
prompt. Maestro incorporates two key innovations: 1) self-critique, where
specialized multimodal LLM (MLLM) agents act as 'critics' to identify
weaknesses in generated images, correct for under-specification, and provide
interpretable edit signals, which are then integrated by a 'verifier' agent
while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge
for head-to-head comparisons between iteratively generated images, eschewing
problematic images, and evolving creative prompt candidates that align with
user intents. Extensive experiments on complex T2I tasks using black-box models
demonstrate that Maestro significantly improves image quality over initial
prompts and state-of-the-art automated methods, with effectiveness scaling with
more advanced MLLM components. This work presents a robust, interpretable, and
effective pathway towards self-improving T2I generation.

</details>


### [4] [Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions](https://arxiv.org/abs/2509.10707)
*Sajjad Abdoli,Rudi Cilibrasi,Rima Al-Shikh*

Main category: cs.AI

TL;DR: This paper investigates the evaluation behaviors of GPT models in assessing AI-generated outputs, revealing distinct 'evaluation personalities' and biases.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the behaviors, strategies, and biases of AI systems like GPT when they evaluate outputs from other AI systems to prevent cascading biases in large-scale AI evaluations.

Method: The study analyzes descriptions created by NVIDIA's Describe Anything Model and assesses them using three GPT models. Controlled experiments with Gemini 2.5 Pro as an independent generator validate findings, and cross-family analyses highlight differences between GPT and Gemini evaluation tactics.

Result: GPT-4o-mini showcases consistency, GPT-4o excels at error detection, and GPT-5 is highly variable and conservative. All GPT models demonstrate a 2:1 bias favoring negative over positive assessments, but this pattern is family-specific.

Conclusion: Robust AI evaluation requires diverse perspectives from different AI architectures, as evaluation competence does not scale directly with general AI capability.

Abstract: As AI systems increasingly evaluate other AI outputs, understanding their
assessment behavior becomes crucial for preventing cascading biases. This study
analyzes vision-language descriptions generated by NVIDIA's Describe Anything
Model and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to
uncover distinct "evaluation personalities" the underlying assessment
strategies and biases each model demonstrates. GPT-4o-mini exhibits systematic
consistency with minimal variance, GPT-4o excels at error detection, while
GPT-5 shows extreme conservatism with high variability. Controlled experiments
using Gemini 2.5 Pro as an independent question generator validate that these
personalities are inherent model properties rather than artifacts. Cross-family
analysis through semantic similarity of generated questions reveals significant
divergence: GPT models cluster together with high similarity while Gemini
exhibits markedly different evaluation strategies. All GPT models demonstrate a
consistent 2:1 bias favoring negative assessment over positive confirmation,
though this pattern appears family-specific rather than universal across AI
architectures. These findings suggest that evaluation competence does not scale
with general capability and that robust AI assessment requires diverse
architectural perspectives.

</details>


### [5] [AI Answer Engine Citation Behavior An Empirical Analysis of the GEO16 Framework](https://arxiv.org/abs/2509.10762)
*Arlen Kumar,Leanid Palkhouski*

Main category: cs.AI

TL;DR: The paper presents GEO-16, a framework to audit citation quality in AI answer engines. It evaluates page features and provides a scoring system to predict citation likelihood.


<details>
  <summary>Details</summary>
Motivation: There is a growing need to understand how AI answer engines select and cite web sources when generating responses for users.

Method: The authors introduced a 16-pillar auditing framework called GEO-16, collected 1,702 citations from three AI engines using 70 prompts, and assessed quality metrics on 1,100 unique URLs.

Result: The study found differences among engines in citation page quality. Logistic models showed strong correlations between citation likelihood and high GEO scores, along with specific page quality metrics like metadata.

Conclusion: Findings suggest ways publishers can optimize for higher citations by focusing on page quality attributes. The study provides a practical playbook and highlights its scope, limitations, and reproducibility.

Abstract: AI answer engines increasingly mediate access to domain knowledge by
generating responses and citing web sources. We introduce GEO-16, a 16 pillar
auditing framework that converts on page quality signals into banded pillar
scores and a normalized GEO score G that ranges from 0 to 1. Using 70 product
intent prompts, we collected 1,702 citations across three engines (Brave
Summary, Google AI Overviews, and Perplexity) and audited 1,100 unique URLs. In
our corpus, the engines differed in the GEO quality of the pages they cited,
and pillars related to Metadata and Freshness, Semantic HTML, and Structured
Data showed the strongest associations with citation. Logistic models with
domain clustered standard errors indicate that overall page quality is a strong
predictor of citation, and simple operating points (for example, G at least
0.70 combined with at least 12 pillar hits) align with substantially higher
citation rates in our data. We report per engine contrasts, vertical effects,
threshold analysis, and diagnostics, then translate findings into a practical
playbook for publishers. The study is observational and focuses on English
language B2B SaaS pages; we discuss limitations, threats to validity, and
reproducibility considerations.

</details>


### [6] [AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise](https://arxiv.org/abs/2509.10769)
*Tara Bogavelli,Roshnee Sharma,Hari Subramani*

Main category: cs.AI

TL;DR: This paper evaluates 18 agentic configurations of large language models across four system dimensions to identify optimal designs for enterprise tasks.


<details>
  <summary>Details</summary>
Motivation: The study aims to address gaps in understanding of how design dimensions interact in multi-agent systems, especially within enterprise applications.

Method: The authors benchmark and evaluate 18 distinct agentic configurations, examining orchestration strategy, prompt implementation, memory architecture, and tool integration.

Result: They find model-specific architectural preferences and highlight weaknesses, with success rates ranging only from 35.3% to 70.8% on enterprise tasks.

Conclusion: The findings challenge the one-size-fits-all approach in agentic AI design and aim to inform future system architectures for improved performance.

Abstract: While individual components of agentic architectures have been studied in
isolation, there remains limited empirical understanding of how different
design dimensions interact within complex multi-agent systems. This study aims
to address these gaps by providing a comprehensive enterprise-specific
benchmark evaluating 18 distinct agentic configurations across state-of-the-art
large language models. We examine four critical agentic system dimensions:
orchestration strategy, agent prompt implementation (ReAct versus function
calling), memory architecture, and thinking tool integration. Our benchmark
reveals significant model-specific architectural preferences that challenge the
prevalent one-size-fits-all paradigm in agentic AI systems. It also reveals
significant weaknesses in overall agentic performance on enterprise tasks with
the highest scoring models achieving a maximum of only 35.3\% success on the
more complex task and 70.8\% on the simpler task. We hope these findings inform
the design of future agentic systems by enabling more empirically backed
decisions regarding architectural components and model selection.

</details>


### [7] [LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering](https://arxiv.org/abs/2509.10818)
*Boris Kovalerchuk,Brent D. Fegley*

Main category: cs.AI

TL;DR: The paper discusses enhancing decision-making using LLMs by addressing their hallucination issues and introducing a computationally tractable expert mental model (EMM) for improved prompt engineering.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the challenges faced by LLMs in decision-making tasks due to hallucinations and gaps in knowledge, while advancing techniques to better integrate expert mental models into computational processes.

Method: It introduces a four-step EMM algorithm involving factor identification, hierarchical structuring, specification generation, and detailed model creation to improve LLM prompt engineering.

Result: The proposed methodology outlines how human-machine dialogues and Boolean functions can help create the expert mental model, addressing missing critical information gaps.

Conclusion: The structured EMM technique can make LLM-based decision-making more accurate and efficient, accommodating complex domain expertise for challenging tasks like deciding on proposal responses.

Abstract: Difficult decision-making problems abound in various disciplines and domains.
The proliferation of generative techniques, especially large language models
(LLMs), has excited interest in using them for decision support. However, LLMs
cannot yet resolve missingness in their training data, leading to
hallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by
incorporating external information retrieval, reducing hallucinations and
improving accuracy. Yet, RAG and related methods are only partial solutions, as
they may lack access to all necessary sources or key missing information. Even
everyday issues often challenge LLMs' abilities. Submitting longer prompts with
context and examples is one approach to address knowledge gaps, but designing
effective prompts is non-trivial and may not capture complex mental models of
domain experts. For tasks with missing critical information, LLMs are
insufficient, as are many existing systems poorly represented in available
documents. This paper explores how LLMs can make decision-making more
efficient, using a running example of evaluating whether to respond to a call
for proposals. We propose a technology based on optimized human-machine
dialogue and monotone Boolean and k-valued functions to discover a
computationally tractable personal expert mental model (EMM) of
decision-making. Our EMM algorithm for LLM prompt engineering has four steps:
(1) factor identification, (2) hierarchical structuring of factors, (3)
generating a generalized expert mental model specification, and (4) generating
a detailed generalized expert mental model from that specification.

</details>


### [8] [From Grounding to Skolemization: A Logic-Constrained Vector Symbolic Architecture for Complex Query Answering](https://arxiv.org/abs/2509.10837)
*Yuyin Lu,Hegang Chen,Yanghui Rao*

Main category: cs.AI

TL;DR: The paper introduces LVSA, a neuro-symbolic system, to address the trade-off in Complex Query Answering over Knowledge Graphs, achieving both logical soundness and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle the inherent trade-off between logical soundness and computational efficiency in answering complex queries on incomplete Knowledge Graphs.

Method: It establishes the Grounding-Skolemization dichotomy as a framework, and introduces LVSA, combining a differentiable Skolemization module, a neural negator, and a logical optimization protocol.

Result: LVSA ensures universality for all EFO$_1$ queries and demonstrates superior performance over Skolemization methods while reducing costs significantly compared to Grounding approaches.

Conclusion: LVSA harmonizes logic and computation, offering a robust solution for Complex Query Answering across Knowledge Graphs.

Abstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs),
typically formalized as reasoning with Existential First-Order predicate logic
with one free variable (EFO$_1$), faces a fundamental trade-off between logical
soundness and computational efficiency. This work establishes the
Grounding-Skolemization dichotomy for systematically analyzing CQA methods
through the lens of formal logic. While Grounding-based methods inherently
suffer from combinatorial explosion, most Skolemization-based methods neglect
to explicitly model Skolem functions and compromise logical consistency. To
address these limitations, we propose the Logic-constrained Vector Symbolic
Architecture (LVSA), a neuro-symbolic framework that unifies a differentiable
Skolemization module and a neural negator, as well as a logical
constraint-driven optimization protocol to harmonize geometric and logical
requirements. Theoretically, LVSA guarantees universality for all EFO$_1$
queries. Empirically, it outperforms state-of-the-art Skolemization-based
methods and reduces inference costs by orders of magnitude compared to
Grounding-based baselines.

</details>


### [9] [Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?](https://arxiv.org/abs/2509.10875)
*Jesse Gardner,Vladimir A. Baulin*

Main category: cs.AI

TL;DR: The paper critiques the agent-centric paradigm in AI, highlighting its ambiguities and biases, and proposes non-agentic and systemic frameworks as alternatives for robust general intelligence.


<details>
  <summary>Details</summary>
Motivation: The paper aims to challenge the prevailing agent-centric approach in AI research, which is deemed limiting due to its anthropocentric biases and conceptual ambiguities.

Method: The researchers conducted a systematic review of literature to contrast agentic, agential, and non-agentic systems, analyzing the challenges in defining properties such as autonomy and goal-directedness.

Result: Key findings indicate that the agentic framing of AI systems, especially LLMs, can obscure computational mechanisms and mislead conceptual understanding.

Conclusion: Shifting focus from agent-centric frameworks to system-level dynamics and material intelligence inspired by complex systems and biology is necessary for advancing scalable, non-anthropomorphic general intelligence.

Abstract: The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)
research, guiding development from foundational theories to contemporary
applications like Large Language Model (LLM)-based systems. This paper
critically re-evaluates the necessity and optimality of this agent-centric
paradigm. We argue that its persistent conceptual ambiguities and inherent
anthropocentric biases may represent a limiting framework. We distinguish
between agentic systems (AI inspired by agency, often semi-autonomous, e.g.,
LLM-based agents), agential systems (fully autonomous, self-producing systems,
currently only biological), and non-agentic systems (tools without the
impression of agency). Our analysis, based on a systematic review of relevant
literature, deconstructs the agent paradigm across various AI frameworks,
highlighting challenges in defining and measuring properties like autonomy and
goal-directedness. We argue that the 'agentic' framing of many AI systems,
while heuristically useful, can be misleading and may obscure the underlying
computational mechanisms, particularly in Large Language Models (LLMs). As an
alternative, we propose a shift in focus towards frameworks grounded in
system-level dynamics, world modeling, and material intelligence. We conclude
that investigating non-agentic and systemic frameworks, inspired by complex
systems, biology, and unconventional computing, is essential for advancing
towards robust, scalable, and potentially non-anthropomorphic forms of general
intelligence. This requires not only new architectures but also a fundamental
reconsideration of our understanding of intelligence itself, moving beyond the
agent metaphor.

</details>


### [10] [Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding](https://arxiv.org/abs/2509.10931)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: The paper introduces HaPLa, a method for universal jailbreaking of LLMs, achieving over 95% success on GPT models by exploiting architectural weaknesses.


<details>
  <summary>Details</summary>
Motivation: The rise of misuse in LLMs for harmful purposes highlights the urgent need to understand and defend against potential vulnerabilities.

Method: The HaPLa approach uses abductive framing for indirect harmful query inference and symbolic encoding to obfuscate harmful content from LLMs.

Result: HaPLa demonstrates a 95% attack success rate on GPT models and 70% success across other LLMs using black-box attack strategies.

Conclusion: Defensively tuning LLMs remains challenging as making them safer often compromises their utility in benign tasks.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but their potential misuse for harmful purposes remains a
significant concern. To strengthen defenses against such vulnerabilities, it is
essential to investigate universal jailbreak attacks that exploit intrinsic
weaknesses in the architecture and learning paradigms of LLMs. In response, we
propose \textbf{H}armful \textbf{P}rompt \textbf{La}undering (HaPLa), a novel
and broadly applicable jailbreaking technique that requires only black-box
access to target models. HaPLa incorporates two primary strategies: 1)
\textit{abductive framing}, which instructs LLMs to infer plausible
intermediate steps toward harmful activities, rather than directly responding
to explicit harmful queries; and 2) \textit{symbolic encoding}, a lightweight
and flexible approach designed to obfuscate harmful content, given that current
LLMs remain sensitive primarily to explicit harmful keywords. Experimental
results show that HaPLa achieves over 95% attack success rate on GPT-series
models and 70% across all targets. Further analysis with diverse symbolic
encoding rules also reveals a fundamental challenge: it remains difficult to
safely tune LLMs without significantly diminishing their helpfulness in
responding to benign queries.

</details>


### [11] [Public Data Assisted Differentially Private In-Context Learning](https://arxiv.org/abs/2509.10932)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: The paper introduces a private in-context learning (ICL) algorithm for Large Language Models (LLMs) that combines differential privacy (DP) with task-related public data to improve utility while maintaining privacy.


<details>
  <summary>Details</summary>
Motivation: The study addresses the risk of private data leakage during in-context learning (ICL) with Large Language Models, particularly under malicious attacks, and the utility loss caused by implementing differential privacy.

Method: A new private in-context learning algorithm that integrates public task-related data alongside differential privacy (DP), aiming to achieve a trade-off between privacy protection and model utility.

Result: The approach successfully enhances the utility of private ICL and exhibits resilience against membership inference attacks, showing improved empirical privacy protection.

Conclusion: The proposed method demonstrates a practical balance between privacy and utility in in-context learning by leveraging public data under a differential privacy framework.

Abstract: In-context learning (ICL) in Large Language Models (LLMs) has shown
remarkable performance across various tasks without requiring fine-tuning.
However, recent studies have highlighted the risk of private data leakage
through the prompt in ICL, especially when LLMs are exposed to malicious
attacks. While differential privacy (DP) provides strong privacy guarantees, it
often significantly reduces the utility of in-context learning (ICL). To
address this challenge, we incorporate task-related public data into the ICL
framework while maintaining the DP guarantee. Based on this approach, we
propose a private in-context learning algorithm that effectively balances
privacy protection and model utility. Through experiments, we demonstrate that
our approach significantly improves the utility of private ICL with the
assistance of public data. Additionally, we show that our method is robust
against membership inference attacks, demonstrating empirical privacy
protection.

</details>


### [12] [Enhancing Computational Cognitive Architectures with LLMs: A Case Study](https://arxiv.org/abs/2509.10972)
*Ron Sun*

Main category: cs.AI

TL;DR: The paper explores integrating Large Language Models (LLMs) into Clarion, a cognitive architecture, to enhance computational capabilities while maintaining psychological realism.


<details>
  <summary>Details</summary>
Motivation: Traditional cognitive architectures, while striving for psychological realism, are limited in computational capabilities. With the rise of more computationally advanced LLMs, combining these with cognitive architectures could help balance real-world complexity with psychological plausibility.

Method: The authors propose integrating LLMs with the Clarion cognitive architecture by leveraging Clarion's implicit-explicit dichotomy to facilitate a seamless synergy.

Result: The integration combines the computational power of LLMs with the psychological strengths of the Clarion cognitive architecture, creating a framework that is both computationally robust and psychologically informed.

Conclusion: Incorporating LLMs into cognitive architectures like Clarion provides a powerful means to address both computational demands and psychological realism, demonstrating potential for further exploration in this synergistic approach.

Abstract: Computational cognitive architectures are broadly scoped models of the human
mind that combine different psychological functionalities (as well as often
different computational methods for these different functionalities) into one
unified framework. They structure them in a psychologically plausible and
validated way. However, such models thus far have only limited computational
capabilities, mostly limited by the computational tools and techniques that
were adopted. More recently, LLMs have proved to be more capable
computationally than any other tools. Thus, in order to deal with both
real-world complexity and psychological realism at the same time, incorporating
LLMs into cognitive architectures naturally becomes an important task. In the
present article, a synergistic combination of the Clarion cognitive
architecture and LLMs is discussed as a case study. The implicit-explicit
dichotomy that is fundamental to Clarion is leveraged for a seamless
integration of Clarion and LLMs. As a result, computational power of LLMs is
combined with psychological nicety of Clarion.

</details>


### [13] [Rethinking Human Preference Evaluation of LLM Rationales](https://arxiv.org/abs/2509.11026)
*Ziang Li,Manasi Ganti,Zixian Ma,Helena Vasconcelos,Qijia He,Ranjay Krishna*

Main category: cs.AI

TL;DR: The paper investigates how to improve the evaluation of natural language rationales generated by large language models (LLMs) by examining fine-grained attributes rather than relying solely on binary preference judgments.


<details>
  <summary>Details</summary>
Motivation: Evaluating natural language rationales from LLMs is challenging because existing binary judgments are opaque and coarse-grained, offering little detailed insight into rationale quality.

Method: The authors identify key attributes defining good rationales from prior literature, assess these attributes using automatic metrics, LLM judgments, and human annotations, and analyze datasets using SHAP to determine which attributes influence human preferences. They also re-evaluate rationales using attribute-specific ELO scores.

Result: The study demonstrates that fine-grained evaluations of rationale attributes provide deeper insights into rationale quality and better explain human preference outcomes.

Conclusion: Fine-grained attribute-based evaluation is a more nuanced and effective approach to assessing rationale quality, advancing interpretability and reliability in LLM-generated explanations.

Abstract: Large language models (LLMs) often generate natural language rationales --
free-form explanations that help improve performance on complex reasoning tasks
and enhance interpretability for human users. However, evaluating these
rationales remains challenging. While recent work has relied on binary
preference judgments from humans or LLM judges, such evaluations are often
opaque and coarse-grained, offering limited insight into what makes one
rationale better than another. In this work, we rethink preference evaluation
for LLM-generated rationales by asking: (1) What attributes define good
rationales? (2) Can human preferences be explained by these attributes? (3) Can
attribute-based evaluation overcome the limitations of binary comparisons? We
identify a set of key rationale attributes from prior literature and assess
them using automatic metrics, LLM judgments, and human annotations. We then
analyze two standard human preference datasets MT Bench and Chatbot Arena using
SHAP to identify which attributes best explain human preference outcomes.
Finally, we re-evaluate model-generated rationales using attribute-specific ELO
scores, revealing more nuanced model comparisons and insights. Our findings
suggest that fine-grained attribute evaluations can better characterize
rationale quality and guide future research toward more interpretable and
reliable evaluation practices.

</details>


### [14] [Free-MAD: Consensus-Free Multi-Agent Debate](https://arxiv.org/abs/2509.11035)
*Yu Cui,Hang Fu,Haibin Zhang,Licheng Wang,Cong Zuo*

Main category: cs.AI

TL;DR: The paper introduces Free-MAD, a novel multi-agent debate framework improving reasoning in large language models by avoiding consensus and introducing score-based evaluations and anti-conformity mechanisms.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent debate methods suffer from scalability issues, error propagation due to conformity, and randomness/unfairness during decision-making, prompting a need for better frameworks.

Method: Free-MAD avoids consensus and introduces a score-based mechanism to evaluate the whole debate trajectory. Anti-conformity is introduced to counteract influence from the majority, and only single-round debates are utilized.

Result: Free-MAD improves reasoning performance across eight benchmarks, reduces token costs through single-round debates, and shows enhanced robustness against real-world attack scenarios compared to existing methods.

Conclusion: Free-MAD provides a scalable and more robust alternative to existing multi-agent debate frameworks, addressing key limitations and improving overall performance.

Abstract: Multi-agent debate (MAD) is an emerging approach to improving the reasoning
capabilities of large language models (LLMs). Existing MAD methods rely on
multiple rounds of interaction among agents to reach consensus, and the final
output is selected by majority voting in the last round. However, this
consensus-based design faces several limitations. First, multiple rounds of
communication increases token overhead and limits scalability. Second, due to
the inherent conformity of LLMs, agents that initially produce correct
responses may be influenced by incorrect ones during the debate process,
causing error propagation. Third, majority voting introduces randomness and
unfairness in the decision-making phase, and can degrade the reasoning
performance.
  To address these issues, we propose \textsc{Free-MAD}, a novel MAD framework
that eliminates the need for consensus among agents. \textsc{Free-MAD}
introduces a novel score-based decision mechanism that evaluates the entire
debate trajectory rather than relying on the last round only. This mechanism
tracks how each agent's reasoning evolves, enabling more accurate and fair
outcomes. In addition, \textsc{Free-MAD} reconstructs the debate phase by
introducing anti-conformity, a mechanism that enables agents to mitigate
excessive influence from the majority. Experiments on eight benchmark datasets
demonstrate that \textsc{Free-MAD} significantly improves reasoning performance
while requiring only a single-round debate and thus reducing token costs. We
also show that compared to existing MAD approaches, \textsc{Free-MAD} exhibits
improved robustness in real-world attack scenarios.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [15] [ReCross: Efficient Embedding Reduction Scheme for In-Memory Computing using ReRAM-Based Crossbar](https://arxiv.org/abs/2509.10627)
*Yu-Hong Lai,Chieh-Lin Tsai,Wen Sheng Lim,Han-Wen Hu,Tei-Wei Kuo,Yuan-Hao Chang*

Main category: cs.AR

TL;DR: Deep learning recommendation models (DLRMs) face memory issues due to large embedding layers. ReCross improves memory operations using ReRAM-based computing, achieving faster execution and better energy efficiency.


<details>
  <summary>Details</summary>
Motivation: DLRMs are hindered by memory bottlenecks caused by high cost and irregularity of access in large embedding layers, leading to inefficiency in terms of energy and speed.

Method: ReCross optimizes embedding access patterns through strategies like intelligent grouping, replication of frequently accessed embeddings, and dynamic processing using a new ADC circuit.

Result: ReCross reduces execution time by 3.97x and improves energy efficiency by 6.1x compared to state-of-the-art IMC solutions.

Conclusion: ReCross successfully overcomes the inefficiencies in DLRM's memory system, providing a much faster and energy-efficient solution leveraging ReRAM-based in-memory computing.

Abstract: Deep learning-based recommendation models (DLRMs) are widely deployed in
commercial applications to enhance user experience. However, the large and
sparse embedding layers in these models impose substantial memory bandwidth
bottlenecks due to high memory access costs and irregular access patterns,
leading to increased inference time and energy consumption. While resistive
random access memory (ReRAM) based crossbars offer a fast and energy-efficient
solution through in-memory embedding reduction operations, naively mapping
embeddings onto crossbar arrays leads to poor crossbar utilization and thus
degrades performance. We present ReCross, an efficient ReRAM-based in-memory
computing (IMC) scheme designed to minimize execution time and enhance energy
efficiency in DLRM embedding reduction. ReCross co-optimizes embedding access
patterns and ReRAM crossbar characteristics by intelligently grouping and
mapping co-occurring embeddings, replicating frequently accessed embeddings
across crossbars, and dynamically selecting in-memory processing operations
using a newly designed dynamic switch ADC circuit that considers runtime energy
trade-offs. Experimental results demonstrate that ReCross achieves a 3.97x
reduction in execution time and a 6.1x improvement in energy efficiency
compared to state-of-the-art IMC approaches.

</details>


### [16] [DOSA: Differentiable Model-Based One-Loop Search for DNN Accelerators](https://arxiv.org/abs/2509.10702)
*Charles Hong,Qijing Huang,Grace Dinh,Mahesh Subedar,Yakun Sophia Shao*

Main category: cs.AR

TL;DR: Introduces DOSA for combined optimization of hardware parameters and algorithm mappings using differentiable models and gradient-based optimization, achieving notable improvements.


<details>
  <summary>Details</summary>
Motivation: Optimize the interplay of hardware parameters and algorithm-to-hardware mappings due to complexities in their simultaneous exploration.

Method: Proposes DOSA, blending differentiable performance models and gradient descent techniques for unified exploration and optimization.

Result: DOSA outperforms random search and Bayesian optimization significantly in enhancing energy-delay product. Demonstrates versatility by optimizing buffer sizes on a real-world DNN accelerator.

Conclusion: DOSA offers a powerful, flexible solution for integrated hardware-design and mapping optimization, addressing challenges of prior independent approaches.

Abstract: In the hardware design space exploration process, it is critical to optimize
both hardware parameters and algorithm-to-hardware mappings. Previous work has
largely approached this simultaneous optimization problem by separately
exploring the hardware design space and the mapspace - both individually large
and highly nonconvex spaces - independently. The resulting combinatorial
explosion has created significant difficulties for optimizers.
  In this paper, we introduce DOSA, which consists of differentiable
performance models and a gradient descent-based optimization technique to
simultaneously explore both spaces and identify high-performing design points.
Experimental results demonstrate that DOSA outperforms random search and
Bayesian optimization by 2.80x and 12.59x, respectively, in improving DNN model
energy-delay product, given a similar number of samples. We also demonstrate
the modularity and flexibility of DOSA by augmenting our analytical model with
a learned model, allowing us to optimize buffer sizes and mappings of a real
DNN accelerator and attain a 1.82x improvement in energy-delay product.

</details>


### [17] [Design and Analysis of Approximate Hardware Accelerators for VVC Intra Angular Prediction](https://arxiv.org/abs/2509.10751)
*Lucas M. Leipnitz de Fraga,Cláudio Machado Diniz*

Main category: cs.AR

TL;DR: This paper introduces efficient multiplierless constant multiplication (MCM) architectures for angular intra prediction in VVC to reduce complexity and hardware resource usage with minimal coding efficiency loss.


<details>
  <summary>Details</summary>
Motivation: VVC improves video compression but increases computational complexity, particularly in angular intra prediction. Hardware resource limitations necessitate optimization strategies.

Method: Proposes averaging subsets of interpolation coefficients to reduce complexity in MCM architectures. Introduced six architectures to analyze trade-offs between coefficient reduction and coding efficiency.

Result: Experimental results show only minor coding efficiency losses (average BD-Rate increase of 2.6%), but significant hardware resource savings (up to 44% reduced circuit area and lower energy consumption).

Conclusion: The proposed architectures effectively balance coding efficiency and hardware resource optimization, providing substantial benefits for VVC implementations in hardware accelerators.

Abstract: The Versatile Video Coding (VVC) standard significantly improves compression
efficiency over its predecessor, HEVC, but at the cost of substantially higher
computational complexity, particularly in intra-frame prediction. This stage
employs various directional modes, each requiring multiple multiplications
between reference samples and constant coefficients. To optimize these
operations at hardware accelerators, multiplierless constant multiplication
(MCM) blocks offer a promising solution. However, VVC's interpolation filters
have more than fifty distinct coefficients, making MCM implementations
resource-intensive. This work proposes an approximation method to reduce the
number of interpolation coefficients by averaging fixed subsets of them,
therefore decreasing MCM block size and potentially lowering circuit area and
power consumption. Six different MCM block architectures for angular intra
prediction are introduced, in which five use the approximation method
introduced in this work, and evaluate the trade-off between coefficient
reduction and coding efficiency compared with a conventional multiplier
architecture. Experimental results in ten videos demonstrate that only two MCM
implementations exceed a 4% BD-Rate increase and 2.6% on average in the worst
case, while two of the MCM implementations have circuit area reduction of 20%
and 44%. For three of the architectures, parallel sample prediction modules
were synthesized, showing a reduction of 30% gate area compared to single
sample processing units, and a reduction in energy consumption for two of the
implementations.

</details>


### [18] [always_comm: An FPGA-based Hardware Accelerator for Audio/Video Compression and Transmission](https://arxiv.org/abs/2509.11503)
*Rishab Parthasarathy,Akshay Attaluri,Gilford Ting*

Main category: cs.AR

TL;DR: The paper discusses a hardware-based video conferencing system utilizing an FPGA, supporting real-time video streaming at 30 FPS and end-to-end verification.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient, real-time hardware video conferencing system implemented on FPGA with high throughput and low latency.

Method: Designed and implemented a stack using M-JPEG codec for compression, a UDP networking stack, and an FPGA for real-time video streaming; evaluated using functional verification and hardware synthesis.

Result: Demonstrated video streaming at 30 FPS, performed end-to-end latency and throughput experiments with successful results on the FPGA.

Conclusion: The proposed architecture is feasible for real-time hardware-based video conferencing, showing good performance in terms of latency and throughput.

Abstract: We present a design for an extensible video conferencing stack implemented
entirely in hardware on a Nexys4 DDR FPGA, which uses the M-JPEG codec to
compress video and a UDP networking stack to communicate between the FPGA and
the receiving computer. This networking stack accepts real-time updates from
both the video codec and the audio controller, which means that video will be
able to be streamed at 30 FPS from the FPGA to a computer. On the computer
side, a Python script reads the Ethernet packets and decodes the packets into
the video and the audio for real time playback. We evaluate this architecture
using both functional, simulation-driven verification in Cocotb and by
synthesizing SystemVerilog RTL code using Vivado for deployment on our Nexys4
DDR FPGA, where we evaluate both end-to-end latency and throughput of video
transmission.

</details>


### [19] [SuperUROP: An FPGA-Based Spatial Accelerator for Sparse Matrix Operations](https://arxiv.org/abs/2509.11529)
*Rishab Parthasarathy*

Main category: cs.AR

TL;DR: The paper introduces an FPGA implementation of the Azul accelerator to efficiently solve sparse linear systems with improved memory utilization and arithmetic intensity.


<details>
  <summary>Details</summary>
Motivation: Solving sparse linear systems is crucial due to their vast applications, but current iterative solvers are inefficient on modern hardware due to poor data reuse and parallelism.

Method: The authors implemented Azul on FPGA by integrating RISC-V CPU cores with a memory hierarchy, enabling a task-based programming model over an NoC, and tested performance equivalency via distributed test cases.

Result: The FPGA implementation was successfully verified to match the performance of Azul's architectural simulation.

Conclusion: The implemented Azul architecture on FPGA demonstrates efficient and functional performance for solving sparse systems on hardware accelerators.

Abstract: Solving sparse systems of linear equations is a fundamental problem in the
field of numerical methods, with applications spanning from circuit design to
urban planning. These problems can have millions of constraints, such as when
laying out transistors on a circuit, or trying to optimize traffic light
timings, making fast sparse solvers extremely important. However, existing
state-of-the-art software-level solutions for solving sparse linear systems,
termed iterative solvers, are extremely inefficient on current hardware. This
inefficiency can be attributed to two key reasons: (1) poor short-term data
reuse, which causes frequent, irregular memory accesses, and (2) complex data
dependencies, which limit parallelism. Hence, in this paper, we present an FPGA
implementation of the existing Azul accelerator, an SRAM-only hardware
accelerator that achieves both high memory bandwidth utilization and arithmetic
intensity. Azul features a grid of tiles, each of which is composed of a
processing element (PE) and a small independent SRAM memory, which are all
connected over a network on chip (NoC). We implement Azul on FPGA using simple
RISC-V CPU cores connected to a memory hierarchy of different FPGA memory
modules. We utilize custom RISC-V ISA augmentations to implement a task-based
programming model for the various PEs, allowing communication over the NoC.
Finally, we design simple distributed test cases so that we can functionally
verify the FPGA implementation, verifying equivalent performance to an
architectural simulation of the Azul framework.

</details>


### [20] [LEGO: Spatial Accelerator Generation and Optimization for Tensor Applications](https://arxiv.org/abs/2509.12053)
*Yujun Lin,Zhekai Zhang,Song Han*

Main category: cs.AR

TL;DR: LEGO is a framework that automates RTL generation for tensor applications, delivering 3.2x speedup and 2.4x energy efficiency compared to existing solutions.


<details>
  <summary>Details</summary>
Motivation: Current tensor applications demand flexible accelerator architectures for handling multimodal data, which traditional RTL generation methods struggle to address.

Method: LEGO employs affine-transformation-based representation to synthesize interconnections, memory systems, and spatial dataflows, followed by lower-level optimizations using linear programming.

Result: LEGO achieves significant performance improvements, with 3.2x faster execution and 2.4x better energy efficiency than Gemmini.

Conclusion: LEGO simplifies spatial architecture design for tensor applications, enhancing both productivity and design flexibility while delivering superior performance and energy efficiency.

Abstract: Modern tensor applications, especially foundation models and generative AI
applications require multiple input modalities (both vision and language),
which increases the demand for flexible accelerator architecture. Existing
frameworks suffer from the trade-off between design flexibility and
productivity of RTL generation: either limited to very few hand-written
templates or cannot automatically generate the RTL. To address this challenge,
we propose the LEGO framework, which targets tensor applications and
automatically generates spatial architecture design and outputs synthesizable
RTL code without handwritten RTL design templates. Leveraging the
affine-transformation-based architecture representation, LEGO front end finds
interconnections between function units, synthesizes the memory system, and
fuses different spatial dataflow designs based on data reuse analysis. LEGO
back end then translates the hardware in a primitive-level graph to perform
lower-level optimizations, and applies a set of linear-programming algorithms
to optimally insert pipeline registers and reduce the overhead of unused logic
when switching spatial dataflows. Our evaluation demonstrates that LEGO can
achieve 3.2x speedup and 2.4x energy efficiency compared to previous work
Gemmini, and can generate one architecture for diverse modern foundation models
in generative AI applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [21] [Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment](https://arxiv.org/abs/2509.10546)
*Gang Cheng,Haibo Jin,Wenbin Zhang,Haohan Wang,Jun Zhuang*

Main category: cs.CL

TL;DR: This paper introduces Risk-Concealment Attacks (RCA) to expose regulatory compliance vulnerabilities in financial LLMs, achieving a high attack success rate.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of focus on regulatory compliance risks in financial applications of LLMs, which predominantly rely on unsafe or harmful intent studies.

Method: The authors developed RCA, a multi-turn attack framework that exploits financial LLM vulnerabilities and created FIN-Bench, a benchmark for systematic evaluation of financial LLM safety.

Result: Experiments reveal RCA achieves a high attack success rate (93.18% on average), successfully bypassing nine LLMs, including 98.28% success on GPT-4.1.

Conclusion: The study highlights gaps in current LLM alignment methods and stresses the importance of enhanced moderation in financial domains.

Abstract: Large Language Models (LLMs) are increasingly integrated into financial
applications, yet existing red-teaming research primarily targets harmful
content, largely neglecting regulatory risks. In this work, we aim to
investigate the vulnerability of financial LLMs through red-teaming approaches.
We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that
iteratively conceals regulatory risks to provoke seemingly compliant yet
regulatory-violating responses from LLMs. To enable systematic evaluation, we
construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in
financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA
effectively bypasses nine mainstream LLMs, achieving an average attack success
rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.
These findings reveal a critical gap in current alignment techniques and
underscore the urgent need for stronger moderation mechanisms in financial
domains. We hope this work offers practical insights for advancing robust and
domain-aware LLM alignment.

</details>


### [22] [No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes](https://arxiv.org/abs/2509.10625)
*Iván Vicente Moreno Cencerrado,Arnau Padrés Masdemont,Anton Gonzalvez Hawthorne,David Demitri Africa,Lorenzo Pacchiardi*

Main category: cs.CL

TL;DR: The paper analyzes large language models (LLMs) to determine if they can predict their correctness in generating answers, using linear probes on activation data before token generation.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore whether LLMs inherently possess mechanisms to self-assess answer correctness before outputting a response, thereby contributing to understanding LLM internals.

Method: Researchers train linear probes on activation data taken immediately after a question is read but before any token generation, focusing on multiple open-source LLM families and analyzing performance across in-distribution and out-of-distribution datasets.

Result: Linear probes successfully predict answer correctness, saturate in intermediate layers, and indicate self-assessment emergence mid-computation. However, models struggle with mathematical reasoning and exhibit confidence correlation for 'I don't know' responses.

Conclusion: The findings advance the understanding of LLM mechanisms for self-assessment, revealing emergent behaviors and offering insights into their predictive confidence and limitations in reasoning.

Abstract: Do large language models (LLMs) anticipate when they will answer correctly?
To study this, we extract activations after a question is read but before any
tokens are generated, and train linear probes to predict whether the model's
forthcoming answer will be correct. Across three open-source model families
ranging from 7 to 70 billion parameters, projections on this "in-advance
correctness direction" trained on generic trivia questions predict success in
distribution and on diverse out-of-distribution knowledge datasets,
outperforming black-box baselines and verbalised predicted confidence.
Predictive power saturates in intermediate layers, suggesting that
self-assessment emerges mid-computation. Notably, generalisation falters on
questions requiring mathematical reasoning. Moreover, for models responding "I
don't know", doing so strongly correlates with the probe score, indicating that
the same direction also captures confidence. By complementing previous results
on truthfulness and other behaviours obtained with probes and sparse
auto-encoders, our work contributes essential findings to elucidate LLM
internals.

</details>


### [23] [Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation](https://arxiv.org/abs/2509.10644)
*Enora Rice,Katharina von der Wense,Alexis Palmer*

Main category: cs.CL

TL;DR: This paper highlights the potential of computational morphology in aiding language documentation but identifies a gap between research outputs and real-world usability. It advocates for User-Centered Design to address this issue, presenting a case study and user findings to propose new research directions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between computational morphology outputs and their actual usability in language documentation, ensuring tools truly meet practical needs.

Method: The authors conducted a user study with documentary linguists using GlossLM, a multilingual top-performing model for IGT generation, and evaluated its usability against linguistic documentary needs.

Result: The study revealed that, despite strong performance metrics, the system fell short in meeting essential user needs like usability within real-world language documentation contexts.

Conclusion: Integrating User-Centered Design can reshape computational morphology research, making tools more effective and unveiling vital research directions, like standardization and personalization for users.

Abstract: Computational morphology has the potential to support language documentation
through tasks like morphological segmentation and the generation of Interlinear
Glossed Text (IGT). However, our research outputs have seen limited use in
real-world language documentation settings. This position paper situates the
disconnect between computational morphology and language documentation within a
broader misalignment between research and practice in NLP and argues that the
field risks becoming decontextualized and ineffectual without systematic
integration of User-Centered Design (UCD). To demonstrate how principles from
UCD can reshape the research agenda, we present a case study of GlossLM, a
state-of-the-art multilingual IGT generation model. Through a small-scale user
study with three documentary linguists, we find that despite strong metric
based performance, the system fails to meet core usability needs in real
documentation contexts. These insights raise new research questions around
model constraints, label standardization, segmentation, and personalization. We
argue that centering users not only produces more effective tools, but surfaces
richer, more relevant research directions

</details>


### [24] [Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts](https://arxiv.org/abs/2509.10663)
*Zineddine Tighidet,Andrea Mogini,Hedi Ben-younes,Jiali Mei,Patrick Gallinari,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: This paper identifies 'entropy neurons' in autoregressive transformer models that suppress context copying when contextual and parametric information conflict, thereby affecting generation outcomes.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand why Large Language Models (LLMs) handle conflicting information inconsistently and explore the mechanism behind context suppression.

Method: The study investigates the role of entropy neurons by analyzing their influence on conflict resolution and generation processes in multiple LLMs, including methods such as controlled ablation.

Result: Entropy neurons are shown to suppress context copying behavior across multiple LLMs, and their removal significantly alters the model's generation process.

Conclusion: The findings deepen insights into how LLMs resolve contextual versus parametric conflicts, linking suppression of copying behavior to specific internal mechanisms.

Abstract: The behavior of Large Language Models (LLMs) when facing contextual
information that conflicts with their internal parametric knowledge is
inconsistent, with no generally accepted explanation for the expected outcome
distribution. Recent work has identified in autoregressive transformer models a
class of neurons -- called entropy neurons -- that produce a significant effect
on the model output entropy while having an overall moderate impact on the
ranking of the predicted tokens. In this paper, we investigate the preliminary
claim that these neurons are involved in inhibiting context copying behavior in
transformers by looking at their role in resolving conflicts between contextual
and parametric information. We show that entropy neurons are responsible for
suppressing context copying across a range of LLMs, and that ablating them
leads to a significant change in the generation process. These results enhance
our understanding of the internal dynamics of LLMs when handling conflicting
information.

</details>


### [25] [Pluralistic Alignment for Healthcare: A Role-Driven Framework](https://arxiv.org/abs/2509.10685)
*Jiayou Zhong,Anudeex Shetty,Chao Jia,Xuanrui Lin,Usman Naseem*

Main category: cs.CL

TL;DR: The paper proposes EthosAgents, a pluralistic alignment framework for large language models, to better respect diverse values in healthcare and other sensitive domains.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods fail to address healthcare-specific pluralistic needs shaped by personal, cultural, and situational factors.

Method: Developing EthosAgents, a lightweight, generalizable, pluralistic alignment approach that simulates diverse perspectives and values.

Result: Empirical evidence demonstrates that EthosAgents improves pluralistic alignment across diverse health-related scenarios for various models.

Conclusion: EthosAgents provides a path to improve diversity-respecting outputs in healthcare and high-stakes domains, highlighting the need for adaptable and normatively aware AI systems.

Abstract: As large language models are increasingly deployed in sensitive domains such
as healthcare, ensuring their outputs reflect the diverse values and
perspectives held across populations is critical. However, existing alignment
approaches, including pluralistic paradigms like Modular Pluralism, often fall
short in the health domain, where personal, cultural, and situational factors
shape pluralism. Motivated by the aforementioned healthcare challenges, we
propose a first lightweight, generalizable, pluralistic alignment approach,
EthosAgents, designed to simulate diverse perspectives and values. We
empirically show that it advances the pluralistic alignment for all three modes
across seven varying-sized open and closed models. Our findings reveal that
health-related pluralism demands adaptable and normatively aware approaches,
offering insights into how these models can better respect diversity in other
high-stakes domains.

</details>


### [26] [Struct-Bench: A Benchmark for Differentially Private Structured Text Generation](https://arxiv.org/abs/2509.10696)
*Shuaiqi Wang,Vikas Raunak,Arturs Backurs,Victor Reis,Pei Zhou,Sihao Chen,Longqi Yang,Zinan Lin,Sergey Yekhanin,Giulia Fanti*

Main category: cs.CL

TL;DR: The paper introduces Struct-Bench, a framework for evaluating differentially private (DP) synthetic datasets derived from structured datasets, especially those with natural language components.


<details>
  <summary>Details</summary>
Motivation: There is a need to evaluate the quality of differentially private synthetic data generated for structured datasets, particularly as existing techniques like FID are inadequate for capturing structural and correlational properties.

Method: The authors propose Struct-Bench, which uses Context-Free Grammar (CFG) to represent dataset structure, and includes 5 real-world and 2 synthetic datasets annotated with CFGs. It also provides metrics, a leaderboard, and aims to standardize evaluations.

Result: The paper demonstrates the difficulty of current DP synthetic generation methods on the benchmark datasets, showcasing Struct-Bench's utility. A case study also illustrates how Struct-Bench improves Private Evolution on structured data.

Conclusion: Struct-Bench offers a valuable, standardized platform for advancing and evaluating DP synthetic data generation methods, especially for structured datasets with natural language. Researchers now have tools to measure and improve synthesis quality.

Abstract: Differentially private (DP) synthetic data generation is a promising
technique for utilizing private datasets that otherwise cannot be exposed for
model training or other analytics. While much research literature has focused
on generating private unstructured text and image data, in enterprise settings,
structured data (e.g., tabular) is more common, often including natural
language fields or components. Existing synthetic data evaluation techniques
(e.g., FID) struggle to capture the structural properties and correlations of
such datasets. In this work, we propose Struct-Bench, a framework and benchmark
for evaluating synthetic datasets derived from structured datasets that contain
natural language data. The Struct-Bench framework requires users to provide a
representation of their dataset structure as a Context-Free Grammar (CFG). Our
benchmark comprises 5 real-world and 2 synthetically generated datasets, each
annotated with CFGs. We show that these datasets demonstrably present a great
challenge even for state-of-the-art DP synthetic data generation methods.
Struct-Bench also includes reference implementations of different metrics and a
leaderboard, thereby providing researchers a standardized evaluation platform
to benchmark and investigate privacy-preserving synthetic data generation
methods. Further, we also present a case study showing how to use Struct-Bench
to improve the synthetic data quality of Private Evolution (PE) on structured
data. The benchmark and the leaderboard have been publicly made available at
https://struct-bench.github.io.

</details>


### [27] [A Survey on Retrieval And Structuring Augmented Generation with Large Language Models](https://arxiv.org/abs/2509.10697)
*Pengcheng Jiang,Siru Ouyang,Yizhu Jiao,Ming Zhong,Runchu Tian,Jiawei Han*

Main category: cs.CL

TL;DR: This paper surveys RAS (Retrieval and Structuring) methods that enhance LLMs by integrating efficient information retrieval and structured knowledge, addressing challenges like hallucination and outdated information.


<details>
  <summary>Details</summary>
Motivation: To address critical challenges faced by LLMs, such as hallucination generation, outdated information, and limited domain expertise, thereby improving their deployment in real-world applications.

Method: The survey examines retrieval methods (sparse, dense, hybrid), text structuring techniques (taxonomy, classification, extraction), and mechanisms for integrating structured information through prompts, reasoning, and embeddings.

Result: The paper identifies challenges in retrieval efficiency, structure quality, and knowledge integration, and highlights future opportunities in areas like multimodal retrieval, cross-lingual structures, and interactive systems.

Conclusion: This work provides useful insights into RAS techniques and their potential to enhance LLMs, guiding future research and applications.

Abstract: Large Language Models (LLMs) have revolutionized natural language processing
with their remarkable capabilities in text generation and reasoning. However,
these models face critical challenges when deployed in real-world applications,
including hallucination generation, outdated knowledge, and limited domain
expertise. Retrieval And Structuring (RAS) Augmented Generation addresses these
limitations by integrating dynamic information retrieval with structured
knowledge representations. This survey (1) examines retrieval mechanisms
including sparse, dense, and hybrid approaches for accessing external
knowledge; (2) explore text structuring techniques such as taxonomy
construction, hierarchical classification, and information extraction that
transform unstructured text into organized representations; and (3) investigate
how these structured representations integrate with LLMs through prompt-based
methods, reasoning frameworks, and knowledge embedding techniques. It also
identifies technical challenges in retrieval efficiency, structure quality, and
knowledge integration, while highlighting research opportunities in multimodal
retrieval, cross-lingual structures, and interactive systems. This
comprehensive overview provides researchers and practitioners with insights
into RAS methods, applications, and future directions.

</details>


### [28] [SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation](https://arxiv.org/abs/2509.10708)
*Iman Barati,Mostafa Amiri,Heshaam Faili*

Main category: cs.CL

TL;DR: This paper introduces SearchInstruct, a method for creating high-quality instruction datasets for supervised fine-tuning (SFT) of language models, targeting challenges in domain-specific data creation.


<details>
  <summary>Details</summary>
Motivation: The work addresses the difficulty of producing tailored datasets for specific domains in supervised fine-tuning, especially due to challenges like domain constraints and data scarcity.

Method: The authors propose to start with limited domain-specific human-generated questions, expand them using a large language model, and dynamically retrieve relevant resources to generate accurate answers.

Result: SearchInstruct improves the diversity and quality of SFT datasets, leading to measurable gains in language model performance in specialized domains.

Conclusion: The method not only enhances training datasets but also supports model tasks like editing, and its full implementation details and resources are made publicly available for community use.

Abstract: Supervised Fine-Tuning (SFT) is essential for training large language models
(LLMs), significantly enhancing critical capabilities such as instruction
following and in-context learning. Nevertheless, creating suitable training
datasets tailored for specific domains remains challenging due to unique domain
constraints and data scarcity. In this paper, we propose SearchInstruct, an
innovative method explicitly designed to construct high quality instruction
datasets for SFT. Our approach begins with a limited set of domain specific,
human generated questions, which are systematically expanded using a large
language model. Subsequently, domain relevant resources are dynamically
retrieved to generate accurate and contextually appropriate answers for each
augmented question. Experimental evaluation demonstrates that SearchInstruct
enhances both the diversity and quality of SFT datasets, leading to measurable
improvements in LLM performance within specialized domains. Additionally, we
show that beyond dataset generation, the proposed method can also effectively
facilitate tasks such as model editing, enabling efficient updates to existing
models. To facilitate reproducibility and community adoption, we provide full
implementation details, the complete set of generated instruction response
pairs, and the source code in a publicly accessible Git repository:
[https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)

</details>


### [29] [PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models](https://arxiv.org/abs/2509.10737)
*Zaur Gouliev,Jennifer Waters,Chengqian Wang*

Main category: cs.CL

TL;DR: The paper evaluates multilingual transformer models' effectiveness in detecting disinformation across 25 languages using a novel corpus. Models like RemBERT excel in low-resource languages, whereas mBERT and XLM struggle.


<details>
  <summary>Details</summary>
Motivation: The spread of disinformation across languages highlights the need for multilingual AI models, as existing solutions mainly benchmark results in English.

Method: Researchers introduced the PolyTruth Disinfo Corpus containing statement pairs in 25 languages to compare five multilingual transformer models on a fake-vs-true classification task.

Result: RemBERT outperformed others in overall accuracy and low-resource languages, while mBERT and XLM showed limitations in handling scarce training data.

Conclusion: The study emphasizes the promise and limitations of current multilingual transformer models for combating disinformation and calls for further exploration using the publicly available dataset.

Abstract: Disinformation spreads rapidly across linguistic boundaries, yet most AI
models are still benchmarked only on English. We address this gap with a
systematic comparison of five multilingual transformer models: mBERT, XLM,
XLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning
classification task. While transformer-based language models have demonstrated
notable success in detecting disinformation in English, their effectiveness in
multilingual contexts still remains up for debate. To facilitate evaluation, we
introduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs
(false claim vs. factual correction) spanning over twenty five languages that
collectively cover five language families and a broad topical range from
politics, health, climate, finance, and conspiracy, half of which are
fact-checked disinformation claims verified by an augmented MindBugs Discovery
dataset. Our experiments revealed performance variations. Models such as
RemBERT achieved better overall accuracy, particularly excelling in
low-resource languages, whereas models like mBERT and XLM exhibit considerable
limitations when training data is scarce. We provide a discussion of these
performance patterns and implications for real-world deployment. The dataset is
publicly available on our GitHub repository to encourage further
experimentation and advancement. Our findings illuminate both the potential and
the current limitations of AI systems for multilingual disinformation
detection.

</details>


### [30] [Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs](https://arxiv.org/abs/2509.10739)
*Mobina Pournemat,Keivan Rezaei,Gaurang Sriramanan,Arman Zarei,Jiaxiang Fu,Yang Wang,Hamid Eghbalzadeh,Soheil Feizi*

Main category: cs.CL

TL;DR: This paper investigates the probabilistic reasoning abilities of large language models (LLMs) and finds both strengths and notable limitations.


<details>
  <summary>Details</summary>
Motivation: To understand and evaluate the probabilistic reasoning capabilities of LLMs over explicit discrete probability distributions.

Method: The authors designed three tasks—mode identification, maximum likelihood estimation, and sample generation—to evaluate LLMs' reasoning using queries related to joint distributions and conditionals.

Result: Large LLMs demonstrated stronger reasoning abilities and unexpected sample generation skills compared to smaller models, but displayed significant sensitivity to notation changes and performance degradation with longer context lengths.

Conclusion: The work highlights both strengths and limitations in LLMs' probabilistic reasoning abilities, providing insights for future model improvements.

Abstract: Despite widespread success in language understanding and generation, large
language models (LLMs) exhibit unclear and often inconsistent behavior when
faced with tasks that require probabilistic reasoning. In this work, we present
the first comprehensive study of the reasoning capabilities of LLMs over
explicit discrete probability distributions. Given observations from a
probability distribution, we evaluate models on three carefully designed tasks,
mode identification, maximum likelihood estimation, and sample generation, by
prompting them to provide responses to queries about either the joint
distribution or its conditionals. These tasks thus probe a range of
probabilistic skills, including frequency analysis, marginalization, and
generative behavior. Through comprehensive empirical evaluations, we
demonstrate that there exists a clear performance gap between smaller and
larger models, with the latter demonstrating stronger inference and surprising
capabilities in sample generation. Furthermore, our investigations reveal
notable limitations, including sensitivity to variations in the notation
utilized to represent probabilistic outcomes and performance degradation of
over 60% as context length increases. Together, our results provide a detailed
understanding of the probabilistic reasoning abilities of LLMs and identify key
directions for future improvement.

</details>


### [31] [Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models](https://arxiv.org/abs/2509.10744)
*Ozan Gokdemir,Neil Getty,Robert Underwood,Sandeep Madireddy,Franck Cappello,Arvind Ramanathan,Ian T. Foster,Rick L. Stevens*

Main category: cs.CL

TL;DR: The paper introduces a framework to create large-scale MCQA benchmarks directly from scientific papers, showcasing its efficiency in radiation and cancer biology.


<details>
  <summary>Details</summary>
Motivation: Current evaluation benchmarks struggle to keep up with the rapid growth of scientific knowledge. The paper aims to address this gap by ensuring that language models are tested on up-to-date, diverse literature.

Method: A modular pipeline for MCQA creation was developed, incorporating processes like PDF parsing, semantic chunking, automated question generation, and model evaluation. The framework was applied to scientific papers in radiation and cancer biology.

Result: The framework generated over 16,000 MCQs from 22,000 scientific papers. Small-scale language models were evaluated, and reasoning-trace retrieval methods improved their performance significantly, surpassing GPT-4 in specific benchmark tests.

Conclusion: Reasoning-trace retrieval is effective for enhancing the capabilities of smaller models on scientific benchmarks, showcasing a scalable solution for creating dynamic, domain-specific evaluation frameworks.

Abstract: As scientific knowledge grows at an unprecedented pace, evaluation benchmarks
must evolve to reflect new discoveries and ensure language models are tested on
current, diverse literature. We propose a scalable, modular framework for
generating multiple-choice question-answering (MCQA) benchmarks directly from
large corpora of scientific papers. Our pipeline automates every stage of MCQA
creation, including PDF parsing, semantic chunking, question generation, and
model evaluation. As a case study, we generate more than 16,000 MCQs from
22,000 open-access articles in radiation and cancer biology. We then evaluate a
suite of small language models (1.1B-14B parameters) on these questions,
comparing baseline accuracy with retrieval-augmented generation (RAG) from
paper-derived semantic chunks and from reasoning traces distilled from GPT-4.1.
We find that reasoning-trace retrieval consistently improves performance on
both synthetic and expert-annotated benchmarks, enabling several small models
to surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.

</details>


### [32] [RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems](https://arxiv.org/abs/2509.10746)
*Adarsh Srinivasan,Jacob Dineen,Muhammad Umar Afzal,Muhammad Uzair Sarfraz,Irbaz B. Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: This paper proposes RECAP, a framework for enhancing emotional reasoning in healthcare language models without retraining.


<details>
  <summary>Details</summary>
Motivation: Address the frequent lack of emotional understanding in healthcare language models, which impacts patient trust and adherence.

Method: Introduce RECAP, a framework that adds structured emotional reasoning using appraisal-theoretic stages and Likert signals during inference.

Result: RECAP improves emotional reasoning by 22-28% in smaller models and 10-13% in larger models across multiple benchmarks, validated by clinicians.

Conclusion: Theory-grounded, modular frameworks like RECAP can enhance medical AI's emotional intelligence while maintaining accountability for deployment.

Abstract: Large language models in healthcare often miss critical emotional cues,
delivering medically sound but emotionally flat advice. This is especially
problematic in clinical contexts where patients are distressed and vulnerable,
and require empathic communication to support safety, adherence, and trust. We
present RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time
framework that adds structured emotional reasoning without retraining. By
decomposing empathy into transparent appraisal-theoretic stages and exposing
per-dimension Likert signals, RECAP produces nuanced, auditable responses.
Across EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by
22-28% on 8B models and 10-13% on larger models over zero-shot baselines.
Clinician evaluations further confirm superior empathetic communication. RECAP
shows that modular, theory-grounded prompting can systematically enhance
emotional intelligence in medical AI while preserving the accountability
required for deployment.

</details>


### [33] [Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction](https://arxiv.org/abs/2509.10798)
*Yijun Liu,Yixuan Wang,Yuzhuang Xu,Shiyu Ji,Yang Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: The paper proposes "Judge Q," a training method to enhance KV cache eviction in LLMs by integrating global information through a soft token list. Experiments show improved performance on benchmarks like LongBench and RULER.


<details>
  <summary>Details</summary>
Motivation: Managing memory and decoding efficiency in LLMs is challenging due to the linear growth of KV cache size as sequences expand. Current eviction methods often neglect global information, which hinders overall performance.

Method: The authors introduce "Judge Q," a method using soft token lists appended to input sequences. These tokens train attention maps to align with original decoded tokens, facilitating better evaluation of KV cache importance.

Result: Under a similar eviction budget, "Judge Q" reduces performance degradation in KV cache eviction compared to existing approaches. Experimental validation on Llama-3.1-8B and Mistral-7B models shows improvements of ~1 point on LongBench and ~3 points on RULER.

Conclusion: "Judge Q" is an effective, lightweight method for improving KV cache eviction in LLMs, enabling better decoding quality and seamless integration into open-source models.

Abstract: Large language models (LLMs) utilize key-value (KV) cache to store historical
information during sequence processing. The size of KV cache grows linearly as
the length of the sequence extends, which seriously affects memory usage and
decoding efficiency. Current methods for KV cache eviction typically utilize
the last window from the pre-filling phase as queries to compute the KV
importance scores for eviction. Although this scheme is simple to implement, it
tends to overly focus on local information, potentially leading to the neglect
or omission of crucial global information. To mitigate this issue, we propose
Judge Q, a novel training method which incorporates a soft token list. This
method only tunes the model's embedding layer at a low training cost. By
concatenating the soft token list at the end of the input sequence, we train
these tokens' attention map to the original input sequence to align with that
of the actual decoded tokens. In this way, the queries corresponding to the
soft tokens can effectively capture global information and better evaluate the
importance of the keys and values within the KV cache, thus maintaining
decoding quality when KV cache is evicted. Under the same eviction budget, our
method exhibits less performance degradation compared to existing eviction
approaches. We validate our approach through experiments conducted on models
such as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks
including LongBench, RULER, and Needle-in-a-Haystack. Results indicate an
improvement of approximately 1 point on the LongBench and over 3 points on
RULER. This proposed methodology can be seamlessly integrated into existing
open-source models with minimal training overhead, thereby enhancing
performance in KV cache eviction scenarios.

</details>


### [34] [Towards Automated Error Discovery: A Study in Conversational AI](https://arxiv.org/abs/2509.10833)
*Dominic Petrak,Thy Thy Tran,Iryna Gurevych*

Main category: cs.CL

TL;DR: This paper proposes Automated Error Discovery and SEEED, a novel approach to detect and define errors in conversational AI, improving error detection and generalization.


<details>
  <summary>Details</summary>
Motivation: Address challenges in current LLMs that struggle to identify unspecified or emerging errors in conversational AI, ensuring reliable performance.

Method: Proposes a framework called Automated Error Discovery and introduces SEEED, which enhances Soft Nearest Neighbor Loss for representation learning and uses Label-Based Sample Ranking to improve error detection.

Result: SEEED outperforms benchmarks like GPT-4o and Phi-4 in error detection accuracy, showing an improvement of up to 8 points and strong generalization capabilities.

Conclusion: SEEED represents a significant advancement in conversational AI error detection, demonstrating improved performance and adaptability to unknown scenarios.

Abstract: Although LLM-based conversational agents demonstrate strong fluency and
coherence, they still produce undesirable behaviors (errors) that are
challenging to prevent from reaching users during deployment. Recent research
leverages large language models (LLMs) to detect errors and guide
response-generation models toward improvement. However, current LLMs struggle
to identify errors not explicitly specified in their instructions, such as
those arising from updates to the response-generation model or shifts in user
behavior. In this work, we introduce Automated Error Discovery, a framework for
detecting and defining errors in conversational AI, and propose SEEED (Soft
Clustering Extended Encoder-Based Error Detection), as an encoder-based
approach to its implementation. We enhance the Soft Nearest Neighbor Loss by
amplifying distance weighting for negative samples and introduce Label-Based
Sample Ranking to select highly contrastive examples for better representation
learning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --
across multiple error-annotated dialogue datasets, improving the accuracy for
detecting unknown errors by up to 8 points and demonstrating strong
generalization to unknown intent detection.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [35] [A Real-Time Diminished Reality Approach to Privacy in MR Collaboration](https://arxiv.org/abs/2509.10466)
*Christian Fane*

Main category: cs.CV

TL;DR: This paper presents a real-time diminished reality system for privacy control in mixed reality settings by digitally removing objects using inpainting techniques and semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address privacy issues in shared-space mixed reality meetings by enabling users to digitally remove sensitive objects from their environment.

Method: The system employs YOLOv11 for object detection, semantic segmentation for precise object selection, and a modified DSTT model for real-time video inpainting using mobile ZED 2i depth camera.

Result: The system achieves real-time performance at over 20 fps on 720p resolution, demonstrating its practical feasibility and portability.

Conclusion: The proposed diminished reality system is effective, portable, and supports privacy-preserving applications in mixed reality settings without requiring prior environmental scanning or fixed viewpoints.

Abstract: Diminished reality (DR) refers to the digital removal of real-world objects
by compositing background content in their place. This thesis presents a
real-time, inpainting-based DR system designed to enable privacy control in
shared-space mixed reality (MR) meetings. The system allows a primary headset
user to selectively remove personal or sensitive items from their environment,
ensuring that those objects are no longer visible to other participants.
Removal is achieved through semantic segmentation and precise object selection,
followed by real-time inpainting from the viewpoint of a secondary observer,
implemented using a mobile ZED 2i depth camera. The solution is designed to be
portable and robust, requiring neither a fixed secondary viewpoint nor prior 3D
scanning of the environment. The system utilises YOLOv11 for object detection
and a modified Decoupled Spatial-Temporal Transformer (DSTT) model for
high-quality video inpainting. At 720p resolution, the pipeline sustains frame
rates exceeding 20 fps, demonstrating the feasibility of real-time diminished
reality for practical privacy-preserving MR applications.

</details>


### [36] [SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning](https://arxiv.org/abs/2509.10555)
*Alejandra Perez,Chinedu Nwoye,Ramtin Raji Kermani,Omid Mohareri,Muhammad Abdullah Jamal*

Main category: cs.CV

TL;DR: The paper introduces SurgLaVi, the largest and most diverse surgical vision-language dataset, along with a new contrastive learning model SurgCLIP, significantly enhancing surgical video understanding.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing surgical vision-language datasets, which suffer from constrained scale, diversity, and semantic quality, thereby impeding progress in surgical VLP.

Method: The authors create SurgLaVi, a comprehensive dataset generated through an automated pipeline for fine-grained transcription and segmentation of surgical videos, using dual-modality filtering to improve annotation quality. They also introduce SurgCLIP, a model with dual encoders for video-text contrastive learning.

Result: SurgCLIP, trained on the SurgLaVi dataset, delivers consistent improvements in critical surgical tasks such as phase, step, action, and tool recognition, outperforming state-of-the-art models.

Conclusion: The SurgLaVi dataset and SurgCLIP model demonstrate how large-scale, semantically rich, and hierarchical datasets can significantly enhance surgical foundational models. SurgLaVi is poised to be a crucial resource in this domain.

Abstract: Vision-language pre-training (VLP) offers unique advantages for surgery by
aligning language with surgical videos, enabling workflow understanding and
transfer across tasks without relying on expert-labeled datasets. However,
progress in surgical VLP remains constrained by the limited scale, procedural
diversity, semantic quality, and hierarchical structure of existing datasets.
In this work, we present SurgLaVi, the largest and most diverse surgical
vision-language dataset to date, comprising nearly 240k clip-caption pairs from
more than 200 procedures, and comprising hierarchical levels at phase-, step-,
and task-level. At the core of SurgLaVi lies a fully automated pipeline that
systematically generates fine-grained transcriptions of surgical videos and
segments them into coherent procedural units. To ensure high-quality
annotations, it applies dual-modality filtering to remove irrelevant and noisy
samples. Within this framework, the resulting captions are enriched with
contextual detail, producing annotations that are both semantically rich and
easy to interpret. To ensure accessibility, we release SurgLaVi-\b{eta}, an
open-source derivative of 113k clip-caption pairs constructed entirely from
public data, which is over four times larger than existing surgical VLP
datasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP,
a CLIP-style video-text contrastive framework with dual encoders, as a
representative base model. SurgCLIP achieves consistent improvements across
phase, step, action, and tool recognition, surpassing prior state-of-the-art
methods, often by large margins. These results validate that large-scale,
semantically rich, and hierarchically structured datasets directly translate
into stronger and more generalizable representations, establishing SurgLaVi as
a key resource for developing surgical foundation models.

</details>


### [37] [Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses](https://arxiv.org/abs/2509.10620)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: A SimCLR-based self-supervised learning (SSL) foundation model for high-resolution 3D brain MRI analysis is proposed, outperforming existing models in diverse tasks even using limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models processing 3D brain MRI are task-specific, unable to generalize across tasks or populations due to limited labeled data. SSL enables the creation of robust models for 2D imaging, but 3D MRI models have limitations.

Method: The model is pre-trained using SimCLR SSL on 44,958 scans from 11 datasets, spanning diverse neurological diseases. Comparisons are made against Masked Autoencoders (MAE) and supervised baselines across prediction tasks.

Result: The SimCLR-based model demonstrates superior performance across all tasks, including Alzheimer's prediction using limited labeled data, outperforming MAE and supervised baselines.

Conclusion: This research introduces an accessible, broadly applicable foundation model for clinical brain MRI analysis, advancing generalization and task adaptability in 3D brain MRI processing.

Abstract: 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly
acquired in clinical settings to monitor a wide range of neurological
conditions, including neurodegenerative disorders and stroke. While deep
learning models have shown promising results analyzing 3D MRI across a number
of brain imaging tasks, most are highly tailored for specific tasks with
limited labeled data, and are not able to generalize across tasks and/or
populations. The development of self-supervised learning (SSL) has enabled the
creation of large medical foundation models that leverage diverse, unlabeled
datasets ranging from healthy to diseased data, showing significant success in
2D medical imaging applications. However, even the very few foundation models
for 3D brain MRI that have been developed remain limited in resolution, scope,
or accessibility. In this work, we present a general, high-resolution
SimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on
18,759 patients (44,958 scans) from 11 publicly available datasets spanning
diverse neurological diseases. We compare our model to Masked Autoencoders
(MAE), as well as two supervised baselines, on four diverse downstream
prediction tasks in both in-distribution and out-of-distribution settings. Our
fine-tuned SimCLR model outperforms all other models across all tasks. Notably,
our model still achieves superior performance when fine-tuned using only 20% of
labeled training samples for predicting Alzheimer's disease. We use publicly
available code and data, and release our trained model at
https://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly
applicable and accessible foundation model for clinical brain MRI analysis.

</details>


### [38] [USCTNet: A deep unfolding nuclear-norm optimization solver for physically consistent HSI reconstruction](https://arxiv.org/abs/2509.10651)
*Xiaoyang Ma,Yiyang Chai,Xinran Qu,Hong Sun*

Main category: cs.CV

TL;DR: The paper introduces USCTNet, a physics-inspired deep learning method to reconstruct hyperspectral images (HSIs) from single RGB images by estimating camera spectral sensitivity (CSS) and illumination for consistency, and optimizing with a low-rank subspace singular-value thresholding operator.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the ill-posed nature of HSI reconstruction from RGB images, improving reconstruction accuracy while ensuring physical and colorimetric consistency.

Method: The method formulates HSI reconstruction as a physics-grounded inverse problem, regularized by a nuclear norm in a learnable transform domain. The forward operator is adaptively defined by estimating CSS and illumination, while a low-rank subspace SVT operator replaces traditional computationally expensive full SVDs. USCTNet integrates these ideas into a deep unfolding framework with learnable proximal updates and a parameter estimation module.

Result: The proposed USCTNet demonstrates consistent improvements in reconstruction accuracy over state-of-the-art RGB-based HSI reconstruction methods in extensive benchmark experiments.

Conclusion: The approach enhances HSI reconstruction by addressing physical inconsistencies and computational inefficiencies, presenting a significant step forward in RGB-to-HSI methods.

Abstract: Reconstructing hyperspectral images (HSIs) from a single RGB image is
ill-posed and can become physically inconsistent when the camera spectral
sensitivity (CSS) and scene illumination are misspecified. We formulate
RGB-to-HSI reconstruction as a physics-grounded inverse problem regularized by
a nuclear norm in a learnable transform domain, and we explicitly estimate CSS
and illumination to define the forward operator embedded in each iteration,
ensuring colorimetric consistency. To avoid the cost and instability of full
singular-value decompositions (SVDs) required by singular-value thresholding
(SVT), we introduce a data-adaptive low-rank subspace SVT operator. Building on
these components, we develop USCTNet, a deep unfolding solver tailored to HSI
that couples a parameter estimation module with learnable proximal updates.
Extensive experiments on standard benchmarks show consistent improvements over
state-of-the-art RGB-based methods in reconstruction accuracy. Code:
https://github.com/psykheXX/USCTNet-Code-Implementation.git

</details>


### [39] [A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI](https://arxiv.org/abs/2509.10683)
*Felicia Liu,Jay J. Yoo,Farzad Khalvati*

Main category: cs.CV

TL;DR: The paper assesses the capability of Large Language Models (LLMs) for glioma classification and segmentation using medical imaging, comparing them with CNNs. CNNs outperformed LLMs on both tasks, indicating LLMs' current inefficacy in image-based applications.


<details>
  <summary>Details</summary>
Motivation: To explore the unexplored potential and effectiveness of LLMs for medical imaging tasks, as they have shown strong text-based healthcare performance.

Method: Tested a vision-language LLM (LLaMA 3.2 Instruct) and custom 3D CNNs on the BraTS 2020 brain MRI dataset for glioma classification and segmentation. Evaluated the models' accuracy, precision, recall, and spatial understanding before and after fine-tuning.

Result: CNNs achieved 80% accuracy in classification, while LLMs reached 76% but struggled with low specificity. Fine-tuning mildly improved LLM performance but reduced overall accuracy. For segmentation, CNNs localized gliomas effectively, but LLMs showed severe spatial limitations, even after fine-tuning.

Conclusion: Current LLMs lack the spatial comprehension and robustness required for medical imaging tasks like glioma classification and segmentation. Rigorous fine-tuning or different training strategies are required for their improvement in image-based applications.

Abstract: Large Language Models (LLMs) have shown strong performance in text-based
healthcare tasks. However, their utility in image-based applications remains
unexplored. We investigate the effectiveness of LLMs for medical imaging tasks,
specifically glioma classification and segmentation, and compare their
performance to that of traditional convolutional neural networks (CNNs). Using
the BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a
general-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after
fine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma
classification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and
balanced precision and recall. The general LLM reached 76% accuracy but
suffered from a specificity of only 18%, often misclassifying Low-Grade tumors.
Fine-tuning improved specificity to 55%, but overall performance declined
(e.g., accuracy dropped to 72%). For segmentation, three methods - center
point, bounding box, and polygon extraction, were implemented. CNNs accurately
localized gliomas, though small tumors were sometimes missed. In contrast, LLMs
consistently clustered predictions near the image center, with no distinction
of glioma size, location, or placement. Fine-tuning improved output formatting
but failed to meaningfully enhance spatial accuracy. The bounding polygon
method yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in
both tasks. LLMs showed limited spatial understanding and minimal improvement
from fine-tuning, indicating that, in their current form, they are not
well-suited for image-based tasks. More rigorous fine-tuning or alternative
training strategies may be needed for LLMs to achieve better performance,
robustness, and utility in the medical space.

</details>


### [40] [Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation](https://arxiv.org/abs/2509.10687)
*Hao Zhang,Chun-Han Yao,Simon Donné,Narendra Ahuja,Varun Jampani*

Main category: cs.CV

TL;DR: Stable Part Diffusion 4D (SP4D) generates RGB and kinematic part videos from monocular inputs by leveraging a dual-branch diffusion model and innovative spatial encoding for flexible part segmentation.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of conventional appearance-based part segmentation methods by introducing kinematic-aware structural components consistent across views and time.

Method: SP4D uses a dual-branch diffusion architecture for RGB and part map synthesis paired with spatial color encoding and Bidirectional Diffusion Fusion (BiDiFuse) for cross-branch consistency.

Result: SP4D successfully generates 2D part maps that can be lifted to 3D skeletal structures and skinning weights, and performs strongly in diverse scenarios, including novel and articulated poses.

Conclusion: SP4D offers an effective solution for generating kinematic-aware video outputs, supporting animation and motion-related tasks while showcasing strong generalization capabilities.

Abstract: We present Stable Part Diffusion 4D (SP4D), a framework for generating paired
RGB and kinematic part videos from monocular inputs. Unlike conventional part
segmentation methods that rely on appearance-based semantic cues, SP4D learns
to produce kinematic parts - structural components aligned with object
articulation and consistent across views and time. SP4D adopts a dual-branch
diffusion model that jointly synthesizes RGB frames and corresponding part
segmentation maps. To simplify the architecture and flexibly enable different
part counts, we introduce a spatial color encoding scheme that maps part masks
to continuous RGB-like images. This encoding allows the segmentation branch to
share the latent VAE from the RGB branch, while enabling part segmentation to
be recovered via straightforward post-processing. A Bidirectional Diffusion
Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a
contrastive part consistency loss to promote spatial and temporal alignment of
part predictions. We demonstrate that the generated 2D part maps can be lifted
to 3D to derive skeletal structures and harmonic skinning weights with few
manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K,
a curated dataset of over 20K rigged objects selected and processed from
Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part
video sequences. Experiments show that SP4D generalizes strongly to diverse
scenarios, including real-world videos, novel generated objects, and rare
articulated poses, producing kinematic-aware outputs suitable for downstream
animation and motion-related tasks.

</details>


### [41] [SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition](https://arxiv.org/abs/2509.10710)
*Sven Schreiber,Noha Sarhan,Simone Frintrop,Christian Wilms*

Main category: cs.CV

TL;DR: SegSLR is a new ISLR system that combines RGB data and pose information using video segmentation to maintain hand and body detail. It outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current ISLR methods lose critical hand shape and orientation details when combining RGB and pose information. This gap motivated the development of a hybrid approach to retain these details.

Method: SegSLR uses zero-shot video segmentation to combine pose data for rough hand/body localization with RGB information, focusing on relevant signer's body parts to maintain detail.

Result: SegSLR surpasses state-of-the-art performance on the ChaLearn249 IsoGD dataset and demonstrates its effectiveness through various ablation studies.

Conclusion: Focusing on signer's hands and body parts using segmented data improves ISLR systems, validating the design decisions of SegSLR.

Abstract: Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB
data or signer pose information. However, combining these modalities often
results in the loss of crucial details, such as hand shape and orientation, due
to imprecise representations like bounding boxes. Therefore, we propose the
ISLR system SegSLR, which combines RGB and pose information through promptable
zero-shot video segmentation. Given the rough localization of the hands and the
signer's body from pose information, we segment the respective parts through
the video to maintain all relevant shape information. Subsequently, the
segmentations focus the processing of the RGB data on the most relevant body
parts for ISLR. This effectively combines RGB and pose information. Our
evaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR
outperforms state-of-the-art methods. Furthermore, ablation studies indicate
that SegSLR strongly benefits from focusing on the signer's body and hands,
justifying our design choices.

</details>


### [42] [SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation](https://arxiv.org/abs/2509.10748)
*Jecia Z. Y. Mao,Francis X Creighton,Russell H Taylor,Manish Sahu*

Main category: cs.CV

TL;DR: The paper introduces SCOPE, a speech-guided framework that combines large language models with vision foundation models for adaptable segmentation and tracking of surgical scenes.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of domain-specific models that rely on predefined labels and labeled data, and enable dynamic and adaptable segmentation and tracking of surgical scenes in real-time.

Method: The SCOPE framework uses large language models for reasoning and vision foundation models for perception, incorporating speech-guided intuitive feedback for collaborative segmentation and tracking during surgery.

Result: The framework was successfully evaluated on publicly available and in-house datasets, showcasing its ability to provide on-the-fly segmentation and tracking. Dynamic capabilities were further demonstrated through a live mock experiment.

Conclusion: SCOPE demonstrates the potential for developing hands-free, surgeon-centric tools for adaptable and dynamic surgical environments through human-AI collaboration.

Abstract: Accurate segmentation and tracking of relevant elements of the surgical scene
is crucial to enable context-aware intraoperative assistance and decision
making. Current solutions remain tethered to domain-specific, supervised models
that rely on labeled data and required domain-specific data to adapt to new
surgical scenarios and beyond predefined label categories. Recent advances in
prompt-driven vision foundation models (VFM) have enabled open-set, zero-shot
segmentation across heterogeneous medical images. However, dependence of these
models on manual visual or textual cues restricts their deployment in
introperative surgical settings. We introduce a speech-guided collaborative
perception (SCOPE) framework that integrates reasoning capabilities of large
language model (LLM) with perception capabilities of open-set VFMs to support
on-the-fly segmentation, labeling and tracking of surgical instruments and
anatomy in intraoperative video streams. A key component of this framework is a
collaborative perception agent, which generates top candidates of VFM-generated
segmentation and incorporates intuitive speech feedback from clinicians to
guide the segmentation of surgical instruments in a natural human-machine
collaboration paradigm. Afterwards, instruments themselves serve as interactive
pointers to label additional elements of the surgical scene. We evaluated our
proposed framework on a subset of publicly available Cataract1k dataset and an
in-house ex-vivo skull-base dataset to demonstrate its potential to generate
on-the-fly segmentation and tracking of surgical scene. Furthermore, we
demonstrate its dynamic capabilities through a live mock ex-vivo experiment.
This human-AI collaboration paradigm showcase the potential of developing
adaptable, hands-free, surgeon-centric tools for dynamic operating-room
environments.

</details>


### [43] [Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation](https://arxiv.org/abs/2509.10759)
*Yi-Ruei Liu,You-Zhe Xie,Yu-Hsiang Hsu,I-Sheng Fang,Yu-Lun Liu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: The paper introduces 4D-GRT, a method combining Gaussian Splatting with ray tracing to simulate camera effects. It outperforms existing methods in speed and quality for real-world camera effect rendering.


<details>
  <summary>Details</summary>
Motivation: Common vision systems fail with real-world camera effects due to insufficient training data or ineffective modeling approaches.

Method: 4D-GRT utilizes a two-stage pipeline: reconstructing dynamic scenes using 4D Gaussian Splatting, then generating videos with physically accurate camera effects via ray tracing.

Result: 4D-GRT demonstrates superior or equal rendering quality while achieving the fastest rendering speed compared to baseline methods.

Conclusion: 4D-GRT effectively models real-world camera effects, providing a benchmark for evaluating dynamic scenes under different camera conditions.

Abstract: Common computer vision systems typically assume ideal pinhole cameras but
fail when facing real-world camera effects such as fisheye distortion and
rolling shutter, mainly due to the lack of learning from training data with
camera effects. Existing data generation approaches suffer from either high
costs, sim-to-real gaps or fail to accurately model camera effects. To address
this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage
pipeline that combines 4D Gaussian Splatting with physically-based ray tracing
for camera effect simulation. Given multi-view videos, 4D-GRT first
reconstructs dynamic scenes, then applies ray tracing to generate videos with
controllable, physically accurate camera effects. 4D-GRT achieves the fastest
rendering speed while performing better or comparable rendering quality
compared to existing baselines. Additionally, we construct eight synthetic
dynamic scenes in indoor environments across four camera effects as a benchmark
to evaluate generated videos with camera effects.

</details>


### [44] [EditDuet: A Multi-Agent System for Video Non-Linear Editing](https://arxiv.org/abs/2509.10761)
*Marcelo Sandoval-Castaneda,Bryan Russell,Josef Sivic,Gregory Shakhnarovich,Fabian Caba Heilbron*

Main category: cs.CV

TL;DR: This paper proposes automating video editing using a multi-agent system with an Editor agent and a Critic agent, where the Editor performs edits based on natural language instructions and the Critic provides feedback.


<details>
  <summary>Details</summary>
Motivation: To simplify the video editing process by automating intricate tasks that traditionally required user engagement.

Method: The paper employs a multi-agent framework consisting of an Editor agent that performs video edits using standard editing tools based on language instructions, and a Critic agent that evaluates and provides feedback on the edits.

Result: The proposed system was tested through qualitative and quantitative user studies, demonstrating superior performance over existing methods in coverage, time constraint satisfaction, and user preference.

Conclusion: Automating video editing with specialized agents enhances editing quality and efficiency, making it a practical tool for users across various applications.

Abstract: Automated tools for video editing and assembly have applications ranging from
filmmaking and advertisement to content creation for social media. Previous
video editing work has mainly focused on either retrieval or user interfaces,
leaving actual editing to the user. In contrast, we propose to automate the
core task of video editing, formulating it as sequential decision making
process. Ours is a multi-agent approach. We design an Editor agent and a Critic
agent. The Editor takes as input a collection of video clips together with
natural language instructions and uses tools commonly found in video editing
software to produce an edited sequence. On the other hand, the Critic gives
natural language feedback to the editor based on the produced sequence or
renders it if it is satisfactory. We introduce a learning-based approach for
enabling effective communication across specialized agents to address the
language-driven video editing task. Finally, we explore an LLM-as-a-judge
metric for evaluating the quality of video editing system and compare it with
general human preference. We evaluate our system's output video sequences
qualitatively and quantitatively through a user study and find that our system
vastly outperforms existing approaches in terms of coverage, time constraint
satisfaction, and human preference.

</details>


### [45] [Enhancement Without Contrast: Stability-Aware Multicenter Machine Learning for Glioma MRI Imaging](https://arxiv.org/abs/2509.10767)
*Sajad Amiri,Shahram Taeb,Sara Gharibi,Setareh Dehghanfard,Somayeh Sadat Mehrnia,Mehrdad Oveisi,Ilker Hacihaliloglu,Arman Rahmim,Mohammad R. Salmanpour*

Main category: cs.CV

TL;DR: The paper introduces a machine learning framework to reliably predict glioma MRI contrast enhancement using non-contrast images, offering an alternative to gadolinium-based contrast agents.


<details>
  <summary>Details</summary>
Motivation: Safety, cost, and accessibility concerns associated with gadolinium-based contrast agents necessitate alternative methods for glioma imaging.

Method: A stability-aware machine learning framework tested across multiple datasets and configurations using rotational validation to identify reproducible pipelines for contrast enhancement prediction.

Result: The proposed framework achieved high cross-validation accuracy (0.91–0.96) and external testing accuracy (0.87–0.98), demonstrating strong performance despite cohort heterogeneity.

Conclusion: Stability-aware model selection supports robust and generalizable ML pipelines for non-contrast MRI prediction, reducing reliance on contrast agents and advancing neuro-oncology imaging methods.

Abstract: Gadolinium-based contrast agents (GBCAs) are central to glioma imaging but
raise safety, cost, and accessibility concerns. Predicting contrast enhancement
from non-contrast MRI using machine learning (ML) offers a safer alternative,
as enhancement reflects tumor aggressiveness and informs treatment planning.
Yet scanner and cohort variability hinder robust model selection. We propose a
stability-aware framework to identify reproducible ML pipelines for multicenter
prediction of glioma MRI contrast enhancement. We analyzed 1,446 glioma cases
from four TCIA datasets (UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG).
Non-contrast T1WI served as input, with enhancement derived from paired
post-contrast T1WI. Using PyRadiomics under IBSI standards, 108 features were
extracted and combined with 48 dimensionality reduction methods and 25
classifiers, yielding 1,200 pipelines. Rotational validation was trained on
three datasets and tested on the fourth. Cross-validation prediction accuracies
ranged from 0.91 to 0.96, with external testing achieving 0.87 (UCSF-PDGM),
0.98 (UPENN-GB), and 0.95 (BRATS-Africa), with an average of 0.93. F1,
precision, and recall were stable (0.87 to 0.96), while ROC-AUC varied more
widely (0.50 to 0.82), reflecting cohort heterogeneity. The MI linked with ETr
pipeline consistently ranked highest, balancing accuracy and stability. This
framework demonstrates that stability-aware model selection enables reliable
prediction of contrast enhancement from non-contrast glioma MRI, reducing
reliance on GBCAs and improving generalizability across centers. It provides a
scalable template for reproducible ML in neuro-oncology and beyond.

</details>


### [46] [Group Evidence Matters: Tiling-based Semantic Gating for Dense Object Detection](https://arxiv.org/abs/2509.10779)
*Yilun Xiao*

Main category: cs.CV

TL;DR: This paper introduces a post-processing framework to boost recall in dense small object detection from UAV imagery by leveraging overlapping tiling and clustering steps without retraining detectors.


<details>
  <summary>Details</summary>
Motivation: Dense small objects in UAV imagery are often missed due to challenges like long-range viewpoints, occlusion, and clutter, necessitating methods to recover these missed detections.

Method: The proposed framework utilizes overlapping tiling to expose low-confidence candidates, Spatial and Semantic Gates (based on DBSCAN clustering) for geometry and appearance validation, and controlled confidence reweighting before non-maximum suppression fusion.

Result: Experiments on the VisDrone dataset report a recall improvement from 0.685 to 0.778 (+0.093), with a reduction in precision but yielding an F1 score of 0.669. The average post-processing latency is 0.095 seconds per image.

Conclusion: The proposed approach enhances recall for applications requiring high recall, such as monitoring and counting in far-field UAV imagery, and is detector-agnostic, requiring no retraining. Future work aims to reduce semantic clustering costs and incorporate temporal information.

Abstract: Dense small objects in UAV imagery are often missed due to long-range
viewpoints, occlusion, and clutter[cite: 5]. This paper presents a
detector-agnostic post-processing framework that converts overlap-induced
redundancy into group evidence[cite: 6]. Overlapping tiling first recovers
low-confidence candidates[cite: 7]. A Spatial Gate (DBSCAN on box centroids)
and a Semantic Gate (DBSCAN on ResNet-18 embeddings) then validates group
evidence[cite: 7]. Validated groups receive controlled confidence reweighting
before class-aware NMS fusion[cite: 8]. Experiments on VisDrone show a recall
increase from 0.685 to 0.778 (+0.093) and a precision adjustment from 0.801 to
0.595, yielding F1=0.669[cite: 9]. Post-processing latency averages 0.095 s per
image[cite: 10]. These results indicate recall-first, precision-trade-off
behavior that benefits recall-sensitive applications such as far-field counting
and monitoring[cite: 10]. Ablation confirms that tiling exposes missed objects,
spatial clustering stabilizes geometry, semantic clustering enforces appearance
coherence, and reweighting provides calibrated integration with the
baseline[cite: 11]. The framework requires no retraining and integrates with
modern detectors[cite: 12]. Future work will reduce semantic gating cost and
extend the approach with temporal cues[cite: 13].

</details>


### [47] [InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts](https://arxiv.org/abs/2509.10813)
*Weipeng Zhong,Peizhou Cao,Yichen Jin,Li Luo,Wenzhe Cai,Jingli Lin,Hanqing Wang,Zhaoyang Lyu,Tai Wang,Bo Dai,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: InternScenes is a large-scale, diverse, and realistic 3D indoor scene dataset containing approximately 40,000 scenes, designed for Embodied AI applications.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing 3D scene datasets, which often lack scale, diversity, realistic small-item layouts, and suffer from object collisions.

Method: The authors introduce InternScenes by integrating scenes from real-world scans, procedural generation, and designer-created layouts. They utilize a comprehensive processing pipeline to enhance simulatability, add interactive objects, and resolve object collisions.

Result: InternScenes comprises 40,000 scenes with 1.96 million objects across 288 object classes, featuring realistic layouts with an average of 41.5 objects per region. Benchmarks show the dataset enables training and poses new challenges for scene layout generation and point-goal navigation.

Conclusion: InternScenes advances the field of Embodied AI by providing a comprehensive and scalable dataset, facilitating both model training and navigation in complex indoor environments. The open-sourcing of data, models, and benchmarks benefits the research community.

Abstract: The advancement of Embodied AI heavily relies on large-scale, simulatable 3D
scene datasets characterized by scene diversity and realistic layouts. However,
existing datasets typically suffer from limitations in data scale or diversity,
sanitized layouts lacking small items, and severe object collisions. To address
these shortcomings, we introduce \textbf{InternScenes}, a novel large-scale
simulatable indoor scene dataset comprising approximately 40,000 diverse scenes
by integrating three disparate scene sources, real-world scans, procedurally
generated scenes, and designer-created scenes, including 1.96M 3D objects and
covering 15 common scene types and 288 object classes. We particularly preserve
massive small items in the scenes, resulting in realistic and complex layouts
with an average of 41.5 objects per region. Our comprehensive data processing
pipeline ensures simulatability by creating real-to-sim replicas for real-world
scans, enhances interactivity by incorporating interactive objects into these
scenes, and resolves object collisions by physical simulations. We demonstrate
the value of InternScenes with two benchmark applications: scene layout
generation and point-goal navigation. Both show the new challenges posed by the
complex and realistic layouts. More importantly, InternScenes paves the way for
scaling up the model training for both tasks, making the generation and
navigation in such complex scenes possible. We commit to open-sourcing the
data, models, and benchmarks to benefit the whole community.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [48] [Asynchronous Gathering of Opaque Robots with Mobility Faults](https://arxiv.org/abs/2509.10711)
*Subhajit Pramanick,Saswata Jana,Partha Sarathi Mandal,Gokarna Sharma*

Main category: cs.DC

TL;DR: This paper addresses robot gathering under different fault and asynchrony conditions, proposing algorithms with varying light colors and analyzing time complexities.


<details>
  <summary>Details</summary>
Motivation: To overcome previous impossibility results in robot gathering under fault systems using models with limited resources and conditions.

Method: The authors establish its impossibility and propose algorithms using mobility fault model leveraging color lights and asynchronous scheduling.

Result: Four key results are presented: impossibility with 2-colored lights under certain conditions, solutions with 3-colored lights, algorithms for unknown (N,f) systems with time-color trade-offs.

Conclusion: Introduces new deterministic algorithms optimized for time, colors, and system conditions making progress in fault-tolerant robot gathering.

Abstract: We consider the fundamental benchmarking problem of gathering in an
$(N,f)$-fault system consisting of $N$ robots, of which at most $f$ might fail
at any execution, under asynchrony. Two seminal results established
impossibility of a solution in the oblivious robot (OBLOT) model in a
$(2,0)$-fault system under semi-synchrony and in a $(3,1)$-Byzantine fault
system under asynchrony. Recently, a breakthrough result circumvented the first
impossibility result by giving a deterministic algorithm in a $(2,0)$-fault
system under asynchrony in the luminous robot (LUMI) model using 2-colored
lights. However, a breakthrough result established impossibility of gathering
in a $(2,1)$-crash system in the LUMI model under semi-synchrony. In this
paper, we consider a {\em mobility fault} model in which a robot crash only
impacts it mobility but not the operation of the light.
  We establish four results under asynchrony in LUMI with the mobility fault
model. We show that it is impossible to solve gathering in a $(2,1)$-mobility
fault system using 2-colored lights, and then give a solution using 3-colored
lights, which is optimal w.r.t. the number of colors. We then consider an
$(N,f)$-mobility fault system, $f<N$, both $N,f$ not known, and give two
deterministic algorithms that exhibit a nice time-color trade-off: The first
with time $O(N)$ using 7-colored lights and the second with time
$O(\max\{\ell,f\})$ using 26-colored lights, where $\ell< N$ is the number of
distinct convex layers of robot positions in the initial configuration.
Interestingly, for $l, f = O(1)$, our result is optimal. Our algorithms for an
$(N,f)$-mobility fault system are the first to be analysed time complexity, can
withstand obstructed visibility (opaque robot model) and asynchronous
scheduling.

</details>


### [49] [MinatoLoader: Accelerating Machine Learning Training Through Efficient Data Preprocessing](https://arxiv.org/abs/2509.10712)
*Rahma Nouaji,Stella Bitchebe,Ricardo Macedo,Oana Balmau*

Main category: cs.DC

TL;DR: MinatoLoader improves GPU utilization and ML training speed significantly by addressing inefficiencies in data preprocessing.


<details>
  <summary>Details</summary>
Motivation: Common data loaders in ML frameworks are inefficient, causing high GPU idleness due to variability in preprocessing times, leading to training delays.

Method: MinatoLoader utilizes background preprocessing and prioritizes fast-to-preprocess data while slow samples are handled parallelly, optimizing batch construction.

Result: MinatoLoader achieves up to 7.5× training speed improvement and increases GPU utilization from 46.4% to 90.45% across diverse workloads on high-performance GPUs.

Conclusion: MinatoLoader significantly enhances ML training efficiency, achieving faster convergence without compromising model accuracy.

Abstract: Data loaders are used by Machine Learning (ML) frameworks like PyTorch and
TensorFlow to apply transformations to data before feeding it into the
accelerator. This operation is called data preprocessing. Data preprocessing
plays an important role in the ML training workflow because if it is
inefficiently pipelined with the training, it can yield high GPU idleness,
resulting in important training delays. Unfortunately, existing data loaders
turn out to waste GPU resources, with $76\%$ GPU idleness when using the
PyTorch data loader, for example. One key source of inefficiency is the
variability in preprocessing time across samples within the same dataset.
Existing data loaders are oblivious to this variability, and they construct
batches without any consideration of slow or fast samples. In this case, the
entire batch is delayed by a single slow sample, stalling the training pipeline
and resulting in head-of-line blocking.
  To address these inefficiencies, we present MinatoLoader, a general-purpose
data loader for PyTorch that accelerates training and improves GPU utilization.
MinatoLoader is designed for a single-server setup, containing multiple GPUs.
It continuously prepares data in the background and actively constructs batches
by prioritizing fast-to-preprocess samples, while slower samples are processed
in parallel.
  We evaluate MinatoLoader on servers with V100 and A100 GPUs. On a machine
with four A100 GPUs, MinatoLoader improves the training time of a wide range of
workloads by up to $7.5\times$ ($3.6\times$ on average) over PyTorch DataLoader
and Pecan, and up to $3\times$ ($2.2\times$ on average) over DALI. It also
increases average GPU utilization from 46.4\% with PyTorch to 90.45\%, while
preserving model accuracy and enabling faster convergence.

</details>


### [50] [Coordinated Reinforcement Learning Prefetching Architecture for Multicore Systems](https://arxiv.org/abs/2509.10719)
*Mohammed Humaid Siddiqui,Fernando Guzman,Yufei Wu,Ruishu Ann*

Main category: cs.DC

TL;DR: CRL-Pythia improves multicore system performance and reduces redundancy in prefetching by leveraging coordinated reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The increasing redundancy in traditional prefetchers and performance loss of modern prefetchers like Pythia in multicore systems highlight the need for a novel approach.

Method: The paper introduces CRL-Pythia, a prefetcher based on coordinated reinforcement learning designed to enable cross-core sharing of information and cooperative prefetching.

Result: CRL-Pythia outperforms traditional Pythia configurations, enhancing IPC by approximately 12% for bandwidth-constrained workloads and showing robustness and scalability.

Conclusion: CRL-Pythia is an efficient and practical solution for modern multicore systems, addressing prefetching redundancy and boosting performance.

Abstract: Hardware prefetching is critical to fill the performance gap between CPU
speeds and slower memory accesses. With multicore architectures becoming
commonplace, traditional prefetchers are severely challenged. Independent core
operation creates significant redundancy (up to 20% of prefetch requests are
duplicates), causing unnecessary memory bus traffic and wasted bandwidth.
Furthermore, cutting-edge prefetchers such as Pythia suffer from about a 10%
performance loss when scaling from a single-core to a four-core system. To
solve these problems, we propose CRL-Pythia, a coordinated reinforcement
learning based prefetcher specifically designed for multicore systems. In this
work, CRL-Pythia addresses these issues by enabling cross-core sharing of
information and cooperative prefetching decisions, which greatly reduces
redundant prefetch requests and improves learning convergence across cores. Our
experiments demonstrate that CRL-Pythia outperforms single Pythia
configurations in all cases, with approximately 12% IPC (instructions per
cycle) improvement for bandwidth-constrained workloads, while imposing moderate
hardware overhead. Our sensitivity analyses also verify its robustness and
scalability, thereby making CRL-Pythia a practical and efficient solution to
contemporary multicore systems.

</details>


### [51] [Enhancing Type Safety in MPI with Rust: A Statically Verified Approach for RSMPI](https://arxiv.org/abs/2509.10803)
*Nafees Iqbal,Jed Brown*

Main category: cs.DC

TL;DR: The paper introduces a type-safe communication framework for the Message Passing Interface (MPI) using the Rust programming language to eliminate errors and improve robustness in high-performance computing.


<details>
  <summary>Details</summary>
Motivation: MPI, while widely used in HPC, has a low-level interface that is prone to runtime errors and lacks type safety. Rust's strong type system can address these issues without affecting performance.

Method: The framework is built on the RSMPI library and introduces the TypedCommunicator abstraction, which enforces static type safety in point-to-point communication. It uses Rust’s Equivalence trait to ensure type compatibility, with support for both compile-time and runtime validation.

Result: The approach eliminates common MPI errors, enhances developer productivity, and maintains performance through Rust’s zero-cost abstractions.

Conclusion: The framework enhances the reliability of MPI programming by incorporating type safety and aligns with Rust's principles. It also sets the stage for future work on type-safe collective operations in parallel computing.

Abstract: The Message Passing Interface (MPI) is a fundamental tool for building
high-performance computing (HPC) applications, enabling efficient communication
across distributed systems. Despite its widespread adoption, MPI's low-level
interface and lack of built-in type safety make it prone to runtime errors,
undefined behavior, and debugging challenges, especially in large-scale
applications. Rust, a modern systems programming language, offers a compelling
solution with its strong type system, which enforces memory and type safety at
compile time without compromising performance. This paper introduces a
type-safe communication framework for MPI, built on the RSMPI library, to
address the limitations of traditional MPI programming. At its core is the
TypedCommunicator, an abstraction that enforces static type safety in
point-to-point communication operations. By leveraging Rust's Equivalence
trait, our framework guarantees that only compatible types can participate in
communication, catching mismatches either at compile time or through runtime
validation. The framework supports both single-value and slice-based
communication, providing an intuitive API for diverse data structures. Our
implementation demonstrates that this approach eliminates common MPI errors,
improves developer productivity, and maintains performance, adhering to Rust's
principle of zero-cost abstractions. This work lays the foundation for
extending type safety to collective operations, advancing the robustness of
parallel computing in Rust.

</details>


### [52] [Chameleon: Taming Dynamic Operator Sequences for Memory-Intensive LLM Training](https://arxiv.org/abs/2509.11076)
*Zibo Wang,Yuhang Zhou,Zhibin Wang,Shipeng Li,Xinjing Huang,Chendong Cai,Bingxu Mu,Yuqing Sun,Zhiheng Hu,Bin She,Shu You,Guanghuan Fang,Rong Gu,Wanchun Dou,Guihai Chen,Chen Tian*

Main category: cs.DC

TL;DR: This paper introduces Chameleon, a swap-based memory optimization framework for training large language models (LLMs) in Eager Mode, addressing dynamic operator sequences.


<details>
  <summary>Details</summary>
Motivation: The rising memory demands of large language models often exceed hardware capabilities, necessitating efficient memory optimization methods without compromising accuracy or performance, especially in dynamic environments like Eager Mode.

Method: Chameleon integrates a lightweight online profiler for continuous monitoring, employs strategic swap policy generation with minimal operator data, and enhances execution modules for precise application and efficiency.

Result: The system reduces profiling overhead by 84.25%, allows training of models up to 4x larger than hardware memory, adapts to operator changes, and achieves a performance boost of up to 38.94% compared to other methods.

Conclusion: Chameleon successfully addresses the limitations of conventional swap methods, enabling efficient training of larger LLMs while adapting dynamically to operator sequence changes.

Abstract: The increasing size of large language models (LLMs) has led to a surge in
memory requirements during training, often exceeding the capacity of
high-bandwidth memory (HBM). Swap-based memory optimization incurs neither
accuracy loss nor additional end-to-end overhead when effectively overlapped,
thus being an attractive solution. However, existing swap methods assume
consistent operator sequences, which is impractical in Eager Mode, where
operator sequences can vary during change.
  We propose Chameleon, which redesigns the end-to-end process of swap-based
memory optimization and is the first work to consider varying operator
sequences in Eager Mode. Chameleon (i) introduces a lightweight online profiler
to enable continuous profiling for monitoring operator sequences, (ii)
generates effective swap policies with limited operator information, and (iii)
optimizes the policy execution module for accurate policy application and
better performance. Experimental results demonstrate that Chameleon reduces
profiling overhead by 84.25%, enables training models up to 4x larger than
hardware memory while adapting to changes in operator sequences, improves
performance by up to 38.94% compared to recomputation or high-degree
parallelism.

</details>


### [53] [GFS: A Preemption-aware Scheduling Framework for GPU Clusters with Predictive Spot Instance Management](https://arxiv.org/abs/2509.11134)
*Jiaang Duan,Shenglin Xu,Shiyou Qian,Dingyu Yang,Kangjin Wang,Chenzhi Liao,Yinghao Yu,Qin Hua,Hanwen Hu,Qi Wang,Wenchao Wu,Dongqing Bao,Tianyu Lu,Jian Cao,Guangtao Xue,Guodong Yang,Liping Zhang,Gang Chen*

Main category: cs.DC

TL;DR: GFS is a scheduling framework aimed at optimizing GPU resource allocation between high-priority and low-priority tasks, reducing eviction rates and queuing delays.


<details>
  <summary>Details</summary>
Motivation: The paper addresses inefficiencies in GPU management, particularly high eviction rates and long queuing times under current scheduling mechanisms for cloud computing tasks.

Method: GFS employs three core strategies: a lightweight forecasting model for tenant GPU demand, dynamic spot allocation for LP tasks, and a preemptive scheduling policy to prioritize HP tasks while minimizing LP impact.

Result: GFS decreases eviction rates by 33.0%, queuing delays by 44.1%, and improves GPU allocation rates by 22.8%. In real production, it generates $459,715 in monthly benefits for a 10,000-GPU cluster.

Conclusion: GFS is an effective solution for enhancing GPU scheduling efficiency, enabling cost savings and improved resource utilization in cloud environments.

Abstract: The surge in large language models (LLMs) has fundamentally reshaped the
landscape of GPU usage patterns, creating an urgent need for more efficient
management strategies. While cloud providers employ spot instances to reduce
costs for low-priority (LP) tasks, existing schedulers still grapple with high
eviction rates and lengthy queuing times. To address these limitations, we
present GFS, a novel preemptive scheduling framework that enhances
service-level objective (SLO) compliance for high-priority (HP) tasks while
minimizing preemptions to LP tasks. Firstly, GFS utilizes a lightweight
forecasting model that predicts GPU demand among different tenants, enabling
proactive resource management. Secondly, GFS employs a dynamic allocation
mechanism to adjust the spot quota for LP tasks with guaranteed durations.
Lastly, GFS incorporates a preemptive scheduling policy that prioritizes HP
tasks while minimizing the impact on LP tasks. We demonstrate the effectiveness
of GFS through both real-world implementation and simulations. The results show
that GFS reduces eviction rates by 33.0\%, and cuts queuing delays by 44.1\%
for LP tasks. Furthermore, GFS enhances the GPU allocation rate by up to 22.8\%
in real production clusters. In a production cluster of more than 10,000 GPUs,
GFS yields roughly \$459,715 in monthly benefits.

</details>


### [54] [Linear Complexity $\mathcal{H}^2$ Direct Solver for Fine-Grained Parallel Architectures](https://arxiv.org/abs/2509.11152)
*Wajih Boukaram,David Keyes,Sherry Li,Yang Liu,George Turkiyyah*

Main category: cs.DC

TL;DR: This paper introduces a linear complexity direct solver for matrices in hierarchical formats, optimized for fine-grained parallel architectures. It achieves efficient factorization and solution with compact, parallelizable algorithms.


<details>
  <summary>Details</summary>
Motivation: The need to solve large-scale dense matrices efficiently on modern parallel architectures motivated the development of a scalable and memory-efficient algorithm.

Method: The solver leverages a strong-admissibility-based hierarchical ($\mathcal{H}^2$) representation and recursive skeletonization factorization while avoiding dynamic memory allocations through prefix-sum memory management.

Result: The solver achieves linear scaling in time and memory for matrices with sizes up to one million and demonstrates parallel scaling across 16 threads.

Conclusion: The proposed methodology is effective for large-scale dense matrices, offering linear complexity and scalable performance while requiring minimal input information about the matrix or its origin.

Abstract: We present factorization and solution phases for a new linear complexity
direct solver designed for concurrent batch operations on fine-grained parallel
architectures, for matrices amenable to hierarchical representation. We focus
on the strong-admissibility-based $\mathcal{H}^2$ format, where strong
recursive skeletonization factorization compresses remote interactions. We
build upon previous implementations of $\mathcal{H}^2$ matrix construction for
efficient factorization and solution algorithm design, which are illustrated
graphically in stepwise detail. The algorithms are ``blackbox'' in the sense
that the only inputs are the matrix and right-hand side, without analytical or
geometrical information about the origin of the system. We demonstrate linear
complexity scaling in both time and memory on four representative families of
dense matrices up to one million in size. Parallel scaling up to 16 threads is
enabled by a multi-level matrix graph coloring and avoidance of dynamic memory
allocations thanks to prefix-sum memory management. An experimental backward
error analysis is included. We break down the timings of different phases,
identify phases that are memory-bandwidth limited, and discuss alternatives for
phases that may be sensitive to the trend to employ lower precisions for
performance.

</details>


### [55] [Adaptive K-PackCache: Cost-Centric Data Caching in Cloud](https://arxiv.org/abs/2509.11156)
*Suvarthi Sarkar,Aadarshraj Sah,Poddutoori Sweeya Reddy,Aryabartta Sahu*

Main category: cs.DC

TL;DR: The paper introduces a general K packing for caching data efficiently in content delivery networks (CDNs), using a proposed online algorithm, Adaptive K PackCache (AKPC).


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of pairwise caching in CDNs by enabling variable size data bundles that minimize costs while maximizing access efficiency.

Method: The problem, termed K PackCache, is modeled to optimize transfer and memory rental costs. AKPC is proposed as an online algorithm that dynamically adjusts data bundling based on access patterns and content correlation. It employs clique merging and supports batch processing.

Result: The AKPC algorithm achieved cost reductions of up to 63% and 55% on Netflix and Spotify data, respectively, compared to online baselines. Its results were within 15% and 13% of the theoretical optimal performance.

Conclusion: The AKPC method is scalable and effective for real-world caching systems, providing significant cost savings and competitive performance in managing data access patterns.

Abstract: Recent advances in data analytics have enabled the accurate prediction of
user access patterns, giving rise to the idea of packed caching delivering
multiple co accessed data items together as a bundle. This improves caching
efficiency, as accessing one item often implies the need for others. Prior work
has explored only 2 item pairwise packing. In this paper, we extend the concept
to general K packing, allowing variable size bundles for improved flexibility
and performance. We formulate the K PackCache problem from a content delivery
network CDN operator perspective, aiming to minimize total cost comprising two
components: transfer cost modeled as a base cost plus a linearly increasing
term with the number of items packed, and memory rental cost for caching, which
depends on how long and how much is stored. Overpacking increases cost due to
low utility, underpacking leads to missed sharing opportunities. We propose an
online algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,
and splits data cliques based on user access patterns and content correlation.
Our approach supports batch requests, enables approximate clique merging, and
offers a formal competitive guarantee. Through extensive evaluation on the
Netflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55
percentage over online baselines, respectively, and achieves performance within
15 and 13 percentage of the optimal. This demonstrates its scalability and
effectiveness for real world caching systems.

</details>


### [56] [Energy-Efficient Joint Offloading and Resource Allocation for Deadline-Constrained Tasks in Multi-Access Edge Computing](https://arxiv.org/abs/2509.11162)
*Chuanchao Gao,Arvind Easwaran*

Main category: cs.DC

TL;DR: This paper addresses energy-efficient task offloading and resource allocation in multi-access edge computing, proposing an algorithm that achieves near-optimal performance.


<details>
  <summary>Details</summary>
Motivation: Optimize energy saving for IoT devices while considering task deadlines and resource constraints.

Method: Formulated the problem as NP-Hard Integer Nonlinear Programming and introduced a Graph-Matching-based Approximation Algorithm (GMA).

Result: The GMA algorithm achieves near-optimal energy saving, matching 97% of the optimal value on average.

Conclusion: The GMA algorithm provides an effective approximation solution for energy-efficient task offloading in challenging computational environments.

Abstract: This paper addresses the deadline-constrained task offloading and resource
allocation problem in multi-access edge computing. We aim to determine where
each task is offloaded and processed, as well as corresponding communication
and computation resource allocations, to maximize the total saved energy for
IoT devices, while considering task deadline and system resource constraints.
Especially, our system allows each task to be offloaded to one of its
accessible access points (APs) and processed on a server that is not co-located
with its offloading AP. We formulate this problem as an Integer Nonlinear
Programming problem and show it is NP-Hard. To address this problem, we propose
a Graph-Matching-based Approximation Algorithm ($\mathtt{GMA}$), the first
approximation algorithm of its kind. $\mathtt{GMA}$ leverages linear
relaxation, tripartite graph construction, and a Linear Programming rounding
technique. We prove that $\mathtt{GMA}$ is a
$\frac{1-\alpha}{2+\epsilon}$-approximation algorithm, where $\epsilon$ is a
small positive value, and $\alpha$ ($0$$\le$$\alpha$$<$$1$) is a system
parameter that ensures the resource allocated to any task by an AP or a server
cannot exceed $\alpha$ times its resource capacity. Experiments show that, in
practice, $\mathtt{GMA}$'s energy saving achieves $97\%$ of the optimal value
on average.

</details>


### [57] [Parallel/Distributed Tabu Search for Scheduling Microprocessor Tasks in Hybrid Flowshop](https://arxiv.org/abs/2509.11396)
*Adam Janiak,Damian Kowalczyk,Maciej Lichtenstein*

Main category: cs.DC

TL;DR: This paper addresses minimizing makespan in a hybrid flow shop scheduling problem with multiprocessor tasks using a tabu search algorithm.


<details>
  <summary>Details</summary>
Motivation: To improve scheduling efficiency in hybrid flow shops with multiprocessor tasks, which are more complex than traditional flow shops.

Method: The authors propose a tabu search algorithm that employs parallel and distributed mechanisms to evaluate neighborhoods while optimizing for heterogeneous network environments.

Result: Development of an advanced scheduling algorithm aimed at achieving better balance and efficiency in hybrid flow shop scenarios.

Conclusion: The proposed algorithm effectively tackles the challenge of makespan minimization in complex scheduling environments utilizing parallel and distributed approaches.

Abstract: The paper deals with the makespan minimization in the hybrid flow shop
scheduling problem with multiprocessor tasks. The hybrid flow shop (HFS)
generalizes the classical flow shop processor configuration by replacing each
processor (processing stage) by some number of identical parallel processors.
Similarly, the multiprocessor tasks generalize the classical assumption, by
allowing a task to require more than one processor simultaneously for its
processing. In this work we present the algorithm for solving the problem based
on the tabu search technique. The proposed algorithm uses parallel and
distributed mechanisms for neighborhood evaluation and well balances
heterogeneous network environment.

</details>


### [58] [Machine Learning-Driven Predictive Resource Management in Complex Science Workflows](https://arxiv.org/abs/2509.11512)
*Tasnuva Chowdhury,Tadashi Maeno,Fatih Furkan Akman,Joseph Boudreau,Sankha Dutta,Shengyu Feng,Adolfy Hoisie,Kuan-Chieh Hsu,Raees Khan,Jaehyung Kim,Ozgur O. Kilic,Scott Klasky,Alexei Klimentov,Tatiana Korchuganova,Verena Ingrid Martinez Outschoorn,Paul Nilsson,David K. Park,Norbert Podhorszki,Yihui Ren,John Rembrandt Steele,Frédéric Suter,Sairam Sri Vatsavai,Torre Wenaus,Wei Yang,Yiming Yang,Shinjae Yoo*

Main category: cs.DC

TL;DR: This paper introduces machine learning models within a workflow management system to predict resource requirements in scientific data processing, improving efficiency and decision-making.


<details>
  <summary>Details</summary>
Motivation: As scientific experiments grow in scale and complexity, managing data processing workflows requires accurate resource allocation, which remains challenging due to variable scenarios, skill levels, and computing options.

Method: The study presents a novel pipeline of machine learning models integrated into the PanDA workflow management system to predict resource requirements based on limited upfront data.

Result: The machine learning models accurately forecast resource needs, enabling informed workflow decisions and improving the handling of large, diverse workloads.

Conclusion: Integrating advanced machine learning models into workflow management systems can overcome challenges of resource allocation, enhancing the efficiency and scalability of scientific data processing workflows.

Abstract: The collaborative efforts of large communities in science experiments, often
comprising thousands of global members, reflect a monumental commitment to
exploration and discovery. Recently, advanced and complex data processing has
gained increasing importance in science experiments. Data processing workflows
typically consist of multiple intricate steps, and the precise specification of
resource requirements is crucial for each step to allocate optimal resources
for effective processing. Estimating resource requirements in advance is
challenging due to a wide range of analysis scenarios, varying skill levels
among community members, and the continuously increasing spectrum of computing
options. One practical approach to mitigate these challenges involves initially
processing a subset of each step to measure precise resource utilization from
actual processing profiles before completing the entire step. While this
two-staged approach enables processing on optimal resources for most of the
workflow, it has drawbacks such as initial inaccuracies leading to potential
failures and suboptimal resource usage, along with overhead from waiting for
initial processing completion, which is critical for fast-turnaround analyses.
In this context, our study introduces a novel pipeline of machine learning
models within a comprehensive workflow management system, the Production and
Distributed Analysis (PanDA) system. These models employ advanced machine
learning techniques to predict key resource requirements, overcoming challenges
posed by limited upfront knowledge of characteristics at each step. Accurate
forecasts of resource requirements enable informed and proactive
decision-making in workflow management, enhancing the efficiency of handling
diverse, complex workflows across heterogeneous resources.

</details>


### [59] [Towards the Distributed Large-scale k-NN Graph Construction by Graph Merge](https://arxiv.org/abs/2509.11697)
*Cheng Zhang,Wan-Lei Zhao,Shihai Xiao,Jiajie Yao,Xuecang Zhang*

Main category: cs.DC

TL;DR: This paper introduces scalable graph construction methods, Two-way Merge and Multi-way Merge, for building k-NN graphs and indexing graphs in a distributed manner.


<details>
  <summary>Details</summary>
Motivation: To solve the challenge of constructing k-NN or indexing graphs for massive multimedia data that exceeds the capacity of a single machine, ensuring support for real-time interactions and instant search/recommendation.

Method: The paper proposes two parallelizable algorithms, Two-way Merge and Multi-way Merge, for merging subgraphs on a single node. It further extends these methods for distributed graph construction across multiple nodes.

Result: Experiments demonstrate billion-scale k-NN graph construction in approximately 17 hours using three nodes, and indexing graph construction achieving comparable NN search performance while requiring significantly less time.

Conclusion: The presented approach enables efficient, scalable large-scale graph construction, supporting real-time applications and overcoming memory capacity limitations.

Abstract: In order to support the real-time interaction with LLMs and the instant
search or the instant recommendation on social media, it becomes an imminent
problem to build k-NN graph or indexing graph for the massive number of
vectorized multimedia data. In such scenarios, the scale of the data or the
scale of the graph may exceed the processing capacity of a single machine. This
paper aims to address the graph construction problem of such scale via
efficient graph merge. For the graph construction on a single node, two generic
and highly parallelizable algorithms, namely Two-way Merge and Multi-way Merge
are proposed to merge subgraphs into one. For the graph construction across
multiple nodes, a multi-node procedure based on Two-way Merge is presented. The
procedure makes it feasible to construct a large-scale k-NN graph/indexing
graph on either a single node or multiple nodes when the data size exceeds the
memory capacity of one node. Extensive experiments are conducted on both
large-scale k-NN graph and indexing graph construction. For the k-NN graph
construction, the large-scale and high-quality k-NN graphs are constructed by
graph merge in parallel. Typically, a billion-scale k-NN graph can be built in
approximately 17h when only three nodes are employed. For the indexing graph
construction, similar NN search performance as the original indexing graph is
achieved with the merged indexing graphs while requiring much less time of
construction.

</details>


### [60] [A Uniqueness Theorem for Distributed Computation under Physical Constraint](https://arxiv.org/abs/2509.11754)
*Zhiyuan Ren,Mingxuan Lu,Wenchi Cheng*

Main category: cs.DC

TL;DR: This paper introduces a rigorous framework to solve the challenges in In-Network Computing (INC), specifically addressing the trilemma between communication efficiency, bounded memory, and robust scalability. It establishes a unique, optimal computational paradigm and provides proofs regarding its fundamental properties.


<details>
  <summary>Details</summary>
Motivation: To address the acute trilemma in INC environments—communication efficiency, bounded memory, and robust scalability—which current distributed paradigms struggle to handle given their physical hardware constraints.

Method: The authors establish an axiomatic system based on physical constraints and prove the existence of a unique computational paradigm known as Self-Describing Parallel Flows (SDPF), which adheres to these axioms.

Result: The paper demonstrates that SDPF is the only paradigm that satisfies the predefined axioms. It is proven to be convergent, Turing-complete, and minimal, providing a foundational solution analogous to the CAP theorem in distributed computing.

Conclusion: The work reveals that SDPF is an inevitable paradigm for distributed computation flows in high-constraint environments, offering a logical and optimal solution to problems stemming from physical hardware limitations.

Abstract: Foundational models of computation often abstract away physical hardware
limitations. However, in extreme environments like In-Network Computing (INC),
these limitations become inviolable laws, creating an acute trilemma among
communication efficiency, bounded memory, and robust scalability. Prevailing
distributed paradigms, while powerful in their intended domains, were not
designed for this stringent regime and thus face fundamental challenges. This
paper demonstrates that resolving this trilemma requires a shift in perspective
- from seeking engineering trade-offs to deriving solutions from logical
necessity. We establish a rigorous axiomatic system that formalizes these
physical constraints and prove that for the broad class of computations
admitting an idempotent merge operator, there exists a unique, optimal
paradigm. Any system satisfying these axioms must converge to a single normal
form: Self-Describing Parallel Flows (SDPF), a purely data-centric model where
stateless executors process flows that carry their own control logic. We
further prove this unique paradigm is convergent, Turing-complete, and minimal.
In the same way that the CAP theorem established a boundary for what is
impossible in distributed state management, our work provides a constructive
dual: a uniqueness theorem that reveals what is \textit{inevitable} for
distributed computation flows under physical law.

</details>


### [61] [LASLiN: A Learning-Augmented Peer-to-Peer Network](https://arxiv.org/abs/2509.11904)
*Julien Dallot,Caio Caldeira,Arash Pourdamghani,Olga Goussevskaia,Stefan Schmid*

Main category: cs.DC

TL;DR: The paper proposes a learning-augmented peer-to-peer (P2P) network protocol that adapts its topology based on traffic predictions, optimizing communication paths and maintaining robust performance even when predictions are inaccurate.


<details>
  <summary>Details</summary>
Motivation: To design a P2P network that optimally adjusts its topology based on traffic demands while retaining formal guarantees on routing efficiency and robustness, even in the presence of inaccurate predictions.

Method: The authors use dynamic programming to solve demand-aware skip list network (SLN) problems in a centralized setting and also propose the Uniform P2P protocol, a generalization of SLN that incorporates learning-augmented traffic predictions to develop LASLiN.

Result: LASLiN achieves significantly better performance with accurate traffic predictions and maintains polylogarithmic performance guarantees in the worst-case scenario of inaccurate predictions. It also provides enhanced performance for highly sparse demand patterns.

Conclusion: By effectively combining traffic predictions with robust P2P network design, the proposed LASLiN protocol strikes a balance between optimal efficiency and resilience, setting a new standard for demand-aware P2P networks.

Abstract: We introduce a learning-augmented peer-to-peer (P2P) network design that
leverages the predictions of traffic patterns to optimize the network's
topology. While keeping formal guarantees on the standard P2P metrics (routing
path length, maximum degree), we optimize the network in a demand-aware manner
and minimize the path lengths weighted by the peer-to-peer communication
demands. Our protocol is learning-augmented, meaning that each node receives an
individual, possibly inaccurate prediction about the future traffic patterns,
with the goal of improving the network's performances. We strike a trade-off
between significantly improved performances when the predictions are correct
(consistency) and polylogarithmic performances when the predictions are
arbitrary (robustness).
  We have two main contributions. First, we consider the centralized setting
and show that the problem of constructing an optimum static skip list network
(SLN) is solvable in polynomial time and can be computed via dynamic
programming. This problem is the natural demand-aware extension of the optimal
skip list problem.
  Second, we introduce the Uniform P2P protocol which generalizes skip list
networks (SLN) by relaxing the node's heights from discrete to continuous. We
show that Uniform achieves state-of-the-art performances: logarithmic routing
and maximum degree, both with high probability. We then use Uniform to build a
learning-augmented P2P protocol in order to incorporate demand-awareness,
leading to our main contribution, LASLiN. We prove that the performances of
LASLiN are consistent with those of an optimum static SLN with correct
predictions (given via our dynamic programming approach), and are at most a
logarithmic factor off the state-of-the-art P2P protocols if the predictions
are arbitrary wrong. For the special case of highly sparse demands, we show
that LASLiN achieves improved performances.

</details>


### [62] [UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code Translation in HPC](https://arxiv.org/abs/2509.12136)
*Tomer Bitan,Tal Kadosh,Erel Kaplan,Shira Meiri,Le Chen,Peter Morales,Niranjan Hasabnis,Gal Oren*

Main category: cs.DC

TL;DR: This paper introduces UniPar, a framework to evaluate the use of large language models (LLMs) for translating parallel programming code between CUDA, OpenMP, and serial code, showing that enhanced methods can significantly improve performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem of translating code between different parallel programming languages, an essential challenge in high-performance computing. Existing tools are outdated and limited, while the rise of LLMs provides a new potential avenue for effective translations.

Method: The authors developed UniPar, evaluating LLMs like GPT-4o-mini and LLaMA-3.3-70B-Instruct using strategies such as hyperparameter optimization, prompting techniques, supervised fine-tuning, and compiler-based iterative feedback. They also introduced a dataset, PARATRANS, for testing cross-paradigm and serial-to-parallel translations.

Result: The study found that off-the-shelf LLMs perform poorly in translating parallel code, achieving low functional correctness (15%). However, the UniPar methodology improved these outcomes significantly, doubling compilation rates (69%) and correctness (33%).

Conclusion: UniPar demonstrates that combining fine-tuning, hyperparameter optimization, and compiler-guided repair can notably enhance LLM performance in parallel code translation, offering a promising path for future improvements in this domain.

Abstract: Translating programs between various parallel programming languages is an
important problem in the high-performance computing (HPC) community. Existing
tools for this problem are either too narrow in scope and/or outdated. Recent
explosive growth in the popularity of large language models (LLMs) and their
ability to generate and translate code offers a potential alternative approach.
Toward that end, we first need to systematically evaluate the ability of LLMs
to translate between parallel languages.
  In this work, we introduce UniPar, a systematic evaluation framework for
LLM-based parallel code translation. Specifically, in this work, we target
translations between serial code, CUDA, and OpenMP. Our goal is to assess how
well current instruction-tuned LLMs -- specifically GPT-4o-mini and
LLaMA-3.3-70B-Instruct -- can be used out of the box or enhanced through known
strategies. We evaluated four major usage modes: hyperparameter optimization
for decoding, zero- and few-shot prompting, supervised fine-tuning, and
iterative feedback through compiler-based repair. As a part of the evaluation,
we construct a new dataset called PARATRANS, covering both serial-to-parallel
translation and cross-paradigm transformations.
  Our findings reveal that while off-the-shelf models struggle under the
default settings (e.g., GPT-4o-mini achieves only 46% compilation and 15%
functional correctness), our UniPar methodology -- combining fine-tuning,
hyperparameter tuning, and compiler-guided repair -- improves performance by up
to 2X (69% compilation and 33% correctness). We believe that our findings will
provide useful insights for researchers to further improve LLMs for the
parallel language translation problem.
  UniPar source code and PARATRANS dataset are available at our GitHub
repository https://github.com/Scientific-Computing-Lab/UniPar_AI.

</details>


### [63] [Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization](https://arxiv.org/abs/2509.12138)
*Mengjiao Han,Andres Sewell,Joseph Insley,Janet Knowles,Victor A. Mateevitsi,Michael E. Papka,Steve Petruzza,Silvio Rizzi*

Main category: cs.DC

TL;DR: The paper proposes a distributed 3D Gaussian Splatting (3D-GS) pipeline for scalable scientific visualization on high-performance computing (HPC) systems, achieving significant speedup while maintaining image quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing 3D Gaussian Splatting approaches for scientific visualization, which are restricted to single-GPU systems and cannot efficiently handle large datasets on HPC systems.

Method: The authors designed a distributed 3D-GS pipeline that partitions data across multiple nodes, enables parallel training of Gaussian splats on multi-nodes and multi-GPUs, and combines the results for global rendering. They also introduce ghost cells to handle partition boundaries and background masks to filter irrelevant pixels.

Result: The proposed approach achieved up to a 3X speedup using 8 nodes on an HPC system (Polaris) with the Richtmyer-Meshkov dataset, which contains approximately 106.7 million Gaussians, while preserving image quality.

Conclusion: Distributed 3D-GS facilitates scalable visualization of large-scale scientific datasets, demonstrating its potential for future in situ applications.

Abstract: 3D Gaussian Splatting (3D-GS) has recently emerged as a powerful technique
for real-time, photorealistic rendering by optimizing anisotropic Gaussian
primitives from view-dependent images. While 3D-GS has been extended to
scientific visualization, prior work remains limited to single-GPU settings,
restricting scalability for large datasets on high-performance computing (HPC)
systems. We present a distributed 3D-GS pipeline tailored for HPC. Our approach
partitions data across nodes, trains Gaussian splats in parallel using
multi-nodes and multi-GPUs, and merges splats for global rendering. To
eliminate artifacts, we add ghost cells at partition boundaries and apply
background masks to remove irrelevant pixels. Benchmarks on the
Richtmyer-Meshkov datasets (about 106.7M Gaussians) show up to 3X speedup
across 8 nodes on Polaris while preserving image quality. These results
demonstrate that distributed 3D-GS enables scalable visualization of
large-scale scientific data and provide a foundation for future in situ
applications.

</details>


### [64] [When MoE Meets Blockchain: A Trustworthy Distributed Framework of Large Models](https://arxiv.org/abs/2509.12141)
*Weihao Zhu,Long Shi,Kang Wei,Zhen Mei,Zhe Wang,Jiaheng Wang,Jun Li*

Main category: cs.DC

TL;DR: The paper introduces a blockchain-aided Mixture of Experts framework (B-MoE) to improve trustworthiness and security of distributed MoE architectures on edge networks.


<details>
  <summary>Details</summary>
Motivation: Traditional cloud-based Mixture of Experts (MoE) architectures face issues like high latency, bandwidth usage, and data privacy concerns when deployed on edge networks.

Method: The B-MoE framework integrates three layers: edge, blockchain, and storage. The edge layer processes tasks using distributed experts, the blockchain layer ensures decentralized trust and verification, and the storage layer holds the necessary models.

Result: Experimental outcomes showed that the B-MoE framework is more resistant to data manipulation attacks during both training and inference compared to conventional distributed MoE setups.

Conclusion: Using blockchain, the B-MoE framework enhances the security and robustness of distributed MoE architectures, addressing critical trust issues in edge network scenarios.

Abstract: As an enabling architecture of Large Models (LMs), Mixture of Experts (MoE)
has become prevalent thanks to its sparsely-gated mechanism, which lowers
computational overhead while maintaining learning performance comparable to
dense LMs. The essence of MoE lies in utilizing a group of neural networks
(called experts) with each specializing in different types of tasks, along with
a trainable gating network that selectively activates a subset of these experts
to handle specific tasks. Traditional cloud-based MoE encounters challenges
such as prolonged response latency, high bandwidth consumption, and data
privacy leakage. To address these issues, researchers have proposed to deploy
MoE over distributed edge networks. However, a key concern of distributed MoE
frameworks is the lack of trust in data interactions among distributed experts
without the surveillance of any trusted authority, and thereby prone to
potential attacks such as data manipulation. In response to the security issues
of traditional distributed MoE, we propose a blockchain-aided trustworthy MoE
(B-MoE) framework that consists of three layers: the edge layer, the blockchain
layer, and the storage layer. In this framework, the edge layer employs the
activated experts downloaded from the storage layer to process the learning
tasks, while the blockchain layer functions as a decentralized trustworthy
network to trace, verify, and record the computational results of the experts
from the edge layer. The experimental results demonstrate that B-MoE is more
robust to data manipulation attacks than traditional distributed MoE during
both the training and inference processes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [65] [The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results](https://arxiv.org/abs/2509.10463)
*Qiuyu Chen,Xin Jin,Yue Song,Xihui Liu,Shuai Yang,Tao Yang,Ziqiang Li,Jianguo Huang,Yuntao Wei,Ba'ao Xie,Nicu Sebe,Wenjun,Zeng,Jooyeol Yun,Davide Abati,Mohamed Omran,Jaegul Choo,Amir Habibian,Auke Wiggers,Masato Kobayashi,Ning Ding,Toru Tamaki,Marzieh Gheisari,Auguste Genovesio,Yuheng Chen,Dingkun Liu,Xinyao Yang,Xinping Xu,Baicheng Chen,Dongrui Wu,Junhao Geng,Lexiang Lv,Jianxin Lin,Hanzhe Liang,Jie Zhou,Xuanxin Chen,Jinbao Wang,Can Gao,Zhangyi Wang,Zongze Li,Bihan Wen,Yixin Gao,Xiaohan Pan,Xin Li,Zhibo Chen,Baorui Peng,Zhongming Chen,Haoran Jin*

Main category: cs.LG

TL;DR: The paper summarizes the focus and outcomes of the 1st International Workshop on Disentangled Representation Learning in practical applications, featuring 9 papers exploring advancements and methodologies in DRL.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between theoretical promises of Disentangled Representation Learning (DRL) and its use in real-world applications beyond synthetic benchmarks.

Method: Reviewed methodologies integrating novel inductive biases, exploration of diffusion models, and specialized applications like 3D-aware disentanglement and autonomous driving.

Result: Nine papers were presented, offering insights into practical applications of DRL across diverse domains and examining robustness, interpretability, and generalization.

Conclusion: The workshop highlighted the progress and challenges in transitioning DRL from theoretical frameworks to impactful real-world applications.

Abstract: This paper reviews the 1st International Workshop on Disentangled
Representation Learning for Controllable Generation (DRL4Real), held in
conjunction with ICCV 2025. The workshop aimed to bridge the gap between the
theoretical promise of Disentangled Representation Learning (DRL) and its
application in realistic scenarios, moving beyond synthetic benchmarks.
DRL4Real focused on evaluating DRL methods in practical applications such as
controllable generation, exploring advancements in model robustness,
interpretability, and generalization. The workshop accepted 9 papers covering a
broad range of topics, including the integration of novel inductive biases
(e.g., language), the application of diffusion models to DRL, 3D-aware
disentanglement, and the expansion of DRL into specialized domains like
autonomous driving and EEG analysis. This summary details the workshop's
objectives, the themes of the accepted papers, and provides an overview of the
methodologies proposed by the authors.

</details>


### [66] [Moment Estimates and DeepRitz Methods on Learning Diffusion Systems with Non-gradient Drifts](https://arxiv.org/abs/2509.10495)
*Fanze Kong,Chen-Chih Lai,Yubin Lu*

Main category: cs.LG

TL;DR: The paper introduces the Moment-DeepRitz Method to learn drift decompositions in complex diffusion systems, emphasizing robustness to noisy data and adaptability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling conservative-dissipative dynamics in generalized diffusion systems from noisy and complex data.

Method: The Moment-DeepRitz Method splits learning into two phases, combining moment dynamics modeling with the DeepRitz framework for drift decomposition.

Result: Through numerical experiments, the method successfully handles rough potentials and oscillatory rotations while being robust to noise.

Conclusion: The Moment-DeepRitz Method is a reliable and adaptable solution for analyzing drift decompositions in systems with conservative-dissipative dynamics.

Abstract: Conservative-dissipative dynamics are ubiquitous across a variety of complex
open systems. We propose a data-driven two-phase method, the Moment-DeepRitz
Method, for learning drift decompositions in generalized diffusion systems
involving conservative-dissipative dynamics. The method is robust to noisy
data, adaptable to rough potentials and oscillatory rotations. We demonstrate
its effectiveness through several numerical experiments.

</details>


### [67] [SOH-KLSTM: A Hybrid Kolmogorov-Arnold Network and LSTM Model for Enhanced Lithium-Ion Battery Health Monitoring](https://arxiv.org/abs/2509.10496)
*Imen Jarraya,Safa Ben Atitallah,Fatimah Alahmeda,Mohamed Abdelkadera,Maha Drissa,Fatma Abdelhadic,Anis Koubaaa*

Main category: cs.LG

TL;DR: The paper proposes a hybrid SOH prediction framework (SOH-KLSTM) combining KAN and LSTM to improve Lithium battery health monitoring by addressing non-linear and temporal degradation challenges.


<details>
  <summary>Details</summary>
Motivation: To enhance SOH estimation techniques for Lithium batteries, addressing limitations in conventional methods that fail to effectively represent non-linear and temporal degradation aspects.

Method: The study introduces SOH-KLSTM, a hybrid framework leveraging Kolmogorov-Arnold Network (KAN) for non-linear approximations and Long Short-Term Memory (LSTM) for time series predictions.

Result: The proposed approach effectively captures complex degradation behaviors in Lithium batteries.

Conclusion: SOH-KLSTM demonstrates the potential to improve the accuracy and reliability of Lithium battery health monitoring by integrating KAN and LSTM strengths.

Abstract: Accurate and reliable State Of Health (SOH) estimation for Lithium (Li)
batteries is critical to ensure the longevity, safety, and optimal performance
of applications like electric vehicles, unmanned aerial vehicles, consumer
electronics, and renewable energy storage systems. Conventional SOH estimation
techniques fail to represent the non-linear and temporal aspects of battery
degradation effectively. In this study, we propose a novel SOH prediction
framework (SOH-KLSTM) using Kolmogorov-Arnold Network (KAN)-Integrated
Candidate Cell State in LSTM for Li batteries Health Monitoring. This hybrid
approach combines the ability of LSTM to learn long-term dependencies for
accurate time series predictions with KAN's non-linear approximation
capabilities to effectively capture complex degradation behaviors in Lithium
batteries.

</details>


### [68] [Verifying Computational Graphs in Production-Grade Distributed Machine Learning Frameworks](https://arxiv.org/abs/2509.10694)
*Kahfi S. Zulkifli,Wenbo Qian,Shaowei Zhu,Yuan Zhou,Zhen Zhang,Chang Lou*

Main category: cs.LG

TL;DR: Scalify is a lightweight framework that checks the semantic equivalence of large machine learning models, using advanced techniques to detect silent errors and improve debugging.


<details>
  <summary>Details</summary>
Motivation: Modern machine learning techniques often introduce silent errors in very large models due to parallelism and optimization, and existing solutions to address this are either ineffective or impractical.

Method: Scalify uses equality saturation and Datalog-style reasoning to verify computational graphs, scales through graph partitioning and parallel rewriting, and converts debugging results into actionable insights.

Result: The framework successfully verified massive models like Llama-3.1-405B within minutes and identified five unknown bugs in production frameworks at Amazon.

Conclusion: Scalify provides an effective, scalable solution for debugging and verifying large machine learning models, enabling faster and more reliable production readiness.

Abstract: Modern machine learning frameworks support very large models by incorporating
parallelism and optimization techniques. Yet, these very techniques add new
layers of complexity, introducing silent errors that severely degrade model
performance. Existing solutions are either ad hoc or too costly for production.
  We present Scalify, a lightweight framework that exposes silent errors by
verifying semantic equivalence of computational graphs using equality
saturation and Datalog-style reasoning. To scale, Scalify partitions graphs
with parallel rewriting and layer memoization, reuses rewrite templates, and
augments equality saturation with relational reasoning and symbolic bijection
inference. It further localizes discrepancies to precise code sites, turning
verification results into actionable debugging guidance. Scalify verifies
models as large as Llama-3.1-405B within minutes on a commodity machine and
exposed five unknown bugs in Amazon production machine learning frameworks.

</details>


### [69] [Exploring Multi-view Symbolic Regression methods in physical sciences](https://arxiv.org/abs/2509.10500)
*Etienne Russeil,Fabrício Olivetti de França,Konstantin Malanchev,Guillaume Moinard,Maxime Cherrey*

Main category: cs.LG

TL;DR: This paper evaluates and compares Multi-view Symbolic Regression (MvSR) implementations in Operon, PySR, phy-SO, and eggp across real-world datasets, highlighting accuracy and sparseness while noting features leading to better models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to extend the understanding of MvSR approaches, which mitigate data scarcity and overfitting by finding parametric functions for multiple datasets, analyzing their efficacy for symbolic regression.

Method: The study tests and compares MvSR implementations across different tools, utilizing real-world datasets to evaluate their accuracy, simplicity (fewer free parameters), and influences of their features.

Result: All tested implementations (Operon, PySR, phy-SO, and eggp) perform well with good accuracy and sparse (interpretable) solutions. Certain features are identified that generate better models more frequently.

Conclusion: The paper provides actionable guidelines for improving MvSR developments based on observed behaviors and comparative findings across implementations.

Abstract: Describing the world behavior through mathematical functions help scientists
to achieve a better understanding of the inner mechanisms of different
phenomena. Traditionally, this is done by deriving new equations from first
principles and careful observations. A modern alternative is to automate part
of this process with symbolic regression (SR). The SR algorithms search for a
function that adequately fits the observed data while trying to enforce
sparsity, in the hopes of generating an interpretable equation. A particularly
interesting extension to these algorithms is the Multi-view Symbolic Regression
(MvSR). It searches for a parametric function capable of describing multiple
datasets generated by the same phenomena, which helps to mitigate the common
problems of overfitting and data scarcity. Recently, multiple implementations
added support to MvSR with small differences between them. In this paper, we
test and compare MvSR as supported in Operon, PySR, phy-SO, and eggp, in
different real-world datasets. We show that they all often achieve good
accuracy while proposing solutions with only few free parameters. However, we
find that certain features enable a more frequent generation of better models.
We conclude by providing guidelines for future MvSR developments.

</details>


### [70] [From Noise to Precision: A Diffusion-Driven Approach to Zero-Inflated Precipitation Prediction](https://arxiv.org/abs/2509.10501)
*Wentao Gao,Jiuyong Li,Lin Liu,Thuc Duy Le,Xiongren Chen,Xiaojing Du,Jixue Liu,Yanchang Zhao,Yun Chen*

Main category: cs.LG

TL;DR: The paper introduces the Zero Inflation Diffusion Framework (ZIDF) to tackle zero-inflated data challenges in precipitation forecasting.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in precipitation forecasting caused by datasets with a high frequency of zero values and sparse non-zero occurrences.

Method: The proposed Zero Inflation Diffusion Framework (ZIDF) combines Gaussian perturbation for data smoothing, Transformer models for understanding temporal patterns, and diffusion mechanisms for restoring original data structures.

Result: The ZIDF outperforms existing models, reducing Mean Squared Error (MSE) by up to 56.7% and Mean Absolute Error (MAE) by 21.1% compared to a baseline model.

Conclusion: ZIDF demonstrates robustness in handling sparse data and shows potential applicability to similar domains facing zero-inflated data challenges.

Abstract: Zero-inflated data pose significant challenges in precipitation forecasting
due to the predominance of zeros with sparse non-zero events. To address this,
we propose the Zero Inflation Diffusion Framework (ZIDF), which integrates
Gaussian perturbation for smoothing zero-inflated distributions,
Transformer-based prediction for capturing temporal patterns, and
diffusion-based denoising to restore the original data structure. In our
experiments, we use observational precipitation data collected from South
Australia along with synthetically generated zero-inflated data. Results show
that ZIDF demonstrates significant performance improvements over multiple
state-of-the-art precipitation forecasting models, achieving up to 56.7\%
reduction in MSE and 21.1\% reduction in MAE relative to the baseline
Non-stationary Transformer. These findings highlight ZIDF's ability to robustly
handle sparse time series data and suggest its potential generalizability to
other domains where zero inflation is a key challenge.

</details>


### [71] [FEDEXCHANGE: Bridging the Domain Gap in Federated Object Detection for Free](https://arxiv.org/abs/2509.10503)
*Haolin Yuan,Jingtao Li,Weiming Zhuang,Chen Chen,Lingjuan Lyu*

Main category: cs.LG

TL;DR: FEDEXCHANGE is a new framework for Federated Object Detection (FOD) that improves cross-domain performance using server-side model exchange, addressing domain gaps without extra computational cost for clients.


<details>
  <summary>Details</summary>
Motivation: The paper addresses low performance in Federated Object Detection caused by domain variations while ensuring compatibility with hardware limitations of edge devices.

Method: FEDEXCHANGE introduces a server-side dynamic model exchange strategy alternating between model aggregation and exchange. Local models cluster and learn from diverse domains without direct data sharing.

Result: FEDEXCHANGE achieves 1.6X better mean average precision in challenging conditions (e.g., rain) while reducing computational resource use to 0.8X compared to baseline methods.

Conclusion: The proposed framework improves cross-domain generalization in FOD without additional client-side computational burdens, demonstrating better efficiency and performance.

Abstract: Federated Object Detection (FOD) enables clients to collaboratively train a
global object detection model without accessing their local data from diverse
domains. However, significant variations in environment, weather, and other
domain specific factors hinder performance, making cross domain generalization
a key challenge. Existing FOD methods often overlook the hardware constraints
of edge devices and introduce local training regularizations that incur high
computational costs, limiting real-world applicability. In this paper, we
propose FEDEXCHANGE, a novel FOD framework that bridges domain gaps without
introducing additional local computational overhead. FEDEXCHANGE employs a
server side dynamic model exchange strategy that enables each client to gain
insights from other clients' domain data without direct data sharing.
Specifically, FEDEXCHANGE allows the server to alternate between model
aggregation and model exchange. During aggregation rounds, the server
aggregates all local models as usual. In exchange rounds, FEDEXCHANGE clusters
and exchanges local models based on distance measures, allowing local models to
learn from a variety of domains. As all operations are performed on the server
side, clients can achieve improved cross domain utility without any additional
computational overhead. Extensive evaluations demonstrate that FEDEXCHANGE
enhances FOD performance, achieving 1.6X better mean average precision in
challenging domains, such as rainy conditions, while requiring only 0.8X the
computational resources compared to baseline methods.

</details>


### [72] [Retrosynthesis Planning via Worst-path Policy Optimisation in Tree-structured MDPs](https://arxiv.org/abs/2509.10504)
*Mianchu Wang,Giovanni Montana*

Main category: cs.LG

TL;DR: The paper presents Interactive Retrosynthesis Planning (InterRetro), a new method designed to optimize retrosynthesis planning by focusing on the worst-case sensitivity of synthetic routes.


<details>
  <summary>Details</summary>
Motivation: Existing retrosynthesis methods fail to account for the vulnerability of synthesis trees to weak links, as they optimize for average performance instead of focusing on the worst-case scenarios.

Method: The authors reformulate retrosynthesis planning as a worst-path optimization problem using tree-structured Markov Decision Processes (MDPs), introducing InterRetro to enhance outcomes through self-imitation and interaction with the tree MDP.

Result: InterRetro achieves state-of-the-art results by solving 100% of targets on the Retro*-190 benchmark, reducing synthetic route length by 4.9%, and achieving strong performance with minimal training data.

Conclusion: This work represents a significant advancement in computational retrosynthesis planning, making synthetic routes more robust and efficient by addressing worst-path sensitivities.

Abstract: Retrosynthesis planning aims to decompose target molecules into available
building blocks, forming a synthesis tree where each internal node represents
an intermediate compound and each leaf ideally corresponds to a purchasable
reactant. However, this tree becomes invalid if any leaf node is not a valid
building block, making the planning process vulnerable to the "weakest link" in
the synthetic route. Existing methods often optimise for average performance
across branches, failing to account for this worst-case sensitivity. In this
paper, we reframe retrosynthesis as a worst-path optimisation problem within
tree-structured Markov Decision Processes (MDPs). We prove that this
formulation admits a unique optimal solution and offers monotonic improvement
guarantees. Building on this insight, we introduce Interactive Retrosynthesis
Planning (InterRetro), a method that interacts with the tree MDP, learns a
value function for worst-path outcomes, and improves its policy through
self-imitation, preferentially reinforcing past decisions with high estimated
advantage. Empirically, InterRetro achieves state-of-the-art results, solving
100% of targets on the Retro*-190 benchmark, shortening synthetic routes by
4.9%, and achieving promising performance using only 10% of the training data -
representing a significant advance in computational retrosynthesis planning.

</details>


### [73] [AttnBoost: Retail Supply Chain Sales Insights via Gradient Boosting Perspective](https://arxiv.org/abs/2509.10506)
*Muxin Ge,Hanyu Ma,Yiyang Wu,Xiaoli Ma,Yadi Liu,Ye Aung Moe,Weizheng Xie*

Main category: cs.LG

TL;DR: The paper introduces AttnBoost, a gradient boosting model enhanced with attention mechanisms to tackle product demand forecasting challenges in retail.


<details>
  <summary>Details</summary>
Motivation: Forecasting product demand in retail is difficult due to noisy data, rapidly changing consumer behavior, and the need for adaptive mechanisms in predictive models.

Method: The authors enhance gradient boosting decision trees (GBDT) by incorporating feature-level attention, enabling dynamic adjustment of feature importance during the boosting rounds.

Result: AttnBoost outperforms standard machine learning and deep learning models on retail sales data by improving predictive accuracy and interpretability. Ablation studies highlight its ability to reduce overfitting and provide actionable insights.

Conclusion: Attention-guided boosting, as demonstrated by AttnBoost, offers a scalable and interpretable solution for complex forecasting challenges in retail supply chains.

Abstract: Forecasting product demand in retail supply chains presents a complex
challenge due to noisy, heterogeneous features and rapidly shifting consumer
behavior. While traditional gradient boosting decision trees (GBDT) offer
strong predictive performance on structured data, they often lack adaptive
mechanisms to identify and emphasize the most relevant features under changing
conditions. In this work, we propose AttnBoost, an interpretable learning
framework that integrates feature-level attention into the boosting process to
enhance both predictive accuracy and explainability. Specifically, the model
dynamically adjusts feature importance during each boosting round via a
lightweight attention mechanism, allowing it to focus on high-impact variables
such as promotions, pricing, and seasonal trends. We evaluate AttnBoost on a
large-scale retail sales dataset and demonstrate that it outperforms standard
machine learning and deep tabular models, while also providing actionable
insights for supply chain managers. An ablation study confirms the utility of
the attention module in mitigating overfitting and improving interpretability.
Our results suggest that attention-guided boosting represents a promising
direction for interpretable and scalable AI in real-world forecasting
applications.

</details>


### [74] [The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback](https://arxiv.org/abs/2509.10509)
*Sai Teja Reddy Adapala*

Main category: cs.LG

TL;DR: The study explores model collapse in large language models and introduces the "Anti-Ouroboros Effect," showing that selective feedback can improve model performance instead of degrading it.


<details>
  <summary>Details</summary>
Motivation: The paper investigates model collapse, where recursively trained LLMs degrade, and aims to find a mechanism to prevent or reverse this issue for AI safety.

Method: The researchers applied a selective feedback mechanism during recursive training of a Gemma 2B model on summarization tasks, contrasting results with experiments on simpler models.

Result: The Gemma 2B model showed a performance improvement of 6.6% in ROUGE-L F1 under quality-filtered conditions, whereas controls experienced performance degradation.

Conclusion: Selective feedback can induce systemic resilience in LLMs, suggesting a scalable strategy for developing safer and more robust AI systems.

Abstract: The stability of recursively trained large language models (LLMs) is a
foundational problem for AI safety. Prevailing theory predicts model collapse,
a progressive degradation when models are trained on their own output. We
challenge this narrative by introducing a selective feedback mechanism.
Contrary to expectation, instead of merely slowing decay, our experiments
provide strong evidence that this pressure reverses it, inducing a
statistically significant performance improvement in a Gemma 2B model on a
complex summarization task. We name this phenomenon the Anti-Ouroboros Effect.
We contrast this with a foundational experiment using a simple classifier,
where the theoretical degenerative loop was validated, highlighting the unique
dynamics of high-dimensional models. Our findings establish that systemic
resilience can be an emergent property of LLMs under simple selection pressure,
suggesting a powerful and scalable principle for developing safer and more
robust AI systems. Across five generations, a quality-filtered condition
improved by 6.6% in ROUGE-L F1 score, whereas an unfiltered control degraded by
3.5% and a random-filter control degraded by 4.2%

</details>


### [75] [LogGuardQ: A Cognitive-Enhanced Reinforcement Learning Framework for Cybersecurity Anomaly Detection in Security Logs](https://arxiv.org/abs/2509.10511)
*Umberto Gonçalves de Sousa*

Main category: cs.LG

TL;DR: LogGuardQ integrates human-inspired cognitive mechanics and adaptive exploration into RL, significantly outperforming DQN and PPO in anomaly detection tasks.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning struggles with exploration, stability, and adaptability in dynamic settings, hindering efficient decision-making in real-world applications.

Method: Introduces LogGuardQ, combining a dual-memory cognitive framework, adaptive exploration via temperature decay, and curiosity-driven strategies, tested on simulated access logs.

Result: LogGuardQ achieves a 96.0% anomaly detection rate (versus DQN's 93.0% and PPO's 47.1%), higher precision and recall, improved rewards, and better stability confirmed by statistical tests.

Conclusion: LogGuardQ effectively bridges cognitive science and RL, providing a scalable and adaptive system with strong potential for cybersecurity and uncertainty-based decision-making.

Abstract: Reinforcement learning (RL) has transformed sequential decision-making, but
traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy
Optimization (PPO) often struggle with efficient exploration, stability, and
adaptability in dynamic environments. This study presents LogGuardQ (Adaptive
Log Guard with Cognitive enhancement), a novel framework that integrates a
dual-memory system inspired by human cognition and adaptive exploration
strategies driven by temperature decay and curiosity. Evaluated on a dataset of
1,000,000 simulated access logs with 47.9% anomalies over 20,000 episodes,
LogGuardQ achieves a 96.0% detection rate (versus 93.0% for DQN and 47.1% for
PPO), with precision of 0.4776, recall of 0.9996, and an F1-score of 0.6450.
The mean reward is 20.34 \pm 44.63 across all episodes (versus 18.80 \pm 43.98
for DQN and -0.17 \pm 23.79 for PPO), with an average of 5.0 steps per episode
(constant across models). Graphical analyses, including learning curves
smoothed with a Savgol filter (window=501, polynomial=2), variance trends,
action distributions, and cumulative detections, demonstrate LogGuardQ's
superior stability and efficiency. Statistical tests (Mann-Whitney U) confirm
significant performance advantages (e.g., p = 0.0002 vs. DQN with negligible
effect size, p < 0.0001 vs. PPO with medium effect size, and p < 0.0001 for DQN
vs. PPO with small effect size). By bridging cognitive science and RL,
LogGuardQ offers a scalable approach to adaptive learning in uncertain
environments, with potential applications in cybersecurity, intrusion
detection, and decision-making under uncertainty.

</details>


### [76] [A Service-Oriented Adaptive Hierarchical Incentive Mechanism for Federated Learning](https://arxiv.org/abs/2509.10512)
*Jiaxing Cao,Yuzhou Gao,Jiwei Huang*

Main category: cs.LG

TL;DR: The paper proposes an adaptive incentive mechanism for federated learning to balance utilities among task publishers (TP), local model owners (LMOs), and workers by employing game theory and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: FL often lacks sufficient training data, necessitating a mechanism that motivates participation and ensures fairness among all stakeholders in the training process.

Method: The authors propose a Stackelberg game model between TPs and LMOs to maximize utilities, while interactions between LMOs and workers are modeled as a multi-agent Markov decision process. Deep reinforcement learning helps identify optimal strategies, and a new algorithm ASOSA stabilizes the participants' strategies.

Result: The proposed method demonstrates efficacy through extensive numerical experiments, showcasing its ability to balance and optimize utilities among participants.

Conclusion: The incentive mechanism successfully aligns the objectives of FL's stakeholders, providing a structured approach to optimize and stabilize their interactions in a data-restrained environment.

Abstract: Recently, federated learning (FL) has emerged as a novel framework for
distributed model training. In FL, the task publisher (TP) releases tasks, and
local model owners (LMOs) use their local data to train models. Sometimes, FL
suffers from the lack of training data, and thus workers are recruited for
gathering data. To this end, this paper proposes an adaptive incentive
mechanism from a service-oriented perspective, with the objective of maximizing
the utilities of TP, LMOs and workers. Specifically, a Stackelberg game is
theoretically established between the LMOs and TP, positioning TP as the leader
and the LMOs as followers. An analytical Nash equilibrium solution is derived
to maximize their utilities. The interaction between LMOs and workers is
formulated by a multi-agent Markov decision process (MAMDP), with the optimal
strategy identified via deep reinforcement learning (DRL). Additionally, an
Adaptively Searching the Optimal Strategy Algorithm (ASOSA) is designed to
stabilize the strategies of each participant and solve the coupling problems.
Extensive numerical experiments are conducted to validate the efficacy of the
proposed method.

</details>


### [77] [Mixture-of-Clustered-Experts: Advancing Expert Specialization and Generalization in Instruction Tuning](https://arxiv.org/abs/2509.10513)
*Sugyeong Eo,Jungjun Lee,Chanjun Park,Heuiseok Lim*

Main category: cs.LG

TL;DR: The paper introduces Mixture-of-Clustered-Experts (MoCE), a dual-stage routing mechanism that improves over traditional Mixture-of-Experts by enhancing expert specialization for instruction tuning scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of improving expert specialization and generalization in Mixture-of-Experts (MoE), especially in instruction-tuning scenarios with heterogeneous inputs.

Method: Proposed a dual-stage routing mechanism in MoCE where sequence-level features are used for expert group routing, followed by token-level activation of top-k experts within the group.

Result: Demonstrates superior performance and generalization capabilities of MoCE over strong baselines through comprehensive benchmarks.

Conclusion: MoCE effectively partitions heterogeneous inputs and achieves better expert group specialization while maintaining scalability and robustness of traditional MoE models.

Abstract: A sparse Mixture-of-Experts (MoE) architecture has emerged as a highly
scalable solution by conditionally activating sub-modules without a
proportional increase in computational costs. However, improving expert
specialization to enhance performance and generalization remains a challenge
for MoE, especially in instruction tuning scenarios characterized by
significant input heterogeneity. In this work, we propose the
Mixture-of-Clustered-Experts (MoCE) to address this limitation through a
dual-stage routing mechanism. The first stage in the mechanism performs expert
group routing based on sequence-level features, while the second stage
activates the top-$k$ experts within the group at the token level. This
approach enables the effective partitioning of heterogeneous inputs based on
their knowledge requirements, encouraging expert group specialization while
maintaining the advantages of token-level routing. We evaluate MoCE across a
comprehensive set of benchmarks, demonstrating its consistent superiority over
strong baselines and its enhanced generalization capabilities. Detailed
analysis further highlights the robustness and effectiveness of MoCE.

</details>


### [78] [A Differential Manifold Perspective and Universality Analysis of Continuous Attractors in Artificial Neural Networks](https://arxiv.org/abs/2509.10514)
*Shaoxin Tian,Hongkai Liu,Yuying Yang,Jiali Yu,Zizheng Miao,Xuming Huang,Zhishuai Liu,Zhang Yi*

Main category: cs.LG

TL;DR: The paper introduces a novel framework using differential manifolds to study continuous attractors in neural systems, aiming for cross-architectural generalizability.


<details>
  <summary>Details</summary>
Motivation: Existing studies lack a unified framework for analyzing continuous attractors across diverse dynamical systems, limiting applicability to biological and artificial neural systems.

Method: The paper applies differential manifold theory to analyze continuous attractors, linking them to local Jacobian matrix eigenvalues and exploring singular value stratification in classification models.

Result: Compatibility with prior work is verified, along with universal applicability of singular value stratification in neural networks and datasets.

Conclusion: Continuous attractors likely occur ubiquitously in neural networks, necessitating a general theory to unify observations; the proposed framework serves as a strong mathematical foundation.

Abstract: Continuous attractors are critical for information processing in both
biological and artificial neural systems, with implications for spatial
navigation, memory, and deep learning optimization. However, existing research
lacks a unified framework to analyze their properties across diverse dynamical
systems, limiting cross-architectural generalizability. This study establishes
a novel framework from the perspective of differential manifolds to investigate
continuous attractors in artificial neural networks. It verifies compatibility
with prior conclusions, elucidates links between continuous attractor phenomena
and eigenvalues of the local Jacobian matrix, and demonstrates the universality
of singular value stratification in common classification models and datasets.
These findings suggest continuous attractors may be ubiquitous in general
neural networks, highlighting the need for a general theory, with the proposed
framework offering a promising foundation given the close mathematical
connection between eigenvalues and singular values.

</details>


### [79] [Adaptive Preference Optimization with Uncertainty-aware Utility Anchor](https://arxiv.org/abs/2509.10515)
*Xiaobo Wang,Zixia Jia,Jiaqi Li,Qi Liu,Zilong Zheng*

Main category: cs.LG

TL;DR: This paper introduces UAPO, a framework for offline preference optimization in LLMs, overcoming limitations of traditional Bradley-Terry-based methods and enabling robust training with unpaired data.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing Bradley-Terry reward modeling in LLM alignment, such as reliance on paired data and assumptions about human rationality.

Method: Introduce UAPO, a framework using an anchoring function to estimate uncertainty from preference data annotations, allowing training with unpaired data and enhancing robustness.

Result: Experimental results show UAPO achieves competitive outcomes, utilizing data more efficiently without strict pairing constraints.

Conclusion: UAPO provides a flexible, efficient, and robust approach to offline preference optimization in aligning LLMs.

Abstract: Offline preference optimization methods are efficient for large language
models (LLMs) alignment. Direct Preference optimization (DPO)-like learning,
one of the most popular approaches, stands out for its efficiency in reward
modeling. However, these methods typically follow the convention to use
Bradley-Terry (BT) reward modeling that faces several critical assumptions,
including the requirement for pairwise training data, model distribution
shifting, human rationality assumption, etc. To address these limitations, we
propose a general framework for offline preference optimization methods,
Adaptive Preference Optimization with Utility Anchor (UAPO), which introduces
an anchoring function to estimate the uncertainties brought from preference
data annotation. Our method enables training even in scenarios where the data
is unpaired, significantly enhancing data utilization efficiency. Moreover, the
anchor design makes UAPO more robust in the training process. Experimental
results demonstrate that UAPO achieves competitive outcomes without the strict
dependency on data pairing, paving the way for more flexible and effective
preference optimization methods.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [80] [Predator-Prey Model: Driven Hunt for Accelerated Grokking](https://arxiv.org/abs/2509.10562)
*I. A. Lopatin,S. V. Kozyrev,A. N. Pechen*

Main category: cs.NE

TL;DR: A machine learning optimization method mimics predator-prey dynamics to enhance learning efficiency, particularly avoiding pitfalls in ravine-like landscapes.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in optimization processes, especially avoiding stagnation in complex landscapes like narrow ravines in delayed generalization problems.

Method: Introduces two interacting agents: a predator chasing prey and a prey escaping predator. This dynamic guides optimization through difficult landscape terrains, like ravines.

Result: Demonstrated up to 100x faster learning in delayed generalization problems using this predator-prey-driven optimization approach compared to standard methods.

Conclusion: The predator-prey interaction method effectively enhances optimization and learning across complex landscapes, proving highly superior in specific grokking problems.

Abstract: A machine learning method is proposed using two agents that simulate the
biological behavior of a predator and a prey. In this method, the predator and
the prey interact with each other - the predator chases the prey while the prey
runs away from the predator - to perform an optimization on the landscape. This
method allows, for the case of a ravine landscape (i.e., a landscape with
narrow ravines and with gentle slopes along the ravines) to avoid getting
optimization stuck in the ravine. For this, in the optimization over a ravine
landscape the predator drives the prey along the ravine. Thus we also call this
approach, for the case of ravine landscapes, the driven hunt method. For some
examples of grokking (i.e., delayed generalization) problems we show that this
method allows for achieving up to a hundred times faster learning compared to
the standard learning procedure.

</details>


### [81] [Deep Reinforcement Learning-Assisted Component Auto-Configuration of Differential Evolution Algorithm for Constrained Optimization: A Foundation Model](https://arxiv.org/abs/2509.11016)
*Xu Yang,Rui Wang,Kaiwen Li,Wenhua Li,Ling Wang*

Main category: cs.NE

TL;DR: This paper proposes SuperDE, a framework using Deep Reinforcement Learning (DRL) for automated configuration of Differential Evolution (DE) to tackle constrained optimization problems (COPs) effectively.


<details>
  <summary>Details</summary>
Motivation: The adaptability of evolutionary algorithms is limited due to their dependence on manual design and the challenges imposed by dynamic, real-world problems. Existing solutions lack efficiency, generalization, and strong convergence, highlighting the need for a more flexible algorithmic approach.

Method: The authors propose SuperDE, a model powered by Deep Reinforcement Learning. SuperDE trains offline using meta-learning over various COPs and uses a Double Deep Q-Network to dynamically and adaptively configure DE components in real-time.

Result: SuperDE achieved significant performance improvements over existing state-of-the-art algorithms on benchmark constrained optimization problems.

Conclusion: The proposed SuperDE framework demonstrates enhanced adaptability, generalization, and optimization performance, addressing the existing limitations of evolutionary algorithms on constrained optimization problems.

Abstract: Despite significant efforts to manually design high-performance evolutionary
algorithms, their adaptability remains limited due to the dynamic and
ever-evolving nature of real-world problems. The "no free lunch" theorem
highlights that no single algorithm performs optimally across all problems.
While online adaptation methods have been proposed, they often suffer from
inefficiency, weak convergence, and limited generalization on constrained
optimization problems (COPs).
  To address these challenges, we introduce a novel framework for automated
component configuration in Differential Evolution (DE) algorithm to address
COPs, powered by Deep Reinforcement Learning (DRL). Specifically, we propose
SuperDE, a foundation model that dynamically configures DE's evolutionary
components based on real-time evolution. Trained offline through meta-learning
across a wide variety of COPs, SuperDE is capable of recommending optimal
per-generation configurations for unseen problems in a zero-shot manner.
Utilizing a Double Deep Q-Network (DDQN), SuperDE adapts its configuration
strategies in response to the evolving population states during optimization.
Experimental results demonstrate that SuperDE significantly outperforms
existing state-of-the-art algorithms on benchmark test suites, achieving
superior generalization and optimization performance.

</details>


### [82] [Application of Machine Learning for Correcting Defect-induced Neuromorphic Circuit Inference Errors](https://arxiv.org/abs/2509.11113)
*Vedant Sawal,Hiu Yung Wong*

Main category: cs.NE

TL;DR: The paper introduces a lightweight neural network approach to correct inference errors caused by stuck-at faults in fully analog ReRAM-based neuromorphic circuits, recovering up to 35% accuracy loss.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address inference errors and reliability issues in neuromorphic systems caused by stuck-at faults and spatial defects in ReRAM-based architectures, which are critical for scalable and energy-efficient IoT applications.

Method: A Design-Technology Co-Optimization (DTCO) simulation framework is used to model multiple spatial defects, with a lightweight neural network trained on circuit output voltages employed for corrective measures.

Result: The corrective method enhances inference accuracy from 55% to 90% on handwritten digit recognition tasks, generalizes to unseen defect types, and improves circuit robustness significantly.

Conclusion: The proposed approach offers scalable, energy-efficient solutions for improving yield, reliability, and adaptability of neuromorphic systems in edge IoT applications, supporting adaptive learning for dynamic fault profiles.

Abstract: This paper presents a machine learning-based approach to correct inference
errors caused by stuck-at faults in fully analog ReRAM-based neuromorphic
circuits. Using a Design-Technology Co-Optimization (DTCO) simulation
framework, we model and analyze six spatial defect types-circular,
circular-complement, ring, row, column, and checkerboard-across multiple layers
of a multi-array neuromorphic architecture. We demonstrate that the proposed
correction method, which employs a lightweight neural network trained on the
circuit's output voltages, can recover up to 35% (from 55% to 90%) inference
accuracy loss in defective scenarios. Our results, based on handwritten digit
recognition tasks, show that even small corrective networks can significantly
improve circuit robustness. This method offers a scalable and energy-efficient
path toward enhanced yield and reliability for neuromorphic systems in edge and
internet-of-things (IoTs) applications. In addition to correcting the specific
defect types used during training, our method also demonstrates the ability to
generalize-achieving reasonable accuracy when tested on different types of
defects not seen during training. The framework can be readily extended to
support real-time adaptive learning, enabling on-chip correction for dynamic or
aging-induced fault profiles.

</details>


### [83] [Time to Play: Simulating Early-Life Animal Dynamics Enhances Robotics Locomotion Discovery](https://arxiv.org/abs/2509.11755)
*Paul Templier,Hannah Janmohamed,David Labonte,Antoine Cully*

Main category: cs.NE

TL;DR: The paper introduces SMOL, a novel curriculum for robotic locomotion inspired by developmental changes in animals, enhancing performance and diversity through dynamic modulation of actuator strength.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of dynamic training in robotics, inspired by how animals adapt their locomotion as their body morphology changes during growth and ageing.

Method: The SMOL curriculum integrates into the MAP-Elites framework, dynamically adjusting robot actuator strength to simulate the varying power-to-weight ratio seen in animals throughout development.

Result: Empirical tests demonstrate that SMOL improves the performance and diversity of robotic locomotion behaviors by leveraging early advantageous physics to develop robust skills.

Conclusion: SMOL successfully mimics natural developmental changes, providing insights into enhancing robotic systems and allowing robots to develop more adaptable and diverse locomotion behaviors.

Abstract: Developmental changes in body morphology profoundly shape locomotion in
animals, yet artificial agents and robots are typically trained under static
physical parameters. Inspired by ontogenetic scaling of muscle power in
biology, we propose Scaling Mechanical Output over Lifetime (SMOL), a novel
curriculum that dynamically modulates robot actuator strength to mimic natural
variations in power-to-weight ratio during growth and ageing. Integrating SMOL
into the MAP-Elites quality-diversity framework, we vary the torque in standard
robotics tasks to mimic the evolution of strength in animals as they grow up
and as their body changes. Through comprehensive empirical evaluation, we show
that the SMOL schedule consistently elevates both performance and diversity of
locomotion behaviours across varied control scenarios, by allowing agents to
leverage advantageous physics early on to discover skills that act as stepping
stones when they reach their final standard body properties. Based on studies
of the total power output in humans, we also implement the SMOL-Human schedule
that models isometric body variations due to non-linear changes like puberty,
and study its impact on robotics locomotion.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [84] [Mechanizing Synthetic Tait Computability in Istari](https://arxiv.org/abs/2509.11418)
*Runming Li,Yue Yao,Robert Harper*

Main category: cs.PL

TL;DR: This paper mechanizes Synthetic Tait Computability (STC) in the Istari proof assistant and demonstrates its applicability to formalizing type theory meta-theorems.


<details>
  <summary>Details</summary>
Motivation: To provide a machine-checked formalization of Synthetic Tait Computability (STC) for simplifying the development and validation of gluing models used to prove meta-theorems of type theories.

Method: The authors developed a reusable library in Istari proof assistant, consisting of synthetic phase distinction constructs like modalities, extension types, and strict glue types, and applied it to canonicity case studies of dependent type theories.

Result: The formalization successfully replicated the core STC constructions in Istari, maintaining the theoretical elegance of manual arguments while ensuring correctness through automated verification.

Conclusion: The work confirms that STC can be mechanized in a proof assistant like Istari, enabling reliable and efficient reasoning about type theories and their meta-theorems.

Abstract: Categorical gluing is a powerful technique for proving meta-theorems of type
theories such as canonicity and normalization. Synthetic Tait Computability
(STC) provides an abstract treatment of the complex gluing models by
internalizing the gluing category into a modal dependent type theory with a
phase distinction. This work presents a mechanization of STC in the Istari
proof assistant. Istari is a Martin-L\"{o}f-style extensional type theory with
equality reflection. Equality reflection eliminates the nuisance of transport
reasoning typically found in intensional proof assistants. This work develops a
reusable library for synthetic phase distinction, including modalities,
extension types, and strict glue types, and applies it to two case studies: (1)
a canonicity model for dependent type theory with dependent products and
booleans with large elimination, and (2) a Kripke canonicity model for the
cost-aware logical framework. Our results demonstrate that the core STC
constructions can be formalized essentially verbatim in Istari, preserving the
elegance of the on-paper arguments while ensuring machine-checked correctness.

</details>


### [85] [Expressive Power of One-Shot Control Operators and Coroutines](https://arxiv.org/abs/2509.11901)
*Kentaro Kobayashi,Yukiyoshi Kameyama*

Main category: cs.PL

TL;DR: This paper conducts a formal mathematical comparison of one-shot control operators' expressive powers, linking them with concepts like effect handlers, delimited continuations, and asymmetric coroutines.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to bridge a gap in the study of one-shot control operators, which are gaining traction for their balance between computational expressiveness and efficiency.

Method: The authors apply Felleisen's macro-expressiveness framework to measure and compare the expressive powers of various one-shot control operators.

Result: They validate that asymmetric coroutines can macro-express one-shot effect handlers and delimited-control operators. However, they also establish that the reverse is not true, revising prior informal arguments to ensure a formal basis.

Conclusion: The study contributes a rigorous formal foundation for understanding the relationships and expressive hierarchies among one-shot control operators, while correcting misconceptions in prior work.

Abstract: Control operators, such as exceptions and effect handlers, provide a means of
representing computational effects in programs abstractly and modularly. While
most theoretical studies have focused on multi-shot control operators, one-shot
control operators -- which restrict the use of captured continuations to at
most once -- are gaining attention for their balance between expressiveness and
efficiency. This study aims to fill the gap. We present a mathematically
rigorous comparison of the expressive power among one-shot control operators,
including effect handlers, delimited continuations, and even asymmetric
coroutines. Following previous studies on multi-shot control operators, we
adopt Felleisen's macro-expressiveness as our measure of expressiveness. We
verify the folklore that one-shot effect handlers and one-shot
delimited-control operators can be macro-expressed by asymmetric coroutines,
but not vice versa. We explain why a previous informal argument fails, and how
to revise it to make a valid macro-translation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [86] [Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2509.10570)
*Wei Dai,Shengen Wu,Wei Wu,Zhenhao Wang,Sisuo Lyu,Haicheng Liao,Limin Yu,Weiping Ding,Runwei Guan,Yutao Yue*

Main category: cs.RO

TL;DR: The paper reviews recent advances in trajectory prediction in autonomous driving using Large Foundation Models (LFMs) like Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs).


<details>
  <summary>Details</summary>
Motivation: Trajectory prediction is crucial for autonomous driving safety, but conventional deep learning methods face issues like lack of interpretability, high dependence on annotated data, and poor generalization in rare scenarios.

Method: The paper explores three methodologies: trajectory-language mapping, multimodal fusion, and constraint-based reasoning using LFMs to improve prediction safety and contextual reasoning.

Result: LFMs enable interpretable reasoning by integrating linguistic and scene semantics, improving generalization and prediction safety in complex environments.

Conclusion: LFMs are promising for trajectory prediction, offering better interpretability and generalization. Challenges like computational delays and data scarcity need addressing, with future focus on low-latency inference and motion foundation models.

Abstract: Trajectory prediction serves as a critical functionality in autonomous
driving, enabling the anticipation of future motion paths for traffic
participants such as vehicles and pedestrians, which is essential for driving
safety. Although conventional deep learning methods have improved accuracy,
they remain hindered by inherent limitations, including lack of
interpretability, heavy reliance on large-scale annotated data, and weak
generalization in long-tail scenarios. The rise of Large Foundation Models
(LFMs) is transforming the research paradigm of trajectory prediction. This
survey offers a systematic review of recent advances in LFMs, particularly
Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for
trajectory prediction. By integrating linguistic and scene semantics, LFMs
facilitate interpretable contextual reasoning, significantly enhancing
prediction safety and generalization in complex environments. The article
highlights three core methodologies: trajectory-language mapping, multimodal
fusion, and constraint-based reasoning. It covers prediction tasks for both
vehicles and pedestrians, evaluation metrics, and dataset analyses. Key
challenges such as computational latency, data scarcity, and real-world
robustness are discussed, along with future research directions including
low-latency inference, causality-aware modeling, and motion foundation models.

</details>


### [87] [STL-Based Motion Planning and Uncertainty-Aware Risk Analysis for Human-Robot Collaboration with a Multi-Rotor Aerial Vehicle](https://arxiv.org/abs/2509.10692)
*Giuseppe Silano,Amr Afifi,Martin Saska,Antonio Franchi*

Main category: cs.RO

TL;DR: The paper introduces a method for improving human-robot collaboration using a Multi-Rotor Aerial Vehicle (MRAV) through Signal Temporal Logic and an uncertainty-aware risk analysis.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance safety, timing, and human preferences in human-robot collaboration tasks, focusing on ergonomics and comfort.

Method: The authors utilize Signal Temporal Logic for encoding mission objectives, apply gradient-based optimization for trajectory planning, incorporate a risk analysis for uncertainties, and implement an event-triggered replanning strategy.

Result: Simulations in MATLAB and Gazebo demonstrate the method's success in creating safe, efficient, and adaptive trajectories for an object handover task inspired by power line maintenance scenarios.

Conclusion: The approach effectively improves human-robot collaboration by generating resilient and dynamic mission plans while addressing safety and comfort.

Abstract: This paper presents a novel approach to motion planning and risk analysis for
enhancing human-robot collaboration using a Multi-Rotor Aerial Vehicle (MRAV).
The proposed method uses Signal Temporal Logic (STL) to encode key mission
objectives, such as safety, timing, and human preferences, with a strong focus
on ergonomics and comfort. An optimization framework generates dynamically
feasible trajectories while considering the MRAV's physical constraints. Given
the nonlinear and non-convex nature of the problem, smooth approximations and
gradient-based techniques assist in handling the problem's computational
complexity. Additionally, an uncertainty-aware risk analysis is incorporated to
assess potential deviations from the mission specifications, providing insights
into the likelihood of mission success under uncertain conditions. Further, an
event-triggered replanning strategy is implemented to respond to unforeseen
events and external disturbances. The approach is validated through MATLAB and
Gazebo simulations, using an object handover task in a mock-up environment
inspired by power line maintenance scenarios. The results highlight the
method's effectiveness in achieving safe, efficient, and resilient human-robot
collaboration.

</details>


### [88] [A Survey on LiDAR-based Autonomous Aerial Vehicles](https://arxiv.org/abs/2509.10730)
*Yunfan Ren,Yixi Cai,Haotian Li,Nan Chen,Fangcheng Zhu,Longji Yin,Fanze Kong,Rundong Li,Fu Zhang*

Main category: cs.RO

TL;DR: This paper surveys LiDAR-based UAV advancements in design, perception, and control, emphasizing their application in complex environments and future directions.


<details>
  <summary>Details</summary>
Motivation: LiDAR technology has become vital for UAVs, providing reliable navigation in GPS-denied environments and enabling advanced autonomy.

Method: The paper examines LiDAR sensor evolution, integrates software components for perception and control, and reviews practical UAV applications, concluding with challenges and future research proposals.

Result: LiDAR significantly enhances UAV autonomy, enabling diverse practical applications such as industrial use and UAV swarm collaboration in challenging environments.

Conclusion: The survey highlights LiDAR as a key enabler for advanced UAV systems, suggesting research directions for overcoming challenges and improving multi-UAV collaboration.

Abstract: This survey offers a comprehensive overview of recent advancements in
LiDAR-based autonomous Unmanned Aerial Vehicles (UAVs), covering their design,
perception, planning, and control strategies. Over the past decade, LiDAR
technology has become a crucial enabler for high-speed, agile, and reliable UAV
navigation, especially in GPS-denied environments. The paper begins by
examining the evolution of LiDAR sensors, emphasizing their unique advantages
such as high accuracy, long-range depth measurements, and robust performance
under various lighting conditions, making them particularly well-suited for UAV
applications. The integration of LiDAR with UAVs has significantly enhanced
their autonomy, enabling complex missions in diverse and challenging
environments. Subsequently, we explore essential software components, including
perception technologies for state estimation and mapping, as well as trajectory
planning and control methodologies, and discuss their adoption in LiDAR-based
UAVs. Additionally, we analyze various practical applications of the
LiDAR-based UAVs, ranging from industrial operations to supporting different
aerial platforms and UAV swarm deployments. The survey concludes by discussing
existing challenges and proposing future research directions to advance
LiDAR-based UAVs and enhance multi-UAV collaboration. By synthesizing recent
developments, this paper aims to provide a valuable resource for researchers
and practitioners working to push the boundaries of LiDAR-based UAV systems.

</details>


### [89] [Analytical Design and Development of a Modular and Intuitive Framework for Robotizing and Enhancing the Existing Endoscopic Procedures](https://arxiv.org/abs/2509.10735)
*Mohammad Rafiee Javazm,Yash Kulkarni,Jiaqi Xue,Naruhiko Ikoma,Farshid Alambeigi*

Main category: cs.RO

TL;DR: This paper presents a mechatronic framework to ease the operation of endoscopic devices, featuring advanced gripping and feeder mechanisms along with a user-friendly interface.


<details>
  <summary>Details</summary>
Motivation: Manual control of endoscopic devices is challenging for clinicians and leads to issues such as fatigue, distractions, and increased workload.

Method: The paper introduces a modular framework consisting of a nested collet-chuck mechanism, a feeder mechanism for insertion/retraction, and an intuitive user interface. Mathematical modeling was applied for design analysis.

Result: The framework's performance and mathematical modeling were validated through simulation and experimental studies, demonstrating effectiveness.

Conclusion: The mechatronic framework simplifies endoscope control, reduces clinician challenges, and can be readily integrated into existing devices.

Abstract: Despite the widespread adoption of endoscopic devices for several cancer
screening procedures, manual control of these devices still remains challenging
for clinicians, leading to several critical issues such as increased workload,
fatigue, and distractions. To address these issues, in this paper, we introduce
the design and development of an intuitive, modular, and easily installable
mechatronic framework. This framework includes (i) a novel nested collet-chuck
gripping mechanism that can readily be integrated and assembled with the
existing endoscopic devices and control their bending degrees-of-freedom
(DoFs); (ii) a feeder mechanism that can control the insertion/retraction DoF
of a colonoscope, and (iii) a complementary and intuitive user interface that
enables simultaneous control of all DoFs during the procedure. To analyze the
design of the proposed mechanisms, we also introduce a mathematical modeling
approach and a design space for optimal selection of the parameters involved in
the design of gripping and feeder mechanisms. Our simulation and experimental
studies thoroughly demonstrate the performance of the proposed mathematical
modeling and robotic framework.

</details>


### [90] [FastTrack: GPU-Accelerated Tracking for Visual SLAM](https://arxiv.org/abs/2509.10757)
*Kimia Khabiri,Parsa Hosseininejad,Shishir Gopinath,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: This paper introduces a GPU-accelerated approach to enhance the tracking performance in visual-inertial SLAM systems, achieving up to 2.8x improvement.


<details>
  <summary>Details</summary>
Motivation: SLAM systems require timely processing of image and IMU data for accurate frame localization. Slow processing could result in poor localization or tracking loss.

Method: The researchers leveraged GPU computing, specifically CUDA, to speed up key tracking components like stereo feature matching and local map tracking within the ORB-SLAM3 framework.

Result: The GPU-based enhancements improved tracking performance by up to 2.8x on both desktop systems and Jetson Xavier NX in stereo-inertial mode, as validated on SLAM datasets EuRoC and TUM-VI.

Conclusion: GPU acceleration significantly boosts tracking efficiency in visual-inertial SLAM systems, making them more reliable for real-world applications.

Abstract: The tracking module of a visual-inertial SLAM system processes incoming image
frames and IMU data to estimate the position of the frame in relation to the
map. It is important for the tracking to complete in a timely manner for each
frame to avoid poor localization or tracking loss. We therefore present a new
approach which leverages GPU computing power to accelerate time-consuming
components of tracking in order to improve its performance. These components
include stereo feature matching and local map tracking. We implement our design
inside the ORB-SLAM3 tracking process using CUDA. Our evaluation demonstrates
an overall improvement in tracking performance of up to 2.8x on a desktop and
Jetson Xavier NX board in stereo-inertial mode, using the well-known SLAM
datasets EuRoC and TUM-VI.

</details>


### [91] [RSL-RL: A Learning Library for Robotics Research](https://arxiv.org/abs/2509.10771)
*Clemens Schwarke,Mayank Mittal,Nikita Rudin,David Hoeller,Marco Hutter*

Main category: cs.RO

TL;DR: RSL-RL is a streamlined, open-source reinforcement learning library for robotics with GPU-optimized performance and ease of customization.


<details>
  <summary>Details</summary>
Motivation: To provide a lightweight, robotics-focused reinforcement learning library that facilitates easy algorithm adaptation and high-performance simulation.

Method: The library is tailored for GPU-only training and includes widely used RL algorithms and robotics-specific techniques.

Result: Validated in both simulation benchmarks and real-world robotic experiments, showcasing its practicality and efficiency.

Conclusion: RSL-RL serves as a versatile and effective tool for developing robotic controllers, optimized for use in robotics research.

Abstract: RSL-RL is an open-source Reinforcement Learning library tailored to the
specific needs of the robotics community. Unlike broad general-purpose
frameworks, its design philosophy prioritizes a compact and easily modifiable
codebase, allowing researchers to adapt and extend algorithms with minimal
overhead. The library focuses on algorithms most widely adopted in robotics,
together with auxiliary techniques that address robotics-specific challenges.
Optimized for GPU-only training, RSL-RL achieves high-throughput performance in
large-scale simulation environments. Its effectiveness has been validated in
both simulation benchmarks and in real-world robotic experiments, demonstrating
its utility as a lightweight, extensible, and practical framework to develop
learning-based robotic controllers. The library is open-sourced at:
https://github.com/leggedrobotics/rsl_rl.

</details>


### [92] [Follow-Bench: A Unified Motion Planning Benchmark for Socially-Aware Robot Person Following](https://arxiv.org/abs/2509.10796)
*Hanjing Ye,Weixi Situ,Jianwei Peng,Yu Zhan,Bingyi Xia,Kuanqi Cai,Hong Zhang*

Main category: cs.RO

TL;DR: This paper delves into robot person following (RPF) applications, creating a unified benchmark (Follow-Bench) and evaluating planner methods in simulations and real-world settings, focusing on safety and comfort.


<details>
  <summary>Details</summary>
Motivation: Robot person following has significant relevance in assisting individuals, ensuring safety, and addressing dynamic environments like eldercare and logistics.

Method: The study creates Follow-Bench, simulates diverse RPF scenarios, re-implements six planners, and evaluates top-performing planners on real robots.

Result: The analysis yielded valuable insights into safety-comfort trade-offs in RPF systems and demonstrated how existing planners operate in real-world environments.

Conclusion: This research highlights the need for refined RPF systems, uncovers existing challenges, and suggests promising directions for future explorations.

Abstract: Robot person following (RPF) -- mobile robots that follow and assist a
specific person -- has emerging applications in personal assistance, security
patrols, eldercare, and logistics. To be effective, such robots must follow the
target while ensuring safety and comfort for both the target and surrounding
people. In this work, we present the first end-to-end study of RPF, which (i)
surveys representative scenarios, motion-planning methods, and evaluation
metrics with a focus on safety and comfort; (ii) introduces Follow-Bench, a
unified benchmark simulating diverse scenarios, including various target
trajectory patterns, dynamic-crowd flows, and environmental layouts; and (iii)
re-implements six popular RPF planners, ensuring that both safety and comfort
are systematically considered. Moreover, we evaluate the two highest-performing
planners from our benchmark on a differential-drive robot to provide insights
into real-world deployment. Extensive simulation and real-world experiments
provide quantitative insights into the safety-comfort trade-offs of existing
planners, while revealing open challenges and future research directions.

</details>


### [93] [A Universal Wire Testing Machine for Enhancing the Performance of Wire-Driven Robots](https://arxiv.org/abs/2509.10862)
*Temma Suzuki,Kento Kawaharazuka,Kei Okada*

Main category: cs.RO

TL;DR: The paper introduces a Universal Wire Testing Machine to analyze and improve the performance of wire-driven robotic mechanisms through measurements and adjustments of wire characteristics.


<details>
  <summary>Details</summary>
Motivation: To address the issue of large modeling errors in wire-driven mechanisms, which limits their adoption in industrial and research robotics.

Method: Development of a Universal Wire Testing Machine used to measure key wire characteristics such as initial wire stretch, tension transmission efficiency with passive pulleys, and dynamic behavior of variable-length wires.

Result: Data measured using the testing machine improved force control in a wire-driven robot, significantly reducing the end-effector force error.

Conclusion: Utilizing precise wire characteristic measurements enhances the performance of wire-driven robotic systems.

Abstract: Compared with gears and linkages, wires constitute a lightweight,
low-friction transmission mechanism. However, because wires are flexible
materials, they tend to introduce large modeling errors, and their adoption in
industrial and research robots remains limited.In this study, we built a
Universal Wire Testing Machine that enables measurement and adjustment of wire
characteristics to improve the performance of wire-driven mechanisms. Using
this testing machine, we carried out removal of initial wire stretch,
measurement of tension transmission efficiency for eight different diameters of
passive pulleys, and measurement of the dynamic behavior of variable-length
wires. Finally, we applied the data obtained from this testing machine to the
force control of an actual wire-driven robot, reducing the end-effector force
error.

</details>


### [94] [Nav-R1: Reasoning and Navigation in Embodied Scenes](https://arxiv.org/abs/2509.10884)
*Qingxiang Liu,Ting Huang,Zeyu Zhang,Hao Tang*

Main category: cs.RO

TL;DR: This paper introduces Nav-R1, an embodied foundation model designed for effective 3D environment navigation by integrating structured reasoning and efficient control mechanisms.


<details>
  <summary>Details</summary>
Motivation: Embodied navigation systems face challenges of incoherent reasoning and balancing long-horizon semantic reasoning with reactive control for real-time navigation.

Method: The authors create Nav-CoT-110K, a step-by-step reasoning dataset, and a GRPO-based reinforcement learning framework with three rewards for structured reasoning and robust navigation. They also adopt a Fast-in-Slow paradigm to separate semantic reasoning from reactive navigation.

Result: Nav-R1 outperform strong baselines in embodied AI benchmarks, achieving over 8% improvement in reasoning and navigation. It also demonstrates robustness during real-world mobile robot deployment.

Conclusion: Nav-R1 bridges reasoning and navigation in embodied AI tasks with a novel dataset, learning framework, and reasoning paradigm, showing strong results both in benchmarks and practical applications.

Abstract: Embodied navigation requires agents to integrate perception, reasoning, and
action for robust interaction in complex 3D environments. Existing approaches
often suffer from incoherent and unstable reasoning traces that hinder
generalization across diverse environments, and difficulty balancing
long-horizon semantic reasoning with low-latency control for real-time
navigation. To address these challenges, we propose Nav-R1, an embodied
foundation model that unifies reasoning in embodied environments. We first
construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought
(CoT) for embodied tasks, which enables cold-start initialization with
structured reasoning. Building on this foundation, we design a GRPO-based
reinforcement learning framework with three complementary rewards: format,
understanding, and navigation, to improve structural adherence, semantic
grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow
reasoning paradigm, decoupling deliberate semantic reasoning from low-latency
reactive control for efficient yet coherent navigation. Extensive evaluations
on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms
strong baselines, with over 8% average improvement in reasoning and navigation
performance. Real-world deployment on a mobile robot further validates its
robustness under limited onboard resources. Code:
https://github.com/AIGeeksGroup/Nav-R1. Website:
https://aigeeksgroup.github.io/Nav-R1.

</details>


### [95] [Design of scalable orthogonal digital encoding architecture for large-area flexible tactile sensing in robotics](https://arxiv.org/abs/2509.10888)
*Weijie Liu,Ziyi Qiu,Shihang Wang,Deqing Mei,Yancheng Wang*

Main category: cs.RO

TL;DR: The paper proposes a novel decentralized encoding strategy inspired by code division multiple access to improve tactile sensors, achieving human-like scalability and responsiveness.


<details>
  <summary>Details</summary>
Motivation: Existing tactile sensors face challenges in mimicking human skin due to issues like wiring complexity and scalability.

Method: Introduced a decentralized encoding architecture enabling parallel signal transmission using orthogonal digital base codes, validated with a 16-node sensing array.

Result: Achieved high-speed pressure distribution reconstruction in 12.8ms with only a single transmission wire, scalable to thousands of nodes while maintaining sub-20ms latency.

Conclusion: The approach redefines signal encoding paradigms in soft electronics, enabling scalable intelligent systems with human-like tactile perception.

Abstract: Human-like embodied tactile perception is crucial for the next-generation
intelligent robotics. Achieving large-area, full-body soft coverage with high
sensitivity and rapid response, akin to human skin, remains a formidable
challenge due to critical bottlenecks in encoding efficiency and wiring
complexity in existing flexible tactile sensors, thus significantly hinder the
scalability and real-time performance required for human skin-level tactile
perception. Herein, we present a new architecture employing code division
multiple access-inspired orthogonal digital encoding to overcome these
challenges. Our decentralized encoding strategy transforms conventional serial
signal transmission by enabling parallel superposition of energy-orthogonal
base codes from distributed sensing nodes, drastically reducing wiring
requirements and increasing data throughput. We implemented and validated this
strategy with off-the-shelf 16-node sensing array to reconstruct the pressure
distribution, achieving a temporal resolution of 12.8 ms using only a single
transmission wire. Crucially, the architecture can maintain sub-20ms latency
across orders-of-magnitude variations in node number (to thousands of nodes).
By fundamentally redefining signal encoding paradigms in soft electronics, this
work opens new frontiers in developing scalable embodied intelligent systems
with human-like sensory capabilities.

</details>


### [96] [ViSTR-GP: Online Cyberattack Detection via Vision-to-State Tensor Regression and Gaussian Processes in Automated Robotic Operations](https://arxiv.org/abs/2509.10948)
*Navid Aftabi,Philip Samaha,Jin Ma,Long Cheng,Ramy Harik,Dan Li*

Main category: cs.RO

TL;DR: A framework, ViSTR-GP, uses an independent vision-based channel to detect sophisticated data-integrity attacks in robotic manufacturing systems early and reliably.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the increasingly critical cybersecurity risks in industrial robotic systems, particularly detecting data-integrity attacks, which exploit vulnerabilities to manipulate operational data and are challenging to detect using current methods.

Method: ViSTR-GP utilizes an overhead camera outside the controller’s authority to cross-check encoder data via a vision-based surrogate paired with a matrix-variate Gaussian process and interpretable test statistics. It employs SAM-Track for segmentation, low-rank tensor regression, and monitors residuals in real-time.

Result: On a real-world robotic testbed with synchronized video and encoder data, ViSTR-GP accurately recovered joint angles and detected data-integrity attacks earlier and with higher frequency than existing baselines, outperforming especially in subtle attack scenarios.

Conclusion: The framework demonstrates that adding an independent physical channel enables early and frequent detection of attacks without complex modifications to manufacturing systems, improving cybersecurity robustness in automated factories.

Abstract: Industrial robotic systems are central to automating smart manufacturing
operations. Connected and automated factories face growing cybersecurity risks
that can potentially cause interruptions and damages to physical operations.
Among these attacks, data-integrity attacks often involve sophisticated
exploitation of vulnerabilities that enable an attacker to access and
manipulate the operational data and are hence difficult to detect with only
existing intrusion detection or model-based detection. This paper addresses the
challenges in utilizing existing side-channels to detect data-integrity attacks
in robotic manufacturing processes by developing an online detection framework,
ViSTR-GP, that cross-checks encoder-reported measurements against a
vision-based estimate from an overhead camera outside the controller's
authority. In this framework, a one-time interactive segmentation initializes
SAM-Track to generate per-frame masks. A low-rank tensor-regression surrogate
maps each mask to measurements, while a matrix-variate Gaussian process models
nominal residuals, capturing temporal structure and cross-joint correlations. A
frame-wise test statistic derived from the predictive distribution provides an
online detector with interpretable thresholds. We validate the framework on a
real-world robotic testbed with synchronized video frame and encoder data,
collecting multiple nominal cycles and constructing replay attack scenarios
with graded end-effector deviations. Results on the testbed indicate that the
proposed framework recovers joint angles accurately and detects data-integrity
attacks earlier with more frequent alarms than all baselines. These
improvements are most evident in the most subtle attacks. These results show
that plants can detect data-integrity attacks by adding an independent physical
channel, bypassing the controller's authority, without needing complex
instrumentation.

</details>


### [97] [ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation](https://arxiv.org/abs/2509.10952)
*Yangcen Liu,Woo Chul Shin,Yunhai Han,Zhenyang Chen,Harish Ravichandar,Danfei Xu*

Main category: cs.RO

TL;DR: The paper proposes ImMimic, a framework to teach robots manipulation tasks using human video data and a small set of robot demonstrations, addressing domain gaps via retargeting and interpolation methods.


<details>
  <summary>Details</summary>
Motivation: Teaching robots through direct human video data is scalable but challenged by domain differences, such as visual and physical discrepancies, preventing direct imitation.

Method: The approach, ImMimic, utilizes Dynamic Time Warping to map human hand movements to robot joints and applies MixUp interpolation to create intermediate domains for co-training.

Result: ImMimic shows improved task success rates and smoother executions across various manipulation tasks and robot embodiments, demonstrating its effectiveness.

Conclusion: ImMimic successfully bridges the gap between human and robotic domains, making robot learning from human videos feasible and effective for real-world applications.

Abstract: Learning robot manipulation from abundant human videos offers a scalable
alternative to costly robot-specific data collection. However, domain gaps
across visual, morphological, and physical aspects hinder direct imitation. To
effectively bridge the domain gap, we propose ImMimic, an embodiment-agnostic
co-training framework that leverages both human videos and a small amount of
teleoperated robot demonstrations. ImMimic uses Dynamic Time Warping (DTW) with
either action- or visual-based mapping to map retargeted human hand poses to
robot joints, followed by MixUp interpolation between paired human and robot
trajectories. Our key insights are (1) retargeted human hand trajectories
provide informative action labels, and (2) interpolation over the mapped data
creates intermediate domains that facilitate smooth domain adaptation during
co-training. Evaluations on four real-world manipulation tasks (Pick and Place,
Push, Hammer, Flip) across four robotic embodiments (Robotiq, Fin Ray, Allegro,
Ability) show that ImMimic improves task success rates and execution
smoothness, highlighting its efficacy to bridge the domain gap for robust robot
manipulation. The project website can be found at
https://sites.google.com/view/immimic.

</details>


### [98] [Pogosim -- a Simulator for Pogobot robots](https://arxiv.org/abs/2509.10968)
*Leo Cazenille,Loona Macabre,Nicolas Bredeche*

Main category: cs.RO

TL;DR: The paper introduces Pogosim, a simulator for Pogobots, aiding in swarm robotic algorithm development efficiently by minimizing manual testing and allowing seamless integration with real-world robots.


<details>
  <summary>Details</summary>
Motivation: Developing swarm robotics algorithms on physical robots is labor-intensive and resource-draining, especially as complexity scales.

Method: The authors designed Pogosim, a simulation tool for Pogobots that shares the same codebase with the real robots, thereby ensuring streamlined transitions and enabling parallel simulation execution.

Result: Pogosim accelerates the development process, enables efficient parameter optimization for user code, and simplifies the transition between simulation and real-world application on Pogobots.

Conclusion: Pogosim is a cost-effective tool for swarm robotics, enabling scalable, efficient testing and algorithm optimization while maintaining fidelity to real-world implementations.

Abstract: Pogobots are a new type of open-source/open-hardware robots specifically
designed for swarm robotics research. Their cost-effective and modular design,
complemented by vibration-based and wheel-based locomotion, fast infrared
communication and extensive software architecture facilitate the implementation
of swarm intelligence algorithms. However, testing even simple distributed
algorithms directly on robots is particularly labor-intensive. Scaling to more
complex problems or calibrate user code parameters will have a prohibitively
high strain on available resources. In this article we present Pogosim, a fast
and scalable simulator for Pogobots, designed to reduce as much as possible
algorithm development costs. The exact same code will be used in both
simulation and to experimentally drive real robots. This article details the
software architecture of Pogosim, explain how to write configuration files and
user programs and how simulations approximate or differ from experiments. We
describe how a large set of simulations can be launched in parallel, how to
retrieve and analyze the simulation results, and how to optimize user code
parameters using optimization algorithms.

</details>


### [99] [Autonomous Close-Proximity Photovoltaic Panel Coating Using a Quadcopter](https://arxiv.org/abs/2509.10979)
*Dimitri Jacquemont,Carlo Bosio,Teaya Yang,Ruiqi Zhang,Ozgur Orun,Shuai Li,Reza Alam,Thomas M. Schutzius,Simo A. Makiharju,Mark W. Mueller*

Main category: cs.RO

TL;DR: This study introduces a quadcopter-based system for automated application of coatings on photovoltaic (PV) panels, enhancing panel efficiency through periodic and low-cost maintenance.


<details>
  <summary>Details</summary>
Motivation: The efficiency of photovoltaic panels is critical for renewable energy applications, but coatings that improve their performance degrade over time. Current reapplication methods are costly and labor-intensive, motivating the development of an autonomous, efficient solution.

Method: The authors designed a quadcopter equipped with a liquid dispersion mechanism and an onboard sensor-based localization stack. This includes visual-inertial odometry and a model-based control system tailored to account for the quadcopter's mass changes and ground effects during operation.

Result: The proposed system demonstrated robust autonomous capabilities in both indoor and outdoor environments using the onboard navigation and control systems.

Conclusion: Using UAVs for maintaining photovoltaic panels is a feasible and effective solution, providing flexibility and cost savings compared to traditional methods.

Abstract: Photovoltaic (PV) panels are becoming increasingly widespread in the domain
of renewable energy, and thus, small efficiency gains can have massive effects.
Anti-reflective and self-cleaning coatings enhance panel performance but
degrade over time, requiring periodic reapplication. Uncrewed Aerial Vehicles
(UAVs) offer a flexible and autonomous way to apply protective coatings more
often and at lower cost compared to traditional manual coating methods. In this
letter, we propose a quadcopter-based system, equipped with a liquid dispersion
mechanism, designed to automate such tasks. The localization stack only uses
onboard sensors, relying on visual-inertial odometry and the relative position
of the PV panel detected with respect to the quadcopter. The control relies on
a model-based controller that accounts for the ground effect and the mass
decrease of the quadcopter during liquid dispersion. We validate the autonomy
capabilities of our system through extensive indoor and outdoor experiments.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [100] [Quality Assessment of Tabular Data using Large Language Models and Code Generation](https://arxiv.org/abs/2509.10572)
*Ashlesha Akella,Akshar Kaul,Krishnasuri Narayanam,Sameep Mehta*

Main category: cs.SE

TL;DR: The paper introduces a three-stage framework combining statistical outlier detection and LLM-driven code generation to enhance tabular data quality validation.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based validation methods for tabular data often suffer from inefficiencies, excessive human involvement, and high computational costs, highlighting the need for automated and scalable approaches.

Method: The framework combines clustering for initial filtering, LLMs for generating semantic quality rules, and code generation to implement validation. It incorporates retrieval-augmented generation for domain knowledge and employs guardrails for accuracy and consistency.

Result: The proposed method was validated on benchmark datasets, demonstrating its ability to generate accurate and semantically valid quality rules efficiently.

Conclusion: The approach enhances automated data quality validation, reduces dependency on manual effort, and offers scalability for large tabular datasets.

Abstract: Reliable data quality is crucial for downstream analysis of tabular datasets,
yet rule-based validation often struggles with inefficiency, human
intervention, and high computational costs. We present a three-stage framework
that combines statistical inliner detection with LLM-driven rule and code
generation. After filtering data samples through traditional clustering, we
iteratively prompt LLMs to produce semantically valid quality rules and
synthesize their executable validators through code-generating LLMs. To
generate reliable quality rules, we aid LLMs with retrieval-augmented
generation (RAG) by leveraging external knowledge sources and domain-specific
few-shot examples. Robust guardrails ensure the accuracy and consistency of
both rules and code snippets. Extensive evaluations on benchmark datasets
confirm the effectiveness of our approach.

</details>


### [101] [Reasonable Experiments in Model-Based Systems Engineering](https://arxiv.org/abs/2509.10649)
*Johan Cederbladh,Loek Cleophas,Eduard Kamburjan,Lucas Lima,Rakshit Mittal,Hans Vangheluwe*

Main category: cs.SE

TL;DR: The paper introduces a framework using case-based reasoning and domain knowledge to efficiently manage and reuse experimental data in system design, validated through a vehicular energy system case study.


<details>
  <summary>Details</summary>
Motivation: To reduce time, effort, and costs in system design by reusing experimental data instead of conducting unnecessary experiments.

Method: Introduced a framework incorporating case-based reasoning with domain knowledge for intelligent reuse of experimental data, supported by a general architecture design.

Result: Validated the proposed framework using an industrial vehicular energy system-design case study, confirming its effectiveness.

Conclusion: The framework accelerates design efforts by enabling intelligent reuse of experimental data, reducing redundancy in experimentation and enhancing efficiency in model-based systems engineering.

Abstract: With the current trend in Model-Based Systems Engineering towards Digital
Engineering and early Validation & Verification, experiments are increasingly
used to estimate system parameters and explore design decisions. Managing such
experimental configuration metadata and results is of utmost importance in
accelerating overall design effort. In particular, we observe it is important
to 'intelligent-ly' reuse experiment-related data to save time and effort by
not performing potentially superfluous, time-consuming, and resource-intensive
experiments. In this work, we present a framework for managing experiments on
digital and/or physical assets with a focus on case-based reasoning with domain
knowledge to reuse experimental data efficiently by deciding whether an
already-performed experiment (or associated answer) can be reused to answer a
new (potentially different) question from the engineer/user without having to
set up and perform a new experiment. We provide the general architecture for
such an experiment manager and validate our approach using an industrial
vehicular energy system-design case study.

</details>


### [102] [Arguzz: Testing zkVMs for Soundness and Completeness Bugs](https://arxiv.org/abs/2509.10819)
*Christoph Hochrainer,Valentin Wüstholz,Maria Christakis*

Main category: cs.SE

TL;DR: Arguzz is an automated testing tool for identifying soundness and completeness bugs in zero-knowledge virtual machines (zkVMs), validating their cryptographic proofs by detecting faulty executions.


<details>
  <summary>Details</summary>
Motivation: zkVMs are deployed in blockchain applications to enable verifiable off-chain computation, but their complexity can lead to critical bugs affecting soundness and completeness.

Method: Arguzz uses a novel approach combining metamorphic testing with fault injection to generate equivalent program pairs and test zkVMs for issues by simulating faults.

Result: The tool tested six zkVMs and uncovered eleven bugs in three systems, including a $50,000 bug in RISC Zero, showcasing Arguzz's effectiveness.

Conclusion: Systematic testing via tools like Arguzz is essential to ensuring the reliability and security of zkVMs, especially given their significant role in decentralized applications.

Abstract: Zero-knowledge virtual machines (zkVMs) are increasingly deployed in
decentralized applications and blockchain rollups since they enable verifiable
off-chain computation. These VMs execute general-purpose programs, frequently
written in Rust, and produce succinct cryptographic proofs. However, zkVMs are
complex, and bugs in their constraint systems or execution logic can cause
critical soundness (accepting invalid executions) or completeness (rejecting
valid ones) issues.
  We present Arguzz, the first automated tool for testing zkVMs for soundness
and completeness bugs. To detect such bugs, Arguzz combines a novel variant of
metamorphic testing with fault injection. In particular, it generates
semantically equivalent program pairs, merges them into a single Rust program
with a known output, and runs it inside a zkVM. By injecting faults into the
VM, Arguzz mimics malicious or buggy provers to uncover overly weak
constraints.
  We used Arguzz to test six real-world zkVMs (RISC Zero, Nexus, Jolt, SP1,
OpenVM, and Pico) and found eleven bugs in three of them. One RISC Zero bug
resulted in a $50,000 bounty, despite prior audits, demonstrating the critical
need for systematic testing of zkVMs.

</details>


### [103] [TPSQLi: Test Prioritization for SQL Injection Vulnerability Detection in Web Applications](https://arxiv.org/abs/2509.10920)
*Guan-Yan Yang,Farn Wang,You-Zong Gu,Ya-Wen Teng,Kuo-Hui Yeh,Ping-Hsueh Ho,Wei-Ling Wen*

Main category: cs.SE

TL;DR: The paper addresses injection attacks by proposing a dynamic test prioritization method for SQL injection vulnerabilities, optimizing software testing for better efficiency.


<details>
  <summary>Details</summary>
Motivation: Injection attacks, particularly SQL injection, are a top concern in software security, and there is a need for advanced tools to manage increasing test complexity in agile development.

Method: The method dynamically adjusts defense strength vectors for SQL injection testing, leveraging results from prior tests to optimize the subsequent test workflows.

Result: The approach enhances the efficiency of vulnerability detection and mitigation by tailoring testing mechanisms to software-specific needs.

Conclusion: The dynamic and temporal adjustment framework allows more efficient and effective management of SQL injection risks.

Abstract: The rapid proliferation of network applications has led to a significant
increase in network attacks. According to the OWASP Top 10 Projects report
released in 2021, injection attacks rank among the top three vulnerabilities in
software projects. This growing threat landscape has increased the complexity
and workload of software testing, necessitating advanced tools to support agile
development cycles. This paper introduces a novel test prioritization method
for SQL injection vulnerabilities to enhance testing efficiency. By leveraging
previous test outcomes, our method adjusts defense strength vectors for
subsequent tests, optimizing the testing workflow and tailoring defense
mechanisms to specific software needs. This approach aims to improve the
effectiveness and efficiency of vulnerability detection and mitigation through
a flexible framework that incorporates dynamic adjustments and considers the
temporal aspects of vulnerability exposure.

</details>


### [104] [When the Code Autopilot Breaks: Why LLMs Falter in Embedded Machine Learning](https://arxiv.org/abs/2509.10946)
*Roberto Morabito,Guanghan Wu*

Main category: cs.SE

TL;DR: The paper explores failure modes in LLM-powered embedded ML pipelines and proposes a taxonomy of errors, highlighting systemic fragilities and suggesting improvements.


<details>
  <summary>Details</summary>
Motivation: To investigate and understand the diverse and unpredictable failure modes of LLM-generated code in embedded ML workflows, which are not easily caught by standard validation techniques.

Method: An empirical investigation was conducted using an autopilot framework, analyzing the effects of prompt formats, model behaviors, and structural assumptions on failure modes across multiple LLMs.

Result: The study identified various error-prone behaviors, created a taxonomy of failure categories, and uncovered common root causes that affect the success and reliability of LLM-based workflows.

Conclusion: The paper emphasizes the need for improved tools and techniques to enhance reliability and traceability in LLM-powered embedded ML systems.

Abstract: Large Language Models (LLMs) are increasingly used to automate software
generation in embedded machine learning workflows, yet their outputs often fail
silently or behave unpredictably. This article presents an empirical
investigation of failure modes in LLM-powered ML pipelines, based on an
autopilot framework that orchestrates data preprocessing, model conversion, and
on-device inference code generation. We show how prompt format, model behavior,
and structural assumptions influence both success rates and failure
characteristics, often in ways that standard validation pipelines fail to
detect. Our analysis reveals a diverse set of error-prone behaviors, including
format-induced misinterpretations and runtime-disruptive code that compiles but
breaks downstream. We derive a taxonomy of failure categories and analyze
errors across multiple LLMs, highlighting common root causes and systemic
fragilities. Though grounded in specific devices, our study reveals broader
challenges in LLM-based code generation. We conclude by discussing directions
for improving reliability and traceability in LLM-powered embedded ML systems.

</details>


### [105] [ViScratch: Using Large Language Models and Gameplay Videos for Automated Feedback in Scratch](https://arxiv.org/abs/2509.11065)
*Yuan Si,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: The paper introduces ViScratch, a debugging tool for Scratch that uses gameplay videos and block code to better identify and fix bugs, showing superior performance over existing tools.


<details>
  <summary>Details</summary>
Motivation: Debugging in block-based environments like Scratch is challenging because current tools ignore the visual nature of these platforms, focusing only on textual or manual inputs.

Method: ViScratch uses a two-stage pipeline leveraging a vision-language model to align gameplay video symptoms with code structure, identifying bugs and proposing repairs verified in the Scratch virtual machine.

Result: The system significantly outperforms state-of-the-art tools and human testers in bug identification and repair, validating the importance of integrating video for debugging.

Conclusion: ViScratch demonstrates that gameplay video can be a crucial debugging asset in visual programming, paving the way for future LLM-based debugging innovations that include multimodal inputs.

Abstract: Block-based programming environments such as Scratch are increasingly popular
in programming education, in particular for young learners. While the use of
blocks helps prevent syntax errors, semantic bugs remain common and difficult
to debug. Existing tools for Scratch debugging rely heavily on predefined rules
or user manual inputs, and crucially, they ignore the platform's inherently
visual nature.
  We introduce ViScratch, the first multimodal feedback generation system for
Scratch that leverages both the project's block code and its generated gameplay
video to diagnose and repair bugs. ViScratch uses a two-stage pipeline: a
vision-language model first aligns visual symptoms with code structure to
identify a single critical issue, then proposes minimal, abstract syntax tree
level repairs that are verified via execution in the Scratch virtual machine.
  We evaluate ViScratch on a set of real-world Scratch projects against
state-of-the-art LLM-based tools and human testers. Results show that gameplay
video is a crucial debugging signal: ViScratch substantially outperforms prior
tools in both bug identification and repair quality, even without access to
project descriptions or goals. This work demonstrates that video can serve as a
first-class specification in visual programming environments, opening new
directions for LLM-based debugging beyond symbolic code alone.

</details>


### [106] [Hardness, Structural Knowledge, and Opportunity: An Analytical Framework for Modular Performance Modeling](https://arxiv.org/abs/2509.11000)
*Omid Gheibi,Christian Kästner,Pooyan Jamshidi*

Main category: cs.SE

TL;DR: The paper explores how structural knowledge and system characteristics impact performance modeling and introduces the concept of modeling "hardness." Key findings link modular complexity and structural knowledge to modeling opportunities for task-specific improvements.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling how configurations impact system performance in the face of exponentially growing configuration spaces and establish a formal understanding of the impact of structural aspects and knowledge on performance modeling.

Method: The paper conducts controlled experiments with synthetic system models and establishes an analytical matrix to investigate the relationship between structural aspects, structural knowledge, and performance modeling improvements.

Result: The study finds that modeling hardness is primarily influenced by the number of modules and options per module. The level of structural knowledge and modeling hardness both significantly enhance modeling opportunities, with their impact varying by performance metric.

Conclusion: System designers can strategically use insights about structural characteristics and modeling hardness to select appropriate modeling approaches for specific performance tasks, optimizing time and resources.

Abstract: Performance-influence models are beneficial for understanding how
configurations affect system performance, but their creation is challenging due
to the exponential growth of configuration spaces. While gray-box approaches
leverage selective "structural knowledge" (like the module execution graph of
the system) to improve modeling, the relationship between this knowledge, a
system's characteristics (we call them "structural aspects"), and potential
model improvements is not well understood. This paper addresses this gap by
formally investigating how variations in structural aspects (e.g., the number
of modules and options per module) and the level of structural knowledge impact
the creation of "opportunities" for improved "modular performance modeling". We
introduce and quantify the concept of modeling "hardness", defined as the
inherent difficulty of performance modeling. Through controlled experiments
with synthetic system models, we establish an "analytical matrix" to measure
these concepts. Our findings show that modeling hardness is primarily driven by
the number of modules and configuration options per module. More importantly,
we demonstrate that both higher levels of structural knowledge and increased
modeling hardness significantly enhance the opportunity for improvement. The
impact of these factors varies by performance metric; for ranking accuracy
(e.g., in debugging task), structural knowledge is more dominant, while for
prediction accuracy (e.g., in resource management task), hardness plays a
stronger role. These results provide actionable insights for system designers,
guiding them to strategically allocate time and select appropriate modeling
approaches based on a system's characteristics and a given task's objectives.

</details>


### [107] [Rethinking Technology Stack Selection with AI Coding Proficiency](https://arxiv.org/abs/2509.11132)
*Xiaoyu Zhang,Weipeng Jiang,Juan Zhai,Shiqing Ma,Qingshuang Bao,Chenhao Lin,Chao Shen,Tianlin Li,Yang Liu*

Main category: cs.SE

TL;DR: The paper introduces 'AI coding proficiency' as a metric to evaluate whether LLMs can effectively generate high-quality code using third-party libraries, potentially influencing technology selection in software development.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for selecting software technologies often overlook whether LLMs can effectively use them to create high-quality code. Poor integration between LLMs and certain technologies results in technical inefficiencies.

Method: The authors conducted an empirical study, analyzing 170 third-party libraries and 61 coding tasks to measure AI coding proficiency across six LLMs.

Result: The study revealed significant performance disparities among libraries and LLMs, with some libraries showing up to 84% differences in code quality scores. These result in additional engineering costs and potential narrowing of technology choices.

Conclusion: The paper advocates for incorporating AI coding proficiency into technology selection practices to safeguard ecosystem diversity and maintain balanced AI-driven software development.

Abstract: Large language models (LLMs) are now an integral part of software development
workflows and are reshaping the whole process. Traditional technology stack
selection has not caught up. Most of the existing selection methods focus
solely on the inherent attributes of the technology, overlooking whether the
LLM can effectively leverage the chosen technology. For example, when
generating code snippets using popular libraries like Selenium (one of the most
widely used test automation tools with over 33k GitHub stars), existing LLMs
frequently generate low-quality code snippets (e.g., using deprecated APIs and
methods, or containing syntax errors). As such, teams using LLM assistants risk
choosing technologies that cannot be used effectively by LLMs, yielding high
debugging effort and mounting technical debt. We foresee a practical question
in the LLM era, is a technology ready for AI-assisted development? In this
paper, we first propose the concept, AI coding proficiency, the degree to which
LLMs can utilize a given technology to generate high-quality code snippets. We
conduct the first comprehensive empirical study examining AI proficiency across
170 third-party libraries and 61 task scenarios, evaluating six widely used
LLMs. Our findings reveal that libraries with similar functionalities can
exhibit up to 84% differences in the quality score of LLM-generated code, while
different models also exhibit quality gaps among their generation results using
the same library. These gaps translate into real engineering costs and can
steer developer choices toward a narrow set of libraries with high AI coding
proficiency, threatening technological diversity in the ecosystem. We call on
the community to integrate AI proficiency assessments into technology selection
frameworks and develop mitigation strategies, preserving competitive balance in
AI-driven development.

</details>


### [108] [UserTrace: User-Level Requirements Generation and Traceability Recovery from Software Project Repositories](https://arxiv.org/abs/2509.11238)
*Dongming Jin,Zhi Jin,Yiran Zhang,Zheng Fang,Linyu Li,Yuanpeng He,Xiaohong Chen,Weisong Sun*

Main category: cs.SE

TL;DR: UserTrace, a multi-agent system, addresses gaps in generating user-level requirements and ensures live traceability between requirements, code, and their evolution.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on implementation-level requirements and overlook user-level requirements and evolution traceability, affecting validation of AI-generated software.

Method: UserTrace employs four specialized agents (Code Reviewer, Searcher, Writer, Verifier) in a three-phase process: structuring repository dependencies, deriving implementation-level requirements, and synthesizing user-level requirements.

Result: Evaluation shows UserTrace improves completeness, correctness, and helpfulness of user-level requirements and achieves better trace link precision than RT benchmarks.

Conclusion: UserTrace enhances end-user validation of AI-generated software and strengthens traceability, addressing critical deficiencies in current techniques.

Abstract: Software maintainability critically depends on high-quality requirements
descriptions and explicit traceability between requirements and code. Although
automated code summarization (ACS) and requirements traceability (RT)
techniques have been widely studied, existing ACS methods mainly generate
implementation-level (i.e., developer-oriented) requirements (IRs) for
fine-grained units (e.g., methods), while RT techniques often overlook the
impact of project evolution. As a result, user-level (i.e., end user-oriented)
requirements (URs) and live trace links remain underexplored, despite their
importance for supporting user understanding and for validating whether
AI-generated software aligns with user intent. To address this gap, we propose
UserTrace, a multi-agent system that automatically generates URs and recovers
live trace links (from URs to IRs to code) from software repositories.
UserTrace coordinates four specialized agents (i.e., Code Reviewer, Searcher,
Writer, and Verifier) through a three-phase process: structuring repository
dependencies, deriving IRs for code units, and synthesizing URs with
domain-specific context. Our comparative evaluation shows that UserTrace
produces URs with higher completeness, correctness, and helpfulness than an
established baseline, and achieves superior precision in trace link recovery
compared to five state-of-the-art RT approaches. A user study further
demonstrates that UserTrace helps end users validate whether the AI-generated
repositories align with their intent.

</details>


### [109] [Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation](https://arxiv.org/abs/2509.11252)
*Chengze li,Yitong Zhang,Jia Li,Liyi Cai,Ge Li*

Main category: cs.SE

TL;DR: This paper examines diffusion LLMs as an alternative to autoregressive models for code generation, analyzing their advantages and limitations based on a systematic study.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies and rigidity in autoregressive LLMs used for code generation by evaluating diffusion LLMs, which promise multi-token prediction and flexible generation order.

Method: Conduct an empirical study on 9 diffusion LLMs across 4 common benchmarks, comparing their performance to autoregressive LLMs.

Result: Findings reveal that diffusion LLMs are competitive with autoregressive models, excel in long code understanding, and demonstrate strong length extrapolation capabilities.

Conclusion: Diffusion LLMs show promise for advancing code generation, with room for optimization. The study identifies impactful factors and proposes directions for improvement, contributing to further research with open-source resources.

Abstract: LLMs have become the mainstream approaches to code generation. Existing LLMs
mainly employ autoregressive generation, i.e. generating code token-by-token
from left to right. However, the underlying autoregressive generation has two
limitations in code generation. First, autoregressive LLMs only generate a
token at each step, showing low efficiency in practice. Second, programming is
a non-sequential process involving back-and-forth editing, while autoregressive
LLMs only employ the left-to-right generation order. These two intrinsic
limitations hinder the further development of LLMs in code generation.
Recently, diffusion LLMs have emerged as a promising alternative. Diffusion
LLMs address the above limitations with two advances, including multi-token
prediction (i.e. generating multiple tokens at each step) and flexible
generation order (i.e. flexibly determining which positions to generate
tokens). However, there is no systematic study exploring diffusion LLMs in code
generation. To bridge the knowledge gap, we present the first empirical study
of diffusion LLMs for code generation. Our study involves 9 representative
diffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on
the results, we summarize the following findings. (1) Existing diffusion LLMs
are competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs
have a stronger length extrapolation ability than autoregressive LLMs and
perform better in long code understanding. (3) We explore factors impacting the
effectiveness and efficiency of diffusion LLMs, and provide practical guidance.
(4) We discuss several promising further directions to improve diffusion LLMs
on code generation. We open-source all source code, data, and results to
facilitate the following research. The code is publicly available at
https://github.com/zhangyitonggg/dllm4code.

</details>


### [110] [A Web-Based Environment for the Specification and Generation of Smart Legal Contracts](https://arxiv.org/abs/2509.11258)
*Regan Meloche,Durga Sivakumar,Amal A. Anda,Sofana Alfuhaid,Daniel Amyot,Luigi Logrippo,John Mylopoulos*

Main category: cs.SE

TL;DR: The paper addresses automating monitoring of contract compliance through a web-based environment that bridges legal agreements and smart contracts.


<details>
  <summary>Details</summary>
Motivation: To overcome the gap between natural language legal contracts and smart contract implementations, enabling more efficient compliance monitoring.

Method: A web-based tool is developed to assist in refining Symboleo specifications derived from legal templates, and it automates the generation of smart contracts for deployment on Hyperledger Fabric.

Result: The introduced environment successfully facilitates the conversion of legal contracts into deployable smart contracts, with a case study in the transactive energy domain demonstrating its utility.

Conclusion: The proposed solution shows promise for accelerating legal compliance workflows by integrating smart contract automation into the process.

Abstract: Monitoring the compliance of contract performance against legal obligations
is important in order to detect violations, ideally, as soon as they occur.
Such monitoring can nowadays be achieved through the use of smart contracts,
which provide protection against tampering as well as some level of automation
in handling violations. However, there exists a large gap between natural
language contracts and smart contract implementations. This paper introduces a
Web-based environment that partly fills that gap by supporting the
user-assisted refinement of Symboleo specifications corresponding to legal
contract templates, followed by the automated generation of monitoring smart
contracts deployable on the Hyperledger Fabric platform. This environment,
illustrated using a sample contract from the transactive energy domain, shows
much potential in accelerating the development of smart contracts in a legal
compliance context.

</details>


### [111] [Weakly Supervised Vulnerability Localization via Multiple Instance Learning](https://arxiv.org/abs/2509.11312)
*Wenchao Gu,Yupan Chen,Yanlin Wang,Hongyu Zhang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: The paper introduces WAVES, an approach for vulnerability localization that uses function-level labels to predict statement-level vulnerabilities without requiring additional labeling efforts.


<details>
  <summary>Details</summary>
Motivation: Existing methods in software vulnerability detection are coarse-grained at the function or file level, leaving developers with extensive manual work to localize specific vulnerable statements.

Method: WAVES uses a technique called weakly supervised learning combined with multiple instance learning to derive pseudo labels from function-level ground-truth labels. These pseudo labels serve to train models for identifying vulnerable code statements.

Result: Experiments on three benchmark datasets show WAVES offers comparable vulnerability detection performance and outstanding statement-level vulnerability localization compared to prior methods.

Conclusion: WAVES addresses the high costs of manual labeling at the statement-level, providing an efficient and effective tool for vulnerability detection and localization.

Abstract: Software vulnerability detection has emerged as a significant concern in the
field of software security recently, capturing the attention of numerous
researchers and developers. Most previous approaches focus on coarse-grained
vulnerability detection, such as at the function or file level. However, the
developers would still encounter the challenge of manually inspecting a large
volume of code inside the vulnerable function to identify the specific
vulnerable statements for modification, indicating the importance of
vulnerability localization. Training the model for vulnerability localization
usually requires ground-truth labels at the statement-level, and labeling
vulnerable statements demands expert knowledge, which incurs high costs. Hence,
the demand for an approach that eliminates the need for additional labeling at
the statement-level is on the rise. To tackle this problem, we propose a novel
approach called WAVES for WeAkly supervised Vulnerability Localization via
multiplE inStance learning, which does not need the additional statement-level
labels during the training. WAVES has the capability to determine whether a
function is vulnerable (i.e., vulnerability detection) and pinpoint the
vulnerable statements (i.e., vulnerability localization). Specifically,
inspired by the concept of multiple instance learning, WAVES converts the
ground-truth label at the function-level into pseudo labels for individual
statements, eliminating the need for additional statement-level labeling. These
pseudo labels are utilized to train the classifiers for the function-level
representation vectors. Extensive experimentation on three popular benchmark
datasets demonstrates that, in comparison to previous baselines, our approach
achieves comparable performance in vulnerability detection and state-of-the-art
performance in statement-level vulnerability localization.

</details>


### [112] [Large Language Models (LLMs) for Requirements Engineering (RE): A Systematic Literature Review](https://arxiv.org/abs/2509.11446)
*Mohammad Amin Zadenoori,Jacek Dąbrowski,Waad Alhoshan,Liping Zhao,Alessio Ferrari*

Main category: cs.SE

TL;DR: The paper reviews 74 studies from 2023-2024 on using Large Language Models (LLMs) in Requirements Engineering (RE), exploring publication trends, activities, strategies, and methods. It highlights shifts in focus, underlining challenges and opportunities.


<details>
  <summary>Details</summary>
Motivation: LLMs have potential to enhance complex language-intensive processes in RE, prompting a need to systematically evaluate their applications for advancing the field.

Method: A systematic literature review categorizing 74 studies by dimensions such as publication trends, RE activities, prompting approaches, and evaluation strategies.

Result: LLMs are primarily leveraged for requirements elicitation and validation, showing differences from prior NLP approaches. Notable gaps include limited industrial application and integration.

Conclusion: Future research should focus on real-world testing, improved prompting methods, and new applications to maximize LLMs' relevance to RE and integration with broader software engineering workflows.

Abstract: Large Language Models (LLMs) are finding applications in numerous domains,
and Requirements Engineering (RE) is increasingly benefiting from their
capabilities to assist with complex, language-intensive tasks. This paper
presents a systematic literature review of 74 primary studies published between
2023 and 2024, examining how LLMs are being applied in RE. The study
categorizes the literature according to several dimensions, including
publication trends, RE activities, prompting strategies, and evaluation
methods. Our findings indicate notable patterns, among which we observe
substantial differences compared to previous works leveraging standard Natural
Language Processing (NLP) techniques. Most of the studies focus on using LLMs
for requirements elicitation and validation, rather than defect detection and
classification, which were dominant in the past. Researchers have also
broadened their focus and addressed novel tasks, e.g., test generation,
exploring the integration of RE with other software engineering (SE)
disciplines. Although requirements specifications remain the primary focus,
other artifacts are increasingly considered, including issues from issue
tracking systems, regulations, and technical manuals. The studies mostly rely
on GPT-based models, and often use Zero-shot or Few-shot prompting. They are
usually evaluated in controlled environments, with limited use in industry
settings and limited integration in complex workflows. Our study outlines
important future directions, such as leveraging the potential to expand the
influence of RE in SE, exploring less-studied tasks, improving prompting
methods, and testing in real-world environments. Our contribution also helps
researchers and practitioners use LLMs more effectively in RE, by providing a
list of identified tools leveraging LLMs for RE, as well as datasets.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [113] [Biomarkers of brain diseases](https://arxiv.org/abs/2509.10547)
*Pascal Helson,Arvind Kumar*

Main category: q-bio.NC

TL;DR: The paper critiques the reliance on cohort comparisons for brain disease biomarkers and suggests multimodal and longitudinal data as an alternative.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitation of current practices in analyzing brain data for clinical use, focusing on the challenge of identifying reliable biomarkers due to degeneracy in brain features.

Method: The authors use reasoning and a thought experiment to argue against traditional cohort comparisons and advocate for multimodal and longitudinal analyses.

Result: More data and advanced algorithms alone are insufficient to pinpoint reliable biomarkers for brain diseases.

Conclusion: Shifting to multimodal and longitudinal brain data analyses is essential for identifying robust biomarkers for brain diseases and improving clinical applications.

Abstract: Despite the diversity of brain data acquired and advanced AI-based algorithms
to analyze them, brain features are rarely used in clinics for diagnosis and
prognosis. Here we argue that the field continues to rely on cohort comparisons
to seek biomarkers, despite the well-established degeneracy of brain features.
Using a thought experiment, we show that more data and more powerful algorithms
will not be sufficient to identify biomarkers of brain diseases. We argue that
instead of comparing patient versus healthy controls using single data type, we
should use multimodal (e.g. brain activity, neurotransmitters, neuromodulators,
brain imaging) and longitudinal brain data to guide the grouping before
defining multidimensional biomarkers for brain diseases.

</details>


### [114] [How Easterners and Westerners perceive ADHD differently](https://arxiv.org/abs/2509.10549)
*Xing-Chan Lin*

Main category: q-bio.NC

TL;DR: This paper reviews literature on ADHD's positive traits across Western and Asian contexts, seeking to balance the predominantly Western focus in research.


<details>
  <summary>Details</summary>
Motivation: To address the prevalent deficit-centered view of ADHD and explore the potential strengths and adaptive traits associated with the condition, especially emphasizing underrepresented Asian contexts.

Method: Analyzing existing literature on ADHD strengths from both Western countries (UK, Netherlands, Canada) and Asian countries (Hong Kong, Malaysia, Singapore).

Result: The review highlights evidence of ADHD's adaptive traits across various cultural settings but identifies a research gap in Asian contexts, compared to Western studies.

Conclusion: Broadening the scope of ADHD research to include underrepresented regions offers a more holistic understanding of ADHD, including its strengths.

Abstract: Attention Deficit Hyperactivity Disorder has traditionally been
conceptualized as a neurodevelopmental condition associated with deficits such
as inattention, impulsivity, and poor time management. Such perspectives often
emphasize pathology and functional limitations. More recent scholarship,
however, has begun to reconceptualize ADHD by identifying potential adaptive
characteristics, including heightened energy, periods of hyperfocus, and
advanced cognitive flexibility. A growing body of qualitative research also
examines self reported experiences of high functioning individuals with ADHD,
highlighting positive traits and successful coping strategies. Despite these
developments, the majority of existing studies remain concentrated in Western
contexts, particularly the United Kingdom, Netherlands, and Canada. This review
seeks to address this imbalance by analyzing literature on ADHD strengths in
Europe and North America alongside emerging studies from Hong Kong, Malaysia,
and Singapore.

</details>


### [115] [Trial-Level Time-frequency EEG Desynchronization as a Neural Marker of Pain](https://arxiv.org/abs/2509.10552)
*D. A. Blanco-Mora,A. Dierolf,J. Gonçalves,M. van Der Meulen*

Main category: q-bio.NC

TL;DR: The study explores EEG oscillations as potential non-verbal markers for pain intensity, finding robust beta-band desynchronization linked to subjective pain ratings.


<details>
  <summary>Details</summary>
Motivation: Current pain measurement relies heavily on self-reports, limiting utility for non-communicative patients and translational research.

Method: The researchers used high-density EEG to analyze electrical stimulation trials, examining event-related desynchronization (ERD) in alpha and beta bands across Pain and No-Pain conditions.

Result: Beta-band ERD effectively differentiated Pain from No-Pain and corresponded to subjective pain intensity ratings, with moderators like age and gender affecting the link.

Conclusion: EEG oscillations offer promise as reliable, individualized, nonverbal indicators for pain, warranting validation in clinical settings and integration with multimodal approaches.

Abstract: Pain remains one of the most pressing health challenges, yet its measurement
still relies heavily on self-report, limiting monitoring in non-communicative
patients and hindering translational research. Neural oscillations recorded
with electroencephalography (EEG) provide a promising avenue for identifying
reproducible markers of nociceptive processing. Prior studies have reported
pain-related event-related desynchronization (ERD) in the alpha and beta bands,
but most rely on trial-averaging, obscuring variability that may be critical
for perception. We analyzed high-density EEG from 59 healthy participants who
underwent electrical stimulation under Pain and No-Pain conditions. Per-trial
time-frequency decomposition revealed robust beta-band ERD in frontal-central
electrodes that differentiated Pain from No-Pain trials. Generalized linear
mixed models demonstrated that ERD scaled with subjective intensity ratings
(VAS), and that age and gender moderated this relationship. Reverse models
further showed that ERD predicted VAS ratings across participants, underscoring
its potential as a nonverbal marker of pain. These findings provide preliminary
evidence that trial-level EEG oscillations can serve as reliable indicators of
pain and open avenues for individualized, report-free pain monitoring. Future
work should validate these results in patient populations and extend analyses
to multimodal approaches combining EEG, MRI, and attention-based modulation
strategies.

</details>


### [116] [HiLWS: A Human-in-the-Loop Weak Supervision Framework for Curating Clinical and Home Video Data for Neurological Assessment](https://arxiv.org/abs/2509.10557)
*Atefeh Irani,Maryam S. Mirian,Alex Lassooij,Reshad Hosseini,Hadi Moradi,Martin J. McKeown*

Main category: q-bio.NC

TL;DR: This paper introduces HiLWS, a novel human-in-the-loop weak supervision framework for curating and analyzing home-recorded videos for motor symptom assessment in Parkinson's Disease (PD).


<details>
  <summary>Details</summary>
Motivation: To address the challenges of using home-recorded videos (e.g. visual degradation, inconsistent execution, and annotation noise) for assessing motor symptoms in conditions like Parkinson's Disease, where scalable alternatives to in-clinic evaluations are needed.

Method: HiLWS applies a two-stage human-in-the-loop weak supervision approach. It first aggregates expert-provided annotations into probabilistic labels for training models. Then, it refines model predictions with expert input in a second round of supervision. The pipeline also incorporates quality filtering, optimized pose estimation, and task segmentation with context-aware evaluation metrics.

Result: HiLWS identifies failure modes in home-recorded data, shows the importance of context-sensitive curation, and demonstrates the effectiveness of the framework in improving medical video analysis and clinical relevance.

Conclusion: HiLWS provides a scalable, robust solution to address challenges with home-recorded video data for motor symptom assessment, highlighting the critical role of context-sensitive strategies and expert involvement.

Abstract: Video-based assessment of motor symptoms in conditions such as Parkinson's
disease (PD) offers a scalable alternative to in-clinic evaluations, but
home-recorded videos introduce significant challenges, including visual
degradation, inconsistent task execution, annotation noise, and domain shifts.
We present HiLWS, a cascaded human-in-the-loop weak supervision framework for
curating and annotating hand motor task videos from both clinical and home
settings. Unlike conventional single-stage weak supervision methods, HiLWS
employs a novel cascaded approach, first applies weak supervision to aggregate
expert-provided annotations into probabilistic labels, which are then used to
train machine learning models. Model predictions, combined with expert input,
are subsequently refined through a second stage of weak supervision. The
complete pipeline includes quality filtering, optimized pose estimation, and
task-specific segment extraction, complemented by context-sensitive evaluation
metrics that assess both visual fidelity and clinical relevance by prioritizing
ambiguous cases for expert review. Our findings reveal key failure modes in
home recorded data and emphasize the importance of context-sensitive curation
strategies for robust medical video analysis.

</details>


### [117] [On a Geometry of Interbrain Networks](https://arxiv.org/abs/2509.10650)
*Nicolás Hinrichs,Noah Guzmán,Melanie Weber*

Main category: q-bio.NC

TL;DR: The paper proposes a novel geometric framework for analyzing inter-brain synchrony in social neuroscience using curvature-based entropy.


<details>
  <summary>Details</summary>
Motivation: Current approaches to studying inter-brain synchrony are limited to correlation-based methods, which lack explanatory depth for dynamic social interactions. The study is motivated by the success of geometric methods in network science.

Method: A discrete geometry framework is introduced to study neural networks' evolving structures. It applies entropy metrics from curvature distributions to detect key transitions in inter-brain connectivity during social interactions.

Result: The method enhances hyperscanning methodologies by providing deeper insights into the neural mechanisms governing social interactions.

Conclusion: The proposed geometric approach offers a robust and innovative avenue for advancing the analysis of neural interactions in social neuroscience.

Abstract: Effective analysis in neuroscience benefits significantly from robust
conceptual frameworks. Traditional metrics of interbrain synchrony in social
neuroscience typically depend on fixed, correlation-based approaches,
restricting their explanatory capacity to descriptive observations. Inspired by
the successful integration of geometric insights in network science, we propose
leveraging discrete geometry to examine the dynamic reconfigurations in neural
interactions during social exchanges. Unlike conventional synchrony approaches,
our method interprets inter-brain connectivity changes through the evolving
geometric structures of neural networks. This geometric framework is realized
through a pipeline that identifies critical transitions in network connectivity
using entropy metrics derived from curvature distributions. By doing so, we
significantly enhance the capacity of hyperscanning methodologies to uncover
underlying neural mechanisms in interactive social behavior.

</details>


### [118] [Causal Emergence of Consciousness through Learned Multiscale Neural Dynamics in Mice](https://arxiv.org/abs/2509.10891)
*Zhipeng Wang,Yingqi Rong,Kaiwei Liu,Mingzhe Yang,Jiang Zhang,Jing He*

Main category: q-bio.NC

TL;DR: The paper introduces a machine learning framework that identifies multiscale causal variables from calcium imaging data in the mouse brain to explore the connection between neural activity and consciousness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of bridging the gap between macroscopic experiences and microscopic neuronal activity in the study of consciousness, and the limitations of existing theories that focus on single scales.

Method: The authors developed a machine learning framework to infer multiscale causal variables and their dynamics from high-resolution calcium imaging in the dorsal cortex of mice.

Result: The study found that lower-level variables aggregate input-driven information, while higher-level variables exhibit causal dynamics through metastable states in wakefulness, transitioning to localized stochastic dynamics during anesthesia. A dominant top-level variable captures most causal power but other scales also contribute, indicating high emergent complexity.

Conclusion: The findings establish a multiscale causal framework demonstrating the relationship between neural activity and conscious states, accounting for dynamics at various scales.

Abstract: Consciousness spans macroscopic experience and microscopic neuronal activity,
yet linking these scales remains challenging. Prevailing theories, such as
Integrated Information Theory, focus on a single scale, overlooking how causal
power and its dynamics unfold across scales. Progress is constrained by scarce
cross-scale data and difficulties in quantifying multiscale causality and
dynamics. Here, we present a machine learning framework that infers multiscale
causal variables and their dynamics from near-cellular-resolution calcium
imaging in the mouse dorsal cortex. At lower levels, variables primarily
aggregate input-driven information, whereas at higher levels they realize
causality through metastable or saddle-point dynamics during wakefulness,
collapsing into localized, stochastic dynamics under anesthesia. A
one-dimensional top-level conscious variable captures the majority of causal
power, yet variables across other scales also contribute substantially, giving
rise to high emergent complexity in the conscious state. Together, these
findings provide a multiscale causal framework that links neural activity to
conscious states.

</details>


### [119] [Residual Gaze Behavior During Navigation in Blindness and Low Vision](https://arxiv.org/abs/2509.11530)
*Junchi Feng,Fernanda Garcia-Pina,Mahya Beheshti,Todd E Hudson,William Seiple,John-Ross Rizzo*

Main category: q-bio.NC

TL;DR: The study examines gaze strategies in outdoor navigation across vision impairment levels, finding systematic changes with worsening vision and emphasizing the importance of personalized rehabilitation and assistive technologies.


<details>
  <summary>Details</summary>
Motivation: The study aims to examine how gaze behaviors vary during outdoor navigation among people with blindness, low vision, and full sight, addressing a gap in understanding of how visual impairment shapes mobility strategies.

Method: A comparative eye-tracking study was conducted involving fully sighted, low vision, blind, and fully blind participants. Metrics such as fixation counts, rate, area, direction, peak locations, and walking speed were quantified using wearable eye trackers.

Result: Walking speed decreased with worsening vision. Fixation count and rate increased, while fixation spatial coverage decreased as vision worsened. Fully sighted participants showed the most consistent fixation patterns. Peak fixation varied more with increasing impairment.

Conclusion: Gaze strategies vary along a continuum from fully sighted to fully blind individuals. Adaptive techniques and rehabilitation experiences strongly influence behavior, underscoring the need for custom rehabilitation and assistive tech tailored to residual gaze patterns.

Abstract: Background: Outdoor navigation poses significant challenges for people with
blindness or low vision, yet the role of gaze behavior in supporting mobility
remains underexplored. Fully sighted individuals typically adopt consistent
scanning strategies, whereas those with visual impairments rely on
heterogeneous adaptations shaped by residual vision and experience.
  Methods: We conducted a comparative eye-tracking study of fully sighted, low
vision, blind, and fully blind participants navigating outdoor routes. Using a
wearable eye tracker, we quantified fixation counts, fixation rate, fixation
area, direction, peak fixation location, and walking speed.
  Results: Walking speed declined systematically with worsening vision.
Fixation count increased with greater impairment, reflecting slower travel
times and more frequent sampling. Fixation rate rose with worsening vision,
though between-group differences were generally not significant between most
groups. Fixation spatial coverage decreased along the continuum of vision loss.
Fixation patterns were most consistent in the fully sighted group. Peak
fixation locations were centered in fully sighted participants but shifted
outward and became more variable with impairment.
  Conclusion: Gaze strategies during navigation form a graded continuum across
vision groups, with fully sighted and fully blind participants at opposite
poles and low vision and blind groups spanning the middle. Visual acuity alone
does not predict functional gaze use, as rehabilitation experience and adaptive
strategies strongly shape behavior. These findings highlight the need for
personalized rehabilitation and assistive technologies, with residual gaze
patterns offering insight into mobility capacity and training opportunities for
safer navigation.

</details>


### [120] [Representational drift under spontaneous activity -- self-organized criticality enhances representational reliability](https://arxiv.org/abs/2509.11545)
*Zhuda Yang,Junhao Liang,Wing Ho Yung,Changsong Zhou*

Main category: q-bio.NC

TL;DR: Examines how critical spontaneous states in neural systems enhance reliable information representation amid changes caused by plasticity and spontaneous activity.


<details>
  <summary>Details</summary>
Motivation: To address how neural systems maintain reliable representations despite the variability introduced by spontaneous critical states and synaptic changes.

Method: Used experimental observations from mouse visual cortex combined with a computational excitation-inhibition network model with homeostatic plasticity to investigate representational reliability.

Result: The model reproduced experimental findings showing representational drift coupled with restricted representational geometry and demonstrated enhanced cross-session representation in the critical state compared to non-critical states due to low synapse weight variations.

Conclusion: Critical spontaneous states serve as a functional mechanism enabling reliable information representation in dynamically changing neural networks, which explains stable perception and behavior despite ongoing synaptic rewiring.

Abstract: Neural systems face the challenge of maintaining reliable representations
amid variations from plasticity and spontaneous activity. In particular, the
spontaneous dynamics in neuronal circuit is known to operate near a highly
variable critical state, which intuitively contrasts with the requirement of
reliable representation. It is intriguing to understand how reliable
representation could be maintained or even enhanced by critical spontaneous
states. We firstly examined the co-existence of the scale-free avalanche in the
spontaneous activity of mouse visual cortex with restricted representational
geometry manifesting representational reliability amid the representational
drift with respect to the visual stimulus. To explore how critical spontaneous
state influences the neural representation, we built an excitation-inhibition
network with homeostatic plasticity, which self-organizes to the critical
spontaneous state. This model successfully reproduced both representational
drift and restricted representational geometry observed experimentally, in
contrast with randomly shuffled plasticity which causes accumulated drift of
representational geometry. We further showed that the self-organized critical
state enhances the cross-session low-dimensional representation, comparing to
the non-critical state, by restricting the synapse weight into a low variation
space. Our findings suggest that spontaneous self-organized criticality serves
not only as a ubiquitous property of neural systems but also as a functional
mechanism for maintaining reliable information representation under
continuously changing networks, providing a potential explanation how the brain
maintains consistent perception and behavior despite ongoing synaptic rewiring.

</details>


### [121] [Quantifying Mental States in Work Environment: Mathematical Perspectives](https://arxiv.org/abs/2509.12162)
*Aymen Balti,Assane Wade,Abdelatif Oujbara,M. A.,Aziz-Alaoui,Hicham Bellarabi,Frederic Dutertre,Benjamin Ambrosio*

Main category: q-bio.NC

TL;DR: This study uses VR and EEG signals analyzed by a KNN algorithm to infer emotional valence during stress scenarios.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve understanding and modeling of emotional and mental state dynamics under stress.

Method: 87 participants were exposed to a VR stress scenario. Emotional valence was derived from questionnaires, and EEG data was analyzed using a KNN algorithm.

Result: The KNN algorithm was successfully trained to infer emotional valence from EEG data.

Conclusion: This research supports the development of mathematical models for dynamic emotional and mental states, leveraging technology like EEG and VR.

Abstract: This article presents a study involving 87 participants exposed to a
stressful scenario in a virtual reality (VR) environment. An algorithm was
developed to assign a positive or negative valence based on questionnaire
responses. EEG signals were recorded, and a k-nearest neighbors (KNN) algorithm
was trained to infer emotional valence from these signals. Our objective is to
further develop mathematical models capable of describing the dynamic evolution
of emotional and mental states.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [122] [Variable Selection Using Relative Importance Rankings](https://arxiv.org/abs/2509.10853)
*Tien-En Chang,Argon Chen*

Main category: stat.ML

TL;DR: This paper proposes using relative importance (RI) measures for variable selection prior to model creation, demonstrating their superior ranking ability and competitive modeling performance compared to methods like lasso.


<details>
  <summary>Details</summary>
Motivation: Current methods for variable selection, like marginal correlation, fail to account for dependencies among predictors, limiting their effectiveness. The authors aim to explore RI measures as a solution.

Method: The study implements general dominance (GD), comprehensive relative importance (CRI), and CRI.Z to compare RI-based variable selection approaches against widely-used benchmarks like lasso.

Result: RI measures outperform marginal correlation in ranking predictors and lead to predictive models that rival or exceed the performance of lasso methods, particularly in settings with highly correlated predictors.

Conclusion: The results highlight RI-based methods as strong and competitive alternatives to conventional variable selection frameworks, warranting increased attention in statistics and machine learning fields.

Abstract: Although conceptually related, variable selection and relative importance
(RI) analysis have been treated quite differently in the literature. While RI
is typically used for post-hoc model explanation, this paper explores its
potential for variable ranking and filter-based selection before model
creation. Specifically, we anticipate strong performance from the RI measures
because they incorporate both direct and combined effects of predictors,
addressing a key limitation of marginal correlation that ignores dependencies
among predictors. We implement and evaluate the RI-based variable selection
methods using general dominance (GD), comprehensive relative importance (CRI),
and a newly proposed, computationally efficient variant termed CRI.Z.
  We first demonstrate how the RI measures more accurately rank the variables
than the marginal correlation, especially when there are suppressed or weak
predictors. We then show that predictive models built on these rankings are
highly competitive, often outperforming state-of-the-art methods such as the
lasso and relaxed lasso. The proposed RI-based methods are particularly
effective in challenging cases involving clusters of highly correlated
predictors, a setting known to cause failures in many benchmark methods.
Although lasso methods have dominated the recent literature on variable
selection, our study reveals that the RI-based method is a powerful and
competitive alternative. We believe these underutilized tools deserve greater
attention in statistics and machine learning communities. The code is available
at: https://github.com/tien-endotchang/RI-variable-selection.

</details>


### [123] [Kernel-based Stochastic Approximation Framework for Nonlinear Operator Learning](https://arxiv.org/abs/2509.11070)
*Jia-Qi Yang,Lei Shi*

Main category: stat.ML

TL;DR: This paper proposes a stochastic approximation framework for learning nonlinear operators in infinite-dimensional spaces using general operator-valued kernels, achieving dimension-free convergence rates and overcoming the curse of dimensionality.


<details>
  <summary>Details</summary>
Motivation: To develop a robust framework for learning nonlinear operators in infinite-dimensional spaces, addressing challenges like dimensionality and operator misspecifications.

Method: The authors introduce a framework using Mercer operator-valued kernels and vector-valued RKHSs, focusing on compact and diagonal kernel classes. They quantify misspecification error via interpolation spaces and establish convergence rates for nonlinear operator learning.

Result: The proposed framework achieves dimension-free polynomial convergence rates, allowing for effective learning of nonlinear operators. Numerical experiments on the Navier-Stokes equations validate its practical applicability.

Conclusion: The framework broadens the scope of operator learning while overcoming dimensionality challenges, offering rigorous guarantees and practical applications in complex nonlinear systems.

Abstract: We develop a stochastic approximation framework for learning nonlinear
operators between infinite-dimensional spaces utilizing general Mercer
operator-valued kernels. Our framework encompasses two key classes: (i) compact
kernels, which admit discrete spectral decompositions, and (ii) diagonal
kernels of the form $K(x,x')=k(x,x')T$, where $k$ is a scalar-valued kernel and
$T$ is a positive operator on the output space. This broad setting induces
expressive vector-valued reproducing kernel Hilbert spaces (RKHSs) that
generalize the classical $K=kI$ paradigm, thereby enabling rich structural
modeling with rigorous theoretical guarantees. To address target operators
lying outside the RKHS, we introduce vector-valued interpolation spaces to
precisely quantify misspecification error. Within this framework, we establish
dimension-free polynomial convergence rates, demonstrating that nonlinear
operator learning can overcome the curse of dimensionality. The use of general
operator-valued kernels further allows us to derive rates for intrinsically
nonlinear operator learning, going beyond the linear-type behavior inherent in
diagonal constructions of $K=kI$. Importantly, this framework accommodates a
wide range of operator learning tasks, ranging from integral operators such as
Fredholm operators to architectures based on encoder-decoder representations.
Moreover, we validate its effectiveness through numerical experiments on the
two-dimensional Navier-Stokes equations.

</details>


### [124] [Maximum diversity, weighting and invariants of time series](https://arxiv.org/abs/2509.11146)
*Byungchang So*

Main category: stat.ML

TL;DR: This paper examines the concept of magnitude, connecting it to continuity, and proposes new invariant tools for periodic time series analysis, supported by a machine learning use case.


<details>
  <summary>Details</summary>
Motivation: To deepen the understanding of magnitude in metric spaces by examining its continuity and extending its application to time series analysis.

Method: The study investigates the continuity properties of magnitude and weighting in relation to maximum diversity and introduces new invariant measures for time series analysis.

Result: The invariants derived from the continuity of magnitude improved machine learning performance in a real-world dataset use case.

Conclusion: The paper demonstrates that magnitude, understood through continuity, has practical applications in data analysis, particularly enhancing periodic time series representation and machine learning outcomes.

Abstract: Magnitude, obtained as a special case of Euler characteristic of enriched
category, represents a sense of the size of metric spaces and is related to
classical notions such as cardinality, dimension, and volume. While the studies
have explained the meaning of magnitude from various perspectives, continuity
also gives a valuable view of magnitude. Based on established results about
continuity of magnitude and maximum diversity, this article focuses on
continuity of weighting, a distribution whose totality is magnitude, and its
variation corresponding to maximum diversity. Meanwhile, recent studies also
illuminated the connection between magnitude and data analysis by applying
magnitude theory to point clouds representing the data or the set of model
parameters. This article will also provide an application for time series
analysis by introducing a new kind of invariants of periodic time series, where
the invariance follows directly from the continuity results. As a use-case, a
simple machine learning experiment is conducted with real-world data, in which
the suggested invariants improved the performance.

</details>


### [125] [Predictable Compression Failures: Why Language Models Actually Hallucinate](https://arxiv.org/abs/2509.11208)
*Leon Chlon,Ahmed Karim,Maggie Chlon*

Main category: stat.ML

TL;DR: The paper explores how large language models (LLMs) lack permutation invariance despite Bayesian inference capabilities and proposes a solution using transformers that minimize expected conditional description length.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the issue that LLMs, while performing near-Bayesian inference, fail to maintain permutation invariance on exchangeable data, which is a critical aspect for reliability.

Method: The authors propose that transformers minimize expected conditional description length over data orderings and provide a new theoretical framework including quantified martingale violation bounds, an Expectation-level Decompression Law, and deployable decision planners.

Result: Key findings include a $\log n$ scaling law for order deviations, improved model reliability via permutation mixtures, and significant reduction in hallucinations in pre-specified audits.

Conclusion: The paper identifies and addresses permutation-related deficiencies in LLMs, turning hallucination issues into predictable compression errors and introducing a principled framework for better information management.

Abstract: Large language models perform near-Bayesian inference yet violate permutation
invariance on exchangeable data. We resolve this by showing transformers
minimize expected conditional description length (cross-entropy) over
orderings, $\mathbb{E}_\pi[\ell(Y \mid \Gamma_\pi(X))]$, which admits a
Kolmogorov-complexity interpretation up to additive constants, rather than the
permutation-invariant description length $\ell(Y \mid X)$. This makes them
Bayesian in expectation, not in realization. We derive (i) a Quantified
Martingale Violation bound showing order-induced deviations scale as $O(\log
n)$ with constants; (ii) the Expectation-level Decompression Law linking
information budgets to reliability for Bernoulli predicates; and (iii)
deployable planners (B2T/RoH/ISR) for answer/abstain decisions. Empirically,
permutation dispersion follows $a+b\ln n$ (Qwen2-7B $b \approx 0.377$,
Llama-3.1-8B $b \approx 0.147$); permutation mixtures improve ground-truth
likelihood/accuracy; and randomized dose-response shows hallucinations drop by
$\sim 0.13$ per additional nat. A pre-specified audit with a fixed ISR=1.0
achieves near-0\% hallucinations via calibrated refusal at 24\% abstention. The
framework turns hallucinations into predictable compression failures and
enables principled information budgeting.

</details>


### [126] [Contrastive Network Representation Learning](https://arxiv.org/abs/2509.11316)
*Zihan Dong,Xin Zhou,Ryumei Nakada,Lexin Li,Linjun Zhang*

Main category: stat.ML

TL;DR: The paper introduces ACERL, a novel contrastive learning-based method for network edge embedding, designed for subject-specific brain connectivity data, and demonstrates its effectiveness theoretically and empirically.


<details>
  <summary>Details</summary>
Motivation: To address challenges in analyzing brain connectivity data, characterized by high-dimensionality, sparsity, and lack of covariates.

Method: Adaptive Contrastive Edge Representation Learning (ACERL) combines contrastive learning of augmented network pairs with an adaptive random masking mechanism and establishes theoretical error bounds.

Result: ACERL achieves minimax optimal convergence rates and shows robust performance in network classification, edge detection, and community detection across synthetic and brain connectivity data.

Conclusion: ACERL is both theoretically grounded and empirically validated, achieving competitive or superior results compared to traditional methods like sparse PCA.

Abstract: Network representation learning seeks to embed networks into a
low-dimensional space while preserving the structural and semantic properties,
thereby facilitating downstream tasks such as classification, trait prediction,
edge identification, and community detection. Motivated by challenges in brain
connectivity data analysis that is characterized by subject-specific,
high-dimensional, and sparse networks that lack node or edge covariates, we
propose a novel contrastive learning-based statistical approach for network
edge embedding, which we name as Adaptive Contrastive Edge Representation
Learning (ACERL). It builds on two key components: contrastive learning of
augmented network pairs, and a data-driven adaptive random masking mechanism.
We establish the non-asymptotic error bounds, and show that our method achieves
the minimax optimal convergence rate for edge representation learning. We
further demonstrate the applicability of the learned representation in multiple
downstream tasks, including network classification, important edge detection,
and community detection, and establish the corresponding theoretical
guarantees. We validate our method through both synthetic data and real brain
connectivities studies, and show its competitive performance compared to the
baseline method of sparse principal components analysis.

</details>


### [127] [Next-Generation Reservoir Computing for Dynamical Inference](https://arxiv.org/abs/2509.11338)
*Rok Cestnik,Erik A. Martens*

Main category: stat.ML

TL;DR: The paper introduces a new implementation of next-generation reservoir computing for modeling dynamic systems, offering scalable, flexible, and stable frameworks applicable to various tasks like attractor reconstruction and bifurcation estimation.


<details>
  <summary>Details</summary>
Motivation: The need to improve reservoir computing methods for better scalability, flexibility, and performance in modeling dynamical systems from time series data.

Method: The paper proposes a pseudorandom nonlinear projection of time-delay embedded input, avoiding the limitations of traditional polynomial-based projections. Benchmark tasks are used to validate the approach.

Result: The proposed models perform well in attractor reconstruction and bifurcation diagram estimation, even with partial and noisy data. They also generalize beyond training data and maintain long-term stability.

Conclusion: The framework is highly adaptable for precise control of system states, making it suitable for surrogate modeling and digital twin applications.

Abstract: We present a simple and scalable implementation of next-generation reservoir
computing for modeling dynamical systems from time series data. Our approach
uses a pseudorandom nonlinear projection of time-delay embedded input, allowing
an arbitrary dimension of the feature space, thus providing a flexible
alternative to the polynomial-based projections used in previous
next-generation reservoir computing variants. We apply the method to benchmark
tasks -- including attractor reconstruction and bifurcation diagram estimation
-- using only partial and noisy observations. We also include an exploratory
example of estimating asymptotic oscillation phases. The models remain stable
over long rollouts and generalize beyond training data. This framework enables
the precise control of system state and is well suited for surrogate modeling
and digital twin applications.

</details>


### [128] [Some Robustness Properties of Label Cleaning](https://arxiv.org/abs/2509.11379)
*Chen Cheng,John Duchi*

Main category: stat.ML

TL;DR: The paper shows that machine learning methods relying on aggregated labels (e.g., labels derived from noisy responses) exhibit superior robustness and consistency compared to using raw labels. This is especially true in settings where task losses are mis-specified.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of standard machine learning methods that fail when task losses are slightly mis-specified, offering a methodology that incorporates data aggregation to improve outcomes.

Method: The approach involves leveraging aggregated label information instead of raw labels to refine noisy signals, ensuring better robustness and consistency in machine learning models.

Result: The study demonstrates that using aggregated labels leads to stronger consistency guarantees and assures convergence to optimal classifiers, even under loss mis-specification.

Conclusion: The findings underline the importance of considering the entire data analysis pipeline, from data collection to prediction, highlighting the benefits of incorporating aggregated information for more robust methodologies.

Abstract: We demonstrate that learning procedures that rely on aggregated labels, e.g.,
label information distilled from noisy responses, enjoy robustness properties
impossible without data cleaning. This robustness appears in several ways. In
the context of risk consistency -- when one takes the standard approach in
machine learning of minimizing a surrogate (typically convex) loss in place of
a desired task loss (such as the zero-one mis-classification error) --
procedures using label aggregation obtain stronger consistency guarantees than
those even possible using raw labels. And while classical statistical scenarios
of fitting perfectly-specified models suggest that incorporating all possible
information -- modeling uncertainty in labels -- is statistically efficient,
consistency fails for ``standard'' approaches as soon as a loss to be minimized
is even slightly mis-specified. Yet procedures leveraging aggregated
information still converge to optimal classifiers, highlighting how
incorporating a fuller view of the data analysis pipeline, from collection to
model-fitting to prediction time, can yield a more robust methodology by
refining noisy signals.

</details>


### [129] [A Particle-Flow Algorithm for Free-Support Wasserstein Barycenters](https://arxiv.org/abs/2509.11435)
*Kisung You*

Main category: stat.ML

TL;DR: The paper introduces a new free-support algorithm for computing Wasserstein barycenters using geometry-aware particle-flow updates, avoiding entropic regularization, and ensuring theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing methods for computing Wasserstein barycenters, which often rely on entropic regularization, leading to loss of sharp features.

Method: A particle-flow algorithm based on the Riemannian geometry of Wasserstein space, using averaged optimal-transport displacements and barycentric projections for updates.

Result: The proposed method is theoretically grounded with guarantees such as consistency, monotone descent, and stability. It demonstrates scalability and accuracy in various applications, including probability distribution averaging, image classification, and clustering.

Conclusion: The algorithm offers a computationally efficient, scalable, and principled approach to computing Wasserstein barycenters, providing an alternative to traditional solvers.

Abstract: The Wasserstein barycenter extends the Euclidean mean to the space of
probability measures by minimizing the weighted sum of squared 2-Wasserstein
distances. We develop a free-support algorithm for computing Wasserstein
barycenters that avoids entropic regularization and instead follows the formal
Riemannian geometry of Wasserstein space. In our approach, barycenter atoms
evolve as particles advected by averaged optimal-transport displacements, with
barycentric projections of optimal transport plans used in place of Monge maps
when the latter do not exist. This yields a geometry-aware particle-flow update
that preserves sharp features of the Wasserstein barycenter while remaining
computationally tractable. We establish theoretical guarantees, including
consistency of barycentric projections, monotone descent and convergence to
stationary points, stability with respect to perturbations of the inputs, and
resolution consistency as the number of atoms increases. Empirical studies on
averaging probability distributions, Bayesian posterior aggregation, image
prototypes and classification, and large-scale clustering demonstrate accuracy
and scalability of the proposed particle-flow approach, positioning it as a
principled alternative to both linear programming and regularized solvers.

</details>


### [130] [Learning Majority-to-Minority Transformations with MMD and Triplet Loss for Imbalanced Classification](https://arxiv.org/abs/2509.11511)
*Suman Cha,Hyunjoong Kim*

Main category: stat.ML

TL;DR: The paper proposes a novel oversampling method using parametric transformations and loss regularizers to better address class imbalance issues in supervised classification.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of class imbalance in supervised learning, which can degrade model performance, especially in critical fields like healthcare and fraud detection.

Method: The authors propose learning a parametric transformation to map majority samples into the minority distribution, minimizing the maximum mean discrepancy (MMD) for global distribution alignment and using a triplet loss regularizer to improve boundary awareness.

Result: The proposed method shows consistent improvement across 29 datasets (synthetic and real-world) in evaluation metrics like AUROC, G-mean, F1-score, and MCC, outperforming traditional oversampling techniques and generative baselines.

Conclusion: The method is robust, computationally efficient, and demonstrates practical utility for class imbalance issues in real-world datasets.

Abstract: Class imbalance in supervised classification often degrades model performance
by biasing predictions toward the majority class, particularly in critical
applications such as medical diagnosis and fraud detection. Traditional
oversampling techniques, including SMOTE and its variants, generate synthetic
minority samples via local interpolation but fail to capture global data
distributions in high-dimensional spaces. Deep generative models based on GANs
offer richer distribution modeling yet suffer from training instability and
mode collapse under severe imbalance. To overcome these limitations, we
introduce an oversampling framework that learns a parametric transformation to
map majority samples into the minority distribution. Our approach minimizes the
maximum mean discrepancy (MMD) between transformed and true minority samples
for global alignment, and incorporates a triplet loss regularizer to enforce
boundary awareness by guiding synthesized samples toward challenging borderline
regions. We evaluate our method on 29 synthetic and real-world datasets,
demonstrating consistent improvements over classical and generative baselines
in AUROC, G-mean, F1-score, and MCC. These results confirm the robustness,
computational efficiency, and practical utility of the proposed framework for
imbalanced classification tasks.

</details>


### [131] [E-ROBOT: a dimension-free method for robust statistics and machine learning via Schrödinger bridge](https://arxiv.org/abs/2509.11532)
*Davide La Vecchia,Hang Liu*

Main category: stat.ML

TL;DR: The E-ROBOT framework combines the robustness of ROBOT with the benefits of entropic regularization, defining a robust Sinkhorn divergence with favorable sample complexity.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of standard ROBOT methods, such as the curse of dimensionality, and aims to improve computational and statistical performance in high-dimensional tasks.

Method: The authors propose E-ROBOT, which integrates robustness parameters (λ) and regularization strength (ε) in defining a robust Sinkhorn divergence based on the Schrödinger bridge problem.

Result: The robust Sinkhorn divergence demonstrates a sample complexity of O(n^-1/2), allowing its usage in high-dimensional machine learning applications such as goodness-of-fit testing, barycenters for corrupted images, gradient flows, and color transfer.

Conclusion: E-ROBOT offers both theoretical advancements and practical ease of integration, opening up new pathways for research and applications in statistics and machine learning.

Abstract: We propose the Entropic-regularized Robust Optimal Transport (E-ROBOT)
framework, a novel method that combines the robustness of ROBOT with the
computational and statistical benefits of entropic regularization. We show
that, rooted in the Schr\"{o}dinger bridge problem theory, E-ROBOT defines the
robust Sinkhorn divergence $\overline{W}_{\varepsilon,\lambda}$, where the
parameter $\lambda$ controls robustness and $\varepsilon$ governs the
regularization strength. Letting $n\in \mathbb{N}$ denote the sample size, a
central theoretical contribution is establishing that the sample complexity of
$\overline{W}_{\varepsilon,\lambda}$ is $\mathcal{O}(n^{-1/2})$, thereby
avoiding the curse of dimensionality that plagues standard ROBOT. This
dimension-free property unlocks the use of $\overline{W}_{\varepsilon,\lambda}$
as a loss function in large-dimensional statistical and machine learning tasks.
With this regard, we demonstrate its utility through four applications:
goodness-of-fit testing; computation of barycenters for corrupted 2D and 3D
shapes; definition of gradient flows; and image colour transfer. From the
computation standpoint, a perk of our novel method is that it can be easily
implemented by modifying existing (\texttt{Python}) routines. From the
theoretical standpoint, our work opens the door to many research directions in
statistics and machine learning: we discuss some of them.

</details>


### [132] [SpaPool: Soft Partition Assignment Pooling for__Graph Neural Networks](https://arxiv.org/abs/2509.11675)
*Rodrigue Govan,Romane Scherrer,Philippe Fournier-Viger,Nazha Selmaoui-Folcher*

Main category: stat.ML

TL;DR: The paper introduces SpaPool, a new pooling method for graph neural networks combining dense and sparse techniques. It clusters vertices adaptively to maintain graph structure while reducing size effectively.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing graph pooling methods that may compromise structural integrity or efficiency.

Method: SpaPool utilizes an adaptive clustering mechanism that leverages both dense and sparse pooling techniques to preserve graph structure while reducing graph size.

Result: Experimental results show that SpaPool performs competitively with existing pooling methods, excelling on small-scale graphs.

Conclusion: SpaPool is a promising method for graph-related tasks, particularly in applications requiring efficient graph processing with maintained structural quality.

Abstract: This paper introduces SpaPool, a novel pooling method that combines the
strengths of both dense and sparse techniques for a graph neural network.
SpaPool groups vertices into an adaptive number of clusters, leveraging the
benefits of both dense and sparse approaches. It aims to maintain the
structural integrity of the graph while reducing its size efficiently.
Experimental results on several datasets demonstrate that SpaPool achieves
competitive performance compared to existing pooling techniques and excels
particularly on small-scale graphs. This makes SpaPool a promising method for
applications requiring efficient and effective graph processing.

</details>


### [133] [Identifiable Autoregressive Variational Autoencoders for Nonlinear and Nonstationary Spatio-Temporal Blind Source Separation](https://arxiv.org/abs/2509.11962)
*Mika Sipilä,Klaus Nordhausen,Sara Taskinen*

Main category: stat.ML

TL;DR: This paper introduces an identifiable autoregressive variational autoencoder for modeling multivariate spatio-temporal data, emphasizing blind source separation and spatio-temporal prediction performance.


<details>
  <summary>Details</summary>
Motivation: The need to address challenges in modeling complex spatio-temporal relationships in multivariate data and leveraging recent advances in nonlinear blind source separation.

Method: An identifiable autoregressive variational autoencoder is proposed, designed to handle nonstationary autoregressive processes while ensuring identifiability of latent components.

Result: Simulation studies and evaluations on air pollution and weather datasets show improved blind source separation and prediction performance compared to state-of-the-art methods.

Conclusion: The proposed method proves effective in handling complex spatio-temporal dependencies and demonstrates its potential as a robust tool in multivariate spatio-temporal modeling and prediction.

Abstract: The modeling and prediction of multivariate spatio-temporal data involve
numerous challenges. Dimension reduction methods can significantly simplify
this process, provided that they account for the complex dependencies between
variables and across time and space. Nonlinear blind source separation has
emerged as a promising approach, particularly following recent advances in
identifiability results. Building on these developments, we introduce the
identifiable autoregressive variational autoencoder, which ensures the
identifiability of latent components consisting of nonstationary autoregressive
processes. The blind source separation efficacy of the proposed method is
showcased through a simulation study, where it is compared against
state-of-the-art methods, and the spatio-temporal prediction performance is
evaluated against several competitors on air pollution and weather datasets.

</details>


### [134] [MMM: Clustering Multivariate Longitudinal Mixed-type Data](https://arxiv.org/abs/2509.12166)
*Francesco Amato,Julien Jacques*

Main category: stat.ML

TL;DR: The paper introduces the Mixture of Mixed-Matrices (MMM) model to cluster multivariate longitudinal mixed-type data by modeling dependencies and heterogeneity. An MCMC-EM algorithm is used for inference, with synthetic and real-world data evaluations presented.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of clustering algorithms capable of handling multivariate longitudinal data of mixed types due to the challenges in modeling complex data dependencies and types.

Method: The MMM model reorganizes data into a three-way structure, assumes non-continuous variables are latent continuous variables, and performs clustering via a mixture of matrix-variate normal distributions. Inference is achieved through an MCMC-EM algorithm.

Result: The proposed MMM model demonstrates inference abilities on synthetic data and is successfully applied to a real-world financial dataset, showcasing its utility and robustness.

Conclusion: The MMM model effectively clusters multivariate longitudinal mixed-type data, modeling heterogeneity, response associations, and temporal dependencies without conditional independence assumptions.

Abstract: Multivariate longitudinal data of mixed-type are increasingly collected in
many science domains. However, algorithms to cluster this kind of data remain
scarce, due to the challenge to simultaneously model the within- and
between-time dependence structures for multivariate data of mixed kind. We
introduce the Mixture of Mixed-Matrices (MMM) model: reorganizing the data in a
three-way structure and assuming that the non-continuous variables are
observations of underlying latent continuous variables, the model relies on a
mixture of matrix-variate normal distributions to perform clustering in the
latent dimension. The MMM model is thus able to handle continuous, ordinal,
binary, nominal and count data and to concurrently model the heterogeneity, the
association among the responses and the temporal dependence structure in a
parsimonious way and without assuming conditional independence. The inference
is carried out through an MCMC-EM algorithm, which is detailed. An evaluation
of the model through synthetic data shows its inference abilities. A real-world
application on financial data is presented.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [135] [Vital Signs Monitoring with mmWave OFDM JCAS System](https://arxiv.org/abs/2509.11767)
*Jakub Dobosz,Maximilian Engelhardt,Diego Dupleich,Maciej Stapor,Pawel Kulakowski*

Main category: cs.ET

TL;DR: This paper showcases the use of a 26.5 GHz bistatic OFDM wireless system to monitor human heart and breathing rates in different conditions, identifying factors impacting signal detection.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of wireless methods for monitoring human vital signs within joint communication and sensing (JCAS) applications.

Method: Experimental evaluation of an indoor bistatic OFDM JCAS system operating at 26.5 GHz frequency and up to 1 GHz bandwidth under various scenarios involving human subjects in line-of-sight and non-line-of-sight configurations.

Result: The system is generally effective in detecting vital signs but performance is influenced by subject clothing, activities, distance, and angle to the system. Bandwidth showed no significant impact, as vital information is phase-encoded.

Conclusion: Wireless detection of vital signs is feasible but performance is scenario-dependent, highlighting both the potential and limitations of the technology for real-world applications.

Abstract: Wireless techniques for monitoring human vital signs, such as heart and
breathing rates, offer a promising solution in the context of joint
communication and sensing (JCAS) with applications in medicine, sports, safety,
security, and even the military. This paper reports experimental results
obtained at the Fraunhofer Institute for Integrated Circuits in Ilmenau,
demonstrating the effectiveness of an indoor orthogonal frequency-division
multiplexing (OFDM) JCAS system for detecting human heart and breathing rates.
The system operated in a bistatic configuration at an FR2 frequency of 26.5 GHz
with a variable bandwidth of up to 1 GHz. Measurements were taken under various
scenarios, including a subject lying down, sitting, or walking, in both
line-of-sight and non-line-of-sight conditions, and with one or two subjects
present simultaneously. The results indicate that while vital sign detection is
generally feasible, its effectiveness is influenced by several factors, such as
the subjects clothing, activity, as well as the distance and angle relative to
the sensing system. In addition, no significant influence of bandwidth was
detected since the vital signs information is encoded in the phase of the
signal.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [136] [Geometric Phase of Stochastic Oscillators](https://arxiv.org/abs/2509.10900)
*Yangyang Du*

Main category: math-ph

TL;DR: The study explores the relationship between two phase definitions in stochastic oscillators using the geometric phase framework, providing a unified interpretation and insights into noise-induced oscillatory behavior.


<details>
  <summary>Details</summary>
Motivation: To provide a physical interpretation and understanding of the relationship between the mean-return-time phase and stochastic asymptotic phase in stochastic oscillators.

Method: The paper utilizes concepts from geometric phase, probability currents, and generalized Doob's h-transform to analyze and unify different phase definitions in stochastic oscillators.

Result: The findings show that the difference between the two phase definitions is governed by a geometric drift term analogous to curvature, bridging concepts in spectral theory, stochastic dynamics, and geometric phase.

Conclusion: The study establishes a unified framework for distinct phase definitions, offering insights into noise effects on oscillators and suggesting broader applicability to coupled stochastic and neural models.

Abstract: Several definitions of phase have been proposed for stochastic oscillators,
among which the mean-return-time phase and the stochastic asymptotic phase have
drawn particular attention. Quantitative comparisons between these two
definitions have been done in previous studies, but physical interpretations of
such a relation are still missing. In this work, we illustrate this relation
using the geometric phase, which is an essential concept in both classical and
quantum mechanics. We use properties of probability currents and the
generalized Doob's h-transform to explain how the geometric phase arises in
stochastic oscillators. Such an analogy is also reminiscent of the
noise-induced phase shift in oscillatory systems with deterministic
perturbation, allowing us to compare the phase responses in deterministic and
stochastic oscillators. The resulting framework unifies these distinct phase
definitions and reveals that their difference is governed by a geometric drift
term analogous to curvature. This interpretation bridges spectral theory,
stochastic dynamics, and geometric phase, and provides new insight into how
noise reshapes oscillatory behavior. Our results suggest broader applications
of geometric-phase concepts to coupled stochastic oscillators and neural
models.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [137] [When Purple Perceived Only at Fixation: A Fixation and Distance-Dependent Color Illusion](https://arxiv.org/abs/2509.11582)
*Hinnerk Schulz-Hildebrandt*

Main category: physics.optics

TL;DR: This paper introduces a new optical illusion where purple structures near the fixation point appear purple, but those farther away shift to a blue hue.


<details>
  <summary>Details</summary>
Motivation: To investigate and describe an optical illusion that deviates from typical color perception and depends on the distance of viewing.

Method: Observation of color perception changes for purple structures at varying viewing distances.

Result: The study finds that fixation point structures are perceived as purple, while remote structures exhibit a blue hue that reverts to purple as viewing distance increases.

Conclusion: Distance-dependent color perception phenomena are highlighted, revealing how spatial factors affect visual processing of identical colors.

Abstract: In this paper a novel optical illusion is described in which purple
structures are perceived as purple at the point of fixation, while the
surrounding structures of the same purple color are perceived toward a blue
hue. As the viewing distance increases, a greater number of purple structures
revert to a purple appearance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [138] [ILA: Correctness via Type Checking for Fully Homomorphic Encryption](https://arxiv.org/abs/2509.11559)
*Tarakaram Gollamudi,Anitha Gollamudi,Joshua Gancher*

Main category: cs.CR

TL;DR: The paper introduces a type system and intermediate language (IR) to statically track noise and handle errors in RLWE-based Fully Homomorphic Encryption (FHE) circuits.


<details>
  <summary>Details</summary>
Motivation: FHE circuits are prone to noise accumulation and wraparound errors, which are difficult to track and ensure correctness with existing libraries or compilers.

Method: This work proposes a correctness-oriented intermediate representation (IR) for arithmetic circuits, backed by a type system that tracks quantitative bounds such as ciphertext noise without requiring the secret key.

Result: The type system ensures strong functional correctness for FHE circuits, and the IR is general, working with multiple FHE schemes (BGV, BFV, TFHE) through an axiomatized FHE model.

Conclusion: The proposed method simplifies the development of FHE applications by ensuring correctness and compatibility across different FHE schemes.

Abstract: RLWE-based Fully Homomorphic Encryption (FHE) schemes add some small
\emph{noise} to the message during encryption. The noise accumulates with each
homomorphic operation. When the noise exceeds a critical value, the FHE circuit
produces an incorrect output. This makes developing FHE applications quite
subtle, as one must closely track the noise to ensure correctness. However,
existing libraries and compilers offer limited support to statically track the
noise. Additionally, FHE circuits are also plagued by wraparound errors that
are common in finite modulus arithmetic. These two limitations of existing
compilers and libraries make FHE applications too difficult to develop with
confidence.
  In this work, we present a \emph{correctness-oriented} IR, Intermediate
Language for Arithmetic circuits, for type-checking circuits intended for
homomorphic evaluation. Our IR is backed by a type system that tracks low-level
quantitative bounds (e.g., ciphertext noise) without using the secret key.
Using our type system, we identify and prove a strong \emph{functional
correctness} criterion for \ila circuits. Additionally, we have designed \ila
to be maximally general: our core type system does not directly assume a
particular FHE scheme, but instead axiomatizes a \emph{model} of FHE. We
instantiate this model with the exact FHE schemes (BGV, BFV and TFHE), and
obtain functional correctness for free.

</details>


### [139] [Side-channel Inference of User Activities in AR/VR Using GPU Profiling](https://arxiv.org/abs/2509.10703)
*Seonghun Son,Chandrika Mukherjee,Reham Mohamed Aburas,Berk Gulmezoglu,Z. Berkay Celik*

Main category: cs.CR

TL;DR: OVRWatcher introduces a side-channel attack exploiting low-resolution GPU metrics to infer user activities and preferences on AR/VR devices without concurrent app execution.


<details>
  <summary>Details</summary>
Motivation: To address security concerns on AR/VR devices where sensitive user information can be exposed through malicious monitoring techniques.

Method: OVRWatcher monitors low-resolution (1Hz) GPU usage via background scripts to infer user activities and interactions without requiring direct app data access or additional SDKs.

Result: OVRWatcher achieves over 99% accuracy in app fingerprinting and over 98% accuracy in object-level inference of user activities and preferences.

Conclusion: OVRWatcher underscores the need for enhanced security measures in AR/VR devices to mitigate the risks posed by side-channel attacks leveraging low-resolution GPU metrics.

Abstract: Over the past decade, AR/VR devices have drastically changed how we interact
with the digital world. Users often share sensitive information, such as their
location, browsing history, and even financial data, within third-party apps
installed on these devices, assuming a secure environment protected from
malicious actors. Recent research has revealed that malicious apps can exploit
such capabilities and monitor benign apps to track user activities, leveraging
fine-grained profiling tools, such as performance counter APIs. However,
app-to-app monitoring is not feasible on all AR/VR devices (e.g., Meta Quest),
as a concurrent standalone app execution is disabled. In this paper, we present
OVRWatcher, a novel side-channel primitive for AR/VR devices that infers user
activities by monitoring low-resolution (1Hz) GPU usage via a background
script, unlike prior work that relies on high-resolution profiling. OVRWatcher
captures correlations between GPU metrics and 3D object interactions under
varying speeds, distances, and rendering scenarios, without requiring
concurrent app execution, access to application data, or additional SDK
installations. We demonstrate the efficacy of OVRWatcher in fingerprinting both
standalone AR/VR and WebXR applications. OVRWatcher also distinguishes virtual
objects, such as products in immersive shopping apps selected by real users and
the number of participants in virtual meetings, thereby revealing users'
product preferences and potentially exposing confidential information from
those meetings. OVRWatcher achieves over 99% accuracy in app fingerprinting and
over 98% accuracy in object-level inference.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [140] [A Tree Clock Data Structure for Causal Orderings in Concurrent Executions](https://arxiv.org/abs/2201.06325)
*Umang Mathur,Andreas Pavlogiannis,Hünkar Can Tunç,Mahesh Viswanathan*

Main category: cs.LO

TL;DR: The paper introduces tree clocks as an optimized data structure to compute causal orders in concurrent program analysis. It greatly improves computation efficiency compared to traditional vector clocks, especially for a large number of threads.


<details>
  <summary>Details</summary>
Motivation: Vector clocks, though widely used to compute causal orderings in concurrent program executions, suffer from a computational bottleneck as their basic operations scale linearly with the number of threads.

Method: The authors propose tree clocks as a replacement for vector clocks. Joining and copying operations in tree clocks depend on the number of modified entries rather than the total thread count, reducing computational overhead.

Result: Tree clocks achieved significant speed improvements in benchmarks over vector clocks: 2.02x faster for Mazurkiewicz (MAZ), 2.66x faster for schedulable-happens-before (SHB), and 2.97x faster for happens-before (HB) analyses.

Conclusion: Tree clocks offer optimal, versatile, and efficient computation for causal orderings in concurrent programs, outperforming vector clocks and presenting broad applicability in concurrent program analyses.

Abstract: Dynamic techniques are a scalable and effective way to analyze concurrent
programs. Instead of analyzing all behaviors of a program, these techniques
detect errors by focusing on a single program execution. Often a crucial step
in these techniques is to define a causal ordering between events in the
execution, which is then computed using vector clocks, a simple data structure
that stores logical times of threads. The two basic operations of vector
clocks, namely join and copy, require $\Theta(k)$ time, where $k$ is the number
of threads. Thus they are a computational bottleneck when $k$ is large.
  In this work, we introduce tree clocks, a new data structure that replaces
vector clocks for computing causal orderings in program executions. Joining and
copying tree clocks takes time that is roughly proportional to the number of
entries being modified, and hence the two operations do not suffer the a-priori
$\Theta(k)$ cost per application. We show that when used to compute the classic
happens-before (HB) partial order, tree clocks are optimal, in the sense that
no other data structure can lead to smaller asymptotic running time. Moreover,
we demonstrate that tree clocks can be used to compute other partial orders,
such as schedulable-happens-before (SHB) and the standard Mazurkiewicz (MAZ)
partial order, and thus are a versatile data structure. Our experiments show
that just by replacing vector clocks with tree clocks, the computation becomes
from $2.02 \times$ faster (MAZ) to $2.66 \times$ (SHB) and $2.97 \times$ (HB)
on average per benchmark. These results illustrate that tree clocks have the
potential to become a standard data structure with wide applications in
concurrent analyses.

</details>


### [141] [Proceedings 9th edition of Working Formal Methods Symposium](https://arxiv.org/abs/2509.11877)
*Andrei Arusoaie,Horaţiu Cheval,Radu Iosif*

Main category: cs.LO

TL;DR: This document reports on the 9th Working Formal Methods Symposium held in Romania in 2025.


<details>
  <summary>Details</summary>
Motivation: To document and share findings, discussions, and advancements presented during the symposium.

Method: Compilation of proceedings from the symposium event held at Alexandru Ioan Cuza University.

Result: The 9th Working Formal Methods Symposium proceedings were successfully compiled and made available.

Conclusion: The symposium provided valuable contributions to the field of formal methods, demonstrating ongoing scholarly engagement.

Abstract: This volume contains the proceedings of the 9th Working Formal Methods
Symposium, which was held at the Alexandru Ioan Cuza University, Ia\c{s}i,
Romania on September 17-19, 2025.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [142] [Design and accuracy trade-offs in Computational Statistics](https://arxiv.org/abs/2509.10934)
*Tiancheng Xu,Alan L. Cox,Scott Rixner*

Main category: math.NA

TL;DR: This paper explores the use of the posit floating-point format for statistical computations in log-space, presenting significant improvements in accuracy, resource efficiency, and performance.


<details>
  <summary>Details</summary>
Motivation: Statistical computations in log-space can mitigate numerical underflow but are shown to have high costs in accuracy, performance, and resource utilization.

Method: Proposes using the posit floating-point format and performs comparative analysis with binary64 and logarithm representations in terms of operations, statistical applications, and FPGA implementations.

Result: Posit-based accelerators demonstrated up to two orders of magnitude higher accuracy, 60% resource utilization reduction, and 1.3x speedup over log-space accelerators on FPGA.

Conclusion: The posit format offers superior accuracy, resource efficiency, and performance for statistical computations and FPGA accelerators compared to traditional log-space methods.

Abstract: Statistical computations are becoming increasingly important. These
computations often need to be performed in log-space because probabilities
become extremely small due to repeated multiplications. While using logarithms
effectively prevents numerical underflow, this paper shows that its cost is
high in performance, resource utilization, and, notably, numerical accuracy.
This paper then argues that using posit, a recently proposed floating-point
format, is a better strategy for statistical computations operating on
extremely small numbers because of its unique encoding mechanism. To that end,
this paper performs a comprehensive analysis comparing posit, binary64, and
logarithm representations, examining both individual arithmetic operations,
statistical bioinformatics applications, and their accelerators. FPGA
implementation results highlight that posit-based accelerators can achieve up
to two orders of magnitude higher accuracy, up to 60\% lower resource
utilization, and up to $1.3\times$ speedup, compared to log-space accelerators.
Such improvement translates to $2\times$ performance per unit resource on the
FPGA.

</details>
