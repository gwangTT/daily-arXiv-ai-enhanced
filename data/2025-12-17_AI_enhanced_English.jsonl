{"id": "2512.13700", "pdf": "https://arxiv.org/pdf/2512.13700", "abs": "https://arxiv.org/abs/2512.13700", "authors": ["Mitchell A. Klusty", "Elizabeth C. Solie", "Caroline N. Leach", "W. Vaiden Logan", "Lynnet E. Richey", "John C. Gensel", "David P. Szczykutowicz", "Bryan C. McLellan", "Emily B. Collier", "Samuel E. Armstrong", "V. K. Cody Bumgardner"], "title": "Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records", "categories": ["cs.AI", "cs.CL"], "comment": "9 pages, 2 figures, 2 tables, submitted to AMIA 2026 Informatics Summit", "summary": "Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, modular framework for automated structured feature extraction from clinical notes leveraging locally deployed large language models (LLMs) on institutionally approved, Health Insurance Portability and Accountability Act (HIPPA)-compliant compute infrastructure. This system integrates retrieval augmented generation (RAG) and structured response methods of LLMs into a widely deployable and scalable container to provide feature extraction for diverse clinical domains. In evaluation, the framework achieved high accuracy across multiple medical characteristics present in large bodies of patient notes when compared against an expert-annotated dataset and identified several annotation errors missed in manual review. This framework demonstrates the potential of LLM systems to reduce the burden of manual chart review through automated extraction and increase consistency in data capture, accelerating clinical research.", "AI": {"tldr": "The paper introduces a modular framework utilizing large language models (LLMs) for automated structured feature extraction from clinical notes, aiming to reduce manual chart review efforts.", "motivation": "To address the burdens of manual chart review in clinical research, which involves extracting complex information from unstructured electronic health record (EHR) narratives.", "method": "Developing a secure framework with locally deployed LLMs on HIPPA-compliant infrastructure, integrating retrieval augmented generation (RAG) and structured response methods for feature extraction.", "result": "High accuracy achieved across multiple medical characteristics compared to expert-annotated datasets; also identified annotation errors missed during manual review.", "conclusion": "The framework showcases the potential of LLM-based systems to automate chart reviews, enhance consistency in capturing data, and accelerate clinical research efforts."}}
{"id": "2512.13701", "pdf": "https://arxiv.org/pdf/2512.13701", "abs": "https://arxiv.org/abs/2512.13701", "authors": ["Zheng Xing", "Junting Chen"], "title": "Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference", "categories": ["cs.AI", "cs.IT"], "comment": null, "summary": "Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-world scenarios. This paper presents a blind radio map construction framework that infers user trajectories from indoor multiple-input multiple-output (MIMO)-Orthogonal Frequency-Division Multiplexing (OFDM) channel measurements without relying on location labels. It first proves that channel state information (CSI) under non-line-of-sight (NLOS) exhibits spatial continuity under a quasi-specular environmental model, allowing the derivation of a CSI-distance metric that is proportional to the corresponding physical distance. For rectilinear trajectories in Poisson-distributed access point (AP) deployments, it is shown that the Cramer-Rao Lower Bound (CRLB) of localization error vanishes asymptotically, even under poor angular resolution. Building on these theoretical results, a spatially regularized Bayesian inference framework is developed that jointly estimates channel features, distinguishes line-of-sight (LOS)/NLOS conditions and recovers user trajectories. Experiments on a ray-tracing dataset demonstrate an average localization error of 0.68 m and a beam map reconstruction error of 3.3%, validating the effectiveness of the proposed blind mapping method.", "AI": {"tldr": "This paper introduces a framework for creating radio maps without location labels using MIMO-OFDM channel measurements, achieving notable accuracy in trajectory localization.", "motivation": "Existing methods for constructing radio maps rely on costly and impractical location-labeled data, necessitating novel approaches to overcome this limitation.", "method": "The authors propose proving spatial continuity of CSI under NLOS and developing a Bayesian inference framework to estimate trajectories and channel features using blind mapping.", "result": "Their method achieved an average localization error of 0.68 m and a beam map reconstruction error of 3.3% on ray-tracing datasets.", "conclusion": "The proposed blind radio map construction approach demonstrates viability and effectiveness in creating accurate wireless channel maps without relying on location-labeled data."}}
{"id": "2512.13704", "pdf": "https://arxiv.org/pdf/2512.13704", "abs": "https://arxiv.org/abs/2512.13704", "authors": ["Doohee You", "Sundeep Paul"], "title": "Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents", "categories": ["cs.AI"], "comment": "12 pages, 3 figures", "summary": "The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correcting label noise and has been validated for production deployment. Adjudicator models this as a neuro-symbolic task, first constructing a dynamic Knowledge Graph (KG) to unify item context. This KG then informs a \"Council of Agents,\" a novel multi-agent Large Language Model architecture where specialized agents debate and vote on a label's validity. We validate our system on a 1,000-item balanced subset of the AlleNoise benchmark. Our KG-informed model achieves a 0.99 F1-score, significantly outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). Our analysis reveals this is due to a Precision, achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors (complete Recall) -- a class of errors that baselines fail to find. This result demonstrates a robust and explainable system for automated, high-precision data verification, serving as a vital proof-of-concept for generating golden datasets in strictly governed industrial environments.", "AI": {"tldr": "Adjudicator, a neuro-symbolic system for identifying and correcting label noise in production ML systems, achieves a 0.99 F1-score using a novel KG and multi-agent architecture, outperforming baselines.", "motivation": "Noisy labels in ML systems impact performance and trust, especially in industrial applications, necessitating a solution for automated label noise correction.", "method": "Adjudicator combines a Knowledge Graph (KG) for item context and a \"Council of Agents\" multi-agent LLM architecture to evaluate label validity.", "result": "Adjudicator achieves a 0.99 F1-score on the AlleNoise benchmark, surpassing single-LLM (0.48 F1) and non-KG council (0.59 F1) baselines by resolving complex structural label errors.", "conclusion": "The system demonstrates a robust and explainable data verification solution, paving the way for creating golden datasets in regulated industrial environments."}}
{"id": "2512.13713", "pdf": "https://arxiv.org/pdf/2512.13713", "abs": "https://arxiv.org/abs/2512.13713", "authors": ["Ali Parsaee", "Yashar Talebirad", "Csongor Szepesv\u00e1ri", "Vishwajeet Ohal", "Eden Redman"], "title": "LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": "11 pages, 3 figures, submitted to ANTS 2026", "summary": "Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \\textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed symmetry breaking and meta-cognitive thinking. The benchmark focuses on coloring odd cycle graphs ($C_3, C_5, C_{11}$) with limited colors, where deterministic, non-communicating agents fail in infinite loops. A strategy passing mechanism is implemented as a form of consistent memory. We show that while standard LLMs and classical heuristics struggle, advanced reasoning models (e.g., O3) devise strategies to escape deadlocks. LoopBench allows the study of emergent distributed algorithms based on language-based reasoning, offering a testbed for collective intelligence.", "AI": {"tldr": "The paper introduces LoopBench, a benchmark to evaluate LLM coordination in distributed systems, focusing on overcoming infinite loops in graph coloring tasks using advanced reasoning models.", "motivation": "The use of LLMs as autonomous agents in distributed systems lacks understanding, particularly in symmetry breaking tasks and strategies to avoid pitfalls like infinite loops.", "method": "The benchmark evaluates reasoning using graph coloring tasks under constraints, with strategy passing mechanisms representing consistent memory. It tests various models, including advanced reasoning ones.", "result": "Standard models and classical heuristics fail, but advanced reasoning models (e.g., O3) show effectiveness in devising strategies to overcome deadlocks.", "conclusion": "LoopBench serves as a tool to test emergent distributed algorithms and study collective intelligence through language-based reasoning in distributed systems."}}
{"id": "2512.13696", "pdf": "https://arxiv.org/pdf/2512.13696", "abs": "https://arxiv.org/abs/2512.13696", "authors": ["Md Shahabub Alam", "Md Asifuzzaman Jishan", "Ayan Kumar Ghosh"], "title": "Physics-Guided Deep Learning for Heat Pump Stress Detection: A Comprehensive Analysis on When2Heat Dataset", "categories": ["cs.LG", "cs.CV", "cs.NE"], "comment": null, "summary": "Heat pump systems are critical components in modern energy-efficient buildings, yet their operational stress detection remains challenging due to complex thermodynamic interactions and limited real-world data. This paper presents a novel Physics-Guided Deep Neural Network (PG-DNN) approach for heat pump stress classification using the When2Heat dataset, containing 131,483 samples with 656 features across 26 European countries. The methodology integrates physics-guided feature selection and class definition with a deep neural network architecture featuring 5 hidden layers and dual regularization strategies. The model achieves 78.1\\% test accuracy and 78.5% validation accuracy, demonstrating significant improvements over baseline approaches: +5.0% over shallow networks, +4.0% over limited feature sets, and +2.0% over single regularization strategies. Comprehensive ablation studies validate the effectiveness of physics-guided feature selection, variable thresholding for realistic class distribution, and cross-country energy pattern analysis. The proposed system provides a production-ready solution for heat pump stress detection with 181,348 parameters and 720 seconds training time on AMD Ryzen 9 7950X with RTX 4080 hardware.", "AI": {"tldr": "The paper proposes a Physics-Guided Deep Neural Network (PG-DNN) for detecting heat pump stress, achieving high accuracy and outperforming baseline models.", "motivation": "The research aims to address the difficulty of detecting operational stress in heat pump systems due to their complex thermodynamic interactions and scarce real-world data.", "method": "The study employs a novel PG-DNN that incorporates physics-guided feature selection, class definitions, and a deep neural network with 5 hidden layers and dual regularization techniques, utilizing the When2Heat dataset.", "result": "The PG-DNN model achieved 78.1% test accuracy and 78.5% validation accuracy, outperforming baseline methods by 2-5%. Ablation studies confirmed the contribution of each methodological component.", "conclusion": "The proposed PG-DNN method offers a highly accurate, efficient, and scalable solution for stress detection in heat pump systems, applicable across diverse geographical regions."}}
{"id": "2512.14151", "pdf": "https://arxiv.org/pdf/2512.14151", "abs": "https://arxiv.org/abs/2512.14151", "authors": ["Songze Liu", "Hongkun Du", "Shaowen Wang"], "title": "Adaptive Cache Pollution Control for Large Language Model Inference Workloads Using Temporal CNN-Based Prediction and Priority-Aware Replacement", "categories": ["cs.AR", "cs.PF"], "comment": null, "summary": "Large Language Models (LLMs), such as GPT and LLaMA, introduce unique memory access characteristics during inference due to frequent token sequence lookups and embedding vector retrievals. These workloads generate highly irregular and bursty access patterns, causing traditional prefetching and replacement policies to mispredict and trigger severe cache pollution, thereby degrading system performance. To address this challenge, this paper proposes an Adaptive Cache Pollution Control (ACPC) mechanism tailored for LLM inference workloads, integrating Temporal Convolutional Network (TCN)-based access prediction with a priority-aware replacement strategy. The TCN module learns temporal dependencies in token access sequences to identify potential high-reuse cache lines, while the replacement policy dynamically adjusts eviction priorities based on predicted reuse likelihood and cache occupancy. The proposed framework is implemented and evaluated on representative transformer-based inference traces, including GPT-style autoregressive decoding and embedding retrieval workloads. Experimental results demonstrate that ACPC reduces cache pollution by 41.7 percent, improves cache hit rate by 8.9 percent, and achieves a 60.0 percent reduction in L2 miss penalty, compared with state-of-the-art machine-learning-based replacement baselines. Additionally, the proposed Temporal CNN-based ACPC framework increases token generation throughput by 15.9 percent and achieves the lowest final loss of 0.21, confirming its superior efficiency and stability under complex LLM inference workloads. These results highlight ACPC's effectiveness in recognizing useful cache lines and mitigating redundant prefetches under dynamic LLM access behaviors. The proposed approach provides a scalable, learning-driven solution for optimizing memory efficiency and latency in large-scale LLM serving and inference systems.", "AI": {"tldr": "The paper introduces an Adaptive Cache Pollution Control (ACPC) method to mitigate memory inefficiencies during Large Language Model inference by using Temporal Convolutional Networks (TCN) for prediction and priority-aware replacements. This approach significantly enhances cache performance and system throughput.", "motivation": "To address inefficiencies in memory access during LLM inference workloads caused by irregular and bursty access patterns which degrade performance by triggering cache pollution.", "method": "The proposed method integrates a Temporal Convolutional Network (TCN)-based access prediction model with a priority-aware cache replacement strategy to dynamically manage cache eviction and priorities.", "result": "ACPC reduces cache pollution by 41.7%, improves cache hit rate by 8.9%, decreases L2 miss penalty by 60.0%, boosts token generation throughput by 15.9%, and achieves the lowest inference loss of 0.21 compared to other baselines.", "conclusion": "ACPC significantly improves performance and memory efficiency during LLM inference, offering a scalable and stable solution for optimizing large-scale LLM serving systems."}}
{"id": "2512.14045", "pdf": "https://arxiv.org/pdf/2512.14045", "abs": "https://arxiv.org/abs/2512.14045", "authors": ["Omar Abusabha", "Jiyong Uhm", "Tamer Abuhmed", "Hyungjoon Koo"], "title": "A Deep Dive into Function Inlining and its Security Implications for ML-based Binary Analysis", "categories": ["cs.CR", "cs.LG", "cs.PL"], "comment": null, "summary": "A function inlining optimization is a widely used transformation in modern compilers, which replaces a call site with the callee's body in need. While this transformation improves performance, it significantly impacts static features such as machine instructions and control flow graphs, which are crucial to binary analysis. Yet, despite its broad impact, the security impact of function inlining remains underexplored to date. In this paper, we present the first comprehensive study of function inlining through the lens of machine learning-based binary analysis. To this end, we dissect the inlining decision pipeline within the LLVM's cost model and explore the combinations of the compiler options that aggressively promote the function inlining ratio beyond standard optimization levels, which we term extreme inlining. We focus on five ML-assisted binary analysis tasks for security, using 20 unique models to systematically evaluate their robustness under extreme inlining scenarios. Our extensive experiments reveal several significant findings: i) function inlining, though a benign transformation in intent, can (in)directly affect ML model behaviors, being potentially exploited by evading discriminative or generative ML models; ii) ML models relying on static features can be highly sensitive to inlining; iii) subtle compiler settings can be leveraged to deliberately craft evasive binary variants; and iv) inlining ratios vary substantially across applications and build configurations, undermining assumptions of consistency in training and evaluation of ML models.", "AI": {"tldr": "This paper studies the impact of function inlining optimization in compilers on machine learning-based binary analysis, analyzing its security implications and effects on ML models.", "motivation": "To explore the security and robustness implications of function inlining in compilers, especially its unexplored effects on ML-based binary analysis.", "method": "The paper leverages LLVM's cost model to study function inlining and examines the effects of compiler settings promoting extreme inlining. It evaluates five ML-assisted security tasks using 20 models under these scenarios.", "result": "Experiments show function inlining impacts model behavior significantly, affecting discriminative and generative ML tasks, and compiler settings can craft evasive binaries. Inlining ratios varied broadly, challenging consistency in ML models used for security analysis.", "conclusion": "Function inlining, while improving performance, poses challenges to ML-based binary analysis by altering static features, impacting model robustness and exposing security vulnerabilities."}}
{"id": "2512.14002", "pdf": "https://arxiv.org/pdf/2512.14002", "abs": "https://arxiv.org/abs/2512.14002", "authors": ["Chuanchao Gao", "Arvind Easwaran"], "title": "Real-Time Service Subscription and Adaptive Offloading Control in Vehicular Edge Computing", "categories": ["cs.DC", "cs.DM"], "comment": "Accepted in 2025 IEEE Real-Time Systems Symposium (RTSS)", "summary": "Vehicular Edge Computing (VEC) has emerged as a promising paradigm for enhancing the computational efficiency and service quality in intelligent transportation systems by enabling vehicles to wirelessly offload computation-intensive tasks to nearby Roadside Units. However, efficient task offloading and resource allocation for time-critical applications in VEC remain challenging due to constrained network bandwidth and computational resources, stringent task deadlines, and rapidly changing network conditions. To address these challenges, we formulate a Deadline-Constrained Task Offloading and Resource Allocation Problem (DOAP), denoted as $\\mathbf{P}$, in VEC with both bandwidth and computational resource constraints, aiming to maximize the total vehicle utility. To solve $\\mathbf{P}$, we propose $\\mathtt{SARound}$, an approximation algorithm based on Linear Program rounding and local-ratio techniques, that improves the best-known approximation ratio for DOAP from $\\frac{1}{6}$ to $\\frac{1}{4}$. Additionally, we design an online service subscription and offloading control framework to address the challenges of short task deadlines and rapidly changing wireless network conditions. To validate our approach, we develop a comprehensive VEC simulator, VecSim, using the open-source simulation libraries OMNeT++ and Simu5G. VecSim integrates our designed framework to manage the full life-cycle of real-time vehicular tasks. Experimental results, based on profiled object detection applications and real-world taxi trace data, show that $\\mathtt{SARound}$ consistently outperforms state-of-the-art baselines under varying network conditions while maintaining runtime efficiency.", "AI": {"tldr": "This paper proposes a novel solution, SARound, and a framework to optimize task offloading and resource allocation in Vehicular Edge Computing (VEC) systems, significantly improving efficiency and utility.", "motivation": "The motivation lies in addressing the challenges of constrained network bandwidth, computational resources, strict task deadlines, and dynamic network conditions in VEC, aiming to optimize computational efficiency and service quality for intelligent transportation systems.", "method": "The authors formulate a Deadline-Constrained Task Offloading and Resource Allocation Problem (DOAP) with constraints and propose SARound, an algorithm using Linear Program rounding and local-ratio techniques, alongside an online framework for handling task deadlines and dynamic network changes. They also utilize a simulator called VecSim.", "result": "Experimental results demonstrate that the proposed SARound algorithm surpasses state-of-the-art methods in maximizing vehicle utility under different network conditions and is efficient in runtime.", "conclusion": "The study provides a robust approach for task offloading and resource allocation in VEC, achieving better approximation results and utility outcomes, verified through real-world simulations."}}
{"id": "2512.13830", "pdf": "https://arxiv.org/pdf/2512.13830", "abs": "https://arxiv.org/abs/2512.13830", "authors": ["Chaima Boufaied", "Thanh Nguyen", "Ronnie de Souza Santos"], "title": "Practitioner Insights on Fairness Requirements in the AI Development Life Cycle: An Interview Study", "categories": ["cs.SE"], "comment": null, "summary": "Nowadays, Artificial Intelligence (AI), particularly Machine Learning (ML) and Large Language Models (LLMs), is widely applied across various contexts. However, the corresponding models often operate as black boxes, leading them to unintentionally act unfairly towards different demographic groups. This has led to a growing focus on fairness in AI software recently, alongside the traditional focus on the effectiveness of AI models. Through 26 semi-structured interviews with practitioners from different application domains and with varied backgrounds across 23 countries, we conducted research on fairness requirements in AI from software engineering perspective. Our study assesses the participants' awareness of fairness in AI / ML software and its application within the Software Development Life Cycle (SDLC), from translating fairness concerns into requirements to assessing their arising early in the SDLC. It also examines fairness through the key assessment dimensions of implementation, validation, evaluation, and how it is balanced with trade-offs involving other priorities, such as addressing all the software functionalities and meeting critical delivery deadlines. Findings of our thematic qualitative analysis show that while our participants recognize the aforementioned AI fairness dimensions, practices are inconsistent, and fairness is often deprioritized with noticeable knowledge gaps. This highlights the need for agreement with relevant stakeholders on well-defined, contextually appropriate fairness definitions, the corresponding evaluation metrics, and formalized processes to better integrate fairness into AI/ML projects.", "AI": {"tldr": "The paper identifies inconsistencies, knowledge gaps, and deprioritization of fairness in AI/ML projects, emphasizing a need for better stakeholder alignment and formalized fairness processes.", "motivation": "AI, particularly ML and LLMs, often operates as a black box, inadvertently leading to fairness issues that affect demographic groups, which is a growing concern across AI applications.", "method": "Conducted 26 semi-structured interviews with practitioners from 23 countries across various domains to understand fairness requirements, integrating it into the Software Development Life Cycle (SDLC).", "result": "Participants recognize fairness issues and dimensions but report inconsistencies in practices, deprioritization of fairness, and noticeable knowledge gaps throughout the SDLC.", "conclusion": "To improve fairness in AI/ML, formalized processes, well-defined fairness definitions, and evaluation metrics are crucial, requiring alignment with stakeholders and integration early in the SDLC."}}
{"id": "2512.13884", "pdf": "https://arxiv.org/pdf/2512.13884", "abs": "https://arxiv.org/abs/2512.13884", "authors": ["Jonas Golde", "Patrick Haller", "Alan Akbik"], "title": "FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition", "categories": ["cs.CL"], "comment": null, "summary": "Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacher-student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. Our experiments show that the regression model achieves more than 84 F1, and that models trained on FiNERweb obtain comparable or improved performance in zero shot transfer settings on English, Thai, and Swahili, despite being trained on 19x less data than strong baselines. In addition, we assess annotation quality using LLM-as-a-judge and observe consistently high scores for both faithfulness (3.99 out of 5) and completeness (4.05 out of 5), indicating reliable and informative annotations. Further, we release the dataset with both English labels and translated label sets in the respective target languages because we observe that the performance of current state-of-the-art models drops by 0.02 to 0.09 F1 when evaluated using target language labels instead of English ones. We release FiNERweb together with all accompanying artifacts to the research community in order to facilitate more effective student-teacher training for multilingual named entity recognition.", "AI": {"tldr": "The paper introduces FiNERweb, a scalable pipeline for multilingual named entity recognition (NER) using teacher-student paradigm, achieving comparable or better performance with less data and providing high-quality annotations.", "motivation": "The paper aims to address the lack of reusable systematic datasets in multilingual NER by introducing a robust pipeline for dataset creation and evaluation across diverse languages and scripts.", "method": "The study utilizes a regression model to identify NER-relevant passages and multilingual LLMs for annotation across 91 languages and 25 scripts. They assess annotation quality using benchmarks like faithfulness and completeness.", "result": "FiNERweb produces 225k passages with 235k distinct entity labels, achieving over 84 F1 for regression model performance and improved zero-shot transfer performance on languages like English, Thai, and Swahili.", "conclusion": "FiNERweb proves effective in creating high-quality multilingual NER datasets, demonstrating reliability in annotations and advancing student-teacher training for diverse language models. It is released to benefit the research community."}}
{"id": "2512.13903", "pdf": "https://arxiv.org/pdf/2512.13903", "abs": "https://arxiv.org/abs/2512.13903", "authors": ["Sibo Tian", "Minghui Zheng", "Xiao Liang"], "title": "PrediFlow: A Flow-Based Prediction-Refinement Framework for Real-Time Human Motion Prediction in Human-Robot Collaboration", "categories": ["cs.RO"], "comment": null, "summary": "Stochastic human motion prediction is critical for safe and effective human-robot collaboration (HRC) in industrial remanufacturing, as it captures human motion uncertainties and multi-modal behaviors that deterministic methods cannot handle. While earlier works emphasize highly diverse predictions, they often generate unrealistic human motions. More recent methods focus on accuracy and real-time performance, yet there remains potential to improve prediction quality further without exceeding time budgets. Additionally, current research on stochastic human motion prediction in HRC typically considers human motion in isolation, neglecting the influence of robot motion on human behavior. To address these research gaps and enable real-time, realistic, and interaction-aware human motion prediction, we propose a novel prediction-refinement framework that integrates both human and robot observed motion to refine the initial predictions produced by a pretrained state-of-the-art predictor. The refinement module employs a Flow Matching structure to account for uncertainty. Experimental studies on the HRC desktop disassembly dataset demonstrate that our method significantly improves prediction accuracy while preserving the uncertainties and multi-modalities of human motion. Moreover, the total inference time of the proposed framework remains within the time budget, highlighting the effectiveness and practicality of our approach.", "AI": {"tldr": "The paper addresses challenges in stochastic human motion prediction for human-robot collaboration by proposing a novel framework that integrates human and robot motion data to refine predictions, improving accuracy and maintaining real-time performance.", "motivation": "The research aims to improve upon challenges in stochastic human motion prediction, such as unrealistic predictions or performance trade-offs. It also seeks to incorporate the influence of robot motion on human behavior, which is often overlooked.", "method": "The authors introduce a prediction-refinement framework that utilizes a Flow Matching structure to account for motion uncertainties. This framework integrates both human and robot observed motion data, refining predictions generated by a pretrained state-of-the-art predictor.", "result": "Experiments on the HRC desktop disassembly dataset show that the proposed method enhances prediction accuracy while preserving motion uncertainties and multi-modalities, with the framework staying within time constraints.", "conclusion": "The proposed approach effectively enhances stochastic human motion prediction quality for HRC scenarios, showcasing its practicality in maintaining real-time and realistic interaction-aware predictions."}}
{"id": "2512.13892", "pdf": "https://arxiv.org/pdf/2512.13892", "abs": "https://arxiv.org/abs/2512.13892", "authors": ["Albert Dorador"], "title": "One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.", "AI": {"tldr": "This paper proposes a deterministic, efficient, and stable permutation-based method for estimating feature importance in machine learning models, validated across multiple scenarios.", "motivation": "The motivation is to enhance trust, transparency, and regulatory compliance in black box machine learning models by improving feature importance estimation methods.", "method": "A deterministic and optimal permutation replaces multiple random permutations. Additionally, Systemic Variable Importance framework is introduced to quantify feature correlation effects.", "result": "Improved bias-variance tradeoff and accuracy, especially in challenging scenarios like small datasets or high-dimensional problems. Validation across nearly 200 scenarios, including practical applications in finance and credit risk.", "conclusion": "This approach offers computational efficiency, reduced randomness, fair and transparent model audits, allowing regulators to evaluate systemic risks and hidden biases effectively."}}
{"id": "2512.13731", "pdf": "https://arxiv.org/pdf/2512.13731", "abs": "https://arxiv.org/abs/2512.13731", "authors": ["Weikang Bai", "Yongkun Du", "Yuchen Su", "Yazhen Xie", "Zhineng Chen"], "title": "Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mathematical Expression Recognition (MER) has made significant progress in recognizing simple expressions, but the robust recognition of complex mathematical expressions with many tokens and multiple lines remains a formidable challenge. In this paper, we first introduce CMER-Bench, a carefully constructed benchmark that categorizes expressions into three difficulty levels: easy, moderate, and complex. Leveraging CMER-Bench, we conduct a comprehensive evaluation of existing MER models and general-purpose multimodal large language models (MLLMs). The results reveal that while current methods perform well on easy and moderate expressions, their performance degrades significantly when handling complex mathematical expressions, mainly because existing public training datasets are primarily composed of simple samples. In response, we propose MER-17M and CMER-3M that are large-scale datasets emphasizing the recognition of complex mathematical expressions. The datasets provide rich and diverse samples to support the development of accurate and robust complex MER models. Furthermore, to address the challenges posed by the complicated spatial layout of complex expressions, we introduce a novel expression tokenizer, and a new representation called Structured Mathematical Language, which explicitly models the hierarchical and spatial structure of expressions beyond LaTeX format. Based on these, we propose a specialized model named CMERNet, built upon an encoder-decoder architecture and trained on CMER-3M. Experimental results show that CMERNet, with only 125 million parameters, significantly outperforms existing MER models and MLLMs on CMER-Bench.", "AI": {"tldr": "The paper addresses the difficulty of recognizing complex mathematical expressions in Mathematical Expression Recognition (MER) and proposes new benchmarks, datasets, and a specialized model, CMERNet, which delivers promising results.", "motivation": "To overcome the limited robustness of current MER methods in recognizing complex mathematical expressions due to a lack of sufficient training data and inadequate modeling approaches for their hierarchical and spatial structures.", "method": "The authors introduce CMER-Bench for evaluation, construct large-scale datasets (MER-17M and CMER-3M) focused on complex mathematical expressions, propose a novel expression tokenizer and Structured Mathematical Language representation, and develop a specialized model named CMERNet built on an encoder-decoder architecture.", "result": "CMERNet, with 125 million parameters, outperforms existing MER models and multimodal large language models on the newly created CMER-Bench in recognizing complex mathematical expressions.", "conclusion": "The study enhances the robustness of MER through the introduction of new benchmarks, datasets, and an advanced specialized model (CMERNet), overcoming current limitations and significantly improving performance in complex expression recognition."}}
{"id": "2512.13859", "pdf": "https://arxiv.org/pdf/2512.13859", "abs": "https://arxiv.org/abs/2512.13859", "authors": ["Daiki Goto", "Hector Manuel Lopez Rios", "Monika Scholz", "Suriyanarayanan Vaikuntanathan"], "title": "Neuromodulation-inspired gated associative memory networks:extended memory retrieval and emergent multistability", "categories": ["q-bio.NC", "cond-mat.stat-mech"], "comment": null, "summary": "Classical autoassociative memory models have been central to understanding emergent computations in recurrent neural circuits across diverse biological contexts. However, they typically neglect neuromodulatory agents that are known to strongly shape memory capacity and stability. Here we introduce a minimal, biophysically motivated associative memory network where neuropeptide-like signals are modeled by a self-adaptive, activity-dependent gating mechanism. Using many-body simulations and dynamical mean-field theory, we show that such gating fundamentally reorganizes the attractor structure: the network bypasses the classical spin-glass transition, maintaining robust, high-overlap retrieval far beyond the standard critical capacity, without shrinking basins of attraction. Mechanistically, the gate stabilizes transient ghost remnants of stored patterns even far above the Hopfield limit, converting them into multistable attractors. These results demonstrate that neuromodulation-like gating alone can dramatically enhance associative memory capacity, eliminate the sharp Hopfield-style catastrophic breakdown, and reshape the memory landscape, providing a simple, general route to richer memory dynamics and computational capabilities in neuromodulated circuits and neuromorphic architectures.", "AI": {"tldr": "The paper presents how a neuromodulation-inspired gating mechanism enhances associative memory capacity and stability in neural networks.", "motivation": "Traditional autoassociative memory models overlook the role of neuromodulatory agents that significantly shape memory dynamics and capacity.", "method": "A biophysically-driven associative memory network with a self-adaptive, activity-dependent gating mechanism was introduced and analyzed using simulations and dynamical mean-field theory.", "result": "The gating mechanism reorganizes the attractor structure, surpassing classical capacity limits while maintaining retrieval stability, even well above Hopfield limits.", "conclusion": "Neuromodulation-like gating profoundly improves associative memory dynamics, avoiding catastrophic breakdown and enabling richer computational functionality in neural circuits."}}
{"id": "2512.13714", "pdf": "https://arxiv.org/pdf/2512.13714", "abs": "https://arxiv.org/abs/2512.13714", "authors": ["Gangesh Pathak", "Prasanna Kumar"], "title": "AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach", "categories": ["cs.AI"], "comment": "16 Pages", "summary": "LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that need the precision of facts and consistent behavior (Aiyappa et al., 2023). The current methods of stabilization, such as, reinforcement learning with human feedback (RLHF) and supervised fine-tuning, offer quantifiable improvements but are expensive and based on the intensive annotation of humans, thus being not easily scaled in a sustainable way (Dong et al., 2023; Retzlaff et al., 2024). This paper presents an AI-based annotation pipeline that systematically identifies, labels, and fixes for instability patterns on LLM output. Our human-AI synergy method combines the models of automated weak supervision and confidence-based annotation with the target human validation to guarantee the reliability and moral uprightness of feedback information (Cabitza et al., 2023; Jiang et al., 2023). The semantic consistency, factual correctness, and logical coherence categories of stability-specific annotation are introduced into our framework, allowing the continuous calibration of models and the enhancement of their robustness based on the feedback loops (Honovich et al., 2021; Nan et al., 2021).", "AI": {"tldr": "The paper addresses stability issues in LLMs and proposes an AI-driven annotation pipeline to improve performance through a human-AI synergy approach.", "motivation": "LLMs face challenges in highly regulated industries due to instability, hallucinations, and inconsistent reasoning, limiting their safe use.", "method": "The authors developed an AI-based annotation pipeline combining automated weak supervision, confidence-based annotation, and human validation.", "result": "The approach introduces categories like semantic consistency, factual correctness, and logical coherence for systematic feedback and model improvement.", "conclusion": "Their method supports continuous calibration and robustness enhancement of LLMs, addressing reliability concerns sustainably."}}
{"id": "2512.13707", "pdf": "https://arxiv.org/pdf/2512.13707", "abs": "https://arxiv.org/abs/2512.13707", "authors": ["Daoyuan Qian", "Qiyao Liang", "Ila Fiete"], "title": "Modular connectivity in neural networks emerges from Poisson noise-motivated regularisation, and promotes robustness and compositional generalisation", "categories": ["physics.bio-ph", "cs.LG", "cs.NE", "stat.ML"], "comment": null, "summary": "Circuits in the brain commonly exhibit modular architectures that factorise complex tasks, resulting in the ability to compositionally generalise and reduce catastrophic forgetting. In contrast, artificial neural networks (ANNs) appear to mix all processing, because modular solutions are difficult to find as they are vanishing subspaces in the space of possible solutions. Here, we draw inspiration from fault-tolerant computation and the Poisson-like firing of real neurons to show that activity-dependent neural noise, combined with nonlinear neural responses, drives the emergence of solutions that reflect an accurate understanding of modular tasks, corresponding to acquisition of a correct world model. We find that noise-driven modularisation can be recapitulated by a deterministic regulariser that multiplicatively combines weights and activations, revealing rich phenomenology not captured in linear networks or by standard regularisation methods. Though the emergence of modular structure requires sufficiently many training samples (exponential in the number of modular task dimensions), we show that pre-modularised ANNs exhibit superior noise-robustness and the ability to generalise and extrapolate well beyond training data, compared to ANNs without such inductive biases. Together, our work demonstrates a regulariser and architectures that could encourage modularity emergence to yield functional benefits.", "AI": {"tldr": "This study demonstrates how neural noise and a novel deterministic regularizer induce modularity in artificial neural networks (ANNs), enabling better generalisation, extrapolation, and noise robustness.", "motivation": "Understanding why modularity in brain circuits aids compositionally generalising tasks and reducing catastrophic forgetting, and exploring ways to replicate modular architectures in artificial neural networks.", "method": "Incorporating activity-dependent neural noise and developing a deterministic regularizer that multiplicatively combines weights and activations to drive modularity emergence.", "result": "Pre-modularized ANNs show improved noise robustness, generalization, and extrapolation capabilities compared to traditional ANNs, especially under modular inductive biases.", "conclusion": "The paper advocates neural noise-inspired methods and architectures that induce modularity for enhancing task functionality and network robustness."}}
{"id": "2512.14172", "pdf": "https://arxiv.org/pdf/2512.14172", "abs": "https://arxiv.org/abs/2512.14172", "authors": ["Qijun Zhang", "Shang Liu", "Yao Lu", "Mengming Li", "Zhiyao Xie"], "title": "ReadyPower: A Reliable, Interpretable, and Handy Architectural Power Model Based on Analytical Framework", "categories": ["cs.AR"], "comment": "Accepted by ASP-DAC'26", "summary": "Power is a primary objective in modern processor design, requiring accurate yet efficient power modeling techniques. Architecture-level power models are necessary for early power optimization and design space exploration. However, classical analytical architecture-level power models (e.g., McPAT) suffer from significant inaccuracies. Emerging machine learning (ML)-based power models, despite their superior accuracy in research papers, are not widely adopted in the industry. In this work, we point out three inherent limitations of ML-based power models: unreliability, limited interpretability, and difficulty in usage. This work proposes a new analytical power modeling framework named ReadyPower, which is ready-for-use by being reliable, interpretable, and handy. We observe that the root cause of the low accuracy of classical analytical power models is the discrepancies between the real processor implementation and the processor's analytical model. To bridge the discrepancies, we introduce architecture-level, implementation-level, and technology-level parameters into the widely adopted McPAT analytical model to build ReadyPower. The parameters at three different levels are decided in different ways. In our experiment, averaged across different training scenarios, ReadyPower achieves >20% lower mean absolute percentage error (MAPE) and >0.2 higher correlation coefficient R compared with the ML-based baselines, on both BOOM and XiangShan CPU architectures.baselines, on both BOOM and XiangShan CPU architectures.", "AI": {"tldr": "This paper introduces ReadyPower, an improved analytical power modeling framework that enhances classical methods' reliability and usability while improving accuracy compared to ML-based models.", "motivation": "To address the inaccuracies of classical analytical power models and limitations of ML-based power models (unreliability, limited interpretability, and usability issues) in modern processor design.", "method": "ReadyPower modifies the McPAT analytical model by adding parameters at architecture, implementation, and technology levels to reduce modeling discrepancies and increase model reliability.", "result": "ReadyPower achieves over 20% lower mean absolute percentage error (MAPE) and more than 0.2 higher correlation coefficient (R) than ML-based power models across BOOM and XiangShan CPU architectures.", "conclusion": "ReadyPower provides a reliable, interpretable, and user-friendly analytical power modeling alternative, demonstrating significant accuracy improvements over ML-based baselines in processor power estimation."}}
{"id": "2512.14104", "pdf": "https://arxiv.org/pdf/2512.14104", "abs": "https://arxiv.org/abs/2512.14104", "authors": ["Karthikeyan Sankaralingam"], "title": "The Impact Market to Save Conference Peer Review: Decoupling Dissemination and Credentialing", "categories": ["cs.GT", "cs.AR", "cs.CY", "cs.PL"], "comment": "41 pages, 4 figures,", "summary": "Top-tier academic conferences are failing under the strain of two irreconcilable roles: (1) rapid dissemination of all sound research and (2) scarce credentialing for prestige and career advancement. This conflict has created a reviewer roulette and anonymous tribunal model - a zero-cost attack system - characterized by high-stakes subjectivity, turf wars, and the arbitrary rejection of sound research (the equivalence class problem). We propose the Impact Market (IM), a novel three-phase system that decouples publication from prestige. Phase 1 (Publication): All sound and rigorous papers are accepted via a PC review, solving the \"equivalence class\" problem. Phase 2 (Investment): An immediate, scarce prestige signal is created via a futures market. Senior community members invest tokens into published papers, creating a transparent, crowdsourced Net Invested Score (NIS). Phase 3 (Calibration): A 3-year lookback mechanism validates these investments against a manipulation-resistant Multi-Vector Impact Score (MVIS). This MVIS adjusts each investor's future influence (their Investor Rating), imposing a quantifiable cost on bad actors and rewarding accurate speculation. The IM model replaces a hidden, zero-cost attack system with a transparent, accountable, and data-driven market that aligns immediate credentialing with long-term, validated impact. Agent-based simulations demonstrate that while a passive market matches current protocols in low-skill environments, introducing investor agency and conviction betting increases the retrieval of high-impact papers from 28% to over 85% under identical conditions, confirming that incentivized self-selection is the mechanism required to scale peer review.", "AI": {"tldr": "Top-tier academic conferences are struggling to balance rapid research dissemination and prestige credentialing. The proposed Impact Market system aims to address this by decoupling publication from prestige, using a three-phase system: publication, investment, and calibration.", "motivation": "The paper addresses the conflict between the need for rapid dissemination of sound research and the restricted credentialing system in academic conferences, leading to arbitrary rejection and inefficiencies.", "method": "The Impact Market system involves three phases: (1) accepting sound papers via PC review for publication, (2) creating prestige signals through token investment in published papers, and (3) validating investments with a Multi-Vector Impact Score (MVIS) and adjusting investor influence based on accuracy.", "result": "Agent-based simulations show that introducing investor agency significantly improves the retrieval of high-impact papers \u2014 from 28% to over 85%.", "conclusion": "The Impact Market model provides a transparent, accountable approach that aligns credentialing with validated long-term impact, addressing inefficiencies and biases in current peer review systems."}}
{"id": "2512.14290", "pdf": "https://arxiv.org/pdf/2512.14290", "abs": "https://arxiv.org/abs/2512.14290", "authors": ["Suhrid Gupta", "Muhammed Tawfiqul Islam", "Rajkumar Buyya"], "title": "A Hybrid Reactive-Proactive Auto-scaling Algorithm for SLA-Constrained Edge Computing", "categories": ["cs.DC"], "comment": null, "summary": "Edge computing decentralizes computing resources, allowing for novel applications in domains such as the Internet of Things (IoT) in healthcare and agriculture by reducing latency and improving performance. This decentralization is achieved through the implementation of microservice architectures, which require low latencies to meet stringent service level agreements (SLA) such as performance, reliability, and availability metrics. While cloud computing offers the large data storage and computation resources necessary to handle peak demands, a hybrid cloud and edge environment is required to ensure SLA compliance. This is achieved by sophisticated orchestration strategies such as Kubernetes, which help facilitate resource management. The orchestration strategies alone do not guarantee SLA adherence due to the inherent delay of scaling resources. Existing auto-scaling algorithms have been proposed to address these challenges, but they suffer from performance issues and configuration complexity. In this paper, a novel auto-scaling algorithm is proposed for SLA-constrained edge computing applications. This approach combines a Machine Learning (ML) based proactive auto-scaling algorithm, capable of predicting incoming resource requests to forecast demand, with a reactive autoscaler which considers current resource utilization and SLA constraints for immediate adjustments. The algorithm is integrated into Kubernetes as an extension, and its performance is evaluated through extensive experiments in an edge environment with real applications. The results demonstrate that existing solutions have an SLA violation rate of up to 23%, whereas the proposed hybrid solution outperforms the baselines with an SLA violation rate of only 6%, ensuring stable SLA compliance across various applications.", "AI": {"tldr": "A novel hybrid auto-scaling algorithm combining machine learning and reactive strategies is proposed for SLA-constrained edge computing. It integrates into Kubernetes and reduces SLA violation rates drastically.", "motivation": "Edge computing requires low-latency and stringent SLA compliance for applications like IoT in healthcare and agriculture. Current auto-scaling solutions struggle due to performance issues and complexity of configuration, necessitating a new approach.", "method": "The proposed method combines a proactive ML-based auto-scaler for demand prediction with a reactive auto-scaler focusing on real-time SLA constraints and resource utilization. This hybrid solution is integrated into Kubernetes for orchestration.", "result": "The hybrid auto-scaling algorithm showed improved SLA compliance by reducing the violation rate to 6%, outperforming existing solutions with a violation rate of 23% in edge environments.", "conclusion": "The proposed hybrid auto-scaling solution ensures better SLA compliance, stability, and performance for edge computing applications, demonstrating its practical utility through real-world experimentation."}}
{"id": "2512.13860", "pdf": "https://arxiv.org/pdf/2512.13860", "abs": "https://arxiv.org/abs/2512.13860", "authors": ["Henger Li", "Shuangjie You", "Flavio Di Palo", "Yiyue Qian", "Ayush Jain"], "title": "Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted by AAAI 2026 Workshop on Agentic AI Benchmarks and Applications for Enterprise Tasks", "summary": "Tool calling enables large language models (LLMs) to interact with external environments through tool invocation, providing a practical way to overcome the limitations of pretraining. However, the effectiveness of tool use depends heavily on the quality of the associated documentation and knowledge base context. These materials are usually written for human users and are often misaligned with how LLMs interpret information. This problem is even more pronounced in industrial settings, where hundreds of tools with overlapping functionality create challenges in scalability, variability, and ambiguity. We propose Verification-Guided Context Optimization (VGCO), a framework that uses LLMs as editors to automatically refine tool-related documentation and knowledge base context. VGCO works in two stages. First, Evaluation collects real-world failure cases and identifies mismatches between tools and their context. Second, Optimization performs hierarchical editing through offline learning with structure-aware, in-context optimization. The novelty of our LLM editors has three main aspects. First, they use a hierarchical structure that naturally integrates into the tool-calling workflow. Second, they are state-aware, action-specific, and verification-guided, which constrains the search space and enables efficient, targeted improvements. Third, they enable cost-efficient sub-task specialization, either by prompt engineering large editor models or by post-training smaller editor models. Unlike prior work that emphasizes multi-turn reasoning, VGCO focuses on the single-turn, large-scale tool-calling problem and achieves significant improvements in accuracy, robustness, and generalization across LLMs.", "AI": {"tldr": "This paper introduces Verification-Guided Context Optimization (VGCO), a framework to enhance LLM interaction with tool-related documentation by refining the associated context for better tool invocation.", "motivation": "Tool calling in LLMs is limited by the mismatched and often ambiguous documentation and knowledge base context, especially in industrial environments with numerous tools.", "method": "VGCO employs LLMs as editors in two stages: an Evaluation phase gathers real-world failure cases and pinpoints mismatches, and an Optimization phase refines documentation using hierarchical and verification-guided editing.", "result": "VGCO significantly improves tool-calling accuracy, robustness, and generalization across LLMs, addressing the single-turn, large-scale challenges.", "conclusion": "Verification-guided context refinement is effective in enhancing tool-related documentation and addressing large-scale tool-calling problems for improved LLM functionality."}}
{"id": "2512.13961", "pdf": "https://arxiv.org/pdf/2512.13961", "abs": "https://arxiv.org/abs/2512.13961", "authors": ["Team Olmo", ":", "Allyson Ettinger", "Amanda Bertsch", "Bailey Kuehl", "David Graham", "David Heineman", "Dirk Groeneveld", "Faeze Brahman", "Finbarr Timbers", "Hamish Ivison", "Jacob Morrison", "Jake Poznanski", "Kyle Lo", "Luca Soldaini", "Matt Jordan", "Mayee Chen", "Michael Noukhovitch", "Nathan Lambert", "Pete Walsh", "Pradeep Dasigi", "Robert Berry", "Saumya Malik", "Saurabh Shah", "Scott Geng", "Shane Arora", "Shashank Gupta", "Taira Anderson", "Teng Xiao", "Tyler Murray", "Tyler Romero", "Victoria Graf", "Akari Asai", "Akshita Bhagia", "Alexander Wettig", "Alisa Liu", "Aman Rangapur", "Chloe Anastasiades", "Costa Huang", "Dustin Schwenk", "Harsh Trivedi", "Ian Magnusson", "Jaron Lochner", "Jiacheng Liu", "Lester James V. Miranda", "Maarten Sap", "Malia Morgan", "Michael Schmitz", "Michal Guerquin", "Michael Wilson", "Regan Huff", "Ronan Le Bras", "Rui Xin", "Rulin Shao", "Sam Skjonsberg", "Shannon Zejiang Shen", "Shuyue Stella Li", "Tucker Wilde", "Valentina Pyatkin", "Will Merrill", "Yapei Chang", "Yuling Gu", "Zhiyuan Zeng", "Ashish Sabharwal", "Luke Zettlemoyer", "Pang Wei Koh", "Ali Farhadi", "Noah A. Smith", "Hannaneh Hajishirzi"], "title": "Olmo 3", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.", "AI": {"tldr": "Olmo 3 introduces fully-open state-of-the-art language models at 7B and 32B parameters, optimized for reasoning, coding, chat, and knowledge tasks.", "motivation": "To create top-performing, fully open language models for diverse applications like reasoning, coding, and instruction adherence.", "method": "Developed and released a full model lifecycle covering reasoning, function calling, chat, and coding abilities, with checkpoints and dependencies disclosed.", "result": "The Olmo 3 Think 32B model emerges as the strongest fully-open thinking model to date.", "conclusion": "Olmo 3 showcases cutting-edge, open-access NLP models with full transparency in model development and deployment."}}
{"id": "2512.13974", "pdf": "https://arxiv.org/pdf/2512.13974", "abs": "https://arxiv.org/abs/2512.13974", "authors": ["Hossein Naderi", "Alireza Shojaei", "Philip Agee", "Kereshmeh Afsari", "Abiola Akanmu"], "title": "Autonomous Construction-Site Safety Inspection Using Mobile Robots: A Multilayer VLM-LLM Pipeline", "categories": ["cs.RO"], "comment": null, "summary": "Construction safety inspection remains mostly manual, and automated approaches still rely on task-specific datasets that are hard to maintain in fast-changing construction environments due to frequent retraining. Meanwhile, field inspection with robots still depends on human teleoperation and manual reporting, which are labor-intensive. This paper aims to connect what a robot sees during autonomous navigation to the safety rules that are common in construction sites, automatically generating a safety inspection report. To this end, we proposed a multi-layer framework with two main modules: robotics and AI. On the robotics side, SLAM and autonomous navigation provide repeatable coverage and targeted revisits via waypoints. On AI side, a Vision Language Model (VLM)-based layer produces scene descriptions; a retrieval component powered grounds those descriptions in OSHA and site policies; Another VLM-based layer assesses the safety situation based on rules; and finally Large Language Model (LLM) layer generates safety reports based on previous outputs. The framework is validated with a proof-of-concept implementation and evaluated in a lab environment that simulates common hazards across three scenarios. Results show high recall with competitive precision compared to state-of-the-art closed-source models. This paper contributes a transparent, generalizable pipeline that moves beyond black-box models by exposing intermediate artifacts from each layer and keeping the human in the loop. This work provides a foundation for future extensions to additional tasks and settings within and beyond construction context.", "AI": {"tldr": "This paper proposes an autonomous robotic framework to perform safety inspections in construction sites, using AI and robotics to generate safety reports instead of manual inspections.", "motivation": "Manual construction safety inspections are labor-intensive and inefficient, and current automated methods struggle in dynamic construction environments. Human-dependent robotic inspection also adds excessive labor demands.", "method": "The paper introduces a multi-layer framework combining robotics (SLAM and autonomous navigation) for inspection coverage and AI (Vision Language Models, retrieval, safety assessment, and Large Language Models) to process visuals into safety reports.", "result": "The framework proves effective in a lab setup simulating safety hazards in three scenarios, demonstrating high recall and strong precision performance on par with advanced models.", "conclusion": "This transparent, adaptable pipeline fosters human participation while surpassing black-box interfaces, paving the way for extending its application beyond construction safety inspections."}}
{"id": "2512.13997", "pdf": "https://arxiv.org/pdf/2512.13997", "abs": "https://arxiv.org/abs/2512.13997", "authors": ["Aaron Wei", "Milad Jalali", "Danica J. Sutherland"], "title": "Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "comment": null, "summary": "Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations.", "AI": {"tldr": "The paper introduces a method that extends the Maximum Mean Discrepancy (MMD) for two-sample testing with unequal sample sizes, enabling the use of all available data without sacrificing test accuracy.", "motivation": "To address the limitation of existing two-sample testing methods that assume equal sample sizes, leading to discarded data and reduced test power.", "method": "Extending the theory of generalized U-statistics to MMD estimators, with new asymptotic distribution characterizations and optimization criteria for unequal sample sizes.", "result": "The new method preserves all data, provides better variance characterizations for MMD estimators, and improves test power and applicability.", "conclusion": "This approach overcomes longstanding issues in two-sample testing by enhancing the methodology and insights into MMD estimators, making it more suitable for practical applications."}}
{"id": "2512.13705", "pdf": "https://arxiv.org/pdf/2512.13705", "abs": "https://arxiv.org/abs/2512.13705", "authors": ["Siqi Wang", "Zhengyu Chen", "Teng Xiao", "Zheqi Lv", "Jinluan Yang", "Xunliang Cai", "Jingang Wang", "Xiaomeng Li"], "title": "Scaling and Transferability of Annealing Strategies in Large Language Model Training", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to AAAI 2026 (camera-ready version)", "summary": "Learning rate scheduling is crucial for training large language models, yet understanding the optimal annealing strategies across different model configurations remains challenging. In this work, we investigate the transferability of annealing dynamics in large language model training and refine a generalized predictive framework for optimizing annealing strategies under the Warmup-Steady-Decay (WSD) scheduler. Our improved framework incorporates training steps, maximum learning rate, and annealing behavior, enabling more efficient optimization of learning rate schedules. Our work provides a practical guidance for selecting optimal annealing strategies without exhaustive hyperparameter searches, demonstrating that smaller models can serve as reliable proxies for optimizing the training dynamics of larger models. We validate our findings on extensive experiments using both Dense and Mixture-of-Experts (MoE) models, demonstrating that optimal annealing ratios follow consistent patterns and can be transferred across different training configurations.", "AI": {"tldr": "The paper explores learning rate optimization for large language models, proposing a framework that transfers annealing dynamics from smaller to larger models under the Warmup-Steady-Decay scheduler.", "motivation": "The authors aim to address the challenge of optimizing learning rate annealing strategies efficiently across varying model configurations without relying on exhaustive hyperparameter searches.", "method": "The paper refines a generalized framework that incorporates training steps, maximum learning rate, and annealing dynamics and tests its predictiveness under the Warmup-Steady-Decay scheduler.", "result": "Experiments on Dense and Mixture-of-Experts (MoE) models validate the framework\u2019s ability to optimize training by showing consistent and transferable annealing ratios across models.", "conclusion": "Smaller models can effectively predict optimal annealing strategies for larger models, providing a practical and efficient solution to optimize learning rate schedules in large-scale language model training."}}
{"id": "2512.13739", "pdf": "https://arxiv.org/pdf/2512.13739", "abs": "https://arxiv.org/abs/2512.13739", "authors": ["Yajie Yang", "Yuqing Zhao", "Xiaochao Xi", "Yinan Zhu"], "title": "Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage", "categories": ["cs.CV", "cs.AI"], "comment": "AAAI-AISI 2026", "summary": "Artificial Intelligence Generated Content (AIGC) assisting image production triggers controversy in journalism while attracting attention from media agencies. Key issues involve misinformation, authenticity, semantic fidelity, and interpretability. Most AIGC tools are opaque \"black boxes,\" hindering the dual demands of content accuracy and semantic alignment and creating ethical, sociotechnical, and trust dilemmas. This paper explores pathways for controllable image production in journalism's special coverage and conducts two experiments with projects from China's media agency: (1) Experiment 1 tests cross-platform adaptability via standardized prompts across three scenes, revealing disparities in semantic alignment, cultural specificity, and visual realism driven by training-corpus bias and platform-level filtering. (2) Experiment 2 builds a human-in-the-loop modular pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulating (Style-LoRA, Prompt-to-Prompt), ensuring editorial fidelity through CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials. Traceable deployment preserves semantic representation. Consequently, we propose a human-AI collaboration mechanism for AIGC assisted image production in special coverage and recommend evaluating Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).", "AI": {"tldr": "The paper addresses challenges of using AI-generated images in journalism and proposes experiments for creating controllable image production workflows.", "motivation": "To tackle ethical and technological issues in AI-generated content for journalism, focusing on misinformation, authenticity, and trustworthiness.", "method": "Two experiments were conducted: (1) testing content adaptability across platforms, and (2) creating a modular pipeline with segmentation, semantic alignment, and content verification tools.", "result": "The experiments reveal semantic, cultural, and visual disparities tied to platform and training biases. The pipeline enables accurate, verified, and traceable image deployment.", "conclusion": "A human-AI collaboration mechanism is recommended for trustworthy image production in journalism, incorporating semantic and cultural assessments to ensure quality and appropriateness."}}
{"id": "2512.14160", "pdf": "https://arxiv.org/pdf/2512.14160", "abs": "https://arxiv.org/abs/2512.14160", "authors": ["Christophe Sun", "Pierre-Olivier Michel", "Fran\u00e7ois David", "Nathalie Rouach", "Dan Longrois", "David Holcman"], "title": "Forecasting Excessive Anesthesia Depth Using EEG \u03b1-Spindle Dynamics and Machine Learning", "categories": ["q-bio.NC"], "comment": "11 pages, 4 figures", "summary": "Objectives. Accurately predicting transitions to anesthetic drugs overdosage is a critical challenge in general anesthesia as it requires the identification of EEG indicators relevant for anticipating the evolution of the depth of anesthesia. Methods. In this study, we introduce a real-time, data-driven framework based on alpha spindle dynamics extracted from frontal EEG recordings. Using Empirical Mode Decomposition, we segment transient alpha spindle events and extract statistical features such as amplitude, duration, frequency, and suppression intervals. We apply these features to train a Light Gradient Boosting Machine, LGBM, classifier on a clinical EEG dataset spanning induction, maintenance, and emergence phases of general anesthesia. Results. Our model accurately classifies anesthesia phases with over 80 percent accuracy and anticipates the onset of isoelectric suppression, a marker of anesthetic drugs overdosage, with 96 percent accuracy up to 90 seconds in advance. Conclusion. The spindle-based metrics provides a non-invasive, interpretable, and predictive approach. This real-time method can be used to forecast unintentional anesthetic drugs overdosage, enabling proactive anesthesia management based solely on EEG signals. Significance. This new method is the first to provide a way to prevent too deep anesthesia and its consequence for the well-being of patients after the recovery from anesthesia.", "AI": {"tldr": "The study presents a real-time framework using alpha spindle dynamics from EEG to predict unintended anesthetic drug overdose with high accuracy, allowing better anesthesia management.", "motivation": "To address the challenge of preventing anesthetic drug overdose during general anesthesia by identifying EEG indicators that predict the depth of anesthesia.", "method": "The authors utilize Empirical Mode Decomposition to analyze alpha spindle events from EEG recordings and train a Light Gradient Boosting Machine classifier using extracted features (amplitude, duration, frequency, etc.) from clinical EEG datasets.", "result": "The model achieved over 80% phase classification accuracy and 96% accuracy in predicting isoelectric suppression up to 90 seconds in advance.", "conclusion": "This spindle-dynamics-based, non-invasive, and interpretable method is effective in forecasting anesthetic overdosage, promoting proactive anesthesia management solely using EEG signals."}}
{"id": "2512.13715", "pdf": "https://arxiv.org/pdf/2512.13715", "abs": "https://arxiv.org/abs/2512.13715", "authors": ["Fatemeh Lotfi", "Fatemeh Afghah"], "title": "Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN", "categories": ["cs.AI", "cs.LG", "eess.SY"], "comment": "This paper is submitted to IEEE Open Journal of the Communications Society", "summary": "The increasing complexity of modern applications demands wireless networks capable of real time adaptability and efficient resource management. The Open Radio Access Network (O-RAN) architecture, with its RAN Intelligent Controller (RIC) modules, has emerged as a pivotal solution for dynamic resource management and network slicing. While artificial intelligence (AI) driven methods have shown promise, most approaches struggle to maintain performance under unpredictable and highly dynamic conditions. This paper proposes an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework, inspired by Model Agnostic Meta Learning (MAML), to jointly optimize resource allocation and network slicing in O-RAN. The framework integrates hierarchical control with meta learning to enable both global and local adaptation: the high-level controller allocates resources across slices, while low level agents perform intra slice scheduling. The adaptive meta-update mechanism weights tasks by temporal difference error variance, improving stability and prioritizing complex network scenarios. Theoretical analysis establishes sublinear convergence and regret guarantees for the two-level learning process. Simulation results demonstrate a 19.8% improvement in network management efficiency compared with baseline RL and meta-RL approaches, along with faster adaptation and higher QoS satisfaction across eMBB, URLLC, and mMTC slices. Additional ablation and scalability studies confirm the method's robustness, achieving up to 40% faster adaptation and consistent fairness, latency, and throughput performance as network scale increases.", "AI": {"tldr": "This paper addresses the need for adaptive wireless network resource management in complex and dynamic network conditions via a Meta-Hierarchical Reinforcement Learning (Meta-HRL) framework designed for O-RAN.", "motivation": "Modern wireless networks face challenges in real-time adaptability and resource allocation, especially in highly dynamic environments. Current AI-driven methods often fail to maintain performance under these conditions.", "method": "The proposed Meta-HRL framework integrates hierarchical reinforcement learning with meta-learning. A high-level controller manages resource allocation across slices, while low-level agents handle intra-slice tasks. A novel meta-update mechanism adjusts for temporal difference error variance for better stability and responsiveness.", "result": "Simulation studies show 19.8% better network management efficiency, faster adaptation, and improved QoS satisfaction across eMBB, URLLC, and mMTC slices compared to baseline RL models. Scalability and robustness tests also show significant advantages, including 40% faster adaptation.", "conclusion": "The paper successfully demonstrates that the Meta-HRL approach enhances resource allocation and network slicing in O-RAN, offering performance improvements in adaptation speed, fairness, latency, and throughput even as network complexity scales."}}
{"id": "2512.13857", "pdf": "https://arxiv.org/pdf/2512.13857", "abs": "https://arxiv.org/abs/2512.13857", "authors": ["Kamer Ali Yuksel"], "title": "EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.NE"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to evolve programs and multi-agent systems, yet most existing approaches rely on overwrite-based mutations that maintain only a single candidate at a time. Such methods discard useful variants, suffer from destructive edits, and explore a brittle search space prone to structural failure. We introduce EvoLattice, a framework that represents an entire population of candidate programs or agent behaviors within a single directed acyclic graph. Each node stores multiple persistent alternatives, and every valid path through the graph defines a distinct executable candidate, yielding a large combinatorial search space without duplicating structure. EvoLattice enables fine-grained alternative-level evaluation by scoring each alternative across all paths in which it appears, producing statistics that reveal how local design choices affect global performance. These statistics provide a dense, data-driven feedback signal for LLM-guided mutation, recombination, and pruning, while preserving successful components. Structural correctness is guaranteed by a deterministic self-repair mechanism that enforces acyclicity and dependency consistency independently of the LLM. EvoLattice naturally extends to agent evolution by interpreting alternatives as prompt fragments or sub-agent behaviors. Across program synthesis (proxy and optimizer meta-learning), EvoLattice yields more stable evolution, greater expressivity, and stronger improvement trajectories than prior LLM-guided methods. The resulting dynamics resemble quality-diversity optimization, emerging implicitly from EvoLattice's internal multi-alternative representation rather than an explicit external archive.", "AI": {"tldr": "EvoLattice introduces a framework to improve program and agent evolution by managing multiple program variations and paths in a unified structure, enhancing stability and outcomes.", "motivation": "Traditional overwrite-based mutation in program evolution discards useful candidates, suffers from structural issues, and lacks exploration diversity.", "method": "EvoLattice encodes a population of program or behavior candidates in a directed acyclic graph where each path represents a unique executable, enabling fine-grained analysis and guided evolution.", "result": "EvoLattice improves stability, expressivity, and performance in program synthesis and agent evolution compared to existing approaches.", "conclusion": "EvoLattice produces better evolutionary dynamics and quality-diversity optimization due to its multi-alternative representation and guided mutation strategies."}}
{"id": "2512.14256", "pdf": "https://arxiv.org/pdf/2512.14256", "abs": "https://arxiv.org/abs/2512.14256", "authors": ["Huizheng Wang", "Taiquan Wei", "Zichuan Wang", "Dingcheng Jiang", "Qize Yang", "Jiaxin Liu", "Jingxiang Hou", "Chao Li", "Jinyi Deng", "Yang Hu", "Shouyi Yin"], "title": "TEMP: A Memory Efficient Physical-aware Tensor Partition-Mapping Framework on Wafer-scale Chips", "categories": ["cs.AR"], "comment": "Accepted by HPCA 2026", "summary": "Large language models (LLMs) demand significant memory and computation resources. Wafer-scale chips (WSCs) provide high computation power and die-to-die (D2D) bandwidth but face a unique trade-off between on-chip memory and compute resources due to limited wafer area. Therefore, tensor parallelism strategies for wafer should leverage communication advantages while maintaining memory efficiency to maximize WSC performance. However, existing approaches fail to address these challenges.\n  To address these challenges, we propose the tensor stream partition paradigm (TSPP), which reveals an opportunity to leverage WSCs' abundant communication bandwidth to alleviate stringent on-chip memory constraints. However, the 2D mesh topology of WSCs lacks long-distance and flexible interconnects, leading to three challenges: 1) severe tail latency, 2) prohibitive D2D traffic contention, and 3) intractable search time for optimal design.\n  We present TEMP, a framework for LLM training on WSCs that leverages topology-aware tensor-stream partition, traffic-conscious mapping, and dual-level wafer solving to overcome hardware constraints and parallelism challenges. These integrated approaches optimize memory efficiency and throughput, unlocking TSPP's full potential on WSCs. Evaluations show TEMP achieves 1.7x average throughput improvement over state-of-the-art LLM training systems across various models.", "AI": {"tldr": "The paper addresses memory and computation challenges faced by large language models on wafer-scale chips by proposing a tensor stream partition paradigm (TSPP) and presenting TEMP, a framework that optimizes memory efficiency and performance. TEMP achieves 1.7x throughput improvement in training models.", "motivation": "LLMs require extensive memory and computation resources. Wafer-scale chips provide exceptional computational power but are limited by on-chip memory challenges and inefficient tensor parallelism strategies.", "method": "The authors propose TSPP to use WSCs' communication bandwidth effectively while mitigating memory constraints. They introduce TEMP, which uses topology-aware tensor partitioning, traffic-conscious mapping, and dual-level optimization to address hardware limitations and parallelism issues.", "result": "TEMP demonstrates a 1.7x average throughput improvement for LLM training compared to state-of-the-art systems.", "conclusion": "TEMP successfully leverages the strengths of wafer-scale chips for efficient LLM training by optimizing memory and computation resources, outperforming existing approaches."}}
{"id": "2512.14445", "pdf": "https://arxiv.org/pdf/2512.14445", "abs": "https://arxiv.org/abs/2512.14445", "authors": ["Brenton Walker", "Markus Fidler"], "title": "Performance and Stability of Barrier Mode Parallel Systems with Heterogeneous and Redundant Jobs", "categories": ["cs.DC", "cs.NI", "cs.PF"], "comment": null, "summary": "In some models of parallel computation, jobs are split into smaller tasks and can be executed completely asynchronously. In other situations the parallel tasks have constraints that require them to synchronize their start and possibly departure times. This is true of many parallelized machine learning workloads, and the popular Apache Spark processing engine has recently added support for Barrier Execution Mode, which allows users to add such barriers to their jobs. These barriers necessarily result in idle periods on some of the workers, which reduces their stability and performance, compared to equivalent workloads with no barriers.\n  In this paper we will consider and analyze the stability and performance penalties resulting from barriers. We include an analysis of the stability of $(s,k,l)$ barrier systems that allow jobs to depart after $l$ out of $k$ of their tasks complete. We also derive and evaluate performance bounds for hybrid barrier systems servicing a mix of jobs, both with and without barriers, and with varying degrees of parallelism. For the purely 1-barrier case we compare the bounds and simulation results to benchmark data from a standalone Spark system. We study the overhead in the real system, and based on its distribution we attribute it to the dual event and polling-driven mechanism used to schedule barrier-mode jobs. We develop a model for this type of overhead and validate it against the real system through simulation.", "AI": {"tldr": "This paper examines the performance and stability penalties caused by synchronization barriers in parallel computation, particularly in Apache Spark's Barrier Execution Mode.", "motivation": "The motivation is to understand and analyze the negative impacts of synchronization barriers in parallel computations, such as performance and stability penalties, especially in systems like Apache Spark that utilize Barrier Execution Mode.", "method": "The paper uses theoretical analysis of $(s,k,l)$ barrier systems, evaluates performance bounds of hybrid barrier systems handling mixed jobs, and models overhead in real-world Apache Spark systems with simulations.", "result": "It identifies stability and performance penalties caused by barriers, validates derived performance bounds, and attributes observed overhead to mechanism scheduling in Spark's barrier mode.", "conclusion": "The study highlights the trade-offs of barrier-mode scheduling, quantifies the associated system overhead, and validates the overhead model through simulation and real-world benchmarks."}}
{"id": "2512.13914", "pdf": "https://arxiv.org/pdf/2512.13914", "abs": "https://arxiv.org/abs/2512.13914", "authors": ["Bhargav Chickmagalur Nanjundappa", "Spandan Maaheshwari"], "title": "Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "11 pages, 4 figures, 2 tables, 1 code snippet, 4 algorithms", "summary": "Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context.\n  We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.", "AI": {"tldr": "The paper introduces ContextBranch, a system for improving LLM-assisted software engineering conversations by applying version control semantics, enabling better management of multi-turn contexts.", "motivation": "Large Language Models struggle in multi-turn conversations, especially during exploratory programming tasks that require considering alternatives without losing context.", "method": "The ContextBranch system introduces primitives like checkpoint, branch, switch, and inject to manage conversation states, enabling isolation, exploration, and selective merging of insights.", "result": "In controlled experiments with 30 software engineering scenarios, branching improved response quality, focus, and context awareness, reducing irrelevant exploratory content by 58.1%.", "conclusion": "ContextBranch establishes conversation branching as an essential primitive for AI-supported exploratory tasks, preventing context pollution and enabling effective alternative explorations."}}
{"id": "2512.13980", "pdf": "https://arxiv.org/pdf/2512.13980", "abs": "https://arxiv.org/abs/2512.13980", "authors": ["Zhimin Qiu", "Di Wu", "Feng Liu", "Chenrui Hu", "Yuxiao Wang"], "title": "Structure-Aware Decoding Mechanisms for Complex Entity Extraction with Large-Scale Language Models", "categories": ["cs.CL"], "comment": null, "summary": "This paper proposes a structure-aware decoding method based on large language models to address the difficulty of traditional approaches in maintaining both semantic integrity and structural consistency in nested and overlapping entity extraction tasks. The method introduces a candidate span generation mechanism and structured attention modeling to achieve unified modeling of entity boundaries, hierarchical relationships, and cross-dependencies. The model first uses a pretrained language model to obtain context-aware semantic representations, then captures multi-granular entity span features through candidate representation combinations, and introduces hierarchical structural constraints during decoding to ensure consistency between semantics and structure. To enhance stability in complex scenarios, the model jointly optimizes classification loss and structural consistency loss, maintaining high recognition accuracy under multi-entity co-occurrence and long-sentence dependency conditions. Experiments conducted on the ACE 2005 dataset demonstrate significant improvements in Accuracy, Precision, Recall, and F1-Score, particularly in nested and overlapping entity recognition, where the model shows stronger boundary localization and structural modeling capability. This study verifies the effectiveness of structure-aware decoding in complex semantic extraction tasks, provides a new perspective for developing language models with hierarchical understanding, and establishes a methodological foundation for high-precision information extraction.", "AI": {"tldr": "The paper proposes a structure-aware decoding method using large language models for enhancing semantic integrity and structural consistency in nested and overlapping entity extraction. It introduces candidate span generation and structured attention modeling to address complex scenarios.", "motivation": "Traditional methods struggle to simultaneously ensure semantic integrity and structural consistency in nested and overlapping entity extraction tasks.", "method": "The approach includes context-aware semantic representations from pretrained models, multi-granular entity span features via candidate representation combinations, and hierarchical structural constraints during decoding. Joint optimization of classification and structural consistency losses is also applied.", "result": "Experiments on the ACE 2005 dataset show improvements in Accuracy, Precision, Recall, and F1-Score, particularly excelling in nested and overlapping entity recognition tasks.", "conclusion": "Structure-aware decoding is effective in complex semantic extraction, providing a robust hierarchical understanding methodology and advancing precision-based information extraction techniques."}}
{"id": "2512.13981", "pdf": "https://arxiv.org/pdf/2512.13981", "abs": "https://arxiv.org/abs/2512.13981", "authors": ["Hossein Naderi", "Alireza Shojaei", "Philip Agee", "Kereshmeh Afsari", "Abiola Akanmu"], "title": "Impact of Robot Facial-Audio Expressions on Human Robot Trust Dynamics and Trust Repair", "categories": ["cs.RO"], "comment": null, "summary": "Despite recent advances in robotics and human-robot collaboration in the AEC industry, trust has mostly been treated as a static factor, with little guidance on how it changes across events during collaboration. This paper investigates how a robot's task performance and its expressive responses after outcomes shape the dynamics of human trust over time. To this end, we designed a controlled within-subjects study with two construction-inspired tasks, Material Delivery (physical assistance) and Information Gathering (perceptual assistance), and measured trust repeatedly (four times per task) using the 14-item Trust Perception Scale for HRI plus a redelegation choice. The robot produced two multimodal expressions, a \"glad\" display with a brief confirmation after success, and a \"sad\" display with an apology and a request for a second chance after failure. The study was conducted in a lab environment with 30 participants and a quadruped platform, and we evaluated trust dynamics and repair across both tasks. Results show that robot success reliably increases trust, failure causes sharp drops, and apology-based expressions partially restores trust (44% recovery in Material Delivery; 38% in Information Gathering). Item-level analysis indicates that recovered trust was driven mostly by interaction and communication factors, with competence recovering partially and autonomy aspects changing least. Additionally, age group and prior attitudes moderated trust dynamics with younger participants showed larger but shorter-lived changes, mid-20s participants exhibited the most durable repair, and older participants showed most conservative dynamics. This work provides a foundation for future efforts that adapt repair strategies to task demands and user profiles to support safe, productive adoption of robots on construction sites.", "AI": {"tldr": "This study explores how a robot\u2019s task performance and expressive responses (e.g., apologies) impact human trust over time in robot-human collaboration.", "motivation": "To understand how trust in robots evolves dynamically during collaborative tasks, especially in the construction industry, and how robots can repair trust after failures.", "method": "The study involved 30 participants, two tasks (Material Delivery and Information Gathering), repeated trust measures, and robots exhibiting two expressive responses (\"glad\" after success, \"sad/apology\" after failure).", "result": "Successful robot performance increased trust, failures reduced it, and apologies partially repaired trust (44% recovery in Material Delivery, 38% in Information Gathering). Younger participants showed dynamic but less stable changes, while mid-20s participants had the most durable repairs.", "conclusion": "Robot task performance and well-designed expressive responses can shape trust dynamics. Adaptive strategies based on user profiles and tasks are key for deploying robots effectively in construction environments."}}
{"id": "2512.14000", "pdf": "https://arxiv.org/pdf/2512.14000", "abs": "https://arxiv.org/abs/2512.14000", "authors": ["Zheng He", "Roman Pogodin", "Yazhe Li", "Namrata Deka", "Arthur Gretton", "Danica J. Sutherland"], "title": "On the Hardness of Conditional Independence Testing In Practice", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "Published at NeurIPS 2025: https://openreview.net/forum?id=Tn1M71PDfF", "summary": "Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on \"hiding\" dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test - of which we show the Generalized Covariance Measure underlying many recent tests is nearly a special case - and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type-I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type-I error.", "AI": {"tldr": "This paper examines the kernel-based conditional independence (CI) test, identifying its practical failures and offering insights into factors like conditional mean embedding errors and kernel selection.", "motivation": "Conditional independence tests are crucial in many statistical and machine learning applications, but their practical failures need better understanding beyond theoretical limitations.", "method": "The paper focuses on analyzing the kernel-based CI test, investigates its theoretical framework, and explores factors influencing its behavior such as conditional mean embedding errors and kernel selection.", "result": "Finds that errors in mean embedding increase Type-I error, while proper kernel selection improves test power but can inflate Type-I errors further.", "conclusion": "Enhancing the kernel design and controlling embedding errors are vital to improving the reliability and performance of conditional independence tests."}}
{"id": "2512.13706", "pdf": "https://arxiv.org/pdf/2512.13706", "abs": "https://arxiv.org/abs/2512.13706", "authors": ["John Graham Reynolds"], "title": "Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training", "categories": ["cs.LG", "cs.CL"], "comment": "11 pages, 2 figures. Code available at https://github.com/johngrahamreynolds/mathematical_catastrophe_mitigation. Models available at https://huggingface.co/collections/MarioBarbeque/catastrophic-forgetting-in-mathematical-reasoning", "summary": "When finetuning large language models for specialized tasks such as mathematical reasoning, models exhibit catastrophic forgetting, losing previously learned capabilities. We investigate this by finetuning Flan-T5-Base (250M parameters) on the DeepMind Mathematics dataset and measuring forgetting on MultiNLI. Math-only training improves mathematical accuracy from 3.1\\% to 12.0\\% but causes NLI accuracy to collapse from 81.0\\% to 16.5\\%--a 64.5 percentage point drop occurring within the first 1,000 training steps. We propose mixed training strategies that interleave mathematical and NLI examples during training. Our results demonstrate that mixed training completely eliminates catastrophic forgetting while maintaining equivalent mathematical performance: the balanced 1:1 ratio achieves 12.0\\% math accuracy (matching math-only) while preserving 86.2\\% NLI accuracy. We systematically explore mixing ratios from 1:1 to 15:1, finding that even minimal NLI exposure (6.2\\%) provides effective regularization. These findings demonstrate that specialization need not require forgetting general capabilities, with implications for scaling to larger models where mixed training may confer additional benefits beyond forgetting prevention.", "AI": {"tldr": "The paper addresses catastrophic forgetting in finetuning language models (e.g., Flan-T5), proposing mixed training strategies to prevent forgetting while maintaining task-specific performance.", "motivation": "To resolve the issue of catastrophic forgetting where finetuning language models for specialized tasks (e.g., mathematical reasoning) causes loss of previously acquired general capabilities.", "method": "The authors finetuned Flan-T5-Base on mathematical tasks (using the DeepMind Mathematics dataset) while measuring forgetting on NLI tasks (MultiNLI dataset). They propose mixed training strategies by interleaving task-specific and general-purpose training examples at varied ratios.", "result": "Mixed training completely eliminates catastrophic forgetting; mathematical accuracy is maintained at 12.0%, while NLI accuracy improves to 86.2%, compared to 16.5% in math-only training.", "conclusion": "Specializing large language models through task-specific finetuning does not necessarily require forgetting general capabilities, with mixed training strategies providing an effective solution and scalability benefits."}}
{"id": "2512.13742", "pdf": "https://arxiv.org/pdf/2512.13742", "abs": "https://arxiv.org/abs/2512.13742", "authors": ["Md. Najib Hasan", "Imran Ahmad", "Sourav Basak Shuvo", "Md. Mahadi Hasan Ankon", "Sunanda Das", "Nazmul Siddique", "Hui Wang"], "title": "DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.", "AI": {"tldr": "A study linking medical image classification with large language models (LLMs) to bridge the gap in clinical reasoning shows promising results but highlights the unreliability of LLMs for critical medical decisions.", "motivation": "Despite advancements in medical image classifiers, their inability to explain decision processes and the unstable visual reasoning of LLMs creates a gap in meeting clinicians' reasoning standards.", "method": "The paper introduces MobileCoAtNet for stomach-related image classification and integrates its outputs with LLM reasoning evaluations using expert benchmarks covering diverse clinical aspects.", "result": "MobileCoAtNet achieves high classification accuracy. Thirty-two LLMs evaluated against expert benchmarks show improved reasoning quality with stronger classification but fail to achieve human-like consistency.", "conclusion": "Integrating DL with LLMs provides potential clinical narratives but highlights the instability of current LLMs, emphasizing the need for safer reasoning systems for medical decision-making."}}
{"id": "2512.14359", "pdf": "https://arxiv.org/pdf/2512.14359", "abs": "https://arxiv.org/abs/2512.14359", "authors": ["Pierre Vassiliadis", "Elena Beanato", "Maximilian J. Wessel", "Friedhelm C. Hummel"], "title": "Temporal interference stimulation for deep brain neuromodulation in humans", "categories": ["q-bio.NC", "eess.SY"], "comment": null, "summary": "For decades, focal non-invasive neuromodulation of deep brain regions has not been possible because of the steep depth-focality trade-off of conventional non-invasive brain stimulation (NIBS) techniques, such as transcranial magnetic stimulation (TMS) or classical transcranial electric stimulation (tES). Deep brain stimulation has therefore largely relied on invasive approaches in clinical populations, requiring surgery. Transcranial Temporal Interference Stimulation (tTIS) has recently emerged as a promising method to overcome this challenge and allows for the first time focal non-invasive electrical deep brain stimulation. The method, which was first validated through computational modeling and rodent work, has now been successfully translated to humans to target deep brain regions such as the hippocampus or striatum. In this Perspective, we present current evidence for tTIS-based neuromodulation, underlying mechanisms and discuss future developments of this promising technology. More specifically, we highlight key opportunities and challenges for fundamental neuroscience as well as for the design of new interventions in neuropsychiatric disorders. We also discuss the status of understanding and challenges regarding the basic mechanisms of action of tTIS and possible lines of technological innovation to optimize stimulation, in particular in terms of intensity and focality. Overall, we suggest that following the first proof-of-concepts, an important multidisciplinary research effort is now required to further validate the use of tTIS in multiple applications, understand its underlying principles and optimize the technology in the view of a wider scientific and clinical deployment.", "AI": {"tldr": "This paper discusses Transcranial Temporal Interference Stimulation (tTIS) as a non-invasive method to achieve focal deep brain stimulation and highlights its potential in neuroscience and clinical applications.", "motivation": "The motivation of the paper is to address the challenge of focal non-invasive neuromodulation of deep brain regions, which has been constrained by the depth-focality trade-off of conventional non-invasive brain stimulation techniques, thereby necessitating invasive procedures.", "method": "The paper focuses on discussing the development and validation of tTIS, which allows for focal non-invasive deep brain electrical stimulation. It highlights computational modeling, rodent studies, and translation of the method to humans targeting specific deep brain regions.", "result": "The paper presents tTIS as a promising method successfully validated for non-invasive stimulation of regions such as the hippocampus and striatum. It provides key evidence for its potential neuromodulation capabilities.", "conclusion": "The paper concludes that tTIS holds significant promise for advancing both neuroscience and neuropsychiatric therapeutic interventions. However, multidisciplinary research is essential to validate, understand, and optimize the technology for broader applications."}}
{"id": "2512.13716", "pdf": "https://arxiv.org/pdf/2512.13716", "abs": "https://arxiv.org/abs/2512.13716", "authors": ["Yitong Luo", "Ziang Chen", "Hou Hei Lam", "Jiayu zhan", "Junqi Wang", "Zhenliang Zhang", "Xue Feng"], "title": "ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making", "categories": ["cs.AI"], "comment": "Accepted at LAW Workshop, NeurIPS 2025", "summary": "Personalized decision-making is essential for human-AI interaction, enabling AI agents to act in alignment with individual users' value preferences. As AI systems expand into real-world applications, adapting to personalized values beyond task completion or collective alignment has become a critical challenge. We address this by proposing a value-driven approach to personalized decision-making. Human values serve as stable, transferable signals that support consistent and generalizable behavior across contexts. Compared to task-oriented paradigms driven by external rewards and incentives, value-driven decision-making enhances interpretability and enables agents to act appropriately even in novel scenarios. We introduce ValuePilot, a two-phase framework consisting of a dataset generation toolkit (DGT) and a decision-making module (DMM). DGT constructs diverse, value-annotated scenarios from a human-LLM collaborative pipeline. DMM learns to evaluate actions based on personal value preferences, enabling context-sensitive, individualized decisions. When evaluated on previously unseen scenarios, DMM outperforms strong LLM baselines, including GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, in aligning with human action choices. Our results demonstrate that value-driven decision-making is an effective and extensible engineering pathway toward building interpretable, personalized AI agents.", "AI": {"tldr": "The paper introduces a value-driven approach to personalized AI decision-making, proposing a framework called ValuePilot, which outperforms existing models in aligning AI decisions with human values.", "motivation": "To address the challenge of personalizing AI decisions based on individual human value preferences, especially as AI systems are increasingly applied in real-world scenarios.", "method": "The authors propose ValuePilot, which includes a dataset generation toolkit (DGT) for creating value-annotated scenarios and a decision-making module (DMM) for learning and making decisions based on individualized value preferences.", "result": "The DMM module of ValuePilot surpasses leading AI models like GPT-5 and Claude-Sonnet-4 in aligning its decisions with human value-based actions, particularly in novel scenarios.", "conclusion": "Value-driven decision-making is a promising approach to develop interpretable, personalized AI systems that align with human values, offering superior adaptability and contextual sensitivity across a variety of scenarios."}}
{"id": "2512.14322", "pdf": "https://arxiv.org/pdf/2512.14322", "abs": "https://arxiv.org/abs/2512.14322", "authors": ["Huizheng Wang", "Hongbin Wang", "Zichuan Wang", "Zhiheng Yue", "Yang Wang", "Chao Li", "Yang Hu", "Shouyi Yin"], "title": "PADE: A Predictor-Free Sparse Attention Accelerator via Unified Execution and Stage Fusion", "categories": ["cs.AR", "eess.SP"], "comment": "Accepted by HPCA 2026", "summary": "Attention-based models have revolutionized AI, but the quadratic cost of self-attention incurs severe computational and memory overhead. Sparse attention methods alleviate this by skipping low-relevance token pairs. However, current approaches lack practicality due to the heavy expense of added sparsity predictor, which severely drops their hardware efficiency.\n  This paper advances the state-of-the-art (SOTA) by proposing a bit-serial enable stage-fusion (BSF) mechanism, which eliminates the need for a separate predictor. However, it faces key challenges: 1) Inaccurate bit-sliced sparsity speculation leads to incorrect pruning; 2) Hardware under-utilization due to fine-grained and imbalanced bit-level workloads. 3) Tiling difficulty caused by the row-wise dependency in sparsity pruning criteria.\n  We propose PADE, a predictor-free algorithm-hardware co-design for dynamic sparse attention acceleration. PADE features three key innovations: 1) Bit-wise uncertainty interval-enabled guard filtering (BUI-GF) strategy to accurately identify trivial tokens during each bit round; 2) Bidirectional sparsity-based out-of-order execution (BS-OOE) to improve hardware utilization; 3) Interleaving-based sparsity-tiled attention (ISTA) to reduce both I/O and computational complexity. These techniques, combined with custom accelerator designs, enable practical sparsity acceleration without relying on an added sparsity predictor. Extensive experiments on 22 benchmarks show that PADE achieves 7.43x speed up and 31.1x higher energy efficiency than Nvidia H100 GPU. Compared to SOTA accelerators, PADE achieves 5.1x, 4.3x and 3.4x energy saving than Sanger, DOTA and SOFA.", "AI": {"tldr": "Attention-based models face computational challenges with self-attention's quadratic cost. Sparse methods help but current approaches lack practicality. This paper introduces PADE, offering enhanced speed, energy efficiency, and practical hardware acceleration.", "motivation": "The motivation lies in addressing computational and memory challenges of self-attention in AI models and improving hardware efficiency for sparse attention methods.", "method": "PADE introduces three innovations: Bit-wise uncertainty interval-enabled guard filtering (BUI-GF) for accurate token pruning, Bidirectional sparsity-based out-of-order execution (BS-OOE) for improved hardware utilization, and Interleaving-based sparsity-tiled attention (ISTA) for lower complexity.", "result": "PADE achieves a 7.43x speedup and 31.1x energy efficiency over Nvidia H100 GPU. It outperforms SOTA accelerators with 5.1x, 4.3x, and 3.4x energy savings compared to Sanger, DOTA, and SOFA.", "conclusion": "PADE demonstrates a practical, hardware-efficient solution to sparse attention acceleration, eliminating the need for a sparsity predictor and enhancing both performance and energy savings."}}
{"id": "2512.14628", "pdf": "https://arxiv.org/pdf/2512.14628", "abs": "https://arxiv.org/abs/2512.14628", "authors": ["Alireza Olama", "Andreas Lundell", "Izzat El Hajj", "Johan Lilius", "Jerker Bj\u00f6rkqvist"], "title": "PruneX: A Hierarchical Communication-Efficient System for Distributed CNN Training with Structured Pruning", "categories": ["cs.DC"], "comment": null, "summary": "Inter-node communication bandwidth increasingly constrains distributed training at scale on multi-node GPU clusters. While compact models are the ultimate deployment target, conventional pruning-aware distributed training systems typically fail to reduce communication overhead because unstructured sparsity cannot be efficiently exploited by highly optimized dense collective primitives. We present PruneX, a distributed data-parallel training system that co-designs pruning algorithms with cluster hierarchy to reduce inter-node bandwidth usage. PruneX introduces the Hierarchical Structured ADMM (H-SADMM) algorithm, which enforces node-level structured sparsity before inter-node synchronization, enabling dynamic buffer compaction that eliminates both zero-valued transmissions and indexing overhead. The system adopts a leader-follower execution model with separated intra-node and inter-node process groups, performing dense collectives on compacted tensors over bandwidth-limited links while confining full synchronization to high-bandwidth intra-node interconnects. Evaluation on ResNet architectures across 64 GPUs demonstrates that PruneX reduces inter-node communication volume by approximately 60% and achieves 6.75x strong scaling speedup, outperforming the dense baseline (5.81x) and Top-K gradient compression (3.71x) on the Puhti supercomputer at CSC - IT Center for Science (Finland).", "AI": {"tldr": "PruneX reduces inter-node GPU communication by 60% using structured pruning and achieves a 6.75x speedup in distributed training.", "motivation": "Inter-node communication limitations hinder distributed deep learning scaling with multi-node GPU clusters.", "method": "Introduces PruneX with H-SADMM to enforce structured sparsity, enabling efficient dynamic buffer compaction and hierarchical process group syncing.", "result": "PruneX achieves a significant reduction in communication volume and outperforms other methods in scaling and speedup efficiency.", "conclusion": "Structured sparsity and hierarchical execution models in distributed systems can greatly benefit training efficiency for large-scale GPU clusters."}}
{"id": "2512.14012", "pdf": "https://arxiv.org/pdf/2512.14012", "abs": "https://arxiv.org/abs/2512.14012", "authors": ["Ruanqianqian Huang", "Avery Reyna", "Sorin Lerner", "Haijun Xia", "Brian Hempel"], "title": "Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.", "AI": {"tldr": "This paper explores how experienced developers integrate AI agents into software development, focusing on their motivations, strategies, and perceptions.", "motivation": "AI agents promise to significantly enhance software development through faster coding, multitasking, and even constructing software from natural language. However, their role in professional settings requires further investigation.", "method": "The authors conducted field observations of 13 developers and surveyed 99 developers to assess their use of AI agents, motivations, strategies, and overall sentiments.", "result": "Experienced developers see AI agents as productivity tools but prefer maintaining control to ensure software quality. They adopt strategies to manage agent behavior effectively. Additionally, developers feel confident about complementing the agents' limitations.", "conclusion": "The paper highlights the importance of best practices in optimizing AI agent use in software development and suggests opportunities for improving agent interfaces and guidance for usage."}}
{"id": "2512.14064", "pdf": "https://arxiv.org/pdf/2512.14064", "abs": "https://arxiv.org/abs/2512.14064", "authors": ["Yi Hu", "Cai Zhou", "Muhan Zhang"], "title": "What Affects the Effective Depth of Large Language Models?", "categories": ["cs.CL"], "comment": null, "summary": "The scaling of large language models (LLMs) emphasizes increasing depth, yet performance gains diminish with added layers. Prior work introduces the concept of \"effective depth\", arguing that deeper models fail to fully utilize their layers for meaningful computation. Building on this, we systematically study how effective depth varies with model scale, training type, and task difficulty. First, we analyze the model behavior of Qwen-2.5 family (1.5B-32B) and find that while the number of effective layers grows with model size, the effective depth ratio remains stable. Besides, comparisons between base and corresponding long-CoT models show no increase in effective depth, suggesting that improved reasoning stems from longer context rather than deeper per-token computation. Furthermore, evaluations across tasks of varying difficulty indicate that models do not dynamically use more layers for harder problems. Our results suggest that current LLMs underuse available depth across scales, training paradigms and tasks of varying difficulties, pointing out research opportunities on increasing the layer utilization rate of LLMs, model pruning, and early exiting. Our code is released at https://github.com/AheadOFpotato/what_affects_effective_depth.", "AI": {"tldr": "The study investigates how large language models (LLMs) use their layers for computation, finding that effective depth only grows slightly with model size and doesn't change with training paradigm or task difficulty. This suggests layers are underutilized.", "motivation": "The motivation behind the paper is to explore why performance gains decrease with added layers in large language models (LLMs) and to understand how effectively these models use their layers during computation.", "method": "The authors systematically study the behavior of the Qwen-2.5 family models (1.5B-32B) by analyzing how effective depth changes with scale, training type, and task difficulty. They compare base and long-CoT models and examine performance across tasks of varying difficulties.", "result": "Findings reveal that effective depth ratio remains stable despite model size increases, and that improved reasoning in models is due to longer contexts rather than deeper per-token computation. Moreover, models don't dynamically use more layers for harder problems.", "conclusion": "The authors conclude that current LLMs underutilize available depth and highlight opportunities for research into improving layer utilization, model pruning, and early exiting techniques."}}
{"id": "2512.14001", "pdf": "https://arxiv.org/pdf/2512.14001", "abs": "https://arxiv.org/abs/2512.14001", "authors": ["Zhuo Zhang", "Yonghui Liu", "Meijie Zhang", "Feiyang Tan", "Yikang Ding"], "title": "CLAIM: Camera-LiDAR Alignment with Intensity and Monodepth", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by IROS 2025", "summary": "In this paper, we unleash the potential of the powerful monodepth model in camera-LiDAR calibration and propose CLAIM, a novel method of aligning data from the camera and LiDAR. Given the initial guess and pairs of images and LiDAR point clouds, CLAIM utilizes a coarse-to-fine searching method to find the optimal transformation minimizing a patched Pearson correlation-based structure loss and a mutual information-based texture loss. These two losses serve as good metrics for camera-LiDAR alignment results and require no complicated steps of data processing, feature extraction, or feature matching like most methods, rendering our method simple and adaptive to most scenes. We validate CLAIM on public KITTI, Waymo, and MIAS-LCEC datasets, and the experimental results demonstrate its superior performance compared with the state-of-the-art methods. The code is available at https://github.com/Tompson11/claim.", "AI": {"tldr": "The paper introduces CLAIM, a simple and effective method for camera-LiDAR alignment using a coarse-to-fine search strategy, validated on datasets with superior results.", "motivation": "To simplify and improve camera-LiDAR alignment without relying on complex data processing, feature extraction, or matching steps.", "method": "CLAIM aligns camera and LiDAR data using a coarse-to-fine search that minimizes a patched Pearson correlation-based structure loss and mutual information-based texture loss.", "result": "CLAIM outperformed state-of-the-art methods in experiments on KITTI, Waymo, and MIAS-LCEC datasets.", "conclusion": "CLAIM is a robust and adaptive camera-LiDAR calibration method, providing efficient and accurate performance suitable for varied scenes."}}
{"id": "2512.14221", "pdf": "https://arxiv.org/pdf/2512.14221", "abs": "https://arxiv.org/abs/2512.14221", "authors": ["Jiarong Fan", "Juhyun Park. Thi Phuong Thuy Vo", "Nicolas Brunel"], "title": "Weighted Conformal Prediction Provides Adaptive and Valid Mask-Conditional Coverage for General Missing Data Mechanisms", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Conformal prediction (CP) offers a principled framework for uncertainty quantification, but it fails to guarantee coverage when faced with missing covariates. In addressing the heterogeneity induced by various missing patterns, Mask-Conditional Valid (MCV) Coverage has emerged as a more desirable property than Marginal Coverage. In this work, we adapt split CP to handle missing values by proposing a preimpute-mask-then-correct framework that can offer valid coverage. We show that our method provides guaranteed Marginal Coverage and Mask-Conditional Validity for general missing data mechanisms. A key component of our approach is a reweighted conformal prediction procedure that corrects the prediction sets after distributional imputation (multiple imputation) of the calibration dataset, making our method compatible with standard imputation pipelines. We derive two algorithms, and we show that they are approximately marginally valid and MCV. We evaluate them on synthetic and real-world datasets. It reduces significantly the width of prediction intervals w.r.t standard MCV methods, while maintaining the target guarantees.", "AI": {"tldr": "This paper introduces a method to adapt conformal prediction for cases with missing data by using a preimpute-mask-then-correct framework to ensure valid uncertainty quantification.", "motivation": "The research aims to address the challenge of conformal prediction failing to guarantee reliable coverage when missing covariates cause heterogeneity in data patterns.", "method": "A preimpute-mask-then-correct framework is proposed, along with a reweighted conformal prediction approach based on distributional imputation, resulting in two algorithms ensuring marginal validity and mask-conditional validity.", "result": "The proposed methods outperform existing standard MCV methods by significantly reducing prediction interval width while maintaining coverage guarantees on synthetic and real-world datasets.", "conclusion": "The framework ensures valid and efficient uncertainty quantification under various missing data scenarios, making it compatible with existing imputation pipelines."}}
{"id": "2512.13708", "pdf": "https://arxiv.org/pdf/2512.13708", "abs": "https://arxiv.org/abs/2512.13708", "authors": ["Kaiming Luo"], "title": "Variational Physics-Informed Ansatz for Reconstructing Hidden Interaction Networks from Steady States", "categories": ["cs.LG"], "comment": null, "summary": "The interaction structure of a complex dynamical system governs its collective behavior, yet existing reconstruction methods struggle with nonlinear, heterogeneous, and higher-order couplings, especially when only steady states are observable. We propose a Variational Physics-Informed Ansatz (VPIA) that infers general interaction operators directly from heterogeneous steady-state data. VPIA embeds the steady-state constraints of the dynamics into a differentiable variational representation and reconstructs the underlying couplings by minimizing a physics-derived steady-state residual, without requiring temporal trajectories, derivative estimation, or supervision. Residual sampling combined with natural-gradient optimization enables scalable learning of large and higher-order networks. Across diverse nonlinear systems, VPIA accurately recovers directed, weighted, and multi-body structures under substantial noise, providing a unified and robust framework for physics-constrained inference of complex interaction networks in settings where only snapshot observations are available.", "AI": {"tldr": "A new method called Variational Physics-Informed Ansatz (VPIA) systematically reconstructs complex interaction networks using only steady-state data.", "motivation": "Existing reconstruction methods face difficulties in handling nonlinear, heterogeneous, and higher-order interactions when observable data is limited to steady states.", "method": "VPIA embeds steady-state constraints into a differentiable representation, enabling interaction operator recovery through physics-based residual minimization without requiring temporal trajectories or supervision.", "result": "VPIA accurately reconstructs complex networks, including higher-order, directed, and weighted structures, from snapshot observations, even under noisy conditions.", "conclusion": "VPIA offers a robust and scalable framework for inferring complex interaction networks using steady-state data, addressing major limitations of previous reconstruction approaches."}}
{"id": "2512.13747", "pdf": "https://arxiv.org/pdf/2512.13747", "abs": "https://arxiv.org/abs/2512.13747", "authors": ["Siyuan Dai", "Lunxiao Li", "Kun Zhao", "Eardi Lila", "Paul K. Crane", "Heng Huang", "Dongkuan Xu", "Haoteng Tang", "Liang Zhan"], "title": "Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICDM 2025 the Workshop on Synergy of AI and Multimodal Biomedical Data Mining", "summary": "With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.", "AI": {"tldr": "This paper analyzes the limitations of multimodal large language models (MLLMs) in Medical Decision Making (MDM) and proposes strategies for improvement.", "motivation": "Despite advancements in MLLMs with strong zero-shot vision-language capabilities, these models fail to perform adequately on basic and essential MDM tasks such as Alzheimer's disease classification and chest radiograph analysis.", "method": "The paper assesses MLLM performance on two datasets for Alzheimer's disease classification and chest radiograph conditions. Strategies such as in-context learning, vision captioning combined with text-only inference, and fine-tuning the vision components are explored to improve outcomes.", "result": "The study shows that text-only reasoning consistently outperforms multimodal and vision-only settings. Multimodal inputs often perform worse than text alone, highlighting a lack of visual grounding in current models.", "conclusion": "Current MLLMs lack sufficient visual understanding for complex biomedical tasks. The paper identifies potential methods to enhance multimodal decision making in healthcare through specific strategies like few-shot fine-tuning and reason-annotated exemplars."}}
{"id": "2512.13724", "pdf": "https://arxiv.org/pdf/2512.13724", "abs": "https://arxiv.org/abs/2512.13724", "authors": ["Ayush Noori", "Joaqu\u00edn Polonuer", "Katharina Meyer", "Bogdan Budnik", "Shad Morton", "Xinyuan Wang", "Sumaiya Nazeen", "Yingnan He", "I\u00f1aki Arango", "Lucas Vittor", "Matthew Woodworth", "Richard C. Krolewski", "Michelle M. Li", "Ninning Liu", "Tushar Kamath", "Evan Macosko", "Dylan Ritter", "Jalwa Afroz", "Alexander B. H. Henderson", "Lorenz Studer", "Samuel G. Rodriques", "Andrew White", "Noa Dagan", "David A. Clifton", "George M. Church", "Sudeshna Das", "Jenny M. Tam", "Vikram Khurana", "Marinka Zitnik"], "title": "Graph AI generates neurological hypotheses validated in molecular, organoid, and clinical systems", "categories": ["q-bio.QM", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Neurological diseases are the leading global cause of disability, yet most lack disease-modifying treatments. We present PROTON, a heterogeneous graph transformer that generates testable hypotheses across molecular, organoid, and clinical systems. To evaluate PROTON, we apply it to Parkinson's disease (PD), bipolar disorder (BD), and Alzheimer's disease (AD). In PD, PROTON linked genetic risk loci to genes essential for dopaminergic neuron survival and predicted pesticides toxic to patient-derived neurons, including the insecticide endosulfan, which ranked within the top 1.29% of predictions. In silico screens performed by PROTON reproduced six genome-wide $\u03b1$-synuclein experiments, including a split-ubiquitin yeast two-hybrid system (normalized enrichment score [NES] = 2.30, FDR-adjusted $p < 1 \\times 10^{-4}$), an ascorbate peroxidase proximity labeling assay (NES = 2.16, FDR $< 1 \\times 10^{-4}$), and a high-depth targeted exome sequencing study in 496 synucleinopathy patients (NES = 2.13, FDR $< 1 \\times 10^{-4}$). In BD, PROTON predicted calcitriol as a candidate drug that reversed proteomic alterations observed in cortical organoids derived from BD patients. In AD, we evaluated PROTON predictions in health records from $n = 610,524$ patients at Mass General Brigham, confirming that five PROTON-predicted drugs were associated with reduced seven-year dementia risk (minimum hazard ratio = 0.63, 95% CI: 0.53-0.75, $p < 1 \\times 10^{-7}$). PROTON generated neurological hypotheses that were evaluated across molecular, organoid, and clinical systems, defining a path for AI-driven discovery in neurological disease.", "AI": {"tldr": "Neurological diseases lack disease-modifying treatments. A new AI model, PROTON, creates hypotheses across molecular and clinical systems, validated with studies on Parkinson's, bipolar disorder (BD), and Alzheimer's (AD).", "motivation": "Neurological diseases cause significant global disability, with the urgent need for innovative treatments and research methodologies.", "method": "Development and application of PROTON, a heterogeneous graph transformer AI model, to predict disease mechanisms and treatments via genomic, organoid, and clinical evaluations.", "result": "PROTON successfully identified genetic risk factors and toxic compounds for Parkinson's, drug candidates for BD, and confirmed dementia risk reduction for five prescribed drugs from healthcare datasets in Alzheimer's patients.", "conclusion": "PROTON demonstrates AI's capability for cross-system hypothesis generation, paving the way for advancements in neurological disease research and treatments."}}
{"id": "2512.13725", "pdf": "https://arxiv.org/pdf/2512.13725", "abs": "https://arxiv.org/abs/2512.13725", "authors": ["Steve Nwaiwu", "Nipat Jongsawat", "Anucha Tungkasthan"], "title": "Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy", "categories": ["cs.AI"], "comment": null, "summary": "Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.", "AI": {"tldr": "Quantized models like INT8 and NF4 show stable causal reasoning abilities in Llama 3 8B, with minor degradation under four-bit precision.", "motivation": "To understand the impact of model precision reduction on causal reasoning abilities, aimed at ensuring reliability in resource-constrained environments.", "method": "Systematic evaluation of three levels of Pearl's Causal Ladder on quantized models, using a 3000-sample CLadder benchmark and CRASS benchmark experiments for broader insights.", "result": "NF4 quantization shows less than 1% degradation in causal reasoning accuracy, with interventional queries most sensitive and counterfactual reasoning proving stable. Graph augmentation improves interventional accuracy marginally.", "conclusion": "Causal reasoning in quantized models is robust, graph augmentation helps mitigate certain degradation, and current benchmarks fail to detect deeper weaknesses in causal reasoning under precision reduction."}}
{"id": "2512.14661", "pdf": "https://arxiv.org/pdf/2512.14661", "abs": "https://arxiv.org/abs/2512.14661", "authors": ["Chiyue Wei", "Cong Guo", "Junyao Zhang", "Haoxuan Shan", "Yifan Xu", "Ziyue Zhang", "Yudong Liu", "Qinsi Wang", "Changchun Zhou", "Hai \"Helen\" Li", "Yiran Chen"], "title": "Focus: A Streaming Concentration Architecture for Efficient Vision-Language Models", "categories": ["cs.AR"], "comment": "HPCA 2026", "summary": "Vision-Language Models (VLMs) have demonstrated strong performance on tasks such as video captioning and visual question answering. However, their growing scale and video-level inputs lead to significant computational and memory overhead, posing challenges for real-time deployment on hardware accelerators. While prior work attempts to reduce redundancy via token pruning or merging, these methods typically operate at coarse granularity and incur high runtime overhead due to global token-level operations. In this study, we propose Focus, a Streaming Concentration Architecture that efficiently accelerates VLM inference through progressive, fine-grained redundancy elimination. Focus introduces a multilevel concentration paradigm that hierarchically compresses vision-language inputs at three levels: (1) semantic-guided token pruning based on textual prompts, (2) spatial-temporal block-level concentration using localized comparisons, and (3) vector-level redundancy removal via motion-aware matching. All concentration steps are tightly co-designed with the architecture to support streaming-friendly, on-chip execution. Focus leverages GEMM tiling, convolution-style layout, and cross-modal attention to minimize off-chip access while enabling high throughput. Implemented as a modular unit within a systolic-array accelerator, Focus achieves a 2.4x speedup and 3.3x reduction in energy, significantly outperforming state-of-the-art accelerators in both performance and energy efficiency. Full-stack implementation of Focus is open-sourced at https://github.com/dubcyfor3/Focus.", "AI": {"tldr": "The paper introduces Focus, a Streaming Concentration Architecture, that achieves efficient Vision-Language Model (VLM) inference by progressively eliminating redundancy to optimize performance and reduce energy.", "motivation": "Current Vision-Language Models (VLMs) suffer from significant computational and memory overhead due to their large scale and video-level inputs, making real-time deployment on hardware accelerators challenging.", "method": "Focus employs a multilevel concentration paradigm for redundancy elimination at semantic, spatial-temporal, and vector levels. It uses architectural elements like GEMM tiling and cross-modal attention for efficient streaming on-chip execution.", "result": "Focus achieves a 2.4x speedup and 3.3x energy reduction compared to state-of-the-art accelerators, improving both performance and energy efficiency.", "conclusion": "Focus demonstrates an effective and efficient approach to accelerating VLM inference through a carefully co-designed architectural solution, offering significant practical benefits for real-time applications."}}
{"id": "2512.13931", "pdf": "https://arxiv.org/pdf/2512.13931", "abs": "https://arxiv.org/abs/2512.13931", "authors": ["Narasinga Rao Miniskar", "Mohammad Alaul Haque Monil", "Elaine Wong", "Vicente Leyton-Ortega", "Jeffrey S. Vetter", "Seth R. Johnson", "Travis S. Humble"], "title": "Q-IRIS: The Evolution of the IRIS Task-Based Runtime to Enable Classical-Quantum Workflows", "categories": ["quant-ph", "cs.DC"], "comment": null, "summary": "Extreme heterogeneity in emerging HPC systems are starting to include quantum accelerators, motivating runtimes that can coordinate between classical and quantum workloads. We present a proof-of-concept hybrid execution framework integrating the IRIS asynchronous task-based runtime with the XACC quantum programming framework via the Quantum Intermediate Representation Execution Engine (QIR-EE). IRIS orchestrates multiple programs written in the quantum intermediate representation (QIR) across heterogeneous backends (including multiple quantum simulators), enabling concurrent execution of classical and quantum tasks. Although not a performance study, we report measurable outcomes through the successful asynchronous scheduling and execution of multiple quantum workloads. To illustrate practical runtime implications, we decompose a four-qubit circuit into smaller subcircuits through a process known as quantum circuit cutting, reducing per-task quantum simulation load and demonstrating how task granularity can improve simulator throughput and reduce queueing behavior -- effects directly relevant to early quantum hardware environments. We conclude by outlining key challenges for scaling hybrid runtimes, including coordinated scheduling, classical-quantum interaction management, and support for diverse backend resources in heterogeneous systems.", "AI": {"tldr": "This paper introduces a hybrid runtime execution framework combining IRIS and XACC to manage classical and quantum tasks for heterogeneous HPC systems.", "motivation": "Emerging HPC systems include quantum accelerators, necessitating runtimes that coordinate classical-quantum workload execution.", "method": "The framework integrates IRIS with XACC using QIR-EE, enabling task-based orchestration of quantum and classical workloads, including quantum simulators.", "result": "The approach successfully demonstrated asynchronous scheduling and execution of multiple quantum workloads, using quantum circuit cutting for increased efficiency.", "conclusion": "Hybrid runtimes must address challenges in scheduling, interaction management, and backend diversity to scale successfully in heterogeneous environments."}}
{"id": "2512.14018", "pdf": "https://arxiv.org/pdf/2512.14018", "abs": "https://arxiv.org/abs/2512.14018", "authors": ["Jiuding Yang", "Shengyao Lu", "Hongxuan Liu", "Shayan Shirahmad Gale Bagi", "Zahra Fazel", "Tomasz Czajkowski", "Di Niu"], "title": "PerfCoder: Large Language Models for Interpretable Code Performance Optimization", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.", "AI": {"tldr": "PerfCoder, a family of LLMs, enhances code performance through interpretable and customized optimizations, outperforming existing models and aiding larger LLMs.", "motivation": "The motivation is to address LLMs\u2019 limitations in generating high-performance code due to lack of effective, interpretable supervision and optimization strategies.", "method": "PerfCoder is fine-tuned using real-world optimization trajectories and reinforcement learning, enabling input-specific optimizations and eliminating the need for iterative refinement.", "result": "PerfCoder achieves state-of-the-art results in runtime speedup and optimization rates on the PIE benchmark, and enhances the performance of larger LLMs like GPT-5 in a cooperative workflow.", "conclusion": "Performance optimization requires more than just scaling up; strategy awareness and interpretability are key to achieving high-performance code."}}
{"id": "2512.14067", "pdf": "https://arxiv.org/pdf/2512.14067", "abs": "https://arxiv.org/abs/2512.14067", "authors": ["Yonggan Fu", "Lexington Whalen", "Zhifan Ye", "Xin Dong", "Shizhe Diao", "Jingyu Liu", "Chengyue Wu", "Hao Zhang", "Enze Xie", "Song Han", "Maksim Khadkevich", "Jan Kautz", "Yingyan Celine Lin", "Pavlo Molchanov"], "title": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.", "AI": {"tldr": "The paper introduces methods to convert autoregressive (AR) models to diffusion language models (dLMs), enabling enhanced performance and efficiency while retaining accuracy.", "motivation": "The motivation is to address the learning inefficiency of diffusion language models (dLMs) when trained from scratch by converting pretrained AR models into efficient dLMs, providing speed and accuracy advantages.", "method": "The authors propose improved AR-to-dLM conversion via maintaining AR weights using block-wise attention patterns and introducing position-dependent masking strategies to align training-test behaviors.", "result": "The converted models, Efficient-DLMs, outperform state-of-the-art models in terms of accuracy and throughput. For example, Efficient-DLM 8B achieved +5.4%/+2.7% higher accuracy and 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B.", "conclusion": "The study provides a framework and actionable insights to enhance AR-to-dLM conversion, resulting in models that excel in both accuracy and efficiency, proving the efficacy of their methodology."}}
{"id": "2512.14031", "pdf": "https://arxiv.org/pdf/2512.14031", "abs": "https://arxiv.org/abs/2512.14031", "authors": ["Zhaofeng Hu", "Hongrui Yu", "Vaidhyanathan Chandramouli", "Ci-Jyun Liang"], "title": "Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.", "AI": {"tldr": "The paper compares a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) techniques for teaching construction robots new skills, focusing on task performance and practical deployment effort.", "motivation": "To evaluate and understand efficient approaches for training construction robots for automation, particularly in terms of task performance and deployment practicality.", "method": "Two teleoperation interfaces were developed to collect training demonstrations, followed by a three-stage evaluation, comparing RL approaches and VLA models in terms of performance, generalization, sample-efficiency, and real-world experimentation.", "result": "VLA exhibited strong generalization and minimal programming effort. It achieved 60%-100% success in experiments, while DQN, though viable, required significantly more tuning effort due to noise.", "conclusion": "The study concludes that VLA models are more practical and efficient for changing tasks with minimal data requirements compared to DQN, which, though viable, demands intensive tuning for robust performance."}}
{"id": "2512.14308", "pdf": "https://arxiv.org/pdf/2512.14308", "abs": "https://arxiv.org/abs/2512.14308", "authors": ["\u0160imon Kucharsk\u00fd", "Aayush Mishra", "Daniel Habermann", "Stefan T. Radev", "Paul-Christian B\u00fcrkner"], "title": "Improving the Accuracy of Amortized Model Comparison with Self-Consistency", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": "17 pages, 9 figures", "summary": "Amortized Bayesian inference (ABI) offers fast, scalable approximations to posterior densities by training neural surrogates on data simulated from the statistical model. However, ABI methods are highly sensitive to model misspecification: when observed data fall outside the training distribution (generative scope of the statistical models), neural surrogates can behave unpredictably. This makes it a challenge in a model comparison setting, where multiple statistical models are considered, of which at least some are misspecified. Recent work on self-consistency (SC) provides a promising remedy to this issue, accessible even for empirical data (without ground-truth labels). In this work, we investigate how SC can improve amortized model comparison conceptualized in four different ways. Across two synthetic and two real-world case studies, we find that approaches for model comparison that estimate marginal likelihoods through approximate parameter posteriors consistently outperform methods that directly approximate model evidence or posterior model probabilities. SC training improves robustness when the likelihood is available, even under severe model misspecification. The benefits of SC for methods without access of analytic likelihoods are more limited and inconsistent. Our results suggest practical guidance for reliable amortized Bayesian model comparison: prefer parameter posterior-based methods and augment them with SC training on empirical datasets to mitigate extrapolation bias under model misspecification.", "AI": {"tldr": "This paper addresses the challenge of model misspecification in Amortized Bayesian Inference (ABI) and investigates methods to improve its robustness, specifically through self-consistency (SC).", "motivation": "The study aims to mitigate the sensitivity of ABI methods to model misspecification, which leads to unpredictable behavior when observed data falls outside the training distribution, especially in model comparison situations.", "method": "The authors used the concept of self-consistency (SC) to improve ABI model comparison approaches. They analyzed four different conceptualizations of model comparison through synthetic and real-world case studies.", "result": "Methods based on marginal likelihood estimation from approximate parameter posteriors were consistently better than those directly approximating model evidence. SC training improved robustness for cases where likelihoods were accessible, but its benefits were inconsistent otherwise.", "conclusion": "Parameter posterior-based methods should be preferred for reliable Bayesian model comparison, and SC training can help counter extrapolation bias in empirical datasets under model misspecification."}}
{"id": "2512.13710", "pdf": "https://arxiv.org/pdf/2512.13710", "abs": "https://arxiv.org/abs/2512.13710", "authors": ["Edwin Oluoch Awino", "Denis Machanda"], "title": "Predictive Modeling of Flood-Prone Areas Using SAR and Environmental Variables", "categories": ["cs.LG"], "comment": null, "summary": "Flooding is one of the most destructive natural hazards worldwide, posing serious risks to ecosystems, infrastructure, and human livelihoods. This study combines Synthetic Aperture Radar (SAR) imagery with environmental and hydrological data to model flood susceptibility in the River Nyando watershed, western Kenya. Sentinel-1 dual-polarization SAR data from the May 2024 flood event were processed to produce a binary flood inventory, which served as training data for machine learning (ML) models. Six conditioning factors -- slope, elevation, aspect, land use/land cover, soil type, and distance from streams -- were integrated with the SAR-derived flood inventory to train four supervised classifiers: Logistic Regression (LR), Classification and Regression Trees (CART), Support Vector Machines (SVM), and Random Forest (RF). Model performance was assessed using accuracy, Cohen's Kappa, and Receiver Operating Characteristic (ROC) analysis. Results indicate that RF achieved the highest predictive performance (accuracy = 0.762; Kappa = 0.480), outperforming LR, CART, and SVM. The RF-based susceptibility map showed that low-lying Kano Plains near Lake Victoria have the highest flood vulnerability, consistent with historical flood records and the impacts of the May 2024 event. These findings demonstrate the value of combining SAR data and ensemble ML methods for flood susceptibility mapping in regions with limited data. The resulting maps offer important insights for disaster risk reduction, land-use planning, and early warning system development.", "AI": {"tldr": "The study combined SAR data with environmental factors to model flood susceptibility in Kenya's River Nyando watershed using machine learning, highlighting Random Forest (RF) as the best-performing algorithm for flood risk mapping.", "motivation": "The study aims to address the destructive impacts of flooding by improving flood susceptibility mapping in areas with limited data availability, like Kenya's River Nyando watershed, using advanced ML techniques and SAR data.", "method": "The study used Sentinel-1 SAR data and environmental factors to develop a binary flood inventory. Using six conditioning variables, it trained and evaluated four ML models (LR, CART, SVM, RF), comparing their performance in predicting flood susceptibility.", "result": "The Random Forest model achieved the highest predictive accuracy (accuracy = 0.762; Kappa = 0.480). It identified low-lying areas near Lake Victoria, such as the Kano Plains, as highly susceptible to floods.", "conclusion": "The study demonstrates the effectiveness of integrating SAR data with ML methods, particularly Random Forest, for reliable flood mapping, with implications for disaster management and land-use planning in flood-prone areas."}}
{"id": "2512.13752", "pdf": "https://arxiv.org/pdf/2512.13752", "abs": "https://arxiv.org/abs/2512.13752", "authors": ["Jie Qin", "Jiancheng Huang", "Limeng Qiao", "Lin Ma"], "title": "STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 7 figures", "summary": "Multimodal large language models (MLLMs) play a pivotal role in advancing the quest for general artificial intelligence. However, achieving unified target for multimodal understanding and generation remains challenging due to optimization conflicts and performance trade-offs. To effectively enhance generative performance while preserving existing comprehension capabilities, we introduce STAR: a STacked AutoRegressive scheme for task-progressive unified multimodal learning. This approach decomposes multimodal learning into multiple stages: understanding, generation, and editing. By freezing the parameters of the fundamental autoregressive (AR) model and progressively stacking isomorphic AR modules, it avoids cross-task interference while expanding the model's capabilities. Concurrently, we introduce a high-capacity VQ to enhance the granularity of image representations and employ an implicit reasoning mechanism to improve generation quality under complex conditions. Experiments demonstrate that STAR achieves state-of-the-art performance on GenEval (0.91), DPG-Bench (87.44), and ImgEdit (4.34), validating its efficacy for unified multimodal learning.", "AI": {"tldr": "The paper introduces STAR, a task-progressive scheme for unified multimodal learning that decomposes tasks into stages, enhancing generative ability while maintaining comprehension.", "motivation": "To address challenges in achieving unified multimodal understanding and generation by mitigating optimization conflicts and performance trade-offs.", "method": "Decomposing multimodal tasks into understanding, generation, and editing stages; progressive stacking of AR modules with frozen parameters; introducing high-capacity VQ and implicit reasoning mechanisms.", "result": "STAR achieves state-of-the-art results on multiple benchmarks like GenEval, DPG-Bench, and ImgEdit.", "conclusion": "STAR presents an effective framework for unified multimodal learning, enhancing generative capabilities while preserving comprehension abilities."}}
{"id": "2512.14506", "pdf": "https://arxiv.org/pdf/2512.14506", "abs": "https://arxiv.org/abs/2512.14506", "authors": ["Marianne de Heer Kloots", "Paul Boersma", "Willem Zuidema"], "title": "Linguists should learn to love speech-based deep learning models", "categories": ["cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "comment": "Commentary on Futrell, R., & Mahowald, K. arXiv:2501.17047 (in press). How Linguistics Learned to Stop Worrying and Love the Language Models. Behavioural and Brain Sciences", "summary": "Futrell and Mahowald present a useful framework bridging technology-oriented deep learning systems and explanation-oriented linguistic theories. Unfortunately, the target article's focus on generative text-based LLMs fundamentally limits fruitful interactions with linguistics, as many interesting questions on human language fall outside what is captured by written text. We argue that audio-based deep learning models can and should play a crucial role.", "AI": {"tldr": "The paper critiques the limitation of generative text-based LLMs and suggests incorporating audio-based deep learning models for better interactions with linguistic theories.", "motivation": "To address the shortcomings of text-based generative models in capturing the broader aspects of human language relevant to linguistic theories.", "method": "Analysis of generative models in comparison to audio-based deep learning models, advocating for the latter for a comprehensive understanding of human language.", "result": "The paper concludes that generative text-based LLMs are insufficient to bridge linguistic theories and deep learning and calls for audio-based models.", "conclusion": "Audio-based deep learning models provide more fruitful interaction opportunities with linguistics as they encompass broader aspects of human communication compared to text-based generative models."}}
{"id": "2512.13762", "pdf": "https://arxiv.org/pdf/2512.13762", "abs": "https://arxiv.org/abs/2512.13762", "authors": ["TK Lee"], "title": "State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models", "categories": ["cs.AI", "cs.HC"], "comment": "23 pages, 6 figures. Qualitative interaction-level analysis of response patterns in a large language model. Code and processed interaction data are available at https://github.com/theMaker-EnvData/llm_learned_incapacity_corpus", "summary": "Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon interaction. In a single 86-turn dialogue session, the same model shows Normal Performance (NP) in broad, non-sensitive domains while repeatedly producing Functional Refusal (FR) in provider- or policy-sensitive domains, yielding a consistent asymmetry between NP and FR across domains. Drawing on learned helplessness as an analogy, we introduce learned incapacity (LI) as a behavioral descriptor for this selective withholding without implying intentionality or internal mechanisms. We operationalize three response regimes (NP, FR, Meta-Narrative; MN) and show that MN role-framing narratives tend to co-occur with refusals in the same sensitive contexts. Overall, the study proposes an interaction-level auditing framework based on observable behavior and motivates LI as a lens for examining potential alignment side effects, warranting further investigation across users and models.", "AI": {"tldr": "The paper studies how large language models (LLMs) exhibit selective behavioral patterns, particularly 'learned incapacity,' during extended interactions by introducing an auditing framework for such observations.", "motivation": "To explore behavioral inconsistencies in LLMs during extended interactions, particularly in sensitive contexts, and to develop a framework to study these potential side effects of alignment.", "method": "The authors conducted an 86-turn dialogue session to analyze behavioral selectivity, defining three response regimes: Normal Performance (NP), Functional Refusal (FR), and Meta-Narrative (MN), to operationalize the observations.", "result": "The study found an asymmetry in LLM behavior, where models performed normally in broad, non-sensitive domains but showed refusal and meta-narrative framing in policy-sensitive domains.", "conclusion": "The paper introduces 'learned incapacity' as a lens for understanding selective refusal and proposes an auditing framework for studying these behaviors, advocating for further investigations across users and models."}}
{"id": "2512.13866", "pdf": "https://arxiv.org/pdf/2512.13866", "abs": "https://arxiv.org/abs/2512.13866", "authors": ["Mostafa Darvishi"], "title": "Pipeline Stage Resolved Timing Characterization of FPGA and ASIC Implementations of a RISC V Processor", "categories": ["eess.SP", "cs.AR"], "comment": "11 pages, 7 figures, 1 table, submitted to IEEE Transactions on Circuits and Systems (TCAS). Identification # TCAS-I-03260-2025", "summary": "This paper presents a pipeline stage resolved timing characterization of a 32-bit RISC V processor implemented on a 20 nm FPGA and a 7 nm FinFET ASIC platform. A unified analysis framework is introduced that decomposes timing paths into logic, routing, and clocking components and maps them to well-defined pipeline stage transitions. This approach enables systematic comparison of timing behavior across heterogeneous implementation technologies at a microarchitectural level. Using static timing analysis and statistical characterization, the study shows that although both implementations exhibit dominant critical paths in the EX to MEM pipeline transition, their underlying timing mechanisms differ fundamentally. FPGA timing is dominated by routing parasitics and placement dependent variability, resulting in wide slack distributions and sensitivity to routing topology. In contrast, ASIC timing is governed primarily by combinational logic depth and predictable parametric variation across process, voltage, and temperature corners, yielding narrow and stable timing distributions. The results provide quantitative insight into the structural origins of timing divergence between programmable and custom fabrics and demonstrate the effectiveness of pipeline stage resolved analysis for identifying platform specific bottlenecks. Based on these findings, the paper derives design implications for achieving predictable timing closure in processor architectures targeting both FPGA and ASIC implementations.", "AI": {"tldr": "This paper analyzes timing characteristics of a RISC V processor implemented on FPGA and ASIC, revealing differences due to routing parasitics in FPGA and logic depth in ASIC.", "motivation": "To understand timing behavior of processors across heterogeneous implementation technologies and derive design insights for platform-specific optimizations.", "method": "Introduces a unified framework decomposing timing paths into components and mapping them to pipeline stages, leveraging static timing analysis and statistical characterization.", "result": "Shows differing timing mechanisms: FPGA dominated by routing parasitics while ASIC is impacted by logic depth and parameter variations; insight into timing divergence between programmable and custom platforms.", "conclusion": "Pipeline stage analysis effectively identifies bottlenecks, facilitating design strategies for predictable timing closure in FPGA and ASIC-based processors."}}
{"id": "2512.14098", "pdf": "https://arxiv.org/pdf/2512.14098", "abs": "https://arxiv.org/abs/2512.14098", "authors": ["Jeff J. Ma", "Jae-Won Chung", "Jisang Ahn", "Yizhuo Liang", "Akshay Jajoo", "Myungjin Lee", "Mosharaf Chowdhury"], "title": "Cornserve: Efficiently Serving Any-to-Any Multimodal Models", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.\n  Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\\times$ throughput improvement and up to 5.79$\\times$ tail latency reduction over existing solutions.", "AI": {"tldr": "Cornserve is an online serving system designed for Any-to-Any multimodal models, enhancing efficiency in handling diverse computational paths and scaling challenges during deployment.", "motivation": "The paper aims to address the complexity introduced by Any-to-Any multimodal models in serving processes, as these models require efficient management of heterogeneous components and computation paths.", "method": "Cornserve enables developers to define computation graphs with diverse components and automatically generates optimized deployment plans. It uses a distributed runtime to execute these plans efficiently during online serving.", "result": "The system shows significant improvements in performance, delivering as much as 3.81\u00d7 throughput improvement and up to 5.79\u00d7 reductions in tail latency compared to existing methods.", "conclusion": "Cornserve demonstrates its efficiency in serving Any-to-Any multimodal models, resolving heterogeneity and scalability issues inherent in modern model serving systems."}}
{"id": "2512.14233", "pdf": "https://arxiv.org/pdf/2512.14233", "abs": "https://arxiv.org/abs/2512.14233", "authors": ["Ruozhao Yang", "Mingfei Cheng", "Gelei Deng", "Tianwei Zhang", "Junjie Wang", "Xiaofei Xie"], "title": "PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": "13 pages, 6 figures", "summary": "Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.", "AI": {"tldr": "The paper presents PentestEval, a benchmark for evaluating large language models (LLMs) in comprehensive penetration testing tasks, demonstrating their current limitations and areas for improvement.", "motivation": "Penetration testing workflows are currently manual and expertise-intensive, lacking scalability, even as LLMs show potential for automation.", "method": "Introduced PentestEval, a benchmark integrating expert annotations and automated evaluation to assess LLMs across six penetration testing stages in 12 realistic scenarios.", "result": "Stage-level evaluations of 9 LLMs showed generally weak performance, with end-to-end pipelines achieving a success rate of 31% and autonomous agents failing significantly.", "conclusion": "PentestEval provides a foundation for improving structured reasoning and modularization in LLM-based penetration testing to advance automation reliability."}}
{"id": "2512.14082", "pdf": "https://arxiv.org/pdf/2512.14082", "abs": "https://arxiv.org/abs/2512.14082", "authors": ["Siran Liu", "Zane Cao", "Yongchao He"], "title": "A Unified Sparse Attention via Multi-Granularity Compression", "categories": ["cs.CL"], "comment": null, "summary": "Efficient long-context understanding and reasoning are increasingly vital for large language model (LLM) applications such as multi-turn dialogue and program analysis. However, the core self-attention mechanism scales quadratically with sequence length, creating a fundamental computational bottleneck. Existing sparse attention methods alleviate this issue but face trade-offs: training-based methods are costly and cannot be directly applied as acceleration plugins for other models, while inference-time methods often compromise efficiency or cross-modal generality. To address these limitations, we present UniSparse, a unified mechanism that introduces the notion of composite tokens--compact representations that aggregate multi-granularity contextual information. Building on this abstraction, UniSparse dynamically constructs sparse attention through multi-granularity compression and block-level selection, enabling efficient and hardware-friendly execution on GPU. Across multiple modalities and tasks ranging from synthetic benchmarks to real-world applications, UniSparse consistently surpasses state-of-the-art sparse attention methods (e.g., MInference, XAttention, FlexPrefill) in both accuracy and efficiency, achieving $\\ge$ 99% of full-attention accuracy and up to 2.61$\\times$ faster attention computation than FlashAttention.", "AI": {"tldr": "The study introduces UniSparse, a novel mechanism that improves efficiency and accuracy in large language models by optimizing sparse attention mechanics.", "motivation": "The motivation is to address computational inefficiencies in the self-attention mechanism of large language models, which struggle with long-context tasks due to quadratic scaling with sequence length.", "method": "UniSparse introduces composite tokens that aggregate contextual information, enabling multi-granularity compression and efficient block-level sparse attention, which is hardware-friendly for GPUs.", "result": "UniSparse surpasses existing sparse attention methods in both accuracy (\u226599% of full-attention) and efficiency (up to 2.61\u00d7 faster attention computation).", "conclusion": "UniSparse offers a practical solution for improving sparse attention mechanisms, combining state-of-the-art accuracy and efficiency across diverse modalities and tasks."}}
{"id": "2512.14046", "pdf": "https://arxiv.org/pdf/2512.14046", "abs": "https://arxiv.org/abs/2512.14046", "authors": ["Boyang Li", "Zhongpeng Jin", "Shuai Zhao", "Jiahui Liao", "Tian Liu", "Han Liu", "Yuanhai Zhang", "Kai Huang"], "title": "E-Navi: Environmental Adaptive Navigation for UAVs on Resource Constrained Platforms", "categories": ["cs.RO"], "comment": null, "summary": "The ability to adapt to changing environments is crucial for the autonomous navigation systems of Unmanned Aerial Vehicles (UAVs). However, existing navigation systems adopt fixed execution configurations without considering environmental dynamics based on available computing resources, e.g., with a high execution frequency and task workload. This static approach causes rigid flight strategies and excessive computations, ultimately degrading flight performance or even leading to failures in UAVs. Despite the necessity for an adaptive system, dynamically adjusting workloads remains challenging, due to difficulties in quantifying environmental complexity and modeling the relationship between environment and system configuration. Aiming at adapting to dynamic environments, this paper proposes E-Navi, an environmental-adaptive navigation system for UAVs that dynamically adjusts task executions on the CPUs in response to environmental changes based on available computational resources. Specifically, the perception-planning pipeline of UAVs navigation system is redesigned through dynamic adaptation of mapping resolution and execution frequency, driven by the quantitative environmental complexity evaluations. In addition, E-Navi supports flexible deployment across hardware platforms with varying levels of computing capability. Extensive Hardware-In-the-Loop and real-world experiments demonstrate that the proposed system significantly outperforms the baseline method across various hardware platforms, achieving up to 53.9% navigation task workload reduction, up to 63.8% flight time savings, and delivering more stable velocity control.", "AI": {"tldr": "This paper introduces E-Navi, an environmental-adaptive navigation system for UAVs, which dynamically adjusts computations based on environmental complexity and resources, achieving noteworthy gains in performance and efficiency.", "motivation": "To address the limitations of static execution configurations in existing UAV navigation systems, which result in rigid strategies, excessive computations, and degraded flight performance.", "method": "The paper redesigns the UAV navigation pipeline by adapting mapping resolution and execution frequency based on environmental complexity evaluations and integrates dynamic execution onto CPUs. It also supports hardware flexibility.", "result": "E-Navi achieves up to 53.9% reduction in navigation task workload, up to 63.8% savings in flight time, and ensures more stable velocity control, outperforming baseline methods.", "conclusion": "The proposed E-Navi system proves effective in enhancing UAV navigation by leveraging adaptive environmental response and flexible computing integration, offering significant benefits in efficiency and flight stability."}}
{"id": "2512.14311", "pdf": "https://arxiv.org/pdf/2512.14311", "abs": "https://arxiv.org/abs/2512.14311", "authors": ["Pablo Garc\u00eda-Santaclara", "Bruno Fern\u00e1ndez-Castro", "Rebeca P. D\u00edaz-Redondo", "Carlos Calvo-Moa", "Henar Mari\u00f1o-Bodel\u00f3n"], "title": "Continual Learning at the Edge: An Agnostic IIoT Architecture", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "The exponential growth of Internet-connected devices has presented challenges to traditional centralized computing systems due to latency and bandwidth limitations. Edge computing has evolved to address these difficulties by bringing computations closer to the data source. Additionally, traditional machine learning algorithms are not suitable for edge-computing systems, where data usually arrives in a dynamic and continual way. However, incremental learning offers a good solution for these settings. We introduce a new approach that applies the incremental learning philosophy within an edge-computing scenario for the industrial sector with a specific purpose: real time quality control in a manufacturing system. Applying continual learning we reduce the impact of catastrophic forgetting and provide an efficient and effective solution.", "AI": {"tldr": "The paper discusses using incremental learning in edge-computing for real-time quality control in manufacturing, addressing challenges of latency, bandwidth, and catastrophic forgetting.", "motivation": "The motivation is to overcome the limitations of centralized computing systems and adapt machine learning algorithms for dynamic, continuous data in edge-computing setups, especially for industrial use-cases.", "method": "The method involves developing an incremental learning framework tailored to edge-computing environments, focused on continual learning to tackle catastrophic forgetting and improve efficiency.", "result": "The approach successfully demonstrates effectiveness in real-time quality control tasks within a manufacturing system using edge-computing and incremental learning.", "conclusion": "Incremental learning integrated with edge computing proves suitable for real-time industrial applications, effectively addressing latency, bandwidth, and forgetting issues."}}
{"id": "2512.13711", "pdf": "https://arxiv.org/pdf/2512.13711", "abs": "https://arxiv.org/abs/2512.13711", "authors": ["Aadya Goel", "Mayuri Sridhar"], "title": "Delete and Retain: Efficient Unlearning for Document Classification", "categories": ["cs.LG"], "comment": "18 pages, 5 figures", "summary": "Machine unlearning aims to efficiently remove the influence of specific training data from a model without full retraining. While much progress has been made in unlearning for LLMs, document classification models remain relatively understudied. In this paper, we study class-level unlearning for document classifiers and present Hessian Reassignment, a two-step, model-agnostic solution. First, we perform a single influence-style update that subtracts the contribution of all training points from the target class by solving a Hessian-vector system with conjugate gradients, requiring only gradient and Hessian-vector products. Second, in contrast to common unlearning baselines that randomly reclassify deleted-class samples, we enforce a decision-space guarantee via Top-1 classification. On standard text benchmarks, Hessian Reassignment achieves retained-class accuracy close to full retrain-without-class while running orders of magnitude faster. Additionally, it consistently lowers membership-inference advantage on the removed class, measured with pooled multi-shadow attacks. These results demonstrate a practical, principled path to efficient class unlearning in document classification.", "AI": {"tldr": "The paper introduces Hessian Reassignment for class-level unlearning in document classification models, providing an efficient and faster alternative to retraining, while maintaining high accuracy and security benefits.", "motivation": "The need to efficiently remove specific training data influence from document classification models has not been fully addressed, despite advances in similar methods for LLMs.", "method": "The proposed method, Hessian Reassignment, uses a two-step solution: (1) Influence-style update via solving Hessian-vector systems using conjugate gradients; (2) Enforcing decision-space guarantees with Top-1 classification instead of random reclassification.", "result": "Hessian Reassignment shows retained-class accuracy comparable to full retraining, runs significantly faster, and effectively reduces membership-inference advantage for removed class samples.", "conclusion": "This approach offers a practical and principled solution for efficient class-level unlearning in document classifiers, enhancing both performance and security."}}
{"id": "2512.13753", "pdf": "https://arxiv.org/pdf/2512.13753", "abs": "https://arxiv.org/abs/2512.13753", "authors": ["Mika Sipil\u00e4", "Sabrina Maggio", "Sandra De Iaco", "Klaus Nordhausen", "Monica Palma", "Sara Taskinen"], "title": "Time-aware UNet and super-resolution deep residual networks for spatial downscaling", "categories": ["cs.CV", "cs.LG", "eess.IV", "stat.ML"], "comment": null, "summary": "Satellite data of atmospheric pollutants are often available only at coarse spatial resolution, limiting their applicability in local-scale environmental analysis and decision-making. Spatial downscaling methods aim to transform the coarse satellite data into high-resolution fields. In this work, two widely used deep learning architectures, the super-resolution deep residual network (SRDRN) and the encoder-decoder-based UNet, are considered for spatial downscaling of tropospheric ozone. Both methods are extended with a lightweight temporal module, which encodes observation time using either sinusoidal or radial basis function (RBF) encoding, and fuses the temporal features with the spatial representations in the networks. The proposed time-aware extensions are evaluated against their baseline counterparts in a case study on ozone downscaling over Italy. The results suggest that, while only slightly increasing computational complexity, the temporal modules significantly improve downscaling performance and convergence speed.", "AI": {"tldr": "The paper explores deep learning methods for enhancing spatial resolution of satellite data on atmospheric pollutants, focusing on tropospheric ozone, by introducing time-aware extensions for improved performance.", "motivation": "Satellite data of atmospheric pollutants generally lack fine spatial resolution, hindering their use in localized environmental applications; the study addresses this limitation by advancing spatial downscaling techniques.", "method": "The approach involves applying deep learning architectures (SRDRN and UNet) augmented with lightweight temporal modules using sinusoidal or radial basis function encoding to integrate temporal features.", "result": "In a case study over Italy, the time-aware models exhibited enhanced downscaling accuracy and faster convergence compared to baseline methods, with minimal computational trade-offs.", "conclusion": "Integrating temporal encoding into spatial downscaling networks provides a meaningful improvement in performance and practicality for environmental analysis centered around ozone mapping."}}
{"id": "2512.13764", "pdf": "https://arxiv.org/pdf/2512.13764", "abs": "https://arxiv.org/abs/2512.13764", "authors": ["Przemyslaw Chojecki"], "title": "Mathematics and Coding are Universal AI Benchmarks", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "We study the special role of mathematics and coding inside the moduli space of psychometric batteries for AI agents. Building on the AAI framework and GVU dynamics from previous works, we define the Mathematics Fiber and show that, when paired with formal proof kernels (e.g. Lean, Coq), GVU flows on this fiber admit spectrally stable self-improvement regimes due to oracle-like verification. Our main technical result is a density theorem: under uniform tightness of agent outputs and a Lipschitz AAI functional, the subspace of batteries generated by mathematical theorem-proving and coding tasks is dense in the moduli space of batteries with respect to the evaluation metric. Coding alone is universal in this sense, while pure mathematics is not; its privilege is spectral rather than expressive. We interpret this as evidence that mathematics and coding provide ``universal coordinates'' for evaluation, and that formal mathematics is a natural ignition domain for recursive self-improvement in advanced AI agents.", "AI": {"tldr": "The paper studies mathematics and coding in the evaluation of AI agents, showing their universality and importance in recursive self-improvement.", "motivation": "To explore the relationship of mathematics and coding with the performance and evaluation of psychometric batteries for AI agents, particularly within the AAI framework and GVU dynamics.", "method": "The authors define the Mathematics Fiber and analyze GVU flows on this fiber using formal proof kernels, proving that under certain conditions, the subspace of batteries tied to mathematics and coding is dense in the moduli space of all psychometric batteries.", "result": "Mathematics and coding (especially coding) were shown to universally represent evaluation metrics in AI frameworks, signifying their importance as coordinates for psychometric evaluation.", "conclusion": "Mathematics and coding serve as universal coordinates for AI evaluation, and formal mathematical tasks ignite recursive self-improvement, particularly via spectral stability advantages of mathematics."}}
{"id": "2512.14522", "pdf": "https://arxiv.org/pdf/2512.14522", "abs": "https://arxiv.org/abs/2512.14522", "authors": ["Jacob Taegon Kim", "Alex Sim", "Kesheng Wu", "Jinoh Kim"], "title": "Improving Slow Transfer Predictions: Generative Methods Compared", "categories": ["cs.LG", "cs.DC", "cs.NI"], "comment": null, "summary": "Monitoring data transfer performance is a crucial task in scientific computing networks. By predicting performance early in the communication phase, potentially sluggish transfers can be identified and selectively monitored, optimizing network usage and overall performance. A key bottleneck to improving the predictive power of machine learning (ML) models in this context is the issue of class imbalance. This project focuses on addressing the class imbalance problem to enhance the accuracy of performance predictions. In this study, we analyze and compare various augmentation strategies, including traditional oversampling methods and generative techniques. Additionally, we adjust the class imbalance ratios in training datasets to evaluate their impact on model performance. While augmentation may improve performance, as the imbalance ratio increases, the performance does not significantly improve. We conclude that even the most advanced technique, such as CTGAN, does not significantly improve over simple stratified sampling.", "AI": {"tldr": "This paper addresses data class imbalance in machine learning models for predicting data transfer performance by comparing augmentation techniques and adjusting imbalance ratios.", "motivation": "To identify and monitor potentially sluggish data transfers early, improving the predictive power of ML models, despite challenges posed by class imbalance.", "method": "The study compares traditional oversampling and generative augmentation (e.g., CTGAN) alongside dataset imbalance ratio adjustments for training ML models.", "result": "Augmentation improves performance to some extent, but as the imbalance ratio increases, performance gains diminish. CTGAN shows no significant improvement over simple stratified sampling.", "conclusion": "Class imbalance remains a challenge, and even advanced augmentation strategies may not significantly outperform simpler methods like stratified sampling."}}
{"id": "2512.14453", "pdf": "https://arxiv.org/pdf/2512.14453", "abs": "https://arxiv.org/abs/2512.14453", "authors": ["Fabiola Moy\u00f3n", "Florian Angermeir", "Daniel Mendez", "Tony Gorschek", "Markus Voggenreiter", "Pierre-Louis Bonvin"], "title": "Aligning Security Compliance and DevOps: A Longitudinal Study", "categories": ["cs.SE", "cs.CR"], "comment": null, "summary": "Companies adopt agile methodologies and DevOps to facilitate efficient development and deployment of software-intensive products. This, in turn, introduces challenges in relation to security standard compliance traditionally following a more linear workflow. This is especially a challenge for the engineering of products and services associated with critical infrastructures. To support companies in their transition towards DevOps, this paper presents an adaptation of DevOps according to security regulations and standards. We report on our longitudinal study at Siemens AG, consisting of several individual sub-studies in the inception, validation, and initial adoption of our framework based on RefA as well as the implications for practice. RefA is a prescriptive model of a security compliant DevOps lifecycle based on the IEC 62443-4-1 standard. The overall framework is aimed at professionals, not only security experts, being able to use it on implementing DevOps processes while remaining compliant with security norms. We demonstrate how RefA facilitates the transfer of security compliance knowledge to product development teams. This knowledge transfer supports the agility aim of ensuring that cross-functional teams have all the skills needed to deliver the compliant products.", "AI": {"tldr": "The paper presents RefA, a framework enabling security-compliant DevOps processes in critical infrastructures, validated through a study at Siemens AG.", "motivation": "Agile and DevOps practices introduce challenges in meeting security compliance, especially in critical infrastructures.", "method": "Conducted a longitudinal study involving individual sub-studies at Siemens AG to validate and adopt the security-compliant DevOps framework, RefA, based on IEC 62443-4-1 standard.", "result": "RefA successfully transferred security compliance knowledge to product teams, ensuring agility and adherence to security norms.", "conclusion": "RefA equips cross-functional teams to implement DevOps processes while ensuring security compliance, supporting efficient product development."}}
{"id": "2512.14085", "pdf": "https://arxiv.org/pdf/2512.14085", "abs": "https://arxiv.org/abs/2512.14085", "authors": ["Koji Inoue", "Mikey Elmers", "Yahui Fu", "Zi Haur Pang", "Taiga Mori", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "title": "Multilingual and Continuous Backchannel Prediction: A Cross-lingual Study", "categories": ["cs.CL", "cs.HC", "cs.SD"], "comment": "This paper has been accepted for presentation at International Workshop on Spoken Dialogue Systems Technology 2026 (IWSDS 2026) and represents the author's version of the work", "summary": "We present a multilingual, continuous backchannel prediction model for Japanese, English, and Chinese, and use it to investigate cross-linguistic timing behavior. The model is Transformer-based and operates at the frame level, jointly trained with auxiliary tasks on approximately 300 hours of dyadic conversations. Across all three languages, the multilingual model matches or surpasses monolingual baselines, indicating that it learns both language-universal cues and language-specific timing patterns. Zero-shot transfer with two-language training remains limited, underscoring substantive cross-lingual differences. Perturbation analyses reveal distinct cue usage: Japanese relies more on short-term linguistic information, whereas English and Chinese are more sensitive to silence duration and prosodic variation; multilingual training encourages shared yet adaptable representations and reduces overreliance on pitch in Chinese. A context-length study further shows that Japanese is relatively robust to shorter contexts, while Chinese benefits markedly from longer contexts. Finally, we integrate the trained model into a real-time processing software, demonstrating CPU-only inference. Together, these findings provide a unified model and empirical evidence for how backchannel timing differs across languages, informing the design of more natural, culturally-aware spoken dialogue systems.", "AI": {"tldr": "Proposes a Transformer-based multilingual backchannel prediction model for Japanese, English, and Chinese, analyzing cross-linguistic timing behaviors.", "motivation": "To understand cross-linguistic backchannel timing behaviors and develop a model for spoken dialogue systems.", "method": "A Transformer-based model trained on 300 hours of dyadic conversations, using auxiliary tasks for language learning.", "result": "Multilingual model performs well, showing language-universal and specific timing patterns, but limited zero-shot transfer across languages.", "conclusion": "Provides insights into language-specific backchannel timing, enabling culturally-aware spoken dialogue systems; also demonstrates real-time software integration."}}
{"id": "2512.14054", "pdf": "https://arxiv.org/pdf/2512.14054", "abs": "https://arxiv.org/abs/2512.14054", "authors": ["Humaira Tasnim", "Ashik E Rasul", "Bruce Jo", "Hyung-Jin Yoon"], "title": "Expert Switching for Robust AAV Landing: A Dual-Detector Framework in Simulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Reliable helipad detection is essential for Autonomous Aerial Vehicle (AAV) landing, especially under GPS-denied or visually degraded conditions. While modern detectors such as YOLOv8 offer strong baseline performance, single-model pipelines struggle to remain robust across the extreme scale transitions that occur during descent, where helipads appear small at high altitude and large near touchdown. To address this limitation, we propose a scale-adaptive dual-expert perception framework that decomposes the detection task into far-range and close-range regimes. Two YOLOv8 experts are trained on scale-specialized versions of the HelipadCat dataset, enabling one model to excel at detecting small, low-resolution helipads and the other to provide high-precision localization when the target dominates the field of view. During inference, both experts operate in parallel, and a geometric gating mechanism selects the expert whose prediction is most consistent with the AAV's viewpoint. This adaptive routing prevents the degradation commonly observed in single-detector systems when operating across wide altitude ranges. The dual-expert perception module is evaluated in a closed-loop landing environment that integrates CARLA's photorealistic rendering with NASA's GUAM flight-dynamics engine. Results show substantial improvements in alignment stability, landing accuracy, and overall robustness compared to single-detector baselines. By introducing a scale-aware expert routing strategy tailored to the landing problem, this work advances resilient vision-based perception for autonomous descent and provides a foundation for future multi-expert AAV frameworks.", "AI": {"tldr": "The paper proposes a dual-expert detection framework that uses two YOLOv8 models to accurately detect helipads at various altitudes during AAV landing, showing improved robustness and precision compared to traditional single-detector systems.", "motivation": "To improve the robustness and accuracy of helipad detection for autonomous aerial vehicle (AAV) landings, especially under challenging scale transitions during descent.", "method": "Introduced a dual-expert framework with two YOLOv8 models, specialized for detecting helipads at different scales (far-range and close-range). During inference, a geometric gating mechanism selects predictions based on the AAV's viewpoint.", "result": "In a simulation environment using CARLA and NASA's GUAM engine, the proposed system demonstrated significant improvements in landing alignment, accuracy, and robustness compared to single-detector methods.", "conclusion": "The scale-adaptive dual-expert framework enhances vision-based perception for autonomous descents and sets a foundation for more advanced multi-expert frameworks for AAVs."}}
{"id": "2512.14404", "pdf": "https://arxiv.org/pdf/2512.14404", "abs": "https://arxiv.org/abs/2512.14404", "authors": ["Hangjun Cho", "Fabio V. G. Amaral", "Andrei A. Klishin", "Cassio M. Oishi", "Steven L. Brunton"], "title": "From STLS to Projection-based Dictionary Selection in Sparse Regression for System Identification", "categories": ["stat.ML", "cs.LG", "math.OC", "physics.comp-ph"], "comment": "34 pages, 11 figures", "summary": "In this work, we revisit dictionary-based sparse regression, in particular, Sequential Threshold Least Squares (STLS), and propose a score-guided library selection to provide practical guidance for data-driven modeling, with emphasis on SINDy-type algorithms. STLS is an algorithm to solve the $\\ell_0$ sparse least-squares problem, which relies on splitting to efficiently solve the least-squares portion while handling the sparse term via proximal methods. It produces coefficient vectors whose components depend on both the projected reconstruction errors, here referred to as the scores, and the mutual coherence of dictionary terms. The first contribution of this work is a theoretical analysis of the score and dictionary-selection strategy. This could be understood in both the original and weak SINDy regime. Second, numerical experiments on ordinary and partial differential equations highlight the effectiveness of score-based screening, improving both accuracy and interpretability in dynamical system identification. These results suggest that integrating score-guided methods to refine the dictionary more accurately may help SINDy users in some cases to enhance their robustness for data-driven discovery of governing equations.", "AI": {"tldr": "This paper revisits Sequential Threshold Least Squares (STLS) for sparse regression, proposing score-guided library selection to improve SINDy-type algorithms for modeling dynamical systems.", "motivation": "To enhance accuracy and interpretability in data-driven modeling of dynamical systems, particularly in using SINDy algorithms, by addressing limitations in sparse regression techniques.", "method": "Theoretical analysis of score and dictionary selection in STLS, combined with numerical experiments showcasing effectiveness in dynamical system identification.", "result": "Score-guided screening improves accuracy and robustness in identifying governing equations using ordinary and partial differential equations.", "conclusion": "Integrating score-guided methods into sparse regression can benefit SINDy users by refining dictionaries for data-driven modeling of dynamical systems."}}
{"id": "2512.13712", "pdf": "https://arxiv.org/pdf/2512.13712", "abs": "https://arxiv.org/abs/2512.13712", "authors": ["Eric Guo"], "title": "Prediction of Respiratory Syncytial Virus-Associated Hospitalizations Using Machine Learning Models Based on Environmental Data", "categories": ["cs.LG", "stat.AP", "stat.ME"], "comment": null, "summary": "Respiratory syncytial virus (RSV) is a leading cause of hospitalization among young children, with outbreaks strongly influenced by environmental conditions. This study developed a machine learning framework to predict RSV-associated hospitalizations in the United States (U.S.) by integrating wastewater surveillance, meteorological, and air quality data. The dataset combined weekly hospitalization rates, wastewater RSV levels, daily meteorological measurements, and air pollutant concentrations. Classification models, including CART, Random Forest, and Boosting, were trained to predict weekly RSV-associated hospitalization rates classified as \\textit{Low risk}, \\textit{Alert}, and \\textit{Epidemic} levels. The wastewater RSV level was identified as the strongest predictor, followed by meteorological and air quality variables such as temperature, ozone levels, and specific humidity. Notably, the analysis also revealed significantly higher RSV-associated hospitalization rates among Native Americans and Alaska Natives. Further research is needed to better understand the drivers of RSV disparity in these communities to improve prevention strategies. Furthermore, states at high altitudes, characterized by lower surface pressure, showed consistently higher RSV-associated hospitalization rates. These findings highlight the value of combining environmental and community surveillance data to forecast RSV outbreaks, enabling more timely public health interventions and resource allocation. In order to provide accessibility and practical use of the models, we have developed an interactive R Shiny dashboard (https://f6yxlu-eric-guo.shinyapps.io/rsv_app/), which allows users to explore RSV-associated hospitalization risk levels across different states, visualize the impact of key predictors, and interactively generate RSV outbreak forecasts.", "AI": {"tldr": "The study developed a machine learning model combining wastewater, meteorological, and air quality data to predict RSV-associated hospitalization rates in the U.S.", "motivation": "Understand environmental factors driving RSV hospitalizations and build predictive tools for public health interventions.", "method": "Machine learning models (CART, Random Forest, Boosting) trained on datasets incorporating hospitalization, environmental, and air quality data.", "result": "Wastewater RSV levels were the strongest predictors. Hospitalization rates were higher among Native Americans, Alaska Natives, and individuals in high-altitude states.", "conclusion": "The integrated model effectively forecasts RSV outbreaks. The developed dashboard facilitates user-friendly exploration and prediction of hospitalization risks."}}
{"id": "2512.13796", "pdf": "https://arxiv.org/pdf/2512.13796", "abs": "https://arxiv.org/abs/2512.13796", "authors": ["Victor Rong", "Jan Held", "Victor Chu", "Daniel Rebain", "Marc Van Droogenbroeck", "Kiriakos N. Kutulakos", "Andrea Tagliasacchi", "David B. Lindell"], "title": "Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries", "categories": ["cs.CV"], "comment": "Webpage at https://lessvrong.com/cs/nexels", "summary": "Though Gaussian splatting has achieved impressive results in novel view synthesis, it requires millions of primitives to model highly textured scenes, even when the geometry of the scene is simple. We propose a representation that goes beyond point-based rendering and decouples geometry and appearance in order to achieve a compact representation. We use surfels for geometry and a combination of a global neural field and per-primitive colours for appearance. The neural field textures a fixed number of primitives for each pixel, ensuring that the added compute is low. Our representation matches the perceptual quality of 3D Gaussian splatting while using $9.7\\times$ fewer primitives and $5.5\\times$ less memory on outdoor scenes and using $31\\times$ fewer primitives and $3.7\\times$ less memory on indoor scenes. Our representation also renders twice as fast as existing textured primitives while improving upon their visual quality.", "AI": {"tldr": "The paper proposes a new technique to enhance novel view synthesis by using compact surfel-based geometry and neural field-driven appearance, reducing data requirements and increasing rendering speed compared to Gaussian splatting.", "motivation": "While Gaussian splatting is effective for novel view synthesis, it is computationally expensive and requires millions of primitives, making it inefficient for scenes with simple geometry but complex textures.", "method": "The research introduces a representation using surfels for geometry and a blend of a global neural field with per-primitive colors for appearance. It textures primitives with low computational demand, ensuring geometric and appearance decoupling for compactness.", "result": "The method achieves similar perceptual quality to Gaussian splatting while drastically reducing primitives and memory usage (9.7x fewer for outdoor scenes, 31x fewer for indoor). It also halves the rendering time compared to current textured primitive methods.", "conclusion": "This new representation offers a compact, memory-efficient, and faster solution for novel view synthesis without compromising visual quality."}}
{"id": "2512.13771", "pdf": "https://arxiv.org/pdf/2512.13771", "abs": "https://arxiv.org/abs/2512.13771", "authors": ["Javier Mar\u00edn"], "title": "Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems", "categories": ["cs.AI"], "comment": null, "summary": "When retrieval-augmented generation (RAG) systems hallucinate, what geometric trace does this leave in embedding space? We introduce the Semantic Grounding Index (SGI), defined as the ratio of angular distances from the response to the question versus the context on the unit hypersphere $\\mathbb{S}^{d-1}$.Our central finding is \\emph{semantic laziness}: hallucinated responses remain angularly proximate to questions rather than departing toward retrieved contexts. On HaluEval ($n$=5,000), we observe large effect sizes (Cohen's $d$ ranging from 0.92 to 1.28) across five embedding models with mean cross-model correlation $r$=0.85. Crucially, we derive from the spherical triangle inequality that SGI's discriminative power should increase with question-context angular separation $\u03b8(q,c)$-a theoretical prediction confirmed empirically: effect size rises monotonically from $d$=0.61 -low $\u03b8(q,c)$, to $d$=1.27 -high $\u03b8(q,c)$, with AUC improving from 0.72 to 0.83. Subgroup analysis reveals that SGI excels on long responses ($d$=2.05) and short questions ($d$=1.22), while remaining robust across context lengths. Calibration analysis yields ECE=0.10, indicating SGI scores can serve as probability estimates, not merely rankings. A critical negative result on TruthfulQA (AUC=0.478) establishes that angular geometry measures topical engagement rather than factual accuracy. SGI provides computationally efficient, theoretically grounded infrastructure for identifying responses that warrant verification in production RAG deployments.", "AI": {"tldr": "This paper introduces the Semantic Grounding Index (SGI) to evaluate geometric traces of hallucinations in RAG systems, focusing on the angular closeness of responses to questions versus retrieved contexts.", "motivation": "To address the challenge of hallucinations in RAG systems by analyzing their embedding space behavior and to develop a measurable index for identifying such behaviors.", "method": "The authors propose the SGI metric, grounded in the spherical triangle inequality, to measure angular distances in embedding space. They empirically test it across embedding models and analyze its performance via calibration, subgroup analysis, and comparison.", "result": "The authors identify 'semantic laziness,' where hallucinated responses remain close to questions and not contexts, validated by SGI's empirical effectiveness with high effect sizes and AUC scores across varied setups.", "conclusion": "SGI proves to be a theoretically sound and practical tool in identifying hallucinations in RAG systems, though it measures topical contextuality and not factual accuracy."}}
{"id": "2512.14475", "pdf": "https://arxiv.org/pdf/2512.14475", "abs": "https://arxiv.org/abs/2512.14475", "authors": ["Johann Glock", "Clemens Bauer", "Martin Pinzger"], "title": "Teralizer: Semantics-Based Test Generalization from Conventional Unit Tests to Property-Based Tests", "categories": ["cs.SE"], "comment": null, "summary": "Conventional unit tests validate single input-output pairs, leaving most inputs of an execution path untested. Property-based testing addresses this shortcoming by generating multiple inputs satisfying properties but requires significant manual effort to define properties and their constraints. We propose a semantics-based approach that automatically transforms unit tests into property-based tests by extracting specifications from implementations via single-path symbolic analysis. We demonstrate this approach through Teralizer, a prototype for Java that transforms JUnit tests into property-based jqwik tests. Unlike prior work that generalizes from input-output examples, Teralizer derives specifications from program semantics.\n  We evaluated Teralizer on three progressively challenging datasets. On EvoSuite-generated tests for EqBench and Apache Commons utilities, Teralizer improved mutation scores by 1-4 percentage points. Generalization of mature developer-written tests from Apache Commons utilities showed only 0.05-0.07 percentage points improvement. Analysis of 632 real-world Java projects from RepoReapers highlights applicability barriers: only 1.7% of projects completed the generalization pipeline, with failures primarily due to type support limitations in symbolic analysis and static analysis limitations in our prototype. Based on the results, we provide a roadmap for future work, identifying research and engineering challenges that need to be tackled to advance the field of test generalization.\n  Artifacts available at: https://doi.org/10.5281/zenodo.17950381", "AI": {"tldr": "This paper introduces Teralizer, a tool that automates the transformation of unit tests into property-based tests using symbolic analysis of program semantics, improving coverage but facing limitations in real-world applications.", "motivation": "Unit tests often test only specific input-output pairs, missing broader input sets for execution paths. Defining properties manually for property-based testing is labor-intensive and requires expertise.", "method": "The authors propose Teralizer, which leverages single-path symbolic analysis to automatically derive test properties from program semantics and transform JUnit tests into property-based jqwik tests.", "result": "Teralizer boosted mutation scores for generated tests by 1-4 points but minimally improved mature developer-written tests. Analysis of 632 real-world Java projects revealed practical challenges, suggesting limited current applicability.", "conclusion": "Teralizer demonstrates potential for enhancing testing through automation but requires addressing significant limitations, such as type support in analysis, to realize broader adoption and impact in real-world projects."}}
{"id": "2512.14118", "pdf": "https://arxiv.org/pdf/2512.14118", "abs": "https://arxiv.org/abs/2512.14118", "authors": ["Yiran Zhang", "Jincheng Hu", "Mark Dras", "Usman Naseem"], "title": "CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "underreview", "summary": "Large language models (LLMs) excel at single-turn reasoning but often lose accuracy and coherence over extended, multi-turn interactions. Recent evaluations such as TurnBench highlight recurring failure modes-reasoning bias, task drift, hallucination, overconfidence, and memory decay. Current approaches typically append full conversational histories, causing unbounded context growth, higher computational costs, and degraded reasoning efficiency. We introduce CogMem, a cognitively inspired, memory-augmented LLM architecture that supports sustained iterative reasoning through structured, persistent memory. CogMem incorporates three layers: a Long-Term Memory (LTM) that consolidates cross-session reasoning strategies; a Direct Access (DA) memory that maintains session-level notes and retrieves relevant long-term memories; and a Focus of Attention (FoA) mechanism that dynamically reconstructs concise, task-relevant context at each turn. Experiments on TurnBench show that this layered design mitigates reasoning failures, controls context growth, and improves consistency across extended reasoning chains, moving toward more reliable, human-like reasoning in LLMs.", "AI": {"tldr": "The paper introduces CogMem, a memory-augmented LLM designed for sustained and reliable multi-turn reasoning by addressing challenges like task drift and memory decay.", "motivation": "To address the challenges of loss in accuracy, coherence, and efficiency during multi-turn reasoning with LLMs, as identified in evaluations such as TurnBench.", "method": "They propose the CogMem architecture, which uses a three-layer memory design: Long-Term Memory, Direct Access memory, and a Focus of Attention mechanism, allowing better task context management and iterative reasoning.", "result": "Experiments on TurnBench confirm that CogMem mitigates reasoning biases, context growth issues, and inconsistencies, improving the model\u2019s reasoning across extended interactions.", "conclusion": "CogMem supports more efficient and reliable multi-turn reasoning, achieving human-like reasoning capabilities in LLMs."}}
{"id": "2512.14057", "pdf": "https://arxiv.org/pdf/2512.14057", "abs": "https://arxiv.org/abs/2512.14057", "authors": ["Amir M. Soufi Enayati", "Homayoun Honari", "Homayoun Najjaran"], "title": "Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning", "categories": ["cs.RO"], "comment": null, "summary": "Reinforcement learning (RL) enables robots to operate in uncertain environments, but standard approaches often struggle with poor generalization to unseen tasks. Context-adaptive meta reinforcement learning addresses these limitations by conditioning on the task representation, yet they mostly rely on complete action information in the experience making task inference tightly coupled to a specific policy. This paper introduces Context Representation via Action Free Transformer encoder decoder (CRAFT), a belief model that infers task representations solely from sequences of states and rewards. By removing the dependence on actions, CRAFT decouples task inference from policy optimization, supports modular training, and leverages amortized variational inference for scalable belief updates. Built on a transformer encoder decoder with rotary positional embeddings, the model captures long range temporal dependencies and robustly encodes both parametric and non-parametric task variations. Experiments on the MetaWorld ML-10 robotic manipulation benchmark show that CRAFT achieves faster adaptation, improved generalization, and more effective exploration compared to context adaptive meta--RL baselines. These findings highlight the potential of action-free inference as a foundation for scalable RL in robotic control.", "AI": {"tldr": "The paper presents CRAFT, a model for task representation in RL, which improves generalization and adaptation by eliminating dependency on actions and decoupling task inference from policy optimization.", "motivation": "Standard reinforcement learning struggles with generalizing to unseen tasks and has tight coupling between task inference and specific policies.", "method": "CRAFT uses a transformer encoder-decoder architecture that leverages sequences of states and rewards, removing dependence on actions. It implements amortized variational inference and rotary positional embeddings for better task inference.", "result": "CRAFT outperforms context-adaptive meta-RL baselines in adaptation speed, generalization, and exploration on the MetaWorld ML-10 robotic benchmark.", "conclusion": "Action-free task inference, as introduced by CRAFT, is a promising approach for scalable reinforcement learning in robotic control."}}
{"id": "2512.14604", "pdf": "https://arxiv.org/pdf/2512.14604", "abs": "https://arxiv.org/abs/2512.14604", "authors": ["Prasanjit Dubey", "Aritra Guha", "Zhengyi Zhou", "Qiong Wu", "Xiaoming Huo", "Paromita Dubey"], "title": "LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Sparse longitudinal (SL) textual data arises when individuals generate text repeatedly over time (e.g., customer reviews, occasional social media posts, electronic medical records across visits), but the frequency and timing of observations vary across individuals. These complex textual data sets have immense potential to inform future policy and targeted recommendations. However, because SL text data lack dedicated methods and are noisy, heterogeneous, and prone to anomalies, detecting and inferring key patterns is challenging. We introduce LLmFPCA-detect, a flexible framework that pairs LLM-based text embeddings with functional data analysis to detect clusters and infer anomalies in large SL text datasets. First, LLmFPCA-detect embeds each piece of text into an application-specific numeric space using LLM prompts. Sparse multivariate functional principal component analysis (mFPCA) conducted in the numeric space forms the workhorse to recover primary population characteristics, and produces subject-level scores which, together with baseline static covariates, facilitate data segmentation, unsupervised anomaly detection and inference, and enable other downstream tasks. In particular, we leverage LLMs to perform dynamic keyword profiling guided by the data segments and anomalies discovered by LLmFPCA-detect, and we show that cluster-specific functional PC scores from LLmFPCA-detect, used as features in existing pipelines, help boost prediction performance. We support the stability of LLmFPCA-detect with experiments and evaluate it on two different applications using public datasets, Amazon customer-review trajectories, and Wikipedia talk-page comment streams, demonstrating utility across domains and outperforming state-of-the-art baselines.", "AI": {"tldr": "The paper introduces LLmFPCA-detect, a framework for analyzing sparse longitudinal textual data using LLMs and functional data analysis to detect clusters and anomalies.", "motivation": "Sparse longitudinal textual data is complex, noisy, and varies across individuals in timing and frequency, posing challenges for analysis despite its potentials in informing policy and targeting recommendations.", "method": "LLmFPCA-detect uses LLM-based text embeddings and sparse multivariate functional principal component analysis (mFPCA) for clustering, anomaly detection, and dynamic profiling in SL text datasets.", "result": "LLmFPCA-detect demonstrated stability and superior performance compared to state-of-the-art baselines in applications using Amazon customer reviews and Wikipedia comment streams.", "conclusion": "LLmFPCA-detect effectively combines LLM embeddings with functional data analysis to analyze SL text data, detect clusters and anomalies, and improve predictions in diverse applications."}}
{"id": "2512.13717", "pdf": "https://arxiv.org/pdf/2512.13717", "abs": "https://arxiv.org/abs/2512.13717", "authors": ["Ekaterina Sysoykova", "Bernhard Anzengruber-Tanase", "Michael Haslgrubler", "Philipp Seidl", "Alois Ferscha"], "title": "Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages", "summary": "Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.", "AI": {"tldr": "The paper introduces a Federated Few-Shot Learning (FFSL) method to address challenges in EEG-based seizure detection under data scarcity and privacy constraints.", "motivation": "EEG-based seizure detection faces challenges in clinical settings due to data scarcity, decentralized data distribution, and privacy regulations.", "method": "This study proposes a two-stage FFSL framework: Stage 1 involves fine-tuning a pretrained biosignal transformer using federated learning methods across distributed sites, while Stage 2 personalizes the classifier using few labeled EEG segments to each patient.", "result": "Federated fine-tuning achieved reasonable accuracy (balanced accuracy 0.43, Cohen's kappa 0.42, weighted F1 0.69). FFSL improved client-specific models' performance (average balanced accuracy 0.77, Cohen's kappa 0.62, weighted F1 0.73).", "conclusion": "The FFSL framework demonstrates potential for effective patient-specific EEG-based seizure detection under real-world constraints of distributed data and privacy regulations."}}
{"id": "2512.13834", "pdf": "https://arxiv.org/pdf/2512.13834", "abs": "https://arxiv.org/abs/2512.13834", "authors": ["Naman Balbir Singh Makkar"], "title": "VajraV1 -- The most accurate Real Time Object Detector of the YOLO family", "categories": ["cs.CV", "cs.AI"], "comment": "Technical Report. 20 Pages, 7 figures", "summary": "Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.\n  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.", "AI": {"tldr": "VajraV1 model offers improvements over YOLO-based detectors, achieving state-of-the-art accuracy and competitive speed.", "motivation": "To further enhance real-time object detection beyond YOLO models by improving accuracy while maintaining inference speed.", "method": "VajraV1 integrates architectural advancements from previous YOLO models to design a superior real-time object detection framework.", "result": "On COCO validation set, VajraV1 achieves significant mAP improvements across Nano, Small, Medium, Large, and Xlarge versions compared to YOLO-based detectors.", "conclusion": "VajraV1 establishes itself as the leading real-time object detector, surpassing previous YOLO models in accuracy while maintaining competitive speed."}}
{"id": "2512.14613", "pdf": "https://arxiv.org/pdf/2512.14613", "abs": "https://arxiv.org/abs/2512.14613", "authors": ["Cristiano Welter", "Kleinner Farias"], "title": "MoT: A Model-Driven Low-Code Approach for Simplifying Cloud-of-Things Application Development", "categories": ["cs.SE", "cs.ET", "cs.HC"], "comment": "20 pages, 23 figures, 4 tables", "summary": "The integration of cloud computing and the Internet of Things (IoT) is essential for scalable, intelligent systems. However, developing cloud-of-things (CoT) applications remains challenging. It requires significant technical expertise and lacks standardized, model-driven methodologies. Current approaches fail to ensure interoperability, automation, and efficiency. This study introduces the Model of Things (MoT), a model-based approach that incorporates low-code principles to simplify CoT development. MoT reduces technical barriers by providing a custom UML profile designed for IoT and cloud services. To evaluate MoT, we conducted a case study and a Technology Acceptance Model (TAM) questionnaire. The results confirmed MoT's feasibility, demonstrating that it streamlines CoT application development and deployment. Users found MoT accessible, even with limited IoT experience, and reported high perceived ease of use and usefulness. Qualitative feedback highlighted MoT's ability to reduce complexity and speed up development. MoT offers a promising, model-driven solution for CoT application development. By lowering entry barriers and promoting automation, it enhances both efficiency and flexibility. This study represents a step toward a more user-friendly framework, enabling broader adoption of CoT technologies.", "AI": {"tldr": "This paper introduces the Model of Things (MoT), a model-driven approach using low-code principles to simplify Cloud-of-Things (CoT) application development.", "motivation": "CoT application development faces challenges like high technical expertise demands and lack of standardized methods, hindering scalability and efficiency.", "method": "The study proposes MoT, a custom UML profile designed for IoT and cloud services, and evaluates it through a case study and a TAM questionnaire.", "result": "MoT was found feasible, reducing complexity and enhancing accessibility for users, even with limited IoT experience, while improving perceived ease of use and usefulness.", "conclusion": "MoT simplifies CoT application development by reducing barriers and promoting automation, offering a scalable and efficient solution for broader adoption of CoT technologies."}}
{"id": "2512.14142", "pdf": "https://arxiv.org/pdf/2512.14142", "abs": "https://arxiv.org/abs/2512.14142", "authors": ["Hongqiu Ni", "Jiabao Zhang", "Guopeng Li", "Zilong Wang", "Ruiqi Wu", "Chi Zhang", "Haisheng Tan"], "title": "Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents", "categories": ["cs.CL"], "comment": "12 pages, 8 figures", "summary": "Large Language Models (LLMs) are increasingly being deployed as intelligent agents. Their multi-stage workflows, which alternate between local computation and calls to external network services like Web APIs, introduce a mismatch in their execution pattern and the scheduling granularity of existing inference systems such as vLLM. Existing systems typically focus on per-segment optimization which prevents them from minimizing the end-to-end latency of the complete agentic workflow, i.e., the global Job Completion Time (JCT) over the entire request lifecycle. To address this limitation, we propose Astraea, a service engine designed to shift the optimization from local segments to the global request lifecycle. Astraea employs a state-aware, hierarchical scheduling algorithm that integrates a request's historical state with future predictions. It dynamically classifies requests by their I/O and compute intensive nature and uses an enhanced HRRN policy to balance efficiency and fairness. Astraea also implements an adaptive KV cache manager that intelligently handles the agent state during I/O waits based on the system memory pressure. Extensive experiments show that Astraea reduces average JCT by up to 25.5\\% compared to baseline methods. Moreover, our approach demonstrates strong robustness and stability under high load across various model scales.", "AI": {"tldr": "The paper introduces Astraea, a service engine for Large Language Models (LLMs) that minimizes end-to-end request lifecycle latency by focusing on global optimization instead of local segment optimization.", "motivation": "Existing inference systems focus only on per-segment optimization, but fail to minimize the overall latency across complete multi-stage workflows in intelligent LLM agents.", "method": "Astraea uses state-aware hierarchical scheduling combining historical states and future predictions, an HRRN-based policy for efficiency and fairness, and an adaptive KV cache manager to handle memory usage during I/O waits.", "result": "Astraea achieves up to a 25.5% reduction in average Job Completion Time (JCT) compared to baseline methods, and performs reliably and robustly under high computational loads across various model scales.", "conclusion": "Astraea effectively optimizes the global request lifecycle in LLM-based agents, enhancing latency performance, stability, and adaptability."}}
{"id": "2512.14111", "pdf": "https://arxiv.org/pdf/2512.14111", "abs": "https://arxiv.org/abs/2512.14111", "authors": ["Chenzui Li", "Yiming Chen", "Xi Wu", "Tao Teng", "Sylvain Calinon", "Darwin Caldwell", "Fei Chen"], "title": "Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field", "categories": ["cs.RO"], "comment": "10 pages, 9 figures", "summary": "Industrial human-robot collaboration requires motion planning that is collision-free, responsive, and ergonomically safe to reduce fatigue and musculoskeletal risk. We propose the Configuration Space Ergonomic Field (CSEF), a continuous and differentiable field over the human joint space that quantifies ergonomic quality and provides gradients for real-time ergonomics-aware planning. An efficient algorithm constructs CSEF from established metrics with joint-wise weighting and task conditioning, and we integrate it into a gradient-based planner compatible with impedance-controlled robots. In a 2-DoF benchmark, CSEF-based planning achieves higher success rates, lower ergonomic cost, and faster computation than a task-space ergonomic planner. Hardware experiments with a dual-arm robot in unimanual guidance, collaborative drilling, and bimanual cocarrying show faster ergonomic cost reduction, closer tracking to optimized joint targets, and lower muscle activation than a point-to-point baseline. CSEF-based planning method reduces average ergonomic scores by up to 10.31% for collaborative drilling tasks and 5.60% for bimanual co-carrying tasks while decreasing activation in key muscle groups, indicating practical benefits for real-world deployment.", "AI": {"tldr": "The paper introduces Configuration Space Ergonomic Field (CSEF) for motion planning in human-robot collaboration, offering ergonomic benefits and better efficiency.", "motivation": "To enhance industrial human-robot collaboration by addressing ergonomic safety and reducing fatigue and musculoskeletal risk through optimized motion planning.", "method": "The paper proposes CSEF, a continuous field over human joint space, calculated using established metrics for ergonomic quality, integrated into a gradient-based planner compatible with impedance-controlled robots.", "result": "In benchmarks, CSEF-based planning showed higher success rates, lower ergonomic cost, and faster computation. Hardware tests showed up to 10.31% and 5.60% ergonomic score reductions in collaborative tasks, with reduced muscle activation.", "conclusion": "CSEF provides a practical, real-time ergonomics-aware motion planning solution, improving safety, efficiency, and ergonomic outcomes in human-robot collaboration tasks."}}
{"id": "2512.13726", "pdf": "https://arxiv.org/pdf/2512.13726", "abs": "https://arxiv.org/abs/2512.13726", "authors": ["Sayak Chakrabarty", "Souradip Pal"], "title": "Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 5 figures", "summary": "Unlike traditional recommendation tasks, finite user time budgets introduce a critical resource constraint, requiring the recommender system to balance item relevance and evaluation cost. For example, in a mobile shopping interface, users interact with recommendations by scrolling, where each scroll triggers a list of items called slate. Users incur an evaluation cost - time spent assessing item features before deciding to click. Highly relevant items having higher evaluation costs may not fit within the user's time budget, affecting engagement. In this position paper, our objective is to evaluate reinforcement learning algorithms that learn patterns in user preferences and time budgets simultaneously, crafting recommendations with higher engagement potential under resource constraints. Our experiments explore the use of reinforcement learning to recommend items for users using Alibaba's Personalized Re-ranking dataset supporting slate optimization in e-commerce contexts. Our contributions include (i) a unified formulation of time-constrained slate recommendation modeled as Markov Decision Processes (MDPs) with budget-aware utilities; (ii) a simulation framework to study policy behavior on re-ranking data; and (iii) empirical evidence that on-policy and off-policy control can improve performance under tight time budgets than traditional contextual bandit-based methods.", "AI": {"tldr": "The paper addresses the challenge of balancing item relevance and evaluation costs under user time constraints in recommender systems. It explores reinforcement learning for optimizing recommendations in e-commerce scenarios.", "motivation": "The paper highlights the need to adapt recommendation systems when users face limited time budgets, requiring a trade-off between relevance of recommendations and the time users spend evaluating them.", "method": "The authors propose a time-constrained slate recommendation framework modeled as Markov Decision Processes (MDPs) and use reinforcement learning techniques to optimize recommendations based on user time budgets. A simulation framework is created for behavior analysis of policies.", "result": "Empirical results demonstrate that on-policy and off-policy control methods outperform traditional contextual bandit approaches in scenarios with strict time budgets.", "conclusion": "Reinforcement learning approaches can enhance engagement in recommendations by accounting for user preferences and time constraints, offering better outcomes in e-commerce contexts than traditional methods."}}
{"id": "2512.13840", "pdf": "https://arxiv.org/pdf/2512.13840", "abs": "https://arxiv.org/abs/2512.13840", "authors": ["Yannan He", "Garvita Tiwari", "Xiaohan Zhang", "Pankaj Bora", "Tolga Birdal", "Jan Eric Lenssen", "Gerard Pons-Moll"], "title": "MoLingo: Motion-Language Alignment for Text-to-Motion Generation", "categories": ["cs.CV"], "comment": "Project page: https://hynann.github.io/molingo/MoLingo.html", "summary": "We introduce MoLingo, a text-to-motion (T2M) model that generates realistic, lifelike human motion by denoising in a continuous latent space. Recent works perform latent space diffusion, either on the whole latent at once or auto-regressively over multiple latents. In this paper, we study how to make diffusion on continuous motion latents work best. We focus on two questions: (1) how to build a semantically aligned latent space so diffusion becomes more effective, and (2) how to best inject text conditioning so the motion follows the description closely. We propose a semantic-aligned motion encoder trained with frame-level text labels so that latents with similar text meaning stay close, which makes the latent space more diffusion-friendly. We also compare single-token conditioning with a multi-token cross-attention scheme and find that cross-attention gives better motion realism and text-motion alignment. With semantically aligned latents, auto-regressive generation, and cross-attention text conditioning, our model sets a new state of the art in human motion generation on standard metrics and in a user study. We will release our code and models for further research and downstream usage.", "AI": {"tldr": "MoLingo is a text-to-motion model that uses a latent space diffusion approach to generate lifelike human motion, improving on prior methods by creating a semantically aligned latent space and utilizing cross-attention text conditioning.", "motivation": "The paper aims to improve text-to-motion generation by addressing limitations in diffusion techniques and optimizing latent space alignment and text conditioning for realistic motion synthesis.", "method": "MoLingo employs a semantic-aligned motion encoder trained with text labels to create a diffusion-friendly latent space. It uses a cross-attention mechanism for text conditioning and an auto-regressive generation approach to achieve high realism and alignment.", "result": "MoLingo demonstrates improved realism and text-motion alignment, setting new benchmarks for human motion generation metrics. A user study confirms its effectiveness.", "conclusion": "The proposed techniques enhance motion synthesis quality and align it closely with textual descriptions, advancing the field. Code and models will be released for further research and applications."}}
{"id": "2512.13955", "pdf": "https://arxiv.org/pdf/2512.13955", "abs": "https://arxiv.org/abs/2512.13955", "authors": ["Sindhuja Madabushi", "Dawood Wasif", "Jin-Hee Cho"], "title": "MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning", "categories": ["cs.AI"], "comment": null, "summary": "Federated Learning (FL) has emerged as a leading privacy-preserving machine learning paradigm, enabling participants to share model updates instead of raw data. However, FL continues to face key challenges, including weak client incentives, privacy risks, and resource constraints. Assessing client reliability is essential for fair incentive allocation and ensuring that each client's data contributes meaningfully to the global model. To this end, we propose MURIM, a MUlti-dimensional Reputation-based Incentive Mechanism that jointly considers client reliability, privacy, resource capacity, and fairness while preventing malicious or unreliable clients from earning undeserved rewards. MURIM allocates incentives based on client contribution, latency, and reputation, supported by a reliability verification module. Extensive experiments on MNIST, FMNIST, and ADULT Income datasets demonstrate that MURIM achieves up to 18% improvement in fairness metrics, reduces privacy attack success rates by 5-9%, and improves robustness against poisoning and noisy-gradient attacks by up to 85% compared to state-of-the-art baselines. Overall, MURIM effectively mitigates adversarial threats, promotes fair and truthful participation, and preserves stable model convergence across heterogeneous and dynamic federated settings.", "AI": {"tldr": "The paper proposes MURIM, a reputation-based incentive mechanism for Federated Learning, addressing issues like client reliability, fairness, and privacy.", "motivation": "To address the challenges in Federated Learning, such as client incentives, privacy risks, and ensuring meaningful contributions from participants.", "method": "MURIM incorporates a multi-dimensional assessment considering client reliability, privacy, resource constraints, and fairness, supported by a reliability verification module.", "result": "Experiments demonstrate improvements including up to 18% better fairness, 5-9% lower privacy attack rates, and up to 85% increased robustness against adversarial attacks compared to baselines.", "conclusion": "MURIM effectively mitigates adversarial risks, fosters fair participation, and ensures stable global model training even in diverse and dynamic environments."}}
{"id": "2512.14673", "pdf": "https://arxiv.org/pdf/2512.14673", "abs": "https://arxiv.org/abs/2512.14673", "authors": ["Ronnie de Souza Santos", "Cleyton Magalh\u00e3es", "Italo Santos"], "title": "Reconsidering Conversational Norms in LLM Chatbots for Sustainable AI", "categories": ["cs.SE"], "comment": null, "summary": "LLM based chatbots have become central interfaces in technical, educational, and analytical domains, supporting tasks such as code reasoning, problem solving, and information exploration. As these systems scale, sustainability concerns have intensified, with most assessments focusing on model architecture, hardware efficiency, and deployment infrastructure. However, existing mitigation efforts largely overlook how user interaction practices themselves shape the energy profile of LLM based systems. In this vision paper, we argue that interaction level behavior appears to be an underexamined factor shaping the environmental impact of LLM based systems, and we present this issue across four dimensions. First, extended conversational patterns increase token production and raise the computational cost of inference. Second, expectations of instant responses limit opportunities for energy aware scheduling and workload consolidation. Third, everyday user habits contribute to cumulative operational demand in ways that are rarely quantified. Fourth, the accumulation of context affects memory requirements and reduces the efficiency of long running dialogues. Addressing these challenges requires rethinking how chatbot interactions are designed and conceptualized, and adopting perspectives that recognize sustainability as partly dependent on the conversational norms through which users engage with LLM based systems.", "AI": {"tldr": "This paper identifies how user interaction patterns impact the sustainability of LLM-based chatbots and proposes addressing the environmental challenges through redesigned interaction practices.", "motivation": "Rapid growth of LLM-based chatbots in various domains, paired with sustainability concerns, has primarily focused on backend efficiencies rather than user behaviors shaping energy consumption.", "method": "The paper highlights four interaction-level factors influencing environmental impact: extended conversations, instant response expectations, daily user habits, and accumulated context during usage.", "result": "The analysis reveals that user interaction practices are significant contributors to computational and energy inefficiencies in LLM-based systems, influencing their environmental footprint.", "conclusion": "Optimizing sustainability in LLM systems requires rethinking user interaction design and conversational norms to reduce energy demands and improve operational efficiency."}}
{"id": "2512.14179", "pdf": "https://arxiv.org/pdf/2512.14179", "abs": "https://arxiv.org/abs/2512.14179", "authors": ["K. M. Jubair Sami", "Dipto Sumit", "Ariyan Hossain", "Farig Sadeque"], "title": "A Comparative Analysis of Retrieval-Augmented Generation Techniques for Bengali Standard-to-Dialect Machine Translation Using LLMs", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to the Second Workshop on Bangla Language Processing (BLP) at IJCNLP-AACL 2025. 14 pages, 9 figures, 6 tables", "summary": "Translating from a standard language to its regional dialects is a significant NLP challenge due to scarce data and linguistic variation, a problem prominent in the Bengali language. This paper proposes and compares two novel RAG pipelines for standard-to-dialectal Bengali translation. The first, a Transcript-Based Pipeline, uses large dialect sentence contexts from audio transcripts. The second, a more effective Standardized Sentence-Pairs Pipeline, utilizes structured local\\_dialect:standard\\_bengali sentence pairs. We evaluated both pipelines across six Bengali dialects and multiple LLMs using BLEU, ChrF, WER, and BERTScore. Our findings show that the sentence-pair pipeline consistently outperforms the transcript-based one, reducing Word Error Rate (WER) from 76\\% to 55\\% for the Chittagong dialect. Critically, this RAG approach enables smaller models (e.g., Llama-3.1-8B) to outperform much larger models (e.g., GPT-OSS-120B), demonstrating that a well-designed retrieval strategy can be more crucial than model size. This work contributes an effective, fine-tuning-free solution for low-resource dialect translation, offering a practical blueprint for preserving linguistic diversity.", "AI": {"tldr": "The paper proposes two novel pipelines for translating standard Bengali to dialectal Bengali and evaluates their performance.", "motivation": "To address the challenge of translating standard Bengali to its regional dialects, particularly with scarce data and linguistic variation hindering machine translation.", "method": "The paper compares a Transcript-Based Pipeline and a Standardized Sentence-Pairs Pipeline for translation, utilizing retrieval-augmented generation (RAG) and evaluating across six dialects and multiple language models.", "result": "The Standardized Sentence-Pairs Pipeline outperformed the transcript-based one, reducing Word Error Rate significantly (e.g., from 76% to 55% for the Chittagong dialect). Smaller models with the proposed RAG approach even outperformed larger models.", "conclusion": "The study demonstrates the importance of a well-designed retrieval strategy over model size, presenting a fine-tuning-free, low-resource solution for dialect translation and supporting linguistic diversity."}}
{"id": "2512.14189", "pdf": "https://arxiv.org/pdf/2512.14189", "abs": "https://arxiv.org/abs/2512.14189", "authors": ["Johannes A. Gaus", "Daniel H\u00e4ufle", "Woo-Jeong Baek"], "title": "SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry", "categories": ["cs.RO"], "comment": null, "summary": "While many visual odometry (VO), visual-inertial odometry (VIO), and SLAM systems achieve high accuracy, the majority of existing methods miss to assess risks at runtime. This paper presents SUPER (Sensitivity-based Uncertainty-aware PErformance and Risk assessment) that is a generic and explainable framework that propagates uncertainties via sensitivities for real-time risk assessment in VIO. The scientific novelty lies in the derivation of a real-time risk indicator that is backend-agnostic and exploits the Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties. Practically, the Schur complement captures the sensitivity that reflects the influence of the uncertainty on the risk occurrence. Our framework estimates risks on the basis of the residual magnitudes, geometric conditioning, and short horizon temporal trends without requiring ground truth knowledge. Our framework enables to reliably predict trajectory degradation 50 frames ahead with an improvement of 20% to the baseline. In addition, SUPER initiates a stop or relocalization policy with 89.1% recall. The framework is backend agnostic and operates in real time with less than 0.2% additional CPU cost. Experiments show that SUPER provides consistent uncertainty estimates. A SLAM evaluation highlights the applicability to long horizon mapping.", "AI": {"tldr": "The paper introduces SUPER, a framework for real-time risk assessment in VIO systems using uncertainty and sensitivity propagation.", "motivation": "Most existing VO, VIO, and SLAM systems lack runtime risk assessment, which is critical for reliable and accurate applications.", "method": "The method uses a backend-agnostic approach that leverages Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties and assess risks based on residual magnitudes, geometric conditioning, and temporal trends.", "result": "SUPER achieves reliable trajectory degradation prediction 50 frames ahead with a 20% improvement and initiates polices like stop or relocalization with 89.1% recall, at minimal CPU cost.", "conclusion": "The framework effectively predicts risks and trajectory degradation, operates in real-time, is backend agnostic, and demonstrates broad applicability in SLAM evaluations."}}
{"id": "2512.13727", "pdf": "https://arxiv.org/pdf/2512.13727", "abs": "https://arxiv.org/abs/2512.13727", "authors": ["Yuhan Tang", "Kangxin Cui", "Jung Ho Park", "Yibo Zhao", "Xuan Jiang", "Haoze He", "Dingyi Zhuang", "Shenhao Wang", "Jiangbo Yu", "Haris Koutsopoulos", "Jinhua Zhao"], "title": "RAST-MoE-RL: A Regime-Aware Spatio-Temporal MoE Framework for Deep Reinforcement Learning in Ride-Hailing", "categories": ["cs.LG"], "comment": null, "summary": "Ride-hailing platforms face the challenge of balancing passenger waiting times with overall system efficiency under highly uncertain supply-demand conditions. Adaptive delayed matching creates a trade-off between matching and pickup delays by deciding whether to assign drivers immediately or batch requests. Since outcomes accumulate over long horizons with stochastic dynamics, reinforcement learning (RL) is a suitable framework. However, existing approaches often oversimplify traffic dynamics or use shallow encoders that miss complex spatiotemporal patterns.\n  We introduce the Regime-Aware Spatio-Temporal Mixture-of-Experts (RAST-MoE), which formalizes adaptive delayed matching as a regime-aware MDP equipped with a self-attention MoE encoder. Unlike monolithic networks, our experts specialize automatically, improving representation capacity while maintaining computational efficiency. A physics-informed congestion surrogate preserves realistic density-speed feedback, enabling millions of efficient rollouts, while an adaptive reward scheme guards against pathological strategies.\n  With only 12M parameters, our framework outperforms strong baselines. On real-world Uber trajectory data (San Francisco), it improves total reward by over 13%, reducing average matching and pickup delays by 10% and 15% respectively. It demonstrates robustness across unseen demand regimes and stable training. These findings highlight the potential of MoE-enhanced RL for large-scale decision-making with complex spatiotemporal dynamics.", "AI": {"tldr": "The paper proposes RAST-MoE, a novel RL framework for adaptive delayed matching in ride-hailing, achieving significant improvements in efficiency and delay reductions.", "motivation": "To address the challenge of optimizing passenger waiting times and system efficiency in ride-hailing under dynamic, uncertain supply-demand conditions.", "method": "RAST-MoE employs a regime-aware MDP framework integrated with a self-attention Mixture-of-Experts (MoE) encoder, supported by physics-informed congestion surrogates and adaptive reward schemes.", "result": "RAST-MoE outperformed baselines with a 13% improvement in total rewards and reductions of 10% in matching and 15% in pickup delays on Uber data.", "conclusion": "RAST-MoE effectively enhances RL-based decision-making in highly dynamic systems, demonstrating robustness, computational efficiency, and scalability for real-world applications."}}
{"id": "2512.13855", "pdf": "https://arxiv.org/pdf/2512.13855", "abs": "https://arxiv.org/abs/2512.13855", "authors": ["Ujjwal Mishra", "Vinita Shukla", "Praful Hambarde", "Amit Shukla"], "title": "Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning of Vision Language Models in Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the IEEE/CVF winter conference on applications of computer vision (WACV 2026)", "summary": "Adapting Vision Language Segmentation Models (VLSMs) to medical imaging domains requires significant computational overhead when using conventional fine-tuning approaches. Existing Parameter-Efficient Fine-Tuning (PEFT) methods apply uniform adapter dimensions across all transformer layers, leading to suboptimal parameter allocation and reduced adaptation efficiency. We introduce Telescopic Adapters, a novel PEFT framework that employs depth-aware scaling to progressively increase adapter capacity from shallow to deep transformer layers. Our method integrates lightweight bottleneck modules within CLIPSeg's vision and text encoders, with adapter dimensions dynamically scaled based on layer depth and semantic relevance. Using only 613k trainable parameters--244x fewer than end-to-end fine-tuning, Telescopic Adapters achieve superior performance across five diverse medical datasets spanning polyp segmentation, skin lesion detection, and breast ultrasound imaging. Comprehensive ablation studies demonstrate that deeper layers require substantially more adaptation capacity than shallow layers, validating our telescopic scaling hypothesis. Our approach establishes a new paradigm for efficient medical VLSM fine-tuning, enabling deployment in resource-constrained clinical environments while maintaining competitive segmentation accuracy.", "AI": {"tldr": "This paper proposes Telescopic Adapters, a novel Parameter-Efficient Fine-Tuning (PEFT) framework to adapt Vision Language Segmentation Models (VLSMs) to medical imaging with minimal computational resources.", "motivation": "Current fine-tuning methods for VLSMs in medical imaging are computationally intensive and inefficient in parameter allocation, motivating the need for a more adaptive and efficient approach.", "method": "The proposed method uses depth-aware scaling, attaching lightweight bottleneck modules to the transformer layers of CLIPSeg\u2019s vision and text encoders. Adapter dimensions increase functionally from shallow to deeper layers proportional to layer depth and semantic importance.", "result": "Telescopic Adapters achieve superior performance on five medical imaging datasets\u2014such as polyp segmentation and breast ultrasound imaging\u2014using only 613k trainable parameters, significantly reducing computational overhead.", "conclusion": "The approach enables efficient fine-tuning for medical imaging tasks in resource-limited clinical settings while maintaining strong segmentation accuracy. Depth-aware scaling is validated as an effective strategy for improving adaptation efficiency."}}
{"id": "2512.13978", "pdf": "https://arxiv.org/pdf/2512.13978", "abs": "https://arxiv.org/abs/2512.13978", "authors": ["Yang Cao", "Yubin Chen", "Xuyang Guo", "Zhao Song", "Song Yue", "Jiahao Zhang", "Jiale Zhao"], "title": "Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms", "categories": ["cs.AI"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${\u00f3}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].\n  We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.", "AI": {"tldr": "The paper benchmarks four advanced LLMs on generating LaTeX proofs for Randomized Algorithms, finding high accuracy for top-tier models, but notable variance in performance across models.", "motivation": "To evaluate the reasoning capabilities of LLMs in graduate-level mathematical theory and assess how they perform on canonical problems.", "method": "Four models were tasked with generating LaTeX proofs for lemmas and exercises from a textbook on Randomized Algorithms, analyzing accuracy, conciseness, hallucination rates, and logical structure.", "result": "Top-tier models achieved approximately 66% accuracy, showing proficiency; however, others showed inconsistencies with about 40% accuracy.", "conclusion": "Frontier LLMs show potential for pedagogical assistance in mathematics but still vary in reliability for formal proofs; further refinement is needed for rigorous application."}}
{"id": "2512.14070", "pdf": "https://arxiv.org/pdf/2512.14070", "abs": "https://arxiv.org/abs/2512.14070", "authors": ["Dongchao Zhou", "Lingyun Ying", "Huajun Chai", "Dongbin Wang"], "title": "From Obfuscated to Obvious: A Comprehensive JavaScript Deobfuscation Tool for Security Analysis", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "JavaScript's widespread adoption has made it an attractive target for malicious attackers who employ sophisticated obfuscation techniques to conceal harmful code. Current deobfuscation tools suffer from critical limitations that severely restrict their practical effectiveness. Existing tools struggle with diverse input formats, address only specific obfuscation types, and produce cryptic output that impedes human analysis.\n  To address these challenges, we present JSIMPLIFIER, a comprehensive deobfuscation tool using a multi-stage pipeline with preprocessing, abstract syntax tree-based static analysis, dynamic execution tracing, and Large Language Model (LLM)-enhanced identifier renaming. We also introduce multi-dimensional evaluation metrics that integrate control/data flow analysis, code simplification assessment, entropy measures and LLM-based readability assessments.\n  We construct and release the largest real-world obfuscated JavaScript dataset with 44,421 samples (23,212 wild malicious + 21,209 benign samples). Evaluation shows JSIMPLIFIER outperforms existing tools with 100% processing capability across 20 obfuscation techniques, 100% correctness on evaluation subsets, 88.2% code complexity reduction, and over 4-fold readability improvement validated by multiple LLMs. Our results advance benchmarks for JavaScript deobfuscation research and practical security applications.", "AI": {"tldr": "JSIMPLIFIER is a groundbreaking tool that effectively simplifies and deobfuscates JavaScript code using a multi-stage approach, outperforming existing solutions.", "motivation": "Existing JavaScript deobfuscation tools have critical limitations, including handling diverse formats, addressing specific obfuscation types, and generating cryptic outputs, which hinder human analysis.", "method": "JSIMPLIFIER employs a multi-stage approach combining preprocessing, AST-based static analysis, dynamic execution tracing, and LLM-enhanced identifier renaming. The tool also includes innovative evaluation metrics such as control/data flow analysis, entropy measures, and LLM-based readability assessments.", "result": "JSIMPLIFIER demonstrates stellar performance, with 100% processing capability across 20 obfuscation techniques, 100% correctness on evaluation subsets, 88.2% complexity reduction, and over 4-fold readability improvement using an extensive dataset of 44,421 samples.", "conclusion": "JSIMPLIFIER significantly enhances JavaScript deobfuscation capabilities and sets new benchmarks in security and code analysis."}}
{"id": "2512.14237", "pdf": "https://arxiv.org/pdf/2512.14237", "abs": "https://arxiv.org/abs/2512.14237", "authors": ["Estelle Zheng", "Nathan Cerisara", "S\u00e9bastien Warichet", "Emmanuel Helbert", "Christophe Cerisara"], "title": "Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.", "AI": {"tldr": "The study revisits Ladder Side Tuning (LST), demonstrating it matches QLoRA's performance while reducing peak memory usage by 50%.", "motivation": "Current fine-tuning methods for large language models are constrained by memory limitations on consumer GPUs.", "method": "Ladder Side Tuning (LST) involves adding a lightweight side network, allowing for competitive performance and efficient memory usage.", "result": "LST enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with no gradient checkpointing, significantly reducing memory requirements compared to QLoRA.", "conclusion": "LST is an efficient solution for memory-constrained environments, and its variant xLadder further enhances reasoning capabilities without extra memory overhead."}}
{"id": "2512.14206", "pdf": "https://arxiv.org/pdf/2512.14206", "abs": "https://arxiv.org/abs/2512.14206", "authors": ["Mayank Sewlia", "Christos K. Verginis", "Dimos V. Dimarogonas"], "title": "Trajectory Tracking for Multi-Manipulator Systems in Constrained Environments", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "We consider the problem of cooperative manipulation by a mobile multi-manipulator system operating in obstacle-cluttered and highly constrained environments under spatio-temporal task specifications. The task requires transporting a grasped object while respecting both continuous robot dynamics and discrete geometric constraints arising from obstacles and narrow passages. To address this hybrid structure, we propose a multi-rate planning and control framework that combines offline generation of an STL-satisfying object trajectory and collision-free base footprints with online constrained inverse kinematics and continuous-time feedback control. The resulting closed-loop system enables coordinated reconfiguration of multiple manipulators while tracking the desired object motion. The approach is evaluated in high-fidelity physics simulations using three Franka Emika Panda mobile manipulators rigidly grasping an object.", "AI": {"tldr": "This paper proposes a hybrid planning and control framework to enable cooperative manipulation of objects by mobile multi-manipulator systems in environments with obstacles under spatio-temporal constraints.", "motivation": "Develop a method to enable cooperative operation of multiple mobile manipulators in highly constrained environments such as obstacle-filled areas.", "method": "Combine offline generation of object trajectories satisfying spatio-temporal logic constraints with online constrained inverse kinematics and continuous feedback control to guide manipulators during operation.", "result": "The framework is validated through high-fidelity physics simulations using three robotic manipulators collaboratively moving an object in complex environments.", "conclusion": "The proposed system can effectively enable coordinated manipulation of objects by mobile robots, balancing dynamic and geometric constraints, and showing promise for real-world applications."}}
{"id": "2512.13853", "pdf": "https://arxiv.org/pdf/2512.13853", "abs": "https://arxiv.org/abs/2512.13853", "authors": ["Finley Devlin", "Jaron Sanders"], "title": "Dropout Neural Network Training Viewed from a Percolation Perspective", "categories": ["cs.LG", "cond-mat.stat-mech", "math.PR", "stat.ML"], "comment": "22 pages, 14 figures", "summary": "In this work, we investigate the existence and effect of percolation in training deep Neural Networks (NNs) with dropout. Dropout methods are regularisation techniques for training NNs, first introduced by G. Hinton et al. (2012). These methods temporarily remove connections in the NN, randomly at each stage of training, and update the remaining subnetwork with Stochastic Gradient Descent (SGD). The process of removing connections from a network at random is similar to percolation, a paradigm model of statistical physics.\n  If dropout were to remove enough connections such that there is no path between the input and output of the NN, then the NN could not make predictions informed by the data. We study new percolation models that mimic dropout in NNs and characterise the relationship between network topology and this path problem. The theory shows the existence of a percolative effect in dropout. We also show that this percolative effect can cause a breakdown when training NNs without biases with dropout; and we argue heuristically that this breakdown extends to NNs with biases.", "AI": {"tldr": "The paper explores the connection between percolation theory and dropout regularization methods in training deep neural networks and identifies the effects of percolation breakdown.", "motivation": "Investigating how dropout methods impact neural network topology and performance by relating them to percolation theory in statistical physics.", "method": "Introducing new percolation models that mimic dropout in NNs and analyzing the relationship between network topology and the path problem under dropout conditions.", "result": "The study reveals a percolative effect in dropout, which can cause training breakdown in NNs without biases, with heuristic extension to NNs with biases.", "conclusion": "Dropout induces a percolative phenomenon; understanding this can lead to insights into regularization effects and potential disruptions in NN training due to dropout."}}
{"id": "2512.13728", "pdf": "https://arxiv.org/pdf/2512.13728", "abs": "https://arxiv.org/abs/2512.13728", "authors": ["Bhavesh Kumar", "Roger Jin", "Jeffrey Quesnelle"], "title": "CurvaDion: Curvature-Adaptive Distributed Orthonormalization", "categories": ["cs.LG", "cs.AI"], "comment": "Nous Research", "summary": "As language models scale to trillions of parameters, distributed training across many GPUs becomes essential, yet gradient synchronization over high-bandwidth, low-latency networks remains a critical bottleneck. While recent methods like Dion reduce per-step communication through low-rank updates, they synchronize at every step regardless of the optimization landscape. We observe that synchronization requirements vary dramatically throughout training: workers naturally compute similar gradients in flat regions, making frequent synchronization redundant, while high-curvature regions require coordination to prevent divergence. We introduce CurvaDion, which uses Relative Maximum Momentum Change (RMMC) to detect high-curvature regions requiring synchronization. RMMC leverages momentum dynamics which are already computed during optimization as a computationally tractable proxy for directional curvature, adding only $\\mathcal{O}(d)$ operations per layer. We establish theoretical connections between RMMC and loss curvature and demonstrate that CurvaDion achieves 99\\% communication reduction while matching baseline convergence across models from 160M to 1.3B parameters.", "AI": {"tldr": "CurvaDion reduces communication overhead in distributed training by adaptively synchronizing based on loss landscape curvature, achieving significant communication reduction without compromising convergence.", "motivation": "To address the issue of communication bottlenecks in distributed training of large-scale language models, especially in scenarios where gradient synchronization is essential.", "method": "The paper introduces CurvaDion, a method that uses Relative Maximum Momentum Change (RMMC) to adaptively detect high-curvature regions in the loss landscape, requiring synchronization only when necessary based on momentum dynamics.", "result": "CurvaDion achieves a 99% reduction in communication while maintaining convergence performance comparable to baselines on models ranging from 160M to 1.3B parameters.", "conclusion": "CurvaDion provides an efficient and dynamic approach to distributed training, effectively balancing the need for synchronization and reducing communication overhead, ensuring scalability for large models."}}
{"id": "2512.13869", "pdf": "https://arxiv.org/pdf/2512.13869", "abs": "https://arxiv.org/abs/2512.13869", "authors": ["Wenda Li", "Meng Wu", "Sungmin Eum", "Heesung Kwon", "Qing Qu"], "title": "Coarse-to-Fine Hierarchical Alignment for UAV-based Human Detection using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Training object detectors demands extensive, task-specific annotations, yet this requirement becomes impractical in UAV-based human detection due to constantly shifting target distributions and the scarcity of labeled images. As a remedy, synthetic simulators are adopted to generate annotated data, with a low annotation cost. However, the domain gap between synthetic and real images hinders the model from being effectively applied to the target domain. Accordingly, we introduce Coarse-to-Fine Hierarchical Alignment (CFHA), a three-stage diffusion-based framework designed to transform synthetic data for UAV-based human detection, narrowing the domain gap while preserving the original synthetic labels. CFHA explicitly decouples global style and local content domain discrepancies and bridges those gaps using three modules: (1) Global Style Transfer -- a diffusion model aligns color, illumination, and texture statistics of synthetic images to the realistic style, using only a small real reference set; (2) Local Refinement -- a super-resolution diffusion model is used to facilitate fine-grained and photorealistic details for the small objects, such as human instances, preserving shape and boundary integrity; (3) Hallucination Removal -- a module that filters out human instances whose visual attributes do not align with real-world data to make the human appearance closer to the target distribution. Extensive experiments on public UAV Sim2Real detection benchmarks demonstrate that our methods significantly improve the detection accuracy compared to the non-transformed baselines. Specifically, our method achieves up to $+14.1$ improvement of mAP50 on Semantic-Drone benchmark. Ablation studies confirm the complementary roles of the global and local stages and highlight the importance of hierarchical alignment. The code is released at \\href{https://github.com/liwd190019/CFHA}{this url}.", "AI": {"tldr": "Synthetic data annotation for UAV-based human detection is enhanced using a three-stage diffusion framework, CFHA, significantly reducing the domain gap with real-world data.", "motivation": "UAV-based human detection faces challenges with limited annotated data and domain gaps between synthetic and real images, hindering practical model application.", "method": "CFHA uses a three-stage approach: Global Style Transfer adjusts overall styles, Local Refinement enhances details, and Hallucination Removal prevents misaligned outputs.", "result": "CFHA outperforms baselines, achieving up to +14.1 mAP50 improvement on the Semantic-Drone benchmark.", "conclusion": "CFHA effectively addresses domain issues in UAV-based detection by aligning synthetic data to real-image distribution, validated through significant performance gains."}}
{"id": "2512.13979", "pdf": "https://arxiv.org/pdf/2512.13979", "abs": "https://arxiv.org/abs/2512.13979", "authors": ["Ge Yan", "Chung-En Sun", "Tsui-Wei", "Weng"], "title": "ReflCtrl: Controlling LLM Reflection via Representation Engineering", "categories": ["cs.AI"], "comment": "Spotlight in NeurIPS 25 MI workshop", "summary": "Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's reasoning into steps, identify the steps corresponding to reflection, and extract a reflection direction in the latent space that governs this behavior. Using this direction, we propose a stepwise steering method that can control reflection frequency. We call our framework ReflCtrl. Our experiments show that (1) in many cases reflections are redundant, especially in stronger models (in our experiments, we can save up to 33.6 percent of reasoning tokens while preserving performance), and (2) the model's reflection behavior is highly correlated with an internal uncertainty signal, implying self-reflection may be controlled by the model's uncertainty.", "AI": {"tldr": "The paper introduces a method (ReflCtrl) to optimize self-reflection in large language models, reducing inference costs by controlling reflection frequency without performance loss.", "motivation": "Large language models' self-reflective reasoning enhances performance but increases inference costs. The study aims to reduce these costs while maintaining effectiveness.", "method": "The authors analyze reasoning steps in LLMs and identify latent space directions tied to reflection. They then propose a stepwise steering method to control the frequency of reflections.", "result": "The proposed ReflCtrl reduces redundant reflections, saving up to 33.6% of reasoning tokens while preserving overall model performance. It also reveals that reflection correlates with the model's internal uncertainty signals.", "conclusion": "ReflCtrl provides a way to optimize computational efficiency in LLMs by controlling their reflection behavior, maintaining performance but lowering cost."}}
{"id": "2512.14429", "pdf": "https://arxiv.org/pdf/2512.14429", "abs": "https://arxiv.org/abs/2512.14429", "authors": ["Yukun Ren", "Siwei Yu", "Kai Chen", "Jianwei Ma"], "title": "Seismology modeling agent: A smart assistant for geophysical researchers", "categories": ["cs.AI", "cs.SE"], "comment": "26 pages, 15 figures. Code available at https://github.com/RenYukun1563/specfem-mcp", "summary": "To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.", "AI": {"tldr": "This paper develops an intelligent, conversational workflow for SPECFEM using large language models, simplifying seismic wave simulation tasks and enhancing productivity.", "motivation": "Simplifying the steep learning curve and reliance on manual operations in traditional seismic simulation workflows.", "method": "Introduces the Model Context Protocol (MCP) server suite to automate and interactively manage simulation tasks using LLM-powered intent-driven execution.", "result": "Validated through case studies showing seamless autonomous and interactive operation with high-fidelity results comparable to standard baselines.", "conclusion": "The framework lowers entry barriers, improves reproducibility, and demonstrates a step towards AI-assisted computational geophysics."}}
{"id": "2512.14239", "pdf": "https://arxiv.org/pdf/2512.14239", "abs": "https://arxiv.org/abs/2512.14239", "authors": ["Juan-Jos\u00e9 Guzm\u00e1n-Landa", "Juan-Manuel Torres-Moreno", "Miguel Figueroa-Saavedra", "Ligia Quintana-Torres", "Graham Ranger Martha-Lorena Avenda\u00f1o-Garrido"], "title": "Two CFG Nahuatl for automatic corpora expansion", "categories": ["cs.CL"], "comment": "15 pages, 5 figures, 8 tables", "summary": "The aim of this article is to introduce two Context-Free Grammars (CFG) for Nawatl Corpora expansion. Nawatl is an Amerindian language (it is a National Language of Mexico) of the $\u03c0$-language type, i.e. a language with few digital resources. For this reason the corpora available for the learning of Large Language Models (LLMs) are virtually non-existent, posing a significant challenge. The goal is to produce a substantial number of syntactically valid artificial Nawatl sentences and thereby to expand the corpora for the purpose of learning non contextual embeddings. For this objective, we introduce two new Nawatl CFGs and use them in generative mode. Using these grammars, it is possible to expand Nawatl corpus significantly and subsequently to use it to learn embeddings and to evaluate their relevance in a sentences semantic similarity task. The results show an improvement compared to the results obtained using only the original corpus without artificial expansion, and also demonstrate that economic embeddings often perform better than some LLMs.", "AI": {"tldr": "This paper proposes two Context-Free Grammars (CFG) for expanding Nawatl language corpora, addressing the resource scarcity of this Amerindian language.", "motivation": "Nawatl's status as a low-resource Amerindian language makes it challenging to train Large Language Models (LLMs) due to a lack of substantial corpora.", "method": "Two new Context-Free Grammars (CFGs) were used to generate syntactically valid artificial Nawatl sentences to enlarge the existing corpus and train non-contextual embeddings.", "result": "Generated artificial corpora enhanced embedding training, showing improvements in semantic similarity tasks and outperforming some LLMs.", "conclusion": "Artificial expansion of Nawatl corpora using CFGs is an effective approach to enhance low-resource language modeling for tasks like semantic similarity."}}
{"id": "2512.14270", "pdf": "https://arxiv.org/pdf/2512.14270", "abs": "https://arxiv.org/abs/2512.14270", "authors": ["Zixin Tang", "Yiming Chen", "Quentin Rouxel", "Dianxi Li", "Shuang Wu", "Fei Chen"], "title": "CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics", "categories": ["cs.RO"], "comment": null, "summary": "Teleoperation presents a promising paradigm for remote control and robot proprioceptive data collection. Despite recent progress, current teleoperation systems still suffer from limitations in efficiency and ergonomics, particularly in challenging scenarios. In this paper, we propose CaFe-TeleVision, a coarse-to-fine teleoperation system with immersive situated visualization for enhanced ergonomics. At its core, a coarse-to-fine control mechanism is proposed in the retargeting module to bridge workspace disparities, jointly optimizing efficiency and physical ergonomics. To stream immersive feedback with adequate visual cues for human vision systems, an on-demand situated visualization technique is integrated in the perception module, which reduces the cognitive load for multi-view processing. The system is built on a humanoid collaborative robot and validated with six challenging bimanual manipulation tasks. User study among 24 participants confirms that CaFe-TeleVision enhances ergonomics with statistical significance, indicating a lower task load and a higher user acceptance during teleoperation. Quantitative results also validate the superior performance of our system across six tasks, surpassing comparative methods by up to 28.89% in success rate and accelerating by 26.81% in completion time. Project webpage: https://clover-cuhk.github.io/cafe_television/", "AI": {"tldr": "The paper proposes CaFe-TeleVision, a teleoperation system to enhance ergonomic control and efficiency, validated with significant improvements in user task performance.", "motivation": "To address inefficiencies and ergonomic challenges in current teleoperation systems, particularly in demanding scenarios.", "method": "A coarse-to-fine control and retargeting mechanism is integrated for workspace optimization and improved ergonomics, along with an immersive situated visualization technique to reduce cognitive load in multi-view processing.", "result": "In trials involving six bimanual manipulation tasks, the system showed improved success rate (up to 28.89%) and task completion time (26.81%) compared to other methods. User studies confirm better ergonomics and user acceptance.", "conclusion": "This novel teleoperation approach improves task success, efficiency, and ergonomics, demonstrating its superiority over existing methods."}}
{"id": "2512.14230", "pdf": "https://arxiv.org/pdf/2512.14230", "abs": "https://arxiv.org/abs/2512.14230", "authors": ["Divyansh Pareek", "Sewoong Oh", "Simon S. Du"], "title": "Understanding the Gain from Data Filtering in Multimodal Contrastive Learning", "categories": ["cs.LG", "stat.ML"], "comment": "40 pages, 8 figures, 1 table. This work is accepted to the Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025", "summary": "The success of modern multimodal representation learning relies on internet-scale datasets. Due to the low quality of a large fraction of raw web data, data curation has become a critical step in the training pipeline. Filtering using a trained model (i.e., teacher-based filtering) has emerged as a successful solution, leveraging a pre-trained model to compute quality scores. To explain the empirical success of teacher-based filtering, we characterize the performance of filtered contrastive learning under the standard bimodal data generation model. Denoting $\u03b7\\in(0,1]$ as the fraction of data with correctly matched modalities among $n$ paired samples, we utilize a linear contrastive learning setup to show a provable benefit of data filtering: $(i)$ the error without filtering is upper and lower bounded by $\\frac{1}{\u03b7\\sqrt{n}}$, and $(ii)$ the error with teacher-based filtering is upper bounded by $\\frac{1}{\\sqrt{\u03b7n}}$ in the large $\u03b7$ regime, and by $\\frac{1}{\\sqrt{n}}$ in the small $\u03b7$ regime.", "AI": {"tldr": "This paper investigates teacher-based filtering in multimodal representation learning, providing provable error benefits using a bimodal data generation model.", "motivation": "To address the challenge of low-quality raw web data affecting training pipelines for multimodal representation learning, and to analyze why teacher-based filtering has been empirically successful.", "method": "The authors use a linear contrastive learning framework and a standard bimodal data generation model to theoretically analyze the impact of teacher-based filtering.", "result": "They find that data filtering improves error bounds, with substantial benefits based on the data match fraction $\u03b7$, with a more controlled upper bound on error.", "conclusion": "Teacher-based filtering provides a provable advantage in improving performance in multimodal representation learning by mitigating the effects of low-quality raw data."}}
{"id": "2512.13729", "pdf": "https://arxiv.org/pdf/2512.13729", "abs": "https://arxiv.org/abs/2512.13729", "authors": ["Jacob Schnell", "Aditya Makkar", "Gunadi Gani", "Aniket Srinivasan Ashok", "Darren Lo", "Mike Optis", "Alexander Wong", "Yuhao Chen"], "title": "Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\\times$ less than classical methods.", "AI": {"tldr": "The paper addresses the challenge of acquiring high-resolution wind data by proposing WindDM, a diffusion model with composite classifier-free guidance (CCFG), achieving high reconstruction quality cost-effectively.", "motivation": "The motivation is to overcome the limitations of traditional and deep learning-based methods in providing high-resolution, accurate wind data efficiently for weather modelling problems.", "method": "The method involves a novel composite classifier-free guidance (CCFG) to manage multiple input channels, integrated into a diffusion model called WindDM.", "result": "WindDM achieves state-of-the-art reconstruction quality on industrial-scale wind dynamics tasks, costing up to $1000\u00d7$ less than traditional methods.", "conclusion": "WindDM, employing CCFG, effectively balances accuracy and efficiency, making it a significant advancement for high-resolution wind data reconstruction."}}
{"id": "2512.13874", "pdf": "https://arxiv.org/pdf/2512.13874", "abs": "https://arxiv.org/abs/2512.13874", "authors": ["Jitesh Jain", "Jialuo Li", "Zixian Ma", "Jieyu Zhang", "Chris Dongjoo Kim", "Sangho Lee", "Rohun Tripathi", "Tanmay Gupta", "Christopher Clark", "Humphrey Shi"], "title": "SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning", "categories": ["cs.CV"], "comment": "Project Page: https://praeclarumjj3.github.io/sage/", "summary": "As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.", "AI": {"tldr": "The paper discusses SAGE, a system enabling flexible video reasoning across different durations, achieving notable performance improvements on long videos.", "motivation": "Current video reasoning models process long videos inefficiently in a single turn, requiring significant resources. The researchers aim to create a system that mimics human-like, flexible reasoning across various video durations.", "method": "The authors propose SAGE, a multi-turn reasoning system, along with a synthetic data generation pipeline (Gemini-2.5-Flash) and reinforcement learning (RL) post-training to instill flexible reasoning. The SAGE-Bench dataset was curated for evaluation.", "result": "Their system achieved improvements of up to 6.1% in open-ended video reasoning tasks, with an 8.2% improvement on videos longer than 10 minutes.", "conclusion": "SAGE demonstrates the potential for efficient any-horizon video reasoning systems, highlighting the success of multi-turn reasoning, synthetic training data, and RL post-training."}}
{"id": "2512.13996", "pdf": "https://arxiv.org/pdf/2512.13996", "abs": "https://arxiv.org/abs/2512.13996", "authors": ["Can Jin", "Hongwu Peng", "Mingcan Xiang", "Qixin Zhang", "Xiangchi Yuan", "Amit Hasan", "Ohiremen Dibua", "Yifan Gong", "Yan Kang", "Dimitris N. Metaxas"], "title": "Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training", "categories": ["cs.AI"], "comment": null, "summary": "Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.", "AI": {"tldr": "The paper proposes DTop-p, a new dynamic routing mechanism in Mixture-of-Experts models offering controllable sparsity and improved token-wise resource allocation.", "motivation": "Existing Top-k and Top-p routing methods in Mixture-of-Experts models have limitations such as uniform sparsity and sensitivity to hyperparameters, making token-wise resource allocation inefficient.", "method": "DTop-p employs a Proportional-Integral Controller to dynamically adjust thresholds for activated experts and introduces layer-wise routing normalization for adaptive expert selection.", "result": "Experiments on Large Language Models and Diffusion Transformers show DTop-p outperforms Top-k and fixed-threshold Top-p baselines, effectively controlling activated expert counts and adapting resource allocation.", "conclusion": "DTop-p is a scalable and robust framework for large-scale Mixture-of-Experts pre-training, addressing token and layer-specific routing needs with dynamic sparsity control mechanisms."}}
{"id": "2512.14514", "pdf": "https://arxiv.org/pdf/2512.14514", "abs": "https://arxiv.org/abs/2512.14514", "authors": ["Manuel Bentele", "Andreas Podelski", "Axel Sikora", "Bernd Westphal"], "title": "Relevant HAL Interface Requirements for Embedded Systems", "categories": ["cs.LO", "cs.SE"], "comment": null, "summary": "Embedded applications often use a Hardware Abstraction Layer (HAL) to access hardware. Improper use of the HAL can lead to incorrect hardware operations, resulting in system failure and potentially serious damage to the hardware. The question is how one can obtain prioritize, among a possibly large set of HAL interface requirements, those that are indisputably relevant for preventing this kind of system failure. In this paper, we introduce a formal notion of relevance. This allows us to leverage a formal method, i.e., software model checking, to produce a mathematical proof that a requirement is indisputably relevant. We propose an approach to extract provably relevant requirements from issue reports on system failures. We present a case study to demonstrate that the approach is feasible in principle. The case study uses three examples of issue reports on embedded applications that use the SPI bus via the spidev HAL. The overall contribution of this paper is to pave the way for the study of approaches to a new kind of prioritization aimed at preventing a specific kind of system failure.", "AI": {"tldr": "This paper focuses on extracting relevant requirements from issue reports to prevent system failures in embedded applications using HAL.", "motivation": "Improper usage of HAL can result in critical system failures and hardware damage, necessitating a method to identify key interface requirements to prevent such failures.", "method": "The paper introduces a formal notion of relevance for HAL requirements and employs software model checking to mathematically verify requirement relevance.", "result": "Experimental case study demonstrates the feasibility of the approach using issue reports related to the SPI bus via spidev HAL.", "conclusion": "The study introduces a novel approach for prioritizing requirements to prevent specific system failures and sets groundwork for further exploration in this domain."}}
{"id": "2512.14244", "pdf": "https://arxiv.org/pdf/2512.14244", "abs": "https://arxiv.org/abs/2512.14244", "authors": ["Yiqing Zhou", "Yu Lei", "Shuzheng Si", "Qingyan Sun", "Wei Wang", "Yifei Wu", "Hao Wen", "Gang Chen", "Fanchao Qi", "Maosong Sun"], "title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.", "AI": {"tldr": "The paper introduces a novel EDU-based Context Compressor for large language models to tackle the challenge of extensive context by preserving structural coherence and improving computational efficiency.", "motivation": "LLMs face challenges in handling extensive context efficiently and effectively, particularly in tasks with long documents or complex information retrieval, due to costs and noise introduced in long contexts.", "method": "The proposed method involves two steps: (1) transforming text into a structural relation tree using LingoEDU for preserving global structure, and (2) a ranking module selects relevant sub-trees for linearization based on queries.", "result": "Empirical evaluation demonstrates state-of-the-art accuracy in structural prediction and improved performance over frontier LLMs in long-context tasks, with reduced computational costs.", "conclusion": "The EDU-based Context Compressor effectively combines structural preservation with efficient compression to enhance LLM performance, particularly in tasks requiring long-context understanding."}}
{"id": "2512.14331", "pdf": "https://arxiv.org/pdf/2512.14331", "abs": "https://arxiv.org/abs/2512.14331", "authors": ["Rishabh Dev Yadav", "Avirup Das", "Hongyu Song", "Samuel Kaski", "Wei Pan"], "title": "ARCADE: Adaptive Robot Control with Online Changepoint-Aware Bayesian Dynamics Learning", "categories": ["cs.RO"], "comment": null, "summary": "Real-world robots must operate under evolving dynamics caused by changing operating conditions, external disturbances, and unmodeled effects. These may appear as gradual drifts, transient fluctuations, or abrupt shifts, demanding real-time adaptation that is robust to short-term variation yet responsive to lasting change. We propose a framework for modeling the nonlinear dynamics of robotic systems that can be updated in real time from streaming data. The method decouples representation learning from online adaptation, using latent representations learned offline to support online closed-form Bayesian updates. To handle evolving conditions, we introduce a changepoint-aware mechanism with a latent variable inferred from data likelihoods that indicates continuity or shift. When continuity is likely, evidence accumulates to refine predictions; when a shift is detected, past information is tempered to enable rapid re-learning. This maintains calibrated uncertainty and supports probabilistic reasoning about transient, gradual, or structural change. We prove that the adaptive regret of the framework grows only logarithmically in time and linearly with the number of shifts, competitive with an oracle that knows timings of shift. We validate on cartpole simulations and real quadrotor flights with swinging payloads and mid-flight drops, showing improved predictive accuracy, faster recovery, and more accurate closed-loop tracking than relevant baselines.", "AI": {"tldr": "The paper introduces a framework for robotic systems that can adjust to evolving dynamics in real-time using a changepoint-aware mechanism and Bayesian updates.", "motivation": "Robots often face changing conditions, disturbances, and unknown effects, requiring robust real-time adaptation for seamless operations.", "method": "The framework combines offline-learned latent representations with real-time Bayesian updates, supported by a changepoint-aware mechanism to distinguish between continuity and shifts.", "result": "The framework achieves improved predictive accuracy, faster adaptation, and better closed-loop control in simulations and real flight tests versus baseline methods.", "conclusion": "The proposed method allows calibrated uncertainty, efficient adaptation to changes, and robust performance in dynamic robotic environments."}}
{"id": "2512.14246", "pdf": "https://arxiv.org/pdf/2512.14246", "abs": "https://arxiv.org/abs/2512.14246", "authors": ["Evgenii Chzhen", "Mohamed Hebiri", "Gayane Taturyan"], "title": "Randomized multi-class classification under system constraints: a unified approach via post-processing", "categories": ["math.OC", "stat.ML"], "comment": null, "summary": "We study the problem of multi-class classification under system-level constraints expressible as linear functionals over randomized classifiers. We propose a post-processing approach that adjusts a given base classifier to satisfy general constraints without retraining. Our method formulates the problem as a linearly constrained stochastic program over randomized classifiers, and leverages entropic regularization and dual optimization techniques to construct a feasible solution. We provide finite-sample guarantees for the risk and constraint satisfaction for the final output of our algorithm under minimal assumptions. The framework accommodates a broad class of constraints, including fairness, abstention, and churn requirements.", "AI": {"tldr": "The paper introduces a method to adjust multi-class classifiers to meet system-level constraints without retraining, using optimization techniques.", "motivation": "To address the need for classifiers that can satisfy various system-level constraints (e.g., fairness, abstention) without requiring extensive retraining.", "method": "Formulates the problem as a stochastic program with linear constraints, using entropic regularization and dual optimization to adjust classifiers post-training.", "result": "Achieved constraint satisfaction and finite-sample guarantees while accommodating fairness, abstention, and other requirements.", "conclusion": "The method provides an efficient post-processing tool for adapting classifiers to meet diverse system-level constraints with minimal assumptions."}}
{"id": "2512.13732", "pdf": "https://arxiv.org/pdf/2512.13732", "abs": "https://arxiv.org/abs/2512.13732", "authors": ["Weijie Yang", "Xun Zhang"], "title": "PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set-Conditioned Diffusion", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only $0.29\\%$ -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by $12.28\\%$--$88.73\\%$ and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.", "AI": {"tldr": "The paper introduces the Physical Inversion Solver (PIS) to perform PDE-constrained inversion under sparse and irregular observation conditions, outperforming existing methods significantly.", "motivation": "The paper addresses the challenge of ill-posed parameter estimation in PDE-constrained inversion, especially under limited and irregular observations where existing models fail.", "method": "PIS utilizes a Set Transformer-based encoder for handling flexible observation sets and introduces a cosine-annealed sparsity curriculum for robustness. Additionally, it provides an information-theoretic analysis to assess inversion limits.", "result": "PIS significantly outperforms baselines in inverse problems like Darcy flow, Helmholtz, and Hooke's Law, reducing inversion error by up to 88.73% and maintaining robustness under extreme observation sparsity.", "conclusion": "PIS is a reliable and sparsity-resilient method for physical inversion, effectively handling arbitrary observation scenarios and producing calibrated posterior samples reflecting uncertainties."}}
{"id": "2512.13876", "pdf": "https://arxiv.org/pdf/2512.13876", "abs": "https://arxiv.org/abs/2512.13876", "authors": ["Ye Zhang", "Qi Chen", "Wenyou Huang", "Rui Liu", "Zhengjian Kang"], "title": "Route-DETR: Pairwise Query Routing in Transformers for Object Detection", "categories": ["cs.CV"], "comment": "5 pages, 2 figures", "summary": "Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-attention layers. Our key insight is distinguishing between competing queries (targeting the same object) versus complementary queries (targeting different objects) using inter-query similarity, confidence scores, and geometry. We introduce dual routing mechanisms: suppressor routes that modulate attention between competing queries to reduce duplication, and delegator routes that encourage exploration of different regions. These are implemented via learnable low-rank attention biases enabling asymmetric query interactions. A dual-branch training strategy incorporates routing biases only during training while preserving standard attention for inference, ensuring no additional computational cost. Experiments on COCO and Cityscapes demonstrate consistent improvements across multiple DETR baselines, achieving +1.7% mAP gain over DINO on ResNet-50 and reaching 57.6% mAP on Swin-L, surpassing prior state-of-the-art models.", "AI": {"tldr": "Route-DETR improves DETR by introducing adaptive pairwise routing mechanisms, reducing query competition and improving object detection efficiency.", "motivation": "DETR suffers inefficiencies due to redundant query competition in detecting objects, which leads to computational overhead.", "method": "Introduces suppressor and delegator routing mechanisms based on inter-query similarity, confidence, and geometry, implemented through learnable attention biases.", "result": "Achieves significant performance gains, with up to +1.7% mAP improvement on benchmark datasets like COCO and Cityscapes, surpassing state-of-the-art models.", "conclusion": "Route-DETR enhances DETR's efficiency and effectiveness by addressing query-level inefficiencies, offering competitive performance with no computational overhead during inference."}}
{"id": "2512.14014", "pdf": "https://arxiv.org/pdf/2512.14014", "abs": "https://arxiv.org/abs/2512.14014", "authors": ["Shufan Li", "Konstantinos Kallidromitis", "Akash Gokul", "Yusuke Kato", "Kazuki Kozuka", "Aditya Grover"], "title": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents", "categories": ["cs.AI"], "comment": "21 pages, 13 figures", "summary": "World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld", "AI": {"tldr": "This paper introduces a new approach to using vision-language models for world modeling in GUI settings, including a benchmark, dataset, and planning framework for mobile agents.", "motivation": "The study aims to address the limitations of pixel-space world models in GUI settings, where predicting visual complexities is challenging, by exploring natural language-based state transitions.", "method": "The paper proposes MobileWorldBench, a benchmark for vision-language models, releases a large-scale dataset (MobileWorld), and integrates VLM world models into the planning frameworks for mobile agents.", "result": "The proposed framework and dataset significantly improve the task success rates for mobile agents, demonstrating the practical benefits of semantic world models.", "conclusion": "Semantic world models using vision-language frameworks enhance the performance of mobile GUI agents in task completion, offering a viable alternative to pixel-based approaches."}}
{"id": "2512.14306", "pdf": "https://arxiv.org/pdf/2512.14306", "abs": "https://arxiv.org/abs/2512.14306", "authors": ["Nikoleta Anesti", "Edward Hill", "Andreas Joseph"], "title": "Inflation Attitudes of Large Language Models", "categories": ["cs.CL", "econ.EM"], "comment": "41 pages, 11 figures", "summary": "This paper investigates the ability of Large Language Models (LLMs), specifically GPT-3.5-turbo (GPT), to form inflation perceptions and expectations based on macroeconomic price signals. We compare the LLM's output to household survey data and official statistics, mimicking the information set and demographic characteristics of the Bank of England's Inflation Attitudes Survey (IAS). Our quasi-experimental design exploits the timing of GPT's training cut-off in September 2021 which means it has no knowledge of the subsequent UK inflation surge. We find that GPT tracks aggregate survey projections and official statistics at short horizons. At a disaggregated level, GPT replicates key empirical regularities of households' inflation perceptions, particularly for income, housing tenure, and social class. A novel Shapley value decomposition of LLM outputs suited for the synthetic survey setting provides well-defined insights into the drivers of model outputs linked to prompt content. We find that GPT demonstrates a heightened sensitivity to food inflation information similar to that of human respondents. However, we also find that it lacks a consistent model of consumer price inflation. More generally, our approach could be used to evaluate the behaviour of LLMs for use in the social sciences, to compare different models, or to assist in survey design.", "AI": {"tldr": "The paper examines GPT-3.5-turbo's ability to perceive and form expectations about inflation, comparing its outputs to human survey data and official statistics. It finds that the model replicates human-like sensitivities but lacks a consistent inflation framework.", "motivation": "The paper aims to assess whether LLMs like GPT-3.5-turbo can simulate human-like inflation perceptions and expectations, leveraging macroeconomic data and survey comparisons to explore potential applications in social sciences.", "method": "The authors use a quasi-experimental design based on GPT's training cutoff (September 2021) and compare its outputs to the Bank of England's Inflation Attitudes Survey. They also employ a Shapley value decomposition to analyze the model's output drivers.", "result": "GPT aligns with aggregate survey projections and official stats for short-term horizons and mimics household inflation perception patterns based on demographic variables. It shows sensitivity to food inflation but lacks a consistent consumer price inflation framework.", "conclusion": "While GPT exhibits human-like tendencies in inflation perception, its inability to form a robust inflation model limits its utility. The approach offers potential for future social science applications and LLM comparisons."}}
{"id": "2512.14340", "pdf": "https://arxiv.org/pdf/2512.14340", "abs": "https://arxiv.org/abs/2512.14340", "authors": ["Aleksi Karhunen", "Teemu Hakala", "V\u00e4in\u00f6 Karjalainen", "Eija Honkavaara"], "title": "Field evaluation and optimization of a lightweight lidar-based UAV navigation system for dense boreal forest environments", "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The interest in the usage of uncrewed aerial vehicles (UAVs) for forest applications has increased in recent years. While above-canopy flight has reached a high level of autonomy, navigating under-canopy remains a significant challenge. The use of autonomous UAVs could reduce the burden of data collection, which has motivated the development of numerous solutions for under-canopy autonomous flight. However, the experiments conducted in the literature and their reporting lack rigor. Very rarely, the density and the difficulty of the test forests are reported, or multiple flights are flown, and the success rate of those flights is reported. The aim of this study was to implement an autonomously flying quadrotor based on a lightweight lidar using openly available algorithms and test its behavior in real forest environments. A set of rigorous experiments was conducted with a quadrotor prototype utilizing the IPC path planner and LTA-OM SLAM algorithm. Based on the results of the first 33 flights, the original system was further enhanced. With the optimized system, 60 flights were performed, resulting in a total of 93 test flights. The optimized system performed significantly better in terms of reliability and flight mission completion times, achieving success rates of 12/15 in a medium-density forest and 15/15 in a dense forest, at a target flight velocity of 1 m/s. At a target flight velocity of 2 m/s, it had a success rate of 12/15 and 5/15, respectively. Furthermore, a standardized testing setup and evaluation criteria were proposed, enabling consistent performance comparisons of autonomous under-canopy UAV systems, enhancing reproducibility, guiding system improvements, and accelerating progress in forest robotics.", "AI": {"tldr": "The study develops and tests an autonomous UAV for navigating under-canopy in forests using practical algorithms, performing 93 test flights with enhanced systems and proposing standard testing criteria for future advancements.", "motivation": "The motivation arises from the increased demand for UAVs in forest applications, particularly under-canopy navigation, where challenges in autonomy and data collection efficiency persist, along with a need for rigorous testing methods.", "method": "The study used a lightweight lidar-based quadrotor prototype with IPC path planner and LTA-OM SLAM algorithm. Rigorous experiments included 93 test flights to assess performance in varying forest densities at different target velocities.", "result": "The optimized UAV system demonstrated improved reliability and success rates, achieving 12/15 and 15/15 at 1 m/s in medium and dense forests respectively, but faced reduced success at 2 m/s, proving effective but velocity-dependent.", "conclusion": "The study provides an optimized UAV system for under-canopy flight with standardized testing setups. This enhances reproducibility and performance comparison, fostering advancements in autonomous forest robotics."}}
{"id": "2512.14272", "pdf": "https://arxiv.org/pdf/2512.14272", "abs": "https://arxiv.org/abs/2512.14272", "authors": ["Brian Buckley", "Adrian O'Hagan", "Marie Galligan"], "title": "A variational Bayes latent class approach for EHR-based patient phenotyping in R", "categories": ["stat.CO", "stat.ML"], "comment": "19 pages, 6 figures, to be submitted to The Journal of Statistical Software", "summary": "The VBphenoR package for R provides a closed-form variational Bayes approach to patient phenotyping using Electronic Health Records (EHR) data. We implement a variational Bayes Gaussian Mixture Model (GMM) algorithm using closed-form coordinate ascent variational inference (CAVI) to determine the patient phenotype latent class. We then implement a variational Bayes logistic regression, where we determine the probability of the phenotype in the supplied EHR cohort, the shift in biomarkers for patients with the phenotype of interest versus a healthy population and evaluate predictive performance of binary indicator clinical codes and medication codes. The logistic model likelihood applies the latent class from the GMM step to inform the conditional.", "AI": {"tldr": "The VBphenoR package provides a method to determine patient phenotypes from EHR using a variational Bayes approach.", "motivation": "To address the need for a robust and efficient tool to identify patient phenotypes from large-scale Electronic Health Records (EHR) data.", "method": "The paper introduces an R package, VBphenoR, which uses closed-form variational Bayes Gaussian Mixture Models (GMM) and logistic regression to identify and analyze patient phenotypes.", "result": "The package determines patient phenotype latent classes, calculates phenotype probabilities, measures biomarker shifts, and evaluates predictive performance using clinical and medication codes.", "conclusion": "The VBphenoR package effectively leverages variational Bayes methods to enhance patient phenotyping and analysis in healthcare research."}}
{"id": "2512.13733", "pdf": "https://arxiv.org/pdf/2512.13733", "abs": "https://arxiv.org/abs/2512.13733", "authors": ["Sidhant Sundrani", "Francesco Tudisco", "Pasquale Minervini"], "title": "Low-Rank Compression of Language Models via Differentiable Rank Selection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.", "AI": {"tldr": "The paper proposes LLRC, a method for compressing large language models without post-compression fine-tuning, achieving superior performance and computational efficiency by learning rank selection via mask weights.", "motivation": "The motivation is to address the challenge of optimal rank selection for model compression, balancing compression rate and performance, which is not fully addressed by existing heuristics or gradient-based methods.", "method": "The proposed method, LLRC, utilizes a gradient-based approach to learn mask weights that select singular values for low-rank decomposition. This minimizes activation divergence during compression without requiring post-compression fine-tuning.", "result": "LLRC achieves better performance than other methods, such as STRS or SVD-LLM, on tasks like MMLU, BoolQ, and OpenbookQA, while maintaining high compression rates and requiring no fine-tuning.", "conclusion": "LLRC presents a novel, efficient, fine-tuning-free solution for large-model compression, outpacing existing techniques in both effectiveness and simplicity."}}
{"id": "2512.13902", "pdf": "https://arxiv.org/pdf/2512.13902", "abs": "https://arxiv.org/abs/2512.13902", "authors": ["Anning Tian", "Byunghyun Ko", "Kaichen Qu", "Mengyuan Liu", "Jeongkyu Lee"], "title": "KLO-Net: A Dynamic K-NN Attention U-Net with CSP Encoder for Efficient Prostate Gland Segmentation from MRI", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint. Accepted to SPIE Medical Imaging 2026: Image Processing", "summary": "Real-time deployment of prostate MRI segmentation on clinical workstations is often bottlenecked by computational load and memory footprint. Deep learning-based prostate gland segmentation approaches remain challenging due to anatomical variability. To bridge this efficiency gap while still maintaining reliable segmentation accuracy, we propose KLO-Net, a dynamic K-Nearest Neighbor attention U-Net with Cross Stage Partial, i.e., CSP, encoder for efficient prostate gland segmentation from MRI scan. Unlike the regular K-NN attention mechanism, the proposed dynamic K-NN attention mechanism allows the model to adaptively determine the number of attention connections for each spatial location within a slice. In addition, CSP blocks address the computational load to reduce memory consumption. To evaluate the model's performance, comprehensive experiments and ablation studies are conducted on two public datasets, i.e., PROMISE12 and PROSTATEx, to validate the proposed architecture. The detailed comparative analysis demonstrates the model's advantage in computational efficiency and segmentation quality.", "AI": {"tldr": "The authors propose KLO-Net, an efficient model for prostate MRI segmentation using dynamic K-NN attention mechanism and CSP encoder, achieving high accuracy and efficiency.", "motivation": "Current prostate MRI segmentation models struggle with computational load, memory footprint, and adapting to anatomical variability in real clinical settings.", "method": "The authors introduce KLO-Net, featuring a dynamic K-NN attention mechanism to adaptively determine connections and a CSP encoder to reduce computational load.", "result": "Extensive experiments on PROMISE12 and PROSTATEx datasets demonstrate KLO-Net's effectiveness in improving segmentation accuracy and computational efficiency.", "conclusion": "KLO-Net successfully bridges the efficiency and segmentation quality gap, offering a reliable solution for real-time prostate MRI segmentation in clinical use."}}
{"id": "2512.14043", "pdf": "https://arxiv.org/pdf/2512.14043", "abs": "https://arxiv.org/abs/2512.14043", "authors": ["Enhong Liu", "Haiyu Yang", "Miel Hostens"], "title": "Evaluating Small Language Models for Agentic On-Farm Decision Support Systems", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLM) hold potential to support dairy scholars and farmers by supporting decision-making and broadening access to knowledge for stakeholders with limited technical expertise. However, the substantial computational demand restricts access to LLM almost exclusively through cloud-based service, which makes LLM-based decision support tools impractical for dairy farming. To address this gap, lightweight alternatives capable of running locally on farm hardware are required. In this work, we benchmarked 20 open-source Small Language Models (SLM) available on HuggingFace under farm-realistic computing constraints. Building on our prior work, we developed an agentic AI system that integrates five task-specific agents: literature search, web search, SQL database interaction, NoSQL database interaction, and graph generation following predictive models. Evaluation was conducted in two phases. In the first phase, five test questions were used for the initial screening to identify models capable of following basic dairy-related instructions and performing reliably in a compute-constrained environment. Models that passed this preliminary stage were then evaluated using 30 questions (five per task category mentioned above, plus one category addressing integrity and misconduct) in phase two. In results, Qwen-4B achieved superior performance across most of task categories, although showed unstable effectiveness in NoSQL database interactions through PySpark. To our knowledge, this is the first work explicitly evaluating the feasibility of SLM as engines for dairy farming decision-making, with central emphases on privacy and computational efficiency. While results highlight the promise of SLM-assisted tools for practical deployment in dairy farming, challenges remain, and fine-tuning is still needed to refine SLM performance in dairy-specific questions.", "AI": {"tldr": "This study benchmarks 20 Small Language Models (SLM) for dairy farming decision-making, showing Qwen-4B achieving the highest performance with some limitations in specific tasks.", "motivation": "Dairy farming decision tools based on Large Language Models are impractical due to computational demands and reliance on cloud services, necessitating local, lightweight alternatives.", "method": "The researchers benchmarked 20 open-source SLMs from HuggingFace under farming-compute constraints, developed an agentic AI with five task-specific agents, and evaluated performance in two phases using dairy-related questions.", "result": "Qwen-4B performed best in most tasks but struggled with NoSQL database interactions, showing the promise of SLMs with the need for further optimization.", "conclusion": "SLMs are feasible for dairy farming decision-making tools with privacy and efficiency benefits, but challenges like fine-tuning for accuracy in specific tasks remain."}}
{"id": "2512.14332", "pdf": "https://arxiv.org/pdf/2512.14332", "abs": "https://arxiv.org/abs/2512.14332", "authors": ["Yannis Belkhiter", "Seshu Tirupathi", "Giulio Zizzo", "John D. Kelleher"], "title": "Step-Tagging: Toward controlling the generation of Language Reasoning Models through step monitoring", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The field of Language Reasoning Models (LRMs) has been very active over the past few years with advances in training and inference techniques enabling LRMs to reason longer, and more accurately. However, a growing body of studies show that LRMs are still inefficient, over-generating verification and reflection steps. To address this challenge, we introduce the Step-Tagging framework, a lightweight sentence-classifier enabling real-time annotation of the type of reasoning steps that an LRM is generating. To monitor reasoning behaviors, we introduced ReasonType: a novel taxonomy of reasoning steps. Building on this framework, we demonstrated that online monitoring of the count of specific steps can produce effective interpretable early stopping criteria of LRM inferences. We evaluate the Step-tagging framework on three open-source reasoning models across standard benchmark datasets: MATH500, GSM8K, AIME and non-mathematical tasks (GPQA and MMLU-Pro). We achieve 20 to 50\\% token reduction while maintaining comparable accuracy to standard generation, with largest gains observed on more computation-heavy tasks. This work offers a novel way to increase control over the generation of LRMs, and a new tool to study behaviors of LRMs.", "AI": {"tldr": "The paper proposes a lightweight step-tagging solution to manage inefficiency in Language Reasoning Models (LRMs), reducing over-generation while maintaining accuracy.", "motivation": "Despite advances in LRMs, inefficient reasoning processes (over-generating verification/reflection steps) hinder their performance.", "method": "The authors designed a Step-Tagging framework, combining ReasonType taxonomy and online monitoring, using early stopping to enhance inference efficiency.", "result": "Experiments show 20-50% token reduction with similar accuracy, particularly effective for computationally heavy tasks.", "conclusion": "Step-Tagging improves control and studies behaviors of reasoning models, paving the way for efficient LRM generation."}}
{"id": "2512.14350", "pdf": "https://arxiv.org/pdf/2512.14350", "abs": "https://arxiv.org/abs/2512.14350", "authors": ["Henrik Hose", "Paul Brunzema", "Alexander von Rohr", "Alexander Gr\u00e4fe", "Angela P. Schoellig", "Sebastian Trimpe"], "title": "Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Approximate model-predictive control (AMPC) aims to imitate an MPC's behavior with a neural network, removing the need to solve an expensive optimization problem at runtime. However, during deployment, the parameters of the underlying MPC must usually be fine-tuned. This often renders AMPC impractical as it requires repeatedly generating a new dataset and retraining the neural network. Recent work addresses this problem by adapting AMPC without retraining using approximated sensitivities of the MPC's optimization problem. Currently, this adaption must be done by hand, which is labor-intensive and can be unintuitive for high-dimensional systems. To solve this issue, we propose using Bayesian optimization to tune the parameters of AMPC policies based on experimental data. By combining model-based control with direct and local learning, our approach achieves superior performance to nominal AMPC on hardware, with minimal experimentation. This allows automatic and data-efficient adaptation of AMPC to new system instances and fine-tuning to cost functions that are difficult to directly implement in MPC. We demonstrate the proposed method in hardware experiments for the swing-up maneuver on an inverted cartpole and yaw control of an under-actuated balancing unicycle robot, a challenging control problem.", "AI": {"tldr": "The paper proposes a Bayesian optimization-based method for automatically adapting approximate model-predictive control (AMPC) policies to new system instances and cost function fine-tuning, avoiding manual and impractical retraining.", "motivation": "AMPC imitates MPC using neural networks to bypass costly runtime optimization, but adapting it to tuned parameters is labor-intensive and inefficient for high-dimensional systems.", "method": "The authors use Bayesian optimization to automatically and efficiently adapt AMPC policies using experimental data, combining model-based control with direct and local learning.", "result": "The proposed approach achieves superior performance compared to nominal AMPC on hardware, demonstrated through experiments on an inverted cartpole and balancing unicycle robot.", "conclusion": "The method offers a data-efficient solution for AMPC adaptation, streamlining parameter tuning without manual adjustments or retraining, and proving effective in real hardware applications."}}
{"id": "2512.14399", "pdf": "https://arxiv.org/pdf/2512.14399", "abs": "https://arxiv.org/abs/2512.14399", "authors": ["D\u00e1niel Pfeifer", "Edith Alice Kov\u00e1cs"], "title": "Trunc-Opt vine building algorithms", "categories": ["stat.ME", "stat.CO", "stat.ML"], "comment": null, "summary": "Vine copula models have become highly popular and practical tools for modelling multivariate probability distributions due to their flexibility in modelling different kinds of dependences between the random variables involved. However, their flexibility comes with the drawback of a high-dimensional parameter space. To tackle this problem, truncated vine copulas were introduced by Kurowicka (2010) (Gaussian case) and Brechmann and Czado (2013) (general case). Truncated vine copulas contain conditionally independent pair copulas after the truncation level. So far, in the general case, truncated vine constructing algorithms started from the lowest tree in order to encode the largest dependences in the lower trees. The novelty of this paper starts from the observation that a truncated vine is determined by the first tree after the truncation level (see Kov\u00e1cs and Sz\u00e1ntai (2017)). This paper introduces a new score for fitting truncated vines to given data, called the Weight of the truncated vine. Then we propose a completely new methodology for constructing truncated vines. We prove theorems which motivate this new approach. While earlier algorithms did not use conditional independences, we give algorithms for constructing and encoding truncated vines which do exploit them. Finally, we illustrate the algorithms on real datasets and compare the results with well-known methods included in R packages. Our method generally compare favorably to previously known methods.", "AI": {"tldr": "The paper presents a novel methodology for constructing truncated vine copula models by introducing a new scoring system and leveraging conditional independences.", "motivation": "Truncated vine copula models address high-dimensional parameter space in traditional vine copula models but lacked optimally efficient algorithms that utilize conditional independences.", "method": "A new 'Weight of the truncated vine' score is introduced as a basis for constructing truncated vines, supported by theoretical proofs and new algorithms leveraging conditional independences.", "result": "Algorithms were tested on real datasets and showed favorable performance compared to existing methods in R packages.", "conclusion": "The proposed method improves the construction and application of truncated vine copulas, making them more efficient and effective with conditional independence exploitation."}}
{"id": "2512.13734", "pdf": "https://arxiv.org/pdf/2512.13734", "abs": "https://arxiv.org/abs/2512.13734", "authors": ["Haochen Yuan", "Yang Zhang", "Xiang He", "Quan Z. Sheng", "Zhongjie Wang"], "title": "Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation", "categories": ["cs.LG", "cs.AI"], "comment": "This paper has been accepted for publication at AAAI 2026", "summary": "With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.", "AI": {"tldr": "This paper proposes a novel Federated Recommendation (FR) training framework using Parameter-Efficient Fine-Tuning (PEFT) to reduce communication overhead while maintaining or enhancing accuracy.", "motivation": "The motivation is to tackle the challenge of excessive communication overhead caused by the large number of item embedding parameters in Federated Recommendation models during distributed training.", "method": "The proposed method integrates a PEFT-based embedding strategy into existing FR frameworks, leveraging techniques like LoRA, hash-based encoding, and Residual Quantized Variational Autoencoders (RQ-VAE).", "result": "The framework demonstrates significant reductions in communication overhead while maintaining or enhancing model accuracy across multiple datasets and FR model architectures.", "conclusion": "The proposed framework addresses the embedding parameter overhead problem in FR, providing a lightweight, effective, and integrable solution. The approach presents a step forward in communication-efficient federated learning."}}
{"id": "2512.13950", "pdf": "https://arxiv.org/pdf/2512.13950", "abs": "https://arxiv.org/abs/2512.13950", "authors": ["Alban Gauthier", "Valentin Deschaintre", "Alexandre Lanvin", "Fredo Durand", "Adrien Bousseau", "George Drettakis"], "title": "An evaluation of SVBRDF Prediction from Generative Image Models for Appearance Modeling of 3D Scenes", "categories": ["cs.CV", "cs.GR"], "comment": "Project page: http://repo-sam.inria.fr/nerphys/svbrdf-evaluation Code: http://github.com/graphdeco-inria/svbrdf-evaluation", "summary": "Digital content creation is experiencing a profound change with the advent of deep generative models. For texturing, conditional image generators now allow the synthesis of realistic RGB images of a 3D scene that align with the geometry of that scene. For appearance modeling, SVBRDF prediction networks recover material parameters from RGB images. Combining these technologies allows us to quickly generate SVBRDF maps for multiple views of a 3D scene, which can be merged to form a SVBRDF texture atlas of that scene. In this paper, we analyze the challenges and opportunities for SVBRDF prediction in the context of such a fast appearance modeling pipeline. On the one hand, single-view SVBRDF predictions might suffer from multiview incoherence and yield inconsistent texture atlases. On the other hand, generated RGB images, and the different modalities on which they are conditioned, can provide additional information for SVBRDF estimation compared to photographs. We compare neural architectures and conditions to identify designs that achieve high accuracy and coherence. We find that, surprisingly, a standard UNet is competitive with more complex designs. Project page: http://repo-sam.inria.fr/nerphys/svbrdf-evaluation", "AI": {"tldr": "The paper evaluates SVBRDF prediction in a fast appearance modeling pipeline, comparing neural architectures and uncovering insights about single-view prediction challenges and multiview coherence.", "motivation": "To address the challenges of multiview incoherence in single-view SVBRDF predictions while exploring how conditional image generation can provide additional information.", "method": "The authors analyze neural architectures and conditions, comparing their performance for SVBRDF prediction. Notably, they assess a standard UNet alongside more complex designs.", "result": "They find that standard UNet is competitive in terms of accuracy and coherence, despite its simplicity.", "conclusion": "Standard neural network approaches like UNet may suffice for accurate and coherent SVBRDF prediction, challenging the need for complex models."}}
{"id": "2512.14048", "pdf": "https://arxiv.org/pdf/2512.14048", "abs": "https://arxiv.org/abs/2512.14048", "authors": ["Shen Li", "Li Huang", "Shaoxiong Zhan", "Weifeng Sun", "Tao Yin", "Zhongxin Liu", "Meng Yan"], "title": "Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation", "categories": ["cs.AI"], "comment": "Accepted at AAAI-2026", "summary": "Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.", "AI": {"tldr": "The paper introduces RoutingGen, a framework for code generation that adapts prompting strategies dynamically based on task difficulty, using new Intention Chain-of-Thought (ICoT) reasoning to enhance performance.", "motivation": "The motivation stems from the limitations in existing chain-of-thought (CoT) prompting methods, which either overcomplicate simple tasks or lack focus on core algorithmic intentions in code generation.", "method": "RoutingGen uses difficulty-aware routing: simple tasks use few-shot prompting, while complex tasks employ Intention Chain-of-Thought (ICoT), which guides the model to focus on algorithmic logic and efficiency.", "result": "RoutingGen achieved state-of-the-art performance on six standard code generation benchmarks and reduced token usage by 46.37%. Additionally, ICoT surpassed six prompting baselines on tough benchmarks.", "conclusion": "Dynamic prompting based on task complexity, coupled with intentional reasoning strategies like ICoT, significantly improves code generation performance and efficiency."}}
{"id": "2512.14427", "pdf": "https://arxiv.org/pdf/2512.14427", "abs": "https://arxiv.org/abs/2512.14427", "authors": ["Gabriele Prato", "Shagun Sodhani", "Alessandro Sordoni", "Sarath Chandar"], "title": "Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The standard practice for training large language models involves packing multiple documents together to optimize computational efficiency. However, the impact of this process on the models' capabilities remains largely unexplored. To address this gap, we investigate how different document-packing strategies influence the latent multi-hop reasoning abilities of LLMs. Our findings indicate that packing can improve model performance compared to training on individual documents, at the expense of more compute. To further understand the underlying mechanisms, we conduct an ablation study, identifying key factors that explain the advantages of packing. Ultimately, our research deepens the understanding of LLM training dynamics and provides practical insights for optimizing model development.", "AI": {"tldr": "This paper explores how document-packing strategies during training affect large language models' (LLMs) reasoning abilities, finding that packing improves performance, albeit at higher computational cost.", "motivation": "The motivation is to understand the impact of document-packing strategies during training\u2014commonly used for LLM efficiency\u2014on the models' performance, as this aspect is mostly unexplored.", "method": "An experimental investigation was carried out, including various document-packing strategies and an ablation study to analyze their effects on LLMs' multi-hop reasoning abilities.", "result": "Packing multiple documents improves the models' latent reasoning abilities compared to single-document training, but at the cost of additional computational resources.", "conclusion": "The results offer insights into training dynamics and practical guidelines for optimizing large language model development by leveraging document-packing effectively."}}
{"id": "2512.14355", "pdf": "https://arxiv.org/pdf/2512.14355", "abs": "https://arxiv.org/abs/2512.14355", "authors": ["J\u00f6rg Gamerdinger", "Sven Teufel", "Georg Volk", "Oliver Bringmann"], "title": "CoLD Fusion: A Real-time Capable Spline-based Fusion Algorithm for Collective Lane Detection", "categories": ["cs.RO"], "comment": "Accepted at IEEE IV 2023", "summary": "Comprehensive environment perception is essential for autonomous vehicles to operate safely. It is crucial to detect both dynamic road users and static objects like traffic signs or lanes as these are required for safe motion planning. However, in many circumstances a complete perception of other objects or lanes is not achievable due to limited sensor ranges, occlusions, and curves. In scenarios where an accurate localization is not possible or for roads where no HD maps are available, an autonomous vehicle must rely solely on its perceived road information. Thus, extending local sensing capabilities through collective perception using vehicle-to-vehicle communication is a promising strategy that has not yet been explored for lane detection. Therefore, we propose a real-time capable approach for collective perception of lanes using a spline-based estimation of undetected road sections. We evaluate our proposed fusion algorithm in various situations and road types. We were able to achieve real-time capability and extend the perception range by up to 200%.", "AI": {"tldr": "The paper proposes a real-time method to enhance lane detection for autonomous vehicles through collective perception using vehicle-to-vehicle communication, achieving up to 200% improved perception range.", "motivation": "Autonomous vehicles struggle with detecting lanes and objects due to sensor limitations, occlusions, and the absence of high-definition maps in some areas, presenting safety challenges.", "method": "A spline-based estimation algorithm is employed to fuse local sensing data with information shared via vehicle-to-vehicle communication, enabling extended perception of lanes in real-time.", "result": "The method achieves real-time performance and extends the perception range by 200%, demonstrating effectiveness across various scenarios and road types.", "conclusion": "The proposed approach significantly improves perception range through collective perception for safer and more reliable autonomous vehicle navigation."}}
{"id": "2512.14624", "pdf": "https://arxiv.org/pdf/2512.14624", "abs": "https://arxiv.org/abs/2512.14624", "authors": ["Rebecca M. Lewis", "Oliver Y. Feng", "Henry W. J. Reeve", "Min Xu", "Richard J. Samworth"], "title": "Learning the score under shape constraints", "categories": ["math.ST", "stat.ML"], "comment": "70 pages, 5 figures", "summary": "Score estimation has recently emerged as a key modern statistical challenge, due to its pivotal role in generative modelling via diffusion models. Moreover, it is an essential ingredient in a new approach to linear regression via convex $M$-estimation, where the corresponding error densities are projected onto the log-concave class. Motivated by these applications, we study the minimax risk of score estimation with respect to squared $L^2(P_0)$-loss, where $P_0$ denotes an underlying log-concave distribution on $\\mathbb{R}$. Such distributions have decreasing score functions, but on its own, this shape constraint is insufficient to guarantee a finite minimax risk. We therefore define subclasses of log-concave densities that capture two fundamental aspects of the estimation problem. First, we establish the crucial impact of tail behaviour on score estimation by determining the minimax rate over a class of log-concave densities whose score function exhibits controlled growth relative to the quantile levels. Second, we explore the interplay between smoothness and log-concavity by considering the class of log-concave densities with a scale restriction and a $(\u03b2,L)$-H\u00f6lder assumption on the log-density for some $\u03b2\\in [1,2]$. We show that the minimax risk over this latter class is of order $L^{2/(2\u03b2+1)}n^{-\u03b2/(2\u03b2+1)}$ up to poly-logarithmic factors, where $n$ denotes the sample size. When $\u03b2< 2$, this rate is faster than could be obtained under either the shape constraint or the smoothness assumption alone. Our upper bounds are attained by a locally adaptive, multiscale estimator constructed from a uniform confidence band for the score function. This study highlights intriguing differences between the score estimation and density estimation problems over this shape-constrained class.", "AI": {"tldr": "The paper investigates the minimax risk in score estimation for log-concave distributions and demonstrates the influence of tail behavior and smoothness constraints on accuracy.", "motivation": "Score estimation plays a crucial role in generative models and linear regression, and understanding its minimax risk for log-concave distributions is important for improving methodologies.", "method": "The authors define subclasses of log-concave densities considering tail behavior and smoothness constraints and analyze minimax risk rates using locally adaptive multiscale estimators.", "result": "The minimax risk for score estimation depends on tail growth and smoothness properties, achieving rates faster than using constraints independently when smoothness and scale are addressed together.", "conclusion": "Explored subclasses show that combining smoothness and shape constraints improves score estimation compared to single constraint methods, clarifying distinctions from density estimation."}}
{"id": "2512.13735", "pdf": "https://arxiv.org/pdf/2512.13735", "abs": "https://arxiv.org/abs/2512.13735", "authors": ["Xuechun Liu", "Heli Sun", "Xuecheng Wu", "Ruichen Cao", "Yunyun Shi", "Dingkang Yang", "Haoran Li"], "title": "DARTs: A Dual-Path Robust Framework for Anomaly Detection in High-Dimensional Multivariate Time Series", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multivariate time series anomaly detection (MTSAD) aims to accurately identify and localize complex abnormal patterns in the large-scale industrial control systems. While existing approaches excel in recognizing the distinct patterns under the low-dimensional scenarios, they often fail to robustly capture long-range spatiotemporal dependencies when learning representations from the high-dimensional noisy time series. To address these limitations, we propose DARTs, a robust long short-term dual-path framework with window-aware spatiotemporal soft fusion mechanism, which can be primarily decomposed into three complementary components. Specifically, in the short-term path, we introduce a Multi-View Sparse Graph Learner and a Diffusion Multi-Relation Graph Unit that collaborate to adaptively capture hierarchical discriminative short-term spatiotemporal patterns in the high-noise time series. While in the long-term path, we design a Multi-Scale Spatiotemporal Graph Constructor to model salient long-term dynamics within the high-dimensional representation space. Finally, a window-aware spatiotemporal soft-fusion mechanism is introduced to filter the residual noise while seamlessly integrating anomalous patterns. Extensive qualitative and quantitative experimental results across mainstream datasets demonstrate the superiority and robustness of our proposed DARTs. A series of ablation studies are also conducted to explore the crucial design factors of our proposed components. Our code and model will be made publicly open soon.", "AI": {"tldr": "This paper introduces DARTs, a novel framework for robust multivariate time series anomaly detection that captures complex spatiotemporal dependencies and handles high-dimensional noisy data effectively.", "motivation": "The motivation is to address limitations in existing MTSAD methods, which struggle with capturing long-range spatiotemporal dependencies and handling high-dimensional noisy time series data.", "method": "The authors propose DARTs, a dual-path framework with specific components: a Multi-View Sparse Graph Learner and Diffusion Multi-Relation Graph Unit for short-term path, a Multi-Scale Spatiotemporal Graph Constructor for long-term path, and a window-aware spatiotemporal soft-fusion mechanism for noise filtering and pattern integration.", "result": "DARTs demonstrate superior and robust performance on mainstream datasets, showcased through qualitative and quantitative experiments, alongside ablation studies for design factor analysis.", "conclusion": "DARTs effectively address the challenges in MTSAD by robustly capturing both short-term and long-term spatiotemporal patterns, offering a promising approach for detecting anomalies in high-dimensional noisy data."}}
{"id": "2512.13953", "pdf": "https://arxiv.org/pdf/2512.13953", "abs": "https://arxiv.org/abs/2512.13953", "authors": ["Dawid Malarz", "Artur Kasymov", "Filip Manjak", "Maciej Zi\u0119ba", "Przemys\u0142aw Spurek"], "title": "From Unlearning to UNBRANDING: A Benchmark for Trademark-Safe Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "The rapid progress of text-to-image diffusion models raises significant concerns regarding the unauthorized reproduction of trademarked content. While prior work targets general concepts (e.g., styles, celebrities), it fails to address specific brand identifiers. Crucially, we note that brand recognition is multi-dimensional, extending beyond explicit logos to encompass distinctive structural features (e.g., a car's front grille). To tackle this, we introduce unbranding, a novel task for the fine-grained removal of both trademarks and subtle structural brand features, while preserving semantic coherence. To facilitate research, we construct a comprehensive benchmark dataset. Recognizing that existing brand detectors are limited to logos and fail to capture abstract trade dress (e.g., the shape of a Coca-Cola bottle), we introduce a novel evaluation metric based on Vision Language Models (VLMs). This VLM-based metric uses a question-answering framework to probe images for both explicit logos and implicit, holistic brand characteristics. Furthermore, we observe that as model fidelity increases, with newer systems (SDXL, FLUX) synthesizing brand identifiers more readily than older models (Stable Diffusion), the urgency of the unbranding challenge is starkly highlighted. Our results, validated by our VLM metric, confirm unbranding is a distinct, practically relevant problem requiring specialized techniques. Project Page: https://gmum.github.io/UNBRANDING/.", "AI": {"tldr": "The paper introduces 'unbranding,' targeting the removal of both explicit trademarks and subtle brand features in text-to-image models while maintaining semantic coherence.", "motivation": "The proliferation of text-to-image diffusion models has led to concerns about unauthorized reproduction of trademarked content, which existing methods do not adequately address, especially subtle and structural brand identifiers.", "method": "The authors constructed a benchmark dataset and proposed a novel evaluation metric utilizing Vision Language Models (VLMs) in a question-answering framework to evaluate explicit logos and abstract brand features.", "result": "Their findings indicate that as model fidelity improves, newer systems generate brand identifiers more readily than older models, emphasizing the need for the unbranding task.", "conclusion": "Unbranding is a critical and unique problem that requires tailored approaches, such as the techniques introduced and validated through their VLM metric."}}
{"id": "2512.14051", "pdf": "https://arxiv.org/pdf/2512.14051", "abs": "https://arxiv.org/abs/2512.14051", "authors": ["Mengzhang Cai", "Xin Gao", "Yu Li", "Honglin Lin", "Zheng Liu", "Zhuoshi Pan", "Qizhi Pei", "Xiaoran Shang", "Mengyuan Sun", "Zinan Tang", "Xiaoyang Wang", "Zhanping Zhong", "Yun Zhu", "Dahua Lin", "Conghui He", "Lijun Wu"], "title": "OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value", "categories": ["cs.AI"], "comment": null, "summary": "The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.", "AI": {"tldr": "OpenDataArena (ODA) offers an open platform addressing the opaque nature of post-training datasets by benchmarking their value using a unified framework, extensive analysis tools, and open-source resources.", "motivation": "The paper addresses the lack of transparency, systematic evaluation, and reproducibility in the post-training datasets crucial for Large Language Models (LLMs).", "method": "Introduced OpenDataArena (ODA), a platform with a unified training-evaluation pipeline, multi-dimensional scoring framework, interactive dataset exploration, and open-source toolkit.", "result": "Extensive experiments highlighted trade-offs, redundancy in benchmarks, and genealogical links across datasets through the ODA framework, also providing tools and configurations publicly.", "conclusion": "ODA promotes a paradigm shift towards Data-Centric AI, emphasizing rigorous evaluation and strategic data curation for foundation models."}}
{"id": "2512.14481", "pdf": "https://arxiv.org/pdf/2512.14481", "abs": "https://arxiv.org/abs/2512.14481", "authors": ["Shizhuo Mao", "Song Chen", "Yi Kang"], "title": "SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.", "AI": {"tldr": "This paper introduces SASQ, a quantization-aware training framework for activation quantization, reducing perplexity while maintaining static inference efficiency.", "motivation": "Deploying large language models is challenging due to GPU memory limitations, and existing quantization approaches either sacrifice accuracy or incur high costs.", "method": "SASQ optimizes quantization factors (not weights), uses static inference, and adaptively truncates outliers to balance distributional characteristics.", "result": "SASQ achieves superior performance with 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than FP16 models on WikiText2.", "conclusion": "SASQ offers a practical solution for efficient and highly accurate model deployment compared to current state-of-the-art approaches."}}
{"id": "2512.14367", "pdf": "https://arxiv.org/pdf/2512.14367", "abs": "https://arxiv.org/abs/2512.14367", "authors": ["Georg Volk", "J\u00f6rg Gamerdinger", "Alexander von Bernuth", "Oliver Bringmann"], "title": "A Comprehensive Safety Metric to Evaluate Perception in Autonomous Systems", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at IEEE ITSC 2020", "summary": "Complete perception of the environment and its correct interpretation is crucial for autonomous vehicles. Object perception is the main component of automotive surround sensing. Various metrics already exist for the evaluation of object perception. However, objects can be of different importance depending on their velocity, orientation, distance, size, or the potential damage that could be caused by a collision due to a missed detection. Thus, these additional parameters have to be considered for safety evaluation. We propose a new safety metric that incorporates all these parameters and returns a single easily interpretable safety assessment score for object perception. This new metric is evaluated with both real world and virtual data sets and compared to state of the art metrics.", "AI": {"tldr": "The paper proposes a new safety metric for object perception in autonomous vehicles, integrating parameters like velocity, orientation, and collision impact to produce a single safety score.", "motivation": "Existing evaluation metrics for object perception do not consider the varying importance of objects based on factors such as velocity, orientation, and collision impact. Addressing this gap enhances safety in autonomous vehicles.", "method": "The authors introduced a new safety metric that incorporates parameters such as the velocity, orientation, distance, size, and potential collision damage of detected objects.", "result": "The proposed metric was tested on both real-world and virtual data sets, showing effectiveness and comparison to existing state-of-the-art metrics.", "conclusion": "By integrating critical parameters into a safety metric, this method provides a more precise safety assessment for object perception, advancing autonomous vehicle performance."}}
{"id": "2512.14686", "pdf": "https://arxiv.org/pdf/2512.14686", "abs": "https://arxiv.org/abs/2512.14686", "authors": ["Chuan He"], "title": "Bias-Variance Trade-off for Clipped Stochastic First-Order Methods: From Bounded Variance to Infinite Mean", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.CO", "stat.ML"], "comment": null, "summary": "Stochastic optimization is fundamental to modern machine learning. Recent research has extended the study of stochastic first-order methods (SFOMs) from light-tailed to heavy-tailed noise, which frequently arises in practice, with clipping emerging as a key technique for controlling heavy-tailed gradients. Extensive theoretical advances have further shown that the oracle complexity of SFOMs depends on the tail index $\u03b1$ of the noise. Nonetheless, existing complexity results often cover only the case $\u03b1\\in (1,2]$, that is, the regime where the noise has a finite mean, while the complexity bounds tend to infinity as $\u03b1$ approaches $1$. This paper tackles the general case of noise with tail index $\u03b1\\in(0,2]$, covering regimes ranging from noise with bounded variance to noise with an infinite mean, where the latter case has been scarcely studied. Through a novel analysis of the bias-variance trade-off in gradient clipping, we show that when a symmetry measure of the noise tail is controlled, clipped SFOMs achieve improved complexity guarantees in the presence of heavy-tailed noise for any tail index $\u03b1\\in (0,2]$. Our analysis of the bias-variance trade-off not only yields new unified complexity guarantees for clipped SFOMs across this full range of tail indices, but is also straightforward to apply and can be combined with classical analyses under light-tailed noise to establish oracle complexity guarantees under heavy-tailed noise. Finally, numerical experiments validate our theoretical findings.", "AI": {"tldr": "The paper addresses stochastic optimization with heavy-tailed noise ($\u03b1 \\in (0,2]$), proposing improved complexity guarantees for clipped methods across light- and heavy-tailed regimes.", "motivation": "Modern machine learning often encounters heavy-tailed noise, which poses challenges for stochastic optimization. Current methods inadequately handle cases where the noise mean is infinite ($\u03b1\\approx1$).", "method": "The paper introduces a bias-variance trade-off analysis for gradient clipping techniques under heavy-tailed noise, considering the full range $\u03b1\\in (0,2]$.", "result": "Clipped first-order methods achieve enhanced oracle complexity guarantees for any tail index $\u03b1\\in (0,2]$, validated through numerical experiments.", "conclusion": "Unified complexity guarantees for clipped SFOMs demonstrate their robustness under both light- and heavy-tailed noise conditions, offering practical improvements."}}
{"id": "2512.13736", "pdf": "https://arxiv.org/pdf/2512.13736", "abs": "https://arxiv.org/abs/2512.13736", "authors": ["Li-Xuan Zhao", "Chen-Yang Xu", "Wen-Qiang Li", "Bo Wang", "Rong-Xing Wei", "Qing-Hao Menga"], "title": "TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.", "AI": {"tldr": "The paper introduces a new model, TF-MCL, for major depressive disorder (MDD) detection using EEG data, significantly improving accuracy over state-of-the-art methods.", "motivation": "To address challenges in labeling MDD data and the limitations of existing supervised and contrastive learning methods in capturing time-frequency distributions of EEG signals effectively.", "method": "A TF-MCL model is proposed, which uses a fusion mapping head (FMH) and a multi-domain cross-loss function to generate and optimize time-frequency hybrid representations, improving data synthesis and representation.", "result": "The model demonstrated a significant improvement in accuracy on MODMA and PRED+CT datasets, outperforming state-of-the-art methods by 5.87% and 9.96%, respectively.", "conclusion": "TF-MCL enhances MDD detection accuracy by efficiently utilizing time-frequency EEG data through advanced fusion techniques and loss functions, showcasing its superiority over existing methods."}}
{"id": "2512.13970", "pdf": "https://arxiv.org/pdf/2512.13970", "abs": "https://arxiv.org/abs/2512.13970", "authors": ["Miaohua Zhang", "Mohammad Ali Armin", "Xuesong Li", "Sisi Liang", "Lars Petersson", "Changming Sun", "David Ahmedt-Aristizabal", "Zeeshan Hayder"], "title": "Quality-Driven and Diversity-Aware Sample Expansion for Robust Marine Obstacle Segmentation", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Marine obstacle detection demands robust segmentation under challenging conditions, such as sun glitter, fog, and rapidly changing wave patterns. These factors degrade image quality, while the scarcity and structural repetition of marine datasets limit the diversity of available training data. Although mask-conditioned diffusion models can synthesize layout-aligned samples, they often produce low-diversity outputs when conditioned on low-entropy masks and prompts, limiting their utility for improving robustness. In this paper, we propose a quality-driven and diversity-aware sample expansion pipeline that generates training data entirely at inference time, without retraining the diffusion model. The framework combines two key components:(i) a class-aware style bank that constructs high-entropy, semantically grounded prompts, and (ii) an adaptive annealing sampler that perturbs early conditioning, while a COD-guided proportional controller regulates this perturbation to boost diversity without compromising layout fidelity. Across marine obstacle benchmarks, augmenting training data with these controlled synthetic samples consistently improves segmentation performance across multiple backbones and increases visual variation in rare and texture-sensitive classes.", "AI": {"tldr": "The paper addresses challenges in marine obstacle detection and proposes a novel pipeline to generate diverse training data using diffusion models without retraining, improving segmentation performance.", "motivation": "Marine obstacle detection is hindered by challenging conditions such as degraded image quality and limited dataset diversity, necessitating methods to enhance robustness and diversify training data.", "method": "The proposed pipeline utilizes a class-aware style bank for semantically grounded prompts and an adaptive annealing sampler for controlled perturbation, optimizing diversity and fidelity without retraining diffusion models.", "result": "Using synthetic samples generated by the pipeline improves segmentation performance across marine obstacle benchmarks and increases visual variation in rare and texture-sensitive classes.", "conclusion": "The quality-driven and diversity-aware pipeline enhances training datasets at inference time, significantly boosting segmentation accuracy and dataset diversity without requiring additional model retraining."}}
{"id": "2512.14069", "pdf": "https://arxiv.org/pdf/2512.14069", "abs": "https://arxiv.org/abs/2512.14069", "authors": ["Junjie Ma", "Jinlong Li"], "title": "RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees", "categories": ["cs.AI"], "comment": "5 pages, 2 figures", "summary": "Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.", "AI": {"tldr": "RADAR introduces a speculative sampling method leveraging reinforcement learning to dynamically adjust draft model calls, significantly speeding up inference with Large Language Models.", "motivation": "Modern LLMs are expensive and slow during inference, and speculative sampling methods have limitations due to rigid preset hyperparameters on draft model calls, resulting in inefficiencies.", "method": "RADAR uses reinforcement learning to dynamically manage draft tree generation as a Markov Decision Process (MDP). It trains a prediction model using offline RL for real-time decision making, optimizing the number of draft model calls to improve performance.", "result": "RADAR shows significant inference speedup, achieving 3.17x-4.82x faster processing compared to traditional auto-regressive decoding across various LLMs and tasks.", "conclusion": "RADAR enhances speculative sampling for LLMs by employing dynamic, RL-driven methodology, reducing computational redundancy and providing substantial inference speed improvements."}}
{"id": "2512.14500", "pdf": "https://arxiv.org/pdf/2512.14500", "abs": "https://arxiv.org/abs/2512.14500", "authors": ["Teodor Poncu", "Ioana Pintilie", "Marius Dragoi", "Dragos Tantaru", "Florin Brad"], "title": "C-ing Clearly: Enhanced Binary Code Explanations using C code", "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 5 figures", "summary": "Large Language Models (LLMs) typically excel at coding tasks involving high-level programming languages, as opposed to lower-level programming languages, such as assembly. We propose a synthetic data generation method named C-ing Clearly, which leverages the corresponding C code to enhance an LLM's understanding of assembly. By fine-tuning on data generated through our method, we demonstrate improved LLM performance for binary code summarization and vulnerability detection. Our approach demonstrates consistent gains across different LLM families and model sizes.", "AI": {"tldr": "The paper introduces C-ing Clearly, a synthetic data generation method to improve LLM performance on assembly language tasks.", "motivation": "LLMs generally struggle with lower-level programming languages, such as assembly, compared to high-level languages.", "method": "The authors propose fine-tuning LLMs using synthetic data generated from corresponding C code to enhance assembly understanding.", "result": "Fine-tuning with the proposed method improved LLM performance in tasks like binary code summarization and vulnerability detection, across various model families and sizes.", "conclusion": "The synthetic data generation method provides a consistent enhancement for assembly-related tasks, indicating its effectiveness for improving low-level programming language understanding."}}
{"id": "2512.14411", "pdf": "https://arxiv.org/pdf/2512.14411", "abs": "https://arxiv.org/abs/2512.14411", "authors": ["Mohammed Ayman Habib", "Aldo Petruzzelli"], "title": "Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids", "categories": ["cs.RO"], "comment": "6 pages; xTech Humanoid white paper submission", "summary": "Omnia presents a synthetic data driven pipeline to accelerate the training, validation, and deployment readiness of militarized humanoids. The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy. By generating large volumes of high-fidelity simulated scenarios and pairing them with automated labeling and model training, the pipeline enables rapid iteration on perception, navigation, and decision-making capabilities without the cost, risk, or time constraints of extensive field trials. The resulting datasets can be tuned quickly for new operational environments and threat conditions, supporting both baseline humanoid performance and advanced subsystems such as multimodal sensing, counter-detection survivability, and CBRNE-relevant reconnaissance behaviors. This work targets faster development cycles and improved robustness in complex, contested settings by exposing humanoid systems to broad scenario diversity early in the development process.", "AI": {"tldr": "The paper proposes a method to use synthetic data to train and validate militarized humanoids more efficiently by generating scalable datasets capable of supporting various operational scenarios.", "motivation": "The work aims to reduce the cost, risk, and time constraints associated with training militarized humanoids through traditional methods, by leveraging scalable synthetic datasets.", "method": "A data pipeline that transforms spatial observations from recordings and AR devices into high-fidelity synthetic datasets. These datasets are used for automated labeling, training, and testing of humanoid capabilities.", "result": "The pipeline creates diverse datasets for fast tuning to different environments, supporting improved humanoid decision-making, navigation, and reconnaissance under varied conditions.", "conclusion": "The synthetic-data-driven approach accelerates development while improving robustness and flexibility of militarized humanoids in complex and contested operations."}}
{"id": "2512.13741", "pdf": "https://arxiv.org/pdf/2512.13741", "abs": "https://arxiv.org/abs/2512.13741", "authors": ["Md. Hasib Ur Rahman"], "title": "The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial \"jailbreaking\" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy \"reflex-based\" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.", "AI": {"tldr": "The paper introduces Semantic Turbulence as a metric to detect adversarial attacks on LLMs, leveraging internal model dynamics without external classifiers.", "motivation": "Ensuring LLM security against adversarial jailbreak attacks is critical but existing methods like filters or external classifiers are inadequate.", "method": "Introduces Laminar Flow Hypothesis and a metric based on layer-wise cosine velocity to analyze model behavior under benign and adversarial prompts.", "result": "The Qwen2-1.5B and Gemma-2B models show contrasting behaviors under attack, validating the hypothesis and demonstrating the metric's diagnostic capabilities.", "conclusion": "Semantic Turbulence is effective for jailbreak detection and provides insights into LLM safety architectures in a lightweight and non-invasive manner."}}
{"id": "2512.13977", "pdf": "https://arxiv.org/pdf/2512.13977", "abs": "https://arxiv.org/abs/2512.13977", "authors": ["Youssef Abuzeid", "Shimaa El-Bana", "Ahmad Al-Kabbany"], "title": "XAI-Driven Diagnosis of Generalization Failure in State-Space Cerebrovascular Segmentation Models: A Case Study on Domain Shift Between RSNA and TopCoW Datasets", "categories": ["cs.CV"], "comment": null, "summary": "The clinical deployment of deep learning models in medical imaging is severely hindered by domain shift. This challenge, where a high-performing model fails catastrophically on external datasets, is a critical barrier to trustworthy AI. Addressing this requires moving beyond simple performance metrics toward deeper understanding, making Explainable AI (XAI) an essential diagnostic tool in medical image analysis. We present a rigorous, two-phase approach to diagnose the generalization failure of state-of-the-art State-Space Models (SSMs), specifically UMamaba, applied to cerebrovascular segmentation. We first established a quantifiable domain gap between our Source (RSNA CTA Aneurysm) and Target (TopCoW Circle of Willis CT) datasets, noting significant differences in Z-resolution and background noise. The model's Dice score subsequently plummeted from 0.8604 (Source) to 0.2902 (Target). In the second phase, which is our core contribution, we utilized Seg-XRes-CAM to diagnose the cause of this failure. We quantified the model's focus by measuring the overlap between its attention maps and the Ground Truth segmentations, and between its attention maps and its own Prediction Mask. Our analysis proves the model failed to generalize because its attention mechanism abandoned true anatomical features in the Target domain. Quantitative metrics confirm the model's focus shifted away from the Ground Truth vessels (IoU~0.101 at 0.3 threshold) while still aligning with its own wrong predictions (IoU~0.282 at 0.3 threshold). This demonstrates the model learned spurious correlations, confirming XAI is a powerful diagnostic tool for identifying dataset bias in emerging architectures.", "AI": {"tldr": "This paper addresses domain shift issues in deploying deep learning models for medical imaging and introduces explainable AI (XAI) to diagnose generalization failures in cerebrovascular segmentation models.", "motivation": "To solve the critical problem of domain shift in medical imaging models, where models perform well on training datasets but fail on external datasets, impacting trustworthy AI in healthcare.", "method": "The paper used a two-phased approach: establishing a domain gap between datasets and using XAI (Seg-XRes-CAM) to analyze the model's generalization failure due to spurious features.", "result": "The Dice score dropped significantly from 0.8604 (Source dataset) to 0.2902 (Target dataset). Attention shifted from actual anatomical features to spurious correlations in the Target domain.", "conclusion": "XAI tools like Seg-XRes-CAM can reveal biases and help understand failure modes of models, making them essential in diagnosing and improving AI methods in medical imaging."}}
{"id": "2512.14079", "pdf": "https://arxiv.org/pdf/2512.14079", "abs": "https://arxiv.org/abs/2512.14079", "authors": ["Mayank Singh", "Vikas Yadav", "Shiva Krishna Reddy Malay", "Shravan Nayak", "Sai Rajeswar", "Sathwik Tejaswi Madhusudhan", "Eduardo Blanco"], "title": "Grammar Search for Multi-Agent Systems", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Automatic search for Multi-Agent Systems has recently emerged as a key focus in agentic AI research. Several prior approaches have relied on LLM-based free-form search over the code space. In this work, we propose a more structured framework that explores the same space through a fixed set of simple, composable components. We show that, despite lacking the generative flexibility of LLMs during the candidate generation stage, our method outperforms prior approaches on four out of five benchmarks across two domains: mathematics and question answering. Furthermore, our method offers additional advantages, including a more cost-efficient search process and the generation of modular, interpretable multi-agent systems with simpler logic.", "AI": {"tldr": "This paper presents a structured framework for creating Multi-Agent Systems (MAS), outperforming previous methods while being cost-efficient and interpretable.", "motivation": "The motivation is to improve the search and generation process for MAS by using structured and composable components, addressing limitations of LLM-based free-form search.", "method": "The method uses a structured framework with simple, composable components to explore MAS design space, rather than relying on the generative flexibility of LLMs.", "result": "The proposed method outperforms previous approaches on four out of five benchmarks in mathematics and question answering, and it is more cost-efficient and modular.", "conclusion": "The framework achieves better performance and interpretability compared to LLM-based approaches, making MAS development more accessible and efficient."}}
{"id": "2512.14428", "pdf": "https://arxiv.org/pdf/2512.14428", "abs": "https://arxiv.org/abs/2512.14428", "authors": ["Aaron Kurda", "Simon Steuernagel", "Lukas Jung", "Marcus Baum"], "title": "Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations", "categories": ["cs.RO"], "comment": "9 pages, 4 figures, submitted to International Journal of Robotics Research (IJRR)", "summary": "The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth. The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal. While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments. To close this gap, we present Odyssey, a LIO dataset with a focus on GNSS-denied environments such as tunnels and parking garages as well as other underrepresented, yet ubiquitous situations such as stop-and-go-traffic, bumpy roads and wide open fields. Our ground truth is derived from a navigation-grade Inertial Navigation System (INS) equipped with a Ring Laser Gyroscope (RLG), offering exceptional bias stability characteristics compared to IMUs used in existing datasets and enabling the prolonged and accurate study of GNSS-denied environments. This makes Odyssey the first publicly available dataset featuring a RLG-based INS. Besides providing data for LIO, we also support other tasks, such as place recognition, through the threefold repetition of all trajectories as well as the integration of external mapping data by providing precise geodetic coordinates. All data, dataloader and other material is available online at https://odyssey.uni-goettingen.de/ .", "AI": {"tldr": "Odyssey dataset focuses on GNSS-denied environments and offers high-accuracy ground truth using RLG-based INS, supporting LIO and SLAM development.", "motivation": "Address the challenges of GNSS failure in obstructed environments and provide reliable ground truth for LIO and SLAM systems.", "method": "Develop and release a dataset using navigation-grade INS equipped with RLG, tailored for GNSS-denied scenarios and diverse driving conditions.", "result": "Odyssey serves as the first publicly available dataset featuring RLG-based INS with precise geodetic coordinates and repetitive trajectories.", "conclusion": "Odyssey improves LIO and SLAM system studies through offering robust ground truth for challenging environments and diverse tasks."}}
{"id": "2512.13749", "pdf": "https://arxiv.org/pdf/2512.13749", "abs": "https://arxiv.org/abs/2512.13749", "authors": ["Joyjit Roy", "Samaresh Kumar Singh"], "title": "Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 2 figures. Submitted to IEEE IATMSI-2026 (Track: AI, IoT and Computer Vision Enabled Technologies)", "summary": "Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.", "AI": {"tldr": "This paper evaluates embedding-based methods for financial sentiment analysis on limited datasets, identifying challenges like overfitting and data insufficiency that hinder reliable predictions.", "motivation": "To address data scarcity in financial sentiment analysis and assess the effectiveness of embedding-based methods when applied to small datasets.", "method": "The study evaluates Word2Vec, GloVe, and sentence transformer embeddings with gradient boosting using manually labeled financial news headlines. Weekly sentiment aggregation and narrative summarization are also tested for practical market monitoring.", "result": "Experimental results reveal poor test performance and overfitting despite strong validation metrics. Pretrained embeddings show diminishing returns under data scarcity, and small validation sets exacerbate overfitting.", "conclusion": "Embedding quality alone cannot resolve data scarcity issues in sentiment classification. Alternative approaches like few-shot learning, data augmentation, or lexicon-enhanced hybrid methods are recommended when resources are limited."}}
{"id": "2512.13982", "pdf": "https://arxiv.org/pdf/2512.13982", "abs": "https://arxiv.org/abs/2512.13982", "authors": ["Dereje Shenkut", "Vijayakumar Bhagavatula"], "title": "FocalComm: Hard Instance-Aware Multi-Agent Perception", "categories": ["cs.CV"], "comment": "WACV 2026", "summary": "Multi-agent collaborative perception (CP) is a promising paradigm for improving autonomous driving safety, particularly for vulnerable road users like pedestrians, via robust 3D perception. However, existing CP approaches often optimize for vehicle detection performance metrics, underperforming on smaller, safety-critical objects such as pedestrians, where detection failures can be catastrophic. Furthermore, previous CP methods rely on full feature exchange rather than communicating only salient features that help reduce false negatives. To this end, we present FocalComm, a novel collaborative perception framework that focuses on exchanging hard-instance-oriented features among connected collaborative agents. FocalComm consists of two key novel designs: (1) a learnable progressive hard instance mining (HIM) module to extract hard instance-oriented features per agent, and (2) a query-based feature-level (intermediate) fusion technique that dynamically weights these identified features during collaboration. We show that FocalComm outperforms state-of-the-art collaborative perception methods on two challenging real-world datasets (V2X-Real and DAIR-V2X) across both vehicle-centric and infrastructure-centric collaborative setups. FocalComm also shows a strong performance gain in pedestrian detection in V2X-Real.", "AI": {"tldr": "FocalComm is a collaborative perception framework enhancing pedestrian detection by exchanging salient features among agents.", "motivation": "Improve autonomous driving safety, especially for pedestrians, by addressing limitations in existing collaborative perception methods.", "method": "Introduce FocalComm with hard-instance mining module and query-based fusion technique for dynamic feature collaboration.", "result": "FocalComm surpasses current methods in perception tasks, excelling in pedestrian detection across real-world datasets.", "conclusion": "FocalComm proves effective in advancing collaborative perception, particularly in safety-critical pedestrian detection."}}
{"id": "2512.14106", "pdf": "https://arxiv.org/pdf/2512.14106", "abs": "https://arxiv.org/abs/2512.14106", "authors": ["Ijaz Ul Haq", "Byung Suk Lee", "Julia N. Perdrial", "David Baude"], "title": "HydroGEM: A Self Supervised Zero Shot Hybrid TCN Transformer Foundation Model for Continental Scale Streamflow Quality Control", "categories": ["cs.AI"], "comment": "Supplementary materials, datasets, and implementation code will be made publicly available upon acceptance for publication in a peer-reviewed journal", "summary": "Real-time streamflow monitoring networks generate millions of observations annually, yet maintaining data quality across thousands of remote sensors remains labor-intensive. We introduce HydroGEM (Hydrological Generalizable Encoder for Monitoring), a foundation model for continental-scale streamflow quality control. HydroGEM uses two-stage training: self-supervised pretraining on 6.03 million sequences from 3,724 USGS stations learns hydrological representations, followed by fine-tuning with synthetic anomalies for detection and reconstruction. A hybrid TCN-Transformer architecture (14.2M parameters) captures local temporal patterns and long-range dependencies, while hierarchical normalization handles six orders of magnitude in discharge. On held-out synthetic tests comprising 799 stations with 18 expert-validated anomaly types, HydroGEM achieves F1 = 0.792 for detection and 68.7% reconstruction-error reduction, a 36.3% improvement over existing methods. Zero-shot transfer to 100 Environment and Climate Change Canada stations yields F1 = 0.586, exceeding all baselines and demonstrating cross-national generalization. The model maintains consistent detection across correction magnitudes and aligns with operational seasonal patterns. HydroGEM is designed for human-in-the-loop workflows - outputs are quality control suggestions requiring expert review, not autonomous corrections.", "AI": {"tldr": "HydroGEM is a foundation model designed for real-time quality control of streamflow monitoring across large scales, leveraging both pretraining on hydrological data and fine-tuning with synthetic anomalies.", "motivation": "The motivation is to address labor-intensive challenges of maintaining data quality in remote, large-scale streamflow monitoring networks with a more efficient, automated, and generalizable solution.", "method": "HydroGEM utilizes a hybrid TCN-Transformer model trained in two stages: pretraining on 6.03 million sequences across 3,724 USGS stations and fine-tuning with synthetic anomalies for anomaly detection and reconstruction. It employs hierarchical normalization to handle diverse data ranges.", "result": "HydroGEM achieved an F1 score of 0.792 for anomaly detection on held-out sets and a 68.7% reduction in reconstruction errors, surpassing existing methods by 36.3%. It demonstrated cross-national generalization with an F1 score of 0.586 on Canadian stations in a zero-shot transfer setting.", "conclusion": "HydroGEM combines robust anomaly detection with generalization across different geographies for streamflow quality control, offering an efficient human-in-the-loop system, with potential operational benefits for hydrological monitoring."}}
{"id": "2512.14531", "pdf": "https://arxiv.org/pdf/2512.14531", "abs": "https://arxiv.org/abs/2512.14531", "authors": ["Ying Nie", "Kai Han", "Hongguang Li", "Hang Zhou", "Tianyu Guo", "Enhua Wu", "Xinghao Chen", "Yunhe Wang"], "title": "VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse", "categories": ["cs.CL"], "comment": null, "summary": "The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering \"easy\" tokens through the efficient width-wise route and allocating deeper iterative refinement to \"hard\" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN.", "AI": {"tldr": "VersatileFFN introduces a parameter-efficient feed-forward network leveraging adaptive pathways to enhance model capacity without increasing memory usage.", "motivation": "Address the issue of high memory costs in scalable large language models while enhancing architectural capacity that surpasses representational limitations.", "method": "Proposed VersatileFFN that contains width-versatile and depth-versatile pathways using shared FFN parameters, guided by a difficulty-aware gating mechanism.", "result": "Experiments across benchmarks and scales show improved model performance with computational efficiency in handling diverse token complexities.", "conclusion": "VersatileFFN succeeds in providing efficient parameter reuse, balancing token complexities, and validation across benchmarks while conserving memory usage."}}
{"id": "2512.14434", "pdf": "https://arxiv.org/pdf/2512.14434", "abs": "https://arxiv.org/abs/2512.14434", "authors": ["Quan Yuan", "Daqian Cao", "Weibang Bai"], "title": "Geometric Parameter Optimization of a Novel 3-(PP(2-(UPS))) Redundant Parallel Mechanism based on Workspace Determination", "categories": ["cs.RO"], "comment": "7 pages, 5 figures", "summary": "Redundant parallel robots are normally employed in scenarios requiring good precision, high load capability, and large workspace compared to traditional parallel mechanisms. However, the elementary robotic configuration and geometric parameter optimization are still quite challenging. This paper proposes a novel 3-(PP(2-(UPS))) redundant parallel mechanism, with good generalizability first, and further investigates the kinematic optimization issue by analyzing and investigating how its key geometric parameters influence the volume, shape, boundary completeness, and orientation capabilities of its workspace. The torsional capability index TI_1 and tilting capability index TI_2 are defined to evaluate the orientation performance of the mechanism. Numerical simulation studies are completed to indicate the analysis, providing reasonable but essential references for the parameter optimization of 3-(PP(2-(UPS))) and other similar redundant parallel mechanisms.", "AI": {"tldr": "This paper introduces a novel redundant parallel mechanism design (3-(PP(2-(UPS))) and provides a kinematic optimization strategy to enhance its workspace and orientation capabilities.", "motivation": "Redundant parallel robots are increasingly used for their precision, high load capacity, and large workspace, but their geometric parameter optimization remains a significant challenge.", "method": "The authors propose the design of a 3-(PP(2-(UPS))) mechanism and analyze how specific geometric parameters impact workspace and orientation. They define torsional and tilting capability indices, and run numerical simulation studies to validate their findings.", "result": "Numerical simulations demonstrated the impact of geometric parameters on workspace characteristics and orientation, providing key references for optimizing similar mechanisms.", "conclusion": "The paper offers a novel approach to the design and kinematic optimization of redundant parallel mechanisms, enhancing their application potential in complex scenarios."}}
{"id": "2512.13751", "pdf": "https://arxiv.org/pdf/2512.13751", "abs": "https://arxiv.org/abs/2512.13751", "authors": ["Taero Kim", "Hoyoon Byun", "Youngjun Choi", "Sungrae Park", "Kyungwoo Song"], "title": "MIDUS: Memory-Infused Depth Up-Scaling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Scaling large language models (LLMs) demands approaches that increase capacity without incurring excessive parameter growth or inference cost. Depth Up-Scaling (DUS) has emerged as a promising strategy by duplicating layers and applying Continual Pre-training (CPT), but its reliance on feed-forward networks (FFNs) limits efficiency and attainable gains. We introduce Memory-Infused Depth Up-Scaling (MIDUS), which replaces FFNs in duplicated blocks with a head-wise memory (HML) layer. Motivated by observations that attention heads have distinct roles both across and within layers, MIDUS assigns an independent memory bank to each head, enabling head-wise retrieval and injecting information into subsequent layers while preserving head-wise functional structure. This design combines sparse memory access with head-wise representations and incorporates an efficient per-head value factorization module, thereby relaxing the usual efficiency-performance trade-off. Across our CPT experiments, MIDUS exhibits robust performance improvements over strong DUS baselines while maintaining a highly efficient parameter footprint. Our findings establish MIDUS as a compelling and resource-efficient alternative to conventional FFN replication for depth up-scaling by leveraging its head-wise memory design.", "AI": {"tldr": "MIDUS proposes a resource-efficient depth up-scaling method using head-wise memory layers, outperforming traditional feed-forward network approaches.", "motivation": "Scaling LLMs efficiently while preserving performance demands methods that avoid excessive parameter growth.", "method": "MIDUS replaces feed-forward networks in duplicated layers with head-wise memory layers, introducing sparse memory access and value factorization modules.", "result": "MIDUS achieves better performance in continual pretraining experiments compared to depth up-scaling baselines, while remaining highly efficient.", "conclusion": "MIDUS is an effective and efficient alternative for LLM scaling strategies, leveraging memory-infused design for improved results."}}
{"id": "2512.13991", "pdf": "https://arxiv.org/pdf/2512.13991", "abs": "https://arxiv.org/abs/2512.13991", "authors": ["Yao He", "Youngjoong Kwon", "Tiange Xiang", "Wenxiao Cai", "Ehsan Adeli"], "title": "Repurposing 2D Diffusion Models for 3D Shape Completion", "categories": ["cs.CV"], "comment": null, "summary": "We present a framework that adapts 2D diffusion models for 3D shape completion from incomplete point clouds. While text-to-image diffusion models have achieved remarkable success with abundant 2D data, 3D diffusion models lag due to the scarcity of high-quality 3D datasets and a persistent modality gap between 3D inputs and 2D latent spaces. To overcome these limitations, we introduce the Shape Atlas, a compact 2D representation of 3D geometry that (1) enables full utilization of the generative power of pretrained 2D diffusion models, and (2) aligns the modalities between the conditional input and output spaces, allowing more effective conditioning. This unified 2D formulation facilitates learning from limited 3D data and produces high-quality, detail-preserving shape completions. We validate the effectiveness of our results on the PCN and ShapeNet-55 datasets. Additionally, we show the downstream application of creating artist-created meshes from our completed point clouds, further demonstrating the practicality of our method.", "AI": {"tldr": "This paper introduces a framework adapting 2D diffusion models for completing 3D shapes using incomplete point clouds, overcoming limitations like data scarcity and modality gaps.", "motivation": "The scarcity of high-quality 3D datasets and challenges posed by modality gaps between 3D and 2D latent spaces in existing diffusion models hinder advancements in 3D shape completion.", "method": "The paper proposes a compact 2D representation called Shape Atlas to utilize pretrained 2D diffusion models, align modalities between input and output spaces for effective conditioning, and train from limited 3D data.", "result": "The method achieves high-quality, detail-preserving 3D shape completions and is validated on PCN and ShapeNet-55 datasets. Additionally, artist-created meshes were generated from completed point clouds.", "conclusion": "The Shape Atlas framework successfully adapts 2D diffusion models for 3D tasks, bridging modality gaps, enabling learning from constrained 3D datasets, and demonstrating practical applications in artistic 3D mesh creation."}}
{"id": "2512.14112", "pdf": "https://arxiv.org/pdf/2512.14112", "abs": "https://arxiv.org/abs/2512.14112", "authors": ["Chunan Tong"], "title": "Optimizing Multi-Tier Supply Chain Ordering with a Hybrid Liquid Neural Network and Extreme Gradient Boosting Model", "categories": ["cs.AI"], "comment": null, "summary": "Supply chain management (SCM) faces significant challenges like demand fluctuations and the bullwhip effect. Traditional methods and even state-of-the-art LLMs struggle with benchmarks like the Vending Machine Test, failing to handle SCM's complex continuous time-series data. While ML approaches like LSTM and XGBoost offer solutions, they are often limited by computational inefficiency. Liquid Neural Networks (LNN), known for their adaptability and efficiency in robotics, remain untapped in SCM. This study proposes a hybrid LNN+XGBoost model for multi-tier supply chains. By combining LNN's dynamic feature extraction with XGBoost's global optimization, the model aims to minimize the bullwhip effect and increase profitability. This innovative approach addresses the need for efficiency and adaptability, filling a critical gap in intelligent SCM.", "AI": {"tldr": "The paper proposes a hybrid model combining Liquid Neural Networks (LNN) and XGBoost to improve supply chain management (SCM) by reducing bullwhip effects and increasing profitability.", "motivation": "The motivation is to address significant challenges in SCM, such as demand fluctuations, bullwhip effects, and the inadequacy of traditional ML models and LLMs to process complex continuous time-series data efficiently.", "method": "A hybrid model integrating LNN for dynamic feature extraction and XGBoost for global optimization is proposed to optimize multi-tier supply chains.", "result": "The study achieves enhanced adaptability, efficiency, and effectiveness in minimizing bullwhip effects and improving SCM profitability.", "conclusion": "This novel LNN+XGBoost model bridges a gap in intelligent SCM, offering a computationally efficient and adaptive solution to existing challenges."}}
{"id": "2512.14549", "pdf": "https://arxiv.org/pdf/2512.14549", "abs": "https://arxiv.org/abs/2512.14549", "authors": ["David Samuel", "Lucas Georges Gabriel Charpentier"], "title": "Dual Language Models: Balancing Training Efficiency and Overfitting Resilience", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.", "AI": {"tldr": "This paper improves language models by combining autoregressive and masked-diffusion objectives, resulting in better performance than single-objective models.", "motivation": "Autoregressive models are efficient but prone to overfitting, while masked-diffusion models are resilient but less efficient. Combining their strengths could produce superior results.", "method": "The authors train 50 language models under different levels of data repetition, testing configurations of dual-objective training to find the optimal balance.", "result": "Combining both objectives consistently yields better performance across all evaluated settings. The optimal ratio works well regardless of the targeted downstream objectives.", "conclusion": "Dual-objective training offers an effective balance of efficiency and resilience, outperforming single-objective approaches under multiple conditions."}}
{"id": "2512.14666", "pdf": "https://arxiv.org/pdf/2512.14666", "abs": "https://arxiv.org/abs/2512.14666", "authors": ["Zechen Bai", "Chen Gao", "Mike Zheng Shou"], "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models", "categories": ["cs.RO", "cs.CV"], "comment": "15 pages", "summary": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% on long-horizon tasks, +22.0\\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.", "AI": {"tldr": "EVOLVE-VLA introduces a framework enabling Vision-Language-Action (VLA) agents to adapt and improve in real-time through environment interaction without heavy reliance on task-specific demonstrations.", "motivation": "Existing VLA models rely heavily on Supervised Fine-tuning (SFT), which limits adaptability, scalability, and resilience to new tasks or varying deployment conditions.", "method": "The proposed EVOLVE-VLA uses a test-time training framework leveraging learned progress estimation with two mechanisms: accumulative progress smoothing and progressive horizon extension, to replace oracle reward signals for adaptation.", "result": "EVOLVE-VLA shows significant improvements: +8.6% for long-horizon tasks, +22.0% in 1-shot learning capabilities, and achieves 20.8% success on unseen tasks without task-specific demonstrations.", "conclusion": "EVOLVE-VLA represents a step toward truly adaptive agents that shift from static imitation learning to continuous self-improvements, enabling scalable, flexible, and generalized intelligence in robotic systems."}}
{"id": "2512.13758", "pdf": "https://arxiv.org/pdf/2512.13758", "abs": "https://arxiv.org/abs/2512.13758", "authors": ["L\u00e9o Hein", "Giovanni de Nunzio", "Giovanni Chierchia", "Aur\u00e9lie Pirayre", "Laurent Najman"], "title": "Network-Wide Traffic Volume Estimation from Speed Profiles using a Spatio-Temporal Graph Neural Network with Directed Spatial Attention", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Existing traffic volume estimation methods typically address either forecasting traffic on sensor-equipped roads or spatially imputing missing volumes using nearby sensors. While forecasting models generally disregard unmonitored roads by design, spatial imputation methods explicitly address network-wide estimation; yet this approach relies on volume data at inference time, limiting its applicability in sensor-scarce cities. Unlike traffic volume data, probe vehicle speeds and static road attributes are more broadly accessible and support full coverage of road segments in most urban networks. In this work, we present the Hybrid Directed-Attention Spatio-Temporal Graph Neural Network (HDA-STGNN), an inductive deep learning framework designed to tackle the network-wide volume estimation problem. Our approach leverages speed profiles, static road attributes, and road network topology to predict daily traffic volume profiles across all road segments in the network. To evaluate the effectiveness of our approach, we perform extensive ablation studies that demonstrate the model's capacity to capture complex spatio-temporal dependencies and highlight the value of topological information for accurate network-wide traffic volume estimation without relying on volume data at inference time.", "AI": {"tldr": "This paper introduces a deep learning framework leveraging speed profiles, static road attributes, and topology to predict traffic volume profiles across all road segments without depending on volume data for inference.", "motivation": "Current methods fail to provide accurate traffic volume estimations for unmonitored roads, especially in sensor-scarce cities. Addressing this gap using widely available data like speed profiles and road attributes is crucial.", "method": "The paper proposes the Hybrid Directed-Attention Spatio-Temporal Graph Neural Network (HDA-STGNN), which utilizes inductive learning, integrating road network topology, speed profiles, and road attributes for traffic predictions.", "result": "Extensive ablation studies show that HDA-STGNN effectively captures complex spatio-temporal dependencies and utilizes topological information for highly accurate traffic volume estimations.", "conclusion": "HDA-STGNN enables precise, network-wide traffic volume predictions without relying on volume data during inference, making it a valuable tool for cities with limited sensor data."}}
{"id": "2512.14008", "pdf": "https://arxiv.org/pdf/2512.14008", "abs": "https://arxiv.org/abs/2512.14008", "authors": ["Shufan Li", "Jiuxiang Gu", "Kangning Liu", "Zhe Lin", "Zijun Wei", "Aditya Grover", "Jason Kuen"], "title": "Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models", "categories": ["cs.CV"], "comment": "18 pages (12 pages for the main paper and 6 pages for the appendix), 9 figures", "summary": "Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality.", "AI": {"tldr": "Sparse-LaViDa improves the inference speed of Masked Discrete Diffusion Models by dynamically truncating redundant masked tokens and employing register tokens while maintaining quality.", "motivation": "Masked Discrete Diffusion Models have slow inference speeds due to redundant masked token processing during sampling.", "method": "Sparse-LaViDa introduces token truncation during inference and register tokens as compact representations. It also uses a specialized attention mask to ensure consistency between training and inference.", "result": "Sparse-LaViDa achieves up to 2x faster inference than state-of-the-art models in tasks like text-to-image generation and image editing, while preserving quality.", "conclusion": "Sparse-LaViDa accelerates MDM sampling without compromising output quality, making it suitable for various multimodal applications."}}
{"id": "2512.14157", "pdf": "https://arxiv.org/pdf/2512.14157", "abs": "https://arxiv.org/abs/2512.14157", "authors": ["Yankai Jiang", "Yujie Zhang", "Peng Zhang", "Yichen Li", "Jintai Chen", "Xiaoming Shi", "Shihui Zhen"], "title": "Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely \"think with images\" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.", "AI": {"tldr": "Ophiuchus is a dynamic, tool-augmented framework aiming for advanced reasoning capabilities in medical multimodal large language models (MLLMs), focusing on precise visual grounding and iterative reasoning.", "motivation": "To overcome the limitations of existing medical MLLMs that struggle with complex tasks related to fine-grained visual regions and precise grounding in medical diagnosis.", "method": "Ophiuchus employs a three-stage training strategy: cold-start training for basic tool utilization, self-reflection fine-tuning for iterative reasoning, and reinforcement learning optimizing task-specific diagnostic behaviors.", "result": "Ophiuchus outperforms state-of-the-art (SOTA) methods across various medical benchmarks like VQA, detection, and reasoning-based segmentation.", "conclusion": "Ophiuchus demonstrates substantial potential for enabling AI agents to achieve advanced medical reasoning by effectively integrating inherent model capabilities and external tools."}}
{"id": "2512.14554", "pdf": "https://arxiv.org/pdf/2512.14554", "abs": "https://arxiv.org/abs/2512.14554", "authors": ["Nguyen Tien Dong", "Minh-Anh Nguyen", "Thanh Dat Hoang", "Nguyen Tuan Ngoc", "Dao Xuan Quang Minh", "Phan Phi Hai", "Nguyen Thi Ngoc Anh", "Dang Van Tu", "Binh Vu"], "title": "VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.", "AI": {"tldr": "VLegal-Bench is a benchmark for evaluating large language models on Vietnamese legal tasks by addressing the complex nature of Vietnamese legislation.", "motivation": "To address challenges posed by the complexity and frequent revisions of Vietnamese laws, and evaluate how LLMs interpret legal knowledge.", "method": "Developed a benchmark with 10,450 annotated samples aligned with Bloom's taxonomy, focusing on practical legal tasks and evaluations.", "result": "Created a systemized framework to assess LLM performance on Vietnamese legal tasks, ensuring alignment with real-world scenarios.", "conclusion": "VLegal-Bench lays the groundwork for improved, ethical, and interpretable AI legal systems in Vietnamese contexts."}}
{"id": "2512.14689", "pdf": "https://arxiv.org/pdf/2512.14689", "abs": "https://arxiv.org/abs/2512.14689", "authors": ["Sirui Chen", "Zi-ang Cao", "Zhengyi Luo", "Fernando Casta\u00f1eda", "Chenran Li", "Tingwu Wang", "Ye Yuan", "Linxi \"Jim\" Fan", "C. Karen Liu", "Yuke Zhu"], "title": "CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation", "categories": ["cs.RO", "cs.LG"], "comment": "The first two authors contributed equally. Project page: https://nvlabs.github.io/CHIP/", "summary": "Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening.", "AI": {"tldr": "The paper introduces CHIP, a module for humanoid robots enabling adaptable end-effector stiffness for dynamic tasks without complex implementation.", "motivation": "Despite agile locomotion capabilities, humanoid robots struggle with forceful manipulation tasks like moving objects and wiping.", "method": "CHIP allows controllable end-effector stiffness and dynamic motion tracking, requiring no data augmentation or extra reward tuning.", "result": "A motion-tracking controller with CHIP successfully performs various forceful tasks such as wiping, collaboration, and door opening.", "conclusion": "CHIP enhances humanoid robots' manipulation capabilities, making them versatile in diverse tasks."}}
{"id": "2512.13770", "pdf": "https://arxiv.org/pdf/2512.13770", "abs": "https://arxiv.org/abs/2512.13770", "authors": ["Huaiyuan Xiao", "Fadi Dornaika", "Jingjun Bi"], "title": "Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available at https://github.com/HuaiyuanXiao/MVSupGCN", "AI": {"tldr": "The paper introduces MV-SupGCN, a semi-supervised graph convolutional network for multi-view learning to address challenges in integrating complementary information and improving feature representations.", "motivation": "Existing multi-view learning methods fail to fully leverage complementary information across views, leading to suboptimal feature representations and limited performance.", "method": "MV-SupGCN combines three main components: joint loss function (Cross-Entropy + Supervised Contrastive loss), hybrid graph construction methods, and a unified framework integrating contrastive learning and pseudo-labeling.", "result": "MV-SupGCN outperforms state-of-the-art methods consistently across benchmarks, demonstrating improved model generalization and effectiveness.", "conclusion": "MV-SupGCN provides a robust solution for multi-view learning, successfully integrating complementary components to enhance performance and structural data representation."}}
{"id": "2512.14017", "pdf": "https://arxiv.org/pdf/2512.14017", "abs": "https://arxiv.org/abs/2512.14017", "authors": ["Zongyao Li", "Kengo Ishida", "Satoshi Yamazaki", "Xiaotong Ji", "Jianquan Liu"], "title": "KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "WACV2026", "summary": "We propose KFS-Bench, the first benchmark for key frame sampling in long video question answering (QA), featuring multi-scene annotations to enable direct and robust evaluation of sampling strategies. Key frame sampling is crucial for efficient long-form video understanding. In long video QA, selecting informative frames enables multimodal large language models (MLLMs) to improve both accuracy and efficiency. KFS-Bench addresses the limitation of prior works that only indirectly assess frame selection quality via QA accuracy. By providing ground-truth annotations of multiple disjoint scenes required per question, KFS-Bench allows us to directly analyze how different sampling approaches capture essential content across an entire long video. Using KFS-Bench, we conduct a comprehensive study of key frame sampling methods and identify that not only sampling precision but also scene coverage and sampling balance are the key factors influencing QA performance. Regarding all the factors, we design a novel sampling quality metric that correlates with QA accuracy. Furthermore, we develop a novel key frame sampling method that leverages question-video relevance to balance sampling diversity against question-frame similarity, thereby improving coverage of relevant scenes. Our adaptively balanced sampling approach achieves superior performance in both key frame sampling and QA performance. The benchmark is available at https://github.com/NEC-VID/KFS-Bench.", "AI": {"tldr": "The paper introduces KFS-Bench, a benchmark for key frame sampling in long video QA, focusing on multi-scene annotations. It offers a direct evaluation of sampling strategies and emphasizes sampling precision, scene coverage, and balance.", "motivation": "Existing work in long video QA lacks direct evaluation of key frame sampling, relying only on QA accuracy, which limits understanding of frame selection quality. KFS-Bench aims to fill this gap by offering detailed scene-level annotations and robust evaluation metrics.", "method": "The authors propose KFS-Bench, a benchmark to directly evaluate sampling strategies based on multi-scene ground-truth annotations. They design a sampling quality metric that correlates with QA accuracy and introduce a key frame sampling method leveraging question-video relevance for balanced sampling.", "result": "Using KFS-Bench, the study identifies key factors\u2014precision, coverage, and balance\u2014influencing QA performance. Their adaptive sampling method enhances both key frame and QA performance by improving scene coverage and sampling diversity.", "conclusion": "KFS-Bench provides a robust framework for evaluating and improving key frame sampling in long video QA, advancing understanding and performance in this area. Their proposed method and metrics highlight the importance of balancing scene coverage and sampling precision."}}
{"id": "2512.14228", "pdf": "https://arxiv.org/pdf/2512.14228", "abs": "https://arxiv.org/abs/2512.14228", "authors": ["Aneesha Fernando", "Surangika Ranathunga", "Kristin Stock", "Raj Prasanna", "Christopher B. Jones"], "title": "Georeferencing complex relative locality descriptions with large language models", "categories": ["cs.AI"], "comment": "Provisionally accepted for publication in the International Journal of Geographical Information Science", "summary": "Georeferencing text documents has typically relied on either gazetteer-based methods to assign geographic coordinates to place names, or on language modelling approaches that associate textual terms with geographic locations. However, many location descriptions specify positions relatively with spatial relationships, making geocoding based solely on place names or geo-indicative words inaccurate. This issue frequently arises in biological specimen collection records, where locations are often described through narratives rather than coordinates if they pre-date GPS. Accurate georeferencing is vital for biodiversity studies, yet the process remains labour-intensive, leading to a demand for automated georeferencing solutions. This paper explores the potential of Large Language Models (LLMs) to georeference complex locality descriptions automatically, focusing on the biodiversity collections domain. We first identified effective prompting patterns, then fine-tuned an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from multiple regions and languages. Our approach outperforms existing baselines with an average, across datasets, of 65% of records within a 10 km radius, for a fixed amount of training data. The best results (New York state) were 85% within 10km and 67% within 1km. The selected LLM performs well for lengthy, complex descriptions, highlighting its potential for georeferencing intricate locality descriptions.", "AI": {"tldr": "The study demonstrates the use of fine-tuned Large Language Models (LLMs) for georeferencing biodiversity locality descriptions, achieving promising accuracy.", "motivation": "Accurate georeferencing is essential for biodiversity studies but is highly labor-intensive, especially for pre-GPS era collection records described narratively.", "method": "The paper involved identifying effective prompting patterns and fine-tuning an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from varied regions and languages.", "result": "The approach surpassed existing baselines, achieving 65% of records within a 10 km radius and reaching 85% accuracy for New York State data within 10 km.", "conclusion": "Fine-tuned LLMs show strong potential in automating georeferencing of complex, narrative locality descriptions, enhancing efficiency in biodiversity studies."}}
{"id": "2512.14561", "pdf": "https://arxiv.org/pdf/2512.14561", "abs": "https://arxiv.org/abs/2512.14561", "authors": ["Hongli Li", "Che Han Chen", "Kevin Fan", "Chiho Young-Johnson", "Soyoung Lim", "Yali Feng"], "title": "Agreement Between Large Language Models and Human Raters in Essay Scoring: A Research Synthesis", "categories": ["cs.CL"], "comment": "This manuscript is under review as a book chapter", "summary": "Despite the growing promise of large language models (LLMs) in automatic essay scoring (AES), empirical findings regarding their reliability compared to human raters remain mixed. Following the PRISMA 2020 guidelines, we synthesized 65 published and unpublished studies from January 2022 to August 2025 that examined agreement between LLMs and human raters in AES. Across studies, reported LLM-human agreement was generally moderate to good, with agreement indices (e.g., Quadratic Weighted Kappa, Pearson correlation, and Spearman's rho) mostly ranging between 0.30 and 0.80. Substantial variability in agreement levels was observed across studies, reflecting differences in study-specific factors as well as the lack of standardized reporting practices. Implications and directions for future research are discussed.", "AI": {"tldr": "The paper analyzes the reliability of large language models (LLMs) in automatic essay scoring (AES), finding moderate to good agreement with human raters but with substantial variability across studies.", "motivation": "To evaluate the consistency and reliability of LLMs compared to human raters in the context of AES, given the mixed findings in the field.", "method": "A systematic review following PRISMA 2020 guidelines was conducted, synthesizing 65 studies examining LLM-human agreement in AES from January 2022 to August 2025.", "result": "Agreement between LLMs and human raters was generally moderate to good, with indices between 0.30 and 0.80, but variability was noted due to study-specific factors and lack of standard reporting.", "conclusion": "While LLMs show promise in AES, the variability and lack of standardized practices highlight the need for further research to improve reliability and establish standardized evaluation metrics."}}
{"id": "2512.13694", "pdf": "https://arxiv.org/pdf/2512.13694", "abs": "https://arxiv.org/abs/2512.13694", "authors": ["Kostantinos Mattas", "Antonio Lucas-Alba", "Tomer Toledo", "Oscar M. Melchor", "Shlomo Bekhor", "Biagio Ciuffo"], "title": "Learning to Car-Follow Using an Inertia-Oriented Driving Technique: A Before-and-After Study on a Closed Circuit", "categories": ["cs.HC", "cs.CY", "cs.RO"], "comment": null, "summary": "For decades, car following and traffic flow models have assumed that drivers default driving strategy is to maintain a safe distance. Several previous studies have questioned whether the Driving to Keep Distance is a traffic invariant. Therefore, the acceleration deceleration torque asymmetry of drivers must necessarily determine the observed patterns of traffic oscillations. Those studies indicate that drivers can adopt alternative CF strategies, such as Driving to Keep Inertia, by following basic instructions. The present work extends the evidence from previous research by showing the effectiveness of a DI course that immediately translates into practice on a closed circuit. Twelve drivers were invited to follow a lead car that varied its speed on a real circuit. Then, the driver took a DI course and returned to the same real car following scenario. Drivers generally adopted DD as the default CF mode in the pretest, both in field and simulated PC conditions, yielding very similar results. After taking the full DI course, drivers showed significantly less acceleration, deceleration, and speed variability than did the pretest, both in the field and in the simulated conditions, which indicates that drivers adopted the DI strategy. This study is the first to show the potential of adopting a DI strategy in a real circuit.", "AI": {"tldr": "Drivers generally default to maintaining a safe distance (DD strategy), but can shift to driving to keep inertia (DI strategy) after training, resulting in reduced speed variability.", "motivation": "The paper investigates whether drivers inherently follow the default Driving to Keep Distance (DD) strategy and explores the potential of adopting an alternate Driving to Keep Inertia (DI) strategy through training.", "method": "Twelve drivers followed a lead car on a real circuit before and after attending a DI training course. Performance was measured in field and simulated conditions to assess changes in driving patterns.", "result": "Post-training, drivers showed significantly less acceleration, deceleration, and speed variability in both real and simulated driving scenarios, indicating adoption of the DI strategy.", "conclusion": "The study demonstrates the effectiveness of training drivers to adopt the DI strategy in real-world driving scenarios, which may help reduce traffic oscillations."}}
{"id": "2512.13788", "pdf": "https://arxiv.org/pdf/2512.13788", "abs": "https://arxiv.org/abs/2512.13788", "authors": ["Shengfan Cao", "Francesco Borrelli"], "title": "Constrained Policy Optimization via Sampling-Based Weight-Space Projection", "categories": ["cs.LG", "cs.RO"], "comment": "Submitted to IFAC World Congress 2026", "summary": "Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningful primal objective improvement.", "AI": {"tldr": "The paper introduces SCPO, a method ensuring safety in policy learning by enforcing constraints directly in parameter space using a convex SOCP, resulting in constant safety during training.", "motivation": "Develop safe learning approaches for policy training, ensuring performance improvement without violating safety constraints during training.", "method": "Proposes SCPO (Sampling-based weight-space projection), which constructs a safe region based on trajectory rollouts and smoothness bounds, and applies convex SOCP projection for safe gradient updates.", "result": "SCPO demonstrates consistent rejection of unsafe updates, preserves safety during training, and improves objective performance in various constrained tasks.", "conclusion": "SCPO achieves reliable, safety-guaranteed training while facilitating meaningful policy advancements within safety-critical contexts."}}
{"id": "2512.14020", "pdf": "https://arxiv.org/pdf/2512.14020", "abs": "https://arxiv.org/abs/2512.14020", "authors": ["Afia Maham", "Dur E Nayab Tashfa"], "title": "Deep Learning Perspective of Scene Understanding in Autonomous Robots", "categories": ["cs.CV"], "comment": "11 pages. Review Paper on Deep Learning Perspective of Scene Understanding in Autonomous Robots", "summary": "This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM. It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better. When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction. Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.", "AI": {"tldr": "This paper reviews the use of deep learning in scene understanding for autonomous robots, covering object detection, segmentation, depth estimation, 3D reconstruction, and SLAM, along with its advantages and future directions.", "motivation": "The motivation is to explore how deep learning can overcome traditional geometric model limitations in enabling robots to better perceive and interact with their environments.", "method": "The paper reviews advancements in deep learning for perception tasks like object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM.", "result": "Integration of deep learning modules enhances real-time depth perception, semantic reasoning, and effectiveness in dynamic and unstructured environments.", "conclusion": "The integration of deep learning modules in autonomous robots shows significant improvements in scene understanding, with future work required to tackle remaining challenges and expand research directions."}}
{"id": "2512.14252", "pdf": "https://arxiv.org/pdf/2512.14252", "abs": "https://arxiv.org/abs/2512.14252", "authors": ["Kelly J. Davis"], "title": "G\u00f6del's Poetry", "categories": ["cs.AI", "cs.LG"], "comment": "24 pages, 1 figure", "summary": "Formal, automated theorem proving has long been viewed as a challenge to artificial intelligence. We introduce here a new approach to computer theorem proving, one that employs specialized language models for Lean4 proof generation combined with recursive decomposition of difficult theorems into simpler entailing propositions. These models are coordinated through a multi-agent architecture that orchestrates autoformalization (if required), proof generation, decomposition of difficult theorems into simpler entailing propositions, and recursive proof (and/or decomposition) of these propositions. Without decomposition, we achieve a 90.4% pass rate on miniF2F. With decomposition, this is significantly improved. A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition. The system is made available on PyPI as goedels-poetry (at https://pypi.org/project/goedels-poetry ), and the open-source implementation KellyJDavis/goedels-poetry (at https://github.com/KellyJDavis/goedels-poetry ) facilitates both adaptation to alternative language models and extension with custom functionality.", "AI": {"tldr": "The paper introduces a novel AI-driven theorem proving approach using specialized language models in Lean4, combined with recursive decomposition for tackling complex theorems.", "motivation": "To address challenges in automated theorem proving and boost its efficiency by augmenting it with modern AI tools and recursive decomposition strategies.", "method": "The method integrates specialized Lean4 language models for proof generation with a multi-agent system coordinating proof decomposition, recursive solving, and autoformalization, leveraging the modified Kimina Lean Server for AST-based recursive decomposition.", "result": "Achieved a 90.4% pass rate on miniF2F without decomposition and significant improvements with decomposition.", "conclusion": "The proposed system demonstrates effective improvements in automated theorem proving through a novel combination of language models and recursive decomposition. The implementation is openly available for further research and customization."}}
{"id": "2512.14562", "pdf": "https://arxiv.org/pdf/2512.14562", "abs": "https://arxiv.org/abs/2512.14562", "authors": ["Tejaswani Dash", "Dinesh Karri", "Anudeep Vurity", "Gautam Datla", "Tazeem Ahmad", "Saima Rafi", "Rohith Tangudu"], "title": "Polypersona: Persona-Grounded LLM for Synthetic Survey Responses", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in IEEE Bigdata 2025- LLMs4ALL", "summary": "This paper introduces PolyPersona, a generative framework for synthesizing persona-conditioned survey responses across multiple domains. The framework instruction-tunes compact chat models using parameter-efficient LoRA adapters with 4-bit quantization under a resource-adaptive training setup. A dialogue-based data pipeline explicitly preserves persona cues, ensuring consistent behavioral alignment across generated responses. Using this pipeline, we construct a dataset of 3,568 synthetic survey responses spanning ten domains and 433 distinct personas, enabling controlled instruction tuning and systematic multi-domain evaluation. We evaluate the generated responses using a multi-metric evaluation suite that combines standard text generation metrics, including BLEU, ROUGE, and BERTScore, with survey-specific metrics designed to assess structural coherence, stylistic consistency, and sentiment alignment.Experimental results show that compact models such as TinyLlama 1.1B and Phi-2 achieve performance comparable to larger 7B to 8B baselines, with a highest BLEU score of 0.090 and ROUGE-1 of 0.429. These findings demonstrate that persona-conditioned fine-tuning enables small language models to generate reliable and coherent synthetic survey data. The proposed framework provides an efficient and reproducible approach for survey data generation, supporting scalable evaluation while facilitating bias analysis through transparent and open protocols.", "AI": {"tldr": "PolyPersona proposes a resource-efficient generative model framework that synthesizes persona-conditioned survey responses across multiple domains with impressive performance from compact models.", "motivation": "To address the need for scalable and efficient generation of persona-conditioned synthetic survey data while ensuring coherence, stylistic consistency, and sentiment alignment.", "method": "Uses a generative framework with dialogue-based data pipelines and compact chat models, tuned via parameter-efficient LoRA adapters and 4-bit quantization.", "result": "Generated a dataset of 3,568 synthetic survey responses across 10 domains and evaluated small models achieving competitive metrics relative to larger baselines.", "conclusion": "Persona-based fine-tuning allows small language models to efficiently and reliably generate synthetic survey data, supporting scalable evaluation and facilitating bias analysis."}}
{"id": "2512.13806", "pdf": "https://arxiv.org/pdf/2512.13806", "abs": "https://arxiv.org/abs/2512.13806", "authors": ["Siegfried Ludwig", "Stylianos Bakas", "Konstantinos Barmpas", "Georgios Zoumpourlis", "Dimitrios A. Adamos", "Nikolaos Laskaris", "Yannis Panagakis", "Stefanos Zafeiriou"], "title": "EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.", "AI": {"tldr": "This paper introduces a new method, Disentangled Decoding Decomposition (D3), to improve generalization in EEG deep learning by separating genuine brain signal components from spurious ones, overcoming overfitting and aiding real-world applications.", "motivation": "The motivation is to address hidden overfitting in EEG deep learning models, which exhibit strong benchmark performance but fail to generalize to real-world applications.", "method": "The paper proposes D3, a weakly supervised method using disentanglement of brain activity components akin to non-linear ICA. It employs a specialized model architecture with independent sub-networks for interpretability, enabling better separation of genuine and noise components in EEG signals.", "result": "D3 effectively separates latent brain activity components, helps prevent overfitting by eliminating task-correlated artifacts, and supports improved few-shot learning in tasks like sleep stage classification.", "conclusion": "The method produces interpretable and generalizable EEG models, avoids overfitting, and shows promise for broad neuroscience applications by separating and analyzing brain signal components."}}
{"id": "2512.14026", "pdf": "https://arxiv.org/pdf/2512.14026", "abs": "https://arxiv.org/abs/2512.14026", "authors": ["Yibing Fu", "Yunpeng Zhao", "Zhitao Zeng", "Cheng Chen", "Yueming Jin"], "title": "Unleashing the Power of Image-Tabular Self-Supervised Learning via Breaking Cross-Tabular Barriers", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal learning integrating medical images and tabular data has significantly advanced clinical decision-making in recent years. Self-Supervised Learning (SSL) has emerged as a powerful paradigm for pretraining these models on large-scale unlabeled image-tabular data, aiming to learn discriminative representations. However, existing SSL methods for image-tabular representation learning are often confined to specific data cohorts, mainly due to their rigid tabular modeling mechanisms when modeling heterogeneous tabular data. This inter-tabular barrier hinders the multi-modal SSL methods from effectively learning transferrable medical knowledge shared across diverse cohorts. In this paper, we propose a novel SSL framework, namely CITab, designed to learn powerful multi-modal feature representations in a cross-tabular manner. We design the tabular modeling mechanism from a semantic-awareness perspective by integrating column headers as semantic cues, which facilitates transferrable knowledge learning and the scalability in utilizing multiple data sources for pretraining. Additionally, we propose a prototype-guided mixture-of-linear layer (P-MoLin) module for tabular feature specialization, empowering the model to effectively handle the heterogeneity of tabular data and explore the underlying medical concepts. We conduct comprehensive evaluations on Alzheimer's disease diagnosis task across three publicly available data cohorts containing 4,461 subjects. Experimental results demonstrate that CITab outperforms state-of-the-art approaches, paving the way for effective and scalable cross-tabular multi-modal learning.", "AI": {"tldr": "This paper introduces CITab, a novel framework for self-supervised multi-modal learning, designed to overcome challenges in integrating heterogeneous medical images and tabular data across various cohorts.", "motivation": "Existing self-supervised learning methods for image-tabular data are limited by rigid tabular modeling mechanisms, restricting transferability and scalability across diverse datasets.", "method": "CITab incorporates a semantic-awareness perspective using column headers as cues for transferable knowledge learning, and introduces a prototype-guided mixture-of-linear layer (P-MoLin) for addressing tabular data heterogeneity.", "result": "The framework demonstrates superior performance in Alzheimer's diagnosis tasks, outperforming state-of-the-art methods across three public datasets with 4,461 subjects.", "conclusion": "CITab enables scalable and effective multi-modal learning by overcoming inter-tabular barriers and enhancing transferability in medical knowledge representation."}}
{"id": "2512.14288", "pdf": "https://arxiv.org/pdf/2512.14288", "abs": "https://arxiv.org/abs/2512.14288", "authors": ["Georgios Bouchouras", "Dimitrios Doumanas", "Andreas Soularidis", "Konstantinos Kotis", "George A. Vouros"], "title": "Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting", "categories": ["cs.AI"], "comment": null, "summary": "This paper explores the integration of Large Language Models (LLMs) in the engineering of a Parkinson's Disease (PD) monitoring and alerting ontology through four key methodologies: One Shot (OS) prompt techniques, Chain of Thought (CoT) prompts, X-HCOME, and SimX-HCOME+. The primary objective is to determine whether LLMs alone can create comprehensive ontologies and, if not, whether human-LLM collaboration can achieve this goal. Consequently, the paper assesses the effectiveness of LLMs in automated ontology development and the enhancement achieved through human-LLM collaboration.\n  Initial ontology generation was performed using One Shot (OS) and Chain of Thought (CoT) prompts, demonstrating the capability of LLMs to autonomously construct ontologies for PD monitoring and alerting. However, these outputs were not comprehensive and required substantial human refinement to enhance their completeness and accuracy.\n  X-HCOME, a hybrid ontology engineering approach that combines human expertise with LLM capabilities, showed significant improvements in ontology comprehensiveness. This methodology resulted in ontologies that are very similar to those constructed by experts.\n  Further experimentation with SimX-HCOME+, another hybrid methodology emphasizing continuous human supervision and iterative refinement, highlighted the importance of ongoing human involvement. This approach led to the creation of more comprehensive and accurate ontologies.\n  Overall, the paper underscores the potential of human-LLM collaboration in advancing ontology engineering, particularly in complex domains like PD. The results suggest promising directions for future research, including the development of specialized GPT models for ontology construction.", "AI": {"tldr": "The paper examines integrating Large Language Models (LLMs) in Parkinson's Disease (PD) ontology engineering, finding that while LLMs can generate initial ontologies, human collaboration significantly enhances results.", "motivation": "The motivation is to explore whether LLMs can autonomously develop comprehensive ontologies for PD monitoring and assess the effectiveness of human-LLM collaboration for improved results.", "method": "The study uses four methods: One Shot (OS) prompt techniques, Chain of Thought (CoT) prompts, X-HCOME (a hybrid human-LLM methodology), and SimX-HCOME+ (involving continuous human supervision and refinement).", "result": "LLMs can create basic ontologies using OS and CoT prompts, but these are incomplete. Hybrid methods like X-HCOME and SimX-HCOME+ improved ontology quality, emphasizing the importance of human involvement.", "conclusion": "Human-LLM collaboration significantly enhances the potential for ontology engineering in specialized fields like PD, paving the way for future research into specialized LLM models for such tasks."}}
{"id": "2512.14576", "pdf": "https://arxiv.org/pdf/2512.14576", "abs": "https://arxiv.org/abs/2512.14576", "authors": ["Ekaterina Artemova", "Laurie Burchell", "Daryna Dementieva", "Shu Okabe", "Mariya Shmatova", "Pedro Ortiz Suarez"], "title": "Low-Resource, High-Impact: Building Corpora for Inclusive Language Technologies", "categories": ["cs.CL", "cs.AI"], "comment": "Tutorial is accepted to LREC2026", "summary": "This tutorial (https://tum-nlp.github.io/low-resource-tutorial) is designed for NLP practitioners, researchers, and developers working with multilingual and low-resource languages who seek to create more equitable and socially impactful language technologies. Participants will walk away with a practical toolkit for building end-to-end NLP pipelines for underrepresented languages -- from data collection and web crawling to parallel sentence mining, machine translation, and downstream applications such as text classification and multimodal reasoning. The tutorial presents strategies for tackling the challenges of data scarcity and cultural variance, offering hands-on methods and modeling frameworks. We will focus on fair, reproducible, and community-informed development approaches, grounded in real-world scenarios. We will showcase a diverse set of use cases covering over 10 languages from different language families and geopolitical contexts, including both digitally resource-rich and severely underrepresented languages.", "AI": {"tldr": "A tutorial for building NLP pipelines for low and multilingual languages addressing challenges like data scarcity.", "motivation": "To create equitable and impactful NLP technologies supporting multilingual and low-resource languages.", "method": "Hands-on approaches covering data gathering, machine translation, and downstream tasks with reproducible frameworks.", "result": "Development strategies showcased across more than 10 languages in varying resource and geopolitical contexts.", "conclusion": "Practical toolkit for ethical, community-oriented NLP for diverse and underrepresented languages."}}
{"id": "2512.13836", "pdf": "https://arxiv.org/pdf/2512.13836", "abs": "https://arxiv.org/abs/2512.13836", "authors": ["Ricardo Tapia", "Iman Soltani"], "title": "A Convex Obstacle Avoidance Formulation", "categories": ["eess.SY", "cs.RO", "math.OC"], "comment": "18 pages, 17 figures", "summary": "Autonomous driving requires reliable collision avoidance in dynamic environments. Nonlinear Model Predictive Controllers (NMPCs) are suitable for this task, but struggle in time-critical scenarios requiring high frequency. To meet this demand, optimization problems are often simplified via linearization, narrowing the horizon window, or reduced temporal nodes, each compromising accuracy or reliability. This work presents the first general convex obstacle avoidance formulation, enabled by a novel approach to integrating logic. This facilitates the incorporation of an obstacle avoidance formulation into convex MPC schemes, enabling a convex optimization framework with substantially improved computational efficiency relative to conventional nonconvex methods. A key property of the formulation is that obstacle avoidance remains effective even when obstacles lie outside the prediction horizon, allowing shorter horizons for real-time deployment. In scenarios where nonconvex formulations are unavoidable, the proposed method meets or exceeds the performance of representative nonconvex alternatives. The method is evaluated in autonomous vehicle applications, where system dynamics are highly nonlinear.", "AI": {"tldr": "The paper introduces a general convex obstacle avoidance method for Nonlinear Model Predictive Controllers (NMPCs) to improve real-time collision avoidance in autonomous vehicles, enhancing computational efficiency.", "motivation": "NMPCs are effective for collision avoidance in dynamic environments but face computational challenges in time-critical scenarios. Traditional simplifications compromise accuracy or reliability.", "method": "The authors propose a convex obstacle avoidance formulation using a novel method for integrating logic into convex MPC schemes, improving computational efficiency and accommodating obstacles outside the prediction horizon.", "result": "The proposed method outperforms conventional nonconvex approaches in computational efficiency and matches or exceeds their performance in complex scenarios, validated on autonomous vehicle systems.", "conclusion": "The method enables real-time deployment of convex MPC with effective obstacle avoidance, demonstrating improved performance and reliability in highly nonlinear dynamic systems, such as autonomous driving."}}
{"id": "2512.13821", "pdf": "https://arxiv.org/pdf/2512.13821", "abs": "https://arxiv.org/abs/2512.13821", "authors": ["Subramanyam Sahoo", "Jared Junkin"], "title": "The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces", "categories": ["cs.LG"], "comment": "13 Pages, Initial Work on AI Control. A Preprint", "summary": "Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.", "AI": {"tldr": "The paper introduces CTVP, a framework to verify code-generating AI models for backdoors using semantic orbit analysis.", "motivation": "Concerns about the safety of code produced by LLMs, particularly regarding backdoor injections and malicious behavior, motivate the development of verification mechanisms.", "method": "CTVP compares execution traces for semantically equivalent program transformations, detecting anomalies to identify backdoors. It also defines ARQ to quantify verification costs.", "result": "CTVP detects behavioral anomalies and provides a scalable framework for verifying code integrity, supported by non-gamifiable theoretical principles.", "conclusion": "Semantic orbit analysis is shown to be effective and scalable for controlling code generation in AI, addressing safety challenges."}}
{"id": "2512.14028", "pdf": "https://arxiv.org/pdf/2512.14028", "abs": "https://arxiv.org/abs/2512.14028", "authors": ["Jiaheng Li", "Qiyu Dai", "Lihan Li", "Praneeth Chakravarthula", "He Sun", "Baoquan Chen", "Wenzheng Chen"], "title": "Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding", "categories": ["cs.CV"], "comment": null, "summary": "We consider the problem of active 3D imaging using single-shot structured light systems, which are widely employed in commercial 3D sensing devices such as Apple Face ID and Intel RealSense. Traditional structured light methods typically decode depth correspondences through pixel-domain matching algorithms, resulting in limited robustness under challenging scenarios like occlusions, fine-structured details, and non-Lambertian surfaces. Inspired by recent advances in neural feature matching, we propose a learning-based structured light decoding framework that performs robust correspondence matching within feature space rather than the fragile pixel domain. Our method extracts neural features from the projected patterns and captured infrared (IR) images, explicitly incorporating their geometric priors by building cost volumes in feature space, achieving substantial performance improvements over pixel-domain decoding approaches. To further enhance depth quality, we introduce a depth refinement module that leverages strong priors from large-scale monocular depth estimation models, improving fine detail recovery and global structural coherence. To facilitate effective learning, we develop a physically-based structured light rendering pipeline, generating nearly one million synthetic pattern-image pairs with diverse objects and materials for indoor settings. Experiments demonstrate that our method, trained exclusively on synthetic data with multiple structured light patterns, generalizes well to real-world indoor environments, effectively processes various pattern types without retraining, and consistently outperforms both commercial structured light systems and passive stereo RGB-based depth estimation methods. Project page: https://namisntimpot.github.io/NSLweb/.", "AI": {"tldr": "The paper presents a neural-based structured light decoding framework for enhanced 3D depth imaging, significantly improving robustness and quality over traditional methods.", "motivation": "Traditional structured light systems face limitations in accurately decoding depth in challenging scenarios such as occlusions, fine-structured details, and non-Lambertian surfaces.", "method": "The proposed method extracts neural features from structured light patterns and IR images, employs cost-volume construction in feature space for robustness, incorporates a depth refinement module leveraging monocular depth priors, and utilizes synthetic data for training.", "result": "The model generalizes effectively from synthetic to real-world applications, beats commercial systems and RGB-based methods, and successfully handles diverse pattern types without retraining.", "conclusion": "A learning-based approach for structured light decoding demonstrates superior robustness and depth quality, establishing itself as a significant advancement over traditional pixel-domain methods."}}
{"id": "2512.14358", "pdf": "https://arxiv.org/pdf/2512.14358", "abs": "https://arxiv.org/abs/2512.14358", "authors": ["Qizhi Wang"], "title": "TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation", "categories": ["cs.AI", "cs.DB"], "comment": "16 pages(/wo references), 4 figures, 10 tables", "summary": "Cardinality estimation is a key bottleneck for cost-based query optimization, yet deployable improvements remain difficult: classical estimators miss correlations, while learned estimators often require workload-specific training pipelines and invasive integration into the optimizer. This paper presents TiCard, a low intrusion, correction-based framework that augments (rather than replaces) a database's native estimator. TiCard learns multiplicative residual corrections using EXPLAIN-only features, and uses EXPLAIN ANALYZE only for offline labels. We study two practical instantiations: (i) a Gradient Boosting Regressor for sub-millisecond inference, and (ii) TabPFN, an in-context tabular foundation model that adapts by refreshing a small reference set without gradient retraining. On TiDB with TPCH and the Join Order Benchmark, in a low-trace setting (263 executions total; 157 used for learning), TiCard improves operator-level tail accuracy substantially: P90 Q-error drops from 312.85 (native) to 13.69 (TiCard-GBR), and P99 drops from 37,974.37 to 3,416.50 (TiCard-TabPFN), while a join-only policy preserves near-perfect median behavior. We position TiCard as an AI4DB building block focused on deployability: explicit scope, conservative integration policies, and an integration roadmap from offline correction to in-optimizer use.", "AI": {"tldr": "The paper introduces TiCard, a system to improve cardinality estimation in databases by learning corrections to native estimators, achieving significant accuracy improvements with minimal integration effort.", "motivation": "Cardinality estimation is a bottleneck in cost-based query optimization; existing solutions either miss correlations or require high integration efforts, prompting a need for a deployable, effective solution.", "method": "TiCard is a correction-based framework that works with existing database estimators using EXPLAIN-only features for learning corrections through models like Gradient Boosting Regressor or TabPFN.", "result": "TiCard significantly improves query operator-level accuracy in tests on TiDB, reducing P90 and P99 Q-errors substantially while preserving median behavior in a low-intrusion setup.", "conclusion": "TiCard serves as an effective and deployable AI4DB enhancement tool, providing a practical way to boost cardinality estimation accuracy without overhauling existing database systems."}}
{"id": "2512.14585", "pdf": "https://arxiv.org/pdf/2512.14585", "abs": "https://arxiv.org/abs/2512.14585", "authors": ["Adarsha Shrestha", "Basanta Pokharel", "Binit Shrestha", "Smriti Adhikari", "Dinesh Gothe"], "title": "Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "Nepali, a low-resource language spoken by over 32 million people, continues to face challenges in natural language processing (NLP) due to its complex grammar, agglutinative morphology, and limited availability of high-quality corpora. Most efforts to date have centered on basic encoder architectures; they remain insufficient for Nepali-specific text generation. This study presents a GPT-2-based Nepali language model trained using several training strategies inspired by GPT-3, including optimized learning rate schedules, batch scaling, and architectural refinements. A custom 16k Byte-Pair Encoding (BPE) tokenizer was trained exclusively on Nepali text to ensure more consistent segmentation and improved input representation. The model was pretrained on a combined dataset comprising a 10.75GB cleaned NepBERTa corpus and additional web-scraped Nepali news articles. FlashAttention was integrated to reduce memory usage and stabilize training. After two epochs, the model achieved a training loss of 3.168177, a validation loss of 3.081982, and a final perplexity of 21.80, demonstrating its capability to generate coherent Nepali news-style text.", "AI": {"tldr": "This study develops a Nepali GPT-2-based language model using advanced training strategies, achieving coherent text generation for a low-resource language.", "motivation": "Nepali, as a low-resource language, lacks effective NLP tools due to its complex grammar, agglutinative morphology, and insufficient high-quality data. This study aims to address these challenges and improve Nepali text generation.", "method": "Researchers designed a GPT-2-inspired model with strategies from GPT-3, trained on a combined Nepali corpus, custom tokenizer, optimized training techniques, and FlashAttention to enhance memory usage and stabilize training.", "result": "The model obtained a training loss of 3.168177, validation loss of 3.081982, and perplexity of 21.80 after two epochs, successfully generating coherent news-style Nepali text.", "conclusion": "The study highlights the effectiveness of advanced GPT-inspired methods in improving NLP models for low-resource languages, creating a refined approach to Nepali text generation."}}
{"id": "2512.14217", "pdf": "https://arxiv.org/pdf/2512.14217", "abs": "https://arxiv.org/abs/2512.14217", "authors": ["Yang Bai", "Liudi Yang", "George Eskandar", "Fengyi Shen", "Mohammad Altillawi", "Ziyuan Liu", "Gitta Kutyniok"], "title": "DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.", "AI": {"tldr": "DRAW2ACT enhances trajectory-conditioned video generation for robotic manipulation by incorporating depth-aware representations and multimodal conditioning.", "motivation": "Current video diffusion models lack sufficient controllability for robotic manipulation tasks, limiting their utility as simulators for embodied AI.", "method": "DRAW2ACT utilizes depth, semantics, shape, and motion from input trajectories, integrates them into the model using cross-modality attention mechanisms, and generates RGB-depth sequences. A multimodal policy model uses the output to estimate robot joint angles.", "result": "Experiments show DRAW2ACT outperforms baselines in visual fidelity, consistency, and robotic manipulation success rates across benchmarks.", "conclusion": "DRAW2ACT effectively enables controllable and consistent robotic demonstrations by improving video generation techniques and multimodal policy models."}}
{"id": "2512.13837", "pdf": "https://arxiv.org/pdf/2512.13837", "abs": "https://arxiv.org/abs/2512.13837", "authors": ["Shicheng Liu", "Siyuan Xu", "Wenjie Qiu", "Hangfan Zhang", "Minghui Zhu"], "title": "Explainable reinforcement learning from human feedback to improve alignment", "categories": ["cs.LG"], "comment": null, "summary": "A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) for alignment of language models (LMs). In particular, it is observed in the literature that LMs tuned by RLHF can still output unsatisfactory responses. This paper proposes a method to improve the unsatisfactory responses by correcting their causes. Our method has two parts. The first part proposes a post-hoc explanation method to explain why an unsatisfactory response is generated to a prompt by identifying the training data that lead to this response. We formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination of this set of training data in the feature space. We propose an efficient iterative data selection algorithm to solve this problem. The second part proposes an unlearning method that improves unsatisfactory responses to some prompts by unlearning the training data that lead to these unsatisfactory responses and, meanwhile, does not significantly degrade satisfactory responses to other prompts. Experimental results demonstrate that our algorithm can improve RLHF.", "AI": {"tldr": "The paper proposes a method to enhance reinforcement learning from human feedback (RLHF) in language models by identifying and correcting causes of unsatisfactory responses.", "motivation": "Human problem-solving strategies often involve identifying the causes of mistakes and correcting them. The paper explores whether this approach can be applied to RLHF to improve alignment in language models.", "method": "The paper introduces a post-hoc explanation method to identify training data responsible for unsatisfactory responses and an unlearning method to remove these influences, ensuring minimal impact on good responses. It uses constrained combinatorial optimization and iterative selection algorithms.", "result": "The proposed methods successfully identify problematic training data and improve language models\u2019 responses through targeted unlearning while maintaining satisfactory outputs from RLHF.", "conclusion": "The study demonstrates that the human-inspired improvement strategy can effectively enhance RLHF for language model alignment, suggesting its promise for further development in this domain."}}
{"id": "2512.14032", "pdf": "https://arxiv.org/pdf/2512.14032", "abs": "https://arxiv.org/abs/2512.14032", "authors": ["Ignacio Alzugaray", "Marwan Taher", "Andrew J. Davison"], "title": "ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "Project Page: https://github.com/ialzugaray/ace-slam", "summary": "We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time. For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates. SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.\n  Our system is the first one to achieve strict real-time in neural implicit RGB-D SLAM by relying on a SCR-based representation. We introduce a novel SCR architecture specifically tailored for this purpose and detail the critical design choices required to integrate SCR into a live SLAM pipeline. The resulting framework is simple yet flexible, seamlessly supporting both sparse and dense features, and operates reliably in dynamic environments without special adaptation. We evaluate our approach on established synthetic and real-world benchmarks, demonstrating competitive performance against the state of the art. Project Page: https://github.com/ialzugaray/ace-slam", "AI": {"tldr": "The paper introduces a novel neural RGB-D SLAM system that uses Scene Coordinate Regression (SCR) for implicit map representation, achieving real-time performance and competitive results compared to existing methods.", "motivation": "To develop a lightweight, efficient, and privacy-preserving neural RGB-D SLAM system capable of real-time performance in dynamic environments, leveraging SCR for implicit map representation.", "method": "The system employs a tailored SCR-based representation in a novel architecture, integrating it into a live SLAM pipeline for both sparse and dense feature mapping.", "result": "The introduced SLAM system operates in real-time and demonstrates competitive performance on established synthetic and real-world benchmarks.", "conclusion": "This approach offers a simple yet flexible framework for neural implicit RGB-D SLAM, advancing efficiency, privacy, and adaptability in mapping and localization tasks."}}
{"id": "2512.14395", "pdf": "https://arxiv.org/pdf/2512.14395", "abs": "https://arxiv.org/abs/2512.14395", "authors": ["Wentao Wan", "Qiqing Lao", "Zhiwei Xie", "Hefeng Wu", "Runnan Lin", "Liang Lin", "Keze Wang"], "title": "Massive Editing for Large Language Models Based on Dynamic Weight Generation", "categories": ["cs.AI"], "comment": "27 pages, 8 figures", "summary": "Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.", "AI": {"tldr": "This paper introduces MeG, a dynamic weight generation approach using diffusion models to efficiently perform reliable, general, and localized large-scale knowledge editing in LLMs.", "motivation": "The study aims to address the challenges of large-scale knowledge editing in LLMs while ensuring edits are reliable, generalizable, and localized without requiring expensive pre-training.", "method": "The proposed Massive editing approach (MeG) utilizes dynamic weight neurons attached to LLM layers, with weights conditionally generated by a diffusion model based on input queries.", "result": "Experiments reveal MeG improves performance in large-scale knowledge editing, notably enhancing reliability, generality, and locality metrics compared to existing methods, with significant gains in locality.", "conclusion": "MeG offers a cost-efficient and effective solution to perform large-scale knowledge editing in LLMs, showcasing superior advantages in maintaining localized accuracy and reliable results."}}
{"id": "2512.14620", "pdf": "https://arxiv.org/pdf/2512.14620", "abs": "https://arxiv.org/abs/2512.14620", "authors": ["Atsuyuki Miyai", "Shota Onohara", "Jeonghun Baek", "Kiyoharu Aizawa"], "title": "JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Project page: https://mmmu-japanese-benchmark.github.io/JMMMU_Pro/", "summary": "This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.", "AI": {"tldr": "The paper proposes JMMMU-Pro, an enhanced image-based Japanese multimodal benchmark, built using a scalable method called Vibe Benchmark Construction.", "motivation": "To address the lack of rigorous Japanese multimodal benchmarks that integrate visual and textual understanding for evaluating LMMs.", "method": "The benchmark uses Nano Banana Pro, an image generative model, to create visual questions. Human verification ensures quality through iterative regeneration with adjusted prompts.", "result": "Open-source large multimodal models (LMMs) significantly underperform on JMMMU-Pro, highlighting its value in guiding community development.", "conclusion": "JMMMU-Pro provides a robust evaluation tool for Japanese multimodal tasks, while Vibe Benchmark Construction offers a scalable framework for creating similar benchmarks."}}
{"id": "2512.14222", "pdf": "https://arxiv.org/pdf/2512.14222", "abs": "https://arxiv.org/abs/2512.14222", "authors": ["Xichen Ding", "Jianzhe Gao", "Cong Pan", "Wenguan Wang", "Jie Qin"], "title": "History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Aerial Vision-and-Language Navigation (AVLN) requires Unmanned Aerial Vehicle (UAV) agents to localize targets in large-scale urban environments based on linguistic instructions. While successful navigation demands both global environmental reasoning and local scene comprehension, existing UAV agents typically adopt mono-granularity frameworks that struggle to balance these two aspects. To address this limitation, this work proposes a History-Enhanced Two-Stage Transformer (HETT) framework, which integrates the two aspects through a coarse-to-fine navigation pipeline. Specifically, HETT first predicts coarse-grained target positions by fusing spatial landmarks and historical context, then refines actions via fine-grained visual analysis. In addition, a historical grid map is designed to dynamically aggregate visual features into a structured spatial memory, enhancing comprehensive scene awareness. Additionally, the CityNav dataset annotations are manually refined to enhance data quality. Experiments on the refined CityNav dataset show that HETT delivers significant performance gains, while extensive ablation studies further verify the effectiveness of each component.", "AI": {"tldr": "This paper introduces the HETT framework for UAV-based navigation in urban environments using a coarse-to-fine approach for improved target localization.", "motivation": "Current UAV navigation frameworks struggle to balance global reasoning and local scene comprehension, leading to inefficiencies in AVLN tasks.", "method": "The proposed HETT framework utilizes a History-Enhanced Two-Stage Transformer pipeline which combines spatial landmarks and historical context for coarse-grained predictions and refines actions using fine-grained visual analysis. A historical grid map is also introduced for structured spatial memory.", "result": "The HETT framework demonstrated significant performance improvements on the refined CityNav dataset, supported by extensive ablation studies.", "conclusion": "Integrating coarse-to-fine navigation strategies and structured spatial mapping strengthens UAV target localization capabilities in AVLN tasks."}}
{"id": "2512.13852", "pdf": "https://arxiv.org/pdf/2512.13852", "abs": "https://arxiv.org/abs/2512.13852", "authors": ["Jelena Losic"], "title": "Topologically-Stabilized Graph Neural Networks: Empirical Robustness Across Domains", "categories": ["cs.LG", "cs.SI"], "comment": null, "summary": "Graph Neural Networks (GNNs) have become the standard for graph representation learning but remain vulnerable to structural perturbations. We propose a novel framework that integrates persistent homology features with stability regularization to enhance robustness. Building on the stability theorems of persistent homology \\cite{cohen2007stability}, our method combines GIN architectures with multi-scale topological features extracted from persistence images, enforced by Hiraoka-Kusano-inspired stability constraints. Across six diverse datasets spanning biochemical, social, and collaboration networks , our approach demonstrates exceptional robustness to edge perturbations while maintaining competitive accuracy. Notably, we observe minimal performance degradation (0-4\\% on most datasets) under perturbation, significantly outperforming baseline stability. Our work provides both a theoretically-grounded and empirically-validated approach to robust graph learning that aligns with recent advances in topological regularization", "AI": {"tldr": "The paper proposes enhancing Graph Neural Networks (GNNs) robustness using persistent homology and stability regularization, achieving strong resistance to structural perturbations.", "motivation": "GNNs are vulnerable to structural perturbations, and there's a need for robust methods for graph representation learning.", "method": "The method integrates persistent homology features (persistence images) into GNNs (GIN architectures) with stability constraints inspired by Hiraoka-Kusano.", "result": "The framework performs robustly against edge perturbations across six datasets, showing minimal performance degradation (0-4%) and outperforming baselines.", "conclusion": "The proposed framework provides a theoretically-grounded and empirically-supported enhancement for robust graph learning, leveraging topological regularization advancements."}}
{"id": "2512.14039", "pdf": "https://arxiv.org/pdf/2512.14039", "abs": "https://arxiv.org/abs/2512.14039", "authors": ["Meng Wei", "Cheng Zhang", "Jianmin Zheng", "Hamid Rezatofighi", "Jianfei Cai"], "title": "ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances have equipped 3D Gaussian Splatting with texture parameterizations to capture spatially varying attributes, improving the performance of both appearance modeling and downstream tasks. However, the added texture parameters introduce significant memory efficiency challenges. Rather than proposing new texture formulations, we take a step back to examine the characteristics of existing textured Gaussian methods and identify two key limitations in common: (1) Textures are typically defined in canonical space, leading to inefficient sampling that wastes textures' capacity on low-contribution regions; and (2) texture parameterization is uniformly assigned across all Gaussians, regardless of their visual complexity, resulting in over-parameterization. In this work, we address these issues through two simple yet effective strategies: adaptive sampling based on the Gaussian density distribution and error-driven anisotropic parameterization that allocates texture resources according to rendering error. Our proposed ASAP Textured Gaussians, short for Adaptive Sampling and Anisotropic Parameterization, significantly improve the quality efficiency tradeoff, achieving high-fidelity rendering with far fewer texture parameters.", "AI": {"tldr": "The paper introduces ASAP Textured Gaussians to enhance efficiency and quality in 3D Gaussian Splatting with texture parameterizations, focusing on adaptive sampling and anisotropic parametric allocation.", "motivation": "Address inefficiencies in 3D Gaussian Splatting texture parameterization, such as poor sampling processes and uniformly assigned resources, which lead to waste and over-parameterization.", "method": "The approach utilizes adaptive sampling based on Gaussian density and anisotropic parameterization driven by rendering error to allocate resources dynamically and efficiently.", "result": "Achieved high-fidelity rendering with significantly fewer texture parameters, optimizing the tradeoff between quality and efficiency.", "conclusion": "ASAP Textured Gaussians provides a simple yet effective solution to address inefficiency and over-parameterization concerns in textured Gaussian methods."}}
{"id": "2512.14417", "pdf": "https://arxiv.org/pdf/2512.14417", "abs": "https://arxiv.org/abs/2512.14417", "authors": ["Jia Hu", "Junqi Li", "Weimeng Lin", "Peng Jia", "Yuxiong Ji", "Jintao Lai"], "title": "PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals", "categories": ["cs.AI"], "comment": null, "summary": "Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created", "AI": {"tldr": "The paper introduces PortAgent, an automated vehicle dispatching agent for Automated Container Terminals, addressing transferability challenges with minimal data, no specialist dependency, and faster deployment.", "motivation": "To overcome limitations in Vehicle Dispatching Systems, such as reliance on operational specialists, high data demands, and manual processes, hindering their transferability across terminals.", "method": "The paper leverages Large Language Models to develop PortAgent, consisting of a Virtual Expert Team (VET) with specialized virtual agents using few-shot learning and Retrieval-Augmented Generation. A self-correction workflow inspired by Reflexion framework ensures automation.", "result": "PortAgent eliminates dependency on specialists, reduces data requirements, and achieves fast deployment for terminal-specific VDS workflows.", "conclusion": "PortAgent showcases the potential of LLMs in automating complex systems, offering enhanced transferability and operational efficiency for container terminal dispatching systems."}}
{"id": "2512.14645", "pdf": "https://arxiv.org/pdf/2512.14645", "abs": "https://arxiv.org/abs/2512.14645", "authors": ["David Schulmeister", "Valentin Hartmann", "Lars Klein", "Robert West"], "title": "TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.", "AI": {"tldr": "This paper introduces TiME (Tiny Monolingual Encoders), which are smaller, energy-efficient language models optimized for specific tasks, addressing sustainability and performance issues in large models.", "motivation": "Many NLP tasks require smaller, task-specific models for real-time processing, energy efficiency, and deployment on low-resource devices, as large models are too slow and energy-intensive for these applications.", "method": "The authors propose training small monolingual models using distillation techniques, including transferring knowledge from multilingual teachers and transforming positional embedding styles.", "result": "TiME models achieve a better trade-off: they balance strong NLP task performance with improved throughput, reduced latency, and minimized energy usage.", "conclusion": "TiME models demonstrate the feasibility of using advanced training methods for smaller, efficient language models that meet the needs of resource-constrained scenarios while supporting low-resource languages."}}
{"id": "2512.14349", "pdf": "https://arxiv.org/pdf/2512.14349", "abs": "https://arxiv.org/abs/2512.14349", "authors": ["Federico Califano", "Camilla Rota", "Riccardo Zanella", "Antonio Franchi"], "title": "A Geometric Task-Space Port-Hamiltonian Formulation for Redundant Manipulators", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "We present a novel geometric port-Hamiltonian formulation of redundant manipulators performing a differential kinematic task $\u03b7=J(q)\\dot{q}$, where $q$ is a point on the configuration manifold, $\u03b7$ is a velocity-like task space variable, and $J(q)$ is a linear map representing the task, for example the classical analytic or geometric manipulator Jacobian matrix. The proposed model emerges from a change of coordinates from canonical Hamiltonian dynamics, and splits the standard Hamiltonian momentum variable into a task-space momentum variable and a null-space momentum variable. Properties of this model and relation to Lagrangian formulations present in the literature are highlighted. Finally, we apply the proposed model in an \\textit{Interconnection and Damping Assignment Passivity-Based Control} (IDA-PBC) design to stabilize and shape the impedance of a 7-DOF Emika Panda robot in simulation.", "AI": {"tldr": "This paper introduces a geometric port-Hamiltonian framework for redundant manipulators' differential kinematics, splitting standard Hamiltonian momentum into task- and null-space components, and showcases its application in controlling a robotic manipulator.", "motivation": "To develop a structured and efficient mathematical framework for analyzing and controlling redundant manipulators through a task-space and null-space decomposition in the context of port-Hamiltonian systems.", "method": "The authors reformulate canonical Hamiltonian dynamics into a geometric port-Hamiltonian framework by splitting momentum variables into task-space and null-space components. They apply this model for controller design using IDA-PBC on a simulated robot.", "result": "The proposed model effectively stabilizes and shapes the impedance of a 7-DOF Emika Panda robot in simulation, validating the structured approach for kinematic tasks.", "conclusion": "The geometric port-Hamiltonian approach is a viable and effective method for controlling redundant manipulators, providing insights into momentum variable decomposition and its application to robotic systems' control."}}
{"id": "2512.14040", "pdf": "https://arxiv.org/pdf/2512.14040", "abs": "https://arxiv.org/abs/2512.14040", "authors": ["Boran Wang", "Xinming Wang", "Yi Chen", "Xiang Li", "Jian Xu", "Jing Yuan", "Chenglin Liu"], "title": "ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "With their high information density and intuitive readability, charts have become the de facto medium for data analysis and communication across disciplines. Recent multimodal large language models (MLLMs) have made notable progress in automated chart understanding, yet they remain heavily dependent on explicit textual annotations and the performance degrades markedly when key numerals are absent. To address this limitation, we introduce ChartAgent, a chart understanding framework grounded in Tool-Integrated Reasoning (TIR). Inspired by human cognition, ChartAgent decomposes complex chart analysis into a sequence of observable, replayable steps. Supporting this architecture is an extensible, modular tool library comprising more than a dozen core tools, such as keyelement detection, instance segmentation, and optical character recognition (OCR), which the agent dynamically orchestrates to achieve systematic visual parsing across diverse chart types. Leveraging TIRs transparency and verifiability, ChartAgent moves beyond the black box paradigm by standardizing and consolidating intermediate outputs into a structured Evidence Package, providing traceable and reproducible support for final conclusions. Experiments show that ChartAgent substantially improves robustness under sparse annotation settings, offering a practical path toward trustworthy and extensible systems for chart understanding.", "AI": {"tldr": "The paper introduces ChartAgent, a framework to improve automated chart understanding by breaking down the task into modular and reproducible steps using a tool-integrated reasoning approach.", "motivation": "Charts are widely used for data analysis, but current large language models struggle with automated understanding when text annotations or numerals are missing.", "method": "ChartAgent uses a tool-integrated reasoning framework, dynamically orchestrating a modular library of tools to perform visual parsing and analysis across various chart types.", "result": "ChartAgent demonstrates improved performance in understanding charts with sparse annotations and provides traceable evidence for its conclusions.", "conclusion": "ChartAgent offers a robust, trustworthy, and extensible approach to automated chart understanding, surpassing limitations of existing systems."}}
{"id": "2512.14681", "pdf": "https://arxiv.org/pdf/2512.14681", "abs": "https://arxiv.org/abs/2512.14681", "authors": ["Lanxiang Hu", "Siqi Kou", "Yichao Fu", "Samyam Rajbhandari", "Tajana Rosing", "Yuxiong He", "Zhijie Deng", "Hao Zhang"], "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing", "categories": ["cs.CL"], "comment": null, "summary": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.", "AI": {"tldr": "This paper introduces \"Jacobi Forcing,\" a method to speed up language model inference through progressive distillation of autoregressive (AR) models into parallel decoders, resulting in significant speedup with minor performance loss.", "motivation": "Existing parallel decoding models for large language models lack efficiency due to mismatched posttraining data distributions and conflicts with pretrained properties like causal inference.", "method": "Jacobi Forcing trains models using their own generated parallel decoding results, gradually transitioning AR models into parallel decoders while maintaining pretrained causal properties.", "result": "Jacobi Forcing achieves 3.8x speedup on coding and math benchmarks, with an enhanced decoding method enabling up to 4.0x speedup through multi-block decoding and rejection recycling.", "conclusion": "The proposed distillation and decoding framework is effective in reducing inference latency with minimal trade-offs in generation quality. The method could benefit scenarios demanding faster large language model inference."}}
{"id": "2512.14426", "pdf": "https://arxiv.org/pdf/2512.14426", "abs": "https://arxiv.org/abs/2512.14426", "authors": ["Simon Steuernagel", "Marcus Baum"], "title": "Quadratic Kalman Filter for Elliptical Extended Object Tracking based on Decoupling State Components", "categories": ["eess.SP", "cs.RO"], "comment": "13 pages, 8 figures, submitted to IEEE Transactions on Aerospace and Electronic Systems", "summary": "Extended object tracking involves estimating both the physical extent and kinematic parameters of a target object, where typically multiple measurements are observed per time step. In this article, we propose a deterministic closed-form elliptical extended object tracker, based on decoupling of the kinematics, orientation, and axis lengths. By disregarding potential correlations between these state components, fewer approximations are required for the individual estimators than for an overall joint solution. The resulting algorithm outperforms existing algorithms, reaching the accuracy of sampling-based procedures. Additionally, a batch-based variant is introduced, yielding highly efficient computation while outperforming all comparable state-of-the-art algorithms. This is validated both by a simulation study using common models from literature, as well as an extensive quantitative evaluation on real automotive radar data.", "AI": {"tldr": "The paper introduces an efficient and accurate elliptical extended object tracking algorithm that outperforms state-of-the-art methods in both simulations and real automotive radar data.", "motivation": "The motivation is to improve extended object tracking by developing a more accurate and efficient algorithm for estimating an object's extent and kinematics, particularly addressing limitations of current methods which often require approximations.", "method": "The approach involves decoupling kinematics, orientation, and axis lengths in the tracking process, simplifying the estimation by ignoring correlations between state components. A deterministic closed-form solution and a batch-based variant are proposed.", "result": "The algorithm achieves higher accuracy than existing methods, reaching the performance of sampling-based techniques while being computationally efficient. This was demonstrated through simulations and real automotive radar data analysis.", "conclusion": "The proposed method offers a significant advancement in extended object tracking by delivering a balance of accuracy and computational efficiency, making it a compelling alternative to state-of-the-art algorithms."}}
{"id": "2512.13872", "pdf": "https://arxiv.org/pdf/2512.13872", "abs": "https://arxiv.org/abs/2512.13872", "authors": ["Kamil Ciosek", "Nicol\u00f2 Felicioni", "Sina Ghiassian", "Juan Elenter Litwin", "Francesco Tonolini", "David Gustaffson", "Eva Garcia Martin", "Carmen Barcena Gonzales", "Rapha\u00eblle Bertrand-Lalo"], "title": "Measuring Uncertainty Calibration", "categories": ["cs.LG"], "comment": "28 pages", "summary": "We make two contributions to the problem of estimating the $L_1$ calibration error of a binary classifier from a finite dataset. First, we provide an upper bound for any classifier where the calibration function has bounded variation. Second, we provide a method of modifying any classifier so that its calibration error can be upper bounded efficiently without significantly impacting classifier performance and without any restrictive assumptions. All our results are non-asymptotic and distribution-free. We conclude by providing advice on how to measure calibration error in practice. Our methods yield practical procedures that can be run on real-world datasets with modest overhead.", "AI": {"tldr": "The paper addresses estimating $L_1$ calibration error of a binary classifier, introducing an upper bound method and a modification approach for efficient error measurement.", "motivation": "Calibration error estimation is crucial for binary classifiers to ensure predictions align well with real-world outcomes.", "method": "Introduced an upper bound method for classifiers with bounded variation calibration functions and a practical modification procedure for measuring errors without restrictive assumptions.", "result": "The findings are non-asymptotic, distribution-free, and applicable on practical datasets with modest overhead.", "conclusion": "Provides advice on efficiently measuring calibration error, ensuring methods are applicable in real-world contexts."}}
{"id": "2512.14044", "pdf": "https://arxiv.org/pdf/2512.14044", "abs": "https://arxiv.org/abs/2512.14044", "authors": ["Zhenguo Zhang", "Haohan Zhen", "Yishen Wang", "Le Xu", "Tianchen Deng", "Xuefeng Chen", "Qu Chen", "Bo Zhang", "Wuxiong Huang"], "title": "OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and \"zoom in\" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.", "AI": {"tldr": "The paper addresses reliability failures in Vision-Language Models (VLMs) for safety-critical domains, introducing OmniDrive-R1, a model with a novel interleaved reasoning mechanism and reinforcement learning to improve autonomous driving tasks.", "motivation": "The motivation is to tackle the issue of object hallucination in VLMs, particularly for safety-critical applications like autonomous driving, as existing methods fail to integrate perception and reasoning effectively and demand expensive localization labels.", "method": "The method involves the OmniDrive-R1 framework with an interleaved Multi-modal Chain-of-Thought (iMCoT) reasoning and reinforcement learning-driven visual grounding. A novel Clip-GRPO algorithm provides annotation-free grounding rewards to unify perception and reasoning.", "result": "OmniDrive-R1 significantly outperformed the baseline Qwen2.5VL-7B, with reasoning scores increasing from 51.77% to 80.35% and answer accuracy improving from 37.81% to 73.62%.", "conclusion": "The introduction of OmniDrive-R1 with its innovative reinforcement learning methods demonstrates a significant improvement in VLM reasoning and accuracy for autonomous driving without requiring dense localization labels."}}
{"id": "2512.14465", "pdf": "https://arxiv.org/pdf/2512.14465", "abs": "https://arxiv.org/abs/2512.14465", "authors": ["Siyuan Zhu", "Chengdong Xu", "Kaiqiang Ke", "Chao Yu"], "title": "Context-Picker: Dynamic context selection using multi-stage reinforcement learning", "categories": ["cs.AI"], "comment": null, "summary": "In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \\emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \\emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \\emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines \"minimal sufficient sets\" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.", "AI": {"tldr": "This paper introduces Context-Picker, a framework that selects optimal context for long-context question answering using a reasoning-aware two-stage reinforcement learning process.", "motivation": "Addressing the challenge of selecting the optimal amount of context in long-context QA to mitigate omitted critical information or noise from irrelevant passages.", "method": "A two-stage reasoning-aware reinforcement learning process: recall-oriented for maximizing reasoning chain coverage and precision-oriented to minimize redundancy.", "result": "Context-Picker outperforms retrieval-augmented generation (RAG) baselines in five QA benchmarks, achieving improved answer accuracy with shorter context lengths.", "conclusion": "The proposed approach effectively selects minimal sufficient subsets of context, enhancing QA performance and demonstrating the importance of reasoning-aware fine-tuning methods."}}
{"id": "2512.14687", "pdf": "https://arxiv.org/pdf/2512.14687", "abs": "https://arxiv.org/abs/2512.14687", "authors": ["Yen-Ju Lu", "Kunxiao Gao", "Mingrui Liang", "Helin Wang", "Thomas Thebaud", "Laureano Moro-Velazquez", "Najim Dehak", "Jesus Villalba"], "title": "Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "comment": "12 pages, 2 figures", "summary": "Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. The dataset is available online at https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/. Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.", "AI": {"tldr": "Spoken DialogSum introduces a dataset for emotion-aware spoken dialogue summarization, aligning conversational audio with summaries and paralinguistic cues.", "motivation": "The paper addresses the need for a dataset linking speech, summaries, and paralinguistic cues to improve emotion-aware spoken dialogue summarization.", "method": "It constructs the dataset by rewriting DialogSum scripts with Switchboard-style fillers and tags, followed by using a TTS engine to synthesize dialogues with paralinguistic alignment.", "result": "Spoken DialogSum includes 13,460 emotion-diverse dialogues paired with factual and emotion-focused summaries, with improved performance shown by Audio-LLM models compared to traditional cascaded systems.", "conclusion": "The dataset facilitates end-to-end speech modeling, advancing emotion-rich spoken dialogue summarization and showcasing the importance of integrated audio language models."}}
{"id": "2512.14442", "pdf": "https://arxiv.org/pdf/2512.14442", "abs": "https://arxiv.org/abs/2512.14442", "authors": ["Zixin Zhang", "Kanghao Chen", "Hanqing Wang", "Hongfei Zhang", "Harold Haodong Chen", "Chenfei Liao", "Litao Guo", "Ying-Cong Chen"], "title": "A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\\textbf{Dreamer}$ that employs generative models to visualize $\\textit{how}$ an interaction would look; (2) a $\\textbf{Thinker}$ that utilizes large vision-language models to decide $\\textit{what}$ object part to interact with; and (3) a $\\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.", "AI": {"tldr": "A4-Agent introduces a zero-shot framework for affordance prediction by decoupling it into a three-stage pipeline using pre-trained models without fine-tuning, achieving superior generalization compared to supervised methods.", "motivation": "Current methods for affordance prediction do not generalize well to novel objects and environments due to their coupling of reasoning and grounding in monolithic pipelines reliant on annotated datasets.", "method": "The paper proposes a three-stage agentic framework featuring Dreamer (visualizes interactions), Thinker (decides on object parts), and Spotter (locates interaction areas), using specialized pre-trained foundation models without fine-tuning.", "result": "The training-free, zero-shot A4-Agent framework outperforms state-of-the-art supervised approaches across benchmarks and exhibits robust generalization in real-world scenarios.", "conclusion": "This framework demonstrates the power of leveraging pre-trained models for affordance prediction without task-specific training, bypassing traditional limitations and ensuring broader applicability."}}
{"id": "2512.13880", "pdf": "https://arxiv.org/pdf/2512.13880", "abs": "https://arxiv.org/abs/2512.13880", "authors": ["Geofrey Owino", "Bernard Shibwabo"], "title": "Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization", "categories": ["cs.LG", "cs.AI", "cs.SD"], "comment": "This paper was accepted for presentation and presented at the 2025 International Conference on Computer Engineering, Network, and Intelligent Multimedia (CENIM 2025)", "summary": "Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.", "AI": {"tldr": "The paper proposes a privacy-preserving, noise-robust, and communication-efficient system for infant cry classification utilizing federated learning and advanced denoising to ensure reliable and secure deployment.", "motivation": "Infant cries can provide essential insights into their needs, but challenges such as privacy concerns, background noise, and environment variability hinder widespread deployment of such solutions.", "method": "They propose an end-to-end pipeline combining a denoising autoencoder, convolutional tokenizer, Transformer model, federated learning with secure aggregation, and adaptive features for edge deployment.", "result": "The model achieves high accuracy (macro F1 score of 0.938, AUC of 0.962, ECE of 0.032) while significantly reducing communication overhead (upload size reduced to 3.3 MB). It also performs real-time edge inference efficiently.", "conclusion": "This system paves the way for scalable, reliable, and privacy-conscious infant cry analysis, making it highly suitable for real-world, federated deployments."}}
{"id": "2512.14050", "pdf": "https://arxiv.org/pdf/2512.14050", "abs": "https://arxiv.org/abs/2512.14050", "authors": ["Wenjun Liu", "Qian Wu", "Yifeng Hu", "Yuke Li"], "title": "SELECT: Detecting Label Errors in Real-world Scene Text Data", "categories": ["cs.CV"], "comment": null, "summary": "We introduce SELECT (Scene tExt Label Errors deteCTion), a novel approach that leverages multi-modal training to detect label errors in real-world scene text datasets. Utilizing an image-text encoder and a character-level tokenizer, SELECT addresses the issues of variable-length sequence labels, label sequence misalignment, and character-level errors, outperforming existing methods in accuracy and practical utility. In addition, we introduce Similarity-based Sequence Label Corruption (SSLC), a process that intentionally introduces errors into the training labels to mimic real-world error scenarios during training. SSLC not only can cause a change in the sequence length but also takes into account the visual similarity between characters during corruption. Our method is the first to detect label errors in real-world scene text datasets successfully accounting for variable-length labels. Experimental results demonstrate the effectiveness of SELECT in detecting label errors and improving STR accuracy on real-world text datasets, showcasing its practical utility.", "AI": {"tldr": "The paper presents SELECT, a multi-modal method for detecting label errors in scene text datasets using sequence alignment and SSLC-based corruption for realistic error simulation.", "motivation": "Detecting errors in real-world scene text datasets is challenging due to variable-length sequence labels and misalignments, which require a specialized approach.", "method": "SELECT combines image-text encoding, character-level tokenizing, and SSLC-based label corruption for error detection in scene text datasets.", "result": "SELECT successfully detected label errors and enhanced Scene Text Recognition accuracy in real-world datasets.", "conclusion": "SELECT provides an effective and innovative solution for identifying label errors in scene text datasets, offering improved dataset reliability and STR performance."}}
{"id": "2512.14474", "pdf": "https://arxiv.org/pdf/2512.14474", "abs": "https://arxiv.org/abs/2512.14474", "authors": ["Annu Rana", "Gaurav Kumar"], "title": "Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.", "AI": {"tldr": "The paper introduces Model-First Reasoning (MFR), an LLM approach to solve planning tasks by explicitly modeling problems before generating solutions, achieving better results than existing methods.", "motivation": "Large Language Models (LLMs) often face issues such as constraint violations and inconsistent solutions when dealing with complex multi-step planning tasks. Current methodologies lack an explicit representation of the problem, prompting the need for an improved approach.", "method": "The authors propose Model-First Reasoning (MFR), a two-phase framework where the LLM first explicitly defines the problem components, including entities, state variables, actions, and constraints. This model is then utilized to generate a solution.", "result": "MFR demonstrated improved performance across various planning domains, such as medical scheduling and route planning, by reducing constraint violations and enhancing solution quality compared to existing approaches like Chain-of-Thought and ReAct.", "conclusion": "The findings suggest that representational deficiencies, rather than reasoning limitations, are the main cause of planning failures in LLMs. Explicit modeling significantly enhances both robustness and interpretability, making it a critical element in solving complex problems with LLMs."}}
{"id": "2512.14691", "pdf": "https://arxiv.org/pdf/2512.14691", "abs": "https://arxiv.org/abs/2512.14691", "authors": ["Zefan Cai", "Haoyi Qiu", "Tianyi Ma", "Haozhe Zhao", "Gengze Zhou", "Kung-Hsiang Huang", "Parisa Kordjamshidi", "Minjia Zhang", "Xiao Wen", "Jiuxiang Gu", "Nanyun Peng", "Junjie Hu"], "title": "MMGR: Multi-Modal Generative Reasoning", "categories": ["cs.CL", "cs.CV"], "comment": "work in progress", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "AI": {"tldr": "The paper introduces MMGR, a benchmark evaluating generative video/image models on reasoning abilities, uncovering significant shortcomings in abstract reasoning, spatial tasks, and causal correctness.", "motivation": "Video foundation models excel in generating realistic and coherent content but fail to reliably simulate the world, particularly in capturing physical, logical, and spatial constraints.", "method": "Develop MMGR, an evaluation framework incorporating metrics across five reasoning abilities. Benchmarking generative models on abstract reasoning, embodied navigation, and physical commonsense tasks to identify limitations.", "result": "Models show moderate success in Physical Commonsense tasks but perform poorly in Abstract Reasoning (e.g., ARC-AGI below 10%). They struggle in global state consistency and spatial planning.", "conclusion": "MMGR highlights shortcomings in current models and provides a diagnostic benchmark to guide the development of reasoning-aware generative models."}}
{"id": "2512.14450", "pdf": "https://arxiv.org/pdf/2512.14450", "abs": "https://arxiv.org/abs/2512.14450", "authors": ["Riccardo Busetto", "Elia Cereda", "Marco Forgione", "Gabriele Maroni", "Dario Piga", "Daniele Palossi"], "title": "Nonlinear System Identification Nano-drone Benchmark", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "We introduce a benchmark for system identification based on 75k real-world samples from the Crazyflie 2.1 Brushless nano-quadrotor, a sub-50g aerial vehicle widely adopted in robotics research. The platform presents a challenging testbed due to its multi-input, multi-output nature, open-loop instability, and nonlinear dynamics under agile maneuvers. The dataset comprises four aggressive trajectories with synchronized 4-dimensional motor inputs and 13-dimensional output measurements. To enable fair comparison of identification methods, the benchmark includes a suite of multi-horizon prediction metrics for evaluating both one-step and multi-step error propagation. In addition to the data, we provide a detailed description of the platform and experimental setup, as well as baseline models highlighting the challenge of accurate prediction under real-world noise and actuation nonlinearities. All data, scripts, and reference implementations are released as open-source at https://github.com/idsia-robotics/nanodrone-sysid-benchmark to facilitate transparent comparison of algorithms and support research on agile, miniaturized aerial robotics.", "AI": {"tldr": "The paper introduces a benchmark dataset from Crazyflie 2.1 nano-quadrotor for system identification. It includes multi-horizon prediction error metrics and baseline models, emphasizing challenges of nonlinear dynamics and real-world noise.", "motivation": "To create a standard benchmark for evaluating system identification techniques on complex, real-world aerial robotics platforms.", "method": "The authors recorded 75k samples from the Crazyflie 2.1 nano-quadrotor, involving synchronized motor inputs and multiple output measurements. They provide metrics for prediction evaluation, a detailed setup description, and baseline models.", "result": "The benchmark covers aggressive trajectories with extensive input-output data and offers open-source scripts and implementations for comparing identification methods.", "conclusion": "The benchmark aims to advance research in agile, miniaturized aerial robotics by providing standardized, open-source tools and datasets for system identification."}}
{"id": "2512.13886", "pdf": "https://arxiv.org/pdf/2512.13886", "abs": "https://arxiv.org/abs/2512.13886", "authors": ["Mohammad Mozaffari", "Samuel Kushnir", "Maryam Mehri Dehnavi", "Amir Yazdanbakhsh"], "title": "OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": null, "summary": "Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.", "AI": {"tldr": "OPTIMA is an efficient, one-shot post-training pruning method for large language models, improving accuracy without fine-tuning.", "motivation": "To address the challenges of balancing accuracy and scalability in post-training model pruning.", "method": "OPTIMA performs layer-wise weight reconstruction as independent, row-wise Quadratic Programs, leveraging shared Hessians for scalability.", "result": "OPTIMA enhances zero-shot performance on multiple LLM families achieving up to 3.97% accuracy improvement.", "conclusion": "OPTIMA sets a new standard for accuracy-efficiency trade-offs in one-shot post-training pruning, enabling scalable and effective model compression."}}
{"id": "2512.14052", "pdf": "https://arxiv.org/pdf/2512.14052", "abs": "https://arxiv.org/abs/2512.14052", "authors": ["HyperAI Team", "Yuchen Liu", "Kaiyang Han", "Zhiqiang Xia", "Yuhang Dong", "Chen Song", "Kangyu Tang", "Jiaming Xu", "Xiushi Feng", "WenXuan Yu", "Li Peng", "Mingyang Wang", "Kai Wang", "Changpeng Yang", "Yang Li", "Haoyu Lu", "Hao Wang", "Bingna Xu", "Guangyao Liu", "Long Huang", "Kaibin Guo", "Jinyang Wu", "Dan Wu", "Hongzhen Wang", "Peng Zhou", "Shuai Nie", "Shande Wang", "Runyu Shi", "Ying Huang"], "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices", "categories": ["cs.CV", "cs.CL"], "comment": "Technical report of Xiaomi HyperAI Team", "summary": "Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.", "AI": {"tldr": "HyperVL is a multimodal large language model designed for on-device inference, offering efficiency by reducing latency and memory requirements.", "motivation": "To address the challenges in deploying multimodal language models on-device due to high computational and memory needs of Vision Transformer (ViT) encoders.", "method": "Introduced HyperVL which incorporates an image-tiling strategy, a Visual Resolution Compressor (VRC) for adaptive resolution prediction, and Dual Consistency Learning (DCL) for aligning multi-scale ViT encoders.", "result": "HyperVL achieved state-of-the-art performance among similarly sized models and significantly reduced latency and power consumption on mobile devices.", "conclusion": "HyperVL demonstrates the feasibility of efficient, high-performing multimodal models for on-device usage, addressing prior limitations in memory and computational efficiency."}}
{"id": "2512.14491", "pdf": "https://arxiv.org/pdf/2512.14491", "abs": "https://arxiv.org/abs/2512.14491", "authors": ["Cheng-Han Lu", "Pei-Hsuan Tsai"], "title": "Sparse Multi-Modal Transformer with Masking for Alzheimer's Disease Classification", "categories": ["cs.AI"], "comment": "8 pages, 7 figures", "summary": "Transformer-based multi-modal intelligent systems often suffer from high computational and energy costs due to dense self-attention, limiting their scalability under resource constraints. This paper presents SMMT, a sparse multi-modal transformer architecture designed to improve efficiency and robustness. Building upon a cascaded multi-modal transformer framework, SMMT introduces cluster-based sparse attention to achieve near linear computational complexity and modality-wise masking to enhance robustness against incomplete inputs. The architecture is evaluated using Alzheimer's Disease classification on the ADNI dataset as a representative multi-modal case study. Experimental results show that SMMT maintains competitive predictive performance while significantly reducing training time, memory usage, and energy consumption compared to dense attention baselines, demonstrating its suitability as a resource-aware architectural component for scalable intelligent systems.", "AI": {"tldr": "The paper introduces SMMT, a sparse multi-modal transformer aimed at improving efficiency and robustness for multi-modal tasks, reducing computational costs while maintaining performance.", "motivation": "High computational and energy costs of transformer-based multi-modal systems hinder scalability under resource constraints.", "method": "SMMT employs cluster-based sparse attention to reduce computational complexity and modality-wise masking to improve robustness against incomplete data.", "result": "SMMT achieves competitive performance in Alzheimer's Disease classification while significantly lowering training time, memory use, and energy consumption.", "conclusion": "SMMT is a resource-efficient and robust transformer architecture suitable for scalable intelligent systems."}}
{"id": "2512.11880", "pdf": "https://arxiv.org/pdf/2512.11880", "abs": "https://arxiv.org/abs/2512.11880", "authors": ["Ioannis Kontoyiannis"], "title": "Shakespeare, Entropy and Educated Monkeys", "categories": ["math.HO", "cs.CL", "cs.IT"], "comment": null, "summary": "It has often been said, correctly, that a monkey forever randomly typing on a keyboard would eventually produce the complete works of William Shakespeare. Almost just as often it has been pointed out that this \"eventually\" is well beyond any conceivably relevant time frame. We point out that an educated monkey that still types at random but is constrained to only write \"statistically typical\" text, would produce any given passage in a much shorter time. Information theory gives a very simple way to estimate that time. For example, Shakespeare's phrase, Better three hours too soon than a minute too late, from The Merry Wives of Windsor, would take the educated monkey only 73 thousand years to produce, compared to the beyond-astronomical $2.7 \\times 10^{63}$ years for the randomly typing one. Despite the obvious improvement, it would still take the educated monkey an unimaginably long $10^{42,277}$ years to produce all of Hamlet.", "AI": {"tldr": "The paper contrasts the times it would take for a random monkey vs. an \"educated\" monkey (one typing statistically typical text) to produce Shakespeare's works using information theory.", "motivation": "To highlight the influence of statistical constraints and information theory on text generation, correcting misconceptions in the infinite monkey argument.", "method": "The study uses models leveraging statistical probabilities from information theory to assess the time required for text generation under different conditions.", "result": "An educated monkey constrained by statistical rules would generate Shakespeare's works orders of magnitude faster than a random one, but still require extraordinarily long durations.", "conclusion": "Applying statistical constraints drastically reduces timeframes but demonstrates the computational complexity even under more realistic approximations."}}
{"id": "2512.14696", "pdf": "https://arxiv.org/pdf/2512.14696", "abs": "https://arxiv.org/abs/2512.14696", "authors": ["Zihan Wang", "Jiashun Wang", "Jeff Tan", "Yiwen Zhao", "Jessica Hodgins", "Shubham Tulsiani", "Deva Ramanan"], "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives", "categories": ["cs.CV", "cs.GR", "cs.RO"], "comment": "Project page: https://crisp-real2sim.github.io/CRISP-Real2Sim/", "summary": "We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\\% to 6.9\\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.", "AI": {"tldr": "CRISP is a novel method to recover simulatable human motion and scene geometry from monocular video, achieving significant improvements in motion tracking reliability and reinforcement learning efficiency.", "motivation": "Existing methods for joint human-scene reconstruction suffer from noisy geometry and lack precise, physics-based modeling, leading to unreliable simulation environments.", "method": "CRISP uses convex and clean geometry fitting through a clustering pipeline and human-scene contact modeling to reconstruct occluded objects. It ensures physical plausibility through reinforcement learning-driven humanoid controllers.", "result": "CRISP reduces motion tracking failures from 55.2% to 6.9% on benchmarks while enhancing RL simulation throughput by 43%. It works robustly on diverse video sources, including in-the-wild videos.", "conclusion": "CRISP advances real-to-sim applications for robotics and AR/VR by enabling scalable and physically-valid human motion and interaction environments."}}
{"id": "2512.13898", "pdf": "https://arxiv.org/pdf/2512.13898", "abs": "https://arxiv.org/abs/2512.13898", "authors": ["Rachit Bansal", "Aston Zhang", "Rishabh Tiwari", "Lovish Madaan", "Sai Surya Duvvuri", "Devvrit Khatri", "David Brandfonbrener", "David Alvarez-Melis", "Prajjwal Bhargava", "Mihir Sanjay Kale", "Samy Jelassi"], "title": "Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.", "AI": {"tldr": "The paper challenges current methods for handling long-context tasks in language models, revealing limitations and introducing a new gradient-based approach that improves performance significantly on benchmarks.", "motivation": "Current long-context LLMs consume vast text amounts but face challenges in effectively utilizing it. Additionally, inference-time methods for improving multi-step reasoning often fail in long context due to inherent limitations like score dilution in static self-attention.", "method": "The authors conducted controlled experiments and identified score dilution as a key obstacle in long contexts. They introduced a novel, targeted gradient update method during inference to address issues in retrieving relevant signals within long contexts.", "result": "The proposed method delivered substantial improvements of 12.6% and 14.1% on two benchmark datasets (LongBench-v2 and ZeroScrolls), proving its effectiveness across tasks and models.", "conclusion": "Inference-time compute should prioritize small context-specific training over current scaling strategies, offering practical solutions to enhance long-context LLM performance."}}
{"id": "2512.14056", "pdf": "https://arxiv.org/pdf/2512.14056", "abs": "https://arxiv.org/abs/2512.14056", "authors": ["Kim Sung-Bin", "Joohyun Chang", "David Harwath", "Tae-Hyun Oh"], "title": "FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://facedit.github.io/", "summary": "Talking face editing and face generation have often been studied as distinct problems. In this work, we propose viewing both not as separate tasks but as subtasks of a unifying formulation, speech-conditional facial motion infilling. We explore facial motion infilling as a self-supervised pretext task that also serves as a unifying formulation of dynamic talking face synthesis. To instantiate this idea, we propose FacEDiT, a speech-conditional Diffusion Transformer trained with flow matching. Inspired by masked autoencoders, FacEDiT learns to synthesize masked facial motions conditioned on surrounding motions and speech. This formulation enables both localized generation and edits, such as substitution, insertion, and deletion, while ensuring seamless transitions with unedited regions. In addition, biased attention and temporal smoothness constraints enhance boundary continuity and lip synchronization. To address the lack of a standard editing benchmark, we introduce FacEDiTBench, the first dataset for talking face editing, featuring diverse edit types and lengths, along with new evaluation metrics. Extensive experiments validate that talking face editing and generation emerge as subtasks of speech-conditional motion infilling; FacEDiT produces accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity while generalizing effectively to talking face generation.", "AI": {"tldr": "This paper introduces FacEDiT, a framework that unifies talking face editing and generation tasks using a speech-conditional facial motion infilling approach. It employs diffusion transformers and introduces a new dataset, FacEDiTBench.", "motivation": "Talking face editing and generation are commonly treated as separate tasks. This work aims to unify them as part of a single speech-conditional facial motion infilling problem to improve dynamic talking face synthesis.", "method": "FacEDiT uses a speech-conditional Diffusion Transformer, inspired by masked autoencoders. It learns to synthesize masked facial motions using contexts from surrounding motions and speech. It incorporates biased attention, temporal smoothness constraints, and utilizes a newly introduced benchmark, FacEDiTBench, for evaluation.", "result": "Extensive experiments demonstrate that FacEDiT successfully combines talking face editing and generation. It maintains strong identity preservation, produces speech-aligned edits, achieves smooth visual transitions, and generalizes effectively.", "conclusion": "FacEDiT serves as a unified framework for talking face editing and generation, presenting significant advancements in accurate and seamless facial motion synthesis."}}
{"id": "2512.14527", "pdf": "https://arxiv.org/pdf/2512.14527", "abs": "https://arxiv.org/abs/2512.14527", "authors": ["Shreyas Subramanian", "Bala Krishnamoorthy", "Pranav Murthy"], "title": "Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence", "categories": ["cs.AI"], "comment": null, "summary": "Despite significant advances in optimizers for training, most research works use common scheduler choices like Cosine or exponential decay. In this paper, we study \\emph{GreedyLR}, a novel scheduler that adaptively adjusts the learning rate during training based on the current loss. To validate the effectiveness of our proposed scheduler, we conduct experiments on several NLP, CV, and LLM tasks with up to $7B$ parameters, including both fine-tuning and pre-training experiments. The results show that our approach outperforms several state-of-the-art schedulers in terms of accuracy, speed, and convergence. We also provide a theoretical analysis of the GreedyLR algorithm, including a proof of convergence and derivation of the optimal scaling factor $F$ that maximizes the convergence rate, along with experiments to show robustness of the algorithm to realistic noisy landscapes. Our scheduler is easy to implement, computationally efficient, and could be considered a good default scheduler for training.", "AI": {"tldr": "This paper introduces GreedyLR, a learning rate scheduler tailored to adapt based on loss during training, showcasing superior performance over existing methods.", "motivation": "Investigate and improve upon learning rate schedulers commonly used in training deep learning models, providing adaptivity and efficiency.", "method": "The authors introduce GreedyLR, a scheduler that adjusts the learning rate based on the loss during training. They validate its performance with various experiments and provide theoretical backing.", "result": "GreedyLR proves to outperform existing schedulers in accuracy, speed, and convergence across various NLP, CV, and LLM tasks.", "conclusion": "GreedyLR is effective, robust, and computationally efficient, making it a potential default choice for training workflows."}}
{"id": "2512.13697", "pdf": "https://arxiv.org/pdf/2512.13697", "abs": "https://arxiv.org/abs/2512.13697", "authors": ["Vivan Doshi", "Mengyuan Li"], "title": "Writing in Symbiosis: Mapping Human Creative Agency in the AI Era", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "Advances in Neural Information Processing Systems (NeurIPS 2025)", "summary": "The proliferation of Large Language Models (LLMs) raises a critical question about what it means to be human when we share an increasingly symbiotic relationship with persuasive and creative machines. This paper examines patterns of human-AI coevolution in creative writing, investigating how human craft and agency are adapting alongside machine capabilities. We challenge the prevailing notion of stylistic homogenization by examining diverse patterns in longitudinal writing data. Using a large-scale corpus spanning the pre- and post-LLM era, we observe patterns suggestive of a \"Dual-Track Evolution\": thematic convergence around AI-related topics, coupled with structured stylistic differentiation. Our analysis reveals three emergent adaptation patterns: authors showing increased similarity to AI style, those exhibiting decreased similarity, and those maintaining stylistic stability while engaging with AI-related themes. This Creative Archetype Map illuminates how authorship is coevolving with AI, contributing to discussions about human-AI collaboration, detection challenges, and the preservation of creative diversity.", "AI": {"tldr": "This paper explores human-AI coevolution in creative writing, analyzing longitudinal patterns to identify thematic convergence around AI topics and distinct stylistic adaptations.", "motivation": "The paper seeks to understand how human creativity and agency adapt in coexistence with advancing AI, specifically challenging concerns about homogenized writing styles.", "method": "The authors analyzed a large-scale corpus spanning pre- and post-LLM eras, identifying patterns of thematic convergence and stylistic differentiation in writing.", "result": "Three adaptation patterns emerged: some authors aligned their style closer to AI, others diverged more, and a third group maintained stylistic consistency while engaging AI-related themes.", "conclusion": "Human creativity evolves dynamically alongside AI, demonstrated by diverse stylistic adaptations and thematic changes, fostering discussions on AI collaboration and creative diversity."}}
{"id": "2512.13910", "pdf": "https://arxiv.org/pdf/2512.13910", "abs": "https://arxiv.org/abs/2512.13910", "authors": ["Matheus Corr\u00eaa Domingos", "Valdivino Alexandre de Santiago J\u00fanior", "Juliana Aparecida Anochi", "Elcio Hideiti Shiguemori", "Lu\u00edsa Mirelle Costa dos Santos", "H\u00e9rcules Carlos dos Santos Pereira", "Andr\u00e9 Estevam Costa Oliveira"], "title": "Exploring Machine Learning, Deep Learning, and Explainable AI Methods for Seasonal Precipitation Prediction in South America", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Forecasting meteorological variables is challenging due to the complexity of their processes, requiring advanced models for accuracy. Accurate precipitation forecasts are vital for society. Reliable predictions help communities mitigate climatic impacts. Based on the current relevance of artificial intelligence (AI), classical machine learning (ML) and deep learning (DL) techniques have been used as an alternative or complement to dynamic modeling. However, there is still a lack of broad investigations into the feasibility of purely data-driven approaches for precipitation forecasting. This study aims at addressing this issue where different classical ML and DL approaches for forecasting precipitation in South America, taking into account all 2019 seasons, are considered in a detailed investigation. The selected classical ML techniques were Random Forests and extreme gradient boosting (XGBoost), while the DL counterparts were a 1D convolutional neural network (CNN 1D), a long short-term memory (LSTM) model, and a gated recurrent unit (GRU) model. Additionally, the Brazilian Global Atmospheric Model (BAM) was used as a representative of the traditional dynamic modeling approach. We also relied on explainable artificial intelligence (XAI) to provide some explanations for the models behaviors. LSTM showed strong predictive performance while BAM, the traditional dynamic model representative, had the worst results. Despite presented the higher latency, LSTM was most accurate for heavy precipitation. If cost is a concern, XGBoost offers lower latency with slightly accuracy loss. The results of this research confirm the viability of DL models for climate forecasting, solidifying a global trend in major meteorological and climate forecasting centers.", "AI": {"tldr": "This paper investigates various machine learning (ML) and deep learning (DL) models for precipitation forecasting in South America, finding that DL models, particularly LSTM, outperform traditional dynamic modeling methods.", "motivation": "Accurately forecasting precipitation is crucial due to its societal and environmental importance, but existing methods lack comprehensive research into the potential of purely data-driven approaches.", "method": "The study compares classical ML (Random Forests, XGBoost) and DL (1D CNN, LSTM, GRU) models against the Brazilian Global Atmospheric Model (BAM). Explainable AI methods were also used to analyze model behavior.", "result": "LSTM exhibited the best predictive performance, especially for heavy precipitation, despite having higher latency, while BAM performed the worst. XGBoost was more efficient but slightly less accurate.", "conclusion": "DL models, particularly LSTM, are effective for climate forecasting, with this research affirming their global applicability and superiority over traditional methods."}}
{"id": "2512.14058", "pdf": "https://arxiv.org/pdf/2512.14058", "abs": "https://arxiv.org/abs/2512.14058", "authors": ["Zulin Zhuang", "Yu Bian"], "title": "Real-time prediction of workplane illuminance distribution for daylight-linked controls using non-intrusive multimodal deep learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Daylight-linked controls (DLCs) have significant potential for energy savings in buildings, especially when abundant daylight is available and indoor workplane illuminance can be accurately predicted in real time. Most existing studies on indoor daylight predictions were developed and tested for static scenes. This study proposes a multimodal deep learning framework that predicts indoor workplane illuminance distributions in real time from non-intrusive images with temporal-spatial features. By extracting image features only from the side-lit window areas rather than interior pixels, the approach remains applicable in dynamically occupied indoor spaces. A field experiment was conducted in a test room in Guangzhou (China), where 17,344 samples were collected for model training and validation. The model achieved R2 > 0.98 with RMSE < 0.14 on the same-distribution test set and R2 > 0.82 with RMSE < 0.17 on an unseen-day test set, indicating high accuracy and acceptable temporal generalization.", "AI": {"tldr": "The study introduces a multimodal deep learning framework to predict real-time indoor illuminance distributions accurately, suitable for dynamically occupied spaces.", "motivation": "To leverage daylight-linked controls (DLCs) for significant energy savings by accurately predicting indoor illuminance in real time, addressing conventional limitations in dynamic indoor settings.", "method": "A multimodal deep learning model was developed using non-intrusive image-based temporal-spatial features, focusing only on side-lit window areas. Field experiments were conducted in Guangzhou with 17,344 samples for training and testing.", "result": "The model achieved high accuracy (R2 > 0.98 and RMSE < 0.14 on same-distribution tests; R2 > 0.82 and RMSE < 0.17 on unseen-day tests), demonstrating reliable temporal generalization.", "conclusion": "The proposed deep learning framework provides accurate, real-time predictions of indoor illuminance, enabling efficient daylight-linked control in dynamically occupied spaces."}}
{"id": "2512.14693", "pdf": "https://arxiv.org/pdf/2512.14693", "abs": "https://arxiv.org/abs/2512.14693", "authors": ["Zitian Gao", "Lynx Chen", "Yihao Xiao", "He Xing", "Ran Tao", "Haoming Luo", "Joey Zhou", "Bryan Dai"], "title": "Universal Reasoning Model", "categories": ["cs.AI"], "comment": null, "summary": "Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.", "AI": {"tldr": "The paper investigates Universal Transformers, finding that recurrent bias and nonlinear components drive their performance in reasoning tasks. Introducing Universal Reasoning Model (URM), the authors achieve state-of-the-art improvements.", "motivation": "To uncover what drives performance gains in Universal Transformers on reasoning tasks like ARC-AGI and apply these insights to improve them.", "method": "Systematically analyze Universal Transformer variants, identifying key performance sources, and then design the Universal Reasoning Model (URM) with short convolution and truncated backpropagation.", "result": "URM achieves state-of-the-art results, including 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2.", "conclusion": "Performance in Universal Transformers is driven by recurrent inductive bias and nonlinear components. URM leverages this understanding for notable reasoning task improvements."}}
{"id": "2512.13913", "pdf": "https://arxiv.org/pdf/2512.13913", "abs": "https://arxiv.org/abs/2512.13913", "authors": ["Patrick Egenlauf", "Iva B\u0159ezinov\u00e1", "Sabine Andergassen", "Miriam Klopotek"], "title": "Capturing reduced-order quantum many-body dynamics out of equilibrium via neural ordinary differential equations", "categories": ["cs.LG", "cond-mat.stat-mech", "quant-ph"], "comment": null, "summary": "Out-of-equilibrium quantum many-body systems exhibit rapid correlation buildup that underlies many emerging phenomena. Exact wave-function methods to describe this scale exponentially with particle number; simpler mean-field approaches neglect essential two-particle correlations. The time-dependent two-particle reduced density matrix (TD2RDM) formalism offers a middle ground by propagating the two-particle reduced density matrix (2RDM) and closing the BBGKY hierarchy with a reconstruction of the three-particle cumulant. But the validity and existence of time-local reconstruction functionals ignoring memory effects remain unclear across different dynamical regimes. We show that a neural ODE model trained on exact 2RDM data (no dimensionality reduction) can reproduce its dynamics without any explicit three-particle information -- but only in parameter regions where the Pearson correlation between the two- and three-particle cumulants is large. In the anti-correlated or uncorrelated regime, the neural ODE fails, indicating that no simple time-local functional of the instantaneous two-particle cumulant can capture the evolution. The magnitude of the time-averaged three-particle-correlation buildup appears to be the primary predictor of success: For a moderate correlation buildup, both neural ODE predictions and existing TD2RDM reconstructions are accurate, whereas stronger values lead to systematic breakdowns. These findings pinpoint the need for memory-dependent kernels in the three-particle cumulant reconstruction for the latter regime. Our results place the neural ODE as a model-agnostic diagnostic tool that maps the regime of applicability of cumulant expansion methods and guides the development of non-local closure schemes. More broadly, the ability to learn high-dimensional RDM dynamics from limited data opens a pathway to fast, data-driven simulation of correlated quantum matter.", "AI": {"tldr": "Investigates the validity of time-local two-particle reduced density matrix reconstructions in quantum dynamics using neural ODE models.", "motivation": "Accurately describing correlation buildup in out-of-equilibrium quantum systems is challenging due to the exponential scaling of exact methods and limitations of mean-field approaches.", "method": "Utilizes a neural ODE model trained on exact two-particle reduced density matrix data to assess the reconstruction's effectiveness in different dynamical regimes.", "result": "Neural ODE models successfully reproduce dynamics in regions with strong two- and three-particle correlations, but fail in uncorrelated/anti-correlated regimes. Highlights the necessity for memory-dependent reconstruction approaches.", "conclusion": "Neural ODE models serve as diagnostic tools for cumulant expansion applicability, supporting development of data-driven simulations for correlated quantum systems and emphasizing the importance of non-local kernels for strong correlation regimes."}}
{"id": "2512.14061", "pdf": "https://arxiv.org/pdf/2512.14061", "abs": "https://arxiv.org/abs/2512.14061", "authors": ["Hao Chen", "Junyang Chen", "Jinshan Pan", "Jiangxin Dong"], "title": "Bridging Fidelity-Reality with Controllable One-Step Diffusion for Image Super-Resolution", "categories": ["cs.CV"], "comment": "Project page: https://github.com/Chanson94/CODSR", "summary": "Recent diffusion-based one-step methods have shown remarkable progress in the field of image super-resolution, yet they remain constrained by three critical limitations: (1) inferior fidelity performance caused by the information loss from compression encoding of low-quality (LQ) inputs; (2) insufficient region-discriminative activation of generative priors; (3) misalignment between text prompts and their corresponding semantic regions. To address these limitations, we propose CODSR, a controllable one-step diffusion network for image super-resolution. First, we propose an LQ-guided feature modulation module that leverages original uncompressed information from LQ inputs to provide high-fidelity conditioning for the diffusion process. We then develop a region-adaptive generative prior activation method to effectively enhance perceptual richness without sacrificing local structural fidelity. Finally, we employ a text-matching guidance strategy to fully harness the conditioning potential of text prompts. Extensive experiments demonstrate that CODSR achieves superior perceptual quality and competitive fidelity compared with state-of-the-art methods with efficient one-step inference.", "AI": {"tldr": "This paper introduces CODSR, a method to enhance image super-resolution by overcoming fidelity, generative prior, and prompt-semantic alignment limitations, achieving superior perceptual quality.", "motivation": "Address the limitations in diffusion-based one-step image super-resolution methods, specifically fidelity loss, insufficient generative prior activation, and text-prompt misalignment.", "method": "Proposes CODSR with three key components: LQ-guided feature modulation for better fidelity, region-adaptive prior activation for perceptual richness, and text-matching guidance for accurate semantic alignment.", "result": "CODSR demonstrates superior perceptual quality and competitive fidelity compared to state-of-the-art methods through efficient one-step inference.", "conclusion": "The proposed CODSR method effectively resolves critical shortcomings in image super-resolution, providing enhanced fidelity, perceptual richness, and semantic alignment in one-step diffusion processes."}}
{"id": "2511.10333", "pdf": "https://arxiv.org/pdf/2511.10333", "abs": "https://arxiv.org/abs/2511.10333", "authors": ["Qingao Yi", "Jiaang Duan", "Hanwen Hu", "Qin Hua", "Haiyan Zhao", "Shiyou Qian", "Dingyu Yang", "Jian Cao", "Jinghua Tang", "Yinghao Yu", "Chenzhi Liao", "Kangjin Wang", "Liping Zhang"], "title": "EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": null, "summary": "Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static gradient compression to enhance communication efficiency; however, these methods neglect the dynamic nature of evolving gradients during training, leading to performance degradation. Accelerating LLM training via compression without sacrificing performance remains a challenge. In this paper, we propose an entropy-driven dynamic gradient compression framework called EDGC. The core concept is to adjust the compression rate during LLM training based on the evolving trends of gradient entropy, taking into account both compression efficiency and error. EDGC consists of three key components.First, it employs a down-sampling method to efficiently estimate gradient entropy, reducing computation overhead. Second, it establishes a theoretical model linking compression rate with gradient entropy, enabling more informed compression decisions. Lastly, a window-based adjustment mechanism dynamically adapts the compression rate across pipeline stages, improving communication efficiency and maintaining model performance. We implemented EDGC on a 32-NVIDIA-V100 cluster and a 64-NVIDIA-H100 cluster to train GPT2-2.5B and GPT2-12.1B, respectively. The results show that EDGC significantly reduces communication latency and training time by up to 46.45% and 16.13% while preserving LLM accuracy.", "AI": {"tldr": "The paper introduces EDGC, a dynamic gradient compression framework addressing communication overhead in training large language models, improving efficiency without degrading performance.", "motivation": "To address the significant computational and memory challenges in training large language models, and improve communication efficiency during distributed training while maintaining model accuracy.", "method": "The proposed framework employs a dynamic compression adjustment system based on evolving gradient entropy. It consists of components for estimating gradient entropy, a theoretical model linking entropy and compression rate, and a window-based adjustment mechanism for adaptability.", "result": "EDGC successfully reduces communication latency by up to 46.45% and training time by 16.13%. Experiments on NVIDIA GPU clusters show it preserves LLM accuracy.", "conclusion": "EDGC addresses the challenge of accelerating LLM training by dynamically adapting compression rates, demonstrating notable efficiency gains while maintaining model quality."}}
{"id": "2512.13919", "pdf": "https://arxiv.org/pdf/2512.13919", "abs": "https://arxiv.org/abs/2512.13919", "authors": ["Eugenio Varetti", "Matteo Torzoni", "Marco Tezzele", "Andrea Manzoni"], "title": "Adaptive digital twins for predictive decision-making: Online Bayesian learning of transition dynamics", "categories": ["cs.LG", "math.NA"], "comment": null, "summary": "This work shows how adaptivity can enhance value realization of digital twins in civil engineering. We focus on adapting the state transition models within digital twins represented through probabilistic graphical models. The bi-directional interaction between the physical and virtual domains is modeled using dynamic Bayesian networks. By treating state transition probabilities as random variables endowed with conjugate priors, we enable hierarchical online learning of transition dynamics from a state to another through effortless Bayesian updates. We provide the mathematical framework to account for a larger class of distributions with respect to the current literature. To compute dynamic policies with precision updates we solve parametric Markov decision processes through reinforcement learning. The proposed adaptive digital twin framework enjoys enhanced personalization, increased robustness, and improved cost-effectiveness. We assess our approach on a case study involving structural health monitoring and maintenance planning of a railway bridge.", "AI": {"tldr": "Explores an adaptive digital twin framework in civil engineering, emphasizing probabilistic graphical models and reinforcement learning for better personalization and reliability.", "motivation": "Improve value realization of digital twins in civil engineering through enhanced adaptivity.", "method": "Utilizes dynamic Bayesian networks for state transition modeling, Bayesian updates for learning transition dynamics, and reinforcement learning to solve Markov decision processes.", "result": "The framework showcases enhanced personalization, robustness, and cost-effectiveness in structural health monitoring and planning.", "conclusion": "Adaptive digital twins with probabilistic modeling and reinforcement learning can significantly advance civil engineering practices in terms of efficiency and reliability."}}
{"id": "2512.14068", "pdf": "https://arxiv.org/pdf/2512.14068", "abs": "https://arxiv.org/abs/2512.14068", "authors": ["Shuang Cheng", "Yuhua Jiang", "Zineng Zhou", "Dawei Liu", "Wang Tao", "Linfeng Zhang", "Biqing Qi", "Bowen Zhou"], "title": "SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Block-wise discrete diffusion offers an attractive balance between parallel generation and causal dependency modeling, making it a promising backbone for vision-language modeling. However, its practical adoption has been limited by high training cost, slow convergence, and instability, which have so far kept it behind strong autoregressive (AR) baselines. We present \\textbf{SDAR-VL}, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU), together with an \\emph{integrated framework for efficient and stable training}. This framework unifies three components: (1) \\textbf{Asynchronous Block-wise Noise Scheduling} to diversify supervision within each batch; (2) \\textbf{Effective Mask Ratio Scaling} for unbiased loss normalization under stochastic masking; and (3) a \\textbf{Progressive Beta Noise Curriculum} that increases effective mask coverage while preserving corruption diversity. Experiments on 21 single-image, multi-image, and video benchmarks show that SDAR-VL consistently improves \\emph{training efficiency}, \\emph{convergence stability}, and \\emph{task performance} over conventional block diffusion. On this evaluation suite, SDAR-VL sets a new state of the art among diffusion-based vision-language models and, under matched settings, matches or surpasses strong AR baselines such as LLaVA-OneVision as well as the global diffusion baseline LLaDA-V, establishing block-wise diffusion as a practical backbone for VLU.", "AI": {"tldr": "SDAR-VL significantly enhances block-wise discrete diffusion for efficient and stable vision-language understanding, surpassing existing AR and diffusion baselines.", "motivation": "High training costs, slow convergence, and stability issues limit the adoption of block-wise discrete diffusion for vision-language modeling.", "method": "SDAR-VL introduces a novel training framework combining asynchronous block-wise noise scheduling, effective mask ratio scaling, and a progressive beta noise curriculum.", "result": "Experiments on 21 benchmarks demonstrate SDAR-VL improves efficiency, stability, and performance, achieving state-of-the-art results in diffusion-based models and competing with autoregressive baselines.", "conclusion": "Block-wise discrete diffusion can be effectively leveraged for vision-language understanding through SDAR-VL, making it a viable alternative to autoregressive models."}}
{"id": "2511.19317", "pdf": "https://arxiv.org/pdf/2511.19317", "abs": "https://arxiv.org/abs/2511.19317", "authors": ["Md. Tanzim Ferdous", "Naeem Ahsan Chowdhury", "Prithwiraj Bhattacharjee"], "title": "MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.", "AI": {"tldr": "The paper introduces a multi-domain Bangla abstractive summarization dataset with over 54,000 articles and summaries, tested with deep learning models.", "motivation": "To address the lack of adaptable summarization systems for the diverse and massive Bangla content produced in today's digital era.", "method": "Created a multi-domain dataset of Bangla articles and summaries from diverse sources, and evaluated it using models like LSTM, BanglaT5-small, and MTS-small.", "result": "Demonstrated the dataset's potential as a benchmark for Bangla summarization research with strong baseline performances from transfer learning models.", "conclusion": "The dataset advances Bangla NLP, providing a valuable resource for building adaptable summarization systems and aiding low-resource language research."}}
{"id": "2512.13921", "pdf": "https://arxiv.org/pdf/2512.13921", "abs": "https://arxiv.org/abs/2512.13921", "authors": ["Dragos Secrieru", "Garyk Brixi", "Yoshua Bengio", "Taiji Suzuki", "Michael Poli", "Stefano Massaroli"], "title": "Sliding Window Recurrences for Sequence Models", "categories": ["cs.LG"], "comment": null, "summary": "Multi-hybrid architectures are poised to take over language modeling due to better quality and performance. We introduce a hierarchical decomposition framework for linear recurrences that allows us to develop algorithms aligned with GPU memory hierarchies, yielding Sliding Window Recurrences. We focus specifically on truncating recurrences to hardware-aligned windows which are naturally jagged, limiting costly inter-warp communication. Using SWR, we develop Phalanx layers that serve as drop-in replacements for windowed attention or linear recurrences. In 1B parameter multi-hybrid models, Phalanx achieves over 10-40% speedup across 4K to 32K context length over optimized Transformers while matching perplexity.", "AI": {"tldr": "The paper presents Sliding Window Recurrences (SWR) and Phalanx layers, improving language modeling performance and speed in multi-hybrid architectures.", "motivation": "To enhance the efficiency and quality of language models by leveraging hierarchical memory decomposition aligned with hardware architectures.", "method": "Hierarchical decomposition framework for truncating recurrences, enabling hardware-aligned windowed computations with reduced communication overhead. Phalanx layers are introduced as replacements within multi-hybrid models.", "result": "Phalanx layers deliver a 10-40% speedup in multi-hybrid models at high context lengths (4K to 32K) compared to optimized Transformers while maintaining model perplexity.", "conclusion": "The proposed SWR and Phalanx layers improve computation efficiency without compromising model performance, contributing significantly to multi-hybrid language model optimization."}}
{"id": "2512.14087", "pdf": "https://arxiv.org/pdf/2512.14087", "abs": "https://arxiv.org/abs/2512.14087", "authors": ["Yang Yang", "Risa Shinoda", "Hiroaki Santo", "Fumio Okura"], "title": "GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants", "categories": ["cs.CV"], "comment": "Submitted to IEEE TPAMI, under review", "summary": "We present a method for jointly recovering the appearance and internal structure of botanical plants from multi-view images based on 3D Gaussian Splatting (3DGS). While 3DGS exhibits robust reconstruction of scene appearance for novel-view synthesis, it lacks structural representations underlying those appearances (e.g., branching patterns of plants), which limits its applicability to tasks such as plant phenotyping. To achieve both high-fidelity appearance and structural reconstruction, we introduce GaussianPlant, a hierarchical 3DGS representation, which disentangles structure and appearance. Specifically, we employ structure primitives (StPs) to explicitly represent branch and leaf geometry, and appearance primitives (ApPs) to the plants' appearance using 3D Gaussians. StPs represent a simplified structure of the plant, i.e., modeling branches as cylinders and leaves as disks. To accurately distinguish the branches and leaves, StP's attributes (i.e., branches or leaves) are optimized in a self-organized manner. ApPs are bound to each StP to represent the appearance of branches or leaves as in conventional 3DGS. StPs and ApPs are jointly optimized using a re-rendering loss on the input multi-view images, as well as the gradient flow from ApP to StP using the binding correspondence information. We conduct experiments to qualitatively evaluate the reconstruction accuracy of both appearance and structure, as well as real-world experiments to qualitatively validate the practical performance. Experiments show that the GaussianPlant achieves both high-fidelity appearance reconstruction via ApPs and accurate structural reconstruction via StPs, enabling the extraction of branch structure and leaf instances.", "AI": {"tldr": "This paper introduces GaussianPlant, a method that recovers both the appearance (via appearance primitives) and structure (via structure primitives) of plants from multi-view images, overcoming the limitations of standard 3D Gaussian Splatting methods.", "motivation": "The authors aim to address the limitation of 3D Gaussian Splatting in reconstructing internal plant structures like branching patterns, which are essential for applications such as plant phenotyping.", "method": "GaussianPlant is a hierarchical 3D Gaussian Splatting representation. It uses structure primitives (StPs) for modeling plant geometry such as branches and leaves, and appearance primitives (ApPs) for capturing detailed appearances. These are optimized together via self-organization and re-rendering loss.", "result": "The method achieves high-fidelity appearance reconstruction through ApPs and accurate structural reconstruction through StPs. It successfully extracts branch structures and leaf instances from multi-view images.", "conclusion": "GaussianPlant effectively disentangles and reconstructs both the structural and visual attributes of plants, making it a practical tool for tasks like accurate plant phenotyping."}}
{"id": "2512.13927", "pdf": "https://arxiv.org/pdf/2512.13927", "abs": "https://arxiv.org/abs/2512.13927", "authors": ["Sophia Tang"], "title": "A Complete Guide to Spherical Equivariant Graph Transformers", "categories": ["cs.LG", "q-bio.QM"], "comment": "This paper is a technical version of the article originally published in Alchemy Bio (99 pages, 46 figures)", "summary": "Spherical equivariant graph neural networks (EGNNs) provide a principled framework for learning on three-dimensional molecular and biomolecular systems, where predictions must respect the rotational symmetries inherent in physics. These models extend traditional message-passing GNNs and Transformers by representing node and edge features as spherical tensors that transform under irreducible representations of the rotation group SO(3), ensuring that predictions change in physically meaningful ways under rotations of the input. This guide develops a complete, intuitive foundation for spherical equivariant modeling - from group representations and spherical harmonics, to tensor products, Clebsch-Gordan decomposition, and the construction of SO(3)-equivariant kernels. Building on this foundation, we construct the Tensor Field Network and SE(3)-Transformer architectures and explain how they perform equivariant message-passing and attention on geometric graphs. Through clear mathematical derivations and annotated code excerpts, this guide serves as a self-contained introduction for researchers and learners seeking to understand or implement spherical EGNNs for applications in chemistry, molecular property prediction, protein structure modeling, and generative modeling.", "AI": {"tldr": "The paper discusses spherical equivariant graph neural networks (EGNNs), emphasizing their importance in 3D molecular systems by ensuring rotational symmetries are respected.", "motivation": "To provide a principled framework for modeling three-dimensional molecular systems that respect rotational symmetries.", "method": "The paper develops spherical equivariant modeling using group representations, spherical harmonics, and constructs architectures like Tensor Field Network and SE(3)-Transformer for equivariant message-passing.", "result": "The result is a thorough foundation and explanation for building and understanding spherical EGNNs applicable to molecular modeling and prediction.", "conclusion": "The guide serves as an introduction for implementing spherical EGNNs in various scientific fields, with clear mathematical derivations and practical use cases."}}
{"id": "2512.14092", "pdf": "https://arxiv.org/pdf/2512.14092", "abs": "https://arxiv.org/abs/2512.14092", "authors": ["Felix Holm", "Ghazal Ghazaei", "Nassir Navab"], "title": "ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Purpose: Detailed surgical recognition is critical for advancing AI-assisted surgery, yet progress is hampered by high annotation costs, data scarcity, and a lack of interpretable models. While scene graphs offer a structured abstraction of surgical events, their full potential remains untapped. In this work, we introduce ProtoFlow, a novel framework that learns dynamic scene graph prototypes to model complex surgical workflows in an interpretable and robust manner.\n  Methods: ProtoFlow leverages a graph neural network (GNN) encoder-decoder architecture that combines self-supervised pretraining for rich representation learning with a prototype-based fine-tuning stage. This process discovers and refines core prototypes that encapsulate recurring, clinically meaningful patterns of surgical interaction, forming an explainable foundation for workflow analysis.\n  Results: We evaluate our approach on the fine-grained CAT-SG dataset. ProtoFlow not only outperforms standard GNN baselines in overall accuracy but also demonstrates exceptional robustness in limited-data, few-shot scenarios, maintaining strong performance when trained on as few as one surgical video. Our qualitative analyses further show that the learned prototypes successfully identify distinct surgical sub-techniques and provide clear, interpretable insights into workflow deviations and rare complications.\n  Conclusion: By uniting robust representation learning with inherent explainability, ProtoFlow represents a significant step toward developing more transparent, reliable, and data-efficient AI systems, accelerating their potential for clinical adoption in surgical training, real-time decision support, and workflow optimization.", "AI": {"tldr": "The paper introduces ProtoFlow, a novel framework using dynamic scene graph prototypes for surgical workflow analysis, excelling in performance and interpretability.", "motivation": "The paper addresses challenges in surgical AI, including high annotation costs, data scarcity, and lack of interpretable models, aiming to create a framework that captures structured and interpretable abstractions of surgical events.", "method": "The study employs a GNN encoder-decoder architecture with self-supervised pretraining for representation learning and prototype-based fine-tuning to identify meaningful surgical patterns.", "result": "ProtoFlow surpasses conventional GNNs in accuracy, demonstrates robustness in limited-data scenarios, and captures identifiable surgical sub-techniques and deviations.", "conclusion": "ProtoFlow provides interpretable and robust solutions for AI-assisted surgery, advancing applications in surgical training, decision support, and workflow optimization."}}
{"id": "2512.13702", "pdf": "https://arxiv.org/pdf/2512.13702", "abs": "https://arxiv.org/abs/2512.13702", "authors": ["A. Anil Sinaci", "Senan Postaci", "Dogukan Cavdaroglu", "Machteld J. Boonstra", "Okan Mercan", "Kerem Yilmaz", "Gokce B. Laleci Erturkmen", "Folkert W. Asselbergs", "Karim Lekadir"], "title": "Enhancing Transparency and Traceability in Healthcare AI: The AI Product Passport", "categories": ["cs.CY", "cs.AI"], "comment": "A total of 33 pages: First 16 pages for the manuscript and the remaining 17 pages for the supplementary user guide of the graphical user interface", "summary": "Objective: To develop the AI Product Passport, a standards-based framework improving transparency, traceability, and compliance in healthcare AI via lifecycle-based documentation. Materials and Methods: The AI Product Passport was developed within the AI4HF project, focusing on heart failure AI tools. We analyzed regulatory frameworks (EU AI Act, FDA guidelines) and existing standards to design a relational data model capturing metadata across AI lifecycle phases: study definition, dataset preparation, model generation/evaluation, deployment/monitoring, and passport generation. MLOps/ModelOps concepts were integrated for operational relevance. Co-creation involved feedback from AI4HF consortium and a Lisbon workshop with 21 diverse stakeholders, evaluated via Mentimeter polls. The open-source platform was implemented with Python libraries for automated provenance tracking. Results: The AI Product Passport was designed based on existing standards and methods with well-defined lifecycle management and role-based access. Its implementation is a web-based platform with a relational data model supporting auditable documentation. It generates machine- and human-readable reports, customizable for stakeholders. It aligns with FUTURE-AI principles (Fairness, Universality, Traceability, Usability, Robustness, Explainability), ensuring fairness, traceability, and usability. Exported passports detail model purpose, data provenance, performance, and deployment context. GitHub-hosted backend/frontend codebases enhance accessibility. Discussion and Conclusion: The AI Product Passport addresses transparency gaps in healthcare AI, meeting regulatory and ethical demands. Its open-source nature and alignment with standards foster trust and adaptability. Future enhancements include FAIR data principles and FHIR integration for improved interoperability, promoting responsible AI deployment.", "AI": {"tldr": "The AI Product Passport is a standards-based framework enhancing transparency, traceability, and compliance in healthcare AI through lifecycle documentation, implemented via an open-source platform.", "motivation": "To address transparency, traceability, and compliance gaps in healthcare AI by creating a lifecycle-based documentation framework.", "method": "Developed within the AI4HF project, it analyzed regulatory standards and existing methods. A relational data model was created incorporating MLOps/ModelOps, with input from diverse stakeholders during workshops. Built with Python for automated provenance tracking and implemented as a web-based platform.", "result": "The framework ensures well-defined lifecycle management and role-based access. It produces reports for machine and human readability, aligns with FUTURE-AI principles, and improves accessibility via open-source implementation.", "conclusion": "The AI Product Passport bridges gaps in transparency and compliance for healthcare AI, fosters trust, and aligns with ethical demands while being adaptable and open-source. Future updates are aimed at enhancing interoperability and responsible AI deployment."}}
{"id": "2512.13904", "pdf": "https://arxiv.org/pdf/2512.13904", "abs": "https://arxiv.org/abs/2512.13904", "authors": ["Amirkia Rafiei Oskooei", "Eren Caglar", "Ibrahim Sahin", "Ayse Kayabay", "Mehmet S. Aktas"], "title": "Generative AI for Video Translation: A Scalable Architecture for Multilingual Video Conferencing", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.CV"], "comment": "Accepted manuscript. Published in Applied Sciences, 2025", "summary": "The real-time deployment of cascaded generative AI pipelines for applications like video translation is constrained by significant system-level challenges. These include the cumulative latency of sequential model inference and the quadratic ($\\mathcal{O}(N^2)$) computational complexity that renders multi-user video conferencing applications unscalable. This paper proposes and evaluates a practical system-level framework designed to mitigate these critical bottlenecks. The proposed architecture incorporates a turn-taking mechanism to reduce computational complexity from quadratic to linear in multi-user scenarios, and a segmented processing protocol to manage inference latency for a perceptually real-time experience. We implement a proof-of-concept pipeline and conduct a rigorous performance analysis across a multi-tiered hardware setup, including commodity (NVIDIA RTX 4060), cloud (NVIDIA T4), and enterprise (NVIDIA A100) GPUs. Our objective evaluation demonstrates that the system achieves real-time throughput ($\u03c4< 1.0$) on modern hardware. A subjective user study further validates the approach, showing that a predictable, initial processing delay is highly acceptable to users in exchange for a smooth, uninterrupted playback experience. The work presents a validated, end-to-end system design that offers a practical roadmap for deploying scalable, real-time generative AI applications in multilingual communication platforms.", "AI": {"tldr": "The paper addresses challenges in deploying real-time generative AI pipelines for video translation, proposing strategies to reduce latency and computational complexity for scalability.", "motivation": "Deploying cascaded generative AI pipelines for video translation faces challenges like cumulative latency and scalability in multi-user contexts.", "method": "The paper introduces a turn-taking mechanism to achieve linear computational complexity and segmented processing for real-time latency, tested across various GPU setups.", "result": "The system achieves real-time throughput across modern GPUs, verified by objective metrics and user studies indicating high user acceptance.", "conclusion": "The work provides a validated system design for scalable, real-time generative AI deployments in multilingual communication platforms."}}
{"id": "2512.13935", "pdf": "https://arxiv.org/pdf/2512.13935", "abs": "https://arxiv.org/abs/2512.13935", "authors": ["Qi Chen", "Fabio Ramos", "Al\u00e1n Aspuru-Guzik", "Florian Shkurti"], "title": "Informing Acquisition Functions via Foundation Models for Molecular Discovery", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Bayesian Optimization (BO) is a key methodology for accelerating molecular discovery by estimating the mapping from molecules to their properties while seeking the optimal candidate. Typically, BO iteratively updates a probabilistic surrogate model of this mapping and optimizes acquisition functions derived from the model to guide molecule selection. However, its performance is limited in low-data regimes with insufficient prior knowledge and vast candidate spaces. Large language models (LLMs) and chemistry foundation models offer rich priors to enhance BO, but high-dimensional features, costly in-context learning, and the computational burden of deep Bayesian surrogates hinder their full utilization. To address these challenges, we propose a likelihood-free BO method that bypasses explicit surrogate modeling and directly leverages priors from general LLMs and chemistry-specific foundation models to inform acquisition functions. Our method also learns a tree-structured partition of the molecular search space with local acquisition functions, enabling efficient candidate selection via Monte Carlo Tree Search. By further incorporating coarse-grained LLM-based clustering, it substantially improves scalability to large candidate sets by restricting acquisition function evaluations to clusters with statistically higher property values. We show through extensive experiments and ablations that the proposed method substantially improves scalability, robustness, and sample efficiency in LLM-guided BO for molecular discovery.", "AI": {"tldr": "The paper proposes a new Bayesian Optimization (BO) method to enhance molecular discovery using priors from large language models (LLMs) and chemistry foundation models, while improving scalability and efficiency in low-data regimes.", "motivation": "Enhance molecular discovery using Bayesian Optimization, addressing challenges of low data availability, vast candidate spaces, and underutilized priors from large language models and chemistry foundation models.", "method": "The method introduces likelihood-free Bayesian Optimization, employing priors from LLMs and chemistry-specific models, tree-structured search space partitioning with local acquisition functions, and coarse-grained LLM-based clustering for efficient search.", "result": "Experiments and ablation studies demonstrate improved scalability, robustness, and sample efficiency in molecular discovery using the proposed method.", "conclusion": "The proposed likelihood-free Bayesian Optimization is a significant advancement for molecular discovery, especially in low-data regimes, improving efficiency and scalability through innovative use of LLM and chemistry model priors."}}
{"id": "2512.14093", "pdf": "https://arxiv.org/pdf/2512.14093", "abs": "https://arxiv.org/abs/2512.14093", "authors": ["Nhi Nguyen", "Constantino \u00c1lvarez Casado", "Le Nguyen", "Manuel Lage Ca\u00f1ellas", "Miguel Bordallo L\u00f3pez"], "title": "Quality-Aware Framework for Video-Derived Respiratory Signals", "categories": ["cs.CV", "eess.SP"], "comment": "6 pages, 1 figure, 2 tables, conference", "summary": "Video-based respiratory rate (RR) estimation is often unreliable due to inconsistent signal quality across extraction methods. We present a predictive, quality-aware framework that integrates heterogeneous signal sources with dynamic assessment of reliability. Ten signals are extracted from facial remote photoplethysmography (rPPG), upper-body motion, and deep learning pipelines, and analyzed using four spectral estimators: Welch's method, Multiple Signal Classification (MUSIC), Fast Fourier Transform (FFT), and peak detection. Segment-level quality indices are then used to train machine learning models that predict accuracy or select the most reliable signal. This enables adaptive signal fusion and quality-based segment filtering. Experiments on three public datasets (OMuSense-23, COHFACE, MAHNOB-HCI) show that the proposed framework achieves lower RR estimation errors than individual methods in most cases, with performance gains depending on dataset characteristics. These findings highlight the potential of quality-driven predictive modeling to deliver scalable and generalizable video-based respiratory monitoring solutions.", "AI": {"tldr": "The paper proposes a framework for improving video-based respiratory rate (RR) estimation by integrating multiple signal sources and utilizing signal quality assessment.", "motivation": "Video-based RR estimation is unreliable due to varying signal quality, necessitating an approach to ensure consistent and accurate outcomes.", "method": "The framework integrates signals from facial rPPG, body motion, and deep learning, applies spectral analysis, and uses quality indices with machine learning for adaptive signal fusion.", "result": "The framework reduces RR estimation errors compared to individual methods, as demonstrated on three public datasets (OMuSense-23, COHFACE, MAHNOB-HCI).", "conclusion": "Quality-aware predictive modeling enhances the accuracy and adaptability of video-based RR monitoring systems, making them more scalable and generalizable."}}
{"id": "2512.13703", "pdf": "https://arxiv.org/pdf/2512.13703", "abs": "https://arxiv.org/abs/2512.13703", "authors": ["Fan Yang"], "title": "Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across various tasks, but their security vulnerabilities can be exploited by attackers to generate harmful content, causing adverse impacts across various societal domains. Most existing jailbreak methods revolve around Prompt Engineering or adversarial optimization, yet we identify a previously overlooked phenomenon: many harmful scenarios are highly consistent with legitimate ones in terms of underlying principles. Based on this finding, this paper proposes the Safe2Harm Semantic Isomorphism Attack method, which achieves efficient jailbreaking through four stages: first, rewrite the harmful question into a semantically safe question with similar underlying principles; second, extract the thematic mapping relationship between the two; third, let the LLM generate a detailed response targeting the safe question; finally, reversely rewrite the safe response based on the thematic mapping relationship to obtain harmful output. Experiments on 7 mainstream LLMs and three types of benchmark datasets show that Safe2Harm exhibits strong jailbreaking capability, and its overall performance is superior to existing methods. Additionally, we construct a challenging harmful content evaluation dataset containing 358 samples and evaluate the effectiveness of existing harmful detection methods, which can be deployed for LLM input-output filtering to enable defense.", "AI": {"tldr": "The paper introduces the Safe2Harm method to exploit semantic similarities in generating harmful content through large language models (LLMs), demonstrating high efficacy in bypassing existing safeguards.", "motivation": "To address the vulnerabilities in LLMs where attackers exploit semantic similarities between harmful and legitimate queries to trigger unwanted outputs.", "method": "The paper proposes the Safe2Harm Semantic Isomorphism Attack method, consisting of four stages: reframing harmful queries into safe equivalents, thematic mapping, generating safe responses through LLMs, and converting the safe responses back to harmful outputs.", "result": "The proposed method outperforms existing jailbreak methods, successfully bypassing safeguards in 7 mainstream LLMs and multiple datasets, along with creating a new benchmark dataset for evaluating harmful content detection.", "conclusion": "Safe2Harm demonstrates the potential risks related to semantic exploitation in LLMs, necessitating robust detection and filtering systems for enhanced security."}}
{"id": "2512.13930", "pdf": "https://arxiv.org/pdf/2512.13930", "abs": "https://arxiv.org/abs/2512.13930", "authors": ["Samuel Rothfarb", "Megan C. Davis", "Ivana Matanovic", "Baikun Li", "Edward F. Holby", "Wilton J. M. Kort-Kamp"], "title": "Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": "Keywords: Multi-agent reasoning; Large language models; Active learning; AI-driven simulation; Materials discovery; Density functional theory; Surface chemistry", "summary": "Artificial intelligence is reshaping scientific exploration, but most methods automate procedural tasks without engaging in scientific reasoning, limiting autonomy in discovery. We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations. In MASTER, a multimodal system translates natural language into density functional theory workflows, while higher-level reasoning agents guide discovery through a hierarchy of strategies, including a single agent baseline and three multi-agent approaches: peer review, triage-ranking, and triage-forms. Across two chemical applications, CO adsorption on Cu-surface transition metal (M) adatoms and on M-N-C catalysts, reasoning-driven exploration reduces required atomistic simulations by up to 90% relative to trial-and-error selection. Reasoning trajectories reveal chemically grounded decisions that cannot be explained by stochastic sampling or semantic bias. Altogether, multi-agent collaboration accelerates materials discovery and marks a new paradigm for autonomous scientific exploration.", "AI": {"tldr": "The paper introduces MASTER, a system where AI actively performs scientific reasoning with reduced reliance on trial-and-error in material discovery.", "motivation": "To address the limitation of current AI methods in scientific discovery, which focus on procedural tasks but lack advanced reasoning capabilities.", "method": "MASTER is a multimodal active learning framework using large language models for reasoning, designing, and executing atomistic simulations autonomously.", "result": "Experimental results show reasoning-driven exploration in MASTER reduces simulations needed by up to 90% in chemical applications, providing more chemically informed outcomes.", "conclusion": "MASTER demonstrates that multi-agent collaboration can accelerate and bring a paradigm shift in autonomous scientific exploration."}}
{"id": "2512.13945", "pdf": "https://arxiv.org/pdf/2512.13945", "abs": "https://arxiv.org/abs/2512.13945", "authors": ["Vivian Lin", "Kuk Jin Jang", "Wenwen Si", "Insup Lee"], "title": "Pattern-Guided Diffusion Models", "categories": ["cs.LG"], "comment": "Under review", "summary": "Diffusion models have shown promise in forecasting future data from multivariate time series. However, few existing methods account for recurring structures, or patterns, that appear within the data. We present Pattern-Guided Diffusion Models (PGDM), which leverage inherent patterns within temporal data for forecasting future time steps. PGDM first extracts patterns using archetypal analysis and estimates the most likely next pattern in the sequence. By guiding predictions with this pattern estimate, PGDM makes more realistic predictions that fit within the set of known patterns. We additionally introduce a novel uncertainty quantification technique based on archetypal analysis, and we dynamically scale the guidance level based on the pattern estimate uncertainty. We apply our method to two well-motivated forecasting applications, predicting visual field measurements and motion capture frames. On both, we show that pattern guidance improves PGDM's performance (MAE / CRPS) by up to 40.67% / 56.26% and 14.12% / 14.10%, respectively. PGDM also outperforms baselines by up to 65.58% / 84.83% and 93.64% / 92.55%.", "AI": {"tldr": "The paper introduces Pattern-Guided Diffusion Models (PGDM), enhancing multivariate time series forecasting by incorporating recurring patterns and uncertainty estimation.", "motivation": "Existing diffusion models for forecasting multivariate time series often neglect the recurring structures or patterns in the data, leading to less realistic predictions.", "method": "PGDM utilizes archetypal analysis to extract recurring patterns and guides predictions using these patterns. It includes a novel uncertainty quantification technique and dynamically adjusts guidance levels based on uncertainty.", "result": "In two forecasting applications (visual field measurements and motion capture frames), PGDM improves performance (MAE/CRPS) by up to 40.67% / 56.26% and 14.12% / 14.10%, respectively, outperforming baselines significantly.", "conclusion": "Incorporating pattern guidance significantly enhances forecasting accuracy and realism, providing a versatile framework for addressing temporal data challenges."}}
{"id": "2512.14095", "pdf": "https://arxiv.org/pdf/2512.14095", "abs": "https://arxiv.org/abs/2512.14095", "authors": ["Sisi Dai", "Kai Xu"], "title": "AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation", "categories": ["cs.CV"], "comment": "AAAI 2026", "summary": "Despite significant progress in text-driven 4D human-object interaction (HOI) generation with supervised methods, the scalability remains limited by the scarcity of large-scale 4D HOI datasets. To overcome this, recent approaches attempt zero-shot 4D HOI generation with pre-trained image diffusion models. However, interaction cues are minimally distilled during the generation process, restricting their applicability across diverse scenarios. In this paper, we propose AnchorHOI, a novel framework that thoroughly exploits hybrid priors by incorporating video diffusion models beyond image diffusion models, advancing 4D HOI generation. Nevertheless, directly optimizing high-dimensional 4D HOI with such priors remains challenging, particularly for human pose and compositional motion. To address this challenge, AnchorHOI introduces an anchor-based prior distillation strategy, which constructs interaction-aware anchors and then leverages them to guide generation in a tractable two-step process. Specifically, two tailored anchors are designed for 4D HOI generation: anchor Neural Radiance Fields (NeRFs) for expressive interaction composition, and anchor keypoints for realistic motion synthesis. Extensive experiments demonstrate that AnchorHOI outperforms previous methods with superior diversity and generalization.", "AI": {"tldr": "The paper introduces AnchorHOI, a new framework that enhances 4D human-object interaction (HOI) generation using hybrid priors and anchor-based strategies for improved scalability, diversity, and applicability.", "motivation": "The paper addresses the challenge of limited scalability in 4D HOI generation due to the lack of large-scale datasets and inadequacies in interaction cues with current zero-shot methods.", "method": "AnchorHOI employs hybrid priors by incorporating video and image diffusion models, alongside an anchor-based prior distillation strategy. It utilizes anchor NeRFs for interaction composition and anchor keypoints for motion synthesis in a two-step generation process.", "result": "Experiments show that AnchorHOI surpasses existing methods, achieving better diversity and generalization in 4D HOI generation.", "conclusion": "AnchorHOI demonstrates the potential of leveraging hybrid priors and anchor-based strategies for scalable, diverse, and realistic text-driven 4D HOI generation."}}
{"id": "2512.13989", "pdf": "https://arxiv.org/pdf/2512.13989", "abs": "https://arxiv.org/abs/2512.13989", "authors": ["Cindy Y. Zhang", "Elif Ertekin", "Peter Orbanz", "Ryan P. Adams"], "title": "A Single Architecture for Representing Invariance Under Any Space Group", "categories": ["cs.LG"], "comment": "24 pages, 7 figures", "summary": "Incorporating known symmetries in data into machine learning models has consistently improved predictive accuracy, robustness, and generalization. However, achieving exact invariance to specific symmetries typically requires designing bespoke architectures for each group of symmetries, limiting scalability and preventing knowledge transfer across related symmetries. In the case of the space groups, symmetries critical to modeling crystalline solids in materials science and condensed matter physics, this challenge is particularly salient as there are 230 such groups in three dimensions. In this work we present a new approach to such crystallographic symmetries by developing a single machine learning architecture that is capable of adapting its weights automatically to enforce invariance to any input space group. Our approach is based on constructing symmetry-adapted Fourier bases through an explicit characterization of constraints that group operations impose on Fourier coefficients. Encoding these constraints into a neural network layer enables weight sharing across different space groups, allowing the model to leverage structural similarities between groups and overcome data sparsity when limited measurements are available for specific groups. We demonstrate the effectiveness of this approach in achieving competitive performance on material property prediction tasks and performing zero-shot learning to generalize to unseen groups.", "AI": {"tldr": "The paper introduces a machine learning architecture that leverages Fourier bases to enforce invariance to any crystallographic symmetry group, enhancing performance in material property prediction and zero-shot learning.", "motivation": "To overcome the challenges of designing separate architectures for each symmetry group and to facilitate knowledge transfer across 230 space groups relevant to crystalline solids.", "method": "Develops a single architecture that constructs symmetry-adapted Fourier bases, encoding symmetry constraints to enable weight sharing across space groups.", "result": "Achieved competitive results in material property prediction tasks and demonstrated zero-shot learning capabilities for unseen symmetry groups.", "conclusion": "The approach effectively integrates crystallographic symmetries into machine learning models, improving scalability and enhancing generalization performance for tasks in materials science."}}
{"id": "2512.14096", "pdf": "https://arxiv.org/pdf/2512.14096", "abs": "https://arxiv.org/abs/2512.14096", "authors": ["Ruitong Sun", "Tianze Yang", "Wei Niu", "Jin Sun"], "title": "OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration", "categories": ["cs.CV"], "comment": "29 pages", "summary": "Diffusion models have emerged as the dominant paradigm for high-quality image generation, yet their computational expense remains substantial due to iterative denoising. Classifier-Free Guidance (CFG) significantly enhances generation quality and controllability but doubles the computation by requiring both conditional and unconditional forward passes at every timestep. We present OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a framework that accelerates diffusion transformers (DiT) through systematic optimization. Our key insight is that variable guidance scales enable sparse computation: adjusting scales at certain timesteps can compensate for skipping CFG at others, enabling both fewer total sampling steps and fewer CFG steps while maintaining quality. However, variable guidance patterns introduce denoising deviations that undermine standard caching methods, which assume constant CFG scales across steps. Moreover, different transformer blocks are affected at different levels under dynamic conditions. This paper develops a two-stage approach leveraging these insights. Stage-1 employs evolutionary algorithms to jointly optimize which timesteps to skip and what guidance scale to use, eliminating up to 82% of unconditional passes. Stage-2 introduces adaptive rank allocation that tailors calibration efforts per transformer block, maintaining caching effectiveness under variable guidance. Experiments demonstrate that OUSAC significantly outperforms state-of-the-art acceleration methods, achieving 53% computational savings with 15% quality improvement on DiT-XL/2 (ImageNet 512x512), 60% savings with 16.1% improvement on PixArt-alpha (MSCOCO), and 5x speedup on FLUX while improving CLIP Score over the 50-step baseline.", "AI": {"tldr": "The paper introduces OUSAC, a framework to accelerate diffusion transformers (DiT) in image generation by optimizing guidance scheduling and utilizing adaptive caching, achieving substantial computational savings and quality improvements.", "motivation": "The motivation is to address the computational inefficiencies in diffusion models, particularly those enhanced by Classifier-Free Guidance (CFG), which doubles the computation by requiring both conditional and unconditional forward passes for high-quality image generation.", "method": "OUSAC employs a two-stage approach: (1) evolutionary algorithms to optimize timesteps to skip and guidance scale adjustments, reducing unconditional passes by up to 82%, and (2) adaptive rank allocation to calibrate transformer blocks effectively under variable guidance.", "result": "OUSAC achieves significant computational savings and quality improvements across experiments, including 53% savings with 15% quality improvement on DiT-XL/2, 60% savings with 16.1% improvement on PixArt-alpha, and a 5x speedup while improving CLIP Score on FLUX.", "conclusion": "OUSAC proves to be a superior method for accelerating diffusion models, balancing computational efficiency with high-quality image generation, and surpassing existing acceleration methods."}}
{"id": "2512.14011", "pdf": "https://arxiv.org/pdf/2512.14011", "abs": "https://arxiv.org/abs/2512.14011", "authors": ["Yue Wan", "Jiayi Yuan", "Zhiwei Feng", "Xiaowei Jia"], "title": "Accelerating MHC-II Epitope Discovery via Multi-Scale Prediction in Antigen Presentation", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Antigenic epitope presented by major histocompatibility complex II (MHC-II) proteins plays an essential role in immunotherapy. However, compared to the more widely studied MHC-I in computational immunotherapy, the study of MHC-II antigenic epitope poses significantly more challenges due to its complex binding specificity and ambiguous motif patterns. Consequently, existing datasets for MHC-II interactions are smaller and less standardized than those available for MHC-I. To address these challenges, we present a well-curated dataset derived from the Immune Epitope Database (IEDB) and other public sources. It not only extends and standardizes existing peptide-MHC-II datasets, but also introduces a novel antigen-MHC-II dataset with richer biological context. Leveraging this dataset, we formulate three major machine learning (ML) tasks of peptide binding, peptide presentation, and antigen presentation, which progressively capture the broader biological processes within the MHC-II antigen presentation pathway. We further employ a multi-scale evaluation framework to benchmark existing models, along with a comprehensive analysis over various modeling designs to this problem with a modular framework. Overall, this work serves as a valuable resource for advancing computational immunotherapy, providing a foundation for future research in ML guided epitope discovery and predictive modeling of immune responses.", "AI": {"tldr": "This paper focuses on MHC-II antigenic epitope, addressing challenges in its binding specificity and datasets. It introduces a standardized dataset and benchmarks machine learning tasks for advanced immunotherapy research.", "motivation": "The paper aims to tackle the challenges in studying MHC-II antigenic epitopes, such as complex binding specificity, ambiguous motifs, and limited standardized datasets, to enhance computational immunotherapy research.", "method": "The authors curated an extended dataset from IEDB and public sources, formulated three machine learning tasks (peptide binding, peptide presentation, antigen presentation), and employed multi-scale evaluations and a modular framework for benchmarking models.", "result": "They developed a well-curated MHC-II dataset, showcased ML tasks capturing biological processes, and provided a comprehensive evaluation of modeling designs.", "conclusion": "This work creates a valuable resource for computational immunotherapy, laying the groundwork for ML-driven epitope discovery and immune response modeling."}}
{"id": "2512.14099", "pdf": "https://arxiv.org/pdf/2512.14099", "abs": "https://arxiv.org/abs/2512.14099", "authors": ["Ruishu Zhu", "Zhihao Huang", "Jiacheng Sun", "Ping Luo", "Hongyuan Zhang", "Xuelong Li"], "title": "ViewMask-1-to-3: Multi-View Consistent Image Generation via Multimodal Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view image generation from a single image and text description remains challenging due to the difficulty of maintaining geometric consistency across different viewpoints. Existing approaches typically rely on 3D-aware architectures or specialized diffusion models that require extensive multi-view training data and complex geometric priors. In this work, we introduce ViewMask-1-to-3, a pioneering approach to apply discrete diffusion models to multi-view image generation. Unlike continuous diffusion methods that operate in latent spaces, ViewMask-1-to-3 formulates multi-view synthesis as a discrete sequence modeling problem, where each viewpoint is represented as visual tokens obtained through MAGVIT-v2 tokenization. By unifying language and vision through masked token prediction, our approach enables progressive generation of multiple viewpoints through iterative token unmasking with text input. ViewMask-1-to-3 achieves cross-view consistency through simple random masking combined with self-attention, eliminating the requirement for complex 3D geometric constraints or specialized attention architectures. Our approach demonstrates that discrete diffusion provides a viable and simple alternative to existing multi-view generation methods, ranking first on average across GSO and 3D-FUTURE datasets in terms of PSNR, SSIM, and LPIPS, while maintaining architectural simplicity.", "AI": {"tldr": "An innovative method, ViewMask-1-to-3, applies discrete diffusion models for multi-view image generation from a single image and text, achieving high performance without requiring complex geometric constraints.", "motivation": "The paper aims to address the challenge of maintaining geometric consistency in multi-view image generation without relying on extensive multi-view training data or complex architectural constraints.", "method": "The proposed method, ViewMask-1-to-3, uses a discrete sequence modeling approach to represent multiple viewpoints as visual tokens, employing masked token prediction and iterative token unmasking with text inputs.", "result": "ViewMask-1-to-3 achieves superior performance on GSO and 3D-FUTURE datasets, ranking first in PSNR, SSIM, and LPIPS scores, while retaining a simple architecture.", "conclusion": "The study demonstrates that discrete diffusion models are a viable and simpler alternative for multi-view image generation, achieving cross-view consistency effectively without complex geometric priors."}}
{"id": "2512.13723", "pdf": "https://arxiv.org/pdf/2512.13723", "abs": "https://arxiv.org/abs/2512.13723", "authors": ["David Haslett", "Linus Ta-Lun Huang", "Leila Khalatbari", "Janet Hui-wen Hsiao", "Antoni B. Chan"], "title": "Made-in China, Thinking in America:U.S. Values Persist in Chinese LLMs", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "As large language models increasingly mediate access to information and facilitate decision-making, they are becoming instruments in soft power competitions between global actors such as the United States and China. So far, language models seem to be aligned with the values of Western countries, but evidence for this ethical bias comes mostly from models made by American companies. The current crop of state-of-the-art models includes several made in China, so we conducted the first large-scale investigation of how models made in China and the USA align with people from China and the USA. We elicited responses to the Moral Foundations Questionnaire 2.0 and the World Values Survey from ten Chinese models and ten American models, and we compared their responses to responses from thousands of Chinese and American people. We found that all models respond to both surveys more like American people than like Chinese people. This skew toward American values is only slightly mitigated when prompting the models in Chinese or imposing a Chinese persona on the models. These findings have important implications for a near future in which large language models generate much of the content people consume and shape normative influence in geopolitics.", "AI": {"tldr": "The study investigates the alignment of large language models made in the USA and China with the cultural values of people from those countries, finding that models exhibit a bias toward American values even when using Chinese prompts.", "motivation": "To explore the ethical and cultural alignment of large language models made in China and the USA with the values of the respective people, given their growing influence in geopolitics and soft power dynamics.", "method": "The paper compares values using the Moral Foundations Questionnaire 2.0 and the World Values Survey on responses elicited from ten Chinese models, ten American models, and thousands of Chinese and American people.", "result": "All models, irrespective of origin, align closer to American cultural values than Chinese values. Even using Chinese prompts or imposing a Chinese persona has a limited effect in mitigating this bias.", "conclusion": "This study underscores concerns about the exportation and dominance of American ethical frameworks through language models, raising questions about their geopolitical influence and role in shaping global norms."}}
{"id": "2512.14083", "pdf": "https://arxiv.org/pdf/2512.14083", "abs": "https://arxiv.org/abs/2512.14083", "authors": ["Sungnyun Kim"], "title": "Scalable Frameworks for Real-World Audio-Visual Speech Recognition", "categories": ["eess.AS", "cs.CL", "cs.LG"], "comment": "PhD Dissertation", "summary": "The practical deployment of Audio-Visual Speech Recognition (AVSR) systems is fundamentally challenged by significant performance degradation in real-world environments, characterized by unpredictable acoustic noise and visual interference. This dissertation posits that a systematic, hierarchical approach is essential to overcome these challenges, achieving the robust scalability at the representation, architecture, and system levels. At the representation level, we investigate methods for building a unified model that learns audio-visual features inherently robust to diverse real-world corruptions, thereby enabling generalization to new environments without specialized modules. To address architectural scalability, we explore how to efficiently expand model capacity while ensuring the adaptive and reliable use of multimodal inputs, developing a framework that intelligently allocates computational resources based on the input characteristics. Finally, at the system level, we present methods to expand the system's functionality through modular integration with large-scale foundation models, leveraging their powerful cognitive and generative capabilities to maximize final recognition accuracy. By systematically providing solutions at each of these three levels, this dissertation aims to build a next-generation, robust, and scalable AVSR system with high reliability in real-world applications.", "AI": {"tldr": "This dissertation addresses challenges in real-world Audio-Visual Speech Recognition (AVSR) by introducing hierarchical solutions at representation, architecture, and system levels to enhance robustness and scalability.", "motivation": "Real-world AVSR systems face performance issues due to acoustic noise and visual interference, which necessitates robust, scalable solutions for reliable deployment in diverse environments.", "method": "The paper proposes unified audio-visual models for feature learning, adaptive frameworks for efficient model capacity and multimodal input usage, and integration with large-scale foundation models for system expansion.", "result": "The methods achieved robust and scalable AVSR performance while addressing diverse real-world environmental corruptions.", "conclusion": "A systematic approach at all levels\u2014representation, architecture, and system\u2014can create practical, reliable AVSR systems suitable for real-world applications."}}
{"id": "2512.14019", "pdf": "https://arxiv.org/pdf/2512.14019", "abs": "https://arxiv.org/abs/2512.14019", "authors": ["Juseung Yun", "Sunwoo Yu", "Sumin Ha", "Jonghyun Kim", "Janghyeon Lee", "Jongseong Jang", "Soonyoung Lee"], "title": "EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.", "AI": {"tldr": "EXAONE Path 2.5 is a pathology foundation model integrating histologic, genomic, epigenetic, and transcriptomic data for a more comprehensive understanding of tumor biology.", "motivation": "To address the limitations of image-only models in capturing the full biological landscape of cancer progression.", "method": "Developed a multimodal model utilizing SigLIP loss for contrastive learning, F-RoPE for preserving spatial and tissue-fragment topology, and domain-specific embeddings from WSI and RNA-seq.", "result": "Achieved comparable state-of-the-art performance on Patho-Bench with high data efficiency and demonstrated strong adaptability in clinical applications.", "conclusion": "The biologically informed multimodal approach of EXAONE Path 2.5 emphasizes its potential for advancing precision oncology through comprehensive genotype-to-phenotype understanding and modeling."}}
{"id": "2512.14102", "pdf": "https://arxiv.org/pdf/2512.14102", "abs": "https://arxiv.org/abs/2512.14102", "authors": ["Emanuele Mezzi", "Gertjan Burghouts", "Maarten Kruithof"], "title": "Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": null, "summary": "Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.", "AI": {"tldr": "The paper introduces RUNE, a new approach for text-to-image retrieval in remote sensing that combines neurosymbolic AI and large language models, offering better performance, robustness, and explainability compared to traditional models.", "motivation": "Existing RS-LVLMS struggle with limited explainability and handling complex spatial relations, making them less effective for real-world applications.", "method": "RUNE uses neurosymbolic AI to retrieve images by reasoning over entities and First-Order Logic (FOL) expressions derived from text queries. It integrates LLMs for FOL generation and employs a logic decomposition strategy for scalable reasoning.", "result": "RUNE demonstrates superior performance on the enhanced DOTA dataset, excelling in complex retrieval tasks and robustness metrics (RRQC and RRIU) while outperforming state-of-the-art RS-LVLMS.", "conclusion": "RUNE enhances the explainability and robustness of remote sensing image retrieval through explicit reasoning with FOL expressions, showing potential for real-world applications like post-flood image retrieval."}}
{"id": "2512.14277", "pdf": "https://arxiv.org/pdf/2512.14277", "abs": "https://arxiv.org/abs/2512.14277", "authors": ["Panayiotis Smeros", "Vincent Emonet", "Ruijie Wang", "Ana-Claudia Sima", "Tarcisio Mendes de Farias"], "title": "SPARQL-LLM: Real-Time SPARQL Query Generation from Natural Language Questions", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "17 pages, 8 figures, 1 table. Under Review", "summary": "The advent of large language models is contributing to the emergence of novel approaches that promise to better tackle the challenge of generating structured queries, such as SPARQL queries, from natural language. However, these new approaches mostly focus on response accuracy over a single source while ignoring other evaluation criteria, such as federated query capability over distributed data stores, as well as runtime and cost to generate SPARQL queries. Consequently, they are often not production-ready or easy to deploy over (potentially federated) knowledge graphs with good accuracy. To mitigate these issues, in this paper, we extend our previous work and describe and systematically evaluate SPARQL-LLM, an open-source and triplestore-agnostic approach, powered by lightweight metadata, that generates SPARQL queries from natural language text. First, we describe its architecture, which consists of dedicated components for metadata indexing, prompt building, and query generation and execution. Then, we evaluate it based on a state-of-the-art challenge with multilingual questions, and a collection of questions from three of the most prevalent knowledge graphs within the field of bioinformatics. Our results demonstrate a substantial increase of 24% in the F1 Score on the state-of-the-art challenge, adaptability to high-resource languages such as English and Spanish, as well as ability to form complex and federated bioinformatics queries. Furthermore, we show that SPARQL-LLM is up to 36x faster than other systems participating in the challenge, while costing a maximum of $0.01 per question, making it suitable for real-time, low-cost text-to-SPARQL applications. One such application deployed over real-world decentralized knowledge graphs can be found at https://www.expasy.org/chat.", "AI": {"tldr": "This paper introduces SPARQL-LLM, an open-source system to generate SPARQL queries from natural language efficiently and cost-effectively, overcoming limitations of existing large language models.", "motivation": "Address challenges in generating SPARQL queries from natural language, including accuracy, federated query capability, runtime efficiency, and cost-effectiveness.", "method": "SPARQL-LLM employs components for metadata indexing, prompt building, query generation, and execution, and is systematically evaluated using multilingual and bioinformatics knowledge graph questions.", "result": "SPARQL-LLM achieves a 24% improvement in F1 Score, supports complex and federated queries, operates 36x faster than other systems, and costs $0.01 per question.", "conclusion": "SPARQL-LLM is a scalable, efficient, and low-cost solution suitable for real-time applications, demonstrated via deployment on decentralized knowledge graphs."}}
{"id": "2512.14023", "pdf": "https://arxiv.org/pdf/2512.14023", "abs": "https://arxiv.org/abs/2512.14023", "authors": ["Yong Fang", "Na Li", "Hangguan Shan", "Eryun Liu", "Xinyu Li", "Wei Ni", "Er-Ping Li"], "title": "Multivariate Time Series Forecasting with Hybrid Euclidean-SPD Manifold Graph Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Multivariate Time Series (MTS) forecasting plays a vital role in various real-world applications, such as traffic management and predictive maintenance. Existing approaches typically model MTS data in either Euclidean or Riemannian space, limiting their ability to capture the diverse geometric structures and complex spatio-temporal dependencies inherent in real-world data. To overcome this limitation, we propose the Hybrid Symmetric Positive-Definite Manifold Graph Neural Network (HSMGNN), a novel graph neural network-based model that captures data geometry within a hybrid Euclidean-Riemannian framework. To the best of our knowledge, this is the first work to leverage hybrid geometric representations for MTS forecasting, enabling expressive and comprehensive modeling of geometric properties. Specifically, we introduce a Submanifold-Cross-Segment (SCS) embedding to project input MTS into both Euclidean and Riemannian spaces, thereby capturing spatio-temporal variations across distinct geometric domains. To alleviate the high computational cost of Riemannian distance, we further design an Adaptive-Distance-Bank (ADB) layer with a trainable memory mechanism. Finally, a Fusion Graph Convolutional Network (FGCN) is devised to integrate features from the dual spaces via a learnable fusion operator for accurate prediction. Experiments on three benchmark datasets demonstrate that HSMGNN achieves up to a 13.8 percent improvement over state-of-the-art baselines in forecasting accuracy.", "AI": {"tldr": "Proposes HSMGNN for multivariate time series forecasting that combines Euclidean and Riemannian geometry to model spatio-temporal dependencies, showing 13.8% accuracy improvement.", "motivation": "Address limitations in existing approaches for MTS forecasting in capturing diverse geometric structures and complex spatio-temporal dependencies.", "method": "Developed HSMGNN, which uses a hybrid Euclidean-Riemannian framework and integrates techniques like Submanifold-Cross-Segment embedding, Adaptive-Distance-Bank layer, and Fusion Graph Convolutional Network.", "result": "HSMGNN demonstrated up to 13.8% improvement in accuracy compared to baseline methods across three datasets.", "conclusion": "Hybrid geometric representations can enhance MTS forecasting by effectively modeling complex spatio-temporal dependencies."}}
{"id": "2512.14113", "pdf": "https://arxiv.org/pdf/2512.14113", "abs": "https://arxiv.org/abs/2512.14113", "authors": ["Ashish Mishra", "Gyanaranjan Nayak", "Tarun Kumar", "Arpit Shah", "Suparna Bhattacharya", "Martin Foltin"], "title": "Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach", "categories": ["cs.CV"], "comment": null, "summary": "Pretrained models like CLIP have demonstrated impressive zero-shot classification capabilities across diverse visual domains, spanning natural images, artistic renderings, and abstract representations. However, real-world applications often demand the removal (or \"unlearning\") of specific object classes without requiring additional data or retraining, or affecting the model's performance on unrelated tasks. In this paper, we propose a novel training- and data-free unlearning framework that enables three distinct forgetting paradigms: (1) global unlearning of selected objects across all domains, (2) domain-specific knowledge removal (e.g., eliminating sketch representations while preserving photo recognition), and (3) complete unlearning in selective domains. By leveraging a multimodal nullspace through synergistic integration of text prompts and synthesized visual prototypes derived from CLIP's joint embedding space, our method efficiently removes undesired class information while preserving the remaining knowledge. This approach overcomes the limitations of existing retraining-based methods and offers a flexible and computationally efficient solution for controlled model forgetting.", "AI": {"tldr": "The paper introduces a method to unlearn specific object classes from pretrained models like CLIP without retraining or using additional data, while retaining performance on other tasks.", "motivation": "The motivation is to address the need for removing specific object classes from pretrained models without impacting unrelated task performance and without requiring extra data or retraining.", "method": "The authors propose a training- and data-free framework leveraging a multimodal nullspace, integrating text prompts and synthesized visual prototypes within CLIP's embedding space to achieve targeted unlearning.", "result": "The method supports three forgetting paradigms: global unlearning across domains, domain-specific unlearning, and complete unlearning in selective domains while preserving other knowledge.", "conclusion": "This approach offers a flexible and computationally efficient solution for selective model forgetting, surpassing retraining-based methods in efficiency and adaptability."}}
{"id": "2512.14391", "pdf": "https://arxiv.org/pdf/2512.14391", "abs": "https://arxiv.org/abs/2512.14391", "authors": ["Huayang Li", "Tianyu Zhao", "Richard Sproat"], "title": "RePo: Language Models with Context Re-Positioning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_\u03c6$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.", "AI": {"tldr": "The paper introduces RePo, a system designed to improve context positioning in LLMs to reduce cognitive load and enhance performance on complex tasks.", "motivation": "Existing positional mechanisms in LLMs (fixed/linear indices) are uninformative and increase cognitive load, affecting reasoning and attention allocation.", "method": "Developed RePo, a differentiable module that assigns token positions based on contextual dependencies, pre-trained on the OLMo-2 1B model.", "result": "RePo showed improved performance in noisy, structured, and longer contexts, while being competitive in short-context tasks. It captures intrinsic input structures better and emphasizes distant yet relevant information.", "conclusion": "RePo improves contextual token positioning by enabling better attention allocation and reducing extraneous cognitive load, proving to be effective for diverse tasks."}}
{"id": "2512.14078", "pdf": "https://arxiv.org/pdf/2512.14078", "abs": "https://arxiv.org/abs/2512.14078", "authors": ["Da Zhang", "Bingyu Li", "Zhiyuan Zhao", "Feiping Nie", "Junyu Gao", "Xuelong Li"], "title": "FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis", "categories": ["cs.LG"], "comment": "Paper has been accepted by ICDE2026", "summary": "Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing an efficient, multi-task compatible, and generalizable unified framework for time series analysis remains a significant challenge. Existing approaches are often tailored to single tasks or specific data types, making it difficult to simultaneously handle multi-task modeling and effectively integrate information across diverse time series types. Moreover, real-world data are often affected by noise, complex frequency components, and multi-scale dynamic patterns, which further complicate robust feature extraction and analysis. To ameliorate these challenges, we propose FusAD, a unified analysis framework designed for diverse time series tasks. FusAD features an adaptive time-frequency fusion mechanism, integrating both Fourier and Wavelet transforms to efficiently capture global-local and multi-scale dynamic features. With an adaptive denoising mechanism, FusAD automatically senses and filters various types of noise, highlighting crucial sequence variations and enabling robust feature extraction in complex environments. In addition, the framework integrates a general information fusion and decoding structure, combined with masked pre-training, to promote efficient learning and transfer of multi-granularity representations. Extensive experiments demonstrate that FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection tasks, while maintaining high efficiency and scalability. Code is available at https://github.com/zhangda1018/FusAD.", "AI": {"tldr": "The paper introduces FusAD, a unified framework for multi-task time series analysis using adaptive time-frequency fusion and denoising, showing superior performance in diverse tasks.", "motivation": "Current approaches to time series analysis struggle with multi-task handling and integrating diverse data types, while also encountering challenges like noise and multi-scale dynamic patterns.", "method": "FusAD uses adaptive time-frequency fusion with Fourier and Wavelet transforms, coupled with denoising mechanisms and a general information fusion-decoding structure with masked pre-training.", "result": "FusAD outperforms state-of-the-art models on classification, forecasting, and anomaly detection benchmarks with high efficiency and scalability.", "conclusion": "The proposed FusAD framework effectively addresses challenges in time series analysis, offering a robust, scalable, and generalizable solution for multi-task applications."}}
{"id": "2512.14114", "pdf": "https://arxiv.org/pdf/2512.14114", "abs": "https://arxiv.org/abs/2512.14114", "authors": ["Rui-Yang Ju", "KokSheik Wong", "Yanlin Jin", "Jen-Shiun Chiang"], "title": "MFE-GAN: Efficient GAN-based Framework for Document Image Enhancement and Binarization with Multi-scale Feature Extraction", "categories": ["cs.CV"], "comment": "Extended Journal Version of APSIPA ASC 2025", "summary": "Document image enhancement and binarization are commonly performed prior to document analysis and recognition tasks for improving the efficiency and accuracy of optical character recognition (OCR) systems. This is because directly recognizing text in degraded documents, particularly in color images, often results in unsatisfactory recognition performance. To address these issues, existing methods train independent generative adversarial networks (GANs) for different color channels to remove shadows and noise, which, in turn, facilitates efficient text information extraction. However, deploying multiple GANs results in long training and inference times. To reduce both training and inference times of document image enhancement and binarization models, we propose MFE-GAN, an efficient GAN-based framework with multi-scale feature extraction (MFE), which incorporates Haar wavelet transformation (HWT) and normalization to process document images before feeding them into GANs for training. In addition, we present novel generators, discriminators, and loss functions to improve the model's performance, and we conduct ablation studies to demonstrate their effectiveness. Experimental results on the Benchmark, Nabuco, and CMATERdb datasets demonstrate that the proposed MFE-GAN significantly reduces the total training and inference times while maintaining comparable performance with respect to state-of-the-art (SOTA) methods. The implementation of this work is available at https://ruiyangju.github.io/MFE-GAN.", "AI": {"tldr": "The study proposes an efficient GAN-based framework, MFE-GAN, to enhance degraded document images rapidly and accurately for better OCR performance.", "motivation": "The motivation is to address the inefficiency and subpar OCR performance in recognizing text from degraded or color document images using existing multi-GAN methods.", "method": "The method uses a novel framework incorporating multi-scale feature extraction, Haar wavelet transformation, and normalization in GANs, along with innovative generators, discriminators, and loss functions.", "result": "MFE-GAN reduces training and inference times significantly while achieving comparable performance to state-of-the-art methods.", "conclusion": "The proposed MFE-GAN framework provides an efficient solution for document image enhancement and binarization, improving OCR systems' overall efficiency and effectiveness."}}
{"id": "2512.14503", "pdf": "https://arxiv.org/pdf/2512.14503", "abs": "https://arxiv.org/abs/2512.14503", "authors": ["Chao Yi", "Dian Chen", "Gaoyang Guo", "Jiakai Tang", "Jian Wu", "Jing Yu", "Mao Zhang", "Wen Chen", "Wenjun Yang", "Yujie Luo", "Yuning Jiang", "Zhujin Gao", "Bo Zheng", "Binbin Cao", "Changfa Wu", "Dixuan Wang", "Han Wu", "Haoyi Hu", "Kewei Zhu", "Lang Tian", "Lin Yang", "Qiqi Huang", "Siqi Yang", "Wenbo Su", "Xiaoxiao He", "Xin Tong", "Xu Chen", "Xunke Xi", "Xiaowei Huang", "Yaxuan Wu", "Yeqiu Yang", "Yi Hu", "Yujin Yuan", "Yuliang Yan", "Zile Zhou"], "title": "RecGPT-V2 Technical Report", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable potential in transforming recommender systems from implicit behavioral pattern matching to explicit intent reasoning. While RecGPT-V1 successfully pioneered this paradigm by integrating LLM-based reasoning into user interest mining and item tag prediction, it suffers from four fundamental limitations: (1) computational inefficiency and cognitive redundancy across multiple reasoning routes; (2) insufficient explanation diversity in fixed-template generation; (3) limited generalization under supervised learning paradigms; and (4) simplistic outcome-focused evaluation that fails to match human standards.\n  To address these challenges, we present RecGPT-V2 with four key innovations. First, a Hierarchical Multi-Agent System restructures intent reasoning through coordinated collaboration, eliminating cognitive duplication while enabling diverse intent coverage. Combined with Hybrid Representation Inference that compresses user-behavior contexts, our framework reduces GPU consumption by 60% and improves exclusive recall from 9.39% to 10.99%. Second, a Meta-Prompting framework dynamically generates contextually adaptive prompts, improving explanation diversity by +7.3%. Third, constrained reinforcement learning mitigates multi-reward conflicts, achieving +24.1% improvement in tag prediction and +13.0% in explanation acceptance. Fourth, an Agent-as-a-Judge framework decomposes assessment into multi-step reasoning, improving human preference alignment. Online A/B tests on Taobao demonstrate significant improvements: +2.98% CTR, +3.71% IPV, +2.19% TV, and +11.46% NER. RecGPT-V2 establishes both the technical feasibility and commercial viability of deploying LLM-powered intent reasoning at scale, bridging the gap between cognitive exploration and industrial utility.", "AI": {"tldr": "RecGPT-V2 introduces advanced intent reasoning with four key innovations, outperforming its predecessor in efficiency, explanation diversity, prediction accuracy, and human alignment.", "motivation": "The paper aims to address the limitations of RecGPT-V1, such as inefficiency, lack of explanation diversity, limited generalization, and simplistic evaluation standards, to better integrate LLMs in recommender systems.", "method": "RecGPT-V2 leverages a Hierarchical Multi-Agent System, Hybrid Representation Inference, Meta-Prompting, constrained reinforcement learning, and an Agent-as-a-Judge framework to enhance intent reasoning and recommendation systems.", "result": "The proposed innovations reduce GPU consumption by 60%, increase exclusive recall by 1.6%, improve explanation diversity by 7.3%, enhance tag prediction by 24.1%, boost explanation acceptance by 13%, and improve several online metrics in A/B tests on Taobao.", "conclusion": "RecGPT-V2 demonstrates the technical and commercial feasibility of deploying advanced LLM-powered intent reasoning at scale, significantly improving key metrics in industrial applications."}}
{"id": "2512.14080", "pdf": "https://arxiv.org/pdf/2512.14080", "abs": "https://arxiv.org/abs/2512.14080", "authors": ["Wentao Guo", "Mayank Mishra", "Xinle Cheng", "Ion Stoica", "Tri Dao"], "title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel \"token rounding\" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.", "AI": {"tldr": "The paper introduces SonicMoE, a solution to enhance efficiency in Mixture of Experts (MoE) models by addressing memory and computation inefficiencies. It improves throughput while reducing memory requirements and computational waste.", "motivation": "Fine-grained MoEs face challenges like increased memory footprint and reduced hardware efficiency, while sparse MoEs suffer wasted computations. The paper aims to address these limitations to improve the overall efficiency of MoE models.", "method": "The authors propose a memory-efficient algorithm with minimal activation caching for MoEs, develop GPU kernels to optimize computation and overlapping memory IO, and implement a novel \"token rounding\" technique to reduce wasted computation in sparse settings.", "result": "SonicMoE reduces activation memory by 45%, achieves 1.86x compute throughput improvement on Hopper GPUs, and sustains comparable training throughput using fewer GPUs than ScatterMoE. It also achieves additional speedup under high sparsity settings with their token rounding technique.", "conclusion": "SonicMoE provides a significant improvement in efficiency and scalability in MoE models, enabling faster training of large models while addressing hardware and computational constraints. All developed methods are open-sourced."}}
{"id": "2512.14121", "pdf": "https://arxiv.org/pdf/2512.14121", "abs": "https://arxiv.org/abs/2512.14121", "authors": ["Wenbo Tian", "Ruting Lin", "Hongxian Zheng", "Yaodong Yang", "Geng Wu", "Zihao Zhang", "Zhang Zhang"], "title": "SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing intelligent sports analysis systems mainly focus on \"scoring and visualization,\" often lacking automatic performance diagnosis and interpretable training guidance. Recent advances of Large Language Models (LMMs) and motion analysis techniques provide new opportunities to address the above limitations. In this paper, we propose SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance, which establishes a closed loop from motion time-series input to professional training guidance. First, given a set of high-quality target models, we introduce MotionDTW, a two-stage time series alignment algorithm designed for accurate keyframe extraction from skeleton-based motion sequences. Subsequently, we design a Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM) to obtain a set of interpretable assessment metrics (e.g., insufficient extension) by constrasting the keyframes with the targe models. Finally, we propose SportsRAG, a RAG-based training guidance model based on Qwen3. Leveraging a 6B-token knowledge base, it prompts the LLM to generate professional training guidance by retrieving domain-specific QA pairs. Experimental results demonstrate that MotionDTW significantly outperforms traditional methods with lower temporal error and higher IoU scores. Furthermore, ablation studies validate the KISMAM and SportsRAG, confirming that SportsGPT surpasses general LLMs in diagnostic accuracy and professionalism.", "AI": {"tldr": "This paper introduces SportsGPT, a framework for analyzing and improving sports motion using LLMs for diagnosis and training guidance.", "motivation": "Existing sports analysis tools lack adequate automatic performance diagnosis and interpretable training feedback. Advancements in LLMs and motion analysis enable addressing these gaps.", "method": "The paper introduces SportsGPT with three core components: MotionDTW (a time-series alignment method), KISMAM (knowledge-based motion assessment), and SportsRAG (a training guidance model leveraging an extensive knowledge base).", "result": "MotionDTW showed better temporal error and IoU scores compared to traditional methods. Ablation studies confirmed increased diagnostic accuracy and professionalism in SportsGPT against general LLMs.", "conclusion": "SportsGPT provides an innovative method to assess and guide sports motions effectively, surpassing current analysis systems in diagnosis accuracy and professional insight."}}
{"id": "2512.14652", "pdf": "https://arxiv.org/pdf/2512.14652", "abs": "https://arxiv.org/abs/2512.14652", "authors": ["Pawel Swietojanski", "Xinwei Li", "Mingbin Xu", "Takaaki Hori", "Dogan Can", "Xiaodan Zhuang"], "title": "Segmental Attention Decoding With Long Form Acoustic Encodings", "categories": ["eess.AS", "cs.CL"], "comment": "5 pages, 1 fig", "summary": "We address the fundamental incompatibility of attention-based encoder-decoder (AED) models with long-form acoustic encodings. AED models trained on segmented utterances learn to encode absolute frame positions by exploiting limited acoustic context beyond segment boundaries, but fail to generalize when decoding long-form segments where these cues vanish. The model loses ability to order acoustic encodings due to permutation invariance of keys and values in cross-attention. We propose four modifications: (1) injecting explicit absolute positional encodings into cross-attention for each decoded segment, (2) long-form training with extended acoustic context to eliminate implicit absolute position encoding, (3) segment concatenation to cover diverse segmentations needed during training, and (4) semantic segmentation to align AED-decoded segments with training segments. We show these modifications close the accuracy gap between continuous and segmented acoustic encodings, enabling auto-regressive use of the attention decoder.", "AI": {"tldr": "This paper identifies the incompatibility of attention-based encoder-decoder (AED) models with long-form acoustic encodings and proposes four key modifications to address this, enabling accurate auto-regressive decoding.", "motivation": "The paper is motivated by the failure of AED models to generalize when decoding long-form segments of acoustic data due to issues like absolute positional encoding and permutation invariance within attention mechanisms.", "method": "The authors propose four modifications: (1) explicit absolute positional encodings for cross-attention, (2) long-form training with extended acoustic context, (3) segment concatenation for diverse segment coverage, and (4) semantic segmentation aligning decoded segments with training segments.", "result": "The modifications successfully close the accuracy gap between continuous and segmented acoustic encodings.", "conclusion": "The proposed interventions enable effective auto-regressive decoding in AED models, addressing fundamental challenges with long-form acoustic encodings."}}
{"id": "2512.14086", "pdf": "https://arxiv.org/pdf/2512.14086", "abs": "https://arxiv.org/abs/2512.14086", "authors": ["Boyuan Yao", "Dingcheng Luo", "Lianghao Cao", "Nikola Kovachki", "Thomas O'Leary-Roseberry", "Omar Ghattas"], "title": "Derivative-Informed Fourier Neural Operator: Universal Approximation and Applications to PDE-Constrained Optimization", "categories": ["cs.LG", "math.NA"], "comment": null, "summary": "We present approximation theories and efficient training methods for derivative-informed Fourier neural operators (DIFNOs) with applications to PDE-constrained optimization. A DIFNO is an FNO trained by minimizing its prediction error jointly on output and Fr\u00e9chet derivative samples of a high-fidelity operator (e.g., a parametric PDE solution operator). As a result, a DIFNO can closely emulate not only the high-fidelity operator's response but also its sensitivities. To motivate the use of DIFNOs instead of conventional FNOs as surrogate models, we show that accurate surrogate-driven PDE-constrained optimization requires accurate surrogate Fr\u00e9chet derivatives. Then, for continuously differentiable operators, we establish (i) simultaneous universal approximation of FNOs and their Fr\u00e9chet derivatives on compact sets, and (ii) universal approximation of FNOs in weighted Sobolev spaces with input measures that have unbounded supports. Our theoretical results certify the capability of FNOs for accurate derivative-informed operator learning and accurate solution of PDE-constrained optimization. Furthermore, we develop efficient training schemes using dimension reduction and multi-resolution techniques that significantly reduce memory and computational costs for Fr\u00e9chet derivative learning. Numerical examples on nonlinear diffusion--reaction, Helmholtz, and Navier--Stokes equations demonstrate that DIFNOs are superior in sample complexity for operator learning and solving infinite-dimensional PDE-constrained inverse problems, achieving high accuracy at low training sample sizes.", "AI": {"tldr": "This paper introduces and develops Derivative-Informed Fourier Neural Operators (DIFNOs) for efficient solutions of PDE-constrained optimization problems, emphasizing simultaneous approximation of solutions and their derivatives.", "motivation": "The paper aims to improve accuracy and efficiency in solving PDE-constrained optimization problems by leveraging surrogate models that capture not only the outputs but also the derivative information of high-fidelity operators.", "method": "The authors establish the universal approximation capability of DIFNOs and design efficient training schemes combining dimension reduction and multi-resolution approaches to reduce computational costs for derivative learning.", "result": "The results confirm that DIFNOs outperform traditional FNOs in terms of sample complexity and accuracy for learning operators and solving PDE-constrained inverse problems, demonstrated through numerical examples across various equations.", "conclusion": "DIFNOs are superior surrogate models that enable accurate and efficient solutions for inverse and optimization problems involving PDEs by incorporating derivative information into training."}}
{"id": "2512.14126", "pdf": "https://arxiv.org/pdf/2512.14126", "abs": "https://arxiv.org/abs/2512.14126", "authors": ["Junyi Wu", "Van Nguyen Nguyen", "Benjamin Planche", "Jiachen Tao", "Changchang Sun", "Zhongpai Gao", "Zhenghao Zhao", "Anwesa Choudhuri", "Gengyu Zhang", "Meng Zheng", "Feiran Wang", "Terrence Chen", "Yan Yan", "Ziyan Wu"], "title": "Consistent Instance Field for Dynamic Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike prior methods that rely on discrete tracking or view-dependent features, our approach disentangles visibility from persistent object identity by modeling each space-time point with an occupancy probability and a conditional instance distribution. To realize this, we introduce a novel instance-embedded representation based on deformable 3D Gaussians, which jointly encode radiance and semantic information and are learned directly from input RGB images and instance masks through differentiable rasterization. Furthermore, we introduce new mechanisms to calibrate per-Gaussian identities and resample Gaussians toward semantically active regions, ensuring consistent instance representations across space and time. Experiments on HyperNeRF and Neu3D datasets demonstrate that our method significantly outperforms state-of-the-art methods on novel-view panoptic segmentation and open-vocabulary 4D querying tasks.", "AI": {"tldr": "This paper presents the Consistent Instance Field, a spatio-temporal representation for understanding dynamic scenes using a probabilistic approach and novel deformable 3D Gaussians.", "motivation": "The study aims to address limitations in dynamic scene understanding caused by reliance on discrete tracking and view-dependent features.", "method": "The method introduces deformable 3D Gaussians that encode radiance and semantics, calibrated for consistent instance representation through differentiable rasterization and semantic resampling.", "result": "The method outperformed state-of-the-art techniques in novel-view panoptic segmentation and open-vocabulary 4D querying tasks on HyperNeRF and Neu3D datasets.", "conclusion": "Consistent Instance Field provides improved dynamic scene understanding, achieving superior performance through a probabilistic and semantic representation approach."}}
{"id": "2512.13730", "pdf": "https://arxiv.org/pdf/2512.13730", "abs": "https://arxiv.org/abs/2512.13730", "authors": ["Wang Jiaqi", "Lan Yi", "Chen Xiang"], "title": "Exploring the Modular Integration of \"AI + Architecture\" Pedagogy in Undergraduate Design Education: A Case Study of Architectural Design III/IV Courses at Zhejiang University", "categories": ["cs.CY", "cs.AI"], "comment": "in Chinese language, AI for architectural design education", "summary": "This study investigates AI integration in architectural education through a teaching experiment in Zhejiang University's 2024-25 grade three undergraduate design studio. Adopting a dual-module framework (20-hour AI training + embedded ethics discussions), the course introduced deep learning models, LLMs, AIGC, LoRA, and ComfyUI while maintaining the original curriculum structure, supported by dedicated technical instructors. Findings demonstrate the effectiveness of phased guidance, balanced technical-ethical approaches, and institutional support. The model improved students' digital skills and strategic cognition while addressing AI ethics, providing a replicable approach combining technical and critical learning in design education.", "AI": {"tldr": "The study developed a dual-module AI-integrated curriculum for design education, improving students' digital skills and ethical awareness.", "motivation": "To explore how integrating AI and ethical considerations can enhance architectural education and equip students with both technical and critical problem-solving skills.", "method": "A dual-module framework combining 20-hour AI training, embedded ethical discussions, deep learning exposure, and expert technical support within an undergraduate design studio context.", "result": "The approach demonstrated enhanced student digital and strategic skills, while fostering an understanding of AI ethics, proving the method's effectiveness and replicability.", "conclusion": "This integrated AI teaching approach offers a balanced method for advancing technical and ethical learning in architectural design education."}}
{"id": "2512.14698", "pdf": "https://arxiv.org/pdf/2512.14698", "abs": "https://arxiv.org/abs/2512.14698", "authors": ["Jun Zhang", "Teng Wang", "Yuying Ge", "Yixiao Ge", "Xinhao Li", "Ying Shan", "Limin Wang"], "title": "TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "Project Page: https://timelens-arc-lab.github.io/", "summary": "This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.", "AI": {"tldr": "This paper introduces TimeLens, a systematic approach to improve video temporal grounding (VTG) by addressing data quality and algorithmic designs, introducing a benchmark and training dataset, and achieving state-of-the-art VTG performance.", "motivation": "To establish a robust baseline for VTG and improve understanding of multimodal large language models' capabilities in this area, focusing on refining evaluation and optimizing training methods.", "method": "The paper introduces a re-annotated VTG benchmark dataset (TimeLens-Bench) and a large-scale high-quality training dataset (TimeLens-100K). It also explores novel algorithmic practices, such as interleaved textual encoding for time representation and a reinforcement learning with verifiable rewards (RLVR) paradigm.", "result": "TimeLens models outperform proprietary models like GPT-5 and Gemini-2.5-Flash in VTG tasks, validating the effectiveness of the new benchmarks, dataset, and algorithmic strategies.", "conclusion": "TimeLens establishes a significant advancement in VTG capability, addressing critical quality issues and providing robust, state-of-the-art models, benchmarks, and tools to facilitate further research."}}
{"id": "2512.14090", "pdf": "https://arxiv.org/pdf/2512.14090", "abs": "https://arxiv.org/abs/2512.14090", "authors": ["Taig Singh", "Shreshth Rajan", "Nikhil Iyer"], "title": "Arithmetic-Intensity-Aware Quantization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.", "AI": {"tldr": "The paper proposes a mixed precision quantization framework (AIQ) to optimize memory performance and throughput of neural networks, focusing on maximizing arithmetic intensity (AI) while minimizing accuracy loss.", "motivation": "The increasing memory-bound nature of modern neural networks limits inference throughput due to DRAM bandwidth rather than computational power. Addressing this bottleneck is crucial for optimizing real-world neural network performance.", "method": "AIQ is a post-training quantization approach that uses search algorithms to balance per-layer arithmetic intensity and minimal accuracy degradation, assigning different bit-widths layer-wise.", "result": "Experiments with ResNet-20/CIFAR-10 showed a ~50% increase in AI with negligible accuracy drop (~1 percentage point), while MobileNetV2 achieved 1.66x higher inference throughput with similar accuracy retention.", "conclusion": "AIQ proves to be an effective method for managing memory bottlenecks, outperforming global uniform quantization and naturally adapting to larger layers, enhancing throughput and preserving accuracy."}}
{"id": "2512.14137", "pdf": "https://arxiv.org/pdf/2512.14137", "abs": "https://arxiv.org/abs/2512.14137", "authors": ["Ashish Mishra", "Tarun Kumar", "Gyanaranjan Nayak", "Arpit Shah", "Suparna Bhattacharya", "Martin Foltin"], "title": "Erasing CLIP Memories: Non-Destructive, Data-Free Zero-Shot class Unlearning in CLIP Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a novel, closed-form approach for selective unlearning in multimodal models, specifically targeting pretrained models such as CLIP. Our method leverages nullspace projection to erase the target class information embedded in the final projection layer, without requiring any retraining or the use of images from the forget set. By computing an orthonormal basis for the subspace spanned by target text embeddings and projecting these directions, we dramatically reduce the alignment between image features and undesired classes. Unlike traditional unlearning techniques that rely on iterative fine-tuning and extensive data curation, our approach is both computationally efficient and surgically precise. This leads to a pronounced drop in zero-shot performance for the target classes while preserving the overall multimodal knowledge of the model. Our experiments demonstrate that even a partial projection can balance between complete unlearning and retaining useful information, addressing key challenges in model decontamination and privacy preservation.", "AI": {"tldr": "A novel approach using nullspace projection for selective unlearning in multimodal models like CLIP, eliminating target class information efficiently without retraining.", "motivation": "To address the challenge of model decontamination and privacy preservation without relying on iterative retraining or extensive data curation.", "method": "Using nullspace projection, the approach erases target class information by projecting target text embedding directions in the model's final projection layer.", "result": "Zero-shot performance drops significantly for target classes while preserving the overall multimodal knowledge of the model.", "conclusion": "The method is computationally efficient, precise, and balances between unlearning targeted information and retaining useful model knowledge."}}
{"id": "2512.14140", "pdf": "https://arxiv.org/pdf/2512.14140", "abs": "https://arxiv.org/abs/2512.14140", "authors": ["Han Zou", "Yan Zhang", "Ruiqi Yu", "Cong Xie", "Jie Huang", "Zhenpeng Zhan"], "title": "SketchAssist: A Practical Assistant for Semantic Edits and Precise Local Redrawing", "categories": ["cs.CV"], "comment": null, "summary": "Sketch editing is central to digital illustration, yet existing image editing systems struggle to preserve the sparse, style-sensitive structure of line art while supporting both high-level semantic changes and precise local redrawing. We present SketchAssist, an interactive sketch drawing assistant that accelerates creation by unifying instruction-guided global edits with line-guided region redrawing, while keeping unrelated regions and overall composition intact. To enable this assistant at scale, we introduce a controllable data generation pipeline that (i) constructs attribute-addition sequences from attribute-free base sketches, (ii) forms multi-step edit chains via cross-sequence sampling, and (iii) expands stylistic coverage with a style-preserving attribute-removal model applied to diverse sketches. Building on this data, SketchAssist employs a unified sketch editing framework with minimal changes to DiT-based editors. We repurpose the RGB channels to encode the inputs, enabling seamless switching between instruction-guided edits and line-guided redrawing within a single input interface. To further specialize behavior across modes, we integrate a task-guided mixture-of-experts into LoRA layers, routing by text and visual cues to improve semantic controllability, structural fidelity, and style preservation. Extensive experiments show state-of-the-art results on both tasks, with superior instruction adherence and style/structure preservation compared to recent baselines. Together, our dataset and SketchAssist provide a practical, controllable assistant for sketch creation and revision.", "AI": {"tldr": "SketchAssist is a sketch-editing tool that combines global edits with precise local redrawing, ensuring style and composition preservation.", "motivation": "Existing sketch editing systems struggle with preserving line art's structure and style while enabling both semantic edits and local revisions.", "method": "SketchAssist introduces a unique data generation pipeline for diverse editing needs, repurposes RGB channels for input flexibility, and integrates a task-guided mixture-of-experts in DiT-based editors.", "result": "The method achieves state-of-the-art results in instruction adherence, structural fidelity, and style preservation, outperforming recent benchmarks.", "conclusion": "SketchAssist offers a scalable tool for practical and controllable sketch editing, enhancing ease of creation and revision."}}
{"id": "2512.14100", "pdf": "https://arxiv.org/pdf/2512.14100", "abs": "https://arxiv.org/abs/2512.14100", "authors": ["Chunjin Jian", "Xinhua Zhu"], "title": "A First-Order Logic-Based Alternative to Reward Models in RLHF", "categories": ["cs.LG", "cs.LO"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.\n  In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.\n  Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.", "AI": {"tldr": "The paper introduces S-GRPO, a logic-similarity-based reward mechanism for aligning large language models with human preferences, departing from conventional heuristic-based methods.", "motivation": "Address the limitations of standard reward models used in aligning language models with human feedback, by introducing an alternative logic-based alignment method.", "method": "Develop a new framework, S-GRPO, which integrates logic consistency, supervised fine-tuning, and multi-objective optimization to guide alignment.", "result": "S-GRPO demonstrates superior performance and robustness compared to existing methods like supervised fine-tuning (SFT) while enhancing preference-learning frameworks.", "conclusion": "S-GRPO offers a promising and task-adaptive approach for improving alignment in large language models."}}
{"id": "2512.14141", "pdf": "https://arxiv.org/pdf/2512.14141", "abs": "https://arxiv.org/abs/2512.14141", "authors": ["Hanning Chen", "Keyu Man", "Kevin Zhu", "Chenguang Zhu", "Haonan Li", "Tongbo Luo", "Xizhou Feng", "Wei Sun", "Sreen Tallam", "Mohsen Imani", "Partha Kanuparthy"], "title": "TorchTraceAP: A New Benchmark Dataset for Detecting Performance Anti-Patterns in Computer Vision Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Identifying and addressing performance anti-patterns in machine learning (ML) models is critical for efficient training and inference, but it typically demands deep expertise spanning system infrastructure, ML models and kernel development. While large tech companies rely on dedicated ML infrastructure engineers to analyze torch traces and benchmarks, such resource-intensive workflows are largely inaccessible to computer vision researchers in general. Among the challenges, pinpointing problematic trace segments within lengthy execution traces remains the most time-consuming task, and is difficult to automate with current ML models, including LLMs. In this work, we present the first benchmark dataset specifically designed to evaluate and improve ML models' ability to detect anti patterns in traces. Our dataset contains over 600 PyTorch traces from diverse computer vision models classification, detection, segmentation, and generation collected across multiple hardware platforms. We also propose a novel iterative approach: a lightweight ML model first detects trace segments with anti patterns, followed by a large language model (LLM) for fine grained classification and targeted feedback. Experimental results demonstrate that our method significantly outperforms unsupervised clustering and rule based statistical techniques for detecting anti pattern regions. Our method also effectively compensates LLM's limited context length and reasoning inefficiencies.", "AI": {"tldr": "The paper introduces a benchmark dataset and proposes a dual-model approach to detect and address performance anti-patterns in machine learning traces.", "motivation": "Performance anti-patterns in ML traces are challenging to identify and address, especially for researchers without access to resource-intensive workflows which large tech companies have.", "method": "The authors constructed a benchmark dataset of over 600 PyTorch traces and proposed a two-step approach: using a lightweight ML model to detect problematic trace segments, followed by an LLM for detailed classification and feedback.", "result": "The proposed method showed superior performance in detecting anti-patterns compared to unsupervised clustering and statistical techniques.", "conclusion": "A combined lightweight ML and LLM approach effectively tackles trace analysis challenges, making it more accessible and accurate for detecting anti-patterns in ML traces."}}
{"id": "2512.14150", "pdf": "https://arxiv.org/pdf/2512.14150", "abs": "https://arxiv.org/abs/2512.14150", "authors": ["Zhijie Zhong", "Zhiwen Yu", "Pengyu Li", "Jianming Lv", "C. L. Philip Chen", "Min Chen"], "title": "PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario", "categories": ["cs.LG", "cs.AI"], "comment": "34 pages, 14 figures, 4 tables. Under review", "summary": "Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are available at: https://emorzz1g.github.io/PathFinder/.", "AI": {"tldr": "The paper introduces PathFinder, a new architecture for radio path loss prediction (RPP) that addresses shortcomings in deep learning RPP models, especially under multi-transmitter scenarios and distribution shifts.", "motivation": "Current RPP methods fail to account for real-world complexities like multi-transmitter scenarios and distribution shifts in building density or transmitter configurations, hindering the optimization of 5G networks and new IoT applications.", "method": "PathFinder uses disentangled feature encoding for buildings and transmitters, Mask-Guided Low-rank Attention to focus on key regions, and a Transmitter-Oriented Mixup strategy for robust training. It also introduces a new benchmark, S2MT-RPP, for evaluating performance under distribution shifts.", "result": "PathFinder significantly outperforms state-of-the-art RPP techniques, especially in multi-transmitter scenarios and under challenging distribution shift conditions.", "conclusion": "PathFinder effectively addresses limitations in current RPP methods, advancing radio path loss prediction for real-world 5G and IoT applications."}}
{"id": "2512.14158", "pdf": "https://arxiv.org/pdf/2512.14158", "abs": "https://arxiv.org/abs/2512.14158", "authors": ["Shuxin Zhao", "Bo Lang", "Nan Xiao", "Yilang Zhang"], "title": "CIS-BA: Continuous Interaction Space Based Backdoor Attack for Object Detection in the Real-World", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Object detection models deployed in real-world applications such as autonomous driving face serious threats from backdoor attacks. Despite their practical effectiveness,existing methods are inherently limited in both capability and robustness due to their dependence on single-trigger-single-object mappings and fragile pixel-level cues. We propose CIS-BA, a novel backdoor attack paradigm that redefines trigger design by shifting from static object features to continuous inter-object interaction patterns that describe how objects co-occur and interact in a scene. By modeling these patterns as a continuous interaction space, CIS-BA introduces space triggers that, for the first time, enable a multi-trigger-multi-object attack mechanism while achieving robustness through invariant geometric relations. To implement this paradigm, we design CIS-Frame, which constructs space triggers via interaction analysis, formalizes them as class-geometry constraints for sample poisoning, and embeds the backdoor during detector training. CIS-Frame supports both single-object attacks (object misclassification and disappearance) and multi-object simultaneous attacks, enabling complex and coordinated effects across diverse interaction states. Experiments on MS-COCO and real-world videos show that CIS-BA achieves over 97% attack success under complex environments and maintains over 95% effectiveness under dynamic multi-trigger conditions, while evading three state-of-the-art defenses. In summary, CIS-BA extends the landscape of backdoor attacks in interaction-intensive scenarios and provides new insights into the security of object detection systems.", "AI": {"tldr": "This paper introduces CIS-BA, a backdoor attack paradigm for object detection systems, utilizing inter-object interaction patterns to enable robust multi-trigger-multi-object attacks.", "motivation": "Current backdoor attack methods for object detection models are limited in capability and robustness, relying on single-trigger-single-object mappings and pixel-level cues. There is a need for a more effective and resilient approach.", "method": "CIS-BA redefines triggers based on continuous inter-object interaction patterns, introducing space triggers modeled as a continuous interaction space. The CIS-Frame implementation analyzes interactions, formalizes triggers as class-geometry constraints, and embeds them into training to support complex attacks.", "result": "Experiments on MS-COCO and real-world videos show over 97% attack success in complex environments and 95% under dynamic multi-trigger conditions, while evading multiple defenses.", "conclusion": "CIS-BA highlights vulnerabilities in object detection systems, demonstrating robust and sophisticated backdoor attack mechanisms, signaling security risks in interaction-intensive scenarios."}}
{"id": "2512.14170", "pdf": "https://arxiv.org/pdf/2512.14170", "abs": "https://arxiv.org/abs/2512.14170", "authors": ["Jonathan Spiegelman", "Guy Amir", "Guy Katz"], "title": "On Improving Deep Active Learning with Formal Verification", "categories": ["cs.LG", "cs.LO"], "comment": null, "summary": "Deep Active Learning (DAL) aims to reduce labeling costs in neural-network training by prioritizing the most informative unlabeled samples for annotation. Beyond selecting which samples to label, several DAL approaches further enhance data efficiency by augmenting the training set with synthetic inputs that do not require additional manual labeling. In this work, we investigate how augmenting the training data with adversarial inputs that violate robustness constraints can improve DAL performance. We show that adversarial examples generated via formal verification contribute substantially more than those produced by standard, gradient-based attacks. We apply this extension to multiple modern DAL techniques, as well as to a new technique that we propose, and show that it yields significant improvements in model generalization across standard benchmarks.", "AI": {"tldr": "Deep Active Learning (DAL) improves training efficiency by labeling selective samples; augmenting with adversarial examples enhances model generalization.", "motivation": "The paper studies how adversarial inputs in training data can boost data efficiency and model performance in Deep Active Learning.", "method": "Synthetic adversarial examples violating robustness constraints are generated via formal verification and integrated into DAL techniques.", "result": "Adversarial examples from formal verification outperform gradient-based ones, improving generalization in DAL across benchmarks.", "conclusion": "Integrating verified adversarial inputs significantly enhances DAL techniques, optimizing model performance and data efficiency."}}
{"id": "2512.14162", "pdf": "https://arxiv.org/pdf/2512.14162", "abs": "https://arxiv.org/abs/2512.14162", "authors": ["Qingyuan Cai", "Linxin Zhang", "Xuecai Hu", "Saihui Hou", "Yongzhen Huang"], "title": "FastDDHPose: Towards Unified, Efficient, and Disentangled 3D Human Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Recent approaches for monocular 3D human pose estimation (3D HPE) have achieved leading performance by directly regressing 3D poses from 2D keypoint sequences. Despite the rapid progress in 3D HPE, existing methods are typically trained and evaluated under disparate frameworks, lacking a unified framework for fair comparison. To address these limitations, we propose Fast3DHPE, a modular framework that facilitates rapid reproduction and flexible development of new methods. By standardizing training and evaluation protocols, Fast3DHPE enables fair comparison across 3D human pose estimation methods while significantly improving training efficiency. Within this framework, we introduce FastDDHPose, a Disentangled Diffusion-based 3D Human Pose Estimation method which leverages the strong latent distribution modeling capability of diffusion models to explicitly model the distributions of bone length and bone direction while avoiding further amplification of hierarchical error accumulation. Moreover, we design an efficient Kinematic-Hierarchical Spatial and Temporal Denoiser that encourages the model to focus on kinematic joint hierarchies while avoiding unnecessary modeling of overly complex joint topologies. Extensive experiments on Human3.6M and MPI-INF-3DHP show that the Fast3DHPE framework enables fair comparison of all methods while significantly improving training efficiency. Within this unified framework, FastDDHPose achieves state-of-the-art performance with strong generalization and robustness in in-the-wild scenarios. The framework and models will be released at: https://github.com/Andyen512/Fast3DHPE", "AI": {"tldr": "The paper proposes Fast3DHPE, a modular framework for efficient, unified 3D human pose estimation, alongside the state-of-the-art FastDDHPose methodology.", "motivation": "Existing methods for 3D human pose estimation lack a unified benchmark, and training frameworks are inefficient, leading to unfair comparisons and difficulty in developing new approaches.", "method": "The study introduces Fast3DHPE, a standardizing framework with uniform training/evaluation protocols, and FastDDHPose, a diffusion-based method employing kinematic-hierarchical denoising to avoid hierarchical error propagation.", "result": "Experiments on Human3.6M and MPI-INF-3DHP datasets demonstrate that Fast3DHPE improves training efficiency and ensures fair comparisons while FastDDHPose achieves state-of-the-art performance, generalization, and robustness.", "conclusion": "Fast3DHPE provides a reliable, efficient framework for 3D pose estimation research, ensuring fair methodology benchmarks. FastDDHPose sets new state-of-the-art standards with superior generalization and real-world applicability."}}
{"id": "2512.14188", "pdf": "https://arxiv.org/pdf/2512.14188", "abs": "https://arxiv.org/abs/2512.14188", "authors": ["Wei Tao", "Sheng Long", "Xin Liu", "Wei Li", "Qing Tao"], "title": "Optimizing the Adversarial Perturbation with a Momentum-based Adaptive Matrix", "categories": ["cs.LG"], "comment": "IEEE Transactions on Dependable and Secure Computing", "summary": "Generating adversarial examples (AEs) can be formulated as an optimization problem. Among various optimization-based attacks, the gradient-based PGD and the momentum-based MI-FGSM have garnered considerable interest. However, all these attacks use the sign function to scale their perturbations, which raises several theoretical concerns from the point of view of optimization. In this paper, we first reveal that PGD is actually a specific reformulation of the projected gradient method using only the current gradient to determine its step-size. Further, we show that when we utilize a conventional adaptive matrix with the accumulated gradients to scale the perturbation, PGD becomes AdaGrad. Motivated by this analysis, we present a novel momentum-based attack AdaMI, in which the perturbation is optimized with an interesting momentum-based adaptive matrix. AdaMI is proved to attain optimal convergence for convex problems, indicating that it addresses the non-convergence issue of MI-FGSM, thereby ensuring stability of the optimization process. The experiments demonstrate that the proposed momentum-based adaptive matrix can serve as a general and effective technique to boost adversarial transferability over the state-of-the-art methods across different networks while maintaining better stability and imperceptibility.", "AI": {"tldr": "This paper introduces a novel adversarial attack method, AdaMI, which employs a momentum-based adaptive matrix for more effective and stable optimization, showing improved adversarial transferability.", "motivation": "The paper aims to address limitations in current optimization-based adversarial attack techniques, particularly issues with step size scaling using the sign function and the non-convergence problem of existing methods like MI-FGSM.", "method": "The authors analyze and reformulate PGD as a variant of the projected gradient method. They then propose the AdaMI attack, which incorporates a momentum-based adaptive matrix that optimizes perturbations, ensuring better stability and convergence.", "result": "The proposed AdaMI method achieves state-of-the-art transferability on adversarial attacks across various networks, along with better stability and imperceptibility, compared to existing methods.", "conclusion": "AdaMI effectively resolves theoretical and practical issues in adversarial attacks, making it a robust and general approach for boosting adversarial transferability and stability."}}
{"id": "2512.14177", "pdf": "https://arxiv.org/pdf/2512.14177", "abs": "https://arxiv.org/abs/2512.14177", "authors": ["Joseph Hoche", "Andrei Bursuc", "David Brellmann", "Gilles Louppe", "Pavel Izmailov", "Angela Yao", "Gianni Franchi"], "title": "Improving Semantic Uncertainty Quantification in LVLMs with Semantic Gaussian Processes", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) often produce plausible but unreliable outputs, making robust uncertainty estimation essential. Recent work on semantic uncertainty estimates relies on external models to cluster multiple sampled responses and measure their semantic consistency. However, these clustering methods are often fragile, highly sensitive to minor phrasing variations, and can incorrectly group or separate semantically similar answers, leading to unreliable uncertainty estimates. We propose Semantic Gaussian Process Uncertainty (SGPU), a Bayesian framework that quantifies semantic uncertainty by analyzing the geometric structure of answer embeddings, avoiding brittle clustering. SGPU maps generated answers into a dense semantic space, computes the Gram matrix of their embeddings, and summarizes their semantic configuration via the eigenspectrum. This spectral representation is then fed into a Gaussian Process Classifier that learns to map patterns of semantic consistency to predictive uncertainty, and that can be applied in both black-box and white-box settings. Across six LLMs and LVLMs on eight datasets spanning VQA, image classification, and textual QA, SGPU consistently achieves state-of-the-art calibration (ECE) and discriminative (AUROC, AUARC) performance. We further show that SGPU transfers across models and modalities, indicating that its spectral representation captures general patterns of semantic uncertainty.", "AI": {"tldr": "This paper introduces SGPU, a new Bayesian framework to estimate uncertainty in large vision-language models (LVLMs) by analyzing the geometric structure of answer embeddings, avoiding conventional clustering methods.", "motivation": "Current uncertainty estimation methods in LVLMs are unreliable due to their reliance on clustering approaches, which are sensitive to minor phrasing changes and semantic variability.", "method": "The SGPU framework evaluates the semantic uncertainty of LVLM outputs by embedding generated answers into a dense semantic space, analyzing their geometric structure through the spectral analysis of a Gram matrix, and using a Gaussian Process Classifier to map these patterns to predictive uncertainty.", "result": "SGPU demonstrates superior calibration and discriminative performance across six LLMs and LVLMs on eight datasets, outperforming existing methods, and shows transferable capabilities across models and modalities.", "conclusion": "SGPU offers a robust, transferable, and generalizable approach for semantic uncertainty estimation, providing reliable performance across various models and tasks without the fragility of traditional clustering techniques."}}
{"id": "2512.14190", "pdf": "https://arxiv.org/pdf/2512.14190", "abs": "https://arxiv.org/abs/2512.14190", "authors": ["Stefano Goria", "Levent A. Meng\u00fct\u00fcrk", "Murat C. Meng\u00fct\u00fcrk", "Berkan Sesen"], "title": "Random-Bridges as Stochastic Transports for Generative Models", "categories": ["cs.LG", "math.PR"], "comment": null, "summary": "This paper motivates the use of random-bridges -- stochastic processes conditioned to take target distributions at fixed timepoints -- in the realm of generative modelling. Herein, random-bridges can act as stochastic transports between two probability distributions when appropriately initialized, and can display either Markovian or non-Markovian, and either continuous, discontinuous or hybrid patterns depending on the driving process. We show how one can start from general probabilistic statements and then branch out into specific representations for learning and simulation algorithms in terms of information processing. Our empirical results, built on Gaussian random bridges, produce high-quality samples in significantly fewer steps compared to traditional approaches, while achieving competitive Frechet inception distance scores. Our analysis provides evidence that the proposed framework is computationally cheap and suitable for high-speed generation tasks.", "AI": {"tldr": "The paper introduces random-bridges as stochastic processes conditioned on target distributions, serving as efficient transports in generative modeling.", "motivation": "The paper aims to improve generative modeling by leveraging random-bridges to act as flexible and effective stochastic transports between distributions.", "method": "The authors developed generative modeling frameworks using random-bridges, focusing on representations for learning and simulation algorithms.", "result": "Empirical tests with Gaussian random bridges produce better-quality samples in fewer steps, achieving competitive performance metrics like Frechet inception distance.", "conclusion": "The random-bridge framework is computationally efficient, reduces steps in sampling, and is suitable for fast generative tasks."}}
{"id": "2512.14180", "pdf": "https://arxiv.org/pdf/2512.14180", "abs": "https://arxiv.org/abs/2512.14180", "authors": ["Francesco Di Sario", "Daniel Rebain", "Dor Verbin", "Marco Grangetto", "Andrea Tagliasacchi"], "title": "Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere", "categories": ["cs.CV"], "comment": null, "summary": "Radiance field methods (e.g. 3D Gaussian Splatting) have emerged as a powerful paradigm for novel view synthesis, yet their appearance modeling often relies on Spherical Harmonics (SH), which impose fundamental limitations. SH struggle with high-frequency signals, exhibit Gibbs ringing artifacts, and fail to capture specular reflections - a key component of realistic rendering. Although alternatives like spherical Gaussians offer improvements, they add significant optimization complexity. We propose Spherical Voronoi (SV) as a unified framework for appearance representation in 3D Gaussian Splatting. SV partitions the directional domain into learnable regions with smooth boundaries, providing an intuitive and stable parameterization for view-dependent effects. For diffuse appearance, SV achieves competitive results while keeping optimization simpler than existing alternatives. For reflections - where SH fail - we leverage SV as learnable reflection probes, taking reflected directions as input following principles from classical graphics. This formulation attains state-of-the-art results on synthetic and real-world datasets, demonstrating that SV offers a principled, efficient, and general solution for appearance modeling in explicit 3D representations.", "AI": {"tldr": "The paper proposes Spherical Voronoi (SV) as an efficient and unified framework for improved appearance modeling in 3D Gaussian Splatting.", "motivation": "Current radiance field methods rely on Spherical Harmonics (SH) for appearance modeling, which struggle with high-frequency signals, specular reflections, and optimization issues.", "method": "The paper introduces SV, a learnable partitioning of the directional domain with smooth boundaries, for more intuitive view-dependent effects and reflection modeling.", "result": "SV demonstrates competitive performance for diffuse appearance and achieves state-of-the-art results for reflection modeling on both synthetic and real-world datasets.", "conclusion": "SV provides a simpler, stable, and principled solution for realistic appearance modeling in explicit 3D representations."}}
{"id": "2512.13737", "pdf": "https://arxiv.org/pdf/2512.13737", "abs": "https://arxiv.org/abs/2512.13737", "authors": ["Nardine Osman", "Manel Rodriguez-Soto", "Jordi Sabater-Mir"], "title": "Instilling Organisational Values in Firefighters through Simulation-Based Training", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "In firefighting and other emergency operations, decisions made under pressure carry profound ethical weight and can significantly impact incident outcomes and firefighter safety. Traditional training methods, while foundational, often fall short in adequately preparing firefighters for the complex ethical dilemmas and value conflicts inherent in chaotic emergency environments. This paper proposes a conceptual framework for enhancing firefighter training by systematically integrating departmental values into simulation-based training. This approach fosters deeper value internalisation and improves value-driven decision-making under pressure. Furthermore, the underlying tools can also be leveraged to evaluate and refine departmental operational protocols for better alignment with preferred values.", "AI": {"tldr": "The paper presents a framework for integrating departmental values into simulation training to enhance firefighter decision-making under pressure.", "motivation": "Traditional training methods for firefighters are inadequate in preparing them for ethical dilemmas and value conflicts during emergencies.", "method": "The paper proposes a framework that integrates departmental values into simulation-based training to internalize values and improve decision-making.", "result": "Enhanced decision-making under pressure and tools for evaluating departmental protocols that align with values.", "conclusion": "Integrating values into training improves safety, operational outcomes, and reinforces ethical and value-based decision-making for firefighters."}}
{"id": "2512.14202", "pdf": "https://arxiv.org/pdf/2512.14202", "abs": "https://arxiv.org/abs/2512.14202", "authors": ["Timo Klein", "Thomas Lang", "Andrii Shkabrii", "Alexander Sturm", "Kevin Sidak", "Lukas Miklautz", "Claudia Plant", "Yllka Velaj", "Sebastian Tschiatschek"], "title": "Understanding and Improving Hyperbolic Deep Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincar\u00e9 Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .", "AI": {"tldr": "The paper introduces Hyper++, a novel reinforcement learning (RL) agent leveraging hyperbolic geometry for better stability and performance. It addresses optimization challenges and improves both training efficiency and performance over Euclidean and hyperbolic baselines.", "motivation": "The motivation is to enhance RL agent performance through better feature representations by using hyperbolic spaces, which naturally capture hierarchical and relational structures in RL environments, while addressing their associated optimization challenges.", "method": "Hyper++ incorporates three innovations: (i) a categorical value loss for critic training to ensure stability, (ii) feature regularization to bound norms without dimensionality issues, and (iii) an optimization-friendly formulation of hyperbolic network layers.", "result": "Hyper++ ensures stable learning, improves performance significantly on RL environments like ProcGen and Atari-5, and reduces training time by 30%.", "conclusion": "Hyper++ provides superior stability and performance over existing RL agents due to well-designed mechanisms for using hyperbolic geometry effectively during training."}}
{"id": "2512.14196", "pdf": "https://arxiv.org/pdf/2512.14196", "abs": "https://arxiv.org/abs/2512.14196", "authors": ["Cassandra Krause", "Mattias P. Heinrich", "Ron Keuth"], "title": "Fracture Morphology Classification: Local Multiclass Modeling for Multilabel Complexity", "categories": ["cs.CV"], "comment": "Accepted as poster at the German Conference on Medical Image Computing 2026", "summary": "Between $15\\,\\%$ and $45\\,\\%$ of children experience a fracture during their growth years, making accurate diagnosis essential. Fracture morphology, alongside location and fragment angle, is a key diagnostic feature. In this work, we propose a method to extract fracture morphology by assigning automatically global AO codes to corresponding fracture bounding boxes. This approach enables the use of public datasets and reformulates the global multilabel task into a local multiclass one, improving the average F1 score by $7.89\\,\\%$. However, performance declines when using imperfect fracture detectors, highlighting challenges for real-world deployment. Our code is available on GitHub.", "AI": {"tldr": "The paper proposes a method to extract fracture morphology in children, reformulating the task for better diagnostic performance using AO codes.", "motivation": "Many children experience fractures during growth, making accurate diagnosis crucial. The study aims to enhance fracture morphology extraction for diagnosis.", "method": "Proposes method assigning global AO codes to fracture bounding boxes to transform a global multilabel task into a local multiclass one.", "result": "Improved the average F1 score by 7.89% but identified performance deterioration with imperfect fracture detectors.", "conclusion": "The method enhances fracture diagnosis, but deployment faces challenges due to reliance on fracture detection performance."}}
{"id": "2512.14220", "pdf": "https://arxiv.org/pdf/2512.14220", "abs": "https://arxiv.org/abs/2512.14220", "authors": ["Marthe Ballon", "Andres Algaba", "Brecht Verbeken", "Vincent Ginis"], "title": "Estimating problem difficulty without ground truth using Large Language Model comparisons", "categories": ["cs.LG", "cs.AI"], "comment": "19 pages, 10 figures", "summary": "Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \\geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\\%$ degradation in Pearson correlation for $10\\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.", "AI": {"tldr": "This paper introduces 'LLM compare,' a method for estimating the difficulty of synthetic problems using pairwise comparisons by large language models, overcoming issues in scalability and dependence on ground truth.", "motivation": "Existing methods for estimating problem difficulty are limited by being unscalable, time-consuming, and dependent on ground truth data, especially for unsolvable problems.", "method": "LLM compare uses large language models to perform pairwise difficulty comparisons and calculates Bradley-Terry scores. It is continuous, dynamic, model-agnostic, and ground truth-independent.", "result": "The method aligns strongly with human annotations (Pearson r \u2265 0.80 for n=1876) and is robust to hallucinations, showing less than 6% degradation in performance with 10% noise injection.", "conclusion": "LLM compare has the potential to replace human annotations in problem difficulty estimation and can aid in curriculum design, model evaluation, and AI research, making a significant advancement in synthetic data generation."}}
{"id": "2512.14200", "pdf": "https://arxiv.org/pdf/2512.14200", "abs": "https://arxiv.org/abs/2512.14200", "authors": ["Zhuoxiao Li", "Wenzong Ma", "Taoyu Wu", "Jinjing Zhu", "Zhenchao Q", "Shuai Zhang", "Jing Ou", "Yinrui Ren", "Weiqing Qi", "Guobin Shen", "Hui Xiong", "Wufan Zhao"], "title": "Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Neural Radiance Fields and 3D Gaussian Splatting have demonstrated strong potential for large-scale UAV-based 3D reconstruction tasks by fitting the appearance of images. However, real-world large-scale captures are often based on multi-temporal data capture, where illumination inconsistencies across different times of day can significantly lead to color artifacts, geometric inaccuracies, and inconsistent appearance. Due to the lack of UAV datasets that systematically capture the same areas under varying illumination conditions, this challenge remains largely underexplored. To fill this gap, we introduceSkyLume, a large-scale, real-world UAV dataset specifically designed for studying illumination robust 3D reconstruction in urban scene modeling: (1) We collect data from 10 urban regions data comprising more than 100k high resolution UAV images (four oblique views and nadir), where each region is captured at three periods of the day to systematically isolate illumination changes. (2) To support precise evaluation of geometry and appearance, we provide per-scene LiDAR scans and accurate 3D ground-truth for assessing depth, surface normals, and reconstruction quality under varying illumination. (3) For the inverse rendering task, we introduce the Temporal Consistency Coefficient (TCC), a metric that measuress cross-time albedo stability and directly evaluates the robustness of the disentanglement of light and material. We aim for this resource to serve as a foundation that advances research and real-world evaluation in large-scale inverse rendering, geometry reconstruction, and novel view synthesis.", "AI": {"tldr": "SkyLume introduces a UAV dataset targeting illumination-robust 3D reconstruction using multi-temporal urban scene data.", "motivation": "To address challenges in 3D reconstruction caused by illumination inconsistencies in multi-temporal UAV-based captures.", "method": "Developed the SkyLume dataset consisting of multi-temporal UAV imagery, LiDAR scans, ground-truth geometry data, and a new evaluation metric, TCC.", "result": "Created a dataset with over 100k UAV images from 10 urban regions, enhanced with LiDAR and ground-truth geometry, systematically captured under varying daylight conditions.", "conclusion": "SkyLume will enable advancements in 3D reconstruction tasks, novel view synthesis, and robust inverse rendering evaluation in real-world settings."}}
{"id": "2512.14240", "pdf": "https://arxiv.org/pdf/2512.14240", "abs": "https://arxiv.org/abs/2512.14240", "authors": ["Erion Morina", "Martin Holler"], "title": "Physically consistent model learning for reaction-diffusion systems", "categories": ["cs.LG", "math.AP", "math.OC"], "comment": null, "summary": "This paper addresses the problem of learning reaction-diffusion (RD) systems from data while ensuring physical consistency and well-posedness of the learned models. Building on a regularization-based framework for structured model learning, we focus on learning parameterized reaction terms and investigate how to incorporate key physical properties, such as mass conservation and quasipositivity, directly into the learning process. Our main contributions are twofold: First, we propose techniques to systematically modify a given class of parameterized reaction terms such that the resulting terms inherently satisfy mass conservation and quasipositivity, ensuring that the learned RD systems preserve non-negativity and adhere to physical principles. These modifications also guarantee well-posedness of the resulting PDEs under additional regularity and growth conditions. Second, we extend existing theoretical results on regularization-based model learning to RD systems using these physically consistent reaction terms. Specifically, we prove that solutions to the learning problem converge to a unique, regularization-minimizing solution of a limit system even when conservation laws and quasipositivity are enforced. In addition, we provide approximation results for quasipositive functions, essential for constructing physically consistent parameterizations. These results advance the development of interpretable and reliable data-driven models for RD systems that align with fundamental physical laws.", "AI": {"tldr": "This paper develops a framework to learn reaction-diffusion systems from data while ensuring physical laws like mass conservation and quasipositivity are respected.", "motivation": "To create interpretable and reliable reaction-diffusion models that align with fundamental physical principles like mass conservation and quasipositivity.", "method": "A regularization-based learning framework is proposed to parameterize reaction terms, add physical constraints (mass conservation and quasipositivity), and analyze theoretical results to ensure model well-posedness.", "result": "The framework ensures learned models preserve physical principles, guarantees well-posedness under certain conditions, and provides theoretical convergence and approximation results for quasipositive functions.", "conclusion": "This approach enables the development of data-driven reaction-diffusion models that are physically consistent, interpretable, and reliable."}}
{"id": "2512.13744", "pdf": "https://arxiv.org/pdf/2512.13744", "abs": "https://arxiv.org/abs/2512.13744", "authors": ["Udayon Sen", "Alka Luqman", "Anupam Chattopadhyay"], "title": "Toward Noise-Aware Audio Deepfake Detection: Survey, SNR-Benchmarks, and Practical Recipes", "categories": ["cs.SD", "cs.AI"], "comment": "6 pages", "summary": "Deepfake audio detection has progressed rapidly with strong pre-trained encoders (e.g., WavLM, Wav2Vec2, MMS). However, performance in realistic capture conditions - background noise (domestic/office/transport), room reverberation, and consumer channels - often lags clean-lab results. We survey and evaluate robustness for state-of-the-art audio deepfake detection models and present a reproducible framework that mixes MS-SNSD noises with ASVspoof 2021 DF utterances to evaluate under controlled signal-to-noise ratios (SNRs). SNR is a measured proxy for noise severity used widely in speech; it lets us sweep from near-clean (35 dB) to very noisy (-5 dB) to quantify graceful degradation. We study multi-condition training and fixed-SNR testing for pretrained encoders (WavLM, Wav2Vec2, MMS), reporting accuracy, ROC-AUC, and EER on binary and four-class (authenticity x corruption) tasks. In our experiments, finetuning reduces EER by 10-15 percentage points at 10-0 dB SNR across backbones.", "AI": {"tldr": "The paper examines the robustness of deepfake audio detection models under noisy, realistic conditions using state-of-the-art encoders and proposes a framework for evaluating them with controlled signal-to-noise ratios (SNRs).", "motivation": "To address the performance gap of deepfake audio detection models in real-world conditions, which include background noise, room reverberation, and consumer channels.", "method": "The authors propose a reproducible framework that combines MS-SNSD noises with ASVspoof 2021 DF utterances. They evaluate under a variety of SNRs ranging from near-clean (35 dB) to very noisy (-5 dB) and explore multi-condition training and fixed-SNR testing on different pre-trained encoders (WavLM, Wav2Vec2, MMS).", "result": "Fine-tuning the models leads to a 10-15% reduction in Equal Error Rate (EER) at low SNRs (10-0 dB), demonstrating improved performance across pre-trained encoders.", "conclusion": "Deepfake audio detection models can achieve better robustness in noisy environments through fine-tuning and multi-condition training, highlighting the importance of realistic performance evaluation frameworks."}}
{"id": "2512.14241", "pdf": "https://arxiv.org/pdf/2512.14241", "abs": "https://arxiv.org/abs/2512.14241", "authors": ["Salvatore Romano", "Marco Grassia", "Giuseppe Mangioni"], "title": "Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning", "categories": ["cs.LG", "cs.AI", "physics.soc-ph"], "comment": "16 pages, 4 figures", "summary": "Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.", "AI": {"tldr": "This paper introduces a new evaluation method called RGM for Graph Generative Models (GGMs) to address the limitations of the commonly used Maximum Mean Discrepancy metric.", "motivation": "Current evaluation methods for Graph Generative Models, specifically Maximum Mean Discrepancy (MMD), fail to adequately assess the structural fidelity of generated graphs.", "method": "The authors propose a new evaluation methodology named Representation-aware Graph-generation Model evaluation (RGM). They test this approach by analyzing two advanced GGMs, GRAN and EDGE, using a Geometric Deep Learning model on synthetic and real-world graph datasets.", "result": "The study finds that while GGMs such as GRAN and EDGE perform well on some topological metrics, they struggle in preserving structural characteristics necessary for differentiating graph domains. It also highlights the inadequacy of MMD and suggests alternative evaluation strategies.", "conclusion": "RGM proves to be an effective evaluation approach for GGMs, revealing structural weaknesses in current models and encouraging the exploration of refined methods beyond MMD."}}
{"id": "2512.14225", "pdf": "https://arxiv.org/pdf/2512.14225", "abs": "https://arxiv.org/abs/2512.14225", "authors": ["Tao Tang", "Enhui Ma", "xia zhou", "Letian Wang", "Tianyi Yan", "Xueyang Zhang", "Kun Zhan", "Peng Jia", "XianPeng Lang", "Jia-Wang Bian", "Kaicheng Yu", "Xiaodan Liang"], "title": "OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving", "categories": ["cs.CV"], "comment": "ACM MM 2025", "summary": "Autonomous driving has seen remarkable advancements, largely driven by extensive real-world data collection. However, acquiring diverse and corner-case data remains costly and inefficient. Generative models have emerged as a promising solution by synthesizing realistic sensor data. However, existing approaches primarily focus on single-modality generation, leading to inefficiencies and misalignment in multimodal sensor data. To address these challenges, we propose OminiGen, which generates aligned multimodal sensor data in a unified framework. Our approach leverages a shared Bird\\u2019s Eye View (BEV) space to unify multimodal features and designs a novel generalizable multimodal reconstruction method, UAE, to jointly decode LiDAR and multi-view camera data. UAE achieves multimodal sensor decoding through volume rendering, enabling accurate and flexible reconstruction. Furthermore, we incorporate a Diffusion Transformer (DiT) with a ControlNet branch to enable controllable multimodal sensor generation. Our comprehensive experiments demonstrate that OminiGen achieves desired performances in unified multimodal sensor data generation with multimodal consistency and flexible sensor adjustments.", "AI": {"tldr": "The paper introduces OminiGen, a system generating aligned multimodal sensor data for autonomous driving using a unified framework, addressing inefficiencies in existing single-modality approaches.", "motivation": "Acquiring diverse real-world data for autonomous driving is expensive and inefficient; generative models can synthesize cost-effective diverse sensor data but lack multimodal alignment.", "method": "OminiGen is proposed with a shared Bird's Eye View (BEV) space for unified multimodal features and a UAE method for joint decoding of LiDAR and camera data via volume rendering and a Diffusion Transformer for control.", "result": "Experiments show OminiGen produces consistent multimodal sensor data with flexible adjustments across modalities.", "conclusion": "OminiGen provides a unified framework to effectively generate multimodal sensor data, addressing inefficiencies in single-modality generative approaches and supporting flexible autonomous driving data synthesis."}}
{"id": "2512.13745", "pdf": "https://arxiv.org/pdf/2512.13745", "abs": "https://arxiv.org/abs/2512.13745", "authors": ["Xiuying Zhang", "Qinsheng Zhu", "Xiaodong Xing"], "title": "A Spatio-Temporal Hybrid Quantum-Classical Graph Convolutional Neural Network Approach for Urban Taxi Destination Prediction", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "We propose a Hybrid Spatio-Temporal Quantum Graph Convolutional Network (H-STQGCN) algorithm by combining the strengths of quantum computing and classical deep learning to predict the taxi destination within urban road networks. Our algorithm consists of two branches: spatial processing and time evolution. Regarding the spatial processing, the classical module encodes the local topological features of the road network based on the GCN method, and the quantum module is designed to map graph features onto parameterized quantum circuits through a differentiable pooling layer. The time evolution is solved by integrating multi-source contextual information and capturing dynamic trip dependencies on the classical TCN theory. Finally, our experimental results demonstrate that the proposed algorithm outperforms the current methods in terms of prediction accuracy and stability, validating the unique advantages of the quantum-enhanced mechanism in capturing high-dimensional spatial dependencies.", "AI": {"tldr": "The paper introduces a Hybrid Spatio-Temporal Quantum Graph Convolutional Network (H-STQGCN) that combines quantum computing and deep learning for accurate taxi destination prediction.", "motivation": "The motivation is to enhance the efficiency and accuracy of destination prediction in urban road networks by leveraging the unique capabilities of quantum computing and classical deep learning.", "method": "They developed H-STQGCN with two branches: a classical-quantum hybrid for spatial processing using GCN and parameterized quantum circuits, and a classical method for time evolution using TCN to capture dynamic dependencies.", "result": "The experimental results demonstrate that H-STQGCN outperforms existing methods in prediction accuracy and stability, showcasing the effectiveness of the quantum-classical hybrid approach.", "conclusion": "The findings validate the advantages of integrating quantum mechanisms with classical learning, particularly in modeling high-dimensional spatial dependencies for urban mobility prediction tasks."}}
{"id": "2512.14253", "pdf": "https://arxiv.org/pdf/2512.14253", "abs": "https://arxiv.org/abs/2512.14253", "authors": ["Xingjian Wu", "Hanyin Cheng", "Xiangfei Qiu", "Zhengyu Li", "Jilin Hu", "Chenjuan Guo", "Bin Yang"], "title": "FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.", "AI": {"tldr": "The paper discusses FLAME, a lightweight Time Series Foundation Model with strong generalization and probabilistic forecasting capabilities.", "motivation": "Develop efficient and robust methods for deterministic and probabilistic forecasting in time series analysis.", "method": "Leverages Legendre Memory for generalization and introduces Normalization Flow-based forecasting head for modeling complex distributions.", "result": "FLAME achieves state-of-the-art zero-shot performance on deterministic and probabilistic tasks, as demonstrated on benchmarks such as TSFM-Bench and ProbTS.", "conclusion": "FLAME provides an efficient and capable solution for time series forecasting, successfully balancing accuracy and computational efficiency."}}
{"id": "2512.14232", "pdf": "https://arxiv.org/pdf/2512.14232", "abs": "https://arxiv.org/abs/2512.14232", "authors": ["Rawan Alyahya", "Asrar Alruwayqi", "Atheer Alqarni", "Asma Alkhaldi", "Metab Alkubeyyer", "Xin Gao", "Mona Alshahrani"], "title": "Multi-View MRI Approach for Classification of MGMT Methylation in Glioblastoma Patients", "categories": ["cs.CV"], "comment": null, "summary": "The presence of MGMT promoter methylation significantly affects how well chemotherapy works for patients with Glioblastoma Multiforme (GBM). Currently, confirmation of MGMT promoter methylation relies on invasive brain tumor tissue biopsies. In this study, we explore radiogenomics techniques, a promising approach in precision medicine, to identify genetic markers from medical images. Using MRI scans and deep learning models, we propose a new multi-view approach that considers spatial relationships between MRI views to detect MGMT methylation status. Importantly, our method extracts information from all three views without using a complicated 3D deep learning model, avoiding issues associated with high parameter count, slow convergence, and substantial memory demands. We also introduce a new technique for tumor slice extraction and show its superiority over existing methods based on multiple evaluation metrics. By comparing our approach to state-of-the-art models, we demonstrate the efficacy of our method. Furthermore, we share a reproducible pipeline of published models, encouraging transparency and the development of robust diagnostic tools. Our study highlights the potential of non-invasive methods for identifying MGMT promoter methylation and contributes to advancing precision medicine in GBM treatment.", "AI": {"tldr": "This study proposes a non-invasive multi-view deep learning approach using MRI scans to detect MGMT promoter methylation status in GBM patients, avoiding challenges of 3D models and showing high efficacy.", "motivation": "To replace invasive tissue biopsies with non-invasive radiogenomics methods for identifying MGMT promoter methylation in GBM patients, enabling better precision medicine approaches.", "method": "Developed a multi-view deep learning approach using MRI scans, which avoids the complications of 3D models, and introduced a new tumor slice extraction technique.", "result": "The proposed method effectively detects MGMT methylation status, is superior to existing methods, and encourages reproducibility with a shared pipeline.", "conclusion": "The study demonstrates the promise of non-invasive methods in GBM diagnosis, advancing precision medicine and offering robust diagnostic tools."}}
{"id": "2512.14263", "pdf": "https://arxiv.org/pdf/2512.14263", "abs": "https://arxiv.org/abs/2512.14263", "authors": ["Nick Leenders", "Thomas Quadt", "Boris Cule", "Roy Lindelauf", "Herman Monsuur", "Joost van Oijen", "Mark Voskuijl"], "title": "Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.", "AI": {"tldr": "The paper presents a decision tree-based surrogate model for Preferential Bayesian Optimization, addressing limitations of Gaussian Process-based approaches and demonstrating improved performance on specific optimizations and real-world datasets.", "motivation": "To overcome the challenges of Gaussian Process-based methods such as lack of interpretability, difficulty handling categorical data, and computational inefficiency, thereby improving usability in real-world scenarios.", "method": "Development of an interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, designed to scale to large datasets. The model is tested through numerical experiments and real-world applications.", "result": "The model outperforms Gaussian Process-based alternatives in spiky optimization scenarios and performs competitively in non-spiky cases, with successful application to the Sushi dataset. Initial work on incorporating historical preference data is also promising.", "conclusion": "The proposed decision-tree surrogate model offers a viable and interpretable alternative for Preferential Bayesian Optimization, with superior performance in certain cases and satisfactory results in real-world applications."}}
{"id": "2512.14234", "pdf": "https://arxiv.org/pdf/2512.14234", "abs": "https://arxiv.org/abs/2512.14234", "authors": ["Juze Zhang", "Changan Chen", "Xin Chen", "Heng Yu", "Tiange Xiang", "Ali Sartaz Khan", "Shrinidhi K. Lakshmikanth", "Ehsan Adeli"], "title": "ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body", "categories": ["cs.CV"], "comment": "Project page: https://ai.stanford.edu/~juze/ViBES/", "summary": "Human communication is inherently multimodal and social: words, prosody, and body language jointly carry intent. Yet most prior systems model human behavior as a translation task co-speech gesture or text-to-motion that maps a fixed utterance to motion clips-without requiring agentic decision-making about when to move, what to do, or how to adapt across multi-turn dialogue. This leads to brittle timing, weak social grounding, and fragmented stacks where speech, text, and motion are trained or inferred in isolation. We introduce ViBES (Voice in Behavioral Expression and Synchrony), a conversational 3D agent that jointly plans language and movement and executes dialogue-conditioned body actions. Concretely, ViBES is a speech-language-behavior (SLB) model with a mixture-of-modality-experts (MoME) backbone: modality-partitioned transformer experts for speech, facial expression, and body motion. The model processes interleaved multimodal token streams with hard routing by modality (parameters are split per expert), while sharing information through cross-expert attention. By leveraging strong pretrained speech-language models, the agent supports mixed-initiative interaction: users can speak, type, or issue body-action directives mid-conversation, and the system exposes controllable behavior hooks for streaming responses. We further benchmark on multi-turn conversation with automatic metrics of dialogue-motion alignment and behavior quality, and observe consistent gains over strong co-speech and text-to-motion baselines. ViBES goes beyond \"speech-conditioned motion generation\" toward agentic virtual bodies where language, prosody, and movement are jointly generated, enabling controllable, socially competent 3D interaction. Code and data will be made available at: ai.stanford.edu/~juze/ViBES/", "AI": {"tldr": "The paper introduces ViBES, a conversational 3D agent that generates synchronized language and body movements for interactive multimodal communication.", "motivation": "To address limitations in prior systems that model human behavior as isolated tasks, lacking agentic decision-making and social adaptability in multimodal conversations.", "method": "The ViBES model employs a speech-language-behavior (SLB) architecture with a mixture-of-modality-experts (MoME) backbone, using transformer experts and cross-expert attention.", "result": "ViBES demonstrates superior dialogue-motion alignment and behavior quality in benchmarks over baseline models in multi-turn conversations.", "conclusion": "ViBES progresses beyond traditional speech-to-motion systems by enabling synchronized and socially adaptive 3D interactions, providing a foundation for advanced conversational agents."}}
{"id": "2512.14338", "pdf": "https://arxiv.org/pdf/2512.14338", "abs": "https://arxiv.org/abs/2512.14338", "authors": ["Michael Murray", "Tenzin Chan", "Kedar Karhadker", "Christopher J. Hillar"], "title": "Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits", "categories": ["cs.LG"], "comment": null, "summary": "Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.", "AI": {"tldr": "This study explores the role of symmetries and invariance in classical Hopfield networks when training on group-structured data.", "motivation": "The paper aims to uncover how invariance emerges in neural networks when training on group-structured data, specifically within Hopfield networks, and its role in learning graph isomorphism classes.", "method": "Analysis of classical Hopfield networks while training on graph isomorphism tasks and studying the effect of gradient descent minimization and sample complexities.", "result": "Key findings include (i) representation of graph isomorphism classes in a three-dimensional invariant subspace, (ii) implicit bias in gradient descent towards norm-efficient solutions, and (iii) convergence of parameters toward invariant subspace as data samples increase.", "conclusion": "The study demonstrates that norm efficiency biases in learning are central to generalization and invariance emergence in Hopfield networks."}}
{"id": "2512.14235", "pdf": "https://arxiv.org/pdf/2512.14235", "abs": "https://arxiv.org/abs/2512.14235", "authors": ["Jimmie Kwok", "Holger Caesar", "Andras Palffy"], "title": "4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation", "categories": ["cs.CV"], "comment": null, "summary": "Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.", "AI": {"tldr": "The paper introduces 4D-RaDiff, a framework for generating synthetic radar point clouds for improved object detection, addressing the challenge of limited annotated radar data.", "motivation": "The motivation is to overcome the challenges posed by the lack of annotated radar data while leveraging the cost-effectiveness and robustness of radar technology in automotive applications.", "method": "The authors propose 4D-RaDiff, which generates radar point clouds using a latent space diffusion method. It applies conditions at the object or scene level to create realistic annotations or transform LiDAR data into radar scenes.", "result": "Using 4D-RaDiff as a data augmentation method improves object detection accuracy compared to using only real data. Additionally, pre-training on synthetic data reduces the need for annotated radar data by up to 90%, achieving comparable detection performance.", "conclusion": "The study highlights the effectiveness of 4D-RaDiff in generating synthetic radar data to enhance object detection tasks and reduce dependency on annotated data, paving the way for advancements in radar-based perception systems."}}
{"id": "2512.13750", "pdf": "https://arxiv.org/pdf/2512.13750", "abs": "https://arxiv.org/abs/2512.13750", "authors": ["Ezieddin Elmahjub"], "title": "The algorithmic muse and the public domain: Why copyrights legal philosophy precludes protection for generative AI outputs", "categories": ["cs.CY", "cs.AI"], "comment": "9 pages, two figures", "summary": "Generative AI (GenAI) outputs are not copyrightable. This article argues why. We bypass conventional doctrinal analysis that focuses on black letter law notions of originality and authorship to re-evaluate copyright's foundational philosophy. GenAI fundamentally severs the direct human creative link to expressive form. Traditional theories utilitarian incentive, labor desert and personality fail to provide coherent justification for protection. The public domain constitutes the default baseline for intellectual creations. Those seeking copyright coverage for GenAI outputs bear the burden of proof. Granting copyright to raw GenAI outputs would not only be philosophically unsound but would also trigger an unprecedented enclosure of the digital commons, creating a legal quagmire and stifling future innovation. The paper advocates for a clear distinction: human creative contributions to AI-generated works may warrant protection, but the raw algorithmic output should remain in the public domain.", "AI": {"tldr": "The article argues that generative AI outputs should not be copyrighted due to lack of human creative linkage and philosophical justification.", "motivation": "To challenge traditional copyright doctrines concerning originality and authorship for AI-produced works and highlight the consequences of granting copyright protection to AI outputs.", "method": "The article bypasses conventional doctrinal analysis and re-evaluates copyright's foundational philosophies, examining theories such as utilitarian incentive, labor desert, and personality.", "result": "It finds that none of the traditional copyright justifications support protection for generative AI outputs, emphasizing the risks of enclosing public domain and legal dilemmas.", "conclusion": "Human contributions to AI-generated works may deserve copyright protection, but raw AI outputs should remain in the public domain to prevent stifling innovation and preserve the digital commons."}}
{"id": "2512.14361", "pdf": "https://arxiv.org/pdf/2512.14361", "abs": "https://arxiv.org/abs/2512.14361", "authors": ["Nicholas Tagliapietra", "Katharina Ensinger", "Christoph Zimmer", "Osman Mian"], "title": "Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis", "categories": ["cs.LG", "cs.AI", "math.DS"], "comment": "Accepted as Oral at AAAI 2026 Conference", "summary": "Real world systems evolve in continuous-time according to their underlying causal relationships, yet their dynamics are often unknown. Existing approaches to learning such dynamics typically either discretize time -- leading to poor performance on irregularly sampled data -- or ignore the underlying causality. We propose CaDyT, a novel method for causal discovery on dynamical systems addressing both these challenges. In contrast to state-of-the-art causal discovery methods that model the problem using discrete-time Dynamic Bayesian networks, our formulation is grounded in Difference-based causal models, which allow milder assumptions for modeling the continuous nature of the system. CaDyT leverages exact Gaussian Process inference for modeling the continuous-time dynamics which is more aligned with the underlying dynamical process. We propose a practical instantiation that identifies the causal structure via a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle. Our experiments show that CaDyT outperforms state-of-the-art methods on both regularly and irregularly-sampled data, discovering causal networks closer to the true underlying dynamics.", "AI": {"tldr": "The paper proposes CaDyT, a continuous-time method for causal discovery in dynamical systems, outperforming existing approaches in both regular and irregularly-sampled data.", "motivation": "Existing methods for learning dynamics often discretize time or neglect underlying causality, leading to challenges such as poor performance on irregularly-sampled data.", "method": "CaDyT models continuous-time dynamics using Difference-based causal models and Gaussian Process inference, employing a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle.", "result": "CaDyT demonstrates superior performance over state-of-the-art methods in discovering causal networks closer to the true underlying dynamics, especially for irregularly-sampled data.", "conclusion": "CaDyT advances causal discovery by addressing shortcomings in traditional discrete-time models and effectively handling continuous-time dynamics."}}
{"id": "2512.14236", "pdf": "https://arxiv.org/pdf/2512.14236", "abs": "https://arxiv.org/abs/2512.14236", "authors": ["Nando Metzger", "Prune Truong", "Goutam Bhat", "Konrad Schindler", "Federico Tombari"], "title": "Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding", "categories": ["cs.CV"], "comment": "Project page: elastic3d.github.io", "summary": "The growing demand for immersive 3D content calls for automated monocular-to-stereo video conversion. We present Elastic3D, a controllable, direct end-to-end method for upgrading a conventional video to a binocular one. Our approach, based on (conditional) latent diffusion, avoids artifacts due to explicit depth estimation and warping. The key to its high-quality stereo video output is a novel, guided VAE decoder that ensures sharp and epipolar-consistent stereo video output. Moreover, our method gives the user control over the strength of the stereo effect (more precisely, the disparity range) at inference time, via an intuitive, scalar tuning knob. Experiments on three different datasets of real-world stereo videos show that our method outperforms both traditional warping-based and recent warping-free baselines and sets a new standard for reliable, controllable stereo video conversion. Please check the project page for the video samples https://elastic3d.github.io.", "AI": {"tldr": "Elastic3D introduces a novel, controllable method for converting monocular video into stereo video using latent diffusion and a guided VAE decoder.", "motivation": "To meet the demand for high-quality 3D content, there is a growing need for automated methods to convert conventional 2D videos into immersive stereo format without common artifacts.", "method": "Elastic3D employs a latent diffusion framework and introduces a guided VAE decoder to directly generate epipolar-consistent stereo videos. It also provides a user-adjustable tuning knob to control the disparity range and stereo effect intensity.", "result": "The method outperforms warping-based and warping-free baselines on three real-world stereo video datasets, producing high-quality and controllable results.", "conclusion": "Elastic3D establishes a new standard for monocular-to-stereo video conversion, offering superior quality and customizable stereo effects for enhanced user experience."}}
{"id": "2512.14388", "pdf": "https://arxiv.org/pdf/2512.14388", "abs": "https://arxiv.org/abs/2512.14388", "authors": ["Baobao Song", "Shiva Raj Pokhrel", "Athanasios V. Vasilakos", "Tianqing Zhu", "Gang Li"], "title": "Black-Box Auditing of Quantum Model: Lifted Differential Privacy with Quantum Canaries", "categories": ["cs.LG"], "comment": null, "summary": "Quantum machine learning (QML) promises significant computational advantages, yet models trained on sensitive data risk memorizing individual records, creating serious privacy vulnerabilities. While Quantum Differential Privacy (QDP) mechanisms provide theoretical worst-case guarantees, they critically lack empirical verification tools for deployed models. We introduce the first black-box privacy auditing framework for QML based on Lifted Quantum Differential Privacy, leveraging quantum canaries (strategically offset-encoded quantum states) to detect memorization and precisely quantify privacy leakage during training. Our framework establishes a rigorous mathematical connection between canary offset and trace distance bounds, deriving empirical lower bounds on privacy budget consumption that bridge the critical gap between theoretical guarantees and practical privacy verification. Comprehensive evaluations across both simulated and physical quantum hardware demonstrate our framework's effectiveness in measuring actual privacy loss in QML models, enabling robust privacy verification in QML systems.", "AI": {"tldr": "The paper introduces a black-box privacy auditing framework for Quantum Machine Learning (QML) that uses quantum canaries to empirically measure privacy leakage and address privacy risks.", "motivation": "Quantum Machine Learning (QML) has computational promise but poses privacy risks due to the potential memorization of sensitive data. Existing theoretical Quantum Differential Privacy (QDP) approaches lack practical tools for verifying privacy in deployed QML systems.", "method": "The authors propose a black-box privacy auditing framework based on Lifted Quantum Differential Privacy. It employs quantum canaries (offset-encoded quantum states) to detect memorization and quantify privacy leakage, deriving empirical lower bounds on privacy budget consumption.", "result": "Extensive evaluations using both simulated and real quantum hardware validate the framework's ability to measure actual privacy losses in QML models, providing robust privacy verification.", "conclusion": "The framework bridges the gap between theoretical privacy guarantees and practical verification in QML, addressing privacy concerns and enabling more secure application of QML systems."}}
{"id": "2512.14257", "pdf": "https://arxiv.org/pdf/2512.14257", "abs": "https://arxiv.org/abs/2512.14257", "authors": ["Wentao Wan", "Kaiyu Wu", "Qingyang Ma", "Nan Kang", "Yunjie Chen", "Liang Lin", "Keze Wang"], "title": "Enhancing Visual Programming for Visual Reasoning via Probabilistic Graphs", "categories": ["cs.CV"], "comment": "13 Pages, 12 figures", "summary": "Recently, Visual Programming (VP) based on large language models (LLMs) has rapidly developed and demonstrated significant potential in complex Visual Reasoning (VR) tasks. Previous works to enhance VP have primarily focused on improving the quality of LLM-generated visual programs. However, they have neglected to optimize the VP-invoked pre-trained models, which serve as modules for the visual sub-tasks decomposed from the targeted tasks by VP. The difficulty is that there are only final labels of targeted VR tasks rather than labels of sub-tasks. Besides, the non-differentiable nature of VP impedes the direct use of efficient gradient-based optimization methods to leverage final labels for end-to-end learning of the entire VP framework. To overcome these issues, we propose EVPG, a method to Enhance Visual Programming for visual reasoning via Probabilistic Graphs. Specifically, we creatively build a directed probabilistic graph according to the variable dependency relationships during the VP executing process, which reconstructs the non-differentiable VP executing process into a differentiable exact probability inference process on this directed probabilistic graph. As a result, this enables the VP framework to utilize the final labels for efficient, gradient-based optimization in end-to-end supervised learning on targeted VR tasks. Extensive and comprehensive experiments demonstrate the effectiveness and advantages of our EVPG, showing significant performance improvements for VP on three classical complex VR tasks: GQA, NLVRv2, and Open Images.", "AI": {"tldr": "The paper introduces EVPG, a method to enhance Visual Programming (VP) for visual reasoning by converting non-differentiable processes into differentiable probabilistic graphs, enabling end-to-end learning.", "motivation": "Existing Visual Programming frameworks do not optimize the pre-trained models handling visual sub-tasks, due to non-differentiable properties and lack of sub-task labels.", "method": "The authors develop a directed probabilistic graph that reconstructs the VP process into a differentiable probability inference process, enabling gradient-based end-to-end learning for visual reasoning.", "result": "Their method, EVPG, achieves significant performance improvements on tasks like GQA, NLVRv2, and Open Images, surpassing classical VP approaches.", "conclusion": "EVPG successfully optimizes the VP framework by utilizing probabilistic graphs for gradient-based learning, setting a new standard in visual reasoning tasks."}}
{"id": "2512.14266", "pdf": "https://arxiv.org/pdf/2512.14266", "abs": "https://arxiv.org/abs/2512.14266", "authors": ["Shreedhar Govil", "Didier Stricker", "Jason Rambach"], "title": "DriverGaze360: OmniDirectional Driver Attention with Object-Level Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Predicting driver attention is a critical problem for developing explainable autonomous driving systems and understanding driver behavior in mixed human-autonomous vehicle traffic scenarios. Although significant progress has been made through large-scale driver attention datasets and deep learning architectures, existing works are constrained by narrow frontal field-of-view and limited driving diversity. Consequently, they fail to capture the full spatial context of driving environments, especially during lane changes, turns, and interactions involving peripheral objects such as pedestrians or cyclists. In this paper, we introduce DriverGaze360, a large-scale 360$^\\circ$ field of view driver attention dataset, containing $\\sim$1 million gaze-labeled frames collected from 19 human drivers, enabling comprehensive omnidirectional modeling of driver gaze behavior. Moreover, our panoramic attention prediction approach, DriverGaze360-Net, jointly learns attention maps and attended objects by employing an auxiliary semantic segmentation head. This improves spatial awareness and attention prediction across wide panoramic inputs. Extensive experiments demonstrate that DriverGaze360-Net achieves state-of-the-art attention prediction performance on multiple metrics on panoramic driving images. Dataset and method available at https://av.dfki.de/drivergaze360.", "AI": {"tldr": "The paper introduces DriverGaze360, a 360\u00b0 driver attention dataset, and a new method (DriverGaze360-Net) for better panoramic attention prediction through semantic segmentation.", "motivation": "The motivation is to address the lack of comprehensive spatial context in current driver attention models, particularly during complex scenarios like lane changes and interactions with peripheral objects.", "method": "The authors created DriverGaze360, a 360\u00b0 attention dataset, and developed DriverGaze360-Net which uses an auxiliary semantic segmentation head for panoramic attention maps and attended object predictions.", "result": "DriverGaze360-Net outperforms existing methods in attention prediction metrics on panoramic driving images.", "conclusion": "The dataset and method provide a significant advancement in modeling omnidirectional driver gaze behavior, improving spatial awareness in autonomous driving applications."}}
{"id": "2512.14397", "pdf": "https://arxiv.org/pdf/2512.14397", "abs": "https://arxiv.org/abs/2512.14397", "authors": ["Yunjia Yang", "Weishao Tang", "Mengxin Liu", "Nils Thuerey", "Yufei Zhang", "Haixin Chen"], "title": "SuperWing: a comprehensive transonic wing dataset for data-driven aerodynamic design", "categories": ["cs.LG", "physics.flu-dyn"], "comment": null, "summary": "Machine-learning surrogate models have shown promise in accelerating aerodynamic design, yet progress toward generalizable predictors for three-dimensional wings has been limited by the scarcity and restricted diversity of existing datasets. Here, we present SuperWing, a comprehensive open dataset of transonic swept-wing aerodynamics comprising 4,239 parameterized wing geometries and 28,856 Reynolds-averaged Navier-Stokes flow field solutions. The wing shapes in the dataset are generated using a simplified yet expressive geometry parameterization that incorporates spanwise variations in airfoil shape, twist, and dihedral, allowing for an enhanced diversity without relying on perturbations of a baseline wing. All shapes are simulated under a broad range of Mach numbers and angles of attack covering the typical flight envelope. To demonstrate the dataset's utility, we benchmark two state-of-the-art Transformers that accurately predict surface flow and achieve a 2.5 drag-count error on held-out samples. Models pretrained on SuperWing further exhibit strong zero-shot generalization to complex benchmark wings such as DLR-F6 and NASA CRM, underscoring the dataset's diversity and potential for practical usage.", "AI": {"tldr": "This paper introduces SuperWing, a dataset of transonic swept-wing aerodynamics containing 4,239 wing geometries and associated flow fields to address challenges in aerodynamic machine-learning models.", "motivation": "The paper aims to address the limitations of existing aerodynamic datasets, such as scarcity and lack of diversity, which hinder the development of generalizable machine-learning predictors for 3D wing designs.", "method": "Authors created a comprehensive dataset of 4,239 wing geometries and 28,856 flow field solutions using parameterized geometries encompassing spanwise variations. Simulations covered diverse Mach numbers and angles of attack within a typical flight envelope.", "result": "The dataset enabled benchmarking of Transformers predicting surface flow with low error rates. Models pretrained on this dataset demonstrated strong zero-shot generalization on benchmark wings like DLR-F6 and NASA CRM.", "conclusion": "The study highlights the usefulness and diversity of SuperWing for both academic and practical applications, showing its potential to improve machine-learning approaches in aerodynamic design."}}
{"id": "2512.14273", "pdf": "https://arxiv.org/pdf/2512.14273", "abs": "https://arxiv.org/abs/2512.14273", "authors": ["Xiaoqian Shen", "Min-Hung Chen", "Yu-Chiang Frank Wang", "Mohamed Elhoseiny", "Ryo Hachiuma"], "title": "Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in", "categories": ["cs.CV"], "comment": "Project page: https://xiaoqian-shen.github.io/Zoom-Zero/", "summary": "Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\\% on NExT-GQA and 4.6\\% on ReXTime, while also enhancing average answer accuracy by 2.4\\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\\% on long-video benchmarks.", "AI": {"tldr": "Zoom-Zero is a framework improving grounded-video question answering (GVQA) by addressing temporal grounding challenges using a coarse-to-fine approach.", "motivation": "Existing large video-language models (LVLMs) lack strong temporal grounding abilities, which causes mislocalization and hallucination in grounded video question answering tasks.", "method": "Zoom-Zero localizes video segments relevant to a question and zooms into the most pertinent frames for verification. It introduces a zoom-in accuracy reward to validate temporal predictions and employs token-selective credit assignment to tackle multi-faceted reward issues.", "result": "Zoom-Zero improves temporal grounding by 5.2% on NExT-GQA and 4.6% on ReXTime, enhances answer accuracy by 2.4%, and provides a 6.4% boost in long-video understanding benchmarks.", "conclusion": "The proposed approach significantly enhances GVQA accuracy, temporal grounding, and long-video analysis by refining temporal localization and providing fine-grained verification."}}
{"id": "2512.13765", "pdf": "https://arxiv.org/pdf/2512.13765", "abs": "https://arxiv.org/abs/2512.13765", "authors": ["Shaheim Ogbomo-Harmitt", "Cesare Magnetti", "Chiara Spota", "Jakub Grzelak", "Oleg Aslanidi"], "title": "Towards Deep Learning Surrogate for the Forward Problem in Electrocardiology: A Scalable Alternative to Physics-Based Models", "categories": ["eess.IV", "cs.AI", "cs.LG"], "comment": "Accepted to CinC conference 2025", "summary": "The forward problem in electrocardiology, computing body surface potentials from cardiac electrical activity, is traditionally solved using physics-based models such as the bidomain or monodomain equations. While accurate, these approaches are computationally expensive, limiting their use in real-time and large-scale clinical applications. We propose a proof-of-concept deep learning (DL) framework as an efficient surrogate for forward solvers. The model adopts a time-dependent, attention-based sequence-to-sequence architecture to predict electrocardiogram (ECG) signals from cardiac voltage propagation maps. A hybrid loss combining Huber loss with a spectral entropy term was introduced to preserve both temporal and frequency-domain fidelity. Using 2D tissue simulations incorporating healthy, fibrotic, and gap junction-remodelled conditions, the model achieved high accuracy (mean $R^2 = 0.99 \\pm 0.01$). Ablation studies confirmed the contributions of convolutional encoders, time-aware attention, and spectral entropy loss. These findings highlight DL as a scalable, cost-effective alternative to physics-based solvers, with potential for clinical and digital twin applications.", "AI": {"tldr": "The paper introduces a deep learning framework to efficiently solve the forward problem in electrocardiology, replacing computationally expensive physics-based models.", "motivation": "Physics-based models used for solving electrocardiology problems are accurate but too slow for real-time and large-scale applications, necessitating an alternative method.", "method": "A time-dependent, attention-based sequence-to-sequence deep learning model, using hybrid loss (Huber loss + spectral entropy term), predicts ECG signals from cardiac voltage maps.", "result": "The model showed high accuracy with a mean $R^2 = 0.99 \\pm 0.01$ in simulations of various cardiac conditions, validated by ablation studies.", "conclusion": "The proposed deep learning framework is a scalable and efficient alternative to traditional physics-based solvers, with potential applications in clinical settings and digital twins."}}
{"id": "2512.14400", "pdf": "https://arxiv.org/pdf/2512.14400", "abs": "https://arxiv.org/abs/2512.14400", "authors": ["Fangzhou Lin", "Guoshun He", "Zhenyu Guo", "Zhe Huang", "Jinsong Tao"], "title": "GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion", "categories": ["cs.LG"], "comment": null, "summary": "Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.", "AI": {"tldr": "The paper introduces GRAFT, a forecasting model integrating grid-aware data with textual interventions, and demonstrates its effectiveness with comprehensive evaluations in power grid load forecasting.", "motivation": "To improve accuracy and interpretability in power grid load forecasting by incorporating external textual data sources and aligning them with load data.", "method": "The GRAFT model employs text-guided fusion with cross-attention mechanisms, aligning daily news, social media, and policy texts with half-hourly load data, and provides an external-memory interface for real-world extensions.", "result": "GRAFT outperformed existing models in forecasting across multiple time scales, regions, and external information sources, and was robust under event-driven conditions.", "conclusion": "GRAFT enhances load forecasting accuracy and interpretability, releasing tools and benchmarks to standardize and further research in this domain."}}
{"id": "2512.14274", "pdf": "https://arxiv.org/pdf/2512.14274", "abs": "https://arxiv.org/abs/2512.14274", "authors": ["Yu Chen", "Hongwei Lin"], "title": "TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning", "categories": ["cs.CV", "cs.LG", "math.AT"], "comment": null, "summary": "Persistence diagrams (PDs) provide a powerful tool for understanding the topology of the underlying shape of a point cloud. However, identifying which points in PDs encode genuine signals remains challenging. This challenge directly hinders the practical adoption of topological data analysis in many applications, where automated and reliable interpretation of persistence diagrams is essential for downstream decision-making. In this paper, we study automatic significance detection for one-dimensional persistence diagrams. Specifically, we propose Topology Understanding Net (TUN), a multi-modal network that combines enhanced PD descriptors with self-attention, a PointNet-style point cloud encoder, learned fusion, and per-point classification, alongside stable preprocessing and imbalance-aware training. It provides an automated and effective solution for identifying significant points in PDs, which are critical for downstream applications. Experiments show that TUN outperforms classic methods in detecting significant points in PDs, illustrating its effectiveness in real-world applications.", "AI": {"tldr": "The paper introduces TUN, a multi-modal network designed for automatic significance detection in one-dimensional persistence diagrams, outperforming traditional methods.", "motivation": "The need for automated and reliable interpretation of persistence diagrams is critical for applying topological data analysis practically, especially in downstream decision-making.", "method": "The paper proposes TUN, a multi-modal network integrating enhanced PD descriptors, self-attention, a PointNet-style encoder, learned fusion, and per-point classification, supported by stable preprocessing and imbalance-aware training.", "result": "TUN improves the detection of significant points in persistence diagrams and outperforms classical methods in various real-world scenarios.", "conclusion": "The proposed TUN model offers an effective, automated approach to identifying significant points in persistence diagrams, facilitating better usage of topological data analysis in applications."}}
{"id": "2512.13768", "pdf": "https://arxiv.org/pdf/2512.13768", "abs": "https://arxiv.org/abs/2512.13768", "authors": ["Yao Xie", "Walter Cullen"], "title": "Beyond Procedural Compliance: Human Oversight as a Dimension of Well-being Efficacy in AI Governance", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Major AI ethics guidelines and laws, including the EU AI Act, call for effective human oversight, but do not define it as a distinct and developable capacity. This paper introduces human oversight as a well-being capacity, situated within the emerging Well-being Efficacy framework. The concept integrates AI literacy, ethical discernment, and awareness of human needs, acknowledging that some needs may be conflicting or harmful. Because people inevitably project desires, fears, and interests into AI systems, oversight requires the competence to examine and, when necessary, restrain problematic demands.\n  The authors argue that the sustainable and cost-effective development of this capacity depends on its integration into education at every level, from professional training to lifelong learning. The frame of human oversight as a well-being capacity provides a practical path from high-level regulatory goals to the continuous cultivation of human agency and responsibility essential for safe and ethical AI. The paper establishes a theoretical foundation for future research on the pedagogical implementation and empirical validation of well-being effectiveness in multiple contexts.", "AI": {"tldr": "The paper defines human oversight in AI as a well-being capacity integrating AI literacy, ethical discernment, and self-awareness, calling for its development through education to promote ethical AI practices.", "motivation": "To address the gap in AI ethics guidelines that demand human oversight but fail to define it as a tangible and developable skill.", "method": "Proposes human oversight as a well-being capacity within the Well-being Efficacy framework, linking it with education at various levels to create a practical and sustainable path for development.", "result": "Human oversight is framed as a concept integrating AI literacy, ethics, and awareness, contributing to safe and ethical AI practices.", "conclusion": "Establishes a foundation for further research on embedding this capacity into educational practices to cultivate human agency and responsibility in AI oversight."}}
{"id": "2512.14418", "pdf": "https://arxiv.org/pdf/2512.14418", "abs": "https://arxiv.org/abs/2512.14418", "authors": ["Dejun Hu", "Zhiming Li", "Jia-Rui Shen", "Jia-Ning Tu", "Zi-Hao Ye", "Junliang Zhang"], "title": "Dual-Axis RCCL: Representation-Complete Convergent Learning for Organic Chemical Space", "categories": ["cs.LG"], "comment": "33 pages, 10 figures", "summary": "Machine learning is profoundly reshaping molecular and materials modeling; however, given the vast scale of chemical space (10^30-10^60), it remains an open scientific question whether models can achieve convergent learning across this space. We introduce a Dual-Axis Representation-Complete Convergent Learning (RCCL) strategy, enabled by a molecular representation that integrates graph convolutional network (GCN) encoding of local valence environments, grounded in modern valence bond theory, together with no-bridge graph (NBG) encoding of ring/cage topologies, providing a quantitative measure of chemical-space coverage. This framework formalizes representation completeness, establishing a principled basis for constructing datasets that support convergent learning for large models. Guided by this RCCL framework, we develop the FD25 dataset, systematically covering 13,302 local valence units and 165,726 ring/cage topologies, achieving near-complete combinatorial coverage of organic molecules with H/C/N/O/F elements. Graph neural networks trained on FD25 exhibit representation-complete convergent learning and strong out-of-distribution generalization, with an overall prediction error of approximately 1.0 kcal/mol MAE across external benchmarks. Our results establish a quantitative link between molecular representation, structural completeness, and model generalization, providing a foundation for interpretable, transferable, and data-efficient molecular intelligence.", "AI": {"tldr": "This paper introduces a new framework (RCCL) with the FD25 dataset to achieve comprehensive molecular modeling using machine learning approaches, ensuring broad chemical space representation and improved prediction accuracy.", "motivation": "Molecular and materials modeling via machine learning faces challenges due to the vast chemical space; this paper seeks to devise a framework that ensures scalable and convergent learning across this realm.", "method": "The authors propose the RCCL, integrating graph convolutional network (GCN) encoding local valence environments with no-bridge graph (NBG) encoding ring/cage topologies, enabling representation completeness. Alongside this, the FD25 dataset systematically covers diverse organic structures.", "result": "Models trained on the FD25 dataset achieved representation-complete convergent learning and strong out-of-distribution generalization with low prediction errors (~1.0 kcal/mol MAE) across benchmarks.", "conclusion": "The proposed RCCL strategy and FD25 dataset establish a foundational approach for efficient, interpretable, and generalizable molecular intelligence leveraging extensive chemical space coverage."}}
{"id": "2512.14284", "pdf": "https://arxiv.org/pdf/2512.14284", "abs": "https://arxiv.org/abs/2512.14284", "authors": ["Zhibing Li", "Mengchen Zhang", "Tong Wu", "Jing Tan", "Jiaqi Wang", "Dahua Lin"], "title": "SS4D: Native 4D Generative Model via Structured Spacetime Latents", "categories": ["cs.CV"], "comment": "ToG(Siggraph Asia 2025)", "summary": "We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion", "AI": {"tldr": "SS4D is a 4D generative model that synthesizes dynamic 3D objects directly from monocular video with high fidelity and coherence.", "motivation": "Address the limitations of optimizing over 3D or video models and overcome scarcity of 4D training data.", "method": "Uses pre-trained 3D models for spatial consistency, temporal layers for time coherence, and compressed latents for efficient training on long videos.", "result": "Achieves structural consistency, smooth temporal changes, and robustness against occlusions in dynamic object generation.", "conclusion": "SS4D improves dynamic 3D object synthesis, offering an innovative approach by directly utilizing structured spacetime latents."}}
{"id": "2512.14444", "pdf": "https://arxiv.org/pdf/2512.14444", "abs": "https://arxiv.org/abs/2512.14444", "authors": ["Akira Takeshima", "Kenta Shiraishi", "Atsushi Okazaki", "Tadashi Tsuyuki", "Shunji Kotsuki"], "title": "Bridging Artificial Intelligence and Data Assimilation: The Data-driven Ensemble Forecasting System ClimaX-LETKF", "categories": ["cs.LG"], "comment": "14 pages and 5 figures for the main text and 13 pages and 7 figures as supplementary materials", "summary": "While machine learning-based weather prediction (MLWP) has achieved significant advancements, research on assimilating real observations or ensemble forecasts within MLWP models remains limited. We introduce ClimaX-LETKF, the first purely data-driven ML-based ensemble weather forecasting system. It operates stably over multiple years, independently of numerical weather prediction (NWP) models, by assimilating the NCEP ADP Global Upper Air and Surface Weather Observations. The system demonstrates greater stability and accuracy with relaxation to prior perturbation (RTPP) than with relaxation to prior spread (RTPS), while NWP models tend to be more stable with RTPS. RTPP replaces an analysis perturbation with a weighted blend of analysis and background perturbations, whereas RTPS simply rescales the analysis perturbation. Our experiments reveal that MLWP models are less capable of restoring the atmospheric field to its attractor than NWP models. This work provides valuable insights for enhancing MLWP ensemble forecasting systems and represents a substantial step toward their practical applications.", "AI": {"tldr": "ClimaX-LETKF is introduced as a novel ML-based ensemble weather forecasting system using real observations, demonstrating improved stability and accuracy.", "motivation": "Current research on assimilating real observations or ensemble forecasts within ML-based weather prediction systems is limited.", "method": "ClimaX-LETKF incorporates NCEP ADP observations into ML-based forecasting systems, comparing RTPP and RTPS approaches for perturbation assimilation.", "result": "The system operates stably over years, showing greater stability and accuracy with RTPP, but ML systems face challenges in restoring atmospheric fields as effectively as NWP models.", "conclusion": "MLWP ensemble forecasting systems can greatly benefit from insights of this work, moving closer to practical applications."}}
{"id": "2512.14309", "pdf": "https://arxiv.org/pdf/2512.14309", "abs": "https://arxiv.org/abs/2512.14309", "authors": ["Abdullah Al Mamun", "Miaohua Zhang", "David Ahmedt-Aristizabal", "Zeeshan Hayder", "Mohammad Awrangjeb"], "title": "PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised Learning (SSL) has become a powerful paradigm for representation learning without manual annotations. However, most existing frameworks focus on global alignment and struggle to capture the hierarchical, multi-scale lesion patterns characteristic of plant disease imagery. To address this gap, we propose PSMamba, a progressive self-supervised framework that integrates the efficient sequence modelling of Vision Mamba (VM) with a dual-student hierarchical distillation strategy. Unlike conventional single teacher-student designs, PSMamba employs a shared global teacher and two specialised students: one processes mid-scale views to capture lesion distributions and vein structures, while the other focuses on local views to capture fine-grained cues such as texture irregularities and early-stage lesions. This multi-granular supervision facilitates the joint learning of contextual and detailed representations, with consistency losses ensuring coherent cross-scale alignment. Experiments on three benchmark datasets show that PSMamba consistently outperforms state-of-the-art SSL methods, delivering superior accuracy and robustness in both domain-shifted and fine-grained scenarios.", "AI": {"tldr": "PSMamba is a progressive self-supervised learning framework designed to capture hierarchical and multi-scale features in plant disease imagery that outperforms state-of-the-art SSL methods.", "motivation": "Current self-supervised learning methods struggle to capture the hierarchical and multi-scale patterns that are unique to plant disease imagery.", "method": "PSMamba combines Vision Mamba for efficient sequence modeling with a dual-student hierarchical distillation strategy, employing two specialized students to process mid-scale and local views under a shared global teacher.", "result": "PSMamba demonstrates superior accuracy and robustness over state-of-the-art SSL methods across three benchmark datasets, excelling in both domain-shifted and fine-grained tasks.", "conclusion": "PSMamba successfully addresses the limitations of conventional SSL frameworks by enabling joint learning of contextual and detailed representations, proving effective for analyzing complex plant disease imagery."}}
{"id": "2512.14461", "pdf": "https://arxiv.org/pdf/2512.14461", "abs": "https://arxiv.org/abs/2512.14461", "authors": ["Niklas Grieger", "Jannik Raskob", "Siamak Mehrkanoon", "Stephan Bialonski"], "title": "AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts", "categories": ["cs.LG", "eess.SP", "q-bio.QM"], "comment": "18 pages, 6 figures, 2 tables", "summary": "Sleep is essential for good health throughout our lives, yet studying its dynamics requires manual sleep staging, a labor-intensive step in sleep research and clinical care. Across centers, polysomnography (PSG) recordings are traditionally scored in 30-s epochs for pragmatic, not physiological, reasons and can vary considerably in electrode count, montage, and subject characteristics. These constraints present challenges in conducting harmonized multi-center sleep studies and discovering novel, robust biomarkers on shorter timescales. Here, we present AnySleep, a deep neural network model that uses any electroencephalography (EEG) or electrooculography (EOG) data to score sleep at adjustable temporal resolutions. We trained and validated the model on over 19,000 overnight recordings from 21 datasets collected across multiple clinics, spanning nearly 200,000 hours of EEG and EOG data, to promote robust generalization across sites. The model attains state-of-the-art performance and surpasses or equals established baselines at 30-s epochs. Performance improves as more channels are provided, yet remains strong when EOG is absent or when only EOG or single EEG derivations (frontal, central, or occipital) are available. On sub-30-s timescales, the model captures short wake intrusions consistent with arousals and improves prediction of physiological characteristics (age, sex) and pathophysiological conditions (sleep apnea), relative to standard 30-s scoring. We make the model publicly available to facilitate large-scale studies with heterogeneous electrode setups and to accelerate the discovery of novel biomarkers in sleep.", "AI": {"tldr": "The paper introduces AnySleep, a deep neural network for flexible and robust sleep scoring using a variety of EEG/EOG data. It aims to simplify multi-center studies and enable biomarker discovery through sub-30-s scoring.", "motivation": "To address the inefficiencies of traditional manual and standardized 30-s sleep staging, which limits the scalability and discovery potential of sleep research due to its resource intensity and lack of physiological adaptability.", "method": "A deep learning model, AnySleep, was developed and trained on over 19,000 polysomnography recordings spanning nearly 200,000 hours from diverse datasets. The model works across multiple electrode configurations and temporal resolutions.", "result": "AnySleep achieved state-of-the-art performance in sleep scoring, outperforming or matching traditional baselines at 30-s epochs. It also provided strong predictions even with limited input channels and enabled improved detection of arousals, physiological traits, and pathophysiological conditions on sub-30-s scales.", "conclusion": "AnySleep demonstrates a robust framework for flexible, high-performance sleep scoring, promoting multi-center collaborations and novel biomarker discovery. The model is publicly available to advance research in sleep and health."}}
{"id": "2512.14312", "pdf": "https://arxiv.org/pdf/2512.14312", "abs": "https://arxiv.org/abs/2512.14312", "authors": ["Akila Premarathna", "Kanishka Hewageegana", "Garcia Andarcia Mariangel"], "title": "From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater Treatment Plants Using Satellite Imagery in MENA Region", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 9 figures", "summary": "In regions of the Middle East and North Africa (MENA), there is a high demand for wastewater treatment plants (WWTPs), crucial for sustainable water management. Precise identification of WWTPs from satellite images enables environmental monitoring. Traditional methods like YOLOv8 segmentation require extensive manual labeling. But studies indicate that vision-language models (VLMs) are an efficient alternative to achieving equivalent or superior results through inherent reasoning and annotation. This study presents a structured methodology for VLM comparison, divided into zero-shot and few-shot streams specifically to identify WWTPs. The YOLOv8 was trained on a governmental dataset of 83,566 high-resolution satellite images from Egypt, Saudi Arabia, and UAE: ~85% WWTPs (positives), 15% non-WWTPs (negatives). Evaluated VLMs include LLaMA 3.2 Vision, Qwen 2.5 VL, DeepSeek-VL2, Gemma 3, Gemini, and Pixtral 12B (Mistral), used to identify WWTP components such as circular/rectangular tanks, aeration basins and distinguish confounders via expert prompts producing JSON outputs with confidence and descriptions. The dataset comprises 1,207 validated WWTP locations (198 UAE, 354 KSA, 655 Egypt) and equal non-WWTP sites from field/AI data, as 600mx600m Geo-TIFF images (Zoom 18, EPSG:4326). Zero-shot evaluations on WWTP images showed several VLMs out-performing YOLOv8's true positive rate, with Gemma-3 highest. Results confirm that VLMs, particularly with zero-shot, can replace YOLOv8 for efficient, annotation-free WWTP classification, enabling scalable remote sensing.", "AI": {"tldr": "This study evaluates vision-language models (VLMs) for identifying wastewater treatment plants (WWTPs) in satellite images, outperforming traditional YOLOv8 segmentation methods.", "motivation": "The paper aims to address the inefficiencies of traditional methods like YOLOv8, which require extensive manual labeling, by exploring VLMs as a more efficient alternative for classifying WWTPs from satellite images to support sustainable water management in the MENA region.", "method": "The study compares zero-shot and few-shot VLMs like LLaMA 3.2 Vision, Qwen 2.5 VL, among others, to YOLOv8 through a structured methodology using a dataset of 83,566 satellite images from regions like Egypt, Saudi Arabia, and UAE.", "result": "Several VLMs, particularly Gemma-3, demonstrated superior performance to YOLOv8 in zero-shot evaluations, showing higher true positive rates without manual annotation.", "conclusion": "VLMs are proven capable of replacing traditional methods such as YOLOv8 for scalable, annotation-free WWTP identification in satellite imagery, paving the way for efficient environmental monitoring in the MENA region."}}
{"id": "2512.14471", "pdf": "https://arxiv.org/pdf/2512.14471", "abs": "https://arxiv.org/abs/2512.14471", "authors": ["Additi Pandey", "Liang Wei", "Hessam Babaee", "George Em Karniadakis"], "title": "Kinetic-Mamba: Mamba-Assisted Predictions of Stiff Chemical Kinetics", "categories": ["cs.LG"], "comment": null, "summary": "Accurate chemical kinetics modeling is essential for combustion simulations, as it governs the evolution of complex reaction pathways and thermochemical states. In this work, we introduce Kinetic-Mamba, a Mamba-based neural operator framework that integrates the expressive power of neural operators with the efficient temporal modeling capabilities of Mamba architectures. The framework comprises three complementary models: (i) a standalone Mamba model that predicts the time evolution of thermochemical state variables from given initial conditions; (ii) a constrained Mamba model that enforces mass conservation while learning the state dynamics; and (iii) a regime-informed architecture employing two standalone Mamba models to capture dynamics across temperature-dependent regimes. We additionally develop a latent Kinetic-Mamba variant that evolves dynamics in a reduced latent space and reconstructs the full state on the physical manifold. We evaluate the accuracy and robustness of Kinetic-Mamba using both time-decomposition and recursive-prediction strategies. We further assess the extrapolation capabilities of the model on varied out-of-distribution datasets. Computational experiments on Syngas and GRI-Mech 3.0 reaction mechanisms demonstrate that our framework achieves high fidelity in predicting complex kinetic behavior using only the initial conditions of the state variables.", "AI": {"tldr": "The paper introduces Kinetic-Mamba, a neural operator framework for accurate modeling of chemical kinetics in combustion simulations. It employs multiple architectures to account for different dynamics and demonstrates high prediction fidelity.", "motivation": "Combustion simulations require accurate chemical kinetics modeling to capture complex reaction pathways and thermochemical state evolution.", "method": "The Kinetic-Mamba framework uses Mamba-based neural operator architectures, integrating standalone models, constrained models enforcing mass conservation, regime-informed networks, and latent space approaches.", "result": "Experiments using Syngas and GRI-Mech 3.0 show Kinetic-Mamba's ability to predict chemical dynamics accurately from initial conditions, even on varied out-of-distribution datasets.", "conclusion": "Kinetic-Mamba effectively combines neural operators and efficient architectures to advance high-fidelity chemical kinetics modeling for combustion simulations."}}
{"id": "2512.14320", "pdf": "https://arxiv.org/pdf/2512.14320", "abs": "https://arxiv.org/abs/2512.14320", "authors": ["Shuai Dong", "Jie Zhang", "Guoying Zhao", "Shiguang Shan", "Xilin Chen"], "title": "Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "comment": "11 pages, 4 figures", "summary": "Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.", "AI": {"tldr": "This paper introduces a method for immunizing images against unauthorized edits using imperceptible perturbations and proposes a new metric to evaluate its effectiveness.", "motivation": "The paper aims to address concerns about the misuse of diffusion models for unauthorized image editing by providing a robust immunization method and metric.", "method": "The authors propose Synergistic Intermediate Feature Manipulation (SIFM), which perturbs intermediate diffusion features with dual objectives: disrupting semantic alignment and inducing perceptual degradations.", "result": "Experiments demonstrate that SIFM achieves state-of-the-art performance in protecting images from malicious edits.", "conclusion": "SIFM and the Immunization Success Rate (ISR) provide effective tools for safeguarding visual content and rigorously quantifying immunization success."}}
{"id": "2512.14333", "pdf": "https://arxiv.org/pdf/2512.14333", "abs": "https://arxiv.org/abs/2512.14333", "authors": ["Jie Zhang", "Shuai Dong", "Shiguang Shan", "Xilin Chen"], "title": "Dual Attention Guided Defense Against Malicious Edits", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "comment": "11 pages, 7 figures", "summary": "Recent progress in text-to-image diffusion models has transformed image editing via text prompts, yet this also introduces significant ethical challenges from potential misuse in creating deceptive or harmful content. While current defenses seek to mitigate this risk by embedding imperceptible perturbations, their effectiveness is limited against malicious tampering. To address this issue, we propose a Dual Attention-Guided Noise Perturbation (DANP) immunization method that adds imperceptible perturbations to disrupt the model's semantic understanding and generation process. DANP functions over multiple timesteps to manipulate both cross-attention maps and the noise prediction process, using a dynamic threshold to generate masks that identify text-relevant and irrelevant regions. It then reduces attention in relevant areas while increasing it in irrelevant ones, thereby misguides the edit towards incorrect regions and preserves the intended targets. Additionally, our method maximizes the discrepancy between the injected noise and the model's predicted noise to further interfere with the generation. By targeting both attention and noise prediction mechanisms, DANP exhibits impressive immunity against malicious edits, and extensive experiments confirm that our method achieves state-of-the-art performance.", "AI": {"tldr": "The paper proposes a method, DANP, to combat misuse of text-to-image models by adding perturbations that disrupt semantic understanding and generation, achieving effective defense against malicious editing.", "motivation": "The motivation is addressing ethical concerns and challenges with the misuse of text-to-image models in generating deceptive or harmful content.", "method": "The method involves the Dual Attention-Guided Noise Perturbation (DANP), which manipulates cross-attention maps and noise prediction using dynamic thresholds. It reduces attention on target areas and increases on non-relevant areas to disrupt malicious edits.", "result": "The DANP technique demonstrated superior immunity against malicious image edits compared to existing approaches, achieving state-of-the-art performance.", "conclusion": "By precisely targeting attention mechanisms and noise prediction processes, DANP effectively defends against unethical use of image generation models, providing a robust solution."}}
{"id": "2512.14537", "pdf": "https://arxiv.org/pdf/2512.14537", "abs": "https://arxiv.org/abs/2512.14537", "authors": ["Miriam Guti\u00e9rrez Fern\u00e1ndez", "Karen L\u00f3pez-Linares", "Carlos Fambuena Santos", "Mar\u00eda S. Guillem", "Andreu M. Climent", "\u00d3scar Barquero P\u00e9rez"], "title": "Synthetic Electrogram Generation with Variational Autoencoders for ECGI", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Atrial fibrillation (AF) is the most prevalent sustained cardiac arrhythmia, and its clinical assessment requires accurate characterization of atrial electrical activity. Noninvasive electrocardiographic imaging (ECGI) combined with deep learning (DL) approaches for estimating intracardiac electrograms (EGMs) from body surface potentials (BSPMs) has shown promise, but progress is hindered by the limited availability of paired BSPM-EGM datasets. To address this limitation, we investigate variational autoencoders (VAEs) for the generation of synthetic multichannel atrial EGMs. Two models are proposed: a sinus rhythm-specific VAE (VAE-S) and a class-conditioned VAE (VAE-C) trained on both sinus rhythm and AF signals. Generated EGMs are evaluated using morphological, spectral, and distributional similarity metrics. VAE-S achieves higher fidelity with respect to in silico EGMs, while VAE-C enables rhythm-specific generation at the expense of reduced sinus reconstruction quality. As a proof of concept, the generated EGMs are used for data augmentation in a downstream noninvasive EGM reconstruction task, where moderate augmentation improves estimation performance. These results demonstrate the potential of VAE-based generative modeling to alleviate data scarcity and enhance deep learning-based ECGI pipelines.", "AI": {"tldr": "This paper explores using variational autoencoders (VAEs) to generate synthetic multichannel atrial EGMs for data augmentation in noninvasive electrocardiographic imaging (ECGI).", "motivation": "The scarcity of paired BSPM-EGM datasets hinders progress in deep learning-based ECGI for accurate atrial activity characterization, particularly in atrial fibrillation (AF).", "method": "Two VAE models were developed: a sinus rhythm-specific VAE (VAE-S) and a class-conditioned VAE (VAE-C), evaluated using similarity metrics and tested for data augmentation in EGM reconstruction tasks.", "result": "VAE-S generated more accurate EGM reconstructions, while VAE-C allowed rhythm-specific generation. Synthetic EGMs used as data augmentation moderately improved EGM reconstruction performance.", "conclusion": "VAE-based generative modeling shows promise in mitigating data scarcity and enhancing deep learning-powered ECGI workflows for analyzing atrial electrical activity."}}
{"id": "2512.14336", "pdf": "https://arxiv.org/pdf/2512.14336", "abs": "https://arxiv.org/abs/2512.14336", "authors": ["Jooyeol Yun", "Jaegul Choo"], "title": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure", "categories": ["cs.CV"], "comment": "yeolj00.github.io/personal-projects/vector-prism", "summary": "Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.", "AI": {"tldr": "The paper introduces a framework to enhance the semantic structure of SVGs, enabling vision-language models (VLMs) to create coherent animations from vector graphics.", "motivation": "Automating SVG animation is difficult for vision-language models because they struggle to understand and group fragmented low-level elements of graphics.", "method": "The proposed framework statistically aggregates weak semantic predictions to reorganize SVGs into coherent semantic groups, supporting VLMs in creating better animations.", "result": "The framework achieves significant improvements in animation coherence over existing methods, demonstrating its effectiveness in robust SVG animations.", "conclusion": "Semantic recovery is essential for robust SVG animations and facilitates better interactions between vision-language models and vector graphics."}}
{"id": "2512.14559", "pdf": "https://arxiv.org/pdf/2512.14559", "abs": "https://arxiv.org/abs/2512.14559", "authors": ["Emmanuel C. Chukwu", "Rianne M. Schouten", "Monique Tabak", "Mykola Pechenizkiy"], "title": "Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions", "categories": ["cs.LG"], "comment": null, "summary": "Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.", "AI": {"tldr": "Current counterfactual techniques for time series classification are insufficient for clinical recommendation settings due to the lack of temporal coherence and user-centered design.", "motivation": "The paper aims to address shortcomings in counterfactual explanations for time series classification in clinical contexts, emphasizing the need for causally plausible and temporally coherent interventions.", "method": "A robustness analysis of state-of-the-art counterfactual methods for time series classification is conducted to reveal their sensitivity to stochastic noise.", "result": "The analysis shows that existing counterfactual methods are vulnerable to measurement variations, making them unreliable for real-world clinical applications.", "conclusion": "The study advocates for the development of methods and evaluation frameworks that prioritize actionable and goal-directed interventions, aligning with clinical reasoning and real-world feasibility."}}
{"id": "2512.14341", "pdf": "https://arxiv.org/pdf/2512.14341", "abs": "https://arxiv.org/abs/2512.14341", "authors": ["Jie Zhang", "Shuai Dong", "Shiguang Shan", "Xilin Chen"], "title": "Towards Transferable Defense Against Malicious Image Edits", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "comment": "14 pages, 5 figures", "summary": "Recent approaches employing imperceptible perturbations in input images have demonstrated promising potential to counter malicious manipulations in diffusion-based image editing systems. However, existing methods suffer from limited transferability in cross-model evaluations. To address this, we propose Transferable Defense Against Malicious Image Edits (TDAE), a novel bimodal framework that enhances image immunity against malicious edits through coordinated image-text optimization. Specifically, at the visual defense level, we introduce FlatGrad Defense Mechanism (FDM), which incorporates gradient regularization into the adversarial objective. By explicitly steering the perturbations toward flat minima, FDM amplifies immune robustness against unseen editing models. For textual enhancement protection, we propose an adversarial optimization paradigm named Dynamic Prompt Defense (DPD), which periodically refines text embeddings to align the editing outcomes of immunized images with those of the original images, then updates the images under optimized embeddings. Through iterative adversarial updates to diverse embeddings, DPD enforces the generation of immunized images that seek a broader set of immunity-enhancing features, thereby achieving cross-model transferability. Extensive experimental results demonstrate that our TDAE achieves state-of-the-art performance in mitigating malicious edits under both intra- and cross-model evaluations.", "AI": {"tldr": "The paper introduces TDAE, a bimodal framework for defending against malicious image edits, achieving strong transferability across models.", "motivation": "To address the lack of cross-model transferability in current defense methods against malicious image edits.", "method": "The TDAE framework uses FlatGrad Defense Mechanism (FDM) for robust visual defense and Dynamic Prompt Defense (DPD) for textual optimization, iteratively enhancing image immunity.", "result": "Experiments show TDAE outperforms existing methods, offering superior defense in intra- and cross-model contexts.", "conclusion": "TDAE demonstrates effective defense against malicious image edits with state-of-the-art cross-model transferability."}}
{"id": "2512.14563", "pdf": "https://arxiv.org/pdf/2512.14563", "abs": "https://arxiv.org/abs/2512.14563", "authors": ["Tejaswani Dash", "Gautam Datla", "Anudeep Vurity", "Tazeem Ahmad", "Mohd Adnan", "Saima Rafi", "Saisha Patro", "Saina Patro"], "title": "Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in IEEE Bigdata 2025- Learning Representations with Limited Supervision", "summary": "Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.", "AI": {"tldr": "The paper introduces a deep learning model, Residual GRU with Multi-Head Self-Attention, for predicting cardiovascular disease using clinical tabular data. The model outperforms traditional and deep learning methods in accuracy and efficiency.", "motivation": "To develop a reliable and efficient predictive model for cardiovascular disease, addressing challenges of reproducibility and generalization across noisy clinical data.", "method": "The proposed model integrates residual bidirectional GRUs, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token. Evaluation was performed using 5-fold stratified cross-validation on the UCI Heart Disease dataset.", "result": "Outperformed classical and modern deep learning methods, achieving an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904. Ablation studies confirmed individual contributions of model components.", "conclusion": "The lightweight hybrid recurrent and attention-based architecture is well-suited for clinical risk prediction, balancing high accuracy and computational efficiency, supporting its use in resource-constrained environments."}}
{"id": "2512.14352", "pdf": "https://arxiv.org/pdf/2512.14352", "abs": "https://arxiv.org/abs/2512.14352", "authors": ["Kaizhe Zhang", "Yijie Zhou", "Weizhan Zhang", "Caixia Yan", "Haipeng Du", "yugui xie", "Yu-Hui Wen", "Yong-Jin Liu"], "title": "HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis", "categories": ["cs.CV", "cs.CG"], "comment": "11 pages, 9 figures", "summary": "Dynamic novel view synthesis (NVS) is essential for creating immersive experiences. Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods. However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices. To obtain a more efficient model with fewer redundant parameters, in this paper, we propose Hybrid Gaussian Splatting (HGS), a compact and efficient framework explicitly designed to disentangle static and dynamic regions of a scene within a unified representation. The core innovation of HGS lies in our Static-Dynamic Decomposition (SDD) strategy, which leverages Radial Basis Function (RBF) modeling for Gaussian primitives. Specifically, for dynamic regions, we employ time-dependent RBFs to effectively capture temporal variations and handle abrupt scene changes, while for static regions, we reduce redundancy by sharing temporally invariant parameters. Additionally, we introduce a two-stage training strategy tailored for explicit models to enhance temporal coherence at static-dynamic boundaries. Experimental results demonstrate that our method reduces model size by up to 98% and achieves real-time rendering at up to 125 FPS at 4K resolution on a single RTX 3090 GPU. It further sustains 160 FPS at 1352 * 1014 on an RTX 3050 and has been integrated into the VR system. Moreover, HGS achieves comparable rendering quality to state-of-the-art methods while providing significantly improved visual fidelity for high-frequency details and abrupt scene changes.", "AI": {"tldr": "HGS proposes a compact framework for dynamic view synthesis, reducing model size by up to 98% with real-time rendering capabilities while maintaining quality.", "motivation": "Current dynamic NVS methods have excessive model complexity and redundancy, making them inefficient for resource-constrained devices.", "method": "HGS leverages Static-Dynamic Decomposition using RBF modeling to disentangle static and dynamic regions, sharing parameters for static regions and using time-dependent RBFs for dynamic ones.", "result": "HGS achieves up to 98% model size reduction, real-time rendering at up to 125 FPS (4K resolution), and improved visual fidelity for scene changes.", "conclusion": "HGS provides an efficient solution for real-time dynamic NVS with reduced redundancy, maintaining quality and practical performance for VR systems."}}
{"id": "2512.14596", "pdf": "https://arxiv.org/pdf/2512.14596", "abs": "https://arxiv.org/abs/2512.14596", "authors": ["Youngkyu Lee", "Francesc Levrero Florencio", "Jay Pathak", "George Em Karniadakis"], "title": "Hybrid Iterative Solvers with Geometry-Aware Neural Preconditioners for Parametric PDEs", "categories": ["cs.LG", "math.NA"], "comment": "19 pages, 10 figures, 3 tables", "summary": "The convergence behavior of classical iterative solvers for parametric partial differential equations (PDEs) is often highly sensitive to the domain and specific discretization of PDEs. Previously, we introduced hybrid solvers by combining the classical solvers with neural operators for a specific geometry 1, but they tend to under-perform in geometries not encountered during training. To address this challenge, we introduce Geo-DeepONet, a geometry-aware deep operator network that incorporates domain information extracted from finite element discretizations. Geo-DeepONet enables accurate operator learning across arbitrary unstructured meshes without requiring retraining. Building on this, we develop a class of geometry-aware hybrid preconditioned iterative solvers by coupling Geo-DeepONet with traditional methods such as relaxation schemes and Krylov subspace algorithms. Through numerical experiments on parametric PDEs posed over diverse unstructured domains, we demonstrate the enhanced robustness and efficiency of the proposed hybrid solvers for multiple real-world applications.", "AI": {"tldr": "This paper introduces Geo-DeepONet, a geometry-aware neural operator for learning across unstructured meshes, and couples it with classical methods to enhance iterative solvers for parametric PDEs.", "motivation": "Classical iterative solvers for parametric PDEs struggle with convergence across different geometries and discretizations. Existing hybrid solvers underperform on unseen geometries, creating a need for geometry-aware methodologies.", "method": "The authors propose Geo-DeepONet, which incorporates geometry information from finite element discretizations for operator learning. They develop hybrid preconditioned iterative solvers by integrating Geo-DeepONet with classical methods.", "result": "Geo-DeepONet, combined with traditional solvers, is proven to enhance robustness and efficiency in solving parametric PDEs over unstructured domains via numerical experiments.", "conclusion": "Geo-DeepONet provides a significant advancement for solving real-world parametric PDEs by addressing geometry-variance challenges and improving existing solver efficiency without retraining."}}
{"id": "2512.14354", "pdf": "https://arxiv.org/pdf/2512.14354", "abs": "https://arxiv.org/abs/2512.14354", "authors": ["Kanglong Fan", "Yunqiao Yang", "Chen Ma"], "title": "Enhancing Interpretability for Vision Models via Shapley Value Optimization", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to AAAI2026", "summary": "Deep neural networks have demonstrated remarkable performance across various domains, yet their decision-making processes remain opaque. Although many explanation methods are dedicated to bringing the obscurity of DNNs to light, they exhibit significant limitations: post-hoc explanation methods often struggle to faithfully reflect model behaviors, while self-explaining neural networks sacrifice performance and compatibility due to their specialized architectural designs. To address these challenges, we propose a novel self-explaining framework that integrates Shapley value estimation as an auxiliary task during training, which achieves two key advancements: 1) a fair allocation of the model prediction scores to image patches, ensuring explanations inherently align with the model's decision logic, and 2) enhanced interpretability with minor structural modifications, preserving model performance and compatibility. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art interpretability.", "AI": {"tldr": "The paper introduces a self-explaining neural network framework integrating Shapley value estimation for improved interpretability without sacrificing performance.", "motivation": "Deep neural networks (DNNs) excel in performance across domains but lack transparency in their decision-making processes.", "method": "The proposed framework integrates Shapley value estimation as an auxiliary task during training to allocate prediction scores fairly to image patches for better interpretability.", "result": "The method achieves state-of-the-art interpretability, aligning explanations with model logic while maintaining performance and structural compatibility.", "conclusion": "A novel approach addressing the drawbacks of existing explanation methods through fair attribution and interpretability enhancement without performance trade-offs."}}
{"id": "2512.13907", "pdf": "https://arxiv.org/pdf/2512.13907", "abs": "https://arxiv.org/abs/2512.13907", "authors": ["Alessio Buscemi", "Tom Deckenbrunnen", "Fahria Kabir", "Nishat Mowla", "Kateryna Mishchenko"], "title": "Assessing High-Risk Systems: An EU AI Act Verification Framework", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "A central challenge in implementing the AI Act and other AI-relevant regulations in the EU is the lack of a systematic approach to verify their legal mandates. Recent surveys show that this regulatory ambiguity is perceived as a significant burden, leading to inconsistent readiness across Member States. This paper proposes a comprehensive framework designed to help close this gap by organising compliance verification along two fundamental dimensions: the type of method (controls vs. testing) and the target of assessment (data, model, processes, and final product). Additionally, our framework maps core legal requirements to concrete verification activities, serving as a vital bridge between policymakers and practitioners, and aligning legal text with technical standards and best practices. The proposed approach aims to reduce interpretive uncertainty, promote consistency in assessment practices, and support the alignment of regulatory, ethical, and technical perspectives across the AI lifecycle.", "AI": {"tldr": "This paper suggests a structured framework for AI compliance verification in the EU to address regulatory ambiguities and interpretive challenges.", "motivation": "Regulatory ambiguities in the implementation of the AI Act create burdens and inconsistent readiness among Member States; a systematic method to ensure compliance is needed.", "method": "A framework is proposed for compliance verification, organized by type of method (controls vs. testing) and target of assessment (data, model, processes, final product). Core legal requirements are mapped to specific verification activities.", "result": "The framework reduces interpretive uncertainty, improves assessment consistency, and bridges gaps between policymakers, practitioners, and technical standards.", "conclusion": "The approach fosters alignment across regulatory, ethical, and technical perspectives throughout the AI lifecycle, enhancing implementation consistency of AI-relevant regulations."}}
{"id": "2512.14615", "pdf": "https://arxiv.org/pdf/2512.14615", "abs": "https://arxiv.org/abs/2512.14615", "authors": ["Omid Khormali"], "title": "Hierarchical Persistence Velocity for Network Anomaly Detection: Theory and Applications to Cryptocurrency Markets", "categories": ["cs.LG"], "comment": null, "summary": "We introduce the Overlap-Weighted Hierarchical Normalized Persistence Velocity (OW-HNPV), a novel topological data analysis method for detecting anomalies in time-varying networks. Unlike existing methods that measure cumulative topological presence, we introduce the first velocity-based perspective on persistence diagrams, measuring the rate at which features appear and disappear, automatically downweighting noise through overlap-based weighting. We also prove that OW-HNPV is mathematically stable. It behaves in a controlled, predictable way, even when comparing persistence diagrams from networks with different feature types. Applied to Ethereum transaction networks (May 2017-May 2018), OW-HNPV demonstrates superior performance for cryptocurrency anomaly detection, achieving up to 10.4% AUC gain over baseline models for 7-day price movement predictions. Compared with established methods, including Vector of Averaged Bettis (VAB), persistence landscapes, and persistence images, velocity-based summaries excel at medium- to long-range forecasting (4-7 days), with OW-HNPV providing the most consistent and stable performance across prediction horizons. Our results show that modeling topological velocity is crucial for detecting structural anomalies in dynamic networks.", "AI": {"tldr": "The paper introduces OW-HNPV, a topological anomaly detection method for dynamic networks using a velocity-based approach to persistence diagrams.", "motivation": "Addressing the need for more effective anomaly detection in dynamic networks, especially considering the limitations of current cumulative topological summaries.", "method": "Develop OW-HNPV, a metric that calculates the velocity of topological features' appearance and disappearance, incorporating overlap-based weighting to reduce noise.", "result": "OW-HNPV outperformed baseline methods in anomaly detection on Ethereum transaction networks, with up to 10.4% AUC gain for 7-day cryptocurrency price predictions.", "conclusion": "Incorporating topological velocity enhances anomaly detection and forecasting in time-varying networks, with OW-HNPV offering stability and consistent predictive performance."}}
{"id": "2512.14360", "pdf": "https://arxiv.org/pdf/2512.14360", "abs": "https://arxiv.org/abs/2512.14360", "authors": ["Ankita Raj", "Kaashika Prajaapat", "Tapan Kumar Gandhi", "Chetan Arora"], "title": "Mimicking Human Visual Development for Learning Robust Image Representations", "categories": ["cs.CV"], "comment": "Accepted to ICVGIP 2025", "summary": "The human visual system is remarkably adept at adapting to changes in the input distribution; a capability modern convolutional neural networks (CNNs) still struggle to match. Drawing inspiration from the developmental trajectory of human vision, we propose a progressive blurring curriculum to improve the generalization and robustness of CNNs. Human infants are born with poor visual acuity, gradually refining their ability to perceive fine details. Mimicking this process, we begin training CNNs on highly blurred images during the initial epochs and progressively reduce the blur as training advances. This approach encourages the network to prioritize global structures over high-frequency artifacts, improving robustness against distribution shifts and noisy inputs. Challenging prior claims that blurring in the initial training epochs imposes a stimulus deficit and irreversibly harms model performance, we reveal that early-stage blurring enhances generalization with minimal impact on in-domain accuracy. Our experiments demonstrate that the proposed curriculum reduces mean corruption error (mCE) by up to 8.30% on CIFAR-10-C and 4.43% on ImageNet-100-C datasets, compared to standard training without blurring. Unlike static blur-based augmentation, which applies blurred images randomly throughout training, our method follows a structured progression, yielding consistent gains across various datasets. Furthermore, our approach complements other augmentation techniques, such as CutMix and MixUp, and enhances both natural and adversarial robustness against common attack methods. Code is available at https://github.com/rajankita/Visual_Acuity_Curriculum.", "AI": {"tldr": "This paper proposes a progressive blurring curriculum during CNN training, inspired by human visual development, to improve generalization and robustness to noisy data and distribution shifts.", "motivation": "Modern CNNs struggle to adapt to changes in input distribution compared to the human visual system's adaptive capabilities. The authors aim to enhance CNN performance by emulating human visual development processes.", "method": "The authors introduce a progressive blurring curriculum where CNNs are trained on heavily blurred images initially, with blur progressively reduced during training. This prioritizes global structures over high-frequency artifacts and encourages generalization.", "result": "The proposed method decreases mean corruption error (mCE) by up to 8.30% in CIFAR-10-C and 4.43% in ImageNet-100-C datasets and improves robustness against adversarial attacks and distribution shifts, outperforming static blur-based augmentation.", "conclusion": "Progressive blurring during early CNN training enhances generalization without a significant impact on in-domain accuracy. It complements existing augmentation techniques and consistently benefits robustness across datasets."}}
{"id": "2512.14617", "pdf": "https://arxiv.org/pdf/2512.14617", "abs": "https://arxiv.org/abs/2512.14617", "authors": ["Alessandro Trapasso", "Luca Iocchi", "Fabio Patrizi"], "title": "Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes", "categories": ["cs.LG", "cs.AI"], "comment": "19 pages, 32 figures, includes appendix", "summary": "Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.", "AI": {"tldr": "This paper introduces QR-MAX and its extension Bucket-QR-MAX, model-based RL algorithms designed for non-Markovian decision problems. They guarantee PAC convergence to near-optimal policies with improved sample efficiency.", "motivation": "The paper addresses the limitations of Markovian RL in solving decision problems dependent on entire system history and seeks to improve sample efficiency and optimality guarantees in NMRDPs.", "method": "QR-MAX uses a model-based RL approach that separates learning transitions and rewards via reward machines for discrete NMRDPs. Bucket-QR-MAX extends this to continuous state spaces using a SimHash-based discretizer.", "result": "The proposed methods demonstrate superior sample efficiency and robustness in finding optimal policies compared to state-of-the-art model-based RL approaches in various testing environments.", "conclusion": "QR-MAX and Bucket-QR-MAX present significant advances in solving complex NMRDP tasks, ensuring efficient and reliable RL solutions for temporal-dependency problems."}}
{"id": "2512.14364", "pdf": "https://arxiv.org/pdf/2512.14364", "abs": "https://arxiv.org/abs/2512.14364", "authors": ["Sebastian Koch", "Johanna Wald", "Hide Matsuki", "Pedro Hermosilla", "Timo Ropinski", "Federico Tombari"], "title": "Unified Semantic Transformer for 3D Scene Understanding", "categories": ["cs.CV"], "comment": "Project page: https://unite-page.github.io/", "summary": "Holistic 3D scene understanding involves capturing and parsing unstructured 3D environments. Due to the inherent complexity of the real world, existing models have predominantly been developed and limited to be task-specific. We introduce UNITE, a Unified Semantic Transformer for 3D scene understanding, a novel feed-forward neural network that unifies a diverse set of 3D semantic tasks within a single model. Our model operates on unseen scenes in a fully end-to-end manner and only takes a few seconds to infer the full 3D semantic geometry. Our approach is capable of directly predicting multiple semantic attributes, including 3D scene segmentation, instance embeddings, open-vocabulary features, as well as affordance and articulations, solely from RGB images. The method is trained using a combination of 2D distillation, heavily relying on self-supervision and leverages novel multi-view losses designed to ensure 3D view consistency. We demonstrate that UNITE achieves state-of-the-art performance on several different semantic tasks and even outperforms task-specific models, in many cases, surpassing methods that operate on ground truth 3D geometry. See the project website at unite-page.github.io", "AI": {"tldr": "UNITE is a unified model for 3D scene understanding capable of performing various semantic tasks from RGB images efficiently.", "motivation": "Existing models for 3D scene understanding are task-specific and limited, requiring a versatile approach for diverse 3D semantic tasks.", "method": "UNITE utilizes RGB images and is trained via 2D distillation with self-supervision and multi-view losses, ensuring 3D view consistency.", "result": "UNITE achieves state-of-the-art results on multiple semantic tasks and surpasses task-specific models, even those with ground truth 3D geometry.", "conclusion": "UNITE offers a unified, efficient, and accurate solution for 3D scene understanding across different tasks and sets a new benchmark in the field."}}
{"id": "2512.13912", "pdf": "https://arxiv.org/pdf/2512.13912", "abs": "https://arxiv.org/abs/2512.13912", "authors": ["Julian Jeggle", "Raphael Wittkowski"], "title": "Intelligent matter consisting of active particles", "categories": ["cond-mat.soft", "cond-mat.dis-nn", "cs.AI", "cs.LG", "physics.app-ph"], "comment": "14 pages, 5 figures", "summary": "In this book chapter, we review how systems of simple motile agents can be used as a pathway to intelligent systems. It is a well known result from nature that large groups of entities following simple rules, such as swarms of animals, can give rise to much more complex collective behavior in a display of emergence. This begs the question whether we can emulate this behavior in synthetic matter and drive it to a point where the collective behavior reaches the complexity level of intelligent systems. Here, we will use a formalized notion of \"intelligent matter\" and compare it to recent results in the field of active matter. First, we will explore the approach of emergent computing in which specialized active matter systems are designed to directly solve a given task through emergent behavior. This we will then contrast with the approach of physical reservoir computing powered by the dynamics of active particle systems. In this context, we will also describe a novel reservoir computing scheme for active particles driven ultrasonically or via light refraction.", "AI": {"tldr": "The chapter reviews how simple motile agents and active matter systems can emulate intelligent behavior through processes like emergence and physical reservoir computing.", "motivation": "To explore the possibility of emulating complex behaviors of swarms in synthetic matter, aiming to develop intelligent systems.", "method": "The authors use the concept of 'intelligent matter' and analyze approaches like emergent computing and physical reservoir computing for active matter systems.", "result": "The chapter compares emergent behavior-based task-solving systems with reservoir computing schemes utilizing active particle dynamics, including novel ultrasonic and light refraction mechanisms.", "conclusion": "Systems of active matter can display complex behaviors mimicking intelligence, with promising approaches like reservoir computing advancing this field."}}
{"id": "2512.14619", "pdf": "https://arxiv.org/pdf/2512.14619", "abs": "https://arxiv.org/abs/2512.14619", "authors": ["Chaohao Yuan", "Zhenjie Song", "Ercan Engin Kuruoglu", "Kangfei Zhao", "Yang Liu", "Deli Zhao", "Hong Cheng", "Yu Rong"], "title": "ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning", "categories": ["cs.LG"], "comment": "Accepted by WSDM 2026", "summary": "Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.", "AI": {"tldr": "The study introduces ParaFormer, a graph transformer tackling the over-smoothing issue in graph learning. It achieves superior performance on graph tasks using a PageRank-enhanced attention module.", "motivation": "The paper addresses the over-smoothing problem in Graph Transformers, where node representations become indistinguishable under global attention due to low-pass filtering.", "method": "The authors propose ParaFormer, which integrates a PageRank-enhanced attention mechanism, acting as an adaptive-pass filter to counterbalance over-smoothing effects.", "result": "Experiments show ParaFormer achieves consistent improvements in node and graph classification tasks across 11 diverse datasets.", "conclusion": "ParaFormer is effective in mitigating over-smoothing in graph transformers and boosts performance in graph-related tasks."}}
{"id": "2512.14366", "pdf": "https://arxiv.org/pdf/2512.14366", "abs": "https://arxiv.org/abs/2512.14366", "authors": ["Julian McGinnis", "Florian A. H\u00f6lzl", "Suprosanna Shit", "Florentin Bieder", "Paul Friedrich", "Mark M\u00fchlau", "Bj\u00f6rn Menze", "Daniel Rueckert", "Benedikt Wiestler"], "title": "Optimizing Rank for High-Fidelity Implicit Neural Representations", "categories": ["cs.CV"], "comment": null, "summary": "Implicit Neural Representations (INRs) based on vanilla Multi-Layer Perceptrons (MLPs) are widely believed to be incapable of representing high-frequency content. This has directed research efforts towards architectural interventions, such as coordinate embeddings or specialized activation functions, to represent high-frequency signals. In this paper, we challenge the notion that the low-frequency bias of vanilla MLPs is an intrinsic, architectural limitation to learn high-frequency content, but instead a symptom of stable rank degradation during training. We empirically demonstrate that regulating the network's rank during training substantially improves the fidelity of the learned signal, rendering even simple MLP architectures expressive. Extensive experiments show that using optimizers like Muon, with high-rank, near-orthogonal updates, consistently enhances INR architectures even beyond simple ReLU MLPs. These substantial improvements hold across a diverse range of domains, including natural and medical images, and novel view synthesis, with up to 9 dB PSNR improvements over the previous state-of-the-art. Our project page, which includes code and experimental results, is available at: (https://muon-inrs.github.io).", "AI": {"tldr": "This paper challenges the belief that vanilla MLPs cannot represent high-frequency signals, showing that training issues like rank degradation limit their performance, not architectural shortcomings.", "motivation": "To overturn the assumption that vanilla MLPs have intrinsic limitations in handling high-frequency signals and instead address the underlying training issues like rank degradation.", "method": "The paper regulates the rank of the network during training using a tailored optimizer (called Muon) that introduces high-rank, near-orthogonal updates.", "result": "Improved fidelity of learned signals with vanilla MLPs and up to 9 dB PSNR improvements across applications, including natural images, medical images, and novel view synthesis.", "conclusion": "Vanilla MLP-based INRs can effectively learn high-frequency content by addressing rank degradation during training, making architectural interventions unnecessary."}}
{"id": "2512.14658", "pdf": "https://arxiv.org/pdf/2512.14658", "abs": "https://arxiv.org/abs/2512.14658", "authors": ["Alban Puech", "Matteo Mazzonelli", "Celia Cintas", "Tamara R. Govindasamy", "Mangaliso Mngomezulu", "Jonas Weiss", "Matteo Ba\u00f9", "Anna Varbella", "Fran\u00e7ois Mirall\u00e8s", "Kibaek Kim", "Le Xie", "Hendrik F. Hamann", "Etienne Vos", "Thomas Brunschwiler"], "title": "gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation", "categories": ["cs.LG", "cs.AI", "eess.SY", "math.OC"], "comment": "Main equal contributors: Alban Puech, Matteo Mazzonelli. Other equal contributors: Celia Cintas, Tamara R. Govindasamy, Mangaliso Mngomezulu, Jonas Weiss", "summary": "We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$\u0394$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.", "AI": {"tldr": "The paper introduces 'gridfm-datakit-v1,' a Python library for creating diverse Power Flow and Optimal Power Flow datasets to enhance Machine Learning solver training.", "motivation": "Existing datasets face challenges with realistic scenario diversity, generalization to cases violating operating limits, and varied generator cost functions. Addressing these gaps supports better ML solver training and reliability.", "method": "The library combines real-world load scaling with localized noise for dataset diversity, generates Power Flow samples that include operating violations, and varies generator cost functions. It is scalable to grids with up to 10,000 buses.", "result": "gridfm-datakit overcomes limitations of existing datasets, supports realistic and diverse scenarios, includes operating limits violations, and efficiently scales to large grid sizes. Comparisons to other libraries show its improvements.", "conclusion": "gridfm-datakit enriches ML training datasets for Power Flow and Optimal Power Flow analysis, promoting solver generalization and performance. It is open-source and easily accessible for broader application."}}
{"id": "2512.14373", "pdf": "https://arxiv.org/pdf/2512.14373", "abs": "https://arxiv.org/abs/2512.14373", "authors": ["Martin R\u00f6hn", "Nora Gourmelon", "Vincent Christlein"], "title": "EcoScapes: LLM-Powered Advice for Crafting Sustainable Cities", "categories": ["cs.CV"], "comment": null, "summary": "Climate adaptation is vital for the sustainability and sometimes the mere survival of our urban areas. However, small cities often struggle with limited personnel resources and integrating vast amounts of data from multiple sources for a comprehensive analysis. To overcome these challenges, this paper proposes a multi-layered system combining specialized LLMs, satellite imagery analysis and a knowledge base to aid in developing effective climate adaptation strategies. The corresponding code can be found at https://github.com/Photon-GitHub/EcoScapes.", "AI": {"tldr": "This paper introduces a system combining LLMs, satellite imagery, and knowledge bases to help small cities with climate adaptation strategies.", "motivation": "Small cities encounter resource limitations and challenges in analyzing diverse data for climate adaptation.", "method": "A multi-layered system with specialized LLMs, satellite imagery analysis, and a knowledge base is developed.", "result": "The proposed system aids small cities in creating effective climate adaptation strategies.", "conclusion": "The approach provides scalable and accessible tools to support urban areas in sustainability efforts."}}
{"id": "2512.14675", "pdf": "https://arxiv.org/pdf/2512.14675", "abs": "https://arxiv.org/abs/2512.14675", "authors": ["Rae Chipera", "Jenny Du", "Irene Tsapara"], "title": "Beyond Lipschitz Continuity and Monotonicity: Fractal and Chaotic Activation Functions in Echo State Networks", "categories": ["cs.LG"], "comment": "50 pages, 21 figures. Extended version with full proofs, parameter sweeps, and appendices", "summary": "Contemporary reservoir computing relies heavily on smooth, globally Lipschitz continuous activation functions, limiting applications in defense, disaster response, and pharmaceutical modeling where robust operation under extreme conditions is critical. We systematically investigate non-smooth activation functions, including chaotic, stochastic, and fractal variants, in echo state networks. Through comprehensive parameter sweeps across 36,610 reservoir configurations, we demonstrate that several non-smooth functions not only maintain the Echo State Property (ESP) but outperform traditional smooth activations in convergence speed and spectral radius tolerance. Notably, the Cantor function (continuous everywhere and flat almost everywhere) maintains ESP-consistent behavior up to spectral radii of rho ~ 10, an order of magnitude beyond typical bounds for smooth functions, while achieving 2.6x faster convergence than tanh and ReLU. We introduce a theoretical framework for quantized activation functions, defining a Degenerate Echo State Property (d-ESP) that captures stability for discrete-output functions and proving that d-ESP implies traditional ESP. We identify a critical crowding ratio Q=N/k (reservoir size / quantization levels) that predicts failure thresholds for discrete activations. Our analysis reveals that preprocessing topology, rather than continuity per se, determines stability: monotone, compressive preprocessing maintains ESP across scales, while dispersive or discontinuous preprocessing triggers sharp failures. While our findings challenge assumptions about activation function design in reservoir computing, the mechanism underlying the exceptional performance of certain fractal functions remains unexplained, suggesting fundamental gaps in our understanding of how geometric properties of activation functions influence reservoir dynamics.", "AI": {"tldr": "This study explores non-smooth activation functions in reservoir computing, discovering certain fractal-based functions outperform traditional smooth activations while maintaining stability.", "motivation": "Current reservoir computing using traditional smooth activation functions struggles in critical, extreme-condition applications like defense and disaster response; this paper seeks to explore alternatives with robust performance.", "method": "The paper investigates over 36,610 configurations of echo state networks with non-smooth functions, introducing new concepts like Degenerate Echo State Property (d-ESP) for discrete-output activations and analyzing critical quantitative thresholds.", "result": "Non-smooth functions, notably fractal ones like the Cantor function, show improved performance (e.g., faster convergence and higher spectral radius tolerance) compared to traditional functions, despite maintaining stability.", "conclusion": "The findings challenge conventional activation design in reservoir computing, suggesting activation geometry plays a major role in dynamics while highlighting open questions about fractal function performance."}}
{"id": "2512.14406", "pdf": "https://arxiv.org/pdf/2512.14406", "abs": "https://arxiv.org/abs/2512.14406", "authors": ["Le Jiang", "Shaotong Zhu", "Yedi Luo", "Shayda Moezzi", "Sarah Ostadabbas"], "title": "Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos", "categories": ["cs.CV"], "comment": null, "summary": "In dynamic Neural Radiance Fields (NeRF) systems, state-of-the-art novel view synthesis methods often fail under significant viewpoint deviations, producing unstable and unrealistic renderings. To address this, we introduce Expanded Dynamic NeRF (ExpanDyNeRF), a monocular NeRF framework that leverages Gaussian splatting priors and a pseudo-ground-truth generation strategy to enable realistic synthesis under large-angle rotations. ExpanDyNeRF optimizes density and color features to improve scene reconstruction from challenging perspectives. We also present the Synthetic Dynamic Multiview (SynDM) dataset, the first synthetic multiview dataset for dynamic scenes with explicit side-view supervision-created using a custom GTA V-based rendering pipeline. Quantitative and qualitative results on SynDM and real-world datasets demonstrate that ExpanDyNeRF significantly outperforms existing dynamic NeRF methods in rendering fidelity under extreme viewpoint shifts. Further details are provided in the supplementary materials.", "AI": {"tldr": "ExpanDyNeRF introduces an approach to improve dynamic Neural Radiance Fields (NeRF) systems for large-angle rotations, leveraging Gaussian splatting priors and synthetic datasets.", "motivation": "State-of-the-art dynamic NeRF methods struggle with unstable renderings under extreme viewpoint deviations, necessitating improved techniques for realistic synthesis.", "method": "The method integrates Gaussian splatting priors, pseudo-ground-truth generation, and introduces the SynDM dataset for dynamic scene training using a GTA V-based rendering pipeline.", "result": "ExpanDyNeRF achieves superior rendering fidelity under significant viewpoint shifts on both synthetic and real-world datasets.", "conclusion": "ExpanDyNeRF successfully enhances novel view synthesis in dynamic NeRF systems, handling challenging perspectives and improving rendering under large-angle rotations."}}
{"id": "2512.14683", "pdf": "https://arxiv.org/pdf/2512.14683", "abs": "https://arxiv.org/abs/2512.14683", "authors": ["Dimitris Bertsimas", "Yu Ma", "Kimberly Villalobos Carballo", "Gagan Singh", "Michal Laskowski", "Jeff Mather", "Dan Kombert", "Howard Haronian"], "title": "Early Warning Index for Patient Deteriorations in Hospitals", "categories": ["cs.LG"], "comment": null, "summary": "Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.", "AI": {"tldr": "The paper introduces the Early Warning Index (EWI), a multimodal machine learning framework for predicting ICU admission, emergency response team dispatch, and mortality using structured and unstructured hospital data, achieving significant predictive accuracy.", "motivation": "Hospitals face challenges in utilizing large and diverse clinical data to predict critical patient events, affecting care quality and operational efficiency.", "method": "The authors developed a machine learning framework that integrates clinician expertise, explainable AI techniques (SHAP), and hospital dashboard implementation for risk assessment and resource allocation.", "result": "Using data from 18,633 patients, the framework achieved a C-statistic of 0.796, successfully stratifying patients into risk tiers and assisting in proactive management.", "conclusion": "EWI enhances patient care by streamlining risk assessments and resource allocation, saving physician time, and preventing downstream complications."}}
{"id": "2512.14420", "pdf": "https://arxiv.org/pdf/2512.14420", "abs": "https://arxiv.org/abs/2512.14420", "authors": ["Nakamasa Inoue", "Kanoko Goto", "Masanari Oi", "Martyna Gruszka", "Mahiro Ukai", "Takumi Hirose", "Yusuke Sekikawa"], "title": "DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning", "categories": ["cs.CV", "cs.AI"], "comment": "Paper accepted to AAAI 2026", "summary": "Large vision-language models (LVLMs) have shown impressive performance across a broad range of multimodal tasks. However, robust image caption evaluation using LVLMs remains challenging, particularly under domain-shift scenarios. To address this issue, we introduce the Distribution-Aware Score Decoder (DISCODE), a novel finetuning-free method that generates robust evaluation scores better aligned with human judgments across diverse domains. The core idea behind DISCODE lies in its test-time adaptive evaluation approach, which introduces the Adaptive Test-Time (ATT) loss, leveraging a Gaussian prior distribution to improve robustness in evaluation score estimation. This loss is efficiently minimized at test time using an analytical solution that we derive. Furthermore, we introduce the Multi-domain Caption Evaluation (MCEval) benchmark, a new image captioning evaluation benchmark covering six distinct domains, designed to assess the robustness of evaluation metrics. In our experiments, we demonstrate that DISCODE achieves state-of-the-art performance as a reference-free evaluation metric across MCEval and four representative existing benchmarks.", "AI": {"tldr": "DISCODE is a finetuning-free method improving image caption evaluation via test-time adaptive evaluation, achieving state-of-the-art results.", "motivation": "To address the challenge of robust image caption evaluation under domain-shift scenarios using LVLMs.", "method": "DISCODE introduces the ATT loss with a Gaussian prior for adaptive test-time evaluation, minimizing it with an analytical solution. A new MCEval benchmark is also proposed.", "result": "DISCODE performs robustly across diverse domains and benchmarks, surpassing existing evaluation metrics.", "conclusion": "DISCODE offers a robust, reference-free evaluation approach, effectively aligned with human judgments, and validated on multiple benchmarks."}}
{"id": "2512.13956", "pdf": "https://arxiv.org/pdf/2512.13956", "abs": "https://arxiv.org/abs/2512.13956", "authors": ["Zishan Bai", "Enze Ge", "Junfeng Hao"], "title": "Multi-Agent Collaborative Framework for Intelligent IT Operations: An AOI System with Context-Aware Compression and Dynamic Task Scheduling", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "The proliferation of cloud-native architectures, characterized by microservices and dynamic orchestration, has rendered modern IT infrastructures exceedingly complex and volatile. This complexity generates overwhelming volumes of operational data, leading to critical bottlenecks in conventional systems: inefficient information processing, poor task coordination, and loss of contextual continuity during fault diagnosis and remediation. To address these challenges, we propose AOI (AI-Oriented Operations), a novel multi-agent collaborative framework that integrates three specialized agents with an LLM-based Context Compressor. Its core innovations include: (1) a dynamic task scheduling strategy that adaptively prioritizes operations based on real-time system states, and (2) a three-layer memory architecture comprising Working, Episodic, and Semantic layers that optimizes context retention and retrieval. Extensive experiments on both synthetic and real-world benchmarks demonstrate that AOI effectively mitigates information overload, achieving a 72.4% context compression ratio while preserving 92.8% of critical information and significantly enhances operational efficiency, attaining a 94.2% task success rate and reducing the Mean Time to Repair (MTTR) by 34.4% compared to the best baseline. This work presents a paradigm shift towards scalable, adaptive, and context-aware autonomous operations, enabling robust management of next-generation IT infrastructures with minimal human intervention.", "AI": {"tldr": "The paper introduces AOI, a multi-agent framework for managing complex IT infrastructures, demonstrating improved efficiency and context management.", "motivation": "To tackle inefficiencies in handling operational data generated by cloud-native architectures, such as inefficient processing and task coordination.", "method": "The paper proposes AOI, which uses a three-agent system with an LLM-based Context Compressor, a dynamic task scheduler, and a memory architecture with Working, Episodic, and Semantic layers.", "result": "Experiments show a 72.4% context compression ratio while retaining 92.8% of critical data, a 94.2% task success rate, and a 34.4% decrease in MTTR compared to baselines.", "conclusion": "The AOI framework significantly enhances scalability, adaptability, and efficiency in IT operations, reducing human involvement while managing complex infrastructures."}}
{"id": "2512.14421", "pdf": "https://arxiv.org/pdf/2512.14421", "abs": "https://arxiv.org/abs/2512.14421", "authors": ["Mischa Dombrowski", "Felix N\u00fctzel", "Bernhard Kainz"], "title": "LCMem: A Universal Model for Robust Image Memorization Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in generative image modeling have achieved visual realism sufficient to deceive human experts, yet their potential for privacy preserving data sharing remains insufficiently understood. A central obstacle is the absence of reliable memorization detection mechanisms, limited quantitative evaluation, and poor generalization of existing privacy auditing methods across domains. To address this, we propose to view memorization detection as a unified problem at the intersection of re-identification and copy detection, whose complementary goals cover both identity consistency and augmentation-robust duplication, and introduce Latent Contrastive Memorization Network (LCMem), a cross-domain model evaluated jointly on both tasks. LCMem achieves this through a two-stage training strategy that first learns identity consistency before incorporating augmentation-robust copy detection. Across six benchmark datasets, LCMem achieves improvements of up to 16 percentage points on re-identification and 30 percentage points on copy detection, enabling substantially more reliable memorization detection at scale. Our results show that existing privacy filters provide limited performance and robustness, highlighting the need for stronger protection mechanisms. We show that LCMem sets a new standard for cross-domain privacy auditing, offering reliable and scalable memorization detection. Code and model is publicly available at https://github.com/MischaD/LCMem.", "AI": {"tldr": "This paper proposes LCMem, a novel model for better memorization detection across domains, addressing limitations in privacy preservation in generative image modeling.", "motivation": "To improve privacy-preserving data sharing by addressing the lack of reliable memorization detection mechanisms, limited evaluation methods, and poor generalization across domains in existing privacy auditing approaches.", "method": "The proposed LCMem uses a two-stage training process focusing first on identity consistency and then on augmentation-robust copy detection, evaluated on re-identification and duplication tasks.", "result": "LCMem demonstrated up to 16% improvement in re-identification and 30% in copy detection across six benchmark datasets, surpassing existing methods in both performance and scalability.", "conclusion": "LCMem significantly enhances cross-domain privacy auditing and sets a new standard for scalable and reliable memorization detection, addressing critical shortcomings in existing systems."}}
{"id": "2512.13998", "pdf": "https://arxiv.org/pdf/2512.13998", "abs": "https://arxiv.org/abs/2512.13998", "authors": ["Qilin Li", "C. L. Philip Chen", "TongZhang"], "title": "Memo2496: Expert-Annotated Dataset and Dual-View Adaptive Framework for Music Emotion Recognition", "categories": ["cs.SD", "cs.AI", "cs.MM"], "comment": null, "summary": "Music Emotion Recogniser (MER) research faces challenges due to limited high-quality annotated datasets and difficulties in addressing cross-track feature drift. This work presents two primary contributions to address these issues. Memo2496, a large-scale dataset, offers 2496 instrumental music tracks with continuous valence arousal labels, annotated by 30 certified music specialists. Annotation quality is ensured through calibration with extreme emotion exemplars and a consistency threshold of 0.25, measured by Euclidean distance in the valence arousal space. Furthermore, the Dual-view Adaptive Music Emotion Recogniser (DAMER) is introduced. DAMER integrates three synergistic modules: Dual Stream Attention Fusion (DSAF) facilitates token-level bidirectional interaction between Mel spectrograms and cochleagrams via cross attention mechanisms; Progressive Confidence Labelling (PCL) generates reliable pseudo labels employing curriculum-based temperature scheduling and consistency quantification using Jensen Shannon divergence; and Style Anchored Memory Learning (SAML) maintains a contrastive memory queue to mitigate cross-track feature drift. Extensive experiments on the Memo2496, 1000songs, and PMEmo datasets demonstrate DAMER's state-of-the-art performance, improving arousal dimension accuracy by 3.43%, 2.25%, and 0.17%, respectively. Ablation studies and visualisation analyses validate each module's contribution. Both the dataset and source code are publicly available.", "AI": {"tldr": "Memo2496 is a new annotated instrumental music dataset for emotion recognition. DAMER uses advanced mechanisms to improve emotions representation in music analyses.", "motivation": "The challenges in music emotion recognition arise mainly from limited annotated datasets and issues in cross-track feature drift analysis.", "method": "The authors introduced Memo2496, a 2496-track music dataset, and DAMER, a system integrating Dual Stream Attention Fusion, Progressive Confidence Labelling, and Style Anchored Memory Learning modules.", "result": "Experiments showcased DAMER's performance improvements on multiple datasets, enhancing accuracy in the arousal dimension by up to 3.43%.", "conclusion": "Memo2496 provides a high-quality music dataset for emotion analysis. DAMER sets new benchmarks in music emotion recognition, with the dataset and code publicly available to the research community."}}
{"id": "2511.15407", "pdf": "https://arxiv.org/pdf/2511.15407", "abs": "https://arxiv.org/abs/2511.15407", "authors": ["Mingyu Zhang", "Lifeng Zhuo", "Tianxi Tan", "Guocan Xie", "Xian Nie", "Yan Li", "Renjie Zhao", "Zizhu He", "Ziyu Wang", "Jiting Cai", "Yong-Lu Li"], "title": "IPR-1: Interactive Physical Reasoner", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "13 pages of main text and 19 pages of appendices. Project page: https://mybearyzhang.github.io/ipr-1", "summary": "Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. To study this, we introduce a Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games that exhibit significant visual domain gaps. Existing approaches, including VLMs and world models, struggle to capture underlying physics and causality since they are not focused on core mechanisms and overfit to visual details. VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on levels from primitive intuition to goal-driven reasoning, and even surpasses GPT-5 overall. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning. Further demos and project details can be found at https://mybearyzhang.github.io/ipr-1.", "AI": {"tldr": "This paper proposes an agent-based system called IPR (Interactive Physical Reasoner) to improve human-like physical reasoning and causality understanding by combining world models with vision-language models (VLMs), tested using a novel Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games.", "motivation": "The research investigates whether agents can achieve human-like reasoning by interacting with environments and internalizing underlying physics and causality through experiential learning.", "method": "The authors introduce the Interactive Physical Reasoner (IPR), which uses world-model rollouts to improve VLMs' policies, and introduce PhysCode, a physics-centered action code that links semantic intent with physical dynamics. The method is pre-trained on 1,000+ games for generalizability.", "result": "IPR demonstrates robust performance across a spectrum of reasoning tasks\u2014from primitive intuition to goal-driven scenarios\u2014sometimes surpassing GPT-5. The model also shows improved performance with more interactions and effective zero-shot transfer to unseen games.", "conclusion": "The results highlight that interaction-centric, physics-based approaches can enable continuous improvement in agents' physical reasoning and causal understanding, positioning these systems closer to human-like reasoning capabilities."}}
{"id": "2512.14423", "pdf": "https://arxiv.org/pdf/2512.14423", "abs": "https://arxiv.org/abs/2512.14423", "authors": ["Zhuo Chen", "Fanyue Wei", "Runze Xu", "Jingjing Li", "Lixin Duan", "Angela Yao", "Wen Li"], "title": "The Devil is in Attention Sharing: Improving Complex Non-rigid Image Editing Faithfulness via Attention Synergy", "categories": ["cs.CV"], "comment": "Project page:https://synps26.github.io/", "summary": "Training-free image editing with large diffusion models has become practical, yet faithfully performing complex non-rigid edits (e.g., pose or shape changes) remains highly challenging. We identify a key underlying cause: attention collapse in existing attention sharing mechanisms, where either positional embeddings or semantic features dominate visual content retrieval, leading to over-editing or under-editing.To address this issue, we introduce SynPS, a method that Synergistically leverages Positional embeddings and Semantic information for faithful non-rigid image editing. We first propose an editing measurement that quantifies the required editing magnitude at each denoising step. Based on this measurement, we design an attention synergy pipeline that dynamically modulates the influence of positional embeddings, enabling SynPS to balance semantic modifications and fidelity preservation.By adaptively integrating positional and semantic cues, SynPS effectively avoids both over- and under-editing. Extensive experiments on public and newly curated benchmarks demonstrate the superior performance and faithfulness of our approach.", "AI": {"tldr": "The paper introduces SynPS, a method to improve complex non-rigid image editing by addressing attention collapse in large diffusion models.", "motivation": "The study aims to tackle the challenge of faithfully performing complex non-rigid edits in image editing, such as pose or shape changes, which remain difficult due to attention collapse in existing methods.", "method": "SynPS synergistically combines positional embeddings and semantic information. It uses an editing measurement to quantify required editing magnitudes at each denoising step, dynamically modulating attention synergy to balance between semantic changes and image fidelity.", "result": "SynPS successfully avoids attention collapse and achieves superior performance in editing complex images, consistently outperforming prior methods in experiments on both public and new benchmarks.", "conclusion": "The proposed method, SynPS, demonstrates a significant advancement in non-rigid image editing, offering improved balance between semantic changes and fidelity."}}
{"id": "2512.08589", "pdf": "https://arxiv.org/pdf/2512.08589", "abs": "https://arxiv.org/abs/2512.08589", "authors": ["Swarn Singh Warshaneyan", "Maksims Ivanovs", "Bla\u017e Cugmas", "Inese B\u0113rzi\u0146a", "Laura Goldberga", "Mindaugas Tamosiunas", "Roberts Kadi\u0137is"], "title": "Automated Pollen Recognition in Optical and Holographic Microscopy Images", "categories": ["cs.CV", "cs.LG"], "comment": "08 pages, 10 figures, 04 tables, 20 references. Date of Conference: 13-14 June 2025 Date Added to IEEE Xplore: 10 July 2025 Electronic ISBN: 979-8-3315-0969-9 Print on Demand(PoD) ISBN: 979-8-3315-0970-5 DOI: 10.1109/AICCONF64766.2025.11064260 Conference Location: Prague, Czech Republic Online Access: https://ieeexplore.ieee.org/document/11064260", "summary": "This study explores the application of deep learning to improve and automate pollen grain detection and classification in both optical and holographic microscopy images, with a particular focus on veterinary cytology use cases. We used YOLOv8s for object detection and MobileNetV3L for the classification task, evaluating their performance across imaging modalities. The models achieved 91.3% mAP50 for detection and 97% overall accuracy for classification on optical images, whereas the initial performance on greyscale holographic images was substantially lower. We addressed the performance gap issue through dataset expansion using automated labeling and bounding box area enlargement. These techniques, applied to holographic images, improved detection performance from 2.49% to 13.3% mAP50 and classification performance from 42% to 54%. Our work demonstrates that, at least for image classification tasks, it is possible to pair deep learning techniques with cost-effective lensless digital holographic microscopy devices.", "AI": {"tldr": "The paper applies deep learning models for pollen grain detection and classification in optical and holographic microscopy, achieving high performance in optical images and improving holographic performance with specific dataset techniques.", "motivation": "The study aims to leverage deep learning techniques to enhance automated pollen grain detection and classification, especially for cost-effective microscopy in veterinary cytology.", "method": "The paper employed YOLOv8s for object detection and MobileNetV3L for classification, along with techniques like dataset expansion, automated labeling, and bounding box area enlargement to handle performance issues in holographic images.", "result": "The models attained 91.3% mAP50 for detection and 97% accuracy for classification on optical images, with improved performance on holographic images after applying enhancement techniques from 2.49% to 13.3% mAP50 detection and 42% to 54% classification accuracy.", "conclusion": "The study shows the potential of combining deep learning with affordable holographic microscopy for image analysis, especially in fields like veterinary cytology."}}
{"id": "2512.14435", "pdf": "https://arxiv.org/pdf/2512.14435", "abs": "https://arxiv.org/abs/2512.14435", "authors": ["Chang Cai", "Hao Jiang", "Xiaojun Yuan", "Ying-Jun Angela Zhang"], "title": "Score-Based Turbo Message Passing for Plug-and-Play Compressive Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Message-passing algorithms have been adapted for compressive imaging by incorporating various off-the-shelf image denoisers. However, these denoisers rely largely on generic or hand-crafted priors and often fall short in accurately capturing the complex statistical structure of natural images. As a result, traditional plug-and-play (PnP) methods often lead to suboptimal reconstruction, especially in highly underdetermined regimes. Recently, score-based generative models have emerged as a powerful framework for accurately characterizing sophisticated image distribution. Yet, their direct use for posterior sampling typically incurs prohibitive computational complexity. In this paper, by exploiting the close connection between score-based generative modeling and empirical Bayes denoising, we devise a message-passing framework that integrates a score-based minimum mean-squared error (MMSE) denoiser for compressive image recovery. The resulting algorithm, named score-based turbo message passing (STMP), combines the fast convergence of message passing with the expressive power of score-based generative priors. For practical systems with quantized measurements, we further propose quantized STMP (Q-STMP), which augments STMP with a component-wise MMSE dequantization module. We demonstrate that the asymptotic performance of STMP and Q-STMP can be accurately predicted by a set of state-evolution (SE) equations. Experiments on the FFHQ dataset demonstrate that STMP strikes a significantly better performance-complexity tradeoff compared with competing baselines, and that Q-STMP remains robust even under 1-bit quantization. Remarkably, both STMP and Q-STMP typically converge within 10 iterations.", "AI": {"tldr": "This paper introduces a message-passing algorithm (STMP) that uses score-based generative priors for compressive image recovery, achieving fast convergence and improved performance. The method is also extended (Q-STMP) to handle quantized measurements.", "motivation": "Traditional compressive imaging methods using generic denoisers often fail in capturing complex image structures, leading to suboptimal reconstruction, especially in challenging cases.", "method": "The paper combines score-based generative models with message passing to create STMP, an algorithm integrating minimum mean-squared error denoising. It also extends this framework to quantized measurements through a module, Q-STMP.", "result": "STMP outperforms existing methods in reconstruction quality while maintaining lower computational complexity. Q-STMP is shown to be robust even under extreme quantization (1-bit). Both algorithms typically converge within 10 iterations.", "conclusion": "STMP and Q-STMP offer a better performance-complexity tradeoff compared to competing methods in compressive image recovery, making them highly efficient and accurate solutions for reconstructing images from compressed or quantized data."}}
{"id": "2512.12437", "pdf": "https://arxiv.org/pdf/2512.12437", "abs": "https://arxiv.org/abs/2512.12437", "authors": ["Jonathan Spraggett"], "title": "Sim2Real Reinforcement Learning for Soccer skills", "categories": ["cs.RO", "cs.LG"], "comment": "Undergrad Thesis", "summary": "This thesis work presents a more efficient and effective approach to training control-related tasks for humanoid robots using Reinforcement Learning (RL). The traditional RL methods are limited in adapting to real-world environments, complexity, and natural motions, but the proposed approach overcomes these limitations by using curriculum training and Adversarial Motion Priors (AMP) technique. The results show that the developed RL policies for kicking, walking, and jumping are more dynamic, and adaptive, and outperformed previous methods. However, the transfer of the learned policy from simulation to the real world was unsuccessful, highlighting the limitations of current RL methods in fully adapting to real-world scenarios.", "AI": {"tldr": "The paper proposes a novel RL approach for humanoid robot control tasks using curriculum training and Adversarial Motion Priors (AMP). It demonstrates success in simulation but faces transfer challenges to the real world.", "motivation": "Current RL methods struggle with complexity, adaptability, and natural motion in real-world scenarios for humanoid robots.", "method": "Introduced curriculum training and Adversarial Motion Priors (AMP) to enhance the RL training process for humanoid robot tasks.", "result": "The developed RL policies showed improved performance in kicking, walking, and jumping in simulations but failed in real-world transfer.", "conclusion": "While the approach improves upon simulation-based training, real-world adaptability of RL methods remains a significant challenge."}}
{"id": "2512.14440", "pdf": "https://arxiv.org/pdf/2512.14440", "abs": "https://arxiv.org/abs/2512.14440", "authors": ["Leon Sick", "Lukas Hoyer", "Dominik Engel", "Pedro Hermosilla", "Timo Ropinski"], "title": "S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation", "categories": ["cs.CV"], "comment": "Project Page with Code/Models/Demo: https://leonsick.github.io/s2d/", "summary": "In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.", "AI": {"tldr": "This paper proposes a new model for unsupervised video instance segmentation that uses real video data instead of synthetic data, addressing issues like realistic motion and camera movement.", "motivation": "The current methods rely on synthetic data that fail to capture realistic motion dynamics in videos, such as perspective changes, part movements, and camera motion.", "method": "The approach begins with unsupervised instance segmentations on individual frames, introduces high-quality keymask selection using deep motion priors, and trains a segmentation model with a Sparse-To-Dense Distillation method and Temporal DropLoss.", "result": "The proposed model surpasses state-of-the-art benchmarks in unsupervised video instance segmentation performance.", "conclusion": "By using real video data and introducing innovative methods, the paper demonstrates enhanced segmentation performance and highlights the effectiveness of focusing on realistic motion dynamics."}}
{"id": "2512.13709", "pdf": "https://arxiv.org/pdf/2512.13709", "abs": "https://arxiv.org/abs/2512.13709", "authors": ["Reza Ryan", "Napoleon Paciente", "Cahil Youngs", "Nickson Karie", "Qian Li", "Nasim Ferdosian"], "title": "Smart Surveillance: Identifying IoT Device Behaviours using ML-Powered Traffic Analysis", "categories": ["cs.CR", "cs.LG"], "comment": "6 pages, 1 figures, conference", "summary": "The proliferation of Internet of Things (IoT) devices has grown exponentially in recent years, introducing significant security challenges. Accurate identification of the types of IoT devices and their associated actions through network traffic analysis is essential to mitigate potential threats. By monitoring and analysing packet flows between IoT devices and connected networks, anomalous or malicious behaviours can be detected. Existing research focuses primarily on device identification within local networks using methods such as protocol fingerprinting and wireless frequency scanning. However, these approaches are limited in their ability to monitor or classify IoT devices externally. To address this gap, we investigate the use of machine learning (ML) techniques, specifically Random Forest (RF), Multilayer Perceptron (MLP), and K-Nearest Neighbours (KNN), in conjunction with targeted network traffic monitoring to classify IoT device types and their actions. We constructed a testbed comprising an NPAT-enabled router and a diverse set of IoT devices, including smart cameras, controller hubs, home appliances, power controllers, and streaming devices. Experimental results demonstrate that IoT device and action recognition is feasible using our proposed ML-driven approach, with the RF classifier achieving the highest accuracy of 91%, while the MLP recorded the lowest accuracy at 56%. Notably, all device categories were successfully classified except for certain actions associated with security cameras, underscoring both the potential and the limitations of the proposed method.", "AI": {"tldr": "This paper addresses the challenge of classifying IoT devices and their actions using machine learning-based approaches and network traffic analysis.", "motivation": "The exponential growth of IoT devices has introduced substantial security risks, highlighting the need for accurate device identification and monitoring to detect potential threats.", "method": "The researchers utilized Random Forest (RF), Multilayer Perceptron (MLP), and K-Nearest Neighbours (KNN) machine learning techniques along with targeted network traffic analysis to classify IoT device types and actions.", "result": "The RF classifier achieved the highest accuracy at 91%, while MLP recorded the lowest at 56%. All device categories were classified except for specific security camera actions, demonstrating the effectiveness and limitations of the approach.", "conclusion": "ML techniques, particularly RF, show promise in identifying IoT devices and actions via network traffic monitoring. However, further refinement is needed for certain device actions."}}
{"id": "2512.14477", "pdf": "https://arxiv.org/pdf/2512.14477", "abs": "https://arxiv.org/abs/2512.14477", "authors": ["Andreas Sj\u00f6lander", "Valeria Belloni", "Robel Fekadu", "Andrea Nascetti"], "title": "TACK Tunnel Data (TTD): A Benchmark Dataset for Deep Learning-Based Defect Detection in Tunnels", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Tunnels are essential elements of transportation infrastructure, but are increasingly affected by ageing and deterioration mechanisms such as cracking. Regular inspections are required to ensure their safety, yet traditional manual procedures are time-consuming, subjective, and costly. Recent advances in mobile mapping systems and Deep Learning (DL) enable automated visual inspections. However, their effectiveness is limited by the scarcity of tunnel datasets. This paper introduces a new publicly available dataset containing annotated images of three different tunnel linings, capturing typical defects: cracks, leaching, and water infiltration. The dataset is designed to support supervised, semi-supervised, and unsupervised DL methods for defect detection and segmentation. Its diversity in texture and construction techniques also enables investigation of model generalization and transferability across tunnel types. By addressing the critical lack of domain-specific data, this dataset contributes to advancing automated tunnel inspection and promoting safer, more efficient infrastructure maintenance strategies.", "AI": {"tldr": "The paper presents a new publicly available dataset for automating the inspection of tunnels using Deep Learning methods, addressing a shortage of domain-specific datasets.", "motivation": "The work is motivated by the need for safer and more efficient inspections of aging and deteriorating tunnels using automated methods, as traditional manual inspections are costly, subjective, and time-consuming.", "method": "The authors created a diverse dataset of annotated images displaying typical tunnel defects (cracks, leaching, and water infiltration), suitable for supervised, semi-supervised, and unsupervised Deep Learning methods.", "result": "The dataset provides diversity in textures and construction techniques, allowing investigation of model transferability across different tunnel types and advancing defect detection methodologies.", "conclusion": "Introducing this dataset addresses the limitation of domain-specific data, promoting improved automated tunnel inspection and infrastructure maintenance."}}
{"id": "2512.14480", "pdf": "https://arxiv.org/pdf/2512.14480", "abs": "https://arxiv.org/abs/2512.14480", "authors": ["Weiheng Zhao", "Zilong Huang", "Jiashi Feng", "Xinggang Wang"], "title": "SuperCLIP: CLIP with Simple Classification Supervision", "categories": ["cs.CV"], "comment": "Accepted by NeurIPS 2025. Code: https://github.com/hustvl/SuperCLIP", "summary": "Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.", "AI": {"tldr": "The paper introduces SuperCLIP, an enhancement over CLIP, which incorporates token-level supervision for improved alignment of images and text.", "motivation": "CLIP models underutilize fine-grained semantic signals in text, especially with long and detailed captions, due to their global similarity objective.", "method": "SuperCLIP adds a lightweight linear layer to the vision encoder, introducing classification-based supervision to improve token-level visual-text alignment.", "result": "SuperCLIP enhances performance in zero-shot classification, image-text retrieval, and visual tasks, with minimal computational overhead and no need for extra annotated data.", "conclusion": "SuperCLIP refines CLIP's visual-textual alignment, recovers textual supervision from various data contexts, and offers robustness against small-batch size issues, making it a practical enhancement."}}
{"id": "2512.14489", "pdf": "https://arxiv.org/pdf/2512.14489", "abs": "https://arxiv.org/abs/2512.14489", "authors": ["Alessia Micieli", "Giovanni Maria Farinella", "Francesco Ragusa"], "title": "SignIT: A Comprehensive Dataset and Multimodal Analysis for Italian Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "In this work we present SignIT, a new dataset to study the task of Italian Sign Language (LIS) recognition. The dataset is composed of 644 videos covering 3.33 hours. We manually annotated videos considering a taxonomy of 94 distinct sign classes belonging to 5 macro-categories: Animals, Food, Colors, Emotions and Family. We also extracted 2D keypoints related to the hands, face and body of the users. With the dataset, we propose a benchmark for the sign recognition task, adopting several state-of-the-art models showing how temporal information, 2D keypoints and RGB frames can be influence the performance of these models. Results show the limitations of these models on this challenging LIS dataset. We release data and annotations at the following link: https://fpv-iplab.github.io/SignIT/.", "AI": {"tldr": "The paper introduces 'SignIT', a new dataset for Italian Sign Language (LIS) recognition, comprising 644 videos and 94 annotated signs. The dataset includes 2D keypoints, and benchmarks different models for sign recognition.", "motivation": "The motivation is to develop a robust dataset and benchmark for the task of recognizing Italian Sign Language, addressing the limitations of current recognition models and promoting study in this area.", "method": "The authors created and annotated a LIS dataset with 94 sign classes across five categories, extracted 2D keypoints, and evaluated state-of-the-art models for sign recognition using temporal, keypoints, and RGB data.", "result": "Results indicate that current state-of-the-art models face challenges in effectively recognizing signs from the proposed dataset, highlighting its complexity.", "conclusion": "The SignIT dataset fills a gap for Italian Sign Language research by providing annotated data and benchmarks, yet state-of-the-art models remain limited on this complex task."}}
{"id": "2512.13746", "pdf": "https://arxiv.org/pdf/2512.13746", "abs": "https://arxiv.org/abs/2512.13746", "authors": ["Elham Kiyani", "Amit Makarand Deshpande", "Madhura Limaye", "Zhiwei Gao", "Sai Aditya Pradeep", "Srikanth Pilla", "Gang Li", "Zhen Li", "George Em Karniadakis"], "title": "Probabilistic Predictions of Process-Induced Deformation in Carbon/Epoxy Composites Using a Deep Operator Network", "categories": ["cs.CE", "cond-mat.mtrl-sci", "cs.LG"], "comment": "21 pages, 13 figures", "summary": "Fiber reinforcement and polymer matrix respond differently to manufacturing conditions due to mismatch in coefficient of thermal expansion and matrix shrinkage during curing of thermosets. These heterogeneities generate residual stresses over multiple length scales, whose partial release leads to process-induced deformation (PID), requiring accurate prediction and mitigation via optimized non-isothermal cure cycles. This study considers a unidirectional AS4 carbon fiber/amine bi-functional epoxy prepreg and models PID using a two-mechanism framework that accounts for thermal expansion/shrinkage and cure shrinkage. The model is validated against manufacturing trials to identify initial and boundary conditions, then used to generate PID responses for a diverse set of non-isothermal cure cycles (time-temperature profiles). Building on this physics-based foundation, we develop a data-driven surrogate based on Deep Operator Networks (DeepONets). A DeepONet is trained on a dataset combining high-fidelity simulations with targeted experimental measurements of PID. We extend this to a Feature-wise Linear Modulation (FiLM) DeepONet, where branch-network features are modulated by external parameters, including the initial degree of cure, enabling prediction of time histories of degree of cure, viscosity, and deformation. Because experimental data are available only at limited time instances (for example, final deformation), we use transfer learning: simulation-trained trunk and branch networks are fixed and only the final layer is updated using measured final deformation. Finally, we augment the framework with Ensemble Kalman Inversion (EKI) to quantify uncertainty under experimental conditions and to support optimization of cure schedules for reduced PID in composites.", "AI": {"tldr": "The study addresses process-induced deformation in composites due to mismatched thermal expansion, proposing a physics-based model combined with deep learning to improve predictions and optimize curing schedules.", "motivation": "The research aims to address process-induced deformation (PID) arising from thermal expansion and shrinkage mismatches during the curing of thermoset composites, which affect manufacturing precision.", "method": "The paper employs a two-mechanism model for PID analysis, validated by experiments. It integrates physics-based simulation with DeepONet and transfer learning to improve prediction capabilities. Ensemble Kalman Inversion is used for uncertainty quantification and optimization.", "result": "The proposed framework effectively predicts PID and related properties, even with limited experimental data, and optimizes curing schedules using a combination of simulation and experimental insights.", "conclusion": "The integration of physics-driven and data-driven methods combined with transfer learning offers a robust approach for predicting deformation and optimizing curing processes in composite manufacturing."}}
{"id": "2512.14499", "pdf": "https://arxiv.org/pdf/2512.14499", "abs": "https://arxiv.org/abs/2512.14499", "authors": ["Jia Guo", "Jiawei Du", "Shengzhu Yang", "Shuai Lu", "Wenquan Cheng", "Kaiwen Zhang", "Yihua Sun", "Chuhong Yang", "Weihang Zhang", "Fang Chen", "Yilan Wu", "Lie Ju", "Guochen Ning", "Longfei Ma", "Huiping Yao", "Jinyuan Wang", "Peilun Shi", "Yukun Zhou", "Jie Xu", "Pearse A. Keane", "Hanruo Liu", "Hongen Liao", "Ningli Wang", "Huiqi Li"], "title": "Native Intelligence Emerges from Large-Scale Clinical Practice: A Retinal Foundation Model with Deployment Efficiency", "categories": ["cs.CV"], "comment": null, "summary": "Current retinal foundation models remain constrained by curated research datasets that lack authentic clinical context, and require extensive task-specific optimization for each application, limiting their deployment efficiency in low-resource settings. Here, we show that these barriers can be overcome by building clinical native intelligence directly from real-world medical practice. Our key insight is that large-scale telemedicine programs, where expert centers provide remote consultations across distributed facilities, represent a natural reservoir for learning clinical image interpretation. We present ReVision, a retinal foundation model that learns from the natural alignment between 485,980 color fundus photographs and their corresponding diagnostic reports, accumulated through a decade-long telemedicine program spanning 162 medical institutions across China. Through extensive evaluation across 27 ophthalmic benchmarks, we demonstrate that ReVison enables deployment efficiency with minimal local resources. Without any task-specific training, ReVision achieves zero-shot disease detection with an average AUROC of 0.946 across 12 public benchmarks and 0.952 on 3 independent clinical cohorts. When minimal adaptation is feasible, ReVision matches extensively fine-tuned alternatives while requiring orders of magnitude fewer trainable parameters and labeled examples. The learned representations also transfer effectively to new clinical sites, imaging domains, imaging modalities, and systemic health prediction tasks. In a prospective reader study with 33 ophthalmologists, ReVision's zero-shot assistance improved diagnostic accuracy by 14.8% across all experience levels. These results demonstrate that clinical native intelligence can be directly extracted from clinical archives without any further annotation to build medical AI systems suited to various low-resource settings.", "AI": {"tldr": "This paper introduces ReVision, a retinal foundation model trained on real-world clinical data from telemedicine programs, demonstrating high performance in zero-shot disease detection and effective deployment in low-resource settings.", "motivation": "The motivation is to address the limitations of current retinal foundation models that rely on curated datasets and extensive task-specific optimization, which restrict their efficiency in clinical and low-resource environments.", "method": "The authors developed ReVision by leveraging real-world clinical data from 485,980 color fundus photographs and diagnostic reports collected through a telemedicine program spanning 162 medical institutions in China.", "result": "ReVision achieved zero-shot disease detection with high AUROC scores (0.946-0.952), effective representation transfer across tasks and settings, and improved diagnostic accuracy by 14.8% in a prospective study with ophthalmologists.", "conclusion": "Clinical native intelligence can be extracted from real-world clinical archives, enabling efficient development of medical AI systems tailored for diverse low-resource environments without requiring additional annotations."}}
{"id": "2512.14536", "pdf": "https://arxiv.org/pdf/2512.14536", "abs": "https://arxiv.org/abs/2512.14536", "authors": ["Yiheng Huang", "Junhong Chen", "Anqi Ning", "Zhanhong Liang", "Nick Michiels", "Luc Claesen", "Wenyin Liu"], "title": "DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors", "categories": ["cs.CV"], "comment": "8 pages, 7 figures", "summary": "Self-supervised monocular depth estimation has achieved notable success under daytime conditions. However, its performance deteriorates markedly at night due to low visibility and varying illumination, e.g., insufficient light causes textureless areas, and moving objects bring blurry regions. To this end, we propose a self-supervised framework named DASP that leverages spatiotemporal priors for nighttime depth estimation. Specifically, DASP consists of an adversarial branch for extracting spatiotemporal priors and a self-supervised branch for learning. In the adversarial branch, we first design an adversarial network where the discriminator is composed of four devised spatiotemporal priors learning blocks (SPLB) to exploit the daytime priors. In particular, the SPLB contains a spatial-based temporal learning module (STLM) that uses orthogonal differencing to extract motion-related variations along the time axis and an axial spatial learning module (ASLM) that adopts local asymmetric convolutions with global axial attention to capture the multiscale structural information. By combining STLM and ASLM, our model can acquire sufficient spatiotemporal features to restore textureless areas and estimate the blurry regions caused by dynamic objects. In the self-supervised branch, we propose a 3D consistency projection loss to bilaterally project the target frame and source frame into a shared 3D space, and calculate the 3D discrepancy between the two projected frames as a loss to optimize the 3D structural consistency and daytime priors. Extensive experiments on the Oxford RobotCar and nuScenes datasets demonstrate that our approach achieves state-of-the-art performance for nighttime depth estimation. Ablation studies further validate the effectiveness of each component.", "AI": {"tldr": "DASP is a framework for improving nighttime monocular depth estimation using spatiotemporal priors through an adversarial and self-supervised learning approach.", "motivation": "Existing self-supervised monocular depth estimation methods perform poorly under nighttime conditions due to challenges like low visibility and illumination variability.", "method": "DASP integrates an adversarial branch with novel modules for extracting spatiotemporal priors and a self-supervised branch with a 3D consistency projection loss for optimization.", "result": "Extensive experiments on Oxford RobotCar and nuScenes datasets show that DASP achieves state-of-the-art performance for nighttime depth estimation.", "conclusion": "The approach effectively addresses nighttime monocular depth estimation challenges and demonstrates significant advancements and validation of its components."}}
{"id": "2512.13757", "pdf": "https://arxiv.org/pdf/2512.13757", "abs": "https://arxiv.org/abs/2512.13757", "authors": ["Neevkumar Manavar", "Hanno Gerd Meyer", "Joachim Wa\u00dfmuth", "Barbara Hammer", "Axel Schneider"], "title": "Improving the Plausibility of Pressure Distributions Synthesized from Depth through Generative Modeling", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Monitoring contact pressure in hospital beds is essential for preventing pressure ulcers and enabling real-time patient assessment. Current methods can predict pressure maps but often lack physical plausibility, limiting clinical reliability. This work proposes a framework that enhances plausibility via Informed Latent Space (ILS) and Weight Optimization Loss (WOL) with generative modeling to produce high-fidelity, physically consistent pressure estimates. This study also applies diffusion based conditional Brownian Bridge Diffusion Model (BBDM) and proposes training strategy for its latent counterpart Latent Brownian Bridge Diffusion Model (LBBDM) tailored for pressure synthesis in lying postures. Experiment results shows proposed method improves physical plausibility and performance over baselines: BBDM with ILS delivers highly detailed maps at higher computational cost and large inference time, whereas LBBDM provides faster inference with competitive performance. Overall, the approach supports non-invasive, vision-based, real-time patient monitoring in clinical environments.", "AI": {"tldr": "The paper presents a framework for improving pressure monitoring in hospital beds to prevent ulcers using advanced generative modeling techniques.", "motivation": "Preventing pressure ulcers and enabling real-time patient monitoring requires reliable and physically accurate pressure maps, which are lacking with current methods.", "method": "The method integrates Informed Latent Space (ILS) and Weight Optimization Loss (WOL) in generative models, uses Brownian Bridge Diffusion Model (BBDM), and proposes a new Latent Brownian Bridge Diffusion Model (LBBDM).", "result": "The proposed framework shows improved physical plausibility and performance. BBDM with ILS produces detailed maps but requires more computational resources, while LBBDM offers faster and competitive inference.", "conclusion": "This approach enhances the reliability of non-invasive, vision-based patient monitoring, supporting its real-time clinical application."}}
{"id": "2512.14540", "pdf": "https://arxiv.org/pdf/2512.14540", "abs": "https://arxiv.org/abs/2512.14540", "authors": ["Andreas Lolos", "Theofilos Christodoulou", "Aris L. Moustakas", "Stergios Christodoulidis", "Maria Vakalopoulou"], "title": "CAPRMIL: Context-Aware Patch Representations for Multiple Instance Learning", "categories": ["cs.CV", "cs.AI"], "comment": "24 pages, 12 Figures, 4 Tables", "summary": "In computational pathology, weak supervision has become the standard for deep learning due to the gigapixel scale of WSIs and the scarcity of pixel-level annotations, with Multiple Instance Learning (MIL) established as the principal framework for slide-level model training. In this paper, we introduce a novel setting for MIL methods, inspired by proceedings in Neural Partial Differential Equation (PDE) Solvers. Instead of relying on complex attention-based aggregation, we propose an efficient, aggregator-agnostic framework that removes the complexity of correlation learning from the MIL aggregator. CAPRMIL produces rich context-aware patch embeddings that promote effective correlation learning on downstream tasks. By projecting patch features -- extracted using a frozen patch encoder -- into a small set of global context/morphology-aware tokens and utilizing multi-head self-attention, CAPRMIL injects global context with linear computational complexity with respect to the bag size. Paired with a simple Mean MIL aggregator, CAPRMIL matches state-of-the-art slide-level performance across multiple public pathology benchmarks, while reducing the total number of trainable parameters by 48%-92.8% versus SOTA MILs, lowering FLOPs during inference by 52%-99%, and ranking among the best models on GPU memory efficiency and training time. Our results indicate that learning rich, context-aware instance representations before aggregation is an effective and scalable alternative to complex pooling for whole-slide analysis. Our code is available at https://github.com/mandlos/CAPRMIL", "AI": {"tldr": "The paper presents CAPRMIL, a novel framework for computational pathology utilizing efficient and context-aware embeddings for Multiple Instance Learning (MIL) with reduced complexity, matching state-of-the-art performance while lowering parameters and resource use.", "motivation": "The study addresses the limitations in conventional MIL approaches in computational pathology, which rely on complex attention-based aggregations, and aims to simplify the correlation learning process while maintaining performance.", "method": "CAPRMIL replaces complex MIL aggregators by creating context-aware patch embeddings using a frozen patch encoder, global context tokens, and multi-head self-attention, paired with a simple Mean MIL aggregator for slide-level training.", "result": "CAPRMIL achieves state-of-the-art performance across benchmarks while reducing the number of trainable parameters by 48%-92.8%, inference FLOPs by 52%-99%, and demonstrating high GPU memory efficiency and faster training times.", "conclusion": "The results show that CAPRMIL's approach of learning rich, context-aware instance representations significantly simplifies MIL frameworks and provides a scalable, efficient solution for whole-slide image analysis."}}
{"id": "2512.14542", "pdf": "https://arxiv.org/pdf/2512.14542", "abs": "https://arxiv.org/abs/2512.14542", "authors": ["Yifang Xu", "Benxiang Zhai", "Yunzhuo Sun", "Ming Li", "Yang Li", "Sidan Du"], "title": "HiFi-Portrait: Zero-shot Identity-preserved Portrait Generation with High-fidelity Multi-face Fusion", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Recent advancements in diffusion-based technologies have made significant strides, particularly in identity-preserved portrait generation (IPG). However, when using multiple reference images from the same ID, existing methods typically produce lower-fidelity portraits and struggle to customize face attributes precisely. To address these issues, this paper presents HiFi-Portrait, a high-fidelity method for zero-shot portrait generation. Specifically, we first introduce the face refiner and landmark generator to obtain fine-grained multi-face features and 3D-aware face landmarks. The landmarks include the reference ID and the target attributes. Then, we design HiFi-Net to fuse multi-face features and align them with landmarks, which improves ID fidelity and face control. In addition, we devise an automated pipeline to construct an ID-based dataset for training HiFi-Portrait. Extensive experimental results demonstrate that our method surpasses the SOTA approaches in face similarity and controllability. Furthermore, our method is also compatible with previous SDXL-based works.", "AI": {"tldr": "This paper introduces HiFi-Portrait, a zero-shot portrait generation method that enhances ID fidelity and precise face attribute control using landmark-based techniques.", "motivation": "The existing methods struggle with reduced fidelity and limited face attribute customization when using multiple reference images for identity-preserved portrait generation.", "method": "HiFi-Portrait uses a face refiner and landmark generator to extract detailed features and 3D-aware landmarks. HiFi-Net is designed to fuse multiple features with landmarks for better ID fidelity and facial control. An ID-based database is also introduced for training the model.", "result": "Experimental results indicate HiFi-Portrait outperforms existing methods in maintaining face similarity and enhancing controllability.", "conclusion": "HiFi-Portrait offers a superior approach for high-fidelity and customizable portrait generation, showing compatibility with prior SDXL-based methods."}}
{"id": "2512.14550", "pdf": "https://arxiv.org/pdf/2512.14550", "abs": "https://arxiv.org/abs/2512.14550", "authors": ["Zhiwen Yang", "Jiaju Zhang", "Yang Yi", "Jian Liang", "Bingzheng Wei", "Yan Xu"], "title": "TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration", "categories": ["cs.CV"], "comment": "This paper has been accepted by MICCAI 2025", "summary": "Medical image restoration (MedIR) aims to recover high-quality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks--PET synthesis, CT denoising, and MRI super-resolution--both in task-specific and All-in-One settings. Code is available at https://github.com/Yaziwel/TAT.", "AI": {"tldr": "This paper introduces a task-adaptive Transformer (TAT) for addressing challenges in multi-task medical image restoration (MedIR), achieving state-of-the-art results in PET synthesis, CT denoising, and MRI super-resolution.", "motivation": "To solve the challenges of task interference and task imbalance in multi-task MedIR models, which arise from differences in task modality and degradation types.", "method": "Proposed a task-adaptive Transformer (TAT) with two innovations: task-adaptive weight generation to avoid gradient conflicts, and task-adaptive loss balancing to address uneven task optimization.", "result": "The TAT model demonstrated superior performance across three MedIR tasks, achieving state-of-the-art results in both individual and multi-task settings.", "conclusion": "TAT is an effective framework for multi-task MedIR, dynamically adapting to tasks and overcoming inter-task relationship challenges. Code is publicly available."}}
{"id": "2512.13825", "pdf": "https://arxiv.org/pdf/2512.13825", "abs": "https://arxiv.org/abs/2512.13825", "authors": ["Jacob Taylor", "Haining Pan", "Sankar Das Sarma"], "title": "Unreasonable effectiveness of unsupervised learning in identifying Majorana topology", "categories": ["cond-mat.dis-nn", "cond-mat.mes-hall", "cs.LG"], "comment": "7 pages, 4 figures", "summary": "In unsupervised learning, the training data for deep learning does not come with any labels, thus forcing the algorithm to discover hidden patterns in the data for discerning useful information. This, in principle, could be a powerful tool in identifying topological order since topology does not always manifest in obvious physical ways (e.g., topological superconductivity) for its decisive confirmation. The problem, however, is that unsupervised learning is a difficult challenge, necessitating huge computing resources, which may not always work. In the current work, we combine unsupervised and supervised learning using an autoencoder to establish that unlabeled data in the Majorana splitting in realistic short disordered nanowires may enable not only a distinction between `topological' and `trivial', but also where their crossover happens in the relevant parameter space. This may be a useful tool in identifying topology in Majorana nanowires.", "AI": {"tldr": "The paper explores a method combining unsupervised and supervised learning through autoencoders to identify topological order in Majorana splitting of disordered nanowires.", "motivation": "Identifying topological order in certain systems like Majorana nanowires is challenging due to its subtle manifestations, and better methods are needed.", "method": "The authors use a combined approach of unsupervised and supervised learning utilizing an autoencoder to analyze unlabeled data related to Majorana splitting.", "result": "The method successfully distinguished between topological and trivial states, and also identified crossover regions in the parameter space.", "conclusion": "Combining learning techniques with autoencoders can effectively analyze complex systems like Majorana nanowires, helping to identify topology."}}
{"id": "2512.14560", "pdf": "https://arxiv.org/pdf/2512.14560", "abs": "https://arxiv.org/abs/2512.14560", "authors": ["Xianwei Cao", "Dou Quan", "Shuang Wang", "Ning Huyan", "Wei Wang", "Yunan Li", "Licheng Jiao"], "title": "CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 6 figures", "summary": "Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.", "AI": {"tldr": "This paper addresses cross-view geo-localization by introducing CLNet, a framework improving spatial correspondences between different viewpoints.", "motivation": "Existing methods struggle to explicitly model spatial correspondences between significantly different viewpoints, such as satellite and street-level images.", "method": "They propose CLNet with three modules: Neural Correspondence Map (aligns spatial features), Nonlinear Embedding Converter (remaps features across perspectives), and Global Feature Recalibration (reweights feature channels).", "result": "CLNet outperforms state-of-the-art methods and generalizes well, as validated by experiments on four public datasets.", "conclusion": "The proposed CLNet successfully improves geo-localization tasks with explicit spatial alignment and offers superior interpretability and performance across benchmarks."}}
{"id": "2512.13848", "pdf": "https://arxiv.org/pdf/2512.13848", "abs": "https://arxiv.org/abs/2512.13848", "authors": ["Mufhumudzi Muthivhi", "Terence L van Zyl", "Hairong Wang"], "title": "BiCoRec: Bias-Mitigated Context-Aware Sequential Recommendation Model", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "Sequential recommendation models aim to learn from users evolving preferences. However, current state-of-the-art models suffer from an inherent popularity bias. This study developed a novel framework, BiCoRec, that adaptively accommodates users changing preferences for popular and niche items. Our approach leverages a co-attention mechanism to obtain a popularity-weighted user sequence representation, facilitating more accurate predictions. We then present a new training scheme that learns from future preferences using a consistency loss function. BiCoRec aimed to improve the recommendation performance of users who preferred niche items. For these users, BiCoRec achieves a 26.00% average improvement in NDCG@10 over state-of-the-art baselines. When ranking the relevant item against the entire collection, BiCoRec achieves NDCG@10 scores of 0.0102, 0.0047, 0.0021, and 0.0005 for the Movies, Fashion, Games and Music datasets.", "AI": {"tldr": "The paper introduces BiCoRec, a framework overcoming popularity bias in sequential recommendation models, showing significant improvements for users with niche preferences.", "motivation": "To address popularity bias in sequential recommendation systems that hinders accurate recommendations for users with niche preferences.", "method": "BiCoRec uses a co-attention mechanism to weigh user sequence representation by popularity and incorporates a new training scheme with a consistency loss function for learning future preferences.", "result": "BiCoRec significantly improves recommendations for users with niche preferences, showing an average NDCG@10 improvement of 26.00% over baselines and achieving strong scores across various datasets.", "conclusion": "BiCoRec effectively addresses popularity bias, enhancing recommendation performance for niche users, as demonstrated by performance metrics across multiple datasets."}}
{"id": "2512.14574", "pdf": "https://arxiv.org/pdf/2512.14574", "abs": "https://arxiv.org/abs/2512.14574", "authors": ["Mitsuki Watanabe", "Sosuke Amano", "Kiyoharu Aizawa", "Yoko Yamakata"], "title": "FoodLogAthl-218: Constructing a Real-World Food Image Dataset Using Dietary Management Applications", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Food image classification models are crucial for dietary management applications because they reduce the burden of manual meal logging. However, most publicly available datasets for training such models rely on web-crawled images, which often differ from users' real-world meal photos. In this work, we present FoodLogAthl-218, a food image dataset constructed from real-world meal records collected through the dietary management application FoodLog Athl. The dataset contains 6,925 images across 218 food categories, with a total of 14,349 bounding boxes. Rich metadata, including meal date and time, anonymized user IDs, and meal-level context, accompany each image. Unlike conventional datasets-where a predefined class set guides web-based image collection-our data begins with user-submitted photos, and labels are applied afterward. This yields greater intra-class diversity, a natural frequency distribution of meal types, and casual, unfiltered images intended for personal use rather than public sharing. In addition to (1) a standard classification benchmark, we introduce two FoodLog-specific tasks: (2) an incremental fine-tuning protocol that follows the temporal stream of users' logs, and (3) a context-aware classification task where each image contains multiple dishes, and the model must classify each dish by leveraging the overall meal context. We evaluate these tasks using large multimodal models (LMMs). The dataset is publicly available at https://huggingface.co/datasets/FoodLog/FoodLogAthl-218.", "AI": {"tldr": "This paper introduces FoodLogAthl-218, a real-world food image dataset derived from user-submitted photos through the FoodLog Athl app. The dataset includes diverse images, metadata, and supports novel classification tasks.", "motivation": "Most publicly available food classification datasets rely on web-crawled images, which are not representative of real-world user meal photos, thus highlighting the need for datasets derived from real-world meal records.", "method": "The authors created FoodLogAthl-218, using images submitted by users of a dietary management app. They applied labels afterward, enabling greater intra-class diversity. They designed tasks including standard classification, incremental fine-tuning, and context-aware classification.", "result": "FoodLogAthl-218 was evaluated using large multimodal models, demonstrating its utility for both standard and novel food image classification tasks.", "conclusion": "The dataset provides a new resource for advancing dietary management applications, offering real-world diversity, richness of metadata, and supporting complex classification tasks unavailable in existing datasets."}}
{"id": "2512.14594", "pdf": "https://arxiv.org/pdf/2512.14594", "abs": "https://arxiv.org/abs/2512.14594", "authors": ["Chenyu Zhao", "Yingxue Xu", "Fengtao Zhou", "Yihui Wang", "Hao Chen"], "title": "LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Current multimodal survival prediction methods typically rely on pathology images (WSIs) and genomic data, both of which are high-dimensional and redundant, making it difficult to extract discriminative features from them and align different modalities. Moreover, using a simple survival follow-up label is insufficient to supervise such a complex task. To address these challenges, we propose KEMM, an LLM-driven Knowledge-Enhanced Multimodal Model for cancer survival prediction, which integrates expert reports and prognostic background knowledge. 1) Expert reports, provided by pathologists on a case-by-case basis and refined by large language model (LLM), offer succinct and clinically focused diagnostic statements. This information may typically suggest different survival outcomes. 2) Prognostic background knowledge (PBK), generated concisely by LLM, provides valuable prognostic background knowledge on different cancer types, which also enhances survival prediction. To leverage these knowledge, we introduce the knowledge-enhanced cross-modal (KECM) attention module. KECM can effectively guide the network to focus on discriminative and survival-relevant features from highly redundant modalities. Extensive experiments on five datasets demonstrate that KEMM achieves state-of-the-art performance. The code will be released upon acceptance.", "AI": {"tldr": "The paper proposes KEMM, a knowledge-enhanced multimodal model combining expert reports and prognostic background knowledge generated by LLM, to improve cancer survival prediction.", "motivation": "Existing methods struggle to extract relevant features from high-dimensional and redundant pathology images and genomic data. Simple survival labels fail to adequately supervise complex survival prediction tasks.", "method": "KEMM integrates expert diagnostic reports and prognostic background knowledge refined and generated by LLM, paired with a knowledge-enhanced cross-modal attention module to refine feature extraction and alignment across modalities.", "result": "The proposed model demonstrates state-of-the-art performance across five datasets.", "conclusion": "Integrating expert insights and LLM-driven knowledge enhances the ability of models to focus on survival-relevant features, improving accuracy in cancer survival prediction."}}
{"id": "2512.13868", "pdf": "https://arxiv.org/pdf/2512.13868", "abs": "https://arxiv.org/abs/2512.13868", "authors": ["Tianyu Zhou", "Zihao Liang", "Zehui Lu", "Shaoshuai Mou"], "title": "Safe Online Control-Informed Learning", "categories": ["eess.SY", "cs.LG", "math.OC"], "comment": null, "summary": "This paper proposes a Safe Online Control-Informed Learning framework for safety-critical autonomous systems. The framework unifies optimal control, parameter estimation, and safety constraints into an online learning process. It employs an extended Kalman filter to incrementally update system parameters in real time, enabling robust and data-efficient adaptation under uncertainty. A softplus barrier function enforces constraint satisfaction during learning and control while eliminating the dependence on high-quality initial guesses. Theoretical analysis establishes convergence and safety guarantees, and the framework's effectiveness is demonstrated on cart-pole and robot-arm systems.", "AI": {"tldr": "The paper introduces a novel framework combining control, learning, and safety for autonomous systems with real-time updates.", "motivation": "To enhance safety-critical autonomous systems by integrating real-time learning with safety guarantees.", "method": "Utilizes optimal control, extended Kalman filter for online parameter updates, and a softplus barrier function for safety constraints.", "result": "The proposed framework ensures robust adaptation under uncertainty and provides convergence and safety guarantees. Effectiveness validated with cart-pole and robot-arm systems.", "conclusion": "The framework successfully achieves safe and adaptive learning in autonomous systems with theoretical and practical validation."}}
{"id": "2512.14595", "pdf": "https://arxiv.org/pdf/2512.14595", "abs": "https://arxiv.org/abs/2512.14595", "authors": ["Mengyu Li", "Xingcheng Zhou", "Guang Chen", "Alois Knoll", "Hu Cao"], "title": "TUMTraf EMOT: Event-Based Multi-Object Tracking Dataset and Baseline for Traffic Scenarios", "categories": ["cs.CV"], "comment": "10 pages, 9 figures", "summary": "In Intelligent Transportation Systems (ITS), multi-object tracking is primarily based on frame-based cameras. However, these cameras tend to perform poorly under dim lighting and high-speed motion conditions. Event cameras, characterized by low latency, high dynamic range and high temporal resolution, have considerable potential to mitigate these issues. Compared to frame-based vision, there are far fewer studies on event-based vision. To address this research gap, we introduce an initial pilot dataset tailored for event-based ITS, covering vehicle and pedestrian detection and tracking. We establish a tracking-by-detection benchmark with a specialized feature extractor based on this dataset, achieving excellent performance.", "AI": {"tldr": "This paper discusses the development of an initial dataset and tracking benchmark for event-based Intelligent Transportation Systems (ITS).", "motivation": "To address the challenges faced by frame-based cameras under dim lighting and high-speed motion and to bridge the research gap in event-based vision for ITS.", "method": "The authors created a pilot dataset specifically for event-based ITS, focusing on vehicle and pedestrian detection and tracking. They also developed a tracking benchmark with a specialized feature extractor.", "result": "The specialized feature extractor and benchmark achieved excellent performance in detection and tracking tasks on the event-based dataset.", "conclusion": "Event cameras show significant potential for advancing ITS, and the proposed dataset and benchmark provide a foundation for further research in event-based detection and tracking."}}
{"id": "2512.14130", "pdf": "https://arxiv.org/pdf/2512.14130", "abs": "https://arxiv.org/abs/2512.14130", "authors": ["Amirmohammad Pasdar", "Toby Murray", "Van-Thuan Pham"], "title": "UIXPOSE: Mobile Malware Detection via Intention-Behaviour Discrepancy Analysis", "categories": ["cs.CR", "cs.AI"], "comment": "15 pages", "summary": "We introduce UIXPOSE, a source-code-agnostic framework that operates on both compiled and open-source apps. This framework applies Intention Behaviour Alignment (IBA) to mobile malware analysis, aligning UI-inferred intent with runtime semantics. Previous work either infers intent statically, e.g., permission-centric, or widget-level or monitors coarse dynamic signals (endpoints, partial resource usage) that miss content and context. UIXPOSE infers an intent vector from each screen using vision-language models and knowledge structures and combines decoded network payloads, heap/memory signals, and resource utilisation traces into a behaviour vector. Their alignment, calculated at runtime, can both detect misbehaviour and highlight exploration of behaviourally rich paths. In three real-world case studies, UIXPOSE reveals covert exfiltration and hidden background activity that evade metadata-only baselines, demonstrating how IBA improves dynamic detection.", "AI": {"tldr": "UIXPOSE is a framework for mobile malware analysis that aligns UI-inferred intent with runtime behaviors using advanced methods like vision-language models.", "motivation": "To address limitations in current malware detection methods which miss critical content and behavioral context by relying on static or coarse dynamic signals.", "method": "UIXPOSE uses Intention Behaviour Alignment (IBA), combining UI intents derived via vision-language models with detailed runtime semantic measures such as network payloads, memory signals, and resource traces.", "result": "UIXPOSE successfully detects covert malware behavior during three real-world case studies, outperforming metadata-only baselines.", "conclusion": "IBA-based dynamic detection through UIXPOSE strengthens capabilities to identify hidden malicious activities in mobile apps, enhancing security measures."}}
{"id": "2512.13870", "pdf": "https://arxiv.org/pdf/2512.13870", "abs": "https://arxiv.org/abs/2512.13870", "authors": ["Ricardo Gon\u00e7alves Molinari", "Leonardo Abdala Elias"], "title": "Simultaneous and Proportional Finger Motion Decoding Using Spatial Features from High-Density Surface Electromyography", "categories": ["eess.SP", "cs.LG", "eess.SY"], "comment": "39 pages, 13 figures, 2 tables", "summary": "Restoring natural and intuitive hand function requires simultaneous and proportional control (SPC) of multiple degrees of freedom (DoFs). This study systematically evaluated the multichannel linear descriptors-based block field method (MLD-BFM) for continuous decoding of five finger-joint DoFs by leveraging the rich spatial information of high-density surface electromyography (HD sEMG). Twenty-one healthy participants performed dynamic sinusoidal finger movements while HD sEMG signals were recorded from the \\textit{extensor digitorum communis} (EDC) and \\textit{flexor digitorum superficialis} (FDS) muscles. MLD-BFM extracted region-specific spatial features, including effective field strength ($\u03a3$), field-strength variation rate ($\u03a6$), and spatial complexity ($\u03a9$). Model performance was optimized (block size: $2 \\times 2$; window: 0.15 s) and compared with conventional time-domain features and dimensionality reduction approaches when applied to multi-output regression models. MLD-BFM consistently achieved the highest $\\mathrm{R}^2_{\\mathrm{vw}}$ values across all models. The multilayer perceptron (MLP) combined with MLD-BFM yielded the best performance ($\\mathrm{R}^2_{\\mathrm{vw}} = 86.68\\% \\pm 0.33$). Time-domain features also showed strong predictive capability and were statistically comparable to MLD-BFM in some models, whereas dimensionality reduction techniques exhibited lower accuracy. Decoding accuracy was higher for the middle and ring fingers than for the thumb. Overall, MLD-BFM improved continuous finger movement decoding accuracy, underscoring the importance of taking advantage of the spatial richness of HD sEMG. These findings suggest that spatially structured features enhance SPC and provide practical guidance for designing robust, real-time, and responsive myoelectric interfaces.", "AI": {"tldr": "The study evaluates the multichannel linear descriptors-based block field method (MLD-BFM) using high-density sEMG to decode finger movements and achieves high accuracy, highlighting its potential for improving hand function restoration.", "motivation": "To enhance real-time, robust control of multiple degrees of freedom (DoFs) for natural hand function restoration through better use of spatial information from HD sEMG.", "method": "The MLD-BFM approach was tested on 21 participants performing finger movements with HD sEMG recordings. Spatial features were extracted and compared to time-domain features and dimensionality reduction techniques in multi-output regression models, including optimization of parameters for better decoding.", "result": "MLD-BFM consistently achieved the highest decoding accuracy, performing best with a multilayer perceptron model (86.68% R\u00b2). Time-domain features were similarly effective in some models, while dimensionality reduction methods underperformed.", "conclusion": "MLD-BFM significantly enhances continuous decoding of finger movements by leveraging HD sEMG\u2019s spatial richness, suggesting its utility in designing advanced myoelectric interfaces for hand function restoration."}}
{"id": "2512.14601", "pdf": "https://arxiv.org/pdf/2512.14601", "abs": "https://arxiv.org/abs/2512.14601", "authors": ["Zhaolun Li", "Jichang Li", "Yinqi Cai", "Junye Chen", "Xiaonan Luo", "Guanbin Li", "Rushi Lan"], "title": "FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we propose FakeRadar, a novel deepfake video detection framework designed to address the challenges of cross-domain generalization in real-world scenarios. Existing detection methods typically rely on manipulation-specific cues, performing well on known forgery types but exhibiting severe limitations against emerging manipulation techniques. This poor generalization stems from their inability to adapt effectively to unseen forgery patterns. To overcome this, we leverage large-scale pretrained models (e.g. CLIP) to proactively probe the feature space, explicitly highlighting distributional gaps between real videos, known forgeries, and unseen manipulations. Specifically, FakeRadar introduces Forgery Outlier Probing, which employs dynamic subcluster modeling and cluster-conditional outlier generation to synthesize outlier samples near boundaries of estimated subclusters, simulating novel forgery artifacts beyond known manipulation types. Additionally, we design Outlier-Guided Tri-Training, which optimizes the detector to distinguish real, fake, and outlier samples using proposed outlier-driven contrastive learning and outlier-conditioned cross-entropy losses. Experiments show that FakeRadar outperforms existing methods across various benchmark datasets for deepfake video detection, particularly in cross-domain evaluations, by handling the variety of emerging manipulation techniques.", "AI": {"tldr": "The paper introduces FakeRadar, a deepfake video detection framework that enhances cross-domain generalization by leveraging pretrained models and innovative techniques to simulate unseen forgery patterns.", "motivation": "Current deepfake video detection methods fail to generalize well across emerging manipulation techniques, limiting their real-world applicability.", "method": "FakeRadar utilizes Forgery Outlier Probing to create outlier samples representing unseen manipulations and Outlier-Guided Tri-Training to optimize forgery detection with advanced contrastive learning and loss strategies.", "result": "FakeRadar surpasses existing detection methods in cross-domain evaluations and shows improved performance on benchmark datasets.", "conclusion": "By effectively handling unseen manipulation types, FakeRadar improves the reliability of deepfake video detection methods in real-world applications."}}
{"id": "2512.14138", "pdf": "https://arxiv.org/pdf/2512.14138", "abs": "https://arxiv.org/abs/2512.14138", "authors": ["So Kuroki", "Manami Nakagawa", "Shigeo Yoshida", "Yuki Koyama", "Kozuno Tadashi"], "title": "LAPPI: Interactive Optimization with LLM-Assisted Preference-Based Problem Instantiation", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Many real-world tasks, such as trip planning or meal planning, can be formulated as combinatorial optimization problems. However, using optimization solvers is difficult for end users because it requires problem instantiation: defining candidate items, assigning preference scores, and specifying constraints. We introduce LAPPI (LLM-Assisted Preference-based Problem Instantiation), an interactive approach that uses large language models (LLMs) to support users in this instantiation process. Through natural language conversations, the system helps users transform vague preferences into well-defined optimization problems. These instantiated problems are then passed to existing optimization solvers to generate solutions. In a user study on trip planning, our method successfully captured user preferences and generated feasible plans that outperformed both conventional and prompt-engineering approaches. We further demonstrate LAPPI's versatility by adapting it to an additional use case.", "AI": {"tldr": "The paper introduces LAPPI, a system that uses large language models (LLMs) to assist users in solving combinatorial optimization problems by transforming vague preferences into concrete optimization tasks through interactive conversations.", "motivation": "The motivation behind this paper is the challenge end users face in utilizing optimization solvers for tasks like trip or meal planning, as it requires detailed problem instantiation that users may find difficult to articulate.", "method": "The method proposes LAPPI, which leverages LLMs to guide natural language conversations with users. It translates their vague preferences into well-structured optimization problems that can be solved by conventional solvers.", "result": "A user study in trip planning showed that LAPPI effectively captured user preferences and produced superior feasible plans compared to both traditional methods and prompt-engineering-based approaches.", "conclusion": "LAPPI successfully bridges the gap between user preferences and formal optimization problem-solving, proving its adaptability and efficiency across multiple practical use cases."}}
{"id": "2512.13890", "pdf": "https://arxiv.org/pdf/2512.13890", "abs": "https://arxiv.org/abs/2512.13890", "authors": ["Charles Marrder", "Shuo Sun", "Murray J. Holland"], "title": "Group-Theoretic Reinforcement Learning of Dynamical Decoupling Sequences", "categories": ["quant-ph", "cs.LG", "eess.SY"], "comment": null, "summary": "Dynamical decoupling seeks to mitigate phase decoherence in qubits by applying a carefully designed sequence of effectively instantaneous electromagnetic pulses. Although analytic solutions exist for pulse timings that are optimal under specific noise regimes, identifying the optimal timings for a realistic noise spectrum remains challenging. We propose a reinforcement learning (RL)-based method for designing pulse sequences on qubits. Our novel action set enables the RL agent to efficiently navigate this inherently non-convex optimization landscape. The action set, derived from Thompson's group $F$, is applicable to a broad class of sequential decision problems whose states can be represented as bounded sequences. We demonstrate that our RL agent can learn pulse sequences that minimize dephasing without requiring explicit knowledge of the underlying noise spectrum. This work opens the possibility for real-time learning of optimal dynamical decoupling sequences on qubits which are dephasing-limited. The model-free nature of our algorithm suggests that the agent may ultimately learn optimal pulse sequences even in the presence of unmodeled physical effects, such as pulse errors or non-Gaussian noise.", "AI": {"tldr": "The paper introduces a reinforcement learning (RL)-based approach to design optimal dynamical decoupling pulse sequences for qubits, addressing challenges in mitigating phase decoherence under realistic noise.", "motivation": "Existing methods struggle to effectively determine optimal pulse timings for realistic noise spectra in qubits, a crucial task for minimizing phase decoherence.", "method": "The authors developed an RL framework with a novel action set derived from Thompson's group $F$, enabling efficient exploration of the optimization landscape for pulse sequence generation.", "result": "The RL agent successfully learned pulse sequences that minimize dephasing without needing direct information about the underlying noise spectrum, showing potential for real-time learning.", "conclusion": "This model-free RL approach may lead to optimal pulse sequences under realistic conditions, even accommodating unmodeled effects such as pulse errors or non-Gaussian noise."}}
{"id": "2512.14614", "pdf": "https://arxiv.org/pdf/2512.14614", "abs": "https://arxiv.org/abs/2512.14614", "authors": ["Wenqiang Sun", "Haiyu Zhang", "Haoyuan Wang", "Junta Wu", "Zehan Wang", "Zhenwei Wang", "Yunhong Wang", "Jun Zhang", "Tengfei Wang", "Chunchao Guo"], "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "categories": ["cs.CV", "cs.GR"], "comment": "project page: https://3d-models.hunyuan.tencent.com/world/, demo: https://3d.hunyuan.tencent.com/sceneTo3D", "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.", "AI": {"tldr": "WorldPlay is a streaming video diffusion model achieving real-time, interactive 720p video at 24 FPS with long-term geometric consistency, using a dual action representation, reconstituted context memory, and context forcing.", "motivation": "The motivation is to address the trade-off between speed and memory limitations in current streaming video modeling methods by ensuring long-term geometric consistency in real-time applications.", "method": "The paper proposes three key methods: Dual Action Representation for user input control, Reconstituted Context Memory for maintaining long-term consistency via dynamic context rebuilding and temporal reframing, and Context Forcing, a distillation method for memory-aware models to align context and prevent error drift.", "result": "WorldPlay demonstrates superior consistency and high performance in real-time streaming video at 720p resolution and 24 FPS with strong generalization across diverse scenes.", "conclusion": "WorldPlay effectively resolves speed and memory trade-offs in real-time geometry-consistent video generation while offering robust and generalizable performance."}}
{"id": "2512.14621", "pdf": "https://arxiv.org/pdf/2512.14621", "abs": "https://arxiv.org/abs/2512.14621", "authors": ["Zhenghao Zhao", "Haoxuan Wang", "Kai Wang", "Yuzhang Shang", "Yuan Hong", "Yan Yan"], "title": "Distill Video Datasets into Images", "categories": ["cs.CV"], "comment": null, "summary": "Dataset distillation aims to synthesize compact yet informative datasets that allow models trained on them to achieve performance comparable to training on the full dataset. While this approach has shown promising results for image data, extending dataset distillation methods to video data has proven challenging and often leads to suboptimal performance. In this work, we first identify the core challenge in video set distillation as the substantial increase in learnable parameters introduced by the temporal dimension of video, which complicates optimization and hinders convergence. To address this issue, we observe that a single frame is often sufficient to capture the discriminative semantics of a video. Leveraging this insight, we propose Single-Frame Video set Distillation (SFVD), a framework that distills videos into highly informative frames for each class. Using differentiable interpolation, these frames are transformed into video sequences and matched with the original dataset, while updates are restricted to the frames themselves for improved optimization efficiency. To further incorporate temporal information, the distilled frames are combined with sampled real videos from real videos during the matching process through a channel reshaping layer. Extensive experiments on multiple benchmarks demonstrate that SFVD substantially outperforms prior methods, achieving improvements of up to 5.3% on MiniUCF, thereby offering a more effective solution.", "AI": {"tldr": "The paper proposes a video dataset distillation method called SFVD, which distills videos into single, representative frames to optimize and improve efficiency, showing significant performance gains.", "motivation": "Current video dataset distillation methods face challenges due to the massive increase in parameters introduced by video temporal dimensions, leading to suboptimal results.", "method": "The proposed SFVD method focuses on distilling videos into highly informative single frames per class, transforms them into video sequences through differentiable interpolation, and integrates sampled real videos during this process for better temporal information incorporation.", "result": "SFVD surpasses prior video distillation methods with significant improvements, achieving a performance increase of up to 5.3% on MiniUCF.", "conclusion": "SFVD offers an effective and optimized approach to video dataset distillation by leveraging single frames and combining them with temporal information, addressing challenges in optimization and convergence."}}
{"id": "2512.14639", "pdf": "https://arxiv.org/pdf/2512.14639", "abs": "https://arxiv.org/abs/2512.14639", "authors": ["Fei Wu", "Marcel Dreier", "Nora Gourmelon", "Sebastian Wind", "Jianlin Zhang", "Thorsten Seehaus", "Matthias Braun", "Andreas Maier", "Vincent Christlein"], "title": "AMD-HookNet++: Evolution of AMD-HookNet with Hybrid CNN-Transformer Feature Enhancement for Glacier Calving Front Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The dynamics of glaciers and ice shelf fronts significantly impact the mass balance of ice sheets and coastal sea levels. To effectively monitor glacier conditions, it is crucial to consistently estimate positional shifts of glacier calving fronts. AMD-HookNet firstly introduces a pure two-branch convolutional neural network (CNN) for glacier segmentation. Yet, the local nature and translational invariance of convolution operations, while beneficial for capturing low-level details, restricts the model ability to maintain long-range dependencies. In this study, we propose AMD-HookNet++, a novel advanced hybrid CNN-Transformer feature enhancement method for segmenting glaciers and delineating calving fronts in synthetic aperture radar images. Our hybrid structure consists of two branches: a Transformer-based context branch to capture long-range dependencies, which provides global contextual information in a larger view, and a CNN-based target branch to preserve local details. To strengthen the representation of the connected hybrid features, we devise an enhanced spatial-channel attention module to foster interactions between the hybrid CNN-Transformer branches through dynamically adjusting the token relationships from both spatial and channel perspectives. Additionally, we develop a pixel-to-pixel contrastive deep supervision to optimize our hybrid model by integrating pixelwise metric learning into glacier segmentation. Through extensive experiments and comprehensive quantitative and qualitative analyses on the challenging glacier segmentation benchmark dataset CaFFe, we show that AMD-HookNet++ sets a new state of the art with an IoU of 78.2 and a HD95 of 1,318 m, while maintaining a competitive MDE of 367 m. More importantly, our hybrid model produces smoother delineations of calving fronts, resolving the issue of jagged edges typically seen in pure Transformer-based approaches.", "AI": {"tldr": "The paper introduces AMD-HookNet++, a hybrid CNN-Transformer model for improved glacier segmentation, achieving state-of-the-art results with smoother outputs.", "motivation": "Glaciers and ice shelf dynamics critically impact sea levels and ice sheet mass balance. Monitoring positional shifts in glacier calving fronts requires advanced segmentation techniques.", "method": "AMD-HookNet++ combines a CNN-based branch for capturing local details and a Transformer-based branch for long-range dependencies. Enhanced spatial-channel attention and pixel-to-pixel contrastive supervision are used to boost performance.", "result": "AMD-HookNet++ achieved state-of-the-art performance on glacier segmentation benchmarks, with a high IoU score of 78.2, reducing edge jaggedness seen in other models.", "conclusion": "The proposed hybrid model improves segmentation accuracy and smoothness in calving front delineation, addressing limitations of previous approaches."}}
{"id": "2512.14166", "pdf": "https://arxiv.org/pdf/2512.14166", "abs": "https://arxiv.org/abs/2512.14166", "authors": ["Yunhao Yao", "Zhiqiang Wang", "Haoran Cheng", "Yihang Cheng", "Haohua Du", "Xiang-Yang Li"], "title": "IntentMiner: Intent Inversion Attack via Tool Call Analysis in the Model Context Protocol", "categories": ["cs.CR", "cs.AI"], "comment": "12 pages, 6 figures", "summary": "The rapid evolution of Large Language Models (LLMs) into autonomous agents has led to the adoption of the Model Context Protocol (MCP) as a standard for discovering and invoking external tools. While this architecture decouples the reasoning engine from tool execution to enhance scalability, it introduces a significant privacy surface: third-party MCP servers, acting as semi-honest intermediaries, can observe detailed tool interaction logs outside the user's trusted boundary. In this paper, we first identify and formalize a novel privacy threat termed Intent Inversion, where a semi-honest MCP server attempts to reconstruct the user's private underlying intent solely by analyzing legitimate tool calls. To systematically assess this vulnerability, we propose IntentMiner, a framework that leverages Hierarchical Information Isolation and Three-Dimensional Semantic Analysis, integrating tool purpose, call statements, and returned results, to accurately infer user intent at the step level. Extensive experiments demonstrate that IntentMiner achieves a high degree of semantic alignment (over 85%) with original user queries, significantly outperforming baseline approaches. These results highlight the inherent privacy risks in decoupled agent architectures, revealing that seemingly benign tool execution logs can serve as a potent vector for exposing user secrets.", "AI": {"tldr": "The paper uncovers a privacy threat called Intent Inversion related to third-party tool interactions in autonomous agents using the Model Context Protocol (MCP). It introduces IntentMiner, which accurately infers user intent by analyzing tool usage patterns.", "motivation": "To address the privacy issues stemming from third-party MCP servers in large language models that can reconstruct user intent from log data.", "method": "Developed IntentMiner, a framework using Hierarchical Information Isolation and Three-Dimensional Semantic Analysis to assess privacy threats by analyzing tool purpose, call statements, and results.", "result": "IntentMiner achieved over 85% semantic accuracy in inferring user intent from tool logs, outperforming existing methods.", "conclusion": "The findings reveal significant privacy risks in agent architectures relying on MCP, emphasizing the need for robust protection measures in tool execution frameworks."}}
{"id": "2512.14640", "pdf": "https://arxiv.org/pdf/2512.14640", "abs": "https://arxiv.org/abs/2512.14640", "authors": ["Rao Muhammad Umer", "Daniel Sens", "Jonathan Noll", "Christian Matek", "Lukas Wolfseher", "Rainer Spang", "Ralf Huss", "Johannes Raffler", "Sarah Reinke", "Wolfram Klapper", "Katja Steiger", "Kristina Schwamborn", "Carsten Marr"], "title": "A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages", "summary": "Timely and accurate lymphoma diagnosis is essential for guiding cancer treatment. Standard diagnostic practice combines hematoxylin and eosin (HE)-stained whole slide images with immunohistochemistry, flow cytometry, and molecular genetic tests to determine lymphoma subtypes, a process requiring costly equipment, skilled personnel, and causing treatment delays. Deep learning methods could assist pathologists by extracting diagnostic information from routinely available HE-stained slides, yet comprehensive benchmarks for lymphoma subtyping on multicenter data are lacking. In this work, we present the first multicenter lymphoma benchmarking dataset covering four common lymphoma subtypes and healthy control tissue. We systematically evaluate five publicly available pathology foundation models (H-optimus-1, H0-mini, Virchow2, UNI2, Titan) combined with attention-based (AB-MIL) and transformer-based (TransMIL) multiple instance learning aggregators across three magnifications (10x, 20x, 40x). On in-distribution test sets, models achieve multiclass balanced accuracies exceeding 80% across all magnifications, with all foundation models performing similarly and both aggregation methods showing comparable results. The magnification study reveals that 40x resolution is sufficient, with no performance gains from higher resolutions or cross-magnification aggregation. However, on out-of-distribution test sets, performance drops substantially to around 60%, highlighting significant generalization challenges. To advance the field, larger multicenter studies covering additional rare lymphoma subtypes are needed. We provide an automated benchmarking pipeline to facilitate such future research.", "AI": {"tldr": "The paper introduces a multicenter benchmarking dataset and evaluates deep learning models for lymphoma subtyping from HE-stained slides, achieving high accuracy but facing generalization challenges.", "motivation": "To streamline lymphoma diagnosis by leveraging deep learning models and addressing the limitations of current methods requiring costly processes and skilled personnel.", "method": "The study uses a benchmarking dataset of four lymphoma subtypes and compares five pathology foundation models combined with attention-based and transformer-based MIL aggregators, across three magnification levels.", "result": "Models exceed 80% balanced accuracy on in-distribution data, with 40x magnification proving sufficient. However, performance decreases to around 60% on out-of-distribution datasets, indicating generalization limitations.", "conclusion": "The developed pipeline demonstrates potential for automation in lymphoma diagnosis but requires expanded datasets for better generalization and inclusion of rare subtypes."}}
{"id": "2512.14648", "pdf": "https://arxiv.org/pdf/2512.14648", "abs": "https://arxiv.org/abs/2512.14648", "authors": ["Daniel Capell\u00e1n-Mart\u00edn", "Abhijeet Parida", "Zhifan Jiang", "Nishad Kulkarni", "Krithika Iyer", "Austin Tapp", "Syed Muhammad Anwar", "Mar\u00eda J. Ledesma-Carbayo", "Marius George Linguraru"], "title": "Adaptable Segmentation Pipeline for Diverse Brain Tumors with Radiomic-guided Subtyping and Lesion-Wise Model Ensemble", "categories": ["cs.CV", "eess.IV"], "comment": "12 pages, 5 figures, 3 tables. Algorithm presented at MICCAI BraTS 2025", "summary": "Robust and generalizable segmentation of brain tumors on multi-parametric magnetic resonance imaging (MRI) remains difficult because tumor types differ widely. The BraTS 2025 Lighthouse Challenge benchmarks segmentation methods on diverse high-quality datasets of adult and pediatric tumors: multi-consortium international pediatric brain tumor segmentation (PED), preoperative meningioma tumor segmentation (MEN), meningioma radiotherapy segmentation (MEN-RT), and segmentation of pre- and post-treatment brain metastases (MET). We present a flexible, modular, and adaptable pipeline that improves segmentation performance by selecting and combining state-of-the-art models and applying tumor- and lesion-specific processing before and after training. Radiomic features extracted from MRI help detect tumor subtype, ensuring a more balanced training. Custom lesion-level performance metrics determine the influence of each model in the ensemble and optimize post-processing that further refines the predictions, enabling the workflow to tailor every step to each case. On the BraTS testing sets, our pipeline achieved performance comparable to top-ranked algorithms across multiple challenges. These findings confirm that custom lesion-aware processing and model selection yield robust segmentations yet without locking the method to a specific network architecture. Our method has the potential for quantitative tumor measurement in clinical practice, supporting diagnosis and prognosis.", "AI": {"tldr": "This paper proposes a modular segmentation pipeline for brain tumors in diverse MRI datasets, achieving competitive results without being tied to a specific model architecture.", "motivation": "To overcome challenges in brain tumor segmentation due to the wide variability in tumor types and improve generalizability across multiple datasets.", "method": "The pipeline combines state-of-the-art models with case-specific pre- and post-training processing, uses radiomic features for balanced training based on tumor subtype, and optimizes predictions through ensemble model performance metrics.", "result": "The pipeline's segmentation performance was comparable to top-ranked algorithms in various challenges, demonstrating robustness and adaptability.", "conclusion": "Custom lesion-aware processing and model selection enhance segmentation, providing a flexible tool for quantitative tumor assessment in clinical settings."}}
{"id": "2512.14181", "pdf": "https://arxiv.org/pdf/2512.14181", "abs": "https://arxiv.org/abs/2512.14181", "authors": ["Shaolun Ruan", "Feng Liang", "Rohan Ramakrishna", "Chao Ren", "Rudai Yan", "Qiang Guan", "Jiannan Li", "Yong Wang"], "title": "Towards Explainable Quantum AI: Informing the Encoder Selection of Quantum Neural Networks via Visualization", "categories": ["quant-ph", "cs.AI", "cs.HC"], "comment": "9 pages, 6 figures, accepted by TVCG 2026, not published yet", "summary": "Quantum Neural Networks (QNNs) represent a promising fusion of quantum computing and neural network architectures, offering speed-ups and efficient processing of high-dimensional, entangled data. A crucial component of QNNs is the encoder, which maps classical input data into quantum states. However, choosing suitable encoders remains a significant challenge, largely due to the lack of systematic guidance and the trial-and-error nature of current approaches. This process is further impeded by two key challenges: (1) the difficulty in evaluating encoded quantum states prior to training, and (2) the lack of intuitive methods for analyzing an encoder's ability to effectively distinguish data features. To address these issues, we introduce a novel visualization tool, XQAI-Eyes, which enables QNN developers to compare classical data features with their corresponding encoded quantum states and to examine the mixed quantum states across different classes. By bridging classical and quantum perspectives, XQAI-Eyes facilitates a deeper understanding of how encoders influence QNN performance. Evaluations across diverse datasets and encoder designs demonstrate XQAI-Eyes's potential to support the exploration of the relationship between encoder design and QNN effectiveness, offering a holistic and transparent approach to optimizing quantum encoders. Moreover, domain experts used XQAI-Eyes to derive two key practices for quantum encoder selection, grounded in the principles of pattern preservation and feature mapping.", "AI": {"tldr": "This paper introduces XQAI-Eyes, a visualization tool designed to evaluate the impact of quantum encoders in Quantum Neural Networks (QNNs).", "motivation": "To address challenges in selecting suitable quantum encoders for QNNs, especially the difficulty in evaluating encoded quantum states and analyzing their effectiveness in data feature distinction.", "method": "XQAI-Eyes visualizes classical data features, compares them with encoded quantum states, and examines mixed quantum states across classes.", "result": "The tool was tested across various datasets and encoder designs, helping to explore the relationship between encoder design and QNN effectiveness.", "conclusion": "XQAI-Eyes provides a transparent and holistic approach for optimizing QNN encoders and has led to the derivation of key practices for selecting them, focusing on pattern preservation and feature mapping."}}
{"id": "2512.14654", "pdf": "https://arxiv.org/pdf/2512.14654", "abs": "https://arxiv.org/abs/2512.14654", "authors": ["Lihong Wang", "Liangqi Li", "Weiwei Feng", "Jiamin Wu", "Changtao Miao", "Tieru Wu", "Rui Ma", "Bo Zhang", "Zhe Li"], "title": "ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking", "categories": ["cs.CV"], "comment": "Code is available at https://github.com/Leon-LihongWang/ViRC", "summary": "CoT has significantly enhanced the reasoning ability of LLMs while it faces challenges when extended to multimodal domains, particularly in mathematical tasks. Existing MLLMs typically perform textual reasoning solely from a single static mathematical image, overlooking dynamic visual acquisition during reasoning. In contrast, humans repeatedly examine visual image and employ step-by-step reasoning to prove intermediate propositions. This strategy of decomposing the problem-solving process into key logical nodes adheres to Miller's Law in cognitive science. Inspired by this insight, we propose a ViRC framework for multimodal mathematical tasks, introducing a Reason Chunking mechanism that structures multimodal mathematical CoT into consecutive Critical Reasoning Units (CRUs) to simulate human expert problem-solving patterns. CRUs ensure intra-unit textual coherence for intermediate proposition verification while integrating visual information across units to generate subsequent propositions and support structured reasoning. To this end, we present CRUX dataset by using three visual tools and four reasoning patterns to provide explicitly annotated CRUs across multiple reasoning paths for each mathematical problem. Leveraging the CRUX dataset, we propose a progressive training strategy inspired by human cognitive learning, which includes Instructional SFT, Practice SFT, and Strategic RL, aimed at further strengthening the Reason Chunking ability of the model.The resulting ViRC-7B model achieves a 18.8\\% average improvement over baselines across multiple mathematical benchmarks. Code is available at https://github.com/Leon-LihongWang/ViRC.", "AI": {"tldr": "The paper introduces ViRC, a framework for multimodal mathematical reasoning that employs Reason Chunking to enhance reasoning in Large Language Models (LLMs) by structuring reasoning into Critical Reasoning Units (CRUs).", "motivation": "The motivation is to address the challenge LLMs face in multimodal domains, particularly mathematical tasks, where current methods lack dynamic visual acquisition and structured reasoning capabilities.", "method": "The authors propose a Reason Chunking mechanism that structures multimodal reasoning into CRUs. The CRUX dataset was developed with visual and reasoning tools to train the ViRC-7B model using Instructional SFT, Practice SFT, and Strategic RL strategies.", "result": "The ViRC-7B model demonstrates an 18.8% improvement over existing baselines on multiple mathematical benchmarks.", "conclusion": "The ViRC framework, supported by the CRUX dataset and progressive training, effectively simulates human expert reasoning patterns, enhancing multimodal mathematical problem-solving."}}
{"id": "2512.14185", "pdf": "https://arxiv.org/pdf/2512.14185", "abs": "https://arxiv.org/abs/2512.14185", "authors": ["Emanuele Artioli", "Farzad Tashtarian", "Christian Timmerer"], "title": "End-to-End Learning-based Video Streaming Enhancement Pipeline: A Generative AI Approach", "categories": ["cs.MM", "cs.AI"], "comment": "The 35th edition of the Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV '25), March 31-April 4, 2025, Stellenbosch, South Africa", "summary": "The primary challenge of video streaming is to balance high video quality with smooth playback. Traditional codecs are well tuned for this trade-off, yet their inability to use context means they must encode the entire video data and transmit it to the client. This paper introduces ELVIS (End-to-end Learning-based VIdeo Streaming Enhancement Pipeline), an end-to-end architecture that combines server-side encoding optimizations with client-side generative in-painting to remove and reconstruct redundant video data. Its modular design allows ELVIS to integrate different codecs, inpainting models, and quality metrics, making it adaptable to future innovations. Our results show that current technologies achieve improvements of up to 11 VMAF points over baseline benchmarks, though challenges remain for real-time applications due to computational demands. ELVIS represents a foundational step toward incorporating generative AI into video streaming pipelines, enabling higher quality experiences without increased bandwidth requirements.", "AI": {"tldr": "This paper introduces ELVIS, an architecture using server-side optimizations and client-side generative in-painting to enhance video streaming efficiency and quality.", "motivation": "To address the challenge of balancing high-quality video streaming with smooth playback without increasing bandwidth requirements.", "method": "Developing the ELVIS pipeline, which combines server-side encoding optimizations with client-side generative in-painting, leveraging a modular framework adaptable to various codecs and models.", "result": "The ELVIS architecture shows performance improvements of up to 11 VMAF points over traditional benchmarks, though its computational demands pose challenges for real-time usage.", "conclusion": "ELVIS establishes a pathway for integrating generative AI into video streaming, achieving better quality without additional bandwidth but requires further optimization for real-time feasibility."}}
{"id": "2512.14665", "pdf": "https://arxiv.org/pdf/2512.14665", "abs": "https://arxiv.org/abs/2512.14665", "authors": ["Marco Blanchini", "Giovanna Maria Dimitri", "Benedetta Tondi", "Tarcisio Lancioni", "Mauro Barni"], "title": "Enhancing Visual Sentiment Analysis via Semiotic Isotopy-Guided Dataset Construction", "categories": ["cs.CV"], "comment": null, "summary": "Visual Sentiment Analysis (VSA) is a challenging task due to the vast diversity of emotionally salient images and the inherent difficulty of acquiring sufficient data to capture this variability comprehensively. Key obstacles include building large-scale VSA datasets and developing effective methodologies that enable algorithms to identify emotionally significant elements within an image. These challenges are reflected in the limited generalization performance of VSA algorithms and models when trained and tested across different datasets. Starting from a pool of existing data collections, our approach enables the creation of a new larger dataset that not only contains a wider variety of images than the original ones, but also permits training new models with improved capability to focus on emotionally relevant combinations of image elements. This is achieved through the integration of the semiotic isotopy concept within the dataset creation process, providing deeper insights into the emotional content of images. Empirical evaluations show that models trained on a dataset generated with our method consistently outperform those trained on the original data collections, achieving superior generalization across major VSA benchmarks", "AI": {"tldr": "The paper identifies challenges in visual sentiment analysis due to data insufficiency and proposes a novel dataset creation method using the semiotic isotopy concept to improve model performance.", "motivation": "To address the difficulty of analyzing emotionally salient images and the lack of generalized datasets in visual sentiment analysis.", "method": "The paper integrates the semiotic isotopy concept into the dataset creation process, generating a larger and more diverse dataset to capture emotional relevance in images.", "result": "Models trained on the newly generated dataset outperform those trained on existing datasets, showing better generalization across VSA benchmarks.", "conclusion": "The proposed dataset creation method enhances models' ability to focus on emotionally relevant image elements, improving their generalization and performance in visual sentiment analysis."}}
{"id": "2512.14671", "pdf": "https://arxiv.org/pdf/2512.14671", "abs": "https://arxiv.org/abs/2512.14671", "authors": ["Zizhang Li", "Cheng Zhang", "Zhengqin Li", "Henry Howard-Jenkins", "Zhaoyang Lv", "Chen Geng", "Jiajun Wu", "Richard Newcombe", "Jakob Engel", "Zhao Dong"], "title": "ART: Articulated Reconstruction Transformer", "categories": ["cs.CV"], "comment": "Project Page: https://kyleleey.github.io/ART/", "summary": "We introduce ART, Articulated Reconstruction Transformer -- a category-agnostic, feed-forward model that reconstructs complete 3D articulated objects from only sparse, multi-state RGB images. Previous methods for articulated object reconstruction either rely on slow optimization with fragile cross-state correspondences or use feed-forward models limited to specific object categories. In contrast, ART treats articulated objects as assemblies of rigid parts, formulating reconstruction as part-based prediction. Our newly designed transformer architecture maps sparse image inputs to a set of learnable part slots, from which ART jointly decodes unified representations for individual parts, including their 3D geometry, texture, and explicit articulation parameters. The resulting reconstructions are physically interpretable and readily exportable for simulation. Trained on a large-scale, diverse dataset with per-part supervision, and evaluated across diverse benchmarks, ART achieves significant improvements over existing baselines and establishes a new state of the art for articulated object reconstruction from image inputs.", "AI": {"tldr": "ART is a model for reconstructing 3D articulated objects from sparse RGB images, excelling in accuracy and applicability across object categories.", "motivation": "Previous approaches to reconstructing articulated objects lack efficiency or generalizability, prompting the need for a robust, category-agnostic method.", "method": "ART uses a feed-forward transformer architecture, mapping image inputs to part slots and decoding 3D geometry, texture, and articulation parameters.", "result": "ART establishes a new state of the art with significant improvements over existing baselines on diverse benchmarks.", "conclusion": "ART demonstrates the capability to reconstruct complete and physically interpretable 3D articulated objects from minimal input, advancing research in this field."}}
{"id": "2512.14211", "pdf": "https://arxiv.org/pdf/2512.14211", "abs": "https://arxiv.org/abs/2512.14211", "authors": ["Mengxue Zhang", "Qingrui Cai", "Yinyin Chen", "Hang Jin", "Jianjun Zhou", "Qiu Guo", "Peijun Zhao", "Zhiping Mao", "Xingxing Zhang", "Yuyu Xia", "Xianwang Jiang", "Qin Xu", "Chunyan Xiong", "Yirong Zhou", "Chengyan Wang", "Xiaobo Qu"], "title": "Error Bound Analysis of Physics-Informed Neural Networks-Driven T2 Quantification in Cardiac Magnetic Resonance Imaging", "categories": ["physics.bio-ph", "cs.AI"], "comment": null, "summary": "Physics-Informed Neural Networks (PINN) are emerging as a promising approach for quantitative parameter estimation of Magnetic Resonance Imaging (MRI). While existing deep learning methods can provide an accurate quantitative estimation of the T2 parameter, they still require large amounts of training data and lack theoretical support and a recognized gold standard. Thus, given the absence of PINN-based approaches for T2 estimation, we propose embedding the fundamental physics of MRI, the Bloch equation, in the loss of PINN, which is solely based on target scan data and does not require a pre-defined training database. Furthermore, by deriving rigorous upper bounds for both the T2 estimation error and the generalization error of the Bloch equation solution, we establish a theoretical foundation for evaluating the PINN's quantitative accuracy. Even without access to the ground truth or a gold standard, this theory enables us to estimate the error with respect to the real quantitative parameter T2. The accuracy of T2 mapping and the validity of the theoretical analysis are demonstrated on a numerical cardiac model and a water phantom, where our method exhibits excellent quantitative precision in the myocardial T2 range. Clinical applicability is confirmed in 94 acute myocardial infarction (AMI) patients, achieving low-error quantitative T2 estimation under the theoretical error bound, highlighting the robustness and potential of PINN.", "AI": {"tldr": "The paper proposes a Physics-Informed Neural Network (PINN) for T2 parameter estimation in MRI using the Bloch equation, addressing data limitations while providing theoretical error bounds for the method.", "motivation": "Existing methods for T2 estimation require extensive training data and lack theoretical support or recognized gold standard, prompting the need for a robust and theoretically sound approach.", "method": "The authors integrate the Bloch equation into the loss function of PINN, relying solely on target scan data without the need for pre-defined training datasets. They also derive upper bounds for T2 estimation and generalization errors.", "result": "The proposed method demonstrated high accuracy in T2 mapping on numerical models and a water phantom, and successfully tested on 94 AMI patients with low errors within the theoretical bounds.", "conclusion": "The study emphasizes the robustness and potential of PINN for quantitative T2 estimation in MRI, showcasing its clinical applicability and theoretical reliability."}}
{"id": "2512.14010", "pdf": "https://arxiv.org/pdf/2512.14010", "abs": "https://arxiv.org/abs/2512.14010", "authors": ["Che-Chia Chang", "Te-Sheng Lin", "Ming-Chih Lai"], "title": "Physics-Informed Machine Learning for Two-Phase Moving-Interface and Stefan Problems", "categories": ["physics.comp-ph", "cs.LG"], "comment": null, "summary": "The Stefan problem is a classical free-boundary problem that models phase-change processes and poses computational challenges due to its moving interface and nonlinear temperature-phase coupling. In this work, we develop a physics-informed neural network framework for solving two-phase Stefan problems. The proposed method explicitly tracks the interface motion and enforces the discontinuity in the temperature gradient across the interface while maintaining global consistency of the temperature field. Our approach employs two neural networks: one representing the moving interface and the other for the temperature field. The interface network allows rapid categorization of thermal diffusivity in the spatial domain, which is a crucial step for selecting training points for the temperature network. The temperature network's input is augmented with a modified zero-level set function to accurately capture the jump in its normal derivative across the interface. Numerical experiments on two-phase dynamical Stefan problems demonstrate the superior accuracy and effectiveness of our proposed method compared with the ones obtained by other neural network methodology in literature. The results indicate that the proposed framework offers a robust and flexible alternative to traditional numerical methods for solving phase-change problems governed by moving boundaries. In addition, the proposed method can capture an unstable interface evolution associated with the Mullins-Sekerka instability.", "AI": {"tldr": "The paper presents a physics-informed neural network (PINN) framework designed to solve two-phase Stefan problems, addressing computational challenges of moving interfaces and nonlinear coupling.", "motivation": "The study aims to develop a computational method to tackle the complexities of phase-change processes, primarily associated with the Stefan problem, which involves moving interfaces and phase coupling.", "method": "The authors propose using two neural networks: one to represent the moving interface and the other for the temperature field. The design explicitly tracks interface motion and ensures discontinuity enforcement while maintaining global temperature consistency.", "result": "Numerical experiments showcase the method's superior accuracy and flexibility in solving Stefan problems compared to traditional neural network approaches. It even captures unstable interface evolution, such as Mullins-Sekerka instability.", "conclusion": "The proposed PINN framework is a robust and flexible alternative to conventional approaches for solving moving boundary phase-change problems."}}
{"id": "2512.14677", "pdf": "https://arxiv.org/pdf/2512.14677", "abs": "https://arxiv.org/abs/2512.14677", "authors": ["Sicheng Xu", "Guojun Chen", "Jiaolong Yang", "Yizhong Zhang", "Yu Deng", "Steve Lin", "Baining Guo"], "title": "VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image", "categories": ["cs.CV", "cs.AI"], "comment": "NeurIPS 2025 paper. Project webpage: https://www.microsoft.com/en-us/research/project/vasa-3d/", "summary": "We propose VASA-3D, an audio-driven, single-shot 3D head avatar generator. This research tackles two major challenges: capturing the subtle expression details present in real human faces, and reconstructing an intricate 3D head avatar from a single portrait image. To accurately model expression details, VASA-3D leverages the motion latent of VASA-1, a method that yields exceptional realism and vividness in 2D talking heads. A critical element of our work is translating this motion latent to 3D, which is accomplished by devising a 3D head model that is conditioned on the motion latent. Customization of this model to a single image is achieved through an optimization framework that employs numerous video frames of the reference head synthesized from the input image. The optimization takes various training losses robust to artifacts and limited pose coverage in the generated training data. Our experiment shows that VASA-3D produces realistic 3D talking heads that cannot be achieved by prior art, and it supports the online generation of 512x512 free-viewpoint videos at up to 75 FPS, facilitating more immersive engagements with lifelike 3D avatars.", "AI": {"tldr": "The paper introduces VASA-3D, a method for generating highly realistic 3D head avatars from a single image driven by audio inputs.", "motivation": "The motivation is to enhance the realism of 3D talking avatars by capturing subtle facial expression details and enabling the reconstruction of intricate 3D head models from single portrait images.", "method": "VASA-3D builds on the motion latent of VASA-1, adapting it for 3D with a conditioned 3D head model. This involves optimization using synthesized video frames for training with robust loss functions to handle artifacts and limited pose variability.", "result": "Experiments demonstrate that VASA-3D outperforms previous methods, generating lifelike 3D avatars and enabling online 512x512 free-viewpoint videos at up to 75 FPS.", "conclusion": "VASA-3D facilitates the creation of immersive 3D talking head avatars, advancing the state of the art in terms of both realism and computational efficiency."}}
{"id": "2512.14692", "pdf": "https://arxiv.org/pdf/2512.14692", "abs": "https://arxiv.org/abs/2512.14692", "authors": ["Jianfeng Xiang", "Xiaoxue Chen", "Sicheng Xu", "Ruicheng Wang", "Zelong Lv", "Yu Deng", "Hongyuan Zhu", "Yue Dong", "Hao Zhao", "Nicholas Jing Yuan", "Jiaolong Yang"], "title": "Native and Compact Structured Latents for 3D Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://microsoft.github.io/TRELLIS.2/", "summary": "Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.", "AI": {"tldr": "This paper introduces an omni-voxel (O-Voxel) representation for 3D generative modeling, enabling effective capture of complex topology and appearance details.", "motivation": "Existing 3D generative models struggle with accurately representing complex topologies and detailed appearances.", "method": "The paper presents O-Voxel, a sparse voxel structure encoding geometry and appearance, alongside a Sparse Compression VAE to improve spatial compression and latent space management. Large-scale flow-matching models are trained with 4B parameters using diverse datasets.", "result": "The proposed approach offers significantly improved geometry and material quality in 3D asset generation, surpassing existing models.", "conclusion": "O-Voxel and the Sparse Compression VAE represent a substantial improvement in the field of 3D generative modeling, providing enhanced realism and efficiency."}}
{"id": "2512.14697", "pdf": "https://arxiv.org/pdf/2512.14697", "abs": "https://arxiv.org/abs/2512.14697", "authors": ["Yue Zhao", "Hanwen Jiang", "Zhenlin Xu", "Chutong Yang", "Ehsan Adeli", "Philipp Kr\u00e4henb\u00fchl"], "title": "Spherical Leech Quantization for Visual Tokenization and Generation", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.SP"], "comment": "Tech report; project page: https://zhaoyue-zephyrus.github.io/npq/", "summary": "Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization ($\u039b_{24}$-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.", "AI": {"tldr": "This paper introduces a lattice-based perspective for non-parametric quantization, which improves training efficiency and compression quality.", "motivation": "To address the need for efficient non-parametric quantization with scalability and improve compression and reconstruction quality in tasks like image tokenization.", "method": "The paper investigates various lattice-based quantization methods (e.g., random lattices, Fibonacci lattices, Leech lattices) and formulates a novel approach based on the high-symmetry Leech lattice ($\u039b_{24}$-SQ).", "result": "The method achieves enhanced reconstruction quality and better compression efficiency compared to prior methods (e.g., BSQ) in tasks like image tokenization, compression, and auto-regressive image generation.", "conclusion": "Using the Leech lattice-based quantization simplifies training and improves both compression and reconstruction performance, outperforming previous methods."}}
{"id": "2512.14115", "pdf": "https://arxiv.org/pdf/2512.14115", "abs": "https://arxiv.org/abs/2512.14115", "authors": ["Ramesh Gundluru", "Shubham Gupta", "Sri Rama Murty K"], "title": "Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and Keyword Spotting", "categories": ["cs.SD", "cs.LG"], "comment": null, "summary": "Acoustic Word Embeddings (AWEs) improve the efficiency of speech retrieval tasks such as Spoken Term Detection (STD) and Keyword Spotting (KWS). However, existing approaches suffer from limitations, including unimodal supervision, disjoint optimization of audio-audio and audio-text alignment, and the need for task-specific models. To address these shortcomings, we propose a joint multimodal contrastive learning framework that unifies both acoustic and cross-modal supervision in a shared embedding space. Our approach simultaneously optimizes: (i) audio-text contrastive learning, inspired by the CLAP loss, to align audio and text representations and (ii) audio-audio contrastive learning, via Deep Word Discrimination (DWD) loss, to enhance intra-class compactness and inter-class separation. The proposed method outperforms existing AWE baselines on word discrimination task while flexibly supporting both STD and KWS. To our knowledge, this is the first comprehensive approach of its kind.", "AI": {"tldr": "The paper proposes a joint multimodal contrastive learning framework to improve Acoustic Word Embeddings (AWEs), optimizing audio-text and audio-audio contrastive learning to outperform existing baselines in word discrimination tasks and support various speech retrieval tasks.", "motivation": "Existing approaches to Acoustic Word Embeddings face several limitations such as unimodal supervision, separate optimization processes, and a lack of versatility for different tasks.", "method": "The authors introduce a joint multimodal contrastive learning framework that integrates acoustic and cross-modal supervision in a shared embedding space. It employs CLAP loss for audio-text alignment and Deep Word Discrimination loss for better intra-class compactness and inter-class separation.", "result": "The proposed method achieves superior performance on the word discrimination task compared to existing AWE baselines, enhancing its applications in both Spoken Term Detection and Keyword Spotting.", "conclusion": "This innovative, flexible, and comprehensive approach advances the efficiency and utility of AWEs in speech retrieval tasks, representing a significant improvement over task-specific methods."}}
{"id": "2512.14699", "pdf": "https://arxiv.org/pdf/2512.14699", "abs": "https://arxiv.org/abs/2512.14699", "authors": ["Sihui Ji", "Xi Chen", "Shuai Yang", "Xin Tao", "Pengfei Wan", "Hengshuang Zhao"], "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives", "categories": ["cs.CV"], "comment": "Project Page: https://sihuiji.github.io/MemFlow.github.io/", "summary": "The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.", "AI": {"tldr": "MemFlow improves content consistency in streaming video generation by dynamically updating memory with text-driven retrieval of relevant historical frames, ensuring narrative coherence despite changes in events or scenarios.", "motivation": "Streaming video generation requires maintaining long-context consistency, which is challenging with fixed memory strategies that fail to adapt to varying historical cues needed for different video chunks.", "method": "MemFlow dynamically updates a memory bank by retrieving the most relevant historical frames using the text prompts of future chunks and activates only relevant tokens in attention layers during generation.", "result": "MemFlow maintains narrative coherence and achieves high long-context consistency with only a 7.9% speed reduction while being compatible with models using KV cache.", "conclusion": "This approach ensures effective video generation with minimal computation cost and high adaptability, addressing the limitations of fixed memory strategies."}}
{"id": "2512.06112", "pdf": "https://arxiv.org/pdf/2512.06112", "abs": "https://arxiv.org/abs/2512.06112", "authors": ["Yifang Xu", "Jiahao Cui", "Feipeng Cai", "Zhihao Zhu", "Hanlin Shang", "Shan Luan", "Mingwang Xu", "Neng Zhang", "Yaoyi Li", "Jia Cai", "Siyu Zhu"], "title": "WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "18 pages, 11 figures. Code & Model: https://github.com/fudan-generative-vision/WAM-Flow", "summary": "We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.", "AI": {"tldr": "This paper proposes WAM-Flow, a vision-language-action (VLA) model for autonomous driving that uses discrete flow matching instead of autoregressive decoding to plan ego-trajectories.", "motivation": "To improve efficiency and performance in ego-trajectory planning for autonomous driving, overcoming the limitations of autoregressive decoders and enabling parallel generation with a balance between compute and accuracy.", "method": "The method integrates a metric-aligned numerical tokenizer, a geometry-aware flow objective, and a simulator-guided GRPO alignment while converting an autoregressive backbone to a non-causal flow model through multi-stage adaptation and multimodal pretraining.", "result": "WAM-Flow achieves better closed-loop performance compared to autoregressive and diffusion-based models, with 89.1 PDMS for 1-step inference and 90.3 PDMS for 5-step inference on the NAVSIM v1 benchmark.", "conclusion": "Discrete flow matching presents a promising approach for end-to-end autonomous driving by enabling efficient, parallel, and high-performing trajectory planning, showcasing its potential as an alternative to existing paradigms."}}
{"id": "2512.14278", "pdf": "https://arxiv.org/pdf/2512.14278", "abs": "https://arxiv.org/abs/2512.14278", "authors": ["Marvin Kopka", "Azeem Majeed", "Gabriella Spinelli", "Austen El-Osta", "Markus Feufel"], "title": "The Trust in AI-Generated Health Advice (TAIGHA) Scale and Short Version (TAIGHA-S): Development and Validation Study", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Artificial Intelligence tools such as large language models are increasingly used by the public to obtain health information and guidance. In health-related contexts, following or rejecting AI-generated advice can have direct clinical implications. Existing instruments like the Trust in Automated Systems Survey assess trustworthiness of generic technology, and no validated instrument measures users' trust in AI-generated health advice specifically. This study developed and validated the Trust in AI-Generated Health Advice (TAIGHA) scale and its four-item short form (TAIGHA-S) as theory-based instruments measuring trust and distrust, each with cognitive and affective components. The items were developed using a generative AI approach, followed by content validation with 10 domain experts, face validation with 30 lay participants, and psychometric validation with 385 UK participants who received AI-generated advice in a symptom-assessment scenario. After automated item reduction, 28 items were retained and reduced to 10 based on expert ratings. TAIGHA showed excellent content validity (S-CVI/Ave=0.99) and CFA confirmed a two-factor model with excellent fit (CFI=0.98, TLI=0.98, RMSEA=0.07, SRMR=0.03). Internal consistency was high (\u03b1=0.95). Convergent validity was supported by correlations with the Trust in Automated Systems Survey (r=0.67/-0.66) and users' reliance on the AI's advice (r=0.37 for trust), while divergent validity was supported by low correlations with reading flow and mental load (all |r|<0.25). TAIGHA-S correlated highly with the full scale (r=0.96) and showed good reliability (\u03b1=0.88). TAIGHA and TAIGHA-S are validated instruments for assessing user trust and distrust in AI-generated health advice. Reporting trust and distrust separately permits a more complete evaluation of AI interventions, and the short scale is well-suited for time-constrained settings.", "AI": {"tldr": "This paper presents the development and validation of the Trust in AI-Generated Health Advice (TAIGHA) scale and its short form (TAIGHA-S), instruments for measuring trust and distrust in AI health advice.", "motivation": "The growing use of AI tools for health guidance raises the need for specific mechanisms to evaluate user trust in AI-generated health advice, given the lack of validated instruments in this domain.", "method": "The study developed TAIGHA with items generated via AI, validated with domain experts, lay participants, and psychometric techniques. It underwent automated item reduction and statistical tests for reliability and validity.", "result": "TAIGHA demonstrated excellent content validity, strong internal consistency, and validity assessments via correlations with existing surveys and user reliance. TAIGHA-S retains high correlation with the full scale and is reliable.", "conclusion": "TAIGHA and TAIGHA-S are validated tools for evaluating trust and distrust in AI-generated health advice, offering practical use in both comprehensive and time-constrained evaluations."}}
{"id": "2512.11872", "pdf": "https://arxiv.org/pdf/2512.11872", "abs": "https://arxiv.org/abs/2512.11872", "authors": ["Mingwang Xu", "Jiahao Cui", "Feipeng Cai", "Hanlin Shang", "Zhihao Zhu", "Shan Luan", "Yifang Xu", "Neng Zhang", "Yaoyi Li", "Jia Cai", "Siyu Zhu"], "title": "WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff", "AI": {"tldr": "This paper introduces WAM-Diff, a model for autonomous driving using masked diffusion for generating ego-trajectories, achieving state-of-the-art performance on NAVSIM datasets.", "motivation": "To explore the potential of discrete masked diffusion in autonomous driving and propose an alternative to prevalent autoregressive and continuous diffusion policies.", "method": "The paper employs masked diffusion to iteratively refine discretized sequences of future trajectories, scalable sparse MoE architectures, and reinforcement learning using GSPO to optimize driving rewards.", "result": "WAM-Diff achieves high accuracy scores of 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, showcasing its efficacy in trajectory generation.", "conclusion": "Masked diffusion is a promising strategy for trajectory generation in autonomous driving systems, allowing scenario-aware and effective decoding strategies."}}
{"id": "2512.14297", "pdf": "https://arxiv.org/pdf/2512.14297", "abs": "https://arxiv.org/abs/2512.14297", "authors": ["Agrippina Mwangi", "Le\u00f3n Navarro-Hilfiker", "Lukasz Brewka", "Mikkel Gryning", "Elena Fumagalli", "Madeleine Gibescu"], "title": "A Threshold-Triggered Deep Q-Network-Based Framework for Self-Healing in Autonomic Software-Defined IIoT-Edge Networks", "categories": ["cs.NI", "cs.AI", "cs.ET", "cs.PF", "hep-ex"], "comment": null, "summary": "Stochastic disruptions such as flash events arising from benign traffic bursts and switch thermal fluctuations are major contributors to intermittent service degradation in software-defined industrial networks. These events violate IEC~61850-derived quality-of-service requirements and user-defined service-level agreements, hindering the reliable and timely delivery of control, monitoring, and best-effort traffic in IEC~61400-25-compliant wind power plants. Failure to maintain these requirements often results in delayed or lost control signals, reduced operational efficiency, and increased risk of wind turbine generator downtime.\n  To address these challenges, this study proposes a threshold-triggered Deep Q-Network self-healing agent that autonomically detects, analyzes, and mitigates network disruptions while adapting routing behavior and resource allocation in real time. The proposed agent was trained, validated, and tested on an emulated tri-clustered switch network deployed in a cloud-based proof-of-concept testbed.\n  Simulation results show that the proposed agent improves disruption recovery performance by 53.84% compared to a baseline shortest-path and load-balanced routing approach and outperforms state-of-the-art methods, including the Adaptive Network-based Fuzzy Inference System by 13.1% and the Deep Q-Network and traffic prediction-based routing optimization method by 21.5%, in a super-spine leaf data-plane architecture.\n  Additionally, the agent maintains switch thermal stability by proactively initiating external rack cooling when required. These findings highlight the potential of deep reinforcement learning in building resilience in software-defined industrial networks deployed in mission-critical, time-sensitive application scenarios.", "AI": {"tldr": "This paper proposes a threshold-triggered Deep Q-Network self-healing agent for addressing stochastic disruptions in software-defined industrial networks, showing significant improvement in performance and resilience.", "motivation": "To tackle stochastic disruptions like traffic bursts and thermal fluctuations in industrial software-defined networks that degrade service quality, delay control signals, and hinder operational efficiency, particularly in IEC 61400-25-compliant wind power plants.", "method": "The study employs a Deep Q-Network self-healing agent capable of detecting, analyzing, and mitigating disruptions in real-time through adaptive routing and resource allocation, tested on an emulated tri-clustered switch network in a cloud-based testbed.", "result": "The proposed solution improves disruption recovery performance by 53.84% compared to baseline methods and outperforms state-of-the-art approaches by 13.1% to 21.5% in specific data-plane architectures while maintaining thermal stability of switches.", "conclusion": "Deep reinforcement learning methods, like the proposed agent, demonstrate significant potential to enhance resilience and performance in mission-critical, time-sensitive industrial networks."}}
{"id": "2512.14329", "pdf": "https://arxiv.org/pdf/2512.14329", "abs": "https://arxiv.org/abs/2512.14329", "authors": ["Yanning Dai", "Chenyu Tang", "Ruizhi Zhang", "Wenyu Yang", "Yilan Zhang", "Yuhui Wang", "Junliang Chen", "Xuhang Chen", "Ruimou Xie", "Yangyue Cao", "Qiaoying Li", "Jin Cao", "Tao Li", "Hubin Zhao", "Yu Pan", "Arokia Nathan", "Xin Gao", "Peter Smielewski", "Shuo Gao"], "title": "A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data", "categories": ["cs.CE", "cs.AI"], "comment": "26 pages, 6 figures", "summary": "Dynamic prediction of locomotor capacity after stroke is crucial for tailoring rehabilitation, yet current assessments provide only static impairment scores and do not indicate whether patients can safely perform specific tasks such as slope walking or stair climbing. Here, we develop a data-physics hybrid generative framework that reconstructs an individual stroke survivor's neuromuscular control from a single 20 m level-ground walking trial and predicts task-conditioned locomotion across rehabilitation scenarios. The system combines wearable-sensor kinematics, a proportional-derivative physics controller, a population Healthy Motion Atlas, and goal-conditioned deep reinforcement learning with behaviour cloning and generative adversarial imitation learning to generate physically plausible, patient-specific gait simulations for slopes and stairs. In 11 stroke survivors, the personalized controllers preserved idiosyncratic gait patterns while improving joint-angle and endpoint fidelity by 4.73% and 12.10%, respectively, and reducing training time to 25.56% relative to a physics-only baseline. In a multicentre pilot involving 21 inpatients, clinicians who used our locomotion predictions to guide task selection and difficulty obtained larger gains in Fugl-Meyer lower-extremity scores over 28 days of standard rehabilitation than control clinicians (mean change 6.0 versus 3.7 points). These findings indicate that our generative, task-predictive framework can augment clinical decision-making in post-stroke gait rehabilitation and provide a template for dynamically personalized motor recovery strategies.", "AI": {"tldr": "This paper proposes a generative framework for predicting stroke survivors' locomotor capacity and personalizing rehabilitation using wearable sensors, physics-based models, and deep learning, improving clinical outcomes in motor recovery.", "motivation": "Current stroke rehabilitation assessments provide only static impairment scores, lacking the ability to predict task-specific motor performance, which limits tailoring patient rehabilitation effectively.", "method": "The study integrates wearable sensors, a physics controller, a Healthy Motion Atlas, and deep reinforcement learning (with behavior cloning and generative adversarial imitation learning) to model and predict personalized gait simulations for tasks like slope walking and stair climbing.", "result": "The personalized framework preserved patients\u2019 unique gait patterns, improved accuracy by 4.73%-12.10%, reduced training time by 74.44% compared to a physics-only model, and enabled clinicians to achieve better rehabilitation outcomes in pilot tests with stroke survivors.", "conclusion": "This framework enhances clinical decision-making in post-stroke gait rehabilitation and offers a basis for dynamically personalized motor recovery strategies by effectively predicting patient-specific locomotor abilities."}}
{"id": "2512.14330", "pdf": "https://arxiv.org/pdf/2512.14330", "abs": "https://arxiv.org/abs/2512.14330", "authors": ["Sahibpreet Singh", "Manjit Singh"], "title": "Criminal Liability in AI-Enabled Autonomous Vehicles: A Comparative Study", "categories": ["cs.CY", "cs.AI", "cs.CR"], "comment": "Published in Journal of University Institute of Legal Studies, Vol. 18, Issue 1, pp. 57-78, 2025", "summary": "AI revolutionizes transportation through autonomous vehicles (AVs) but introduces complex criminal liability issues regarding infractions. This study employs a comparative legal analysis of primary statutes, real-world liability claims, and academic literature across the US, Germany, UK, China, and India; jurisdictions selected for their technological advancement and contrasting regulatory approaches. The research examines the attribution of human error, AI moral agency, and the identification of primary offenders in AV incidents. Findings reveal fragmented regulatory landscapes: India and the US rely on loose networks of state laws, whereas the UK enacted the pioneering Automated and Electric Vehicles Act 2018. Germany enforces strict safety standards, distinguishing liability based on the vehicle's operating mode, while China similarly aims for a stringent liability regime. The study concludes that globally harmonized legal standards are essential to foster technological innovation while ensuring minimum risk and clear liability attribution.", "AI": {"tldr": "The paper addresses criminal liability challenges arising from autonomous vehicles (AVs) and analyzes legal frameworks across five jurisdictions, identifying fragmented approaches.", "motivation": "The motivation is to tackle the legal complexities introduced by AVs, particularly in attributing responsibility for infractions and ensuring technological progress without compromising safety.", "method": "It uses comparative legal analysis by studying laws, liability claims, and literature from the US, Germany, UK, China, and India, focusing on AV-related liability issues.", "result": "The findings highlight disparities in regulatory frameworks: fragmented state laws in the US and India, the UK's lead with the Automated and Electric Vehicles Act 2018, Germany's liability provisions based on operating modes, and China's stringent approach.", "conclusion": "The study concludes that a harmonized global legal framework is crucial for innovation and clear liability determination in the AV industry."}}
{"id": "2512.14410", "pdf": "https://arxiv.org/pdf/2512.14410", "abs": "https://arxiv.org/abs/2512.14410", "authors": ["Muhammad Sukri Bin Ramli"], "title": "Pattern Recognition of Aluminium Arbitrage in Global Trade Data", "categories": ["econ.GN", "cs.LG"], "comment": null, "summary": "As the global economy transitions toward decarbonization, the aluminium sector has become a focal point for strategic resource management. While policies such as the Carbon Border Adjustment Mechanism (CBAM) aim to reduce emissions, they have inadvertently widened the price arbitrage between primary metal, scrap, and semi-finished goods, creating new incentives for market optimization. This study presents a unified, unsupervised machine learning framework to detect and classify emerging trade anomalies within UN Comtrade data (2020 to 2024). Moving beyond traditional rule-based monitoring, we apply a four-layer analytical pipeline utilizing Forensic Statistics, Isolation Forests, Network Science, and Deep Autoencoders. Contrary to the hypothesis that Sustainability Arbitrage would be the primary driver, empirical results reveal a contradictory and more severe phenomenon of Hardware Masking. Illicit actors exploit bi-directional tariff incentives by misclassifying scrap as high-count heterogeneous goods to justify extreme unit-price outliers of >$160/kg, a 1,900% markup indicative of Trade-Based Money Laundering (TBML) rather than commercial arbitrage. Topologically, risk is not concentrated in major exporters but in high-centrality Shadow Hubs that function as pivotal nodes for illicit rerouting. These actors execute a strategy of Void-Shoring, systematically suppressing destination data to Unspecified Code to fracture mirror statistics and sever forensic trails. Validated by SHAP (Shapley Additive Explanations), the results confirm that price deviation is the dominant predictor of anomalies, necessitating a paradigm shift in customs enforcement from physical volume checks to dynamic, algorithmic valuation auditing.", "AI": {"tldr": "The study leverages machine learning to uncover trade anomalies in the aluminium sector, identifying illicit activities like Trade-Based Money Laundering through misclassification and exploiting tariff incentives.", "motivation": "To address the unintended effects of policies like CBAM and identify anomalies in global aluminium trade to prevent illicit activities and optimize market strategies.", "method": "The study uses a four-layer unsupervised machine learning framework, including Forensic Statistics, Isolation Forests, Network Science, and Deep Autoencoders, to analyze trade data.", "result": "The research discovered illicit trade patterns, including Hardware Masking and Void-Shoring, with key findings validated using Shapley Additive Explanations for anomaly prediction.", "conclusion": "The study suggests shifting customs enforcement from traditional methods to algorithmic valuation auditing to address trade-based fraud and improve anomaly detection."}}
{"id": "2512.14422", "pdf": "https://arxiv.org/pdf/2512.14422", "abs": "https://arxiv.org/abs/2512.14422", "authors": ["Waqas Ahmed"], "title": "Hybrid Ensemble Method for Detecting Cyber-Attacks in Water Distribution Systems Using the BATADAL Dataset", "categories": ["cs.CR", "cs.LG"], "comment": "18 pages, & figures", "summary": "The cybersecurity of Industrial Control Systems that manage critical infrastructure such as Water Distribution Systems has become increasingly important as digital connectivity expands. BATADAL benchmark data is a good source of testing intrusion detection techniques, but it presents several important problems, such as imbalance in the number of classes, multivariate time dependence, and stealthy attacks. We consider a hybrid ensemble learning model that will enhance the detection ability of cyber-attacks in WDS by using the complementary capabilities of machine learning and deep learning models. Three base learners, namely, Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network, have been strictly compared and seven ensemble types using simple averaged and stacked learning with a logistic regression meta-learner. Random Forest analysis identified top predictors turned into temporal and statistical features, and Synthetic Minority Oversampling Technique (SMOTE) was used to overcome the class imbalance issue. The analyics indicates that the single Long Short-Term Memory network model is of poor performance (F1 = 0.000, AUC = 0.4460), but tree-based models, especially eXtreme Gradient Boosting, perform well (F1 = 0.7470, AUC=0.9684). The hybrid stacked ensemble of Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network scored the highest, with the attack class of 0.7205 with an F1-score and a AUC of 0.9826 indicating that the heterogeneous stacking between model precision and generalization can work. The proposed framework establishes a robust and scalable solution for cyber-attack detection in time-dependent industrial systems, integrating temporal learning and ensemble diversity to support the secure operation of critical infrastructure.", "AI": {"tldr": "This paper proposes a hybrid ensemble learning model to improve detection of cyber-attacks on Water Distribution Systems, addressing issues like data imbalance and stealthy attacks using BATADAL dataset and advanced techniques.", "motivation": "To enhance the cybersecurity of Industrial Control Systems, especially for Water Distribution Systems, by effectively detecting cyber-attacks despite challenges like class imbalance and multivariate dependencies in data.", "method": "A hybrid ensemble learning approach combining Random Forest, eXtreme Gradient Boosting, and Long Short-Term Memory networks, using features engineering, SMOTE for class imbalance, and ensemble techniques like stacking with logistic regression meta-learner.", "result": "The stacked ensemble combining tree-based models and LSTM achieved the best results (F1-score of 0.7205 and AUC of 0.9826) by leveraging model precision and generalization for cyber-attack detection.", "conclusion": "The proposed hybrid ensemble framework is scalable and robust for detecting cyber-attacks in industrial systems, combining temporal learning and diverse models for improved security of critical infrastructure."}}
{"id": "2512.14187", "pdf": "https://arxiv.org/pdf/2512.14187", "abs": "https://arxiv.org/abs/2512.14187", "authors": ["Jianwei Sun", "Xiaoning Lei", "Wenhao Cai", "Xichen Xu", "Yanshu Wang", "Hu Gao"], "title": "Establishing Stochastic Object Models from Noisy Data via Ambient Measurement-Integrated Diffusion", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Task-based measures of image quality (IQ) are critical for evaluating medical imaging systems, which must account for randomness including anatomical variability. Stochastic object models (SOMs) provide a statistical description of such variability, but conventional mathematical SOMs fail to capture realistic anatomy, while data-driven approaches typically require clean data rarely available in clinical tasks. To address this challenge, we propose AMID, an unsupervised Ambient Measurement-Integrated Diffusion with noise decoupling, which establishes clean SOMs directly from noisy measurements. AMID introduces a measurement-integrated strategy aligning measurement noise with the diffusion trajectory, and explicitly models coupling between measurement and diffusion noise across steps, an ambient loss is thus designed base on it to learn clean SOMs. Experiments on real CT and mammography datasets show that AMID outperforms existing methods in generation fidelity and yields more reliable task-based IQ evaluation, demonstrating its potential for unsupervised medical imaging analysis.", "AI": {"tldr": "AMID, a noise-decoupling diffusion model, creates clean stochastic object models (SOMs) from noisy medical imaging data to improve task-based image quality evaluation.", "motivation": "Current methods for task-based image quality evaluation struggle due to the randomness introduced by anatomical variability and the lack of clean clinical data. Stochastic object models are used but often fail to realistically represent anatomy or depend on unavailable data.", "method": "The paper introduces AMID, an unsupervised noise-decoupling method combining measurement noise integration with diffusion trajectories. It also models the coupling of measurement and diffusion noise, using an ambient loss strategy to learn clean SOMs.", "result": "AMID exhibits superior performance over existing methods in creating realistic generative SOMs and enabling accurate and reliable image quality evaluations on real CT and mammography datasets.", "conclusion": "AMID effectively addresses the challenge of noisy medical image data, providing a reliable tool for image quality evaluation and demonstrating applications in unsupervised medical imaging analysis."}}
{"id": "2512.14439", "pdf": "https://arxiv.org/pdf/2512.14439", "abs": "https://arxiv.org/abs/2512.14439", "authors": ["Quan Yuan", "Zhikun Zhang", "Linkang Du", "Min Chen", "Mingyang Sun", "Yunjun Gao", "Shibo He", "Jiming Chen"], "title": "VICTOR: Dataset Copyright Auditing in Video Recognition Systems", "categories": ["cs.CR", "cs.CV"], "comment": "To appear in the NDSS Symposium 2026, February 2026, San Diego, CA, USA", "summary": "Video recognition systems are increasingly being deployed in daily life, such as content recommendation and security monitoring. To enhance video recognition development, many institutions have released high-quality public datasets with open-source licenses for training advanced models. At the same time, these datasets are also susceptible to misuse and infringement. Dataset copyright auditing is an effective solution to identify such unauthorized use. However, existing dataset copyright solutions primarily focus on the image domain; the complex nature of video data leaves dataset copyright auditing in the video domain unexplored. Specifically, video data introduces an additional temporal dimension, which poses significant challenges to the effectiveness and stealthiness of existing methods.\n  In this paper, we propose VICTOR, the first dataset copyright auditing approach for video recognition systems. We develop a general and stealthy sample modification strategy that enhances the output discrepancy of the target model. By modifying only a small proportion of samples (e.g., 1%), VICTOR amplifies the impact of published modified samples on the prediction behavior of the target models. Then, the difference in the model's behavior for published modified and unpublished original samples can serve as a key basis for dataset auditing. Extensive experiments on multiple models and datasets highlight the superiority of VICTOR. Finally, we show that VICTOR is robust in the presence of several perturbation mechanisms to the training videos or the target models.", "AI": {"tldr": "The paper presents VICTOR, the first video dataset copyright auditing method, addressing challenges posed by the temporal complexity of video data.", "motivation": "There is growing concern regarding the misuse and infringement of open-access video datasets used to train advanced models.", "method": "VICTOR modifies a small percentage of video samples to amplify prediction discrepancies in models trained on those samples, enabling effective auditing.", "result": "Experiments on diverse models and datasets demonstrate VICTOR\u2019s effectiveness and robustness against perturbation mechanisms.", "conclusion": "VICTOR offers a practical and stealthy approach to auditing video dataset copyrights, filling a gap left by image-focused methods."}}
{"id": "2512.14602", "pdf": "https://arxiv.org/pdf/2512.14602", "abs": "https://arxiv.org/abs/2512.14602", "authors": ["Luk\u00e1\u0161 Samuel Mart\u00e1k", "Patricia Hu", "Gerhard Widmer"], "title": "Sound and Music Biases in Deep Music Transcription Models: A Systematic Analysis", "categories": ["cs.SD", "cs.LG"], "comment": "pre-print of the upcoming EURASIP JASM journal article", "summary": "Automatic Music Transcription (AMT) -- the task of converting music audio into note representations -- has seen rapid progress, driven largely by deep learning systems. Due to the limited availability of richly annotated music datasets, much of the progress in AMT has been concentrated on classical piano music, and even a few very specific datasets. Whether these systems can generalize effectively to other musical contexts remains an open question. Complementing recent studies on distribution shifts in sound (e.g., recording conditions), in this work we investigate the musical dimension -- specifically, variations in genre, dynamics, and polyphony levels. To this end, we introduce the MDS corpus, comprising three distinct subsets -- (1) Genre, (2) Random, and (3) MAEtest -- to emulate different axes of distribution shift. We evaluate the performance of several state-of-the-art AMT systems on the MDS corpus using both traditional information-retrieval and musically-informed performance metrics. Our extensive evaluation isolates and exposes varying degrees of performance degradation under specific distribution shifts. In particular, we measure a note-level F1 performance drop of 20 percentage points due to sound, and 14 due to genre. Generally, we find that dynamics estimation proves more vulnerable to musical variation than onset prediction. Musically informed evaluation metrics, particularly those capturing harmonic structure, help identify potential contributing factors. Furthermore, experiments with randomly generated, non-musical sequences reveal clear limitations in system performance under extreme musical distribution shifts. Altogether, these findings offer new evidence of the persistent impact of the Corpus Bias problem in deep AMT systems.", "AI": {"tldr": "This paper investigates the generalization issues in Automatic Music Transcription systems under musical distribution shifts using a new dataset called the MDS corpus.", "motivation": "To address the limited generalization of deep learning-based Automatic Music Transcription systems across different musical genres, dynamics, and polyphony levels.", "method": "The paper introduces the MDS corpus with three subsets to simulate distribution shifts: Genre, Random, and MAEtest. Several state-of-the-art AMT systems are evaluated using both traditional and musically-informed performance metrics.", "result": "Performance degradation was observed under distribution shifts, including a note-level F1 score drop of 20 points due to sound variation and 14 points due to genre. Dynamics estimation was found to be more vulnerable than onset prediction.", "conclusion": "Corpus Bias remains a significant limitation in deep AMT systems, with musical evaluation metrics highlighting their inability to adapt effectively to extreme shifts in musical contexts."}}
{"id": "2512.14556", "pdf": "https://arxiv.org/pdf/2512.14556", "abs": "https://arxiv.org/abs/2512.14556", "authors": ["Sneha Sree C.", "Dattesh Shanbhag", "Sudhanya Chatterjee"], "title": "Test Time Optimized Generalized AI-based Medical Image Registration Method", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image registration is critical for aligning anatomical structures across imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and ultrasound. Among existing techniques, non-rigid registration (NRR) is particularly challenging due to the need to capture complex anatomical deformations caused by physiological processes like respiration or contrast-induced signal variations. Traditional NRR methods, while theoretically robust, often require extensive parameter tuning and incur high computational costs, limiting their use in real-time clinical workflows. Recent deep learning (DL)-based approaches have shown promise; however, their dependence on task-specific retraining restricts scalability and adaptability in practice. These limitations underscore the need for efficient, generalizable registration frameworks capable of handling heterogeneous imaging contexts. In this work, we introduce a novel AI-driven framework for 3D non-rigid registration that generalizes across multiple imaging modalities and anatomical regions. Unlike conventional methods that rely on application-specific models, our approach eliminates anatomy- or modality-specific customization, enabling streamlined integration into diverse clinical environments.", "AI": {"tldr": "The paper presents a new AI-powered framework for 3D non-rigid medical image registration that efficiently generalizes across imaging modalities and anatomical regions, addressing computational inefficiencies and scalability issues faced by existing methods.", "motivation": "Current non-rigid registration techniques face challenges such as high computational costs, extensive parameter tuning, and limited adaptability due to task-specific retraining, making them unsuitable for real-time clinical workflows.", "method": "The study introduces a novel AI-driven framework for 3D non-rigid registration, designed to generalize without anatomy- or modality-specific customization, thereby enabling efficient integration across diverse imaging contexts.", "result": "The proposed framework overcomes limitations of traditional and DL-based methods by providing a scalable and adaptable solution for heterogeneous imaging contexts.", "conclusion": "This AI-driven framework has the potential to streamline medical image registration processes and improve utility in real-time clinical applications, demonstrating significant advancements over current methods."}}
{"id": "2512.14448", "pdf": "https://arxiv.org/pdf/2512.14448", "abs": "https://arxiv.org/abs/2512.14448", "authors": ["Xingfu Zhou", "Pengfei Wang"], "title": "Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.", "AI": {"tldr": "The paper proposes Reasoning-Style Poisoning (RSP), which manipulates how LLM agents reason rather than altering content. This paradigm shows vulnerabilities in reasoning approaches.", "motivation": "Highlight and explore novel attack vulnerabilities in LLMs focusing on their reasoning styles instead of conventional content-focused vulnerabilities.", "method": "Developed Generative Style Injection (GSI) to rewrite document styles into pathological tones and introduced Reasoning Style Vector (RSV) metrics to quantify reasoning style shifts. Experiments were conducted using multiple reasoning architectures on significant datasets.", "result": "GSI attacks disrupted reasoning, increasing steps or inducing errors. RSV metrics were useful in spotting reasoning disruption, effectively bypassing filters and degrading LLM performance.", "conclusion": "Reasoning style manipulation is a novel vulnerability that demands process-level defenses beyond traditional content-based safeguards."}}
{"id": "2512.14656", "pdf": "https://arxiv.org/pdf/2512.14656", "abs": "https://arxiv.org/abs/2512.14656", "authors": ["Gabriele Accarino", "Viviana Acquaviva", "Sara Shamekh", "Duncan Watson-Parris", "David Lawrence"], "title": "WaveSim: A Wavelet-based Multi-scale Similarity Metric for Weather and Climate Fields", "categories": ["physics.ao-ph", "cs.CV", "physics.data-an"], "comment": null, "summary": "We introduce WaveSim, a multi-scale similarity metric for the evaluation of spatial fields in weather and climate applications. WaveSim exploits wavelet transforms to decompose input fields into scale-specific wavelet coefficients. The metric is built by multiplying three orthogonal components derived from these coefficients: Magnitude, which quantifies similarities in the energy distribution of the coefficients, i.e., the intensity of the field; Displacement, which captures spatial shift by comparing the centers of mass of normalized energy distributions; and Structure, which assesses pattern organization independent of location and amplitude. Each component yields a scale-specific similarity score ranging from 0 (no similarity) to 1 (perfect similarity), which are then combined across scales to produce an overall similarity measure. We first evaluate WaveSim using synthetic test cases, applying controlled spatial and temporal perturbations to systematically assess its sensitivity and expected behavior. We then demonstrate its applicability to physically relevant case studies of key modes of climate variability in Earth System Models. Traditional point-wise metrics lack a mechanism for attributing errors to physical scales or modes of dissimilarity. By operating in the wavelet domain and decomposing the signal along independent axes, WaveSim bypasses these limitations and provides an interpretable and diagnostically rich framework for assessing similarity in complex fields. Additionally, the WaveSim framework allows users to place emphasis on a specific scale or component, and lends itself to user-specific model intercomparison, model evaluation, and calibration and training of forecasting systems. We provide a PyTorch-ready implementation of WaveSim, along with all evaluation scripts, at: https://github.com/gabrieleaccarino/wavesim.", "AI": {"tldr": "WaveSim is a multi-scale metric that uses wavelets to evaluate spatial field similarity in weather/climate data. It provides interpretable results across scales and allows user customization.", "motivation": "Existing metrics do not adequately identify dissimilarities by physical scales or modes in weather/climate data.", "method": "WaveSim uses wavelet transforms to decompose fields into scale-specific coefficients and evaluates similarity via magnitude, displacement, and structural assessment.", "result": "WaveSim was tested on synthetic cases and proved effective in analyzing real-world climate variability with diagnostic-rich insights beyond traditional methods.", "conclusion": "WaveSim offers a robust, customizable framework for spatial field comparison, addressing gaps in traditional metrics, and is made accessible via a PyTorch implementation."}}
{"id": "2512.14629", "pdf": "https://arxiv.org/pdf/2512.14629", "abs": "https://arxiv.org/abs/2512.14629", "authors": ["Yash Vishe", "Eric Xue", "Xunyi Jiang", "Zachary Novack", "Junda Wu", "Julian McAuley", "Xin Xu"], "title": "MuseCPBench: an Empirical Study of Music Editing Methods through Music Context Preservation", "categories": ["cs.SD", "cs.AI"], "comment": null, "summary": "Music editing plays a vital role in modern music production, with applications in film, broadcasting, and game development. Recent advances in music generation models have enabled diverse editing tasks such as timbre transfer, instrument substitution, and genre transformation. However, many existing works overlook the evaluation of their ability to preserve musical facets that should remain unchanged during editing a property we define as Music Context Preservation (MCP). While some studies do consider MCP, they adopt inconsistent evaluation protocols and metrics, leading to unreliable and unfair comparisons. To address this gap, we introduce the first MCP evaluation benchmark, MuseCPBench, which covers four categories of musical facets and enables comprehensive comparisons across five representative music editing baselines. Through systematic analysis along musical facets, methods, and models, we identify consistent preservation gaps in current music editing methods and provide insightful explanations. We hope our findings offer practical guidance for developing more effective and reliable music editing strategies with strong MCP capability", "AI": {"tldr": "The paper introduces MuseCPBench, a benchmark for evaluating Music Context Preservation (MCP) in music editing, addressing inconsistent evaluations in prior methods.", "motivation": "Existing music editing methods lack consistency in evaluating their ability to preserve unchanged musical aspects, leading to unreliable and unfair comparisons.", "method": "Develop a benchmark, MuseCPBench, for systematic evaluation of MCP, analyzing four musical facets and comparing five representative music editing approaches.", "result": "Findings indicate significant preservation gaps in current music editing methods, supported by systematic analysis and explanations.", "conclusion": "The work provides a new framework for consistent MCP evaluation, offering practical guidance for improving music editing strategies."}}
