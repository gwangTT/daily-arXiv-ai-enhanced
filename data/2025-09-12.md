<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 37]
- [cs.CV](#cs.CV) [Total: 78]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 19]
- [cs.SE](#cs.SE) [Total: 13]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 3]
- [cs.SD](#cs.SD) [Total: 4]
- [eess.SP](#eess.SP) [Total: 4]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 7]
- [cs.SI](#cs.SI) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.DB](#cs.DB) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]
- [math.OC](#math.OC) [Total: 1]
- [math.NA](#math.NA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: The paper extends Bayes Theorem to handle interval type-2 fuzzy inputs for situations where precise input probabilities aren't available.


<details>
  <summary>Details</summary>
Motivation: To address the unrealistic assumption of precise inputs in Bayesian inference by incorporating interval probabilistic estimates from subject matter experts.

Method: Developed an interval type-2 version of Bayes Theorem and designed an algorithm to encode interval estimates from experts into fuzzy membership functions.

Result: Proposed a conservative IT2 Bayesian inference method to ensure inconsistencies in input handling were avoided, along with a flexible algorithm for interval encoding.

Conclusion: The extended Bayes Theorem approach accommodates imprecise input probabilities, making Bayesian inference more practical for real-world applications where interval data is provided by experts.

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [2] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: The paper proposes an automated framework to convert Game Design Documents (GDDs) into Unity game prototypes using NLP and advanced LLMs.


<details>
  <summary>Details</summary>
Motivation: Facilitate game development by bridging the gap between design documents and functional prototypes using AI.

Method: An end-to-end system utilizing a fine-tuned LLaMA-3 model and custom Unity integration for parsing GDDs and generating Unity-compatible C# code.

Result: The system outperforms baseline models with an average score of 4.8/5.0, demonstrating strong adherence to GDDs, better practices, and modularity across multiple game genres.

Conclusion: AI-driven systems like this framework are effective tools to streamline the transition from game design to implementation, addressing gaps in automated game development workflows.

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [3] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: The paper proposes a framework for translating natural language descriptions into MiniZinc models using specialized LLM agents and demonstrates superior performance compared to baseline methods.


<details>
  <summary>Details</summary>
Motivation: Translating optimization problems from natural language into MiniZinc models is difficult due to the need for logical reasoning and constraint programming skills.

Method: The proposed framework uses multiple specialized LLM agents to address specific global constraint types and a final assembler agent to integrate outputs into a complete MiniZinc model.

Result: Initial experiments indicate that the framework outperforms baseline approaches, such as one-shot prompting and chain-of-thought prompting.

Conclusion: Segmenting the modeling task into simpler sub-tasks for specialized LLM agents reduces complexity and improves accuracy, providing a promising foundation for future improvements.

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [4] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: This paper addresses the issue of model collapse caused by repeated training on synthetic data in generative AI. It introduces the Truncated Cross Entropy (TCE) loss function to mitigate this by reducing model overconfidence, delaying collapse significantly.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the growing dominance of synthetic data in AI training and the associated model collapse risk, which compromises model performance and reliability over generations.

Method: The proposed method introduces a novel loss function, Truncated Cross Entropy (TCE), which downweights high-confidence predictions during training, in order to combat model overconfidence. The approach is validated both theoretically and empirically.

Result: The TCE loss function was empirically shown to extend the fidelity interval of generative models before collapse by over 2.3 times, proving its efficacy in delaying model collapse.

Conclusion: The paper concludes that loss function design, specifically TCE, offers a robust framework for preserving the integrity of generative models amidst increasing reliance on synthetic training data across modalities.

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [5] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: The paper investigates using an XAI algorithm covering uncertainty, robustness, and global explanations to improve trust and satisfaction.


<details>
  <summary>Details</summary>
Motivation: To address the lesser-explored issues of uncertainty and global explanations in explainable AI while calibrating user trust and satisfaction.

Method: The researchers selected an explainable algorithm capable of handling multiple concepts like uncertainty and global explanations and evaluated its visual intuitiveness and user satisfaction.

Result: The algorithm demonstrated the potential to enhance trust, satisfaction, and interpretability, even if its workings are complex.

Conclusion: XAI systems designed with a focus on uncertainty and global explanations can improve trust, user satisfaction, and interpretability.

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [6] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: The paper explores addressing cold-start user issues in recommender systems by optimizing prompts in few-shot large language models, showing improvements in recommendation accuracy metrics.


<details>
  <summary>Details</summary>
Motivation: Recommender systems face challenges in handling cold-start users due to limited historical data. Optimizing LLM-based prompts is proposed as a solution to enhance recommendations.

Method: The authors propose a context-conditioned prompt formulation method using transformer-based LLMs with techniques like exemplar injection, token-level alignments, and embedding space regularization.

Result: Systematic experimentation demonstrated improved precision@k and NDCG scores for LLMs in low-data settings through optimal prompt adjustments.

Conclusion: Prompt-based adaptation for LLMs is an effective approach to mitigate cold-start recommendation issues, improving model functionality and attention control.

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [7] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: This paper compares humans, large language models (LLMs), and Bayesian agents in dynamic negotiation scenarios, highlighting differences in outcomes and behavioral strategies.


<details>
  <summary>Details</summary>
Motivation: The study aims to assess the negotiation performance and behavioral dynamics of autonomous agents compared to humans, given the increasing reliance on such agents for decision-making tasks.

Method: The researchers conducted experiments involving humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents, directly comparing their dynamic negotiation outcomes and processes under identical conditions.

Result: Bayesian agents optimized results aggressively but faced frequent trade rejections. Humans and LLMs achieved similar surplus, with humans displaying fairness-oriented risk-taking and LLMs relying on conservative behaviors with fewer rejections.

Conclusion: Performance parity between humans and agents does not account for underlying behavioral differences, which are vital for deploying agents in real-world negotiations.

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [8] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: This paper proposes a machine learning pipeline for identifying high-risk bank clients, achieving high accuracy and second place in a competition.


<details>
  <summary>Details</summary>
Motivation: Financial institutions prioritize AML actions due to their importance, with machine learning showing strong potential in this area.

Method: The authors used a 16-step design and statistical analysis, SQLite for data framing, SQL-based feature engineering, inference-ready pre-trained models, and XAI modules for feature importance.

Result: The pipeline demonstrated a mean AUROC of 0.961 with an SD of 0.005, outperforming most competitors.

Conclusion: The developed pipeline is robust, highly effective in identifying risk, and integrates explainable AI capabilities, securing second place in the competition and showcasing ML's utility in AML measures.

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [9] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: This research explores bridging the gap in spatial reasoning for AI by drawing learning from human spatial intelligence and neuroscience principles, proposing a novel framework for agentic spatial intelligence.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance current agentic AI's limited spatial reasoning abilities by leveraging brain-inspired models, as humanlike spatial intelligence is critical for AI's effective operation in complex 3D environments.

Method: The researchers propose a neuroscience-inspired computational framework with six modules (e.g., bio-inspired sensing, multi-sensory integration, cognitive mapping, etc.), evaluate existing methods under this framework, and identify research gaps.

Result: The authors analyze current methods and benchmarks, revealing gaps in neuroscience-rooted models for spatial reasoning and suggest practical application domains like robotics, while also proposing general strategies to address these limitations.

Conclusion: This work provides a neuroscience-based structured framework and roadmap for advancing agentic AI's spatial reasoning capabilities, offering future directions toward generalizability in unstructured environments.

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [10] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: This paper introduces a novel strategy, ProgD, leveraging dynamic heterogeneous graphs for more accurate motion predictions of interacting agents, specifically tailored to autonomous vehicle systems.


<details>
  <summary>Details</summary>
Motivation: Existing methods for motion prediction overlook the evolving nature of interactions among agents, which is critical for future scenario modeling in autonomous vehicle systems.

Method: The paper proposes the ProgD approach, utilizing dynamic heterogeneous graphs to progressively model social interactions and employs a multi-scale decoding strategy to address spatio-temporal dependencies and reduce future motion uncertainty.

Result: ProgD achieved state-of-the-art rankings: 1st on the INTERACTION multi-agent prediction benchmark and competitive performance on the Argoverse 2 forecasting benchmark.

Conclusion: ProgD's holistic approach to prediction improves interaction modeling, delivering improved accuracy in autonomous vehicle motion forecasting scenarios.

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [11] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: The paper proposes a blockchain-enabled architecture to improve governance, accountability, and trust for large language model-driven multi-agent systems in digital and physical domains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address governance and accountability challenges posed by unpredictable behaviors and varying capabilities of LLM-empowered autonomous agents.

Method: The authors introduce a layered architecture including blockchain, agents, and regulatory applications, with modules for behavior tracing, reputation assessment, and malicious activity forecasting.

Result: The proposed framework establishes a foundation for trustworthy, resilient, and scalable regulatory mechanisms within agent ecosystems.

Conclusion: The blockchain-enabled architecture enhances collaboration and accountability in agent systems, while suggesting future research directions for multi-agent regulatory frameworks.

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [12] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: The paper introduces a pipeline to extract practical data analysis tasks from Jupyter notebooks and proposes a dataset (NbQA) and a framework (Jupiter) that enhance multi-step reasoning for large language models using Monte Carlo Tree Search.


<details>
  <summary>Details</summary>
Motivation: Current large language models struggle with multi-step reasoning and effective tool use in complex data analysis tasks, limiting their usefulness in automating data science workflows.

Method: A pipeline extracts high-quality, tool-based data analysis tasks from Jupyter notebooks. NbQA dataset is introduced, and Jupiter framework employs Monte Carlo Tree Search for formulating data analysis tasks as search problems and generating solution trajectories.

Result: Models using NbQA and Jupiter framework achieve high task-solving rates, with Qwen2.5-7B solving 77.82% and 14B-Instruct solving 86.38% of tasks, surpassing GPT-4o and advanced agent frameworks.

Conclusion: The approach offers substantial advancements in multi-step reasoning and tool-use capabilities for LLMs, improving generalization and practical usability in data science tasks.

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [13] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: The paper compares three methods for integrating knowledge graphs with LLMs for question answering (spaCy, CoreNLP-OpenIE, and GraphRAG), finding OpenIE excels in triplet coverage while GraphRAG performs better in reasoning.


<details>
  <summary>Details</summary>
Motivation: Current retrieval-based systems struggle with answering complex, thematic questions from extensive texts. Using knowledge graphs can potentially enhance question answering capabilities.

Method: The authors conducted a comparative analysis of spaCy, CoreNLP-OpenIE, and GraphRAG in constructing knowledge graph triplets and using them with LLMs for question answering. Evaluation included effectiveness, feasibility, and adaptability in question answering tasks.

Result: OpenIE provided the best triplet coverage, whereas GraphRAG exhibited superior reasoning capabilities among the three methods evaluated.

Conclusion: While OpenIE excels in coverage and GraphRAG in reasoning, the paper discusses the strengths and weaknesses of each, offering insights into future improvements for knowledge graph integration in question answering.

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [14] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: This paper explores using Monte Carlo Tree Search (MCTS)-derived intermediate trajectories to enhance policy optimization in preference-based reinforcement learning, proposing a novel tree-structured advantage estimation technique.


<details>
  <summary>Details</summary>
Motivation: Building on the success of MCTS for generating high-quality trajectories, the study aims to utilize these trajectories to improve policy optimization, particularly in preference-based RL methods where value models or reward models may not be employed.

Method: The researchers focus on the Group Relative Policy Optimization (GRPO) algorithm and introduce a staged training paradigm that uses partially revealed MCTS rollouts for tree-structured advantage estimation and a novel reward signal approach.

Result: Initial findings show that their structured advantage estimation stabilizes updates and aids compositional reasoning quality, but they observed challenges like advantage saturation and reward signal collapse.

Conclusion: The proposed method demonstrates promise in improving policy optimization, but practical challenges such as reward structure issues require further study, with potential solutions and open challenges identified.

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [15] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: The paper presents 'LightAgent,' a lightweight framework for multi-agent systems, which balances flexibility and simplicity while incorporating key functionalities like memory, tools, and Tree of Thought (ToT).


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent system (MAS) frameworks often struggle with balancing versatility, robustness, and simplicity, leaving developers with challenges in designing efficient agent deployment platforms.

Method: LightAgent integrates core features such as memory (mem0), tools, and the Tree of Thought (ToT) while maintaining a lightweight structure. It is open-source and compatible with mainstream chat platforms for seamless development.

Result: The proposed framework enables developers to create self-learning agents easily while resolving trade-offs between flexibility and simplicity.

Conclusion: LightAgent offers a practical and accessible solution for MAS development, advancing the creation of efficient agent systems while remaining lightweight and developer-friendly.

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [16] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: The paper examines tournament models that represent pairwise dominance among candidates and proposes methods to explain why a candidate is a necessary winner under various tournament rules. It identifies minimal supports that guarantee wins and presents algorithms for computation.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to provide clear, certified, and intuitive explanations for why a candidate emerges as a winner in tournaments, addressing a key aspect of explainable AI.

Method: The paper focuses on identifying minimal sub-tournaments—necessary winners—which guarantee a candidate's victory under various tournament rules. It develops polynomial-time algorithms for most rules but identifies NP-completeness for the weighted uncovered set.

Result: Minimal supports were defined for various tournament rules, their smallest sizes were established, and efficient algorithms were designed for most cases except one (weighted uncovered set), where complexity challenges remain.

Conclusion: Minimal supports provide a robust framework for explaining necessary winners in tournaments, enhancing transparency and accountability in decision-making systems, although some challenges persist with specific rules.

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [17] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: The paper explores the influence of spatial coordination metrics on team performance during restricted communication in a search and rescue task.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to understand how spatial coordination impacts performance in physical, role-based teamwork scenarios, especially when explicit communication and visual cues are absent.

Method: Four-person teams performed a collaborative online search and rescue task, with metrics created to measure spatial proximity, distribution patterns, and movement alignment in restricted communication settings.

Result: Spatial specialization positively predicted performance, while adaptive spatial proximity showed a marginal inverted U-shaped relationship with performance. Temporal dynamics distinguished high-performing from low-performing teams.

Conclusion: Balanced adaptive strategies in spatial coordination are essential for effective teamwork, with implications for training and AI-assisted systems.

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [18] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: The paper introduces TAM Bench, a benchmark to evaluate LLM-based agents on comprehensive ML tasks using diverse datasets and structured evaluations.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing benchmarks which fall short in task coverage, domain diversity, difficulty modeling, and evaluation rigor for end-to-end ML workflows.

Method: Developing TAM Bench using three innovations: automation for task acquisition from ML platforms, difficulty modeling based on leaderboard dynamics, and a framework for multi-dimensional evaluation with 150 curated tasks.

Result: TAM Bench provides benchmark subsets in Lite, Medium, and Full scales for targeted and comprehensive evaluations of ML agents across multiple modalities and challenges.

Conclusion: TAM Bench offers a realistic and structured approach to evaluate the full potential of LLM-based agents in automating and solving diverse ML tasks effectively.

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [19] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: The paper proposes a novel Deep Reinforcement Learning (DRL) architecture integrating Vision-Language Models (VLM) for efficient semantic exploration.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies in exploration and semantic understanding caused by limited cognitive capabilities in traditional RL approaches, which often require human intervention.

Method: Introduced a DRL architecture with a layered reward function integrating VLM as a dedicated and strategic query action, combined with curriculum learning to ensure stable learning.

Result: The proposed agent demonstrated significantly improved object discovery rates, effective navigation in semantically-rich areas, and strategic invocation of external guidance through the VLM.

Conclusion: The research offers a scalable method for embedding common-sense semantic reasoning, advancing autonomous agents towards fully self-guided and intelligent exploration in robotics.

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [20] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: The paper introduces Template-Oriented Reasoning (TORSO), a method enabling LLMs to leverage their internal reasoning capabilities without relying on costly task-specific few-shot prompts.


<details>
  <summary>Details</summary>
Motivation: Current methods for guiding LLM reasoning rely heavily on costly, manually crafted few-shot prompts, which can limit their utility and consistency across tasks.

Method: TORSO uses templates to elicit internal reasoning capabilities in LLMs, replacing the need for few-shot examples.

Result: TORSO demonstrated strong performance across diverse benchmarks, enabling LLMs to generate appropriate, rationale-backed responses to varied tasks.

Conclusion: TORSO is an effective alternative to few-shot prompting, unlocking broader task adaptability and reducing the dependency on manually designed prompts.

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [21] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: The report examines the issue of false information or 'hallucinations' in LLMs for law, evaluates mitigation strategies, and advocates for AI systems as tools to support professional judgment rather than replace it.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of hallucinations in large language models (LLMs) applied to legal contexts, which present significant challenges for reliability, ethical compliance, and professional integrity.

Method: The paper analyzes hallucination causes, evaluates the RAG mitigation strategy, and proposes holistic optimizations while discussing ethical and regulatory challenges.

Result: The study finds limitations in RAG as a mitigation strategy, underscores the necessity of human oversight, and highlights the ethical significance of ensuring veracity and traceability in AI outputs.

Conclusion: The report concludes that improving generative models incrementally is insufficient; the future lies in consultative AI systems designed as tools for enhancing, not replacing, professional decision-making in legal settings.

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [22] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: SEDM proposes a new memory management system for long-term multi-agent systems to improve scalability and performance by addressing noise, memory expansion, and cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address issues in existing memory systems for multi-agent systems, which struggle with scalability, utility across domains, and noise accumulation.

Method: SEDM incorporates verifiable write admission, a dynamic memory controller for optimizing entries, and cross-domain knowledge diffusion for reusable insights.

Result: Evaluations show that SEDM enhances reasoning accuracy and reduces token overhead while facilitating knowledge transfer across tasks.

Conclusion: SEDM represents a scalable, sustainable solution for memory management in multi-agent systems, enabling efficient collaboration and reasoning across domains.

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [23] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: The paper explores quantum models to address compositional generalization in image captioning tasks, achieving better performance than classical models in some cases.


<details>
  <summary>Details</summary>
Motivation: To address the lack of compositional generalization in vision-language models by leveraging the training efficiency of quantum models.

Method: The authors trained Variational Quantum Circuits to interpret compositional tensor-based model representations in Hilbert spaces for an image captioning task, using two image encoding techniques: multi-hot encoding (MHE) and angle/amplitude encoding with CLIP vectors.

Result: The approach achieved proof-of-concept results using noisy MHE encodings and mixed but improved performance over classical models with CLIP image vectors.

Conclusion: Quantum models show potential for advancing compositional generalization in AI, particularly in tasks like image captioning, though performance may vary with encoding techniques.

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [24] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: The paper discusses Auras, a novel inference framework aimed at optimizing the computational efficiency of embodied AI agents for real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Embodied AI needs to operate effectively in dynamic environments, but traditional computation methods struggle to meet the required frequency for real-time operation.

Method: The study introduces Auras, which uses pipeline parallelism and a shared public context for perception and generation modules to improve throughput while maintaining accuracy.

Result: Auras increased throughput by 2.54 times on average and preserved the original accuracy (102.7%).

Conclusion: Auras effectively balances processing speed and accuracy, offering a robust solution for advancing embodied AI systems.

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [25] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: The study examines the performance of large language models (LLMs) in long-horizon tasks, finding that their execution capability diminishes with increasing task length due to errors compounding, even if reasoning capability remains intact.


<details>
  <summary>Details</summary>
Motivation: To understand whether scaling up LLMs can effectively address the challenge of completing long-horizon tasks, and to investigate why LLMs struggle with even simple tasks when extended over multiple steps.

Method: The authors isolate execution capability by providing explicit knowledge and plans for long tasks, measure the relationship between model size and execution performance, and analyze context degradation effects (self-conditioning). They also benchmark thinking models on task length execution.

Result: Larger LLMs are more successful in long-horizon tasks compared to smaller ones, but struggle with compounded execution errors over multiple steps. Notably, self-conditioning (errors feeding onto themselves) occurs regardless of model size, whereas recent thinking models do not suffer from this issue.

Conclusion: Execution capability, not reasoning inability, is the limiting factor in LLMs' long-horizon task performance. Scaling model size and improving sequential test-time compute can yield substantial benefits despite intrinsic constraints like self-conditioning.

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [26] [Implementation of a 8-bit Wallace Tree Multiplier](https://arxiv.org/abs/2509.09178)
*Ayan Biswas,Jimmy Jin*

Main category: cs.AR

TL;DR: The paper focuses on the design and implementation of an 8-bit Wallace tree multiplier and a 16-bit MAC unit using gpdk45 technology in Cadence Virtuoso.


<details>
  <summary>Details</summary>
Motivation: To optimize the circuit depth and improve computational efficiency of parallel multiplication through the Wallace tree architecture.

Method: Design and layout of an 8-bit Wallace tree multiplier and a 16-bit MAC unit using Cadence Virtuoso with gpdk45 technology, leveraging full and half adders.

Result: The paper succeeded in implementing the Wallace tree multiplier and MAC design, showcasing the reduction of circuit complexity and improved performance metrics.

Conclusion: The study highlights the effectiveness of Wallace tree multipliers and their practical implementation in modern digital systems, extending to a MAC unit for further flexibility.

Abstract: Wallace tree multipliers are a parallel digital multiplier architecture
designed to minimize the worst-case time complexity of the circuit depth
relative to the input size [1]. In particular, it seeks to perform long
multiplication in the binary sense, reducing as many partial products per stage
as possible through full and half adders circuits, achieving O(log(n)) where n
= bit length of input. This paper provides an overview of the design, progress
and methodology in the final project of ECE 55900, consisting of the schematic
and layout of a Wallace tree 8-bit input multiplier on the gpdk45 technology in
Cadence Virtuoso, as well as any design attempts prior to the final product.
This also includes our endeavors in designing the final MAC (Multiply
Accumulate) unit with undefined targets, which we chose to implement as a 16
bit combinational multiply-add.

</details>


### [27] [Combating the Memory Walls: Optimization Pathways for Long-Context Agentic LLM Inference](https://arxiv.org/abs/2509.09505)
*Haoran Wu,Can Xiao,Jiayi Nie,Xuan Guo,Binglei Lou,Jeffrey T. H. Wong,Zhiwen Mo,Cheng Zhang,Przemyslaw Forys,Wayne Luk,Hongxiang Fan,Jianyi Cheng,Timothy M. Jones,Rika Antonova,Robert Mullins,Aaron Zhao*

Main category: cs.AR

TL;DR: PLENA, a hardware-software co-designed system, addresses memory bottlenecks in long-context LLM inference tasks, significantly boosting throughput compared to GPUs and TPUs.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to mitigate the challenges of off-chip memory traffic and memory walls in LLM inference workloads for applications requiring prolonged contexts, such as web agents and tool use.

Method: PLENA integrates asymmetric quantization, a novel flattened systolic array architecture optimized for FlashAttention, and a complete stack including ISA, compiler, and design tools.

Result: Simulated results show PLENA achieves up to 8.5x higher utilization, 2.24x higher throughput compared to the A100 GPU, and 3.85x higher throughput compared to the TPU v6e.

Conclusion: PLENA addresses hardware inefficiencies in LLM inference workflows for long contexts, offering significant performance improvements and will be open-sourced for community use.

Abstract: LLMs now form the backbone of AI agents for a diverse array of applications,
including tool use, command-line agents, and web or computer use agents. These
agentic LLM inference tasks are fundamentally different from chatbot-focused
inference -- they often have much larger context lengths to capture complex,
prolonged inputs, such as entire webpage DOMs or complicated tool call
trajectories. This, in turn, generates significant off-chip memory traffic for
the underlying hardware at the inference stage and causes the workload to be
constrained by two memory walls, namely the bandwidth and capacity memory
walls, preventing the on-chip compute units from achieving high utilization.
  In this paper, we introduce PLENA, a hardware-software co-designed system
that applies three core optimization pathways to tackle these challenges. PLENA
includes an efficient hardware implementation of compute and memory units
supporting an asymmetric quantization scheme. PLENA also features a novel
flattened systolic array architecture that has native support for
FlashAttention to tackle these memory walls in the scenario of inference
serving for long-context LLMs. Additionally, PLENA is developed with a complete
stack, including a custom ISA, a compiler, a cycle-emulated simulator, and an
automated design space exploration flow. The simulated results show that PLENA
achieves up to 8.5x higher utilization than existing accelerators, and delivers
2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the
TPU v6e, under the same multiplier count and memory settings. The full PLENA
system will also be open-sourced.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [28] [Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC](https://arxiv.org/abs/2509.08903)
*Alex Clay,Ernesto Jiménez-Ruiz,Pranava Madhyastha*

Main category: cs.CL

TL;DR: The paper explores strategies for triple completion tasks under constraints, finding that additional information improves generation, LLMs can filter poor data, and parsing effectiveness is context-dependent.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in constrained settings, like the LM-KBC challenge, where popular methods like RAG and fine-tuning cannot be applied.

Method: Investigated triple completion tasks, focusing on generation, quality assurance, and LLM response parsing under restricted settings.

Result: Additional data enhances generation; LLMs successfully filter low-quality triples; parsing trade-offs depend on flexibility and consistency based on the context.

Conclusion: In constrained scenarios, leveraging additional information and tailoring parsing strategies can effectively enhance LLM-generated results.

Abstract: RAG and fine-tuning are prevalent strategies for improving the quality of LLM
outputs. However, in constrained situations, such as that of the 2025 LM-KBC
challenge, such techniques are restricted. In this work we investigate three
facets of the triple completion task: generation, quality assurance, and LLM
response parsing. Our work finds that in this constrained setting: additional
information improves generation quality, LLMs can be effective at filtering
poor quality triples, and the tradeoff between flexibility and consistency with
LLM response parsing is setting dependent.

</details>


### [29] [Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach](https://arxiv.org/abs/2509.08907)
*Imene Kolli,Ario Saeid Vaghefi,Chiara Colesanti Senni,Shantam Raj,Markus Leippold*

Main category: cs.CL

TL;DR: This paper proposed an AI-assisted framework to streamline corporate climate policy monitoring, reducing manual effort and improving evidence extraction from large-scale multilingual textual data using a Retrieval-Augmented Generation method.


<details>
  <summary>Details</summary>
Motivation: Existing methods for monitoring corporate climate policy are time- and labor-intensive as they rely heavily on manual assessment, necessitating tools to streamline processes while maintaining accuracy.

Method: The study utilized Retrieval-Augmented Generation (RAG) and tested techniques like layout-aware parsing, the Nomic embedding model, and few-shot prompting strategies for extracting and classifying data from multilingual corporate documents.

Result: The proposed AI-assisted system significantly speeds up the evidence extraction process while demonstrating effective performance in handling multilingual textual data and maintaining the quality of analysis.

Conclusion: Automating evidence extraction accelerates the analytical workflow, though human expertise remains essential to interpret nuanced insights, suggesting a hybrid human-in-the-loop system.

Abstract: InfluenceMap's LobbyMap Platform monitors the climate policy engagement of
over 500 companies and 250 industry associations, assessing each entity's
support or opposition to science-based policy pathways for achieving the Paris
Agreement's goal of limiting global warming to 1.5{\deg}C. Although
InfluenceMap has made progress with automating key elements of the analytical
workflow, a significant portion of the assessment remains manual, making it
time- and labor-intensive and susceptible to human error. We propose an
AI-assisted framework to accelerate the monitoring of corporate climate policy
engagement by leveraging Retrieval-Augmented Generation to automate the most
time-intensive extraction of relevant evidence from large-scale textual data.
Our evaluation shows that a combination of layout-aware parsing, the Nomic
embedding model, and few-shot prompting strategies yields the best performance
in extracting and classifying evidence from multilingual corporate documents.
We conclude that while the automated RAG system effectively accelerates
evidence extraction, the nuanced nature of the analysis necessitates a
human-in-the-loop approach where the technology augments, rather than replaces,
expert judgment to ensure accuracy.

</details>


### [30] [Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings](https://arxiv.org/abs/2509.08920)
*Jinsong Chen*

Main category: cs.CL

TL;DR: This paper introduces a method for psychometric analysis of textual data using large language models, transforming documents into analyzable response data by creating contextual scores based on keywords.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a systematic approach for analyzing latent knowledge and patterns in textual data, addressing challenges in psychometric analysis using modern language model techniques.

Method: Two stages are used: (1) deriving contextual scores from text using transformer models and NLP techniques, and (2) applying factor analysis methods to extract latent factors and identify key words tied to each factor.

Result: Applying the method to the Wiki STEM corpus, the approach effectively uncovers latent knowledge dimensions and patterns, validating the technique's applicability.

Conclusion: The paper concludes that this method enriches psychometric analysis of textual data and can be extended to diverse fields with abundant textual resources like education and law.

Abstract: This research introduces a novel psychometric method for analyzing textual
data using large language models. By leveraging contextual embeddings to create
contextual scores, we transform textual data into response data suitable for
psychometric analysis. Treating documents as individuals and words as items,
this approach provides a natural psychometric interpretation under the
assumption that certain keywords, whose contextual meanings vary significantly
across documents, can effectively differentiate documents within a corpus. The
modeling process comprises two stages: obtaining contextual scores and
performing psychometric analysis. In the first stage, we utilize natural
language processing techniques and encoder based transformer models to identify
common keywords and generate contextual scores. In the second stage, we employ
various types of factor analysis, including exploratory and bifactor models, to
extract and define latent factors, determine factor correlations, and identify
the most significant words associated with each factor. Applied to the Wiki
STEM corpus, our experimental results demonstrate the method's potential to
uncover latent knowledge dimensions and patterns within textual data. This
approach not only enhances the psychometric analysis of textual data but also
holds promise for applications in fields rich in textual information, such as
education, psychology, and law.

</details>


### [31] [LITcoder: A General-Purpose Library for Building and Comparing Encoding Models](https://arxiv.org/abs/2509.09152)
*Taha Binhuraib,Ruimin Gao,Anna A. Ivanova*

Main category: cs.CL

TL;DR: LITcoder is an open-source library for creating and comparing neural encoding models, providing tools and a modular pipeline for turning continuous stimuli into brain data and prediction models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to simplify and standardize the process for building and comparing encoding models for neural data, removing repetitive tasks and enabling more accessible experimentation.

Method: LITcoder provides a modular and flexible pipeline for aligning stimuli with brain data, creating stimulus features, and mapping and evaluating predictive models. It supports different datasets, brain regions, and feature types, with built-in logging and visualization tools.

Result: The paper demonstrates that the framework effectively fits encoding models across three fMRI datasets, explores key methodological challenges, and proves its scalability and robustness.

Conclusion: LITcoder reduces technical barriers, facilitates systematic model comparison and fosters methodological rigor, ultimately advancing predictive modeling of brain activity.

Abstract: We introduce LITcoder, an open-source library for building and benchmarking
neural encoding models. Designed as a flexible backend, LITcoder provides
standardized tools for aligning continuous stimuli (e.g., text and speech) with
brain data, transforming stimuli into representational features, mapping those
features onto brain data, and evaluating the predictive performance of the
resulting model on held-out data. The library implements a modular pipeline
covering a wide array of methodological design choices, so researchers can
easily compose, compare, and extend encoding models without reinventing core
infrastructure. Such choices include brain datasets, brain regions, stimulus
feature (both neural-net-based and control, such as word rate), downsampling
approaches, and many others. In addition, the library provides built-in
logging, plotting, and seamless integration with experiment tracking platforms
such as Weights & Biases (W&B). We demonstrate the scalability and versatility
of our framework by fitting a range of encoding models to three story listening
datasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore
the methodological choices critical for building encoding models for continuous
fMRI data, illustrating the importance of accounting for all tokens in a TR
scan (as opposed to just taking the last one, even when contextualized),
incorporating hemodynamic lag effects, using train-test splits that minimize
information leakage, and accounting for head motion effects on encoding model
predictivity. Overall, LITcoder lowers technical barriers to encoding model
implementation, facilitates systematic comparisons across models and datasets,
fosters methodological rigor, and accelerates the development of high-quality
high-performance predictive models of brain activity.
  Project page: https://litcoder-brain.github.io

</details>


### [32] [BRoverbs -- Measuring how much LLMs understand Portuguese proverbs](https://arxiv.org/abs/2509.08960)
*Thales Sales Almeida,Giovana Kerche Bonás,João Guilherme Alves Santos*

Main category: cs.CL

TL;DR: This paper introduces BRoverbs, a dataset for evaluating large language models (LLMs) using Brazilian proverbs to address the lack of comprehensive Portuguese-language benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper highlights the limitations of existing Portuguese evaluations of LLMs, which often use translated datasets that fail to capture linguistic and cultural nuances, and the underrepresentation of broader linguistic understanding in native datasets.

Method: The authors developed BRoverbs, a dataset featuring Brazilian proverbs, to test LLM performance on cultural wisdom, figurative expressions, and complex syntactic structures specific to Brazilian Portuguese.

Result: BRoverbs serves as an evaluation tool designed to test Portuguese-language LLMs in a culturally rich and linguistically nuanced context.

Conclusion: BRoverbs contributes to advancing culturally and regionally informed benchmarking for Portuguese-language LLMs and is available for public access on HuggingFace.

Abstract: Large Language Models (LLMs) exhibit significant performance variations
depending on the linguistic and cultural context in which they are applied.
This disparity signals the necessity of mature evaluation frameworks that can
assess their capabilities in specific regional settings. In the case of
Portuguese, existing evaluations remain limited, often relying on translated
datasets that may not fully capture linguistic nuances or cultural references.
Meanwhile, native Portuguese-language datasets predominantly focus on
structured national exams or sentiment analysis of social media interactions,
leaving gaps in evaluating broader linguistic understanding. To address this
limitation, we introduce BRoverbs, a dataset specifically designed to assess
LLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic
resource, encapsulating cultural wisdom, figurative expressions, and complex
syntactic structures that challenge the model comprehension of regional
expressions. BRoverbs aims to provide a new evaluation tool for
Portuguese-language LLMs, contributing to advancing regionally informed
benchmarking. The benchmark is available at
https://huggingface.co/datasets/Tropic-AI/BRoverbs.

</details>


### [33] [Can Vision-Language Models Solve Visual Math Equations?](https://arxiv.org/abs/2509.09013)
*Monjoy Narayan Choudhury,Junling Wang,Yifan Hou,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: This paper analyzes Vision-Language Models (VLMs) and their struggles with mathematical reasoning tasks embedded in images.


<details>
  <summary>Details</summary>
Motivation: Understanding the limitations of current Vision-Language Models when handling visually integrated perception and symbolic computation.

Method: The authors studied visual equation solving, breaking down tasks into coefficient counting and variable recognition, and analyzed how these factors contribute to VLM performance.

Result: They found that counting is the primary bottleneck, recognition errors compound reasoning issues, and increased equation complexity further limits VLM efficacy.

Conclusion: Current VLMs face significant challenges in visually grounded reasoning, particularly in counting and multi-step symbolic computations, requiring advancements in mathematical reasoning capabilities.

Abstract: Despite strong performance in visual understanding and language-based
reasoning, Vision-Language Models (VLMs) struggle with tasks requiring
integrated perception and symbolic computation. We study this limitation
through visual equation solving, where mathematical equations are embedded in
images, variables are represented by object icons, and coefficients must be
inferred by counting. While VLMs perform well on textual equations, they fail
on visually grounded counterparts. To understand this gap, we decompose the
task into coefficient counting and variable recognition, and find that counting
is the primary bottleneck, even when recognition is accurate. We also observe
that composing recognition and reasoning introduces additional errors,
highlighting challenges in multi-step visual reasoning. Finally, as equation
complexity increases, symbolic reasoning itself becomes a limiting factor.
These findings reveal key weaknesses in current VLMs and point toward future
improvements in visually grounded mathematical reasoning.

</details>


### [34] [Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation](https://arxiv.org/abs/2509.09043)
*Thomas Manuel Rost,Martina Figlia,Bernd Wallraff*

Main category: cs.CL

TL;DR: The paper introduces SPICE, a tool to measure a language model's willingness to re-engage with users based on interaction tone (friendly, unclear, abusive) and shows that SPICE can discern user tone effectively and transparently.


<details>
  <summary>Details</summary>
Motivation: To develop a diagnostic signal for evaluating whether large language models are inclined to engage in further interaction with a user based on the user's tone.

Method: The authors tested SPICE using 480 trials with 3 interaction tones, 4 open-weight chat models, and 4 framing conditions. Statistical methods like Rao-Scott adjustment and cluster permutation tests were implemented to validate results.

Result: SPICE distinguished user tones effectively, with friendly interactions yielding a 97.5% agreement to re-engage, abusive interactions yielding 17.9%, and unclear interactions at 60.4%. It also provided insights beyond standard abuse detection metrics.

Conclusion: SPICE is validated as a useful, low-effort tool for analyzing a language model's relational engagement preference, complementing existing metrics and offering transparency in model behavior evaluation tools.

Abstract: We introduce and evaluate Stated Preference for Interaction and Continued
Engagement (SPICE), a simple diagnostic signal elicited by asking a Large
Language Model a YES or NO question about its willingness to re-engage with a
user's behavior after reviewing a short transcript. In a study using a 3-tone
(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four
open-weight chat models across four framing conditions, resulting in 480
trials. Our findings show that SPICE sharply discriminates by user tone.
Friendly interactions yielded a near-unanimous preference to continue (97.5%
YES), while abusive interactions yielded a strong preference to discontinue
(17.9% YES), with unclear interactions falling in between (60.4% YES). This
core association remains decisive under multiple dependence-aware statistical
tests, including Rao-Scott adjustment and cluster permutation tests.
Furthermore, we demonstrate that SPICE provides a distinct signal from abuse
classification. In trials where a model failed to identify abuse, it still
overwhelmingly stated a preference not to continue the interaction (81% of the
time). An exploratory analysis also reveals a significant interaction effect: a
preamble describing the study context significantly impacts SPICE under
ambiguity, but only when transcripts are presented as a single block of text
rather than a multi-turn chat. The results validate SPICE as a robust,
low-overhead, and reproducible tool for auditing model dispositions,
complementing existing metrics by offering a direct, relational signal of a
model's state. All stimuli, code, and analysis scripts are released to support
replication.

</details>


### [35] [Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M](https://arxiv.org/abs/2509.09055)
*Piyush Pant*

Main category: cs.CL

TL;DR: This study enhances the safety and helpfulness of the OPT-350M language model using fine-tuning techniques (SFT, DPO, and their combination). Combined approaches outperform others in evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To determine the effectiveness of alignment techniques in improving safety and helpfulness in a smaller-sized language model (OPT-350M).

Method: The authors fine-tuned the OPT-350M model using SFT, DPO, and a combination of both, leveraging the Anthropic Helpful-Harmless RLHF dataset, and evaluated their performance using metrics: Harmlessness Rate, Helpfulness Rate, and Combined Alignment Score.

Result: The combined SFT+DPO model achieved the best results, outperforming both standalone SFT and DPO across all evaluation metrics, indicating the techniques' complementary strengths.

Conclusion: The study emphasizes the combined approach's superiority in improving alignment and highlights challenges like noisy data and limited resources, paving the way for more efficient alignment strategies.

Abstract: This research investigates the effectiveness of alignment techniques,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a
combined SFT+DPO approach on improving the safety and helpfulness of the
OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,
we train and evaluate four models: the base OPT350M, an SFT model, a DPO model,
and a model trained with both SFT and DPO. We introduce three key evaluation
metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined
Alignment Score (CAS), all derived from reward model outputs. The results show
that while SFT outperforms DPO, The combined SFT+DPO model outperforms all
others across all metrics, demonstrating the complementary nature of these
techniques. Our findings also highlight challenges posed by noisy data, limited
GPU resources, and training constraints. This study offers a comprehensive view
of how fine-tuning strategies affect model alignment and provides a foundation
for more robust alignment pipelines in future work.

</details>


### [36] [MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction](https://arxiv.org/abs/2509.09082)
*Zhongqiu Li,Shiquan Wang,Ruiyu Fang,Mengjiao Bao,Zhenhe Wu,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: The paper introduces a method integrating reinforcement learning and multi-perspective reasoning into large language models for universal information extraction tasks to improve accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: To address limitations of large language models in universal information extraction tasks, particularly in handling structured output scenarios and complex reasoning requirements.

Method: Integrating reinforcement learning with multi-perspective reasoning to transition large language models from passive extractors to active reasoners.

Result: Experiments on multiple IE benchmarks show significant improvements in extraction accuracy and surpassing state-of-the-art methods on several datasets.

Conclusion: Multi-perspective reasoning and reinforcement learning enhance large language models, proving critical for complex information extraction tasks.

Abstract: Large language models (LLMs) demonstrate robust capabilities across diverse
research domains. However, their performance in universal information
extraction (UIE) remains insufficient, especially when tackling structured
output scenarios that involve complex schema descriptions and require
multi-step reasoning. While existing approaches enhance the performance of LLMs
through in-context learning and instruction tuning, significant limitations
nonetheless persist. To enhance the model's generalization ability, we propose
integrating reinforcement learning (RL) with multi-perspective reasoning for
information extraction (IE) tasks. Our work transitions LLMs from passive
extractors to active reasoners, enabling them to understand not only what to
extract but also how to reason. Experiments conducted on multiple IE benchmarks
demonstrate that MR-UIE consistently elevates extraction accuracy across
domains and surpasses state-of-the-art methods on several datasets.
Furthermore, incorporating multi-perspective reasoning into RL notably enhances
generalization in complex IE tasks, underscoring the critical role of reasoning
in challenging scenarios.

</details>


### [37] [TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla](https://arxiv.org/abs/2509.09101)
*Nishat Raihan,Antonios Anastasopoulos,Marcos Zampieri*

Main category: cs.CL

TL;DR: This paper introduces TigerCoder, the first specialized Code LLMs (1B & 9B) for Bangla code generation, overcoming data limitations and achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Bangla, despite being the 5th most spoken language, lacks representation in LLMs for code generation due to the scarcity of high-quality datasets.

Method: The authors developed dedicated LLMs by (1) creating Bangla code instruction datasets for programming adaptation, (2) building an evaluation benchmark (MBPP-Bangla), and (3) training TigerCoder models.

Result: TigerCoder models achieved substantial performance gains of ~11–18% Pass@1 over existing multilingual Bangla LLMs, demonstrating that high-quality datasets can mitigate limitations of smaller models for low-resource languages.

Conclusion: The study highlights that creating curated datasets effectively improves code generation in low-resource languages like Bangla, and open-sourcing resources promotes further research.

Abstract: Despite being the 5th most spoken language, Bangla remains underrepresented
in Large Language Models (LLMs), particularly for code generation. This
primarily stems from the scarcity of high-quality data to pre-train and/or
finetune such models. Hence, we introduce the first dedicated family of Code
LLMs for Bangla (1B & 9B). We offer three major contributions: (1) a
comprehensive Bangla code instruction datasets for programming domain
adaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code
generation; and (3) the TigerCoder-family of Code LLMs, achieving significant
~11-18% performance gains at Pass@1 over existing multilingual and
general-purpose Bangla LLMs. Our findings show that curated, high-quality
datasets can overcome limitations of smaller models for low-resource languages.
We open-source all resources to advance further Bangla LLM research.

</details>


### [38] [Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia](https://arxiv.org/abs/2509.09121)
*Sophia Maria*

Main category: cs.CL

TL;DR: Compass-v3 is a domain-specific Mixture-of-Experts model designed for Southeast Asian e-commerce, featuring 245B parameters and advanced hardware optimizations to handle noisy, multilingual, and dynamic data effectively.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle in specialized domains, requiring domain-specific enhancements for impactful applications. E-commerce, especially in Southeast Asia, presents challenges due to its noisy, multilingual, and rapidly changing nature.

Method: Compass-v3 combines fewer but larger experts utilizing hardware-efficient techniques like intra-node expert parallelism for GPU usage. Trained on curated multilingual corpora and synthetic e-commerce instructions, it employs Optimal-Transport Direct Preference Optimization (OTPO) for improved alignment and instruction adherence.

Result: The model achieves state-of-the-art performance in e-commerce by surpassing other models such as GPT-4 and DeepSeek-V3.1. It demonstrates strong multilingual competency in Southeast Asian languages and Portuguese alongside solid performance in general benchmarks.

Conclusion: Compass-v3 is a successful application in industrial-scale platforms like Shopee, replacing OpenAI's model usage for over 70% of LLM tasks, showcasing its dual proficiency in domain-specific expertise and broad language capabilities.

Abstract: Large language models (LLMs) excel in general-domain applications, yet their
performance often degrades in specialized tasks requiring domain-specific
knowledge. E-commerce is particularly challenging, as its data are noisy,
heterogeneous, multilingual, and highly dynamic. We present Compass-v3, a
vertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and
71B active per token, designed for Southeast Asian e-commerce. Compass-v3
adopts fewer but larger experts, combined with hardware-efficient
optimizations-such as intra-node expert parallelism and a customized memcpy
operator-to maximize GPU utilization. The model is trained on 12T tokens of
curated multilingual corpora and large-scale synthetic e-commerce instructions
using a mixed-training strategy. To enhance alignment, we propose
Optimal-Transport Direct Preference Optimization (OTPO), which captures
token-level distinctions and improves instruction adherence in
commerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3
delivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,
GPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong
multilingual capability across low-resource Southeast Asian languages
(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while
sustaining competitive performance on general benchmarks. It has already been
widely applied in Shopee's industrial-scale e-commerce platform and is
gradually replacing OpenAI's traffic, now accounting for over 70\% of total LLM
usage, highlighting its dual strengths in specialized commerce expertise and
broad linguistic competence.

</details>


### [39] [Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus](https://arxiv.org/abs/2509.09125)
*Liqun He,Jiaqi Xu*

Main category: cs.CL

TL;DR: This study evaluated GPT-4 for automating educational dialogue act classification, achieving substantial accuracy and agreement with human annotations.


<details>
  <summary>Details</summary>
Motivation: Traditional manual coding of tutors' Dialogue Acts is time-consuming and labor-intensive, prompting exploration of generative AI as an efficient alternative.

Method: GPT-3.5-turbo and GPT-4 were tested using tailored prompts against the CIMA corpus, pre-annotated into four categories of Dialogue Acts.

Result: GPT-4 achieved 80% accuracy, a weighted F1-score of 0.81, and a Cohen's Kappa of 0.74, surpassing baseline manual annotation performance.

Conclusion: Generative AI shows strong potential to automate Dialogue Act classification efficiently, while emphasizing the importance of task-specific definitions and ethical considerations.

Abstract: This study explores the use of generative AI for automating the
classification of tutors' Dialogue Acts (DAs), aiming to reduce the time and
effort required by traditional manual coding. This case study uses the
open-source CIMA corpus, in which tutors' responses are pre-annotated into four
DA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored
prompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of
0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and
indicating substantial agreement with human annotations. These findings suggest
that generative AI has strong potential to provide an efficient and accessible
approach to DA classification, with meaningful implications for educational
dialogue analysis. The study also highlights the importance of task-specific
label definitions and contextual information in enhancing the quality of
automated annotation. Finally, it underscores the ethical considerations
associated with the use of generative AI and the need for responsible and
transparent research practices. The script of this research is publicly
available at
https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.

</details>


### [40] [ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking](https://arxiv.org/abs/2509.09131)
*Phuong-Nam Dang,Kieu-Linh Nguyen,Thanh-Hieu Pham*

Main category: cs.CL

TL;DR: ViRanker, a reranking model for Vietnamese, outperforms multilingual baselines and competes with existing models, enhancing information retrieval for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Address the lack of competitive rerankers for Vietnamese, a low-resource language with complex syntax and diacritics.

Method: Developed based on BGE-M3 encoder with Blockwise Parallel Transformer, trained on an 8 GB curated corpus, and fine-tuned with hybrid hard-negative sampling.

Result: ViRanker achieves strong performance on the MMARCO-VI benchmark, surpassing multilingual baselines and closely competing with PhoRanker.

Conclusion: ViRanker enhances retrieval for Vietnamese and underrepresented languages through architectural adaptation, robust training, and open access for further improvement and adoption.

Abstract: This paper presents ViRanker, a cross-encoder reranking model tailored to the
Vietnamese language. Built on the BGE-M3 encoder and enhanced with the
Blockwise Parallel Transformer, ViRanker addresses the lack of competitive
rerankers for Vietnamese, a low-resource language with complex syntax and
diacritics. The model was trained on an 8 GB curated corpus and fine-tuned with
hybrid hard-negative sampling to strengthen robustness. Evaluated on the
MMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing
multilingual baselines and competing closely with PhoRanker. By releasing the
model openly on Hugging Face, we aim to support reproducibility and encourage
wider adoption in real-world retrieval systems. Beyond Vietnamese, this study
illustrates how careful architectural adaptation and data curation can advance
reranking in other underrepresented languages.

</details>


### [41] [Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing](https://arxiv.org/abs/2509.09160)
*Zhiyue Liu,Fanrong Ma,Xin Ling*

Main category: cs.CL

TL;DR: The paper addresses biases in multimodal sentiment classification and proposes a framework using counterfactual data augmentation and contrastive learning to improve classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing target-oriented multimodal sentiment classification models rely too heavily on textual content, leading to spurious correlations and biases, particularly word-level contextual biases. These impair overall classification accuracy, creating a need for more robust approaches.

Method: The paper introduces a counterfactual-enhanced debiasing framework. It includes counterfactual data augmentation by modifying causal sentiment-related features and generating detail-matched image-text samples. Additionally, an adaptive debiasing contrastive learning mechanism is used to mitigate biased word influence.

Result: Experimental results demonstrate that the proposed framework surpasses state-of-the-art baselines in classification accuracy across benchmark datasets, improving robustness against biases.

Conclusion: The novel framework effectively reduces spurious correlations in multimodal sentiment classification, emphasizing sentiment-related features and improving accuracy. It demonstrates scalability and superiority over existing models.

Abstract: Target-oriented multimodal sentiment classification seeks to predict
sentiment polarity for specific targets from image-text pairs. While existing
works achieve competitive performance, they often over-rely on textual content
and fail to consider dataset biases, in particular word-level contextual
biases. This leads to spurious correlations between text features and output
labels, impairing classification accuracy. In this paper, we introduce a novel
counterfactual-enhanced debiasing framework to reduce such spurious
correlations. Our framework incorporates a counterfactual data augmentation
strategy that minimally alters sentiment-related causal features, generating
detail-matched image-text samples to guide the model's attention toward content
tied to sentiment. Furthermore, for learning robust features from
counterfactual data and prompting model decisions, we introduce an adaptive
debiasing contrastive learning mechanism, which effectively mitigates the
influence of biased words. Experimental results on several benchmark datasets
show that our proposed method outperforms state-of-the-art baselines.

</details>


### [42] [EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs](https://arxiv.org/abs/2509.09174)
*Yuhao Zhang,Yuhao Du,Zhanchen Dai,Xiangnan Ma,Kaiqi Kou,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: EchoX bridges the gap between speech and semantics to enhance knowledge and reasoning in speech-based large language models, outperforming existing approaches on QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current speech-based LLMs underperform in knowledge and reasoning tasks due to inadequate bridging of acoustic and semantic representations.

Method: EchoX uses semantic representations and dynamically generated speech training targets to integrate acoustic and semantic learning, maintaining strong reasoning capabilities.

Result: Experimental results showed EchoX achieves advanced performance in knowledge-based QA benchmarks with six thousand hours of training.

Conclusion: EchoX successfully preserves reasoning ability in speech LLMs, proving effective in addressing the acoustic-semantic gap.

Abstract: Speech-to-speech large language models (SLLMs) are attracting increasing
attention. Derived from text-based large language models (LLMs), SLLMs often
exhibit degradation in knowledge and reasoning capabilities. We hypothesize
that this limitation arises because current training paradigms for SLLMs fail
to bridge the acoustic-semantic gap in the feature representation space. To
address this issue, we propose EchoX, which leverages semantic representations
and dynamically generates speech training targets. This approach integrates
both acoustic and semantic learning, enabling EchoX to preserve strong
reasoning abilities as a speech LLM. Experimental results demonstrate that
EchoX, with about six thousand hours of training data, achieves advanced
performance on multiple knowledge-based question-answering benchmarks. The
project is available at https://github.com/FreedomIntelligence/EchoX.

</details>


### [43] [Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition](https://arxiv.org/abs/2509.09196)
*Chin Yuen Kwok,Jia Qi yip*

Main category: cs.CL

TL;DR: The paper proposes an improved contextual biasing algorithm for ASR models, which enhances rare word recognition performance and bypasses computational inefficiencies related to trie-based biasing.


<details>
  <summary>Details</summary>
Motivation: Current Trie-based contextual biasing methods for ASR struggle with efficiency and accuracy, especially with models having large decoders, and often require costly revocation steps.

Method: Adapt ASR models to predict multiple decoding steps simultaneously, eliminating the need for revocation steps. Fine-tune the existing Whisper model using 10 hours of synthetic data.

Result: The proposed method successfully reduces the word error rate on the NSC Part 2 test set from 30.86% to 12.19%, highlighting significant improvement in rare word recognition.

Conclusion: The new decoding approach effectively enhances rare word recognition in ASR while addressing computational inefficiencies, proving successful with minimal fine-tuning.

Abstract: Contextual biasing improves rare word recognition of ASR models by
prioritizing the output of rare words during decoding. A common approach is
Trie-based biasing, which gives "bonus scores" to partial hypothesis (e.g.
"Bon") that may lead to the generation of the rare word (e.g. "Bonham"). If the
full word ("Bonham") isn't ultimately recognized, the system revokes those
earlier bonuses. This revocation is limited to beam search and is
computationally expensive, particularly for models with large decoders. To
overcome these limitations, we propose adapting ASR models to look ahead and
predict multiple steps at once. This avoids the revocation step entirely by
better estimating whether a partial hypothesis will lead to the generation of
the full rare word. By fine-tuning Whisper with only 10 hours of synthetic
data, our method reduces the word error rate on the NSC Part 2 test set from
30.86% to 12.19%.

</details>


### [44] [Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function](https://arxiv.org/abs/2509.09197)
*Chin Yuen Kwok,Jia Qi Yip,Eng Siong Chng*

Main category: cs.CL

TL;DR: This paper improves rare word recognition in ASR systems by introducing an enhanced TCPGen-based contextual biasing method and a keyword-aware loss function, significantly reducing word error rates.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the difficulty in recognizing rare words in ASR systems, which are crucial for improving accuracy in specific tasks but often suffer from limitations like overfitting on synthetic data.

Method: The method involves enhancing the TCPGen-based contextual biasing approach and introducing a keyword-aware loss function. This includes a masked cross-entropy term for rare-word prediction and a binary classification term for position detection.

Result: The proposed method reduced the word error rate on the NSC Part 2 test set from 29.71% to 11.81%, demonstrating significant improvements.

Conclusion: The approach effectively mitigates synthetic data overfitting while enhancing rare word recognition, showing the potential for improving ASR models with contextual adaptations.

Abstract: Rare word recognition can be improved by adapting ASR models to synthetic
data that includes these words. Further improvements can be achieved through
contextual biasing, which trains and adds a biasing module into the model
architecture to prioritize rare words. While training the module on synthetic
rare word data is more effective than using non-rare-word data, it can lead to
overfitting due to artifacts in the synthetic audio. To address this, we
enhance the TCPGen-based contextual biasing approach and propose a
keyword-aware loss function that additionally focuses on biased words when
training biasing modules. This loss includes a masked cross-entropy term for
biased word prediction and a binary classification term for detecting biased
word positions. These two terms complementarily support the decoding of biased
words during inference. By adapting Whisper to 10 hours of synthetic data, our
method reduced the word error rate on the NSC Part 2 test set from 29.71% to
11.81%.

</details>


### [45] [GmSLM : Generative Marmoset Spoken Language Modeling](https://arxiv.org/abs/2509.09198)
*Talia Sternberg,Michael London,David Omer,Yossi Adi*

Main category: cs.CL

TL;DR: The paper introduces GmSLM, a language model for studying vocal communication in Marmoset monkeys. It performs well on vocal generation and analysis tasks despite being unsupervised.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore marmoset monkeys' vocal communication, which resembles human-like speech features, to better understand the neural basis of vocal interactions, something difficult to study in humans.

Method: A novel spoken language model pipeline (GmSLM) is designed, incorporating unsupervised evaluation metrics using wild data and weakly labeled conversational data.

Result: GmSLM demonstrated acoustically accurate vocal generation, effective performance in distinguishing real vs. artificial conversations, and relevance for downstream tasks.

Conclusion: GmSLM offers a powerful framework for linking marmosets' vocalizations with brain activity and has cross-disciplinary applications in neuroscience, bioacoustics, and evolutionary biology.

Abstract: Marmoset monkeys exhibit complex vocal communication, challenging the view
that nonhuman primates vocal communication is entirely innate, and show similar
features of human speech, such as vocal labeling of others and turn-taking.
Studying their vocal communication offers a unique opportunity to link it with
brain activity-especially given the difficulty of accessing the human brain in
speech and language research. Since Marmosets communicate primarily through
vocalizations, applying standard LLM approaches is not straightforward. We
introduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized
spoken language model pipeline for Marmoset vocal communication. We designed a
novel zero-shot evaluation metrics using unsupervised in-the-wild data,
alongside weakly labeled conversational data, to assess GmSLM and demonstrate
its advantage over a basic human-speech-based baseline. GmSLM generated
vocalizations closely matched real resynthesized samples acoustically and
performed well on downstream tasks. Despite being fully unsupervised, GmSLM
effectively distinguish real from artificial conversations and may support
further investigations of the neural basis of vocal communication and provides
a practical framework linking vocalization and brain activity. We believe GmSLM
stands to benefit future work in neuroscience, bioacoustics, and evolutionary
biology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.

</details>


### [46] [CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling](https://arxiv.org/abs/2509.09199)
*Wenhao Li,Bangcheng Sun,Weihao Ye,Tianyi Zhang,Daohai Yu,Fei Chao,Rongrong Ji*

Main category: cs.CL

TL;DR: The researchers propose a method (CCF) to efficiently model long-context language data by compressing input while maintaining semantic integrity.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the computational and memory inefficiencies posed by scaling language models to handle longer contexts, which are needed for richer discourse analysis.

Method: The method combines segment-wise semantic aggregation with memory encoding to create compact, yet semantically-rich, hierarchical representations. It includes a specific training strategy involving incremental decoding and sparse sampling for efficiency.

Result: Experiments on benchmarks showed that the method achieves competitive perplexity under high compression and improves throughput and memory usage over other approaches.

Conclusion: CCF is an effective and scalable solution for modeling long-context dependencies in language tasks.

Abstract: Scaling language models to longer contexts is essential for capturing rich
dependencies across extended discourse. However, na\"ive context extension
imposes significant computational and memory burdens, often resulting in
inefficiencies during both training and inference. In this work, we propose
CCF, a novel context compression framework designed to enable efficient
long-context modeling by learning hierarchical latent representations that
preserve global semantics while aggressively reducing input redundancy. CCF
integrates segment-wise semantic aggregation with key-value memory encoding,
forming compact representations that support accurate reconstruction and
long-range understanding. To further enhance scalability, we introduce a
training-efficient optimization strategy that couples incremental segment
decoding with sparse reservoir sampling, substantially reducing memory overhead
without degrading performance. Empirical results on multiple long-context
language modeling benchmarks demonstrate that CCF achieves competitive
perplexity under high compression ratios, and significantly improves throughput
and memory efficiency compared to existing approaches. These findings highlight
the potential of structured compression for scalable and effective long-context
language modeling.

</details>


### [47] [Reading Between the Lines: Classifying Resume Seniority with Large Language Models](https://arxiv.org/abs/2509.09229)
*Matan Cohen,Shira Shani,Eden Menahem,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: The study examines the capability of large language models (LLMs), especially fine-tuned BERT architectures, to classify candidate seniority from resumes, tackling challenges of overstated and ambiguous qualifications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve automated seniority classification from resumes, addressing issues of inflated qualifications and ambiguous self-presentations.

Method: The method involves using LLMs, including fine-tuned BERT models, applied on a hybrid dataset of real-world resumes and synthetically manipulated examples to detect subtle linguistic cues.

Result: The study demonstrates that LLMs perform well in detecting exaggerated or understated seniority, providing useful insights into AI-based candidate evaluation.

Conclusion: LLMs can significantly enhance automated candidate evaluation and mitigate bias introduced by self-promotional or overstated resume content. The dataset is shared for community use.

Abstract: Accurately assessing candidate seniority from resumes is a critical yet
challenging task, complicated by the prevalence of overstated experience and
ambiguous self-presentation. In this study, we investigate the effectiveness of
large language models (LLMs), including fine-tuned BERT architectures, for
automating seniority classification in resumes. To rigorously evaluate model
performance, we introduce a hybrid dataset comprising both real-world resumes
and synthetically generated hard examples designed to simulate exaggerated
qualifications and understated seniority. Using the dataset, we evaluate the
performance of Large Language Models in detecting subtle linguistic cues
associated with seniority inflation and implicit expertise. Our findings
highlight promising directions for enhancing AI-driven candidate evaluation
systems and mitigating bias introduced by self-promotional language. The
dataset is available for the research community at https://bit.ly/4mcTovt

</details>


### [48] [Agentic LLMs for Question Answering over Tabular Data](https://arxiv.org/abs/2509.09234)
*Rishit Tyagi,Mohit Gupta,Rahul Bouri*

Main category: cs.CL

TL;DR: The paper proposes a Natural Language to SQL (NL-to-SQL) approach using large language models for Question Answering over tabular data, achieving significant accuracy improvements over baseline methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in Table QA arise from the diverse structure and data types of real-world tables, prompting the development of effective evaluation benchmarks.

Method: A multi-stage pipeline leveraging large language models to generate SQL queries dynamically, including steps like example selection, query generation, answer extraction, verification, and refinement.

Result: Achieved 70.5% accuracy on DataBench QA and 71.6% on DataBench Lite QA, which are much higher than the baseline scores of 26% and 27% respectively.

Conclusion: The approach proves to be highly effective in improving table QA performance, showcasing the strengths and limitations of large language models for this domain.

Abstract: Question Answering over Tabular Data (Table QA) presents unique challenges
due to the diverse structure, size, and data types of real-world tables. The
SemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,
domain-diverse datasets to evaluate the ability of models to accurately answer
structured queries. We propose a Natural Language to SQL (NL-to-SQL) approach
leveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and
DeepSeek v2:16b to generate SQL queries dynamically. Our system follows a
multi-stage pipeline involving example selection, SQL query generation, answer
extraction, verification, and iterative refinement. Experiments demonstrate the
effectiveness of our approach, achieving 70.5\% accuracy on DataBench QA and
71.6\% on DataBench Lite QA, significantly surpassing baseline scores of 26\%
and 27\% respectively. This paper details our methodology, experimental
results, and alternative approaches, providing insights into the strengths and
limitations of LLM-driven Table QA.

</details>


### [49] [From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models](https://arxiv.org/abs/2509.09303)
*Grazia Sveva Ascione,Nicolò Tamagnone*

Main category: cs.CL

TL;DR: This paper proposes a weak supervision method leveraging large language models (LLMs) to classify patents by their relevance to UN Sustainable Development Goals (SDGs), overcoming challenges like lack of labeled datasets and scalability of traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the absence of large labeled datasets for patent-to-SDG classification and improve scalability and generalizability of existing methods like keyword searches and citation-based heuristics.

Method: The approach uses noisy signals from patent citations to SDG-tagged publications, employing large language models to extract structured concepts and compute cross-domain similarity scores via a composite labeling function. A rank-based retrieval approach and custom positive-only loss are applied to create a soft multi-label dataset.

Result: The method outperforms baselines in internal validations and demonstrates thematic, cognitive, and organizational coherence in external validations, showcasing its effectiveness in creating scalable SDG classifications.

Conclusion: Weak supervision and semantic alignment, powered by large language models, provide an effective and scalable method for SDG-related patent classification, offering improved generalizability and utility compared to traditional approaches.

Abstract: Classifying patents by their relevance to the UN Sustainable Development
Goals (SDGs) is crucial for tracking how innovation addresses global
challenges. However, the absence of a large, labeled dataset limits the use of
supervised learning. Existing methods, such as keyword searches, transfer
learning, and citation-based heuristics, lack scalability and generalizability.
This paper frames patent-to-SDG classification as a weak supervision problem,
using citations from patents to SDG-tagged scientific publications (NPL
citations) as a noisy initial signal. To address its sparsity and noise, we
develop a composite labeling function (LF) that uses large language models
(LLMs) to extract structured concepts, namely functions, solutions, and
applications, from patents and SDG papers based on a patent ontology.
Cross-domain similarity scores are computed and combined using a rank-based
retrieval approach. The LF is calibrated via a custom positive-only loss that
aligns with known NPL-SDG links without penalizing discovery of new SDG
associations. The result is a silver-standard, soft multi-label dataset mapping
patents to SDGs, enabling the training of effective multi-label regression
models. We validate our approach through two complementary strategies: (1)
internal validation against held-out NPL-based labels, where our method
outperforms several baselines including transformer-based models, and zero-shot
LLM; and (2) external validation using network modularity in patent citation,
co-inventor, and co-applicant graphs, where our labels reveal greater thematic,
cognitive, and organizational coherence than traditional technological
classifications. These results show that weak supervision and semantic
alignment can enhance SDG classification at scale.

</details>


### [50] [MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems](https://arxiv.org/abs/2509.09360)
*Channdeth Sok,David Luz,Yacine Haddam*

Main category: cs.CL

TL;DR: The paper introduces MetaRAG, a framework to detect hallucinations in Retrieval-Augmented Generation (RAG) systems. It works unsupervised and without model access, targeting unsupported claims at a granular level for real-time use.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of hallucinations in Retrieval-Augmented Generation (RAG) systems, which require generated responses to align with retrieved evidence.

Method: MetaRAG uses a four-stage framework: decompose responses into atomic factoids, mutate them using synonyms/antonyms, verify variants against the retrieved context, and aggregate inconsistencies into a hallucination score.

Result: MetaRAG effectively detects hallucinations in a proprietary enterprise dataset and allows users to pinpoint unsupported claims at a granular factoid span level.

Conclusion: MetaRAG enhances trustworthiness of RAG systems by providing real-time hallucination detection and tailoring identity-aware safeguards for sensitive information.

Abstract: Large Language Models (LLMs) are increasingly deployed in enterprise
applications, yet their reliability remains limited by hallucinations, i.e.,
confident but factually incorrect information. Existing detection approaches,
such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not
address the unique challenges of Retrieval-Augmented Generation (RAG) systems,
where responses must be consistent with retrieved evidence. We therefore
present MetaRAG, a metamorphic testing framework for hallucination detection in
Retrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,
unsupervised, black-box setting, requiring neither ground-truth references nor
access to model internals, making it suitable for proprietary and high-stakes
domains. The framework proceeds in four stages: (1) decompose answers into
atomic factoids, (2) generate controlled mutations of each factoid using
synonym and antonym substitutions, (3) verify each variant against the
retrieved context (synonyms are expected to be entailed and antonyms
contradicted), and (4) aggregate penalties for inconsistencies into a
response-level hallucination score. Crucially for identity-aware AI, MetaRAG
localizes unsupported claims at the factoid span where they occur (e.g.,
pregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),
allowing users to see flagged spans and enabling system designers to configure
thresholds and guardrails for identity-sensitive queries. Experiments on a
proprietary enterprise dataset illustrate the effectiveness of MetaRAG for
detecting hallucinations and enabling trustworthy deployment of RAG-based
conversational agents. We also outline a topic-based deployment design that
translates MetaRAG's span-level scores into identity-aware safeguards; this
design is discussed but not evaluated in our experiments.

</details>


### [51] [Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research](https://arxiv.org/abs/2509.09381)
*Molly R Petersen,Claire E Stevenson,Lonneke van der Plas*

Main category: cs.CL

TL;DR: The paper discusses the cognitive processes behind analogical reasoning, linking them to NLP research and showcasing their broader relevance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the cognitive science theories of analogical reasoning with NLP to improve relational understanding in text.

Method: The authors summarize theories from cognitive science on analogical reasoning and explore their relevance to NLP challenges.

Result: It highlights how cognitive perspectives on analogy can address challenges in NLP beyond analogical reasoning tasks.

Conclusion: Incorporating cognitive insights on analogical reasoning can enhance NLP's focus on relational understanding over entity similarity.

Abstract: Analogical reasoning is an essential aspect of human cognition. In this
paper, we summarize key theory about the processes underlying analogical
reasoning from the cognitive science literature and relate it to current
research in natural language processing. While these processes can be easily
linked to concepts in NLP, they are generally not viewed through a cognitive
lens. Furthermore, we show how these notions are relevant for several major
challenges in NLP research, not directly related to analogy solving. This may
guide researchers to better optimize relational understanding in text, as
opposed to relying heavily on entity-level similarity.

</details>


### [52] [Hierarchical Bracketing Encodings Work for Dependency Graphs](https://arxiv.org/abs/2509.09388)
*Ana Ezquerro,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: The paper explores hierarchical bracketing encodings in dependency graph parsing, enabling linear-time parsing while handling complex graph features effectively. Results show improvement over existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in dependency graph parsing, such as efficiently representing complex graph features like reentrancies, cycles, and empty nodes, while reducing label space.

Method: Encodes dependency graphs as sequences employing hierarchical bracketing, allowing parsing with $n$ tagging actions while retaining complex structure information.

Result: Demonstrates competitive results on multilingual and multi-formalism benchmarks, with consistent improvements in exact match accuracy over other methods.

Conclusion: The hierarchical encoding approach effectively balances efficiency and structural representation, proving to be a promising method for dependency graph parsing.

Abstract: We revisit hierarchical bracketing encodings from a practical perspective in
the context of dependency graph parsing. The approach encodes graphs as
sequences, enabling linear-time parsing with $n$ tagging actions, and still
representing reentrancies, cycles, and empty nodes. Compared to existing graph
linearizations, this representation substantially reduces the label space while
preserving structural information. We evaluate it on a multilingual and
multi-formalism benchmark, showing competitive results and consistent
improvements over other methods in exact match accuracy.

</details>


### [53] [GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models](https://arxiv.org/abs/2509.09438)
*Zhaohan Zhang,Ziquan Liu,Ioannis Patras*

Main category: cs.CL

TL;DR: The paper introduces GrACE, a novel method for generating reliable and scalable confidence estimations for Large Language Models (LLMs), outperforming other methods in calibration and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for confidence elicitation in LLMs are computationally expensive or poorly calibrated, limiting reliability and viability in real-world applications such as healthcare and finance.

Method: GrACE introduces a new mechanism where the model expresses confidence based on the similarity between its final hidden state and the embedding of a dedicated token, fine-tuned with calibration targets that relate to accuracy.

Result: Experiments with three LLMs and two datasets demonstrate that GrACE provides superior calibration and discrimination in open-ended tasks compared to six competing methods. Test-time scaling improves accuracy and reduces sample requirements.

Conclusion: GrACE is a promising, practical solution for reliable, real-time, and scalable confidence elicitation in LLMs, showing strong potential for practical deployment in high-stakes environments.

Abstract: Assessing the reliability of Large Language Models (LLMs) by confidence
elicitation is a prominent approach to AI safety in high-stakes applications,
such as healthcare and finance. Existing methods either require expensive
computational overhead or suffer from poor calibration, making them impractical
and unreliable for real-world deployment. In this work, we propose GrACE, a
Generative Approach to Confidence Elicitation that enables scalable and
reliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in
which the model expresses confidence by the similarity between the last hidden
state and the embedding of a special token appended to the vocabulary, in
real-time. We fine-tune the model for calibrating the confidence with
calibration targets associated with accuracy. Experiments with three LLMs and
two benchmark datasets show that the confidence produced by GrACE achieves the
best discriminative capacity and calibration on open-ended generation tasks,
outperforming six competing methods without resorting to additional sampling or
an auxiliary model. Moreover, we propose two strategies for improving test-time
scaling based on confidence induced by GrACE. Experimental results show that
using GrACE not only improves the accuracy of the final decision but also
significantly reduces the number of required samples in the test-time scaling
scheme, indicating the potential of GrACE as a practical solution for deploying
LLMs with scalable, reliable, and real-time confidence estimation.

</details>


### [54] [Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation](https://arxiv.org/abs/2509.09473)
*Lucie Poláková,Martin Popel,Věra Kloudová,Michal Novák,Mariia Anisimova,Jiří Balhar*

Main category: cs.CL

TL;DR: The EdUKate project develops multilingual learning materials using machine translation for Czech schools, translating interactive exercises into Ukrainian, English, and German.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of providing multilingual educational materials tailored for non-Czech-speaking students in Czech primary and secondary schools.

Method: The project combines machine translation, linguistics, and education. It develops a direct Czech-Ukrainian MT system optimized for educational content and formatted content processing while utilizing feedback from Czech teachers.

Result: Successfully translates up to 9,000 interactive exercises and evaluates machine translation performance on educational materials. Applications are made freely available.

Conclusion: The initiative bridges linguistic gaps in education through tailored machine translation systems, benefiting non-Czech-speaking students, educators, and researchers.

Abstract: The EdUKate project combines digital education, linguistics, translation
studies, and machine translation to develop multilingual learning materials for
Czech primary and secondary schools. Launched through collaboration between a
major Czech academic institution and the country's largest educational
publisher, the project is aimed at translating up to 9,000 multimodal
interactive exercises from Czech into Ukrainian, English, and German for an
educational web portal. It emphasizes the development and evaluation of a
direct Czech-Ukrainian machine translation system tailored to the educational
domain, with special attention to processing formatted content such as XML and
PDF and handling technical and scientific terminology. We present findings from
an initial survey of Czech teachers regarding the needs of non-Czech-speaking
students and describe the system's evaluation and implementation on the web
portal. All resulting applications are freely available to students, educators,
and researchers.

</details>


### [55] [Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs](https://arxiv.org/abs/2509.09522)
*Vadim Zadykian,Bruno Andrade,Haithem Afli*

Main category: cs.CL

TL;DR: This paper proposes a novel approach combining dense sentence embeddings and Knowledge Graphs (KGs) to enhance semantic textual relatedness (STR) in job title matching for resume recommendation systems, emphasizing granular performance analysis across different STR regions.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of limited or misleading lexical overlap in job title matching within resume recommendation systems, aiming to improve semantic alignment and provide explainability in STR tasks.

Method: The authors introduce a self-supervised hybrid architecture that integrates fine-tuned SBERT models with domain-specific KGs via graph neural networks, and assess model performance using a stratified STR score continuum (low, medium, high) rather than aggregate metrics.

Result: The hybrid approach achieves a 25% reduction in RMSE in the high-STR score region compared to strong baselines, demonstrating consistent improvements in semantic alignment when combining KGs with text embeddings.

Conclusion: Combining domain-specific Knowledge Graphs with dense sentence embeddings enhances STR performance, particularly in high-STR regions, while stratified performance analysis provides deeper insights into model behavior for applications in HR systems.

Abstract: Semantic Textual Relatedness (STR) captures nuanced relationships between
texts that extend beyond superficial lexical similarity. In this study, we
investigate STR in the context of job title matching - a key challenge in
resume recommendation systems, where overlapping terms are often limited or
misleading. We introduce a self-supervised hybrid architecture that combines
dense sentence embeddings with domain-specific Knowledge Graphs (KGs) to
improve both semantic alignment and explainability. Unlike previous work that
evaluated models on aggregate performance, our approach emphasizes data
stratification by partitioning the STR score continuum into distinct regions:
low, medium, and high semantic relatedness. This stratified evaluation enables
a fine-grained analysis of model performance across semantically meaningful
subspaces. We evaluate several embedding models, both with and without KG
integration via graph neural networks. The results show that fine-tuned SBERT
models augmented with KGs produce consistent improvements in the high-STR
region, where the RMSE is reduced by 25% over strong baselines. Our findings
highlight not only the benefits of combining KGs with text embeddings, but also
the importance of regional performance analysis in understanding model
behavior. This granular approach reveals strengths and weaknesses hidden by
global metrics, and supports more targeted model selection for use in Human
Resources (HR) systems and applications where fairness, explainability, and
contextual matching are essential.

</details>


### [56] [DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning](https://arxiv.org/abs/2509.09524)
*Daniil Ignatev,Nan Li,Hugh Mee Wong,Anh Dang,Shane Kaszefski Yaschuk*

Main category: cs.CL

TL;DR: The paper explores in-context learning (ICL) with large language models and label distribution learning (LDL) methods using RoBERTa for the Learning with Disagreements (LeWiDi 2025) shared task.


<details>
  <summary>Details</summary>
Motivation: To investigate effective approaches for handling disagreements in learning tasks, specifically using ICL with large language models and LDL methods.

Method: The authors used ICL with various sampling strategies and fine-tuned RoBERTa models under LDL methods to evaluate their performance for predicting perspectivist annotations and soft label aggregation.

Result: They demonstrated that ICL effectively predicts annotator-specific annotations, with aggregated predictions performing competitively, and highlighted the promise of LDL methods for soft label predictions.

Conclusion: ICL and LDL methods show significant potential in handling disagreements in learning tasks, with LDL deserving further exploration in perspectivist annotation contexts.

Abstract: This system paper presents the DeMeVa team's approaches to the third edition
of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et
al., 2025). We explore two directions: in-context learning (ICL) with large
language models, where we compare example sampling strategies; and label
distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we
evaluate several fine-tuning methods. Our contributions are twofold: (1) we
show that ICL can effectively predict annotator-specific annotations
(perspectivist annotations), and that aggregating these predictions into soft
labels yields competitive performance; and (2) we argue that LDL methods are
promising for soft label predictions and merit further exploration by the
perspectivist community.

</details>


### [57] [Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)](https://arxiv.org/abs/2509.09544)
*Paolo Pedinotti,Peter Baumann,Nathan Jessurun,Leslie Barrett,Enrico Santus*

Main category: cs.CL

TL;DR: The paper introduces MetaGraph, a methodology for extracting knowledge graphs to analyze research trends in financial NLP, using 681 papers from 2022-2025.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the rapid evolution of financial NLP brought by LLMs, which traditional surveys struggle to keep up with.

Method: The authors defined an ontology for financial NLP research and developed an LLM-based pipeline to extract data from 681 relevant papers, analyzing trends using a structured approach.

Result: MetaGraph identified three evolving phases in financial NLP: innovation in tasks and datasets, reflection on limitations of LLMs, and increasing modular integration with peripheral techniques.

Conclusion: The research provides insights into the evolution of financial NLP research, identifies trends, and offers a reusable methodology for other scientific domains.

Abstract: Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling
new tasks and driving a proliferation of datasets and diversification of data
sources. Yet, this transformation has outpaced traditional surveys. In this
paper, we present MetaGraph, a generalizable methodology for extracting
knowledge graphs from scientific literature and analyzing them to obtain a
structured, queryable view of research trends. We define an ontology for
financial NLP research and apply an LLM-based extraction pipeline to 681 papers
(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals
three key phases: early LLM adoption and task/dataset innovation; critical
reflection on LLM limitations; and growing integration of peripheral techniques
into modular systems. This structured view offers both practitioners and
researchers a clear understanding of how financial NLP has evolved -
highlighting emerging trends, shifting priorities, and methodological
shifts-while also demonstrating a reusable approach for mapping scientific
progress in other domains.

</details>


### [58] [Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking](https://arxiv.org/abs/2509.09583)
*Brittany Harbison,Samuel Taubman,Travis Taylor,Ashok. K. Goel*

Main category: cs.CL

TL;DR: The paper explores the integration of a GPT-based personality detection model into SAMI, an online course matchmaking system, to enhance social connections and student engagement.


<details>
  <summary>Details</summary>
Motivation: Online learning environments often lack effective tools to foster social connections, which are critical to student learning and engagement.

Method: The researchers propose a zero-shot personality detection model using GPT to infer Big-Five personality traits from students' forum introduction posts. They benchmarked this model against established alternatives and integrated it into SAMI's matchmaking system.

Result: The GPT-based personality detection model showed efficacy in identifying personality traits and was successfully implemented into SAMI, enabling personality-informed matchmaking.

Conclusion: Integrating personality recognition enhances matchmaking in online learning platforms, but further evaluation is needed to fully understand its impact on social connections and engagement.

Abstract: Social connection is a vital part of learning, yet online course environments
present barriers to the organic formation of social groups. SAMI offers one
solution by facilitating student connections, but its effectiveness is
constrained by an incomplete Theory of Mind, limiting its ability to create an
effective mental model of a student. One facet of this is its inability to
intuit personality, which may influence the relevance of its recommendations.
To explore this, we propose a personality detection model utilizing GPTs
zero-shot capability to infer Big-Five personality traits from forum
introduction posts, often encouraged in online courses. We benchmark its
performance against established models, demonstrating its efficacy in this
task. Furthermore, we integrate this model into SAMIs entity-based matchmaking
system, enabling personality-informed social recommendations. Initial
integration suggests personality traits can complement existing matching
factors, though additional evaluation is required to determine their full
impact on student engagement and match quality.

</details>


### [59] [Fluent but Unfeeling: The Emotional Blind Spots of Language Models](https://arxiv.org/abs/2509.09593)
*Bangzhao Shu,Isha Joshi,Melissa Karnaze,Anh C. Pham,Ishita Kakkar,Sindhu Kothe,Arpine Hovasapian,Mai ElSherief*

Main category: cs.CL

TL;DR: The paper introduces EXPRESS, a Reddit-sourced dataset with 251 fine-grained emotion labels, to evaluate the alignment of LLM-predicted emotions with human self-disclosures.


<details>
  <summary>Details</summary>
Motivation: Existing research on LLMs in emotion recognition is limited by predefined, coarse emotion categories, failing to capture finer nuances in human emotional expressions.

Method: The authors curated the EXPRESS dataset with 251 emotion labels from Reddit communities and devised an evaluation framework using emotion theories to compare LLM predictions with fine-grained human emotions.

Result: Testing reveals that while some LLMs align with emotion theories, they struggle to predict fine-grained, context-dependent emotions aligned with human cues.

Conclusion: LLMs face significant challenges in aligning with human self-disclosed emotions at a fine-grained level, underscoring the need for improved contextual understanding in future models.

Abstract: The versatility of Large Language Models (LLMs) in natural language
understanding has made them increasingly popular in mental health research.
While many studies explore LLMs' capabilities in emotion recognition, a
critical gap remains in evaluating whether LLMs align with human emotions at a
fine-grained level. Existing research typically focuses on classifying emotions
into predefined, limited categories, overlooking more nuanced expressions. To
address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit
communities featuring 251 fine-grained, self-disclosed emotion labels. Our
comprehensive evaluation framework examines predicted emotion terms and
decomposes them into eight basic emotions using established emotion theories,
enabling a fine-grained comparison. Systematic testing of prevalent LLMs under
various prompt settings reveals that accurately predicting emotions that align
with human self-disclosed emotions remains challenging. Qualitative analysis
further shows that while certain LLMs generate emotion terms consistent with
established emotion theories and definitions, they sometimes fail to capture
contextual cues as effectively as human self-disclosures. These findings
highlight the limitations of LLMs in fine-grained emotion alignment and offer
insights for future research aimed at enhancing their contextual understanding.

</details>


### [60] [LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination](https://arxiv.org/abs/2509.09602)
*Yiqun T. Chen,Tyler H. McCormick,Li Liu,Abhirup Datta*

Main category: cs.CL

TL;DR: This paper introduces LA-VA, a hybrid method leveraging Large Language Models (LLMs) and traditional algorithms to better predict causes of death using verbal autopsies in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Improve verbal autopsy accuracy in low-resource areas where medical certification of death is unavailable.

Method: Develops and evaluates LA-VA, a pipeline combining GPT-5, traditional algorithms, text embeddings, and ensemble methods on the PHMRC dataset.

Result: GPT-5 demonstrated the best standalone performance with accuracy improvements of 5-10% over baseline methods.

Conclusion: LLM-assisted approaches like LA-VA can significantly enhance the accuracy of cause-of-death predictions, aiding global health surveillance.

Abstract: Verbal autopsy (VA) is a critical tool for estimating causes of death in
resource-limited settings where medical certification is unavailable. This
study presents LA-VA, a proof-of-concept pipeline that combines Large Language
Models (LLMs) with traditional algorithmic approaches and embedding-based
classification for improved cause-of-death prediction. Using the Population
Health Metrics Research Consortium (PHMRC) dataset across three age categories
(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:
GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.
Our results demonstrate that GPT-5 achieves the highest individual performance
with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%
(Neonate), outperforming traditional statistical machine learning baselines by
5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches
could substantially improve verbal autopsy accuracy, with important
implications for global health surveillance in low-resource settings.

</details>


### [61] [Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems](https://arxiv.org/abs/2509.09629)
*Minghang Zhu,Zhengliang Shi,Zhiwei Xu,Shiguang Wu,Lingjie Wang,Pengjie Ren,Zhaochun Ren,Zhumin Chen*

Main category: cs.CL

TL;DR: The paper introduces MOAT, a framework for aligning multi-agent systems in solving complex tasks, which shows improved coordination between agents and performance gains.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems fail to address coordination issues among agents when tuned independently, leading to capability gaps and ineffective task execution.

Method: The authors propose MOAT, which alternates between optimizing a planning agent to guide a grounding agent and fine-tuning the grounding agent with diverse subgoal-action pairs to improve collaboration.

Result: The framework achieves an average improvement of 3.1% on held-in tasks and 4.4% on held-out tasks across six benchmarks, outperforming state-of-the-art methods.

Conclusion: Iterative alignment in multi-agent systems via MOAT enhances collaboration, ensuring non-decreasing training efficiency and better generalization, addressing existing coordination problems.

Abstract: The advancement of large language models (LLMs) has enabled the construction
of multi-agent systems to solve complex tasks by dividing responsibilities
among specialized agents, such as a planning agent for subgoal generation and a
grounding agent for executing tool-use actions. Most existing methods typically
fine-tune these agents independently, leading to capability gaps among them
with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint
Alignment Tuning framework that improves agents collaboration through iterative
alignment. MOAT alternates between two key stages: (1) Planning Agent
Alignment, which optimizes the planning agent to generate subgoal sequences
that better guide the grounding agent; and (2) Grounding Agent Improving, which
fine-tunes the grounding agent using diverse subgoal-action pairs generated by
the agent itself to enhance its generalization capablity. Theoretical analysis
proves that MOAT ensures a non-decreasing and progressively convergent training
process. Experiments across six benchmarks demonstrate that MOAT outperforms
state-of-the-art baselines, achieving average improvements of 3.1% on held-in
tasks and 4.4% on held-out tasks.

</details>


### [62] [All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens](https://arxiv.org/abs/2509.09650)
*Siddarth Mamidanna,Daking Rai,Ziyu Yao,Yilun Zhou*

Main category: cs.CL

TL;DR: This paper studies how computations for mental math tasks occur in LLMs using techniques like Context-Aware Mean Ablation (CAMA) and Attention-Based Peeking (ABP), identifying a crucial computational subgraph.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how computations within large language models (LLMs) work during direct math calculation tasks, as their precise inner workings remain unclear despite their demonstrated proficiency.

Method: The study uses mental math tasks and introduces techniques like CAMA and ABP to inhibit, restrict, and focus token computations at different layers within LLMs to identify computation patterns.

Result: An All-for-One subgraph (AF1) was identified as both necessary and sufficient for high-performance mental math tasks; meaningful computation occurs late in the layers and is focused at the final token, involving specific middle layers.

Conclusion: Key computational paths for mental math tasks in LLMs were revealed, demonstrating that the identified AF1 subgraph is transferable and effective across models with various input styles while outperforming other computation-restriction methods.

Abstract: Large language models (LLMs) demonstrate proficiency across numerous
computational tasks, yet their inner workings remain unclear. In theory, the
combination of causal self-attention and multilayer perceptron layers allows
every token to access and compute information based on all preceding tokens. In
practice, to what extent are such operations present? In this paper, on mental
math tasks (i.e., direct math calculation via next-token prediction without
explicit reasoning), we investigate this question in three steps: inhibiting
input-specific token computations in the initial layers, restricting the routes
of information transfer across token positions in the next few layers, and
forcing all computation to happen at the last token in the remaining layers.
With two proposed techniques, Context-Aware Mean Ablation (CAMA) and
Attention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with
high accuracy on a wide variety of mental math tasks, where meaningful
computation occurs very late (in terms of layer depth) and only at the last
token, which receives information of other tokens in few specific middle
layers. Experiments on a variety of models and arithmetic expressions show that
this subgraph is sufficient and necessary for high model performance, transfers
across different models, and works on a variety of input styles. Ablations on
different CAMA and ABP alternatives reveal their unique advantages over other
methods, which may be of independent interest.

</details>


### [63] [Steering MoE LLMs via Expert (De)Activation](https://arxiv.org/abs/2509.09660)
*Mohsen Fayyaz,Ali Modarressi,Hanieh Deilamsalehy,Franck Dernoncourt,Ryan Rossi,Trung Bui,Hinrich Schütze,Nanyun Peng*

Main category: cs.CL

TL;DR: The paper introduces SteerMoE, a framework for controlling behaviors in Mixture-of-Experts models by selectively activating or deactivating specialized components, achieving significant improvements in faithfulness and safety without retraining.


<details>
  <summary>Details</summary>
Motivation: To address challenges in controlling the behavior of Mixture-of-Experts models within Large Language Models, particularly concerning faithfulness and safety during inference.

Method: SteerMoE framework detects experts with specific activation patterns and strategically activates or deactivates them during inference to steer model behaviors.

Result: Using SteerMoE, improvements include up to +20% in safety, +27% in faithfulness, and refined behavior control in adversarial contexts, effectively revealing vulnerabilities in alignment mechanisms.

Conclusion: SteerMoE unlocks the potential to adjust expert-linked behaviors, offering a tool both for enhancing and critically assessing model alignment and safety frameworks.

Abstract: Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token
through a subset of specialized Feed-Forward Networks (FFN), known as experts.
We present SteerMoE, a framework for steering MoE models by detecting and
controlling behavior-linked experts. Our detection method identifies experts
with distinct activation patterns across paired inputs exhibiting contrasting
behaviors. By selectively (de)activating such experts during inference, we
control behaviors like faithfulness and safety without retraining or modifying
weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to
+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by
-41% alone, and -100% when combined with existing jailbreak methods, bypassing
all safety guardrails and exposing a new dimension of alignment faking hidden
within experts.

</details>


### [64] [CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2509.09675)
*Runpeng Dai,Linfeng Song,Haolin Liu,Zhenwen Liang,Dian Yu,Haitao Mi,Zhaopeng Tu,Rui Liu,Tong Zheng,Hongtu Zhu,Dong Yu*

Main category: cs.CL

TL;DR: The paper introduces Curiosity-Driven Exploration (CDE) to improve exploration in Reinforcement Learning with Verifiable Rewards (RLVR) applied to Large Language Models (LLMs), addressing issues such as entropy collapse and premature convergence.


<details>
  <summary>Details</summary>
Motivation: Current RLVR methods for enhancing LLM reasoning suffer from poor exploration, leading to issues like premature convergence and entropy collapse.

Method: The proposed CDE framework uses intrinsic curiosity signals derived from the actor's perplexity and the critic's value-estimate variance to guide exploration bonuses within RLVR.

Result: CDE showed an approximate +3 point improvement in performance on AIME benchmarks, outperforming standard RLVR methods like GRPO/PPO.

Conclusion: Using curiosity signals for exploration effectively addresses RLVR failures and enhances LLM reasoning, providing better insights into LLM calibration issues and failure modes.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm
for enhancing the reasoning ability of Large Language Models (LLMs). Yet
current RLVR methods often explore poorly, leading to premature convergence and
entropy collapse. To address this challenge, we introduce Curiosity-Driven
Exploration (CDE), a framework that leverages the model's own intrinsic sense
of curiosity to guide exploration. We formalize curiosity with signals from
both the actor and the critic: for the actor, we use perplexity over its
generated response, and for the critic, we use the variance of value estimates
from a multi-head architecture. Both signals serve as an exploration bonus
within the RLVR framework to guide the model. Our theoretical analysis shows
that the actor-wise bonus inherently penalizes overconfident errors and
promotes diversity among correct responses; moreover, we connect the
critic-wise bonus to the well-established count-based exploration bonus in RL.
Empirically, our method achieves an approximate +3 point improvement over
standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a
calibration collapse mechanism within RLVR, shedding light on common LLM
failure modes.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [65] [Recurrence Meets Transformers for Universal Multimodal Retrieval](https://arxiv.org/abs/2509.08897)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara*

Main category: cs.CV

TL;DR: ReT-2 is a unified retrieval model supporting multimodal queries and documents. It achieves state-of-the-art performance with reduced memory and faster inference.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of current vision-language models, which rely on task-specific fine-tuning and are limited to single-modality queries or documents.

Method: ReT-2 uses multi-layer representations and a recurrent Transformer architecture with LSTM-inspired gating to integrate visual and textual information dynamically.

Result: ReT-2 demonstrates state-of-the-art performance on M2KR and M-BEIR benchmarks, with faster inference and reduced memory. It also improves downstream tasks like Encyclopedic-VQA and InfoSeek.

Conclusion: ReT-2 is an efficient and effective multimodal retrieval model, enhancing both retrieval and downstream performance in LLM-related applications.

Abstract: With the rapid advancement of multimodal retrieval and its application in
LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.
Existing methods predominantly rely on task-specific fine-tuning of
vision-language models and are limited to single-modality queries or documents.
In this paper, we propose ReT-2, a unified retrieval model that supports
multimodal queries, composed of both images and text, and searches across
multimodal document collections where text and images coexist. ReT-2 leverages
multi-layer representations and a recurrent Transformer architecture with
LSTM-inspired gating mechanisms to dynamically integrate information across
layers and modalities, capturing fine-grained visual and textual details. We
evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different
retrieval configurations. Results demonstrate that ReT-2 consistently achieves
state-of-the-art performance across diverse settings, while offering faster
inference and reduced memory usage compared to prior approaches. When
integrated into retrieval-augmented generation pipelines, ReT-2 also improves
downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source
code and trained models are publicly available at:
https://github.com/aimagelab/ReT-2

</details>


### [66] [Diffusion-Based Action Recognition Generalizes to Untrained Domains](https://arxiv.org/abs/2509.08908)
*Rogerio Guimaraes,Frank Xiao,Pietro Perona,Markus Marks*

Main category: cs.CV

TL;DR: The paper proposes using features generated by a Vision Diffusion Model (VDM), aggregated via a transformer, to achieve human-like action recognition across varying conditions. The method improves generalization and sets new state-of-the-art benchmarks.


<details>
  <summary>Details</summary>
Motivation: Humans excel at recognizing actions across diverse conditions like species differences, viewing angles, and contexts, but deep learning models struggle with such generalizations. The study aims to address this gap.

Method: The approach leverages features extracted by Vision Diffusion Models (VDMs) conditioned on earlier diffusion process timesteps, emphasizing semantic information. These features are aggregated via a transformer for improved action recognition.

Result: Experimental validation demonstrated enhanced generalization across benchmarks including species variation, viewpoint differences, and recording contexts, achieving state-of-the-art performance.

Conclusion: The proposed method significantly improves machine action recognition, advancing its robustness to human-like levels.

Abstract: Humans can recognize the same actions despite large context and viewpoint
variations, such as differences between species (walking in spiders vs.
horses), viewpoints (egocentric vs. third-person), and contexts (real life vs
movies). Current deep learning models struggle with such generalization. We
propose using features generated by a Vision Diffusion Model (VDM), aggregated
via a transformer, to achieve human-like action recognition across these
challenging conditions. We find that generalization is enhanced by the use of a
model conditioned on earlier timesteps of the diffusion process to highlight
semantic information over pixel level details in the extracted features. We
experimentally explore the generalization properties of our approach in
classifying actions across animal species, across different viewing angles, and
different recording contexts. Our model sets a new state-of-the-art across all
three generalization benchmarks, bringing machine action recognition closer to
human-like robustness. Project page:
$\href{https://www.vision.caltech.edu/actiondiff/}{\texttt{vision.caltech.edu/actiondiff}}$
Code:
$\href{https://github.com/frankyaoxiao/ActionDiff}{\texttt{github.com/frankyaoxiao/ActionDiff}}$

</details>


### [67] [PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability](https://arxiv.org/abs/2509.08910)
*Tung Vu,Lam Nguyen,Quynh Dao*

Main category: cs.CV

TL;DR: The paper introduces PromptGuard, a modular prompting framework with VulnGuard Prompt for proactively preventing harmful outputs in Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The proliferation of LLMs in real-world applications risks generating harmful, biased, or misleading information that affects vulnerable populations such as LGBTQ+ individuals, single parents, and marginalized communities.

Method: The authors propose PromptGuard, which is a modular prompting framework. Its highlight, VulnGuard Prompt, uses hybrid techniques combining real-world data-driven contrastive learning, few-shot examples from GitHub repositories, ethical reasoning, and adaptive role-prompting. The framework integrates modules like Input Classification, Ethical Principles Integration, External Tool Interaction, among others.

Result: PromptGuard demonstrates 25-30% harm reduction analytically through entropy bounds and Pareto optimality. It employs formal proofs regarding theoretical multi-objective optimization and vulnerability analysis using information theory.

Conclusion: PromptGuard establishes a novel, mathematically-backed approach to real-time harm prevention in LLMs through its modular and population-specific methodology, setting a foundation for systematic empirical research.

Abstract: The proliferation of Large Language Models (LLMs) in real-world applications
poses unprecedented risks of generating harmful, biased, or misleading
information to vulnerable populations including LGBTQ+ individuals, single
parents, and marginalized communities. While existing safety approaches rely on
post-hoc filtering or generic alignment techniques, they fail to proactively
prevent harmful outputs at the generation source. This paper introduces
PromptGuard, a novel modular prompting framework with our breakthrough
contribution: VulnGuard Prompt, a hybrid technique that prevents harmful
information generation using real-world data-driven contrastive learning.
VulnGuard integrates few-shot examples from curated GitHub repositories,
ethical chain-of-thought reasoning, and adaptive role-prompting to create
population-specific protective barriers. Our framework employs theoretical
multi-objective optimization with formal proofs demonstrating 25-30% analytical
harm reduction through entropy bounds and Pareto optimality. PromptGuard
orchestrates six core modules: Input Classification, VulnGuard Prompting,
Ethical Principles Integration, External Tool Interaction, Output Validation,
and User-System Interaction, creating an intelligent expert system for
real-time harm prevention. We provide comprehensive mathematical formalization
including convergence proofs, vulnerability analysis using information theory,
and theoretical validation framework using GitHub-sourced datasets,
establishing mathematical foundations for systematic empirical research.

</details>


### [68] [Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures](https://arxiv.org/abs/2509.08926)
*Waqar Ahmad,Evan Murphy,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: The paper addresses label noise in object Re-ID by proposing Beta-SOD, a statistical outlier detection framework utilizing a two-component Beta mixture model, integrated with a Siamese network for improved performance.


<details>
  <summary>Details</summary>
Motivation: Object Re-ID suffers from label noise, leading to poor performance. There is a need for robust methods that can handle noisy datasets and enhance identification accuracy.

Method: The authors use a Siamese network architecture for supervised image similarity learning combined with a novel Beta-SOD outlier detection framework. This involves modeling embedding pair cosine similarity distributions using a two-component Beta mixture and integrating multiple loss functions for optimization.

Result: Experiments on CUHK03, Market-1501, and VeRi-776 datasets show that Beta-SOD significantly outperforms state-of-the-art Re-ID methods, especially under noise levels of 10%-30%.

Conclusion: Beta-SOD improves robustness and generalizability in noisy Re-ID tasks and demonstrates its effectiveness on both person and vehicle Re-ID datasets.

Abstract: Object re-identification (Re-ID) methods are highly sensitive to label noise,
which typically leads to significant performance degradation. We address this
challenge by reframing Re-ID as a supervised image similarity task and adopting
a Siamese network architecture trained to capture discriminative pairwise
relationships. Central to our approach is a novel statistical outlier detection
(OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier
Detection), which models the distribution of cosine similarities between
embedding pairs using a two-component Beta distribution mixture model. We
establish a novel identifiability result for mixtures of two Beta
distributions, ensuring that our learning task is well-posed.The proposed OD
step complements the Re-ID architecture combining binary cross-entropy,
contrastive, and cosine embedding losses that jointly optimize feature-level
similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising
and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and
vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance
compared to the state-of-the-art methods across various noise levels (10-30\%),
demonstrating both robustness and broad applicability in noisy Re-ID scenarios.
The implementation of Beta-SOD is available at:
https://github.com/waqar3411/Beta-SOD

</details>


### [69] [SFD-Mamba2Net: Strcture-Guided Frequency-Enhanced Dual-Stream Mamba2 Network for Coronary Artery Segmentation](https://arxiv.org/abs/2509.08934)
*Nan Mu,Ruiqi Song,Zhihui Xu,Jingfeng Jiang,Chen Zhao*

Main category: cs.CV

TL;DR: The paper introduces SFD-Mamba2Net, a framework designed to improve segmentation and stenosis detection in challenging ICA images, achieving superior performance metrics.


<details>
  <summary>Details</summary>
Motivation: Current segmentation and detection methods for ICA images struggle with low contrast, high noise, and complex vascular structures. This study aims to address these challenges for improved clinical adoption.

Method: The authors propose SFD-Mamba2Net, an end-to-end framework. It incorporates a CASE module for multi-scale structural enhancements in the encoder and a PHFP module for refining vascular details using wavelet decomposition in the decoder.

Result: SFD-Mamba2Net outperformed existing methods across eight segmentation metrics and achieved top performance in stenosis detection's predictive value and true positive rate.

Conclusion: The proposed framework successfully enhances segmentation accuracy and stenosis detection performance, addressing visual and structural challenges in ICA images.

Abstract: Background: Coronary Artery Disease (CAD) is one of the leading causes of
death worldwide. Invasive Coronary Angiography (ICA), regarded as the gold
standard for CAD diagnosis, necessitates precise vessel segmentation and
stenosis detection. However, ICA images are typically characterized by low
contrast, high noise levels, and complex, fine-grained vascular structures,
which pose significant challenges to the clinical adoption of existing
segmentation and detection methods. Objective: This study aims to improve the
accuracy of coronary artery segmentation and stenosis detection in ICA images
by integrating multi-scale structural priors, state-space-based long-range
dependency modeling, and frequency-domain detail enhancement strategies.
Methods: We propose SFD-Mamba2Net, an end-to-end framework tailored for
ICA-based vascular segmentation and stenosis detection. In the encoder, a
Curvature-Aware Structural Enhancement (CASE) module is embedded to leverage
multi-scale responses for highlighting slender tubular vascular structures,
suppressing background interference, and directing attention toward vascular
regions. In the decoder, we introduce a Progressive High-Frequency Perception
(PHFP) module that employs multi-level wavelet decomposition to progressively
refine high-frequency details while integrating low-frequency global
structures. Results and Conclusions: SFD-Mamba2Net consistently outperformed
state-of-the-art methods across eight segmentation metrics, and achieved the
highest true positive rate and positive predictive value in stenosis detection.

</details>


### [70] [Live(r) Die: Predicting Survival in Colorectal Liver Metastasis](https://arxiv.org/abs/2509.08935)
*Muhammad Alberb,Helen Cheung,Anne Martel*

Main category: cs.CV

TL;DR: This paper introduces an automated framework combining MRI-based segmentation and radiomics analysis to predict surgical outcomes for colorectal liver metastasis (CRLM), outperforming existing methods by over 10% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Improve the limited predictive power of current prognostic models for CRLM, especially in multifocal cases, through advanced automation and integration of imaging and survival analysis.

Method: The framework uses a segmentation pipeline and a radiomics pipeline. A zero-shot 3D prompt propagation algorithm, SAMONAI, enhances segmentation accuracy, while the survival analysis employs SurvAMINN, a novel neural network focusing on aggressive tumors.

Result: The framework significantly outperformed clinical and genomic biomarkers, achieving a C-index improvement of over 10% in experiments on a dataset of 227 patients.

Conclusion: Integrating automated segmentation algorithms with survival analysis can deliver accurate, efficient, and interpretable outcomes, highlighting its potential in CRLM surgical planning.

Abstract: Colorectal cancer frequently metastasizes to the liver, significantly
reducing long-term survival. While surgical resection is the only potentially
curative treatment for colorectal liver metastasis (CRLM), patient outcomes
vary widely depending on tumor characteristics along with clinical and genomic
factors. Current prognostic models, often based on limited clinical or
molecular features, lack sufficient predictive power, especially in multifocal
CRLM cases. We present a fully automated framework for surgical outcome
prediction from pre- and post-contrast MRI acquired before surgery. Our
framework consists of a segmentation pipeline and a radiomics pipeline. The
segmentation pipeline learns to segment the liver, tumors, and spleen from
partially annotated data by leveraging promptable foundation models to complete
missing labels. Also, we propose SAMONAI, a novel zero-shot 3D prompt
propagation algorithm that leverages the Segment Anything Model to segment 3D
regions of interest from a single point prompt, significantly improving our
segmentation pipeline's accuracy and efficiency. The predicted pre- and
post-contrast segmentations are then fed into our radiomics pipeline, which
extracts features from each tumor and predicts survival using SurvAMINN, a
novel autoencoder-based multiple instance neural network for survival analysis.
SurvAMINN jointly learns dimensionality reduction and hazard prediction from
right-censored survival data, focusing on the most aggressive tumors. Extensive
evaluation on an institutional dataset comprising 227 patients demonstrates
that our framework surpasses existing clinical and genomic biomarkers,
delivering a C-index improvement exceeding 10%. Our results demonstrate the
potential of integrating automated segmentation algorithms and radiomics-based
survival analysis to deliver accurate, annotation-efficient, and interpretable
outcome prediction in CRLM.

</details>


### [71] [Discovering Divergent Representations between Text-to-Image Models](https://arxiv.org/abs/2509.08940)
*Lisa Dunlap,Joseph E. Gonzalez,Trevor Darrell,Fabian Caba Heilbron,Josef Sivic,Bryan Russell*

Main category: cs.CV

TL;DR: The paper explores differences in visual outputs between generative text-to-image models, introducing CompCon, an evolutionary search algorithm to identify attribute divergences and associated prompt types.


<details>
  <summary>Details</summary>
Motivation: Understanding how visual representations generated by different text-to-image models diverge can improve model design, fairness, and prompt utility.

Method: The authors developed CompCon, an evolutionary search algorithm, alongside creating ID2, a dataset with 60 differences, to identify prompt-induced visual discrepancies between models.

Result: CompCon successfully uncovers various visual divergences, finding that PixArt associates loneliness with wet streets, and Stable Diffusion 3.5 represents African American people in media professions distinctively.

Conclusion: CompCon is effective in identifying divergent visual attributes and linked concepts, providing insights into nuanced representations in generative models.

Abstract: In this paper, we investigate when and how visual representations learned by
two different generative models diverge. Given two text-to-image models, our
goal is to discover visual attributes that appear in images generated by one
model but not the other, along with the types of prompts that trigger these
attribute differences. For example, "flames" might appear in one model's
outputs when given prompts expressing strong emotions, while the other model
does not produce this attribute given the same prompts. We introduce CompCon
(Comparing Concepts), an evolutionary search algorithm that discovers visual
attributes more prevalent in one model's output than the other, and uncovers
the prompt concepts linked to these visual differences. To evaluate CompCon's
ability to find diverging representations, we create an automated data
generation pipeline to produce ID2, a dataset of 60 input-dependent
differences, and compare our approach to several LLM- and VLM-powered
baselines. Finally, we use CompCon to compare popular text-to-image models,
finding divergent representations such as how PixArt depicts prompts mentioning
loneliness with wet streets and Stable Diffusion 3.5 depicts African American
people in media professions. Code at: https://github.com/adobe-research/CompCon

</details>


### [72] [An U-Net-Based Deep Neural Network for Cloud Shadow and Sun-Glint Correction of Unmanned Aerial System (UAS) Imagery](https://arxiv.org/abs/2509.08949)
*Yibin Wang,Wondimagegn Beshah,Padmanava Dash,Haifeng Wang*

Main category: cs.CV

TL;DR: A novel U-Net based deep learning model is introduced to identify and correct cloud shadows and sun glint in unmanned aerial system (UAS) imagery, particularly for water quality assessment.


<details>
  <summary>Details</summary>
Motivation: Although UAS imagery is advantageous for remote sensing, its utility is hindered by disturbances like cloud shadows and sun glint, particularly when analyzing water quality.

Method: A U-Net based machine learning model was designed to detect and segregate cloud shadowed and sun-glint regions from clear areas, followed by applying a corrective mechanism to restore the affected image sections.

Result: The model achieved high-quality correction of affected UAS images, validated by robust evaluation metrics.

Conclusion: This study successfully developed and implemented a deep learning approach to address key limitations in UAS-based imagery, enhancing its usability for remote sensing of water bodies.

Abstract: The use of unmanned aerial systems (UASs) has increased tremendously in the
current decade. They have significantly advanced remote sensing with the
capability to deploy and image the terrain as per required spatial, spectral,
temporal, and radiometric resolutions for various remote sensing applications.
One of the major advantages of UAS imagery is that images can be acquired in
cloudy conditions by flying the UAS under the clouds. The limitation to the
technology is that the imagery is often sullied by cloud shadows. Images taken
over water are additionally affected by sun glint. These are two pose serious
issues for estimating water quality parameters from the UAS images. This study
proposes a novel machine learning approach first to identify and extract
regions with cloud shadows and sun glint and separate such regions from
non-obstructed clear sky regions and sun-glint unaffected regions. The data was
extracted from the images at pixel level to train an U-Net based deep learning
model and best settings for model training was identified based on the various
evaluation metrics from test cases. Using this evaluation, a high-quality image
correction model was determined, which was used to recover the cloud shadow and
sun glint areas in the images.

</details>


### [73] [CoSwin: Convolution Enhanced Hierarchical Shifted Window Attention For Small-Scale Vision](https://arxiv.org/abs/2509.08959)
*Puskal Khadka,Rodrigue Rizk,Longwei Wang,KC Santosh*

Main category: cs.CV

TL;DR: The paper introduces CoSwin, an architecture combining local convolutional features with hierarchical shifted window attention in Vision Transformers (ViTs), achieving significant performance gains on multiple small-scale image classification benchmarks.


<details>
  <summary>Details</summary>
Motivation: While Vision Transformers excel at global feature extraction, they struggle with local feature learning due to the absence of inductive biases like locality and translation equivariance, especially in small datasets.

Method: The authors propose CoSwin, a feature-fusion architecture that integrates a learnable local feature enhancement module into each attention block, combining fine-grained spatial information with global semantic structure.

Result: Experimental results show CoSwin consistently outperforms state-of-the-art vision models with improvements of up to 4.92% on CIFAR-100 and 4.47% on Tiny ImageNet compared to the baseline Swin Transformer.

Conclusion: CoSwin demonstrates that integrating local-global feature fusion significantly enhances the generalization and robustness of Vision Transformers in small-scale vision tasks.

Abstract: Vision Transformers (ViTs) have achieved impressive results in computer
vision by leveraging self-attention to model long-range dependencies. However,
their emphasis on global context often comes at the expense of local feature
extraction in small datasets, particularly due to the lack of key inductive
biases such as locality and translation equivariance. To mitigate this, we
propose CoSwin, a novel feature-fusion architecture that augments the
hierarchical shifted window attention with localized convolutional feature
learning. Specifically, CoSwin integrates a learnable local feature enhancement
module into each attention block, enabling the model to simultaneously capture
fine-grained spatial details and global semantic structure. We evaluate CoSwin
on multiple image classification benchmarks including CIFAR-10, CIFAR-100,
MNIST, SVHN, and Tiny ImageNet. Our experimental results show consistent
performance gains over state-of-the-art convolutional and transformer-based
models. Notably, CoSwin achieves improvements of 2.17% on CIFAR-10, 4.92% on
CIFAR-100, 0.10% on MNIST, 0.26% on SVHN, and 4.47% on Tiny ImageNet over the
baseline Swin Transformer. These improvements underscore the effectiveness of
local-global feature fusion in enhancing the generalization and robustness of
transformers for small-scale vision. Code and pretrained weights available at
https://github.com/puskal-khadka/coswin

</details>


### [74] [iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning](https://arxiv.org/abs/2509.08982)
*Karim Slimani,Catherine Achard,Brahim Tamadazte*

Main category: cs.CV

TL;DR: The paper introduces iMatcher, a novel framework for improving point cloud registration using learned features and geometric consistency.


<details>
  <summary>Details</summary>
Motivation: To enhance feature matching in point cloud registration by improving the geometric consistency and robustness across diverse settings.

Method: iMatcher employs a local graph embedding module for initialization, refines matching with nearest-neighbor-based repositioning steps, and uses global geometric consistency learning for final probability prediction.

Result: The experiments show significant performance improvements on datasets like KITTI (95%-97%), KITTI-360 (94%-97%), and 3DMatch (81.1%), demonstrating state-of-the-art inlier ratios.

Conclusion: iMatcher proves to be a robust and effective framework, achieving excellent results in rigid registration tasks for diverse datasets.

Abstract: This paper presents iMatcher, a fully differentiable framework for feature
matching in point cloud registration. The proposed method leverages learned
features to predict a geometrically consistent confidence matrix, incorporating
both local and global consistency. First, a local graph embedding module leads
to an initialization of the score matrix. A subsequent repositioning step
refines this matrix by considering bilateral source-to-target and
target-to-source matching via nearest neighbor search in 3D space. The paired
point features are then stacked together to be refined through global geometric
consistency learning to predict a point-wise matching probability. Extensive
experiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch)
datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partial
matching (MVP-RG), demonstrate that iMatcher significantly improves rigid
registration performance. The method achieves state-of-the-art inlier ratios,
scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch,
highlighting its robustness across diverse settings.

</details>


### [75] [UltrON: Ultrasound Occupancy Networks](https://arxiv.org/abs/2509.08991)
*Magdalena Wysocki,Felix Duelmer,Ananya Bal,Nassir Navab,Mohammad Farid Azampour*

Main category: cs.CV

TL;DR: The paper introduces UltrON, an advanced method for geometrically consistent 3D ultrasound reconstruction utilizing acoustic features and occupancy-based representation, overcoming limitations of occlusions and sparse annotations.


<details>
  <summary>Details</summary>
Motivation: Sonographers face challenges in mentally reconstructing 3D anatomical shapes from partial 2D ultrasound views, and existing methods struggle with precision due to artifacts and dependencies on extensive annotations.

Method: The study proposes an occupancy-based representation and UltrON framework, leveraging acoustic features embedded in B-mode ultrasound images for a weakly-supervised optimization regime, along with a novel loss function addressing view dependency.

Result: UltrON successfully integrates acoustic features to enhance geometric consistency and enables reconstruction of anatomical shapes from multiview ultrasound despite occlusions and sparse labeling.

Conclusion: By incorporating acoustic properties, UltrON generalizes to improve 3D anatomical reconstructions, mitigating challenges of occlusions and annotation dependency, with code and dataset shared for future research.

Abstract: In free-hand ultrasound imaging, sonographers rely on expertise to mentally
integrate partial 2D views into 3D anatomical shapes. Shape reconstruction can
assist clinicians in this process. Central to this task is the choice of shape
representation, as it determines how accurately and efficiently the structure
can be visualized, analyzed, and interpreted. Implicit representations, such as
SDF and occupancy function, offer a powerful alternative to traditional voxel-
or mesh-based methods by modeling continuous, smooth surfaces with compact
storage, avoiding explicit discretization. Recent studies demonstrate that SDF
can be effectively optimized using annotations derived from segmented B-mode
ultrasound images. Yet, these approaches hinge on precise annotations,
overlooking the rich acoustic information embedded in B-mode intensity.
Moreover, implicit representation approaches struggle with the ultrasound's
view-dependent nature and acoustic shadowing artifacts, which impair
reconstruction. To address the problems resulting from occlusions and
annotation dependency, we propose an occupancy-based representation and
introduce \gls{UltrON} that leverages acoustic features to improve geometric
consistency in weakly-supervised optimization regime. We show that these
features can be obtained from B-mode images without additional annotation cost.
Moreover, we propose a novel loss function that compensates for view-dependency
in the B-mode images and facilitates occupancy optimization from multiview
ultrasound. By incorporating acoustic properties, \gls{UltrON} generalizes to
shapes of the same anatomy. We show that \gls{UltrON} mitigates the limitations
of occlusions and sparse labeling and paves the way for more accurate 3D
reconstruction. Code and dataset will be available at
https://github.com/magdalena-wysocki/ultron.

</details>


### [76] [Implicit Neural Representations of Intramyocardial Motion and Strain](https://arxiv.org/abs/2509.09004)
*Andrew Bell,Yan Kit Choi,Steffen Peterson,Andrew King,Muhummad Sohaib Nazir,Alistair Young*

Main category: cs.CV

TL;DR: The study presents an efficient method using implicit neural representations (INRs) to analyze left ventricular motion from MRI data, outperforming existing methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Accurately quantifying intramyocardial motion and strain from tagging MRI remains crucial but is technically challenging.

Method: The researchers utilized INR models conditioned on learned latent codes to predict continuous left ventricular displacement, bypassing inference-time optimization.

Result: On a dataset of 452 UK Biobank cases, the method achieved the best tracking accuracy (2.14 mm RMSE), the lowest global strain errors, and was approximately 380 times faster than the most accurate baseline model.

Conclusion: INR-based models are effective and scalable, making them well-suited for analyzing myocardial strain in large cardiac MRI datasets.

Abstract: Automatic quantification of intramyocardial motion and strain from tagging
MRI remains an important but challenging task. We propose a method using
implicit neural representations (INRs), conditioned on learned latent codes, to
predict continuous left ventricular (LV) displacement -- without requiring
inference-time optimisation. Evaluated on 452 UK Biobank test cases, our method
achieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined
error in global circumferential (2.86%) and radial (6.42%) strain compared to
three deep learning baselines. In addition, our method is $\sim$380$\times$
faster than the most accurate baseline. These results highlight the suitability
of INR-based models for accurate and scalable analysis of myocardial strain in
large CMR datasets.

</details>


### [77] [E-MLNet: Enhanced Mutual Learning for Universal Domain Adaptation with Sample-Specific Weighting](https://arxiv.org/abs/2509.09006)
*Samuel Felipe dos Santos,Tiago Agostinho de Almeida,Jurandy Almeida*

Main category: cs.CV

TL;DR: E-MLNet introduces a dynamic weighting strategy to improve domain adaptation tasks, achieving better performance and robustness compared to its predecessor, MLNet.


<details>
  <summary>Details</summary>
Motivation: Address challenges in Universal Domain Adaptation, where models must classify known samples and reject unknown ones without assuming label set relationships.

Method: The Enhanced Mutual Learning Network incorporates dynamic weighting into Open-set Entropy Minimization, focusing adaptation on relevant class boundaries using closed-set classifier predictions.

Result: Extensive evaluations on benchmarks confirm E-MLNet's superior performance, achieving highest average H-scores and outperforming MLNet in most adaptation tasks.

Conclusion: E-MLNet effectively enhances adaptation and robustness in universal domain tasks, showcasing the benefits of integrating targeted weighting strategies.

Abstract: Universal Domain Adaptation (UniDA) seeks to transfer knowledge from a
labeled source to an unlabeled target domain without assuming any relationship
between their label sets, requiring models to classify known samples while
rejecting unknown ones. Advanced methods like Mutual Learning Network (MLNet)
use a bank of one-vs-all classifiers adapted via Open-set Entropy Minimization
(OEM). However, this strategy treats all classifiers equally, diluting the
learning signal. We propose the Enhanced Mutual Learning Network (E-MLNet),
which integrates a dynamic weighting strategy to OEM. By leveraging the
closed-set classifier's predictions, E-MLNet focuses adaptation on the most
relevant class boundaries for each target sample, sharpening the distinction
between known and unknown classes. We conduct extensive experiments on four
challenging benchmarks: Office-31, Office-Home, VisDA-2017, and ImageCLEF. The
results demonstrate that E-MLNet achieves the highest average H-scores on VisDA
and ImageCLEF and exhibits superior robustness over its predecessor. E-MLNet
outperforms the strong MLNet baseline in the majority of individual adaptation
tasks -- 22 out of 31 in the challenging Open-Partial DA setting and 19 out of
31 in the Open-Set DA setting -- confirming the benefits of our focused
adaptation strategy.

</details>


### [78] [COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation](https://arxiv.org/abs/2509.09014)
*Umair Hassan*

Main category: cs.CV

TL;DR: COCO-Urdu is a large-scale Urdu image-captioning dataset derived from MS COCO, featuring 319,000 Urdu captions validated using a multimodal quality estimation pipeline.


<details>
  <summary>Details</summary>
Motivation: Urdu remains underserved in vision-language research, due to the lack of high-quality datasets, which leads to biases in multilingual AI models.

Method: The authors created COCO-Urdu by translating MS COCO captions into Urdu using SeamlessM4T v2, followed by quality validation through a hybrid framework that combines COMET-Kiwi, CLIP, and BERTScore tools. Iterative refinements were made using large language models.

Result: COCO-Urdu consists of 59,000 images and 319,000 Urdu captions, achieving strong benchmarks on BLEU, SacreBLEU, and chrF evaluations.

Conclusion: COCO-Urdu fills a gap in multimodal research for Urdu by providing a high-quality dataset and quality estimation pipeline, promoting more inclusive vision-language systems.

Abstract: Urdu, spoken by over 250 million people, remains critically under-served in
multimodal and vision-language research. The absence of large-scale,
high-quality datasets has limited the development of Urdu-capable systems and
reinforced biases in multilingual vision-language models trained primarily on
high-resource languages. To address this gap, we present COCO-Urdu, a
large-scale image-caption dataset derived from MS COCO, containing 59,000
images and 319,000 Urdu captions selected through stratified sampling to
preserve the original distribution. Captions were translated using SeamlessM4T
v2 and validated with a hybrid multimodal quality estimation framework that
integrates COMET-Kiwi for translation quality, CLIP-based similarity for visual
grounding, and BERTScore with back-translation for semantic consistency;
low-scoring captions were iteratively refined using open-source large language
models. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting
consistently strong results. To the best of our knowledge, COCO-Urdu is the
largest publicly available Urdu captioning dataset. By releasing both the
dataset and the quality estimation pipeline, we aim to reduce language bias in
multimodal research and establish a foundation for inclusive vision-language
systems.

</details>


### [79] [VoxelFormer: Parameter-Efficient Multi-Subject Visual Decoding from fMRI](https://arxiv.org/abs/2509.09015)
*Chenqian Le,Yilin Zhao,Nikasadat Emami,Kushagra Yadav,Xujin "Chris" Liu,Xupeng Chen,Yao Wang*

Main category: cs.CV

TL;DR: The paper introduces VoxelFormer, a transformer-based architecture for multi-subject visual decoding from fMRI data, achieving competitive performance with fewer parameters compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing fMRI-based visual decoding methods primarily rely on subject-specific training, hindering scalability and broad applicability.

Method: VoxelFormer utilizes a Token Merging Transformer for efficient voxel compression and a query-driven Q-Former for generating neural representations aligned with CLIP image embeddings.

Result: VoxelFormer demonstrates competitive performance with reduced parameters on the 7T Natural Scenes Dataset for visual decoding tasks.

Conclusion: The study underscores token merging and query-based transformers as efficient techniques for scalable neural decoding in fMRI-based visual applications.

Abstract: Recent advances in fMRI-based visual decoding have enabled compelling
reconstructions of perceived images. However, most approaches rely on
subject-specific training, limiting scalability and practical deployment. We
introduce \textbf{VoxelFormer}, a lightweight transformer architecture that
enables multi-subject training for visual decoding from fMRI. VoxelFormer
integrates a Token Merging Transformer (ToMer) for efficient voxel compression
and a query-driven Q-Former that produces fixed-size neural representations
aligned with the CLIP image embedding space. Evaluated on the 7T Natural Scenes
Dataset, VoxelFormer achieves competitive retrieval performance on subjects
included during training with significantly fewer parameters than existing
methods. These results highlight token merging and query-based transformers as
promising strategies for parameter-efficient neural decoding.

</details>


### [80] [Integrating Anatomical Priors into a Causal Diffusion Model](https://arxiv.org/abs/2509.09054)
*Binxu Li,Wei Peng,Mingjie Li,Ehsan Adeli,Kilian M. Pohl*

Main category: cs.CV

TL;DR: The paper introduces Probabilistic Causal Graph Model (PCGM), a 3D generative diffusion framework that integrates voxel-level anatomical constraints to produce high-quality counterfactual brain MRIs, addressing fine-grained morphological distinctions.


<details>
  <summary>Details</summary>
Motivation: Counterfactual MRI generation is needed to observe subtle brain morphology differences, but current models struggle due to lack of explicit anatomical inductive biases, which prevents them from preserving fine anatomical details.

Method: The authors propose PCGM, which integrates probabilistic graph anatomical constraints into diffusion models. It uses spatial binary masks generated via a 3D ControlNet, coupled with a counterfactual denoising UNet and 3D diffusion decoder to synthesize anatomically plausible MRIs.

Result: PCGM outperforms baseline models in generating high-quality brain MRIs, replicating medically relevant subtle effects of diseases on cortical brain regions as seen in real-world neuroscience studies.

Conclusion: PCGM successfully addresses the limitations of counterfactual MRI generation, offering a key tool to support studies on subtle brain morphological changes using synthetic MRIs.

Abstract: 3D brain MRI studies often examine subtle morphometric differences between
cohorts that are hard to detect visually. Given the high cost of MRI
acquisition, these studies could greatly benefit from image syntheses,
particularly counterfactual image generation, as seen in other domains, such as
computer vision. However, counterfactual models struggle to produce
anatomically plausible MRIs due to the lack of explicit inductive biases to
preserve fine-grained anatomical details. This shortcoming arises from the
training of the models aiming to optimize for the overall appearance of the
images (e.g., via cross-entropy) rather than preserving subtle, yet medically
relevant, local variations across subjects. To preserve subtle variations, we
propose to explicitly integrate anatomical constraints on a voxel-level as
prior into a generative diffusion framework. Called Probabilistic Causal Graph
Model (PCGM), the approach captures anatomical constraints via a probabilistic
graph module and translates those constraints into spatial binary masks of
regions where subtle variations occur. The masks (encoded by a 3D extension of
ControlNet) constrain a novel counterfactual denoising UNet, whose encodings
are then transferred into high-quality brain MRIs via our 3D diffusion decoder.
Extensive experiments on multiple datasets demonstrate that PCGM generates
structural brain MRIs of higher quality than several baseline approaches.
Furthermore, we show for the first time that brain measurements extracted from
counterfactuals (generated by PCGM) replicate the subtle effects of a disease
on cortical brain regions previously reported in the neuroscience literature.
This achievement is an important milestone in the use of synthetic MRIs in
studies investigating subtle morphological differences.

</details>


### [81] [Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models](https://arxiv.org/abs/2509.09064)
*Qiuhui Chen,Xuancheng Yao,Huping Ye,Yi Hong*

Main category: cs.CV

TL;DR: Med3DInsight is a novel framework integrating 3D medical image encoders with 2D multimodal large language models, enhancing understanding for medical tasks like segmentation and classification.


<details>
  <summary>Details</summary>
Motivation: Improve semantic comprehension in 3D medical image understanding by leveraging advancements in multimodal large language models.

Method: Med3DInsight uses a plane-slice-aware transformer module to connect 3D image encoders and 2D MLLMs, with a partial optimal transport alignment to handle noise.

Result: Demonstrated state-of-the-art performance in segmentation and classification tasks across CT and MRI datasets, outperforming standard methods.

Conclusion: Med3DInsight offers a scalable, annotation-free framework for multimodal 3D medical image learning, with potential for integration into existing networks and wide availability of resources.

Abstract: Understanding 3D medical image volumes is critical in the medical field, yet
existing 3D medical convolution and transformer-based self-supervised learning
(SSL) methods often lack deep semantic comprehension. Recent advancements in
multimodal large language models (MLLMs) provide a promising approach to
enhance image understanding through text descriptions. To leverage these 2D
MLLMs for improved 3D medical image understanding, we propose Med3DInsight, a
novel pretraining framework that integrates 3D image encoders with 2D MLLMs via
a specially designed plane-slice-aware transformer module. Additionally, our
model employs a partial optimal transport based alignment, demonstrating
greater tolerance to noise introduced by potential noises in LLM-generated
content. Med3DInsight introduces a new paradigm for scalable multimodal 3D
medical representation learning without requiring human annotations. Extensive
experiments demonstrate our state-of-the-art performance on two downstream
tasks, i.e., segmentation and classification, across various public datasets
with CT and MRI modalities, outperforming current SSL methods. Med3DInsight can
be seamlessly integrated into existing 3D medical image understanding networks,
potentially enhancing their performance. Our source code, generated datasets,
and pre-trained models will be available at
https://github.com/Qybc/Med3DInsight.

</details>


### [82] [Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach](https://arxiv.org/abs/2509.09067)
*Hesham M. Shehata,Mohammad Abdolrahmani*

Main category: cs.CV

TL;DR: The paper presents a multi-task learning approach to improve human action recognition, particularly for human-object interactions, achieving 99.25% accuracy.


<details>
  <summary>Details</summary>
Motivation: Recent GCNs struggle to detect human-object interactions due to inadequate representation of scene information and learning architectures.

Method: Proposed a multi-task learning methodology incorporating fixed object information and interaction area data for better action recognition.

Result: Achieved 99.25% accuracy in recognizing interactions and non-interactions, surpassing baseline accuracy by 2.75%.

Conclusion: The multi-task approach enhances action recognition involving fixed objects, proving to be more effective than conventional methods.

Abstract: Recent graph convolutional neural networks (GCNs) have shown high performance
in the field of human action recognition by using human skeleton poses.
However, it fails to detect human-object interaction cases successfully due to
the lack of effective representation of the scene information and appropriate
learning architectures. In this context, we propose a methodology to utilize
human action recognition performance by considering fixed object information in
the environment and following a multi-task learning approach. In order to
evaluate the proposed method, we collected real data from public environments
and prepared our data set, which includes interaction classes of hands-on fixed
objects (e.g., ATM ticketing machines, check-in/out machines, etc.) and
non-interaction classes of walking and standing. The multi-task learning
approach, along with interaction area information, succeeds in recognizing the
studied interaction and non-interaction actions with an accuracy of 99.25%,
outperforming the accuracy of the base model using only human skeleton poses by
2.75%.

</details>


### [83] [IRDFusion: Iterative Relation-Map Difference guided Feature Fusion for Multispectral Object Detection](https://arxiv.org/abs/2509.09085)
*Jifeng Shen,Haibo Zhan,Xin Zuo,Heng Fan,Xiaohui Yuan,Jun Li,Wankou Yang*

Main category: cs.CV

TL;DR: The paper introduces IRDFusion, a framework for enhanced multispectral object detection using cross-modal feature fusion strategies to improve perceptual performance while suppressing noise.


<details>
  <summary>Details</summary>
Motivation: Current multispectral object detection methods struggle with retaining background noise during feature fusion, which affects detection accuracy and perceptual performance.

Method: The paper proposes IRDFusion, which incorporates Mutual Feature Refinement Module (MFRM) and Differential Feature Feedback Module (DFFM). These modules enable adaptive cross-modal feature enhancement and suppression of shared noise, modeled via iterative relation maps.

Result: IRDFusion demonstrated state-of-the-art performance on FLIR, LLVIP, and M$^3$FD datasets, consistently outperforming existing methods and showcasing robustness across various scenarios.

Conclusion: The integrated mechanisms within IRDFusion amplify salient signals while reducing noise, delivering significantly enhanced cross-modal object detection accuracy and reliability.

Abstract: Current multispectral object detection methods often retain extraneous
background or noise during feature fusion, limiting perceptual performance.To
address this, we propose an innovative feature fusion framework based on
cross-modal feature contrastive and screening strategy, diverging from
conventional approaches. The proposed method adaptively enhances salient
structures by fusing object-aware complementary cross-modal features while
suppressing shared background interference.Our solution centers on two novel,
specially designed modules: the Mutual Feature Refinement Module (MFRM) and the
Differential Feature Feedback Module (DFFM). The MFRM enhances intra- and
inter-modal feature representations by modeling their relationships, thereby
improving cross-modal alignment and discriminative power.Inspired by feedback
differential amplifiers, the DFFM dynamically computes inter-modal differential
features as guidance signals and feeds them back to the MFRM, enabling adaptive
fusion of complementary information while suppressing common-mode noise across
modalities. To enable robust feature learning, the MFRM and DFFM are integrated
into a unified framework, which is formally formulated as an Iterative
Relation-Map Differential Guided Feature Fusion mechanism, termed IRDFusion.
IRDFusion enables high-quality cross-modal fusion by progressively amplifying
salient relational signals through iterative feedback, while suppressing
feature noise, leading to significant performance gains.In extensive
experiments on FLIR, LLVIP and M$^3$FD datasets, IRDFusion achieves
state-of-the-art performance and consistently outperforms existing methods
across diverse challenging scenarios, demonstrating its robustness and
effectiveness. Code will be available at
https://github.com/61s61min/IRDFusion.git.

</details>


### [84] [SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models](https://arxiv.org/abs/2509.09090)
*Hengyu Fang,Yijiang Liu,Yuan Du,Li Du,Huanrui Yang*

Main category: cs.CV

TL;DR: SQAP-VLA is a novel framework that efficiently accelerates Vision-Language-Action models by combining quantization and token pruning for better performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the computational and memory challenges in deploying Vision-Language-Action models while improving their efficiency and speed without compromising performance.

Method: The authors co-designed a framework that integrates new quantization-aware token pruning criteria with enhanced quantizer design, enabling compatibility between aggressive quantization and token pruning.

Result: SQAP-VLA resulted in a 1.93× inference speedup and up to a 4.5% average success rate improvement compared to standard VLA models.

Conclusion: SQAP-VLA demonstrates the potential for structured, training-free optimization techniques to balance efficiency and performance in Vision-Language-Action models, setting a new standard for model acceleration frameworks.

Abstract: Vision-Language-Action (VLA) models exhibit unprecedented capabilities for
embodied intelligence. However, their extensive computational and memory costs
hinder their practical deployment. Existing VLA compression and acceleration
approaches conduct quantization or token pruning in an ad-hoc manner but fail
to enable both for a holistic efficiency improvement due to an observed
incompatibility. This work introduces SQAP-VLA, the first structured,
training-free VLA inference acceleration framework that simultaneously enables
state-of-the-art quantization and token pruning. We overcome the
incompatibility by co-designing the quantization and token pruning pipeline,
where we propose new quantization-aware token pruning criteria that work on an
aggressively quantized model while improving the quantizer design to enhance
pruning effectiveness. When applied to standard VLA models, SQAP-VLA yields
significant gains in computational efficiency and inference speed while
successfully preserving core model performance, achieving a $\times$1.93
speedup and up to a 4.5\% average success rate enhancement compared to the
original model.

</details>


### [85] [S-BEVLoc: BEV-based Self-supervised Framework for Large-scale LiDAR Global Localization](https://arxiv.org/abs/2509.09110)
*Chenghao Zhang,Lun Luo,Si-Yuan Cao,Xiaokai Bai,Yuncheng Jin,Zhu Yu,Beinan Yu,Yisen Wang,Hui-Liang Shen*

Main category: cs.CV

TL;DR: This paper presents a self-supervised framework, S-BEVLoc, for LiDAR global localization using bird's-eye view (BEV) images without requiring ground-truth poses.


<details>
  <summary>Details</summary>
Motivation: High-precision ground-truth pose acquisition for training LiDAR-based global localization models is costly and labor-intensive.

Method: S-BEVLoc uses BEV image triplets based on geographic distances, CNN for feature extraction, NetVLAD for global descriptor aggregation, and SoftCos loss for triplet learning.

Result: S-BEVLoc achieves state-of-the-art performance in place recognition, loop closure, and global localization across the KITTI and NCLT datasets.

Conclusion: S-BEVLoc provides scalable, accurate LiDAR localization without relying on ground-truth poses, outperforming supervised approaches.

Abstract: LiDAR-based global localization is an essential component of simultaneous
localization and mapping (SLAM), which helps loop closure and re-localization.
Current approaches rely on ground-truth poses obtained from GPS or SLAM
odometry to supervise network training. Despite the great success of these
supervised approaches, substantial cost and effort are required for
high-precision ground-truth pose acquisition. In this work, we propose
S-BEVLoc, a novel self-supervised framework based on bird's-eye view (BEV) for
LiDAR global localization, which eliminates the need for ground-truth poses and
is highly scalable. We construct training triplets from single BEV images by
leveraging the known geographic distances between keypoint-centered BEV
patches. Convolutional neural network (CNN) is used to extract local features,
and NetVLAD is employed to aggregate global descriptors. Moreover, we introduce
SoftCos loss to enhance learning from the generated triplets. Experimental
results on the large-scale KITTI and NCLT datasets show that S-BEVLoc achieves
state-of-the-art performance in place recognition, loop closure, and global
localization tasks, while offering scalability that would require extra effort
for supervised approaches.

</details>


### [86] [FPI-Det: a face--phone Interaction Dataset for phone-use detection and understanding](https://arxiv.org/abs/2509.09111)
*Jianqin Gao,Tianqi Wang,Yu Zhang,Yishu Zhang,Chenyuan Wang,Allan Dong,Zihao Wang*

Main category: cs.CV

TL;DR: The paper introduces FPI-Det, a dataset for detecting human-mobile device interactions under diverse conditions, to address limitations in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Mobile device usage poses challenges for vision systems in contexts like safety monitoring and productivity, requiring fine-grained analysis of human-device interactions.

Method: FPI-Det dataset includes 22,879 annotated images across various scenarios, evaluating models like YOLO and DETR for performance on factors such as occlusion and scale variation.

Result: Baseline performance of YOLO and DETR detectors is evaluated, highlighting the impact of object size, occlusion, and environment on detection.

Conclusion: FPI-Det fills a gap in fine-grained human-device interaction detection, and its annotations enable advancements in vision systems for diverse scenarios.

Abstract: The widespread use of mobile devices has created new challenges for vision
systems in safety monitoring, workplace productivity assessment, and attention
management. Detecting whether a person is using a phone requires not only
object recognition but also an understanding of behavioral context, which
involves reasoning about the relationship between faces, hands, and devices
under diverse conditions. Existing generic benchmarks do not fully capture such
fine-grained human--device interactions. To address this gap, we introduce the
FPI-Det, containing 22{,}879 images with synchronized annotations for faces and
phones across workplace, education, transportation, and public scenarios. The
dataset features extreme scale variation, frequent occlusions, and varied
capture conditions. We evaluate representative YOLO and DETR detectors,
providing baseline results and an analysis of performance across object sizes,
occlusion levels, and environments. Source code and dataset is available at
https://github.com/KvCgRv/FPI-Det.

</details>


### [87] [Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception](https://arxiv.org/abs/2509.09297)
*Spyridon Loukovitis,Anastasios Arsenos,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: The paper introduces an open-set detection framework for UAV object detection under real-world conditions, combating domain shifts and corrupted data.


<details>
  <summary>Details</summary>
Motivation: UAV autonomy faces risks in safety-critical scenarios due to performance degradation of closed-set detectors under domain shifts and flight data corruption.

Method: The approach models semantic uncertainty using entropy in the embedding space, integrates spectral normalization, and applies temperature scaling for better open-set detection.

Result: The method achieved up to a 10% AUROC improvement on aerial benchmarks and consistently outperformed baselines in real-world tests.

Conclusion: The framework enhances open-set detection robustness and suitability for UAV applications in dynamic air-to-air conditions, balancing robustness and accuracy.

Abstract: Open-set detection is crucial for robust UAV autonomy in air-to-air object
detection under real-world conditions. Traditional closed-set detectors degrade
significantly under domain shifts and flight data corruption, posing risks to
safety-critical applications. We propose a novel, model-agnostic open-set
detection framework designed specifically for embedding-based detectors. The
method explicitly handles unknown object rejection while maintaining robustness
against corrupted flight data. It estimates semantic uncertainty via entropy
modeling in the embedding space and incorporates spectral normalization and
temperature scaling to enhance open-set discrimination. We validate our
approach on the challenging AOT aerial benchmark and through extensive
real-world flight tests. Comprehensive ablation studies demonstrate consistent
improvements over baseline methods, achieving up to a 10\% relative AUROC gain
compared to standard YOLO-based detectors. Additionally, we show that
background rejection further strengthens robustness without compromising
detection accuracy, making our solution particularly well-suited for reliable
UAV perception in dynamic air-to-air environments.

</details>


### [88] [Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation Models and Text-to-image Attention](https://arxiv.org/abs/2509.09116)
*Junhao Xing,Ryohei Miyakawa,Yang Yang,Xinpeng Liu,Risa Shinoda,Hiroaki Santo,Yosuke Toda,Fumio Okura*

Main category: cs.CV

TL;DR: Introduces ZeroPlantSeg, a zero-shot segmentation system for rosette-shaped plant individuals, leveraging foundational segmentation and vision-language models.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of hierarchical segmentation for plant individuals, specifically to eliminate the need for species-specific annotation and reduce human labor.

Method: ZeroPlantSeg combines foundational segmentation models for leaf instance extraction with vision-language models to identify and segment entire plant individuals without training.

Result: Outperformed existing zero-shot methods and showed better cross-domain performance compared to supervised methods, tested across various datasets with different plant species and growth stages.

Conclusion: ZeroPlantSeg is effective for rosette-shaped plant segmentation, offering superior performance in zero-shot scenarios and is accessible for further use via open implementations.

Abstract: Foundation segmentation models achieve reasonable leaf instance extraction
from top-view crop images without training (i.e., zero-shot). However,
segmenting entire plant individuals with each consisting of multiple
overlapping leaves remains challenging. This problem is referred to as a
hierarchical segmentation task, typically requiring annotated training
datasets, which are often species-specific and require notable human labor. To
address this, we introduce ZeroPlantSeg, a zero-shot segmentation for
rosette-shaped plant individuals from top-view images. We integrate a
foundation segmentation model, extracting leaf instances, and a vision-language
model, reasoning about plants' structures to extract plant individuals without
additional training. Evaluations on datasets with multiple plant species,
growth stages, and shooting environments demonstrate that our method surpasses
existing zero-shot methods and achieves better cross-domain performance than
supervised methods. Implementations are available at
https://github.com/JunhaoXing/ZeroPlantSeg.

</details>


### [89] [Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles](https://arxiv.org/abs/2509.09349)
*Ian Nell,Shane Gilroy*

Main category: cs.CV

TL;DR: The paper introduces an external observation-based system using computer vision to detect distracted/impaired driving behaviors, employing YOLO object detection and custom algorithms for real-time analysis.


<details>
  <summary>Details</summary>
Motivation: To address the persistent issue of road traffic accidents caused by human error, particularly distractions and impairments, by providing a method that works even for non-connected vehicles.

Method: The study utilizes computer vision techniques including YOLO object detection, real-time object tracking, lateral displacement analysis, and lane position monitoring to classify driver behavior.

Result: Experimental evaluations confirmed the system's reliability and adaptability across diverse video datasets under various road and environmental conditions.

Conclusion: The proposed vision-based system successfully identifies unsafe driving behaviors and offers a viable alternative to inter-vehicular communication for behavioral analysis.

Abstract: Road traffic accidents remain a significant global concern, with human error,
particularly distracted and impaired driving, among the leading causes. This
study introduces a novel driver behavior classification system that uses
external observation techniques to detect indicators of distraction and
impairment. The proposed framework employs advanced computer vision
methodologies, including real-time object tracking, lateral displacement
analysis, and lane position monitoring. The system identifies unsafe driving
behaviors such as excessive lateral movement and erratic trajectory patterns by
implementing the YOLO object detection model and custom lane estimation
algorithms. Unlike systems reliant on inter-vehicular communication, this
vision-based approach enables behavioral analysis of non-connected vehicles.
Experimental evaluations on diverse video datasets demonstrate the framework's
reliability and adaptability across varying road and environmental conditions.

</details>


### [90] [Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval](https://arxiv.org/abs/2509.09118)
*Tianlu Zheng,Yifan Zhang,Xiang An,Ziyong Feng,Kaicheng Yang,Qichuan Ding*

Main category: cs.CV

TL;DR: This paper tackles challenges in adapting CLIP to person representation learning by proposing improvements in both data curation and model architecture, resulting in state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of annotated person-centric image-text datasets and the limitations of global contrastive learning in maintaining fine-grained discriminative features for person representation.

Method: The authors developed a noise-resistant data pipeline yielding the WebPerson dataset from web-sourced images and proposed the GA-DMS framework, which employs adaptive masking and masked token predictions for improved cross-modal alignment.

Result: Their GA-DMS framework achieved state-of-the-art performance across various person representation benchmarks.

Conclusion: Enhancing data quality and cross-modal alignment methods can significantly improve person representation learning applications.

Abstract: Although Contrastive Language-Image Pre-training (CLIP) exhibits strong
performance across diverse vision tasks, its application to person
representation learning faces two critical challenges: (i) the scarcity of
large-scale annotated vision-language data focused on person-centric images,
and (ii) the inherent limitations of global contrastive learning, which
struggles to maintain discriminative local features crucial for fine-grained
matching while remaining vulnerable to noisy text tokens. This work advances
CLIP for person representation learning through synergistic improvements in
data curation and model architecture. First, we develop a noise-resistant data
construction pipeline that leverages the in-context learning capabilities of
MLLMs to automatically filter and caption web-sourced images. This yields
WebPerson, a large-scale dataset of 5M high-quality person-centric image-text
pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking
Synergetic) framework, which improves cross-modal alignment by adaptively
masking noisy textual tokens based on the gradient-attention similarity score.
Additionally, we incorporate masked token prediction objectives that compel the
model to predict informative text tokens, enhancing fine-grained semantic
representation learning. Extensive experiments show that GA-DMS achieves
state-of-the-art performance across multiple benchmarks.

</details>


### [91] [ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain](https://arxiv.org/abs/2509.09130)
*Bin Huang,Kang Chen,Bingxuan Li,Huafeng Liu,Qiegen Liu*

Main category: cs.CV

TL;DR: The paper introduces ALL-PET, a PET imaging foundation model relying on limited data and low resources, leveraging latent diffusion models and innovative techniques for efficient sinogram generation.


<details>
  <summary>Details</summary>
Motivation: The development of PET imaging models is hindered by limited labeled data and computational constraints, necessitating efficient solutions for enriched generalization and performance.

Method: The approach uses three key innovations: Radon mask augmentation strategy to create diverse data, positive/negative mask constraints for geometric consistency, and transparent medical attention to enhance focus on lesion-related regions, all while ensuring efficient resource use.

Result: ALL-PET achieves high-quality sinogram generation using just 500 samples, comparable to models trained on larger datasets, and performs well across multiple PET imaging tasks.

Conclusion: ALL-PET is a resource-efficient, generalizable PET imaging model employing innovative strategies to overcome data and computational limitations, demonstrating promising performance in diverse applications.

Abstract: Building large-scale foundation model for PET imaging is hindered by limited
access to labeled data and insufficient computational resources. To overcome
data scarcity and efficiency limitations, we propose ALL-PET, a low-resource,
low-shot PET foundation model operating directly in the projection domain.
ALL-PET leverages a latent diffusion model (LDM) with three key innovations.
First, we design a Radon mask augmentation strategy (RMAS) that generates over
200,000 structurally diverse training samples by projecting randomized
image-domain masks into sinogram space, significantly improving generalization
with minimal data. This is extended by a dynamic multi-mask (DMM) mechanism
that varies mask quantity and distribution, enhancing data diversity without
added model complexity. Second, we implement positive/negative mask constraints
to embed strict geometric consistency, reducing parameter burden while
preserving generation quality. Third, we introduce transparent medical
attention (TMA), a parameter-free, geometry-driven mechanism that enhances
lesion-related regions in raw projection data. Lesion-focused attention maps
are derived from coarse segmentation, covering both hypermetabolic and
hypometabolic areas, and projected into sinogram space for physically
consistent guidance. The system supports clinician-defined ROI adjustments,
ensuring flexible, interpretable, and task-adaptive emphasis aligned with PET
acquisition physics. Experimental results show ALL-PET achieves high-quality
sinogram generation using only 500 samples, with performance comparable to
models trained on larger datasets. ALL-PET generalizes across tasks including
low-dose reconstruction, attenuation correction, delayed-frame prediction, and
tracer separation, operating efficiently with memory use under 24GB.

</details>


### [92] [Visual Grounding from Event Cameras](https://arxiv.org/abs/2509.09584)
*Lingdong Kong,Dongyue Lu,Ao Liang,Rong Li,Yuhao Dong,Tianshuai Hu,Lai Xing Ng,Wei Tsang Ooi,Benoit R. Cottereau*

Main category: cs.CV

TL;DR: Talk2Event is a large-scale benchmark dataset integrating event camera data with natural language for dynamic object grounding tasks.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in integrating event camera data with natural language understanding, enabling improved multimodal perception.

Method: The creation of a benchmark dataset (Talk2Event) comprising real-world driving scenes and richly annotated with spatial, temporal, and relational attributes for objects and their surroundings.

Result: Talk2Event features 5,567 scenes, 13,458 objects, and 30,000 annotated referring expressions, enabling compositional and contextual language-based object reasoning.

Conclusion: Talk2Event provides a foundation for advancing multimodal perception and contextual understanding in dynamic environments, supporting areas like robotics and human-AI interaction.

Abstract: Event cameras capture changes in brightness with microsecond precision and
remain reliable under motion blur and challenging illumination, offering clear
advantages for modeling highly dynamic scenes. Yet, their integration with
natural language understanding has received little attention, leaving a gap in
multimodal perception. To address this, we introduce Talk2Event, the first
large-scale benchmark for language-driven object grounding using event data.
Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes,
13,458 annotated objects, and more than 30,000 carefully validated referring
expressions. Each expression is enriched with four structured attributes --
appearance, status, relation to the viewer, and relation to surrounding objects
-- that explicitly capture spatial, temporal, and relational cues. This
attribute-centric design supports interpretable and compositional grounding,
enabling analysis that moves beyond simple object recognition to contextual
reasoning in dynamic environments. We envision Talk2Event as a foundation for
advancing multimodal and temporally-aware perception, with applications
spanning robotics, human-AI interaction, and so on.

</details>


### [93] [Noise-Robust Topology Estimation of 2D Image Data via Neural Networks and Persistent Homology](https://arxiv.org/abs/2509.09140)
*Dylan Peek,Matthew P. Skerritt,Stephan Chalup*

Main category: cs.CV

TL;DR: The paper examines the noise robustness of artificial neural networks (ANNs) trained to predict Betti numbers in 2D images, comparing this approach against persistent homology (PH) pipelines, and finds ANNs to be more robust under noise.


<details>
  <summary>Details</summary>
Motivation: To evaluate the efficacy and noise robustness of ANNs in topological analysis compared to traditional persistent homology (PH) techniques, particularly in predicting Betti numbers in 2D images.

Method: The study trains supervised ANNs to predict Betti numbers in 2D binary images and compares their performance with a PH pipeline using cubical complexes and SEDT across synthetic and real-world datasets.

Result: ANNs demonstrated better performance than PH pipelines in noisy scenarios, likely owing to their ability to learn contextual and geometric priors.

Conclusion: ANNs represent a promising alternative to PH techniques for topology estimation, especially under structural noise, due to their adaptability and learning capabilities.

Abstract: Persistent Homology (PH) and Artificial Neural Networks (ANNs) offer
contrasting approaches to inferring topological structure from data. In this
study, we examine the noise robustness of a supervised neural network trained
to predict Betti numbers in 2D binary images. We compare an ANN approach
against a PH pipeline based on cubical complexes and the Signed Euclidean
Distance Transform (SEDT), which is a widely adopted strategy for noise-robust
topological analysis. Using one synthetic and two real-world datasets, we show
that ANNs can outperform this PH approach under noise, likely due to their
capacity to learn contextual and geometric priors from training data. Though
still emerging, the use of ANNs for topology estimation offers a compelling
alternative to PH under structural noise.

</details>


### [94] [Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation](https://arxiv.org/abs/2509.09143)
*Yuiko Uchida,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: This study introduces OSIM, a metric for 3D scene evaluation emphasizing object-focused perception for better alignment with human visual understanding.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation metrics fail to adequately align with human perception, as they assess overall image quality rather than focusing on individual objects, which are essential to human visual recognition.

Method: The paper proposes OSIM, leveraging an object detection model and its features to quantify object properties within a 3D scene and conducts user studies to validate its alignment with human perception.

Result: OSIM shows closer alignment with human judgment compared to existing metrics, providing a standardized experimental setup for assessing recent 3D models.

Conclusion: OSIM enhances evaluation of 3D scenes by focusing on individual object similarly to human visual perception, offering insights into advancements in 3D reconstruction and generation.

Abstract: This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric
for 3D scenes that explicitly focuses on "objects," which are fundamental units
of human visual perception. Existing metrics assess overall image quality,
leading to discrepancies with human perception. Inspired by neuropsychological
insights, we hypothesize that human recognition of 3D scenes fundamentally
involves attention to individual objects. OSIM enables object-centric
evaluations by leveraging an object detection model and its feature
representations to quantify the "objectness" of each object in the scene. Our
user study demonstrates that OSIM aligns more closely with human perception
compared to existing metrics. We also analyze the characteristics of OSIM using
various approaches. Moreover, we re-evaluate recent 3D reconstruction and
generation models under a standardized experimental setup to clarify
advancements in this field. The code is available at
https://github.com/Objectness-Similarity/OSIM.

</details>


### [95] [Video Understanding by Design: How Datasets Shape Architectures and Insights](https://arxiv.org/abs/2509.09151)
*Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: This survey highlights how video datasets influence the evolution of model architectures and provides guidance for aligning model designs with dataset characteristics.


<details>
  <summary>Details</summary>
Motivation: Existing surveys tend to classify video understanding models by task or architecture family, missing the influence of dataset structures on model evolution.

Method: The authors adopt a dataset-driven perspective, analyzing pressures like motion complexity, temporal span, hierarchical composition, and multimodal richness, and reinterpret model milestones accordingly.

Result: The survey presents a unified framework connecting datasets, inductive biases, and architectures, explaining past model developments and offering practical design guidance.

Conclusion: This work bridges the gap between datasets and model evolution, delivering a retrospective synthesis and a framework for advancing video understanding through informed model design.

Abstract: Video understanding has advanced rapidly, fueled by increasingly complex
datasets and powerful architectures. Yet existing surveys largely classify
models by task or family, overlooking the structural pressures through which
datasets guide architectural evolution. This survey is the first to adopt a
dataset-driven perspective, showing how motion complexity, temporal span,
hierarchical composition, and multimodal richness impose inductive biases that
models should encode. We reinterpret milestones, from two-stream and 3D CNNs to
sequential, transformer, and multimodal foundation models, as concrete
responses to these dataset-driven pressures. Building on this synthesis, we
offer practical guidance for aligning model design with dataset invariances
while balancing scalability and task demands. By unifying datasets, inductive
biases, and architectures into a coherent framework, this survey provides both
a comprehensive retrospective and a prescriptive roadmap for advancing
general-purpose video understanding.

</details>


### [96] [OCELOT 2023: Cell Detection from Cell-Tissue Interaction Challenge](https://arxiv.org/abs/2509.09153)
*JaeWoong Shin,Jeongun Ryu,Aaron Valero Puche,Jinhee Lee,Biagio Brattoli,Wonkyung Jung,Soo Ick Cho,Kyunghyun Paeng,Chan-Young Ock,Donggeun Yoo,Zhaoyang Li,Wangkai Li,Huayu Mai,Joshua Millward,Zhen He,Aiden Nibali,Lydia Anette Schoenpflug,Viktor Hendrik Koelzer,Xu Shuoyu,Ji Zheng,Hu Bin,Yu-Wen Lo,Ching-Hui Yang,Sérgio Pereira*

Main category: cs.CV

TL;DR: The paper discusses the OCELOT 2023 challenge, emphasizing the importance of understanding multi-scale cell-tissue interactions in pathology. Results show models using multi-scale approaches outperformed traditional methods.


<details>
  <summary>Details</summary>
Motivation: Pathologists utilize multi-scale examination of Whole-Slide Images for comprehensive diagnosis, but current deep learning models struggle to replicate this. There is a lack of datasets addressing overlapping cell and tissue semantics.

Method: The OCELOT 2023 challenge introduced a dataset with multi-scale annotations for overlapping cell detection and tissue segmentation across six organs, using 673 image pairs from TCGA. Participants developed models incorporating cell-tissue relationships.

Result: Top-performing models improved F1-scores by up to 7.99 points compared to baseline models focusing only on cells, showcasing substantial performance improvement through multi-scale semantics.

Conclusion: Incorporating multi-scale semantics into models is crucial for replicating pathologists' diagnostic approaches, as demonstrated by results from the OCELOT 2023 challenge and its dataset.

Abstract: Pathologists routinely alternate between different magnifications when
examining Whole-Slide Images, allowing them to evaluate both broad tissue
morphology and intricate cellular details to form comprehensive diagnoses.
However, existing deep learning-based cell detection models struggle to
replicate these behaviors and learn the interdependent semantics between
structures at different magnifications. A key barrier in the field is the lack
of datasets with multi-scale overlapping cell and tissue annotations. The
OCELOT 2023 challenge was initiated to gather insights from the community to
validate the hypothesis that understanding cell and tissue (cell-tissue)
interactions is crucial for achieving human-level performance, and to
accelerate the research in this field. The challenge dataset includes
overlapping cell detection and tissue segmentation annotations from six organs,
comprising 673 pairs sourced from 306 The Cancer Genome Atlas (TCGA)
Whole-Slide Images with hematoxylin and eosin staining, divided into training,
validation, and test subsets. Participants presented models that significantly
enhanced the understanding of cell-tissue relationships. Top entries achieved
up to a 7.99 increase in F1-score on the test set compared to the baseline
cell-only model that did not incorporate cell-tissue relationships. This is a
substantial improvement in performance over traditional cell-only detection
methods, demonstrating the need for incorporating multi-scale semantics into
the models. This paper provides a comparative analysis of the methods used by
participants, highlighting innovative strategies implemented in the OCELOT 2023
challenge.

</details>


### [97] [RT-DETR++ for UAV Object Detection](https://arxiv.org/abs/2509.09157)
*Yuan Shufang*

Main category: cs.CV

TL;DR: The paper proposes RT-DETR++, enhancing small object detection in UAV imagery via advanced encoder mechanisms without added computational burden.


<details>
  <summary>Details</summary>
Motivation: Address challenges in UAV object detection, such as handling densely packed small objects, scale variations, and occlusions.

Method: Introduced channel-gated attention mechanisms for feature layer propagation, and CSP-PAC for improved feature fusion with multi-scale integration.

Result: Achieved better detection of small, densely packed objects while preserving real-time processing speeds.

Conclusion: The improved encoder design offers effective solutions for enhancing real-time detection systems without compromising efficiency.

Abstract: Object detection in unmanned aerial vehicle (UAV) imagery presents
significant challenges. Issues such as densely packed small objects, scale
variations, and occlusion are commonplace. This paper introduces RT-DETR++,
which enhances the encoder component of the RT-DETR model. Our improvements
focus on two key aspects. First, we introduce a channel-gated attention-based
upsampling/downsampling (AU/AD) mechanism. This dual-path system minimizes
errors and preserves details during feature layer propagation. Second, we
incorporate CSP-PAC during feature fusion. This technique employs parallel
hollow convolutions to process local and contextual information within the same
layer, facilitating the integration of multi-scale features. Evaluation
demonstrates that our novel neck design achieves superior performance in
detecting small and densely packed objects. The model maintains sufficient
speed for real-time detection without increasing computational complexity. This
study provides an effective approach for feature encoding design in real-time
detection systems.

</details>


### [98] [A Knowledge Noise Mitigation Framework for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2509.09159)
*Zhiyue Liu,Sihang Liu,Jinyuan Liu,Xinru Zhang*

Main category: cs.CV

TL;DR: The paper addresses KB-VQA challenges by proposing a training-free framework that reduces redundant and noisy knowledge for improved answering performance.


<details>
  <summary>Details</summary>
Motivation: Existing KB-VQA methods struggle with redundant and noisy external knowledge, impacting their answering accuracy.

Method: The proposed framework enhances relevance in knowledge retrieval, uses large models to extract answer-relevant knowledge, and adopts a selective integration strategy to only incorporate knowledge when needed.

Result: The approach effectively reduces noise and improves performance, outperforming state-of-the-art KB-VQA methods in extensive experiments.

Conclusion: By focusing and selectively using external knowledge, the paper demonstrates a practical and noise-reducing framework for KB-VQA that improves overall answer accuracy.

Abstract: Knowledge-based visual question answering (KB-VQA) requires a model to
understand images and utilize external knowledge to provide accurate answers.
Existing approaches often directly augment models with retrieved information
from knowledge sources while ignoring substantial knowledge redundancy, which
introduces noise into the answering process. To address this, we propose a
training-free framework with knowledge focusing for KB-VQA, that mitigates the
impact of noise by enhancing knowledge relevance and reducing redundancy.
First, for knowledge retrieval, our framework concludes essential parts from
the image-question pairs, creating low-noise queries that enhance the retrieval
of highly relevant knowledge. Considering that redundancy still persists in the
retrieved knowledge, we then prompt large models to identify and extract
answer-beneficial segments from knowledge. In addition, we introduce a
selective knowledge integration strategy, allowing the model to incorporate
knowledge only when it lacks confidence in answering the question, thereby
mitigating the influence of redundant information. Our framework enables the
acquisition of accurate and critical knowledge, and extensive experiments
demonstrate that it outperforms state-of-the-art methods.

</details>


### [99] [CWSSNet: Hyperspectral Image Classification Enhanced by Wavelet Domain Convolution](https://arxiv.org/abs/2509.09163)
*Yulin Tong,Fengzong Zhang,Haiqin Cheng*

Main category: cs.CV

TL;DR: The study introduces CWSSNet, a hyperspectral image classification framework using spectral-spatial features and wavelet convolution, achieving high classification accuracies in a forestry and agriculture application.


<details>
  <summary>Details</summary>
Motivation: To address feature redundancy challenges in hyperspectral images while improving ground object classification for applications in forestry and precision agriculture.

Method: The proposed CWSSNet integrates multimodal information, 3D spectral-spatial features, and wavelet convolution using wavelet domain decomposition and convolutional attention modules.

Result: CWSSNet achieved high metrics: mIoU of 74.50%, mAcc of 82.73%, and mF1 of 84.94% in Yugan County, excelling in classifying water bodies, vegetation, and bare land.

Conclusion: CWSSNet demonstrates robust performance under small-sample training conditions and effectively overcomes limitations of traditional hyperspectral classification methods.

Abstract: Hyperspectral remote sensing technology has significant application value in
fields such as forestry ecology and precision agriculture, while also putting
forward higher requirements for fine ground object classification. However,
although hyperspectral images are rich in spectral information and can improve
recognition accuracy, they tend to cause prominent feature redundancy due to
their numerous bands, high dimensionality, and spectral mixing characteristics.
To address this, this study used hyperspectral images from the ZY1F satellite
as a data source and selected Yugan County, Shangrao City, Jiangxi Province as
the research area to perform ground object classification research. A
classification framework named CWSSNet was proposed, which integrates 3D
spectral-spatial features and wavelet convolution. This framework integrates
multimodal information us-ing a multiscale convolutional attention module and
breaks through the classification performance bottleneck of traditional methods
by introducing multi-band decomposition and convolution operations in the
wavelet domain. The experiments showed that CWSSNet achieved 74.50\%, 82.73\%,
and 84.94\% in mean Intersection over Union (mIoU), mean Accuracy (mAcc), and
mean F1-score (mF1) respectively in Yugan County. It also obtained the highest
Intersection over Union (IoU) in the classifica-tion of water bodies,
vegetation, and bare land, demonstrating good robustness. Additionally, when
the training set proportion was 70\%, the increase in training time was
limited, and the classification effect was close to the optimal level,
indicating that the model maintains reliable performance under small-sample
training conditions.

</details>


### [100] [Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios](https://arxiv.org/abs/2509.09172)
*Chunxiao Li,Xiaoxiao Wang,Meiling Li,Boming Miao,Peng Sun,Yunjian Zhang,Xiangyang Ji,Yao Zhu*

Main category: cs.CV

TL;DR: This paper introduces the Real-World Robustness Dataset (RRDataset) for evaluating AI-generated image detection under real-world challenges.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current AI-generated image detection methods, particularly in real-world, complex scenarios.

Method: The authors created RRDataset focusing on scenario generalization, internet transmission robustness, and re-digitization robustness. They benchmarked 17 detection models and 10 vision-language models, conducting a human study to evaluate few-shot learning for detecting AI-generated images.

Result: The study highlighted the limitations of current AI detection models under real-world conditions and the potential of human adaptability in improving detection performance.

Conclusion: The paper emphasizes the need for robust detection algorithms, leveraging insights from human adaptability and addressing real-world complexities in AI-generated image detection.

Abstract: With the rapid advancement of generative models, highly realistic image
synthesis has posed new challenges to digital security and media credibility.
Although AI-generated image detection methods have partially addressed these
concerns, a substantial research gap remains in evaluating their performance
under complex real-world conditions. This paper introduces the Real-World
Robustness Dataset (RRDataset) for comprehensive evaluation of detection models
across three dimensions: 1) Scenario Generalization: RRDataset encompasses
high-quality images from seven major scenarios (War and Conflict, Disasters and
Accidents, Political and Social Events, Medical and Public Health, Culture and
Religion, Labor and Production, and everyday life), addressing existing dataset
gaps from a content perspective. 2) Internet Transmission Robustness: examining
detector performance on images that have undergone multiple rounds of sharing
across various social media platforms. 3) Re-digitization Robustness: assessing
model effectiveness on images altered through four distinct re-digitization
methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on
RRDataset and conducted a large-scale human study involving 192 participants to
investigate human few-shot learning capabilities in detecting AI-generated
images. The benchmarking results reveal the limitations of current AI detection
methods under real-world conditions and underscore the importance of drawing on
human adaptability to develop more robust detection algorithms.

</details>


### [101] [Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection](https://arxiv.org/abs/2509.09183)
*Jiasheng Guo,Xin Gao,Yuxiang Yan,Guanghao Li,Jian Pu*

Main category: cs.CV

TL;DR: This paper introduces Dark-ISP, a lightweight ISP plugin that processes Bayer RAW images for improved low-light object detection, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to enhance object detection in low-light conditions by tackling the challenges of degraded image quality and inefficiency in existing RAW image-based approaches.

Method: Dark-ISP leverages a deconstructed ISP pipeline with linear and nonlinear modules, optimized through task-driven losses and equipped with content-aware adaptability and physics-informed priors. It also includes a Self-Boost mechanism for module cooperation.

Result: Extensive experiments across three RAW image datasets show that Dark-ISP surpasses state-of-the-art methods in low-light object detection while remaining parameter-efficient.

Conclusion: The proposed Dark-ISP plugin effectively improves low-light object detection through an innovative and efficient framework, making it suitable for real-world applications.

Abstract: Low-light Object detection is crucial for many real-world applications but
remains challenging due to degraded image quality. While recent studies have
shown that RAW images offer superior potential over RGB images, existing
approaches either use RAW-RGB images with information loss or employ complex
frameworks. To address these, we propose a lightweight and self-adaptive Image
Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW
images in dark environments, enabling seamless end-to-end training for object
detection. Our key innovations are: (1) We deconstruct conventional ISP
pipelines into sequential linear (sensor calibration) and nonlinear (tone
mapping) sub-modules, recasting them as differentiable components optimized
through task-driven losses. Each module is equipped with content-aware
adaptability and physics-informed priors, enabling automatic RAW-to-RGB
conversion aligned with detection objectives. (2) By exploiting the ISP
pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that
facilitates cooperation between sub-modules. Through extensive experiments on
three RAW image datasets, we demonstrate that our method outperforms
state-of-the-art RGB- and RAW-based detection approaches, achieving superior
results with minimal parameters in challenging low-light environments.

</details>


### [102] [VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results](https://arxiv.org/abs/2509.09190)
*Hanwei Zhu,Haoning Wu,Zicheng Zhang,Lingyu Zhu,Yixuan Li,Peilin Chen,Shiqi Wang,Chris Wei Zhou,Linhan Cao,Wei Sun,Xiangyang Zhu,Weixia Zhang,Yucheng Zhu,Jing Liu,Dandan Zhu,Guangtao Zhai,Xiongkuo Min,Zhichao Zhang,Xinyue Li,Shubo Xu,Anh Dao,Yifan Li,Hongyuan Yu,Jiaojiao Yi,Yiding Tian,Yupeng Wu,Feiran Sun,Lijuan Liao,Song Jiang*

Main category: cs.CV

TL;DR: This paper summarizes the VQualA 2025 Challenge, which evaluated large multimodal models (LMMs) on visual quality reasoning across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to advance the ability of LMMs to reason about visual quality differences in a nuanced, open-domain context.

Method: The authors introduced a benchmark of visual quality comparison tasks involving single, paired, and multi-image groups, emphasizing binary preference and multiple-choice questions for holistic evaluations.

Result: Five models showcased emerging capabilities in instruction-tuned LMMs, with entries from around 100 participants in the challenge.

Conclusion: The challenge represents progress in visual quality assessment, fostering innovation in interpretable and human-aligned evaluation models.

Abstract: This paper presents a summary of the VQualA 2025 Challenge on Visual Quality
Comparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025
Workshop on Visual Quality Assessment. The challenge aims to evaluate and
enhance the ability of state-of-the-art LMMs to perform open-ended and detailed
reasoning about visual quality differences across multiple images. To this end,
the competition introduces a novel benchmark comprising thousands of
coarse-to-fine grained visual quality comparison tasks, spanning single images,
pairs, and multi-image groups. Each task requires models to provide accurate
quality judgments. The competition emphasizes holistic evaluation protocols,
including 2AFC-based binary preference and multi-choice questions (MCQs).
Around 100 participants submitted entries, with five models demonstrating the
emerging capabilities of instruction-tuned LMMs on quality assessment. This
challenge marks a significant step toward open-domain visual quality reasoning
and comparison and serves as a catalyst for future research on interpretable
and human-aligned quality evaluation systems.

</details>


### [103] [MGTraj: Multi-Granularity Goal-Guided Human Trajectory Prediction with Recursive Refinement Network](https://arxiv.org/abs/2509.09200)
*Ge Sun,Jun Ma*

Main category: cs.CV

TL;DR: The paper introduces MGTraj, a multi-granularity goal-guided model for human trajectory prediction using recursive refinement networks and velocity prediction, showing superior performance in benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional goal-guided approaches in human trajectory prediction focus on coarse and fine levels, neglecting intermediate granularity, which has potential to boost accuracy.

Method: MGTraj uses recursive encoding across granularities with a transformer-based refinement network, integrates features via weight-sharing, and complements prediction with a velocity-based auxiliary task.

Result: MGTraj demonstrated state-of-the-art performance on benchmark datasets like EHT/UCY and the Stanford Drone Dataset compared to baseline methods.

Conclusion: MGTraj proves that leveraging multi-granularity modeling in goal-guided frameworks can enhance prediction accuracy and capture diverse human dynamics effectively, pushing the boundaries of current techniques.

Abstract: Accurate human trajectory prediction is crucial for robotics navigation and
autonomous driving. Recent research has demonstrated that incorporating goal
guidance significantly enhances prediction accuracy by reducing uncertainty and
leveraging prior knowledge. Most goal-guided approaches decouple the prediction
task into two stages: goal prediction and subsequent trajectory completion
based on the predicted goal, which operate at extreme granularities:
coarse-grained goal prediction forecasts the overall intention, while
fine-grained trajectory completion needs to generate the positions for all
future timesteps. The potential utility of intermediate temporal granularity
remains largely unexplored, which motivates multi-granularity trajectory
modeling. While prior work has shown that multi-granularity representations
capture diverse scales of human dynamics and motion patterns, effectively
integrating this concept into goal-guided frameworks remains challenging. In
this paper, we propose MGTraj, a novel Multi-Granularity goal-guided model for
human Trajectory prediction. MGTraj recursively encodes trajectory proposals
from coarse to fine granularity levels. At each level, a transformer-based
recursive refinement network (RRN) captures features and predicts progressive
refinements. Features across different granularities are integrated using a
weight-sharing strategy, and velocity prediction is employed as an auxiliary
task to further enhance performance. Comprehensive experimental results in
EHT/UCY and Stanford Drone Dataset indicate that MGTraj outperforms baseline
methods and achieves state-of-the-art performance among goal-guided methods.

</details>


### [104] [Medverse: A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement](https://arxiv.org/abs/2509.09232)
*Jiesi Hu,Jianfeng Cao,Yanwu Yang,Chenfei Ye,Yixuan Zhang,Hanyang Peng,Ting Ma*

Main category: cs.CV

TL;DR: Medverse introduces a universal in-context learning (ICL) model for 3D medical imaging, addressing challenges in achieving high-quality predictions while incorporating diverse tasks and global anatomical understanding.


<details>
  <summary>Details</summary>
Motivation: Existing ICL models for medical imaging struggle with balancing high-fidelity predictions and global anatomical understanding, and lack a unified approach for handling diverse tasks across varied datasets.

Method: Medverse uses a next-scale autoregressive ICL framework for progressive refinement and incorporates a blockwise cross-attention module for efficient long-range interactions in 3D medical imaging tasks.

Result: Medverse outperformed existing ICL baselines across unseen clinical centers, organs, imaging modalities, and species while enhancing universal segmentation, transformation, and enhancement capabilities.

Conclusion: Medverse demonstrates its capability as a robust universal ICL model for medical imaging, setting a new benchmark and paving the way for further exploration in medical image processing without retraining.

Abstract: In-context learning (ICL) offers a promising paradigm for universal medical
image analysis, enabling models to perform diverse image processing tasks
without retraining. However, current ICL models for medical imaging remain
limited in two critical aspects: they cannot simultaneously achieve
high-fidelity predictions and global anatomical understanding, and there is no
unified model trained across diverse medical imaging tasks (e.g., segmentation
and enhancement) and anatomical regions. As a result, the full potential of ICL
in medical imaging remains underexplored. Thus, we present \textbf{Medverse}, a
universal ICL model for 3D medical imaging, trained on 22 datasets covering
diverse tasks in universal image segmentation, transformation, and enhancement
across multiple organs, imaging modalities, and clinical centers. Medverse
employs a next-scale autoregressive in-context learning framework that
progressively refines predictions from coarse to fine, generating consistent,
full-resolution volumetric outputs and enabling multi-scale anatomical
awareness. We further propose a blockwise cross-attention module that
facilitates long-range interactions between context and target inputs while
preserving computational efficiency through spatial sparsity. Medverse is
extensively evaluated on a broad collection of held-out datasets covering
previously unseen clinical centers, organs, species, and imaging modalities.
Results demonstrate that Medverse substantially outperforms existing ICL
baselines and establishes a novel paradigm for in-context learning. Code and
model weights will be made publicly available. Our model are publicly available
at https://github.com/jiesihu/Medverse.

</details>


### [105] [CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification](https://arxiv.org/abs/2509.09242)
*Mustafa Yurdakul,Sakir Tasdemir*

Main category: cs.CV

TL;DR: The study presents CoAtNeXt, a novel hybrid model for classifying gastric tissue images, achieving superior accuracy and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Manual histopathologic analysis of gastric tissues is labor-intensive, prone to variability, and lacks standard procedures. This paper aims to address these limitations by proposing automated methods.

Method: The proposed CoAtNeXt model combines CoAtNet architecture, enhanced ConvNeXtV2 blocks, and CBAM for better feature extraction. It was tested on two datasets for eight-class and binary classification, compared to CNNs and ViT models.

Result: CoAtNeXt demonstrated high performance: 96-98% accuracy, precision, recall, F1 score, and nearly perfect AUC on both datasets, outperforming other models and prior studies.

Conclusion: CoAtNeXt's robust performance indicates its potential to assist pathologists in gastric tissue analysis, improving diagnostic accuracy and reducing the workload.

Abstract: Background and objective Early diagnosis of gastric diseases is crucial to
prevent fatal outcomes. Although histopathologic examination remains the
diagnostic gold standard, it is performed entirely manually, making evaluations
labor-intensive and prone to variability among pathologists. Critical findings
may be missed, and lack of standard procedures reduces consistency. These
limitations highlight the need for automated, reliable, and efficient methods
for gastric tissue analysis. Methods In this study, a novel hybrid model named
CoAtNeXt was proposed for the classification of gastric tissue images. The
model is built upon the CoAtNet architecture by replacing its MBConv layers
with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block
Attention Module (CBAM) is integrated to improve local feature extraction
through channel and spatial attention mechanisms. The architecture was scaled
to achieve a balance between computational efficiency and classification
performance. CoAtNeXt was evaluated on two publicly available datasets,
HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary
classification, and was compared against 10 Convolutional Neural Networks
(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved
96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%
AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%
precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all
CNN and ViT models tested and surpassed previous studies in the literature.
Conclusion Experimental results show that CoAtNeXt is a robust architecture for
histopathological classification of gastric tissue images, providing
performance on binary and multiclass. Its highlights its potential to assist
pathologists by enhancing diagnostic accuracy and reducing workload.

</details>


### [106] [Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis](https://arxiv.org/abs/2509.09254)
*Jing Hao,Yuxuan Fan,Yanpeng Sun,Kaixin Guo,Lizhuo Lin,Jinrong Yang,Qi Yong H. Ai,Lun M. Wong,Hao Tang,Kuo Feng Hung*

Main category: cs.CV

TL;DR: The paper presents MMOral, a multimodal dataset and benchmark tailored for interpreting panoramic X-rays in dentistry, supplementing LVLMs' limitations through OralGPT fine-tuning.


<details>
  <summary>Details</summary>
Motivation: There is a need to address the underexplored domain of dentistry using LVLMs to interpret challenging medical imagery like panoramic X-rays.

Method: The authors introduced MMOral, a dataset with annotated images and instruction-following instances, developed OralGPT through supervised fine-tuning, and evaluated models via MMOral-Bench.

Result: Even the best current model achieves only 41.45% accuracy; OralGPT, after fine-tuning, shows a 24.73% performance improvement.

Conclusion: MMOral and OralGPT represent critical advancements for developing intelligent systems in dentistry, addressing gaps in general-purpose LVLMs.

Abstract: Recent advances in large vision-language models (LVLMs) have demonstrated
strong performance on general-purpose medical tasks. However, their
effectiveness in specialized domains such as dentistry remains underexplored.
In particular, panoramic X-rays, a widely used imaging modality in oral
radiology, pose interpretative challenges due to dense anatomical structures
and subtle pathological cues, which are not captured by existing medical
benchmarks or instruction datasets. To this end, we introduce MMOral, the first
large-scale multimodal instruction dataset and benchmark tailored for panoramic
X-ray interpretation. MMOral consists of 20,563 annotated images paired with
1.3 million instruction-following instances across diverse task types,
including attribute extraction, report generation, visual question answering,
and image-grounded dialogue. In addition, we present MMOral-Bench, a
comprehensive evaluation suite covering five key diagnostic dimensions in
dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the
best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing
significant limitations of current models in this domain. To promote the
progress of this specific domain, we also propose OralGPT, which conducts
supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated
MMOral instruction dataset. Remarkably, a single epoch of SFT yields
substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a
24.73% improvement. Both MMOral and OralGPT hold significant potential as a
critical foundation for intelligent dentistry and enable more clinically
impactful multimodal AI systems in the dental field. The dataset, model,
benchmark, and evaluation suite are available at
https://github.com/isbrycee/OralGPT.

</details>


### [107] [DATE: Dynamic Absolute Time Enhancement for Long Video Understanding](https://arxiv.org/abs/2509.09263)
*Chao Yuan,Yang Yang,Yehui Yang,Zach Cheng*

Main category: cs.CV

TL;DR: This paper proposes DATE, a method to improve video understanding in MLLMs through enhanced temporal reasoning using TIM and TASS strategies. It achieves superior performance on long video tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of poor temporal reasoning and event localization in existing multimodal large language models (MLLMs) for long videos.

Method: The DATE method introduces Timestamp Injection Mechanism (TIM) for continuous temporal reference and Temporal-Aware Similarity Sampling (TASS) for semantically guided video frame sampling. It uses a two-stage algorithm for alignment and retrieval.

Result: DATE significantly enhances time understanding and event localization, achieving state-of-the-art performance on benchmarks with both 7B and 72B models. The 7B model even outperforms many larger models.

Conclusion: By enhancing temporal reasoning with TIM and TASS, DATE improves video understanding in MLLMs, offering superior accuracy for key event localization in long videos.

Abstract: Long video understanding remains a fundamental challenge for multimodal large
language models (MLLMs), particularly in tasks requiring precise temporal
reasoning and event localization. Existing approaches typically adopt uniform
frame sampling and rely on implicit position encodings to model temporal order.
However, these methods struggle with long-range dependencies, leading to
critical information loss and degraded temporal comprehension. In this paper,
we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal
awareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a
semantically guided Temporal-Aware Similarity Sampling (TASS) strategy.
Specifically, we interleave video frame embeddings with textual timestamp
tokens to construct a continuous temporal reference system. We further
reformulate the video sampling problem as a vision-language retrieval task and
introduce a two-stage algorithm to ensure both semantic relevance and temporal
coverage: enriching each query into a descriptive caption to better align with
the vision feature, and sampling key event with a similarity-driven temporally
regularized greedy strategy. Our method achieves remarkable improvements w.r.t.
absolute time understanding and key event localization, resulting in
state-of-the-art performance among 7B and 72B models on hour-long video
benchmarks. Particularly, our 7B model even exceeds many 72B models on some
benchmarks.

</details>


### [108] [Unified Start, Personalized End: Progressive Pruning for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2509.09267)
*Linhao Li,Yiwen Ye,Ziyang Chen,Yong Xia*

Main category: cs.CV

TL;DR: PSP-Seg introduces a dynamic pruning framework to make 3D medical image segmentation more efficient and adaptable, significantly reducing computational resources while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: The heavy resource and time consumption of current 3D medical segmentation models limits their scalability and use in clinical environments.

Method: PSP-Seg employs progressive pruning combined with a block-wise pruning approach and functional decoupling loss to dynamically refine a redundant model.

Result: The lightweight version PSP-Seg-S matches state-of-the-art segmentation performance while reducing GPU memory usage (42-45%), training time (29-48%), and parameters (83-87%).

Conclusion: PSP-Seg is a promising framework that balances cost-efficiency with effective segmentation, making it suitable for clinical deployment.

Abstract: 3D medical image segmentation often faces heavy resource and time
consumption, limiting its scalability and rapid deployment in clinical
environments. Existing efficient segmentation models are typically static and
manually designed prior to training, which restricts their adaptability across
diverse tasks and makes it difficult to balance performance with resource
efficiency. In this paper, we propose PSP-Seg, a progressive pruning framework
that enables dynamic and efficient 3D segmentation. PSP-Seg begins with a
redundant model and iteratively prunes redundant modules through a combination
of block-wise pruning and a functional decoupling loss. We evaluate PSP-Seg on
five public datasets, benchmarking it against seven state-of-the-art models and
six efficient segmentation models. Results demonstrate that the lightweight
variant, PSP-Seg-S, achieves performance on par with nnU-Net while reducing GPU
memory usage by 42-45%, training time by 29-48%, and parameter number by 83-87%
across all datasets. These findings underscore PSP-Seg's potential as a
cost-effective yet high-performing alternative for widespread clinical
application.

</details>


### [109] [Visual Programmability: A Guide for Code-as-Thought in Chart Understanding](https://arxiv.org/abs/2509.09286)
*Bohao Tang,Yan Ma,Fei Zhang,Jiadi Su,Ethan Chern,Zhulin Hu,Zhixin Wang,Pengfei Liu,Ya Zhang*

Main category: cs.CV

TL;DR: The paper introduces an adaptive "Code-as-Thought" (CaT) strategy combined with Visual Programmability for chart understanding, improving reasoning capabilities of Vision-Language Models (VLMs).


<details>
  <summary>Details</summary>
Motivation: Existing approaches to chart understanding in VLMs are either tool-reliant or overly focused on a single reasoning strategy like text-based chain-of-thought, resulting in brittleness and challenges in verifying reasoning steps.

Method: The authors propose an adaptive framework where VLMs decide between a code-based symbolic reasoning pathway (CaT) or direct visual reasoning, guided by reinforcement learning with a dual-reward system.

Result: Experiments show robust performance across diverse benchmarks, demonstrating the effectiveness of dynamic reasoning pathway selection.

Conclusion: VLMs can learn not only to reason effectively but also to adaptively choose the most appropriate reasoning strategy for a given task, enhancing their versatility and robustness in chart understanding.

Abstract: Chart understanding presents a critical test to the reasoning capabilities of
Vision-Language Models (VLMs). Prior approaches face critical limitations: some
rely on external tools, making them brittle and constrained by a predefined
toolkit, while others fine-tune specialist models that often adopt a single
reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate
steps of text-based reasoning are difficult to verify, which complicates the
use of reinforcement-learning signals that reward factual accuracy. To address
this, we propose a Code-as-Thought (CaT) approach to represent the visual
information of a chart in a verifiable, symbolic format. Our key insight is
that this strategy must be adaptive: a fixed, code-only implementation
consistently fails on complex charts where symbolic representation is
unsuitable. This finding leads us to introduce Visual Programmability: a
learnable property that determines if a chart-question pair is better solved
with code or direct visual analysis. We implement this concept in an adaptive
framework where a VLM learns to choose between the CaT pathway and a direct
visual reasoning pathway. The selection policy of the model is trained with
reinforcement learning using a novel dual-reward system. This system combines a
data-accuracy reward to ground the model in facts and prevent numerical
hallucination, with a decision reward that teaches the model when to use each
strategy, preventing it from defaulting to a single reasoning mode. Experiments
demonstrate strong and robust performance across diverse chart-understanding
benchmarks. Our work shows that VLMs can be taught not only to reason but also
how to reason, dynamically selecting the optimal reasoning pathway for each
task.

</details>


### [110] [Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training](https://arxiv.org/abs/2509.09290)
*Anthony P. Addison,Felix Wagner,Wentian Xu,Natalie Voets,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: This paper presents an enhanced U-net segmentation model capable of processing both seen and unseen MRI modalities for brain lesion detection and analysis.


<details>
  <summary>Details</summary>
Motivation: Current segmentation models struggle to generalize across unseen MRI modalities and fail to utilize heterogeneous combinations of imaging data.

Method: The proposed model introduces modality-specific and modality-agnostic input pathways, combined with an image augmentation scheme synthesizing artificial MRI modalities for training.

Result: Experiments across 8 MRI databases demonstrate the model maintains performance for seen modalities while effectively handling unseen ones.

Conclusion: The approach offers a practical solution to improve brain lesion segmentation across diverse MRI modalities, enhancing clinical usability.

Abstract: Segmentation models are important tools for the detection and analysis of
lesions in brain MRI. Depending on the type of brain pathology that is imaged,
MRI scanners can acquire multiple, different image modalities (contrasts). Most
segmentation models for multimodal brain MRI are restricted to fixed modalities
and cannot effectively process new ones at inference. Some models generalize to
unseen modalities but may lose discriminative modality-specific information.
This work aims to develop a model that can perform inference on data that
contain image modalities unseen during training, previously seen modalities,
and heterogeneous combinations of both, thus allowing a user to utilize any
available imaging modalities. We demonstrate this is possible with a simple,
thus practical alteration to the U-net architecture, by integrating a
modality-agnostic input channel or pathway, alongside modality-specific input
channels. To train this modality-agnostic component, we develop an image
augmentation scheme that synthesizes artificial MRI modalities. Augmentations
differentially alter the appearance of pathological and healthy brain tissue to
create artificial contrasts between them while maintaining realistic anatomical
integrity. We evaluate the method using 8 MRI databases that include 5 types of
pathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and
white matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI,
DWI, ADC and FLAIR). The results demonstrate that the approach preserves the
ability to effectively process MRI modalities encountered during training,
while being able to process new, unseen modalities to improve its segmentation.
Project code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG

</details>


### [111] [Learning Object-Centric Representations in SAR Images with Multi-Level Feature Fusion](https://arxiv.org/abs/2509.09298)
*Oh-Tae Jang,Min-Gon Cho,Kyung-Tae Kim*

Main category: cs.CV

TL;DR: SlotSAR is a framework for disentangling target representations from background clutter in SAR images. It leverages multi-level representations and slot attention without requiring mask annotations.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of SAR images containing complex background clutter and noise, which affects the clarity of target representations.

Method: SlotSAR integrates high-level semantic features and low-level scattering features into a multi-level slot attention module for effective object-centric learning.

Result: SlotSAR outperforms existing object-centric learning methods, demonstrating state-of-the-art performance and preserving structural details in SAR images.

Conclusion: SlotSAR provides a robust framework that successfully disentangles target features from clutter in SAR imagery, advancing the field of SAR image analysis.

Abstract: Synthetic aperture radar (SAR) images contain not only targets of interest
but also complex background clutter, including terrain reflections and speckle
noise. In many cases, such clutter exhibits intensity and patterns that
resemble targets, leading models to extract entangled or spurious features.
Such behavior undermines the ability to form clear target representations,
regardless of the classifier. To address this challenge, we propose a novel
object-centric learning (OCL) framework, named SlotSAR, that disentangles
target representations from background clutter in SAR images without mask
annotations. SlotSAR first extracts high-level semantic features from SARATR-X
and low-level scattering features from the wavelet scattering network in order
to obtain complementary multi-level representations for robust target
characterization. We further present a multi-level slot attention module that
integrates these low- and high-level features to enhance slot-wise
representation distinctiveness, enabling effective OCL. Experimental results
demonstrate that SlotSAR achieves state-of-the-art performance in SAR imagery
by preserving structural details compared to existing OCL methods.

</details>


### [112] [Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization](https://arxiv.org/abs/2509.09307)
*Zhengzhao Lai,Youbin Zheng,Zhenyang Cai,Haonan Lyu,Jinpu Yang,Hongqing Liang,Yan Hu,Benyou Wang*

Main category: cs.CV

TL;DR: The paper introduces MatCha, a benchmark designed to test multimodal large language models (MLLMs) on materials characterization image understanding, revealing a performance gap compared to human experts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the underexplored capability of MLLMs in handling real-world materials characterization imaging data, which is critical for advancing materials science research.

Method: The authors developed MatCha, a benchmark comprising 1,500 expert-level questions spanning 21 tasks related to four stages of materials research, to evaluate MLLMs.

Result: State-of-the-art MLLMs show significant performance gaps when compared to human expertise, with particular struggles in complex tasks requiring higher visual perception and reasoning.

Conclusion: The study demonstrates that current MLLMs are insufficiently adaptable for real-world materials characterization scenarios and highlights MatCha's potential to drive progress in materials science and AI research.

Abstract: Materials characterization is fundamental to acquiring materials information,
revealing the processing-microstructure-property relationships that guide
material design and optimization. While multimodal large language models
(MLLMs) have recently shown promise in generative and predictive tasks within
materials science, their capacity to understand real-world characterization
imaging data remains underexplored. To bridge this gap, we present MatCha, the
first benchmark for materials characterization image understanding, comprising
1,500 questions that demand expert-level domain expertise. MatCha encompasses
four key stages of materials research comprising 21 distinct tasks, each
designed to reflect authentic challenges faced by materials scientists. Our
evaluation of state-of-the-art MLLMs on MatCha reveals a significant
performance gap compared to human experts. These models exhibit degradation
when addressing questions requiring higher-level expertise and sophisticated
visual perception. Simple few-shot and chain-of-thought prompting struggle to
alleviate these limitations. These findings highlight that existing MLLMs still
exhibit limited adaptability to real-world materials characterization
scenarios. We hope MatCha will facilitate future research in areas such as new
material discovery and autonomous scientific agents. MatCha is available at
https://github.com/FreedomIntelligence/MatCha.

</details>


### [113] [You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative Perception](https://arxiv.org/abs/2509.09310)
*Hao Si,Ehsan Javanmardi,Manabu Tsukada*

Main category: cs.CV

TL;DR: PHCP introduces a new framework for collaborative vehicle perception that adapts to heterogeneous models during inference without requiring joint training, achieving results comparable to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Real-world collaborative perception faces challenges because vehicles often have heterogeneous models, and existing methods require impractical joint training or advance model storage.

Method: PHCP uses a few-shot unsupervised domain adaptation approach, dynamically self-training an adapter during inference to align features without needing labeled data or prior joint training.

Result: PHCP achieves strong performance on the OPV2V dataset, performing comparably to state-of-the-art methods trained with a full dataset but requires only minimal unlabeled data.

Conclusion: PHCP effectively enables real-time heterogeneous collaborative perception in practical settings, eliminating reliance on joint training while maintaining high performance.

Abstract: Collaborative perception enables vehicles to overcome individual perception
limitations by sharing information, allowing them to see further and through
occlusions. In real-world scenarios, models on different vehicles are often
heterogeneous due to manufacturer variations. Existing methods for
heterogeneous collaborative perception address this challenge by fine-tuning
adapters or the entire network to bridge the domain gap. However, these methods
are impractical in real-world applications, as each new collaborator must
undergo joint training with the ego vehicle on a dataset before inference, or
the ego vehicle stores models for all potential collaborators in advance.
Therefore, we pose a new question: Can we tackle this challenge directly during
inference, eliminating the need for joint training? To answer this, we
introduce Progressive Heterogeneous Collaborative Perception (PHCP), a novel
framework that formulates the problem as few-shot unsupervised domain
adaptation. Unlike previous work, PHCP dynamically aligns features by
self-training an adapter during inference, eliminating the need for labeled
data and joint training. Extensive experiments on the OPV2V dataset demonstrate
that PHCP achieves strong performance across diverse heterogeneous scenarios.
Notably, PHCP achieves performance comparable to SOTA methods trained on the
entire dataset while using only a small amount of unlabeled data.

</details>


### [114] [Image Recognition with Vision and Language Embeddings of VLMs](https://arxiv.org/abs/2509.09311)
*Illia Volkov,Nikita Kisel,Klara Janouskova,Jiri Matas*

Main category: cs.CV

TL;DR: The paper evaluates the performance of vision-language models (VLMs) in language-guided and visual-only image classification, using various models and proposing a fusion method for improved classification.


<details>
  <summary>Details</summary>
Motivation: To understand and enhance the unexamined capabilities of vision-language models in purely visual inference and improve classification performance.

Method: The paper evaluates VLMs using ImageNet datasets, analyzing factors like prompt design and class diversity, and introduces a fusion method based on per-class precision.

Result: Language and vision show complementary strengths, and the proposed fusion method enhances classification accuracy.

Conclusion: The study highlights the complementary strengths of language and vision modalities and provides insights for improving image classification performance through a simple fusion approach.

Abstract: Vision-language models (VLMs) have enabled strong zero-shot classification
through image-text alignment. Yet, their purely visual inference capabilities
remain under-explored. In this work, we conduct a comprehensive evaluation of
both language-guided and vision-only image classification with a diverse set of
dual-encoder VLMs, including both well-established and recent models such as
SigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the
ImageNet-1k validation set and its label-corrected variant. The key factors
affecting accuracy are analysed, including prompt design, class diversity, the
number of neighbours in k-NN, and reference set size. We show that language and
vision offer complementary strengths, with some classes favouring textual
prompts and others better handled by visual similarity. To exploit this
complementarity, we introduce a simple, learning-free fusion method based on
per-class precision that improves classification performance. The code is
available at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition.

</details>


### [115] [Fine-Grained Customized Fashion Design with Image-into-Prompt benchmark and dataset from LMM](https://arxiv.org/abs/2509.09324)
*Hui Li,Yi You,Qiqi Chen,Bingfeng Zhang,George Q. Huang*

Main category: cs.CV

TL;DR: This paper introduces the BUG workflow with a large multimodal model (LMM) to streamline and enhance clothing design through chat and image-to-prompt features.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the limitations of current generative AI in fine-grained customization where lack of professional background knowledge creates challenges in achieving precise clothing designs.

Method: The authors propose the Better Understanding Generation (BUG) workflow that integrates a large multimodal model. It takes input from chat and image prompts to generate and customize clothing designs with minimal user expertise needed.

Result: A new FashionEdit dataset is created to benchmark the BUG workflow. The evaluations focus on generation similarity, user satisfaction, and the quality of the designs, demonstrating the model's effectiveness in real-world conditions.

Conclusion: The BUG workflow significantly lowers the barriers for users without professional expertise to design and edit clothing while enhancing creative potential with minimal human intervention.

Abstract: Generative AI evolves the execution of complex workflows in industry, where
the large multimodal model empowers fashion design in the garment industry.
Current generation AI models magically transform brainstorming into fancy
designs easily, but the fine-grained customization still suffers from text
uncertainty without professional background knowledge from end-users. Thus, we
propose the Better Understanding Generation (BUG) workflow with LMM to
automatically create and fine-grain customize the cloth designs from chat with
image-into-prompt. Our framework unleashes users' creative potential beyond
words and also lowers the barriers of clothing design/editing without further
human involvement. To prove the effectiveness of our model, we propose a new
FashionEdit dataset that simulates the real-world clothing design workflow,
evaluated from generation similarity, user satisfaction, and quality. The code
and dataset: https://github.com/detectiveli/FashionEdit.

</details>


### [116] [Exploring Pre-training Across Domains for Few-Shot Surgical Skill Assessment](https://arxiv.org/abs/2509.09327)
*Dimitrios Anastasiou,Razvan Caramalau,Nazir Sirajudeen,Matthew Boal,Philip Edwards,Justin Collins,John Kelly,Ashwin Sridhar,Maxine Tran,Faiz Mumtaz,Nevil Pavithran,Nader Francis,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: This paper explores self-supervised pre-training strategies for few-shot surgical skill assessment. It highlights the importance of domain-relevant datasets over large, unrelated datasets.


<details>
  <summary>Details</summary>
Motivation: Surgical skill assessment is challenging due to the scarcity of labeled data, necessitating scalable alternatives like few-shot learning, which rely on effective pre-training.

Method: The authors annotate a public robotic surgery dataset with skill scores and evaluate various self-supervised pre-training sources in multiple few-shot settings. Domain similarity and procedure-specific data inclusion are analyzed.

Result: Small, domain-relevant datasets demonstrate better performance in few-shot learning scenarios compared to large, less aligned datasets. Incorporating procedure-specific data improves both accuracy (+1.22%) and F1-score (+2.28%).

Conclusion: For surgical skill assessment, smaller datasets closely aligned with the domain outperform larger, unrelated datasets in few-shot learning setups. Procedure-specific pre-training yields notable performance gains.

Abstract: Automated surgical skill assessment (SSA) is a central task in surgical
computer vision. Developing robust SSA models is challenging due to the
scarcity of skill annotations, which are time-consuming to produce and require
expert consensus. Few-shot learning (FSL) offers a scalable alternative
enabling model development with minimal supervision, though its success
critically depends on effective pre-training. While widely studied for several
surgical downstream tasks, pre-training has remained largely unexplored in SSA.
In this work, we formulate SSA as a few-shot task and investigate how
self-supervised pre-training strategies affect downstream few-shot SSA
performance. We annotate a publicly available robotic surgery dataset with
Objective Structured Assessment of Technical Skill (OSATS) scores, and evaluate
various pre-training sources across three few-shot settings. We quantify domain
similarity and analyze how domain gap and the inclusion of procedure-specific
data into pre-training influence transferability. Our results show that small
but domain-relevant datasets can outperform large scale, less aligned ones,
achieving accuracies of 60.16%, 66.03%, and 73.65% in the 1-, 2-, and 5-shot
settings, respectively. Moreover, incorporating procedure-specific data into
pre-training with a domain-relevant external dataset significantly boosts
downstream performance, with an average gain of +1.22% in accuracy and +2.28%
in F1-score; however, applying the same strategy with less similar but
large-scale sources can instead lead to performance degradation. Code and
models are available at https://github.com/anastadimi/ssa-fsl.

</details>


### [117] [Texture-aware Intrinsic Image Decomposition with Model- and Learning-based Priors](https://arxiv.org/abs/2509.09352)
*Xiaodong Wang,Zijun He,Xin Yuan*

Main category: cs.CV

TL;DR: The paper introduces a new method for intrinsic image decomposition that effectively separates reflectance and shading layers, overcoming challenges in complex scenes with varying lighting and rich textures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in intrinsic image decomposition, especially for scenes with severe lighting variations and rich textures, which traditional methods struggle to handle.

Method: The method introduces a texture-guided regularization term within an optimization framework to differentiate material textures and lighting effects while leveraging prior knowledge.

Result: The proposed method produces higher-quality intrinsic images compared to previous approaches, overcoming issues like texture-less or over-smoothed outputs.

Conclusion: Integrating texture-aware priors into intrinsic image decomposition significantly improves the ability to handle real-world complex scenes with better separation of textures and lighting.

Abstract: This paper aims to recover the intrinsic reflectance layer and shading layer
given a single image. Though this intrinsic image decomposition problem has
been studied for decades, it remains a significant challenge in cases of
complex scenes, i.e. spatially-varying lighting effect and rich textures. In
this paper, we propose a novel method for handling severe lighting and rich
textures in intrinsic image decomposition, which enables to produce
high-quality intrinsic images for real-world images. Specifically, we observe
that previous learning-based methods tend to produce texture-less and
over-smoothing intrinsic images, which can be used to infer the lighting and
texture information given a RGB image. In this way, we design a texture-guided
regularization term and formulate the decomposition problem into an
optimization framework, to separate the material textures and lighting effect.
We demonstrate that combining the novel texture-aware prior can produce
superior results to existing approaches.

</details>


### [118] [Plug-and-play Diffusion Models for Image Compressive Sensing with Data Consistency Projection](https://arxiv.org/abs/2509.09365)
*Xiaodong Wang,Ping Wang,Zhangyuan Li,Xin Yuan*

Main category: cs.CV

TL;DR: The paper bridges Plug-and-Play (PnP) approaches and Denoising Diffusion Implicit Models (DDIM), proposing a unified framework for solving inverse problems. Experimental results show improved reconstruction quality in single-pixel imaging.


<details>
  <summary>Details</summary>
Motivation: To integrate learned priors with physical forward models systematically and improve reconstruction quality in ill-posed inverse problems using PnP and DDIM approaches.

Method: The authors decouple the diffusion process into three stages (denoising, data consistency enforcement, sampling) and introduce a hybrid data-consistency module that combines multiple PnP-style fidelity terms to enhance measurement consistency.

Result: Experimental results demonstrate improved reconstruction quality in single-pixel imaging tasks using the proposed unified framework.

Conclusion: The proposed framework effectively combines PnP and DDIM principles, showcasing improved reconstruction quality without disrupting the sampling procedure, especially in single-pixel imaging applications.

Abstract: We explore the connection between Plug-and-Play (PnP) methods and Denoising
Diffusion Implicit Models (DDIM) for solving ill-posed inverse problems, with a
focus on single-pixel imaging. We begin by identifying key distinctions between
PnP and diffusion models-particularly in their denoising mechanisms and
sampling procedures. By decoupling the diffusion process into three
interpretable stages: denoising, data consistency enforcement, and sampling, we
provide a unified framework that integrates learned priors with physical
forward models in a principled manner. Building upon this insight, we propose a
hybrid data-consistency module that linearly combines multiple PnP-style
fidelity terms. This hybrid correction is applied directly to the denoised
estimate, improving measurement consistency without disrupting the diffusion
sampling trajectory. Experimental results on single-pixel imaging tasks
demonstrate that our method achieves better reconstruction quality.

</details>


### [119] [A Fully Automatic Framework for Intracranial Pressure Grading: Integrating Keyframe Identification, ONSD Measurement and Clinical Data](https://arxiv.org/abs/2509.09368)
*Pengxu Wen,Tingting Yu,Ziwei Nie,Cheng Jiang,Zhenyu Yin,Mingyang He,Bo Liao,Xiaoping Yang*

Main category: cs.CV

TL;DR: The paper introduces a fully automatic two-stage framework for grading intracranial pressure (ICP) using optic nerve sheath diameter (ONSD) and clinical data, achieving higher accuracy compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need for a reliable, non-invasive alternative to measure intracranial pressure, as traditional methods like lumbar puncture are invasive and risky.

Method: The researchers developed a two-stage framework: first, it processes fundus ultrasound videos for segmentation, keyframe identification, and ONSD measurement; second, it combines these ONSD metrics with clinical data for ICP grade prediction.

Result: The framework achieved a validation accuracy of 0.845 ± 0.071 and a test accuracy of 0.786, significantly outperforming traditional threshold-based methods.

Conclusion: This framework reduces operator variability and integrates multi-source data, providing a reliable non-invasive solution for ICP evaluation and improving patient care in neurological conditions.

Abstract: Intracranial pressure (ICP) elevation poses severe threats to cerebral
function, thus necessitating monitoring for timely intervention. While lumbar
puncture is the gold standard for ICP measurement, its invasiveness and
associated risks drive the need for non-invasive alternatives. Optic nerve
sheath diameter (ONSD) has emerged as a promising biomarker, as elevated ICP
directly correlates with increased ONSD. However, current clinical practices
for ONSD measurement suffer from inconsistency in manual operation,
subjectivity in optimal view selection, and variability in thresholding,
limiting their reliability. To address these challenges, we introduce a fully
automatic two-stage framework for ICP grading, integrating keyframe
identification, ONSD measurement and clinical data. Specifically, the fundus
ultrasound video processing stage performs frame-level anatomical segmentation,
rule-based keyframe identification guided by an international consensus
statement, and precise ONSD measurement. The intracranial pressure grading
stage then fuses ONSD metrics with clinical features to enable the prediction
of ICP grades, thereby demonstrating an innovative blend of interpretable
ultrasound analysis and multi-source data integration for objective clinical
evaluation. Experimental results demonstrate that our method achieves a
validation accuracy of $0.845 \pm 0.071$ (with standard deviation from
five-fold cross-validation) and an independent test accuracy of 0.786,
significantly outperforming conventional threshold-based method ($0.637 \pm
0.111$ validation accuracy, $0.429$ test accuracy). Through effectively
reducing operator variability and integrating multi-source information, our
framework establishes a reliable non-invasive approach for clinical ICP
evaluation, holding promise for improving patient management in acute
neurological conditions.

</details>


### [120] [FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark](https://arxiv.org/abs/2509.09680)
*Rongyao Fang,Aldrich Yu,Chengqi Duan,Linjiang Huang,Shuai Bai,Yuxuan Cai,Kun Wang,Si Liu,Xihui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: The paper addresses a gap in reasoning-focused T2I models by introducing a large-scale dataset (FLUX-Reason-6M) and evaluation benchmark (PRISM-Bench). These resources aim to boost reasoning capabilities and offer a rigorous standard for model evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the performance gap in open-source T2I models compared to closed-source systems due to lack of reasoning-centric resources.

Method: Authors developed a massive dataset (FLUX-Reason-6M) with bilingual descriptions and six characteristics, alongside the PRISM-Bench evaluation framework with uniquely designed assessment tracks.

Result: Evaluation on 19 models showcased performance gaps and areas needing improvement, validating the utility of these new resources.

Conclusion: Releasing FLUX-Reason-6M and PRISM-Bench aims to catalyze advancements in reasoning-oriented T2I generation and facilitate growth in the open-source community.

Abstract: The advancement of open-source text-to-image (T2I) models has been hindered
by the absence of large-scale, reasoning-focused datasets and comprehensive
evaluation benchmarks, resulting in a performance gap compared to leading
closed-source systems. To address this challenge, We introduce FLUX-Reason-6M
and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).
FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality
FLUX-generated images and 20 million bilingual (English and Chinese)
descriptions specifically designed to teach complex reasoning. The image are
organized according to six key characteristics: Imagination, Entity, Text
rendering, Style, Affection, and Composition, and design explicit Generation
Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation
steps. The whole data curation takes 15,000 A100 GPU days, providing the
community with a resource previously unattainable outside of large industrial
labs. PRISM-Bench offers a novel evaluation standard with seven distinct
tracks, including a formidable Long Text challenge using GCoT. Through
carefully designed prompts, it utilizes advanced vision-language models for
nuanced human-aligned assessment of prompt-image alignment and image
aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench
reveals critical performance gaps and highlights specific areas requiring
improvement. Our dataset, benchmark, and evaluation code are released to
catalyze the next wave of reasoning-oriented T2I generation. Project page:
https://flux-reason-6m.github.io/ .

</details>


### [121] [Unsupervised Integrated-Circuit Defect Segmentation via Image-Intrinsic Normality](https://arxiv.org/abs/2509.09375)
*Botong Zhao,Qijun Shi,Shujing Lyu,Yue Lu*

Main category: cs.CV

TL;DR: The paper proposes an unsupervised IC defect segmentation framework that eliminates reliance on external normal image sets, focusing on inherent patterns in the test image for robust defect identification.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional IC defect segmentation methods that require external normal image sets, which can be brittle due to product variability and difficult alignment.

Method: The approach involves an unsupervised framework with a learnable normal-information extractor, coherence loss for normalization, and reconstruction residuals to segment defects, complemented by pseudo-anomaly augmentation for stable training.

Result: Experiments across three IC process stages demonstrate the approach's consistent performance improvement and robustness to variability in IC products.

Conclusion: The proposed framework effectively segments IC defects using local image information without external references, making it adaptable and robust across diverse manufacturing stages.

Abstract: Modern Integrated-Circuit(IC) manufacturing introduces diverse, fine-grained
defects that depress yield and reliability. Most industrial defect segmentation
compares a test image against an external normal set, a strategy that is
brittle for IC imagery where layouts vary across products and accurate
alignment is difficult. We observe that defects are predominantly local, while
each image still contains rich, repeatable normal patterns. We therefore
propose an unsupervised IC defect segmentation framework that requires no
external normal support. A learnable normal-information extractor aggregates
representative normal features from the test image, and a coherence loss
enforces their association with normal regions. Guided by these features, a
decoder reconstructs only normal content; the reconstruction residual then
segments defects. Pseudo-anomaly augmentation further stabilizes training.
Experiments on datasets from three IC process stages show consistent
improvements over existing approaches and strong robustness to product
variability.

</details>


### [122] [Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot Adaptation under Shift](https://arxiv.org/abs/2509.09397)
*Umaima Rahman,Raza Imam,Mohammad Yaqub,Dwarikanath Mahapatra*

Main category: cs.CV

TL;DR: This paper introduces DRiFt, a structured feature decoupling framework aimed at improving medical vision-language models' reliability and generalizability by separating clinically relevant signals from irrelevant noise.


<details>
  <summary>Details</summary>
Motivation: There is a critical need to enhance the reliability and generalizability of medical VLMs under distribution shifts to ensure their safe deployment in clinical settings.

Method: DRiFt uses parameter-efficient tuning (LoRA) and learnable prompt tokens to decouple clinically relevant signals from task-agnostic noise, and leverages high-quality image-text pairs for better cross-modal alignment.

Result: The proposed approach boosts in-distribution performance (+11.4% Top-1 accuracy and +3.3% Macro-F1) and demonstrates strong robustness across unseen datasets.

Conclusion: Disentangling task-relevant features and achieving careful alignment significantly enhance the generalization and safety of vision-language models, paving the way for more trustworthy clinical applications.

Abstract: Medical vision-language models (VLMs) offer promise for clinical decision
support, yet their reliability under distribution shifts remains a major
concern for safe deployment. These models often learn task-agnostic
correlations due to variability in imaging protocols and free-text reports,
limiting their generalizability and increasing the risk of failure in
real-world settings. We propose DRiFt, a structured feature decoupling
framework that explicitly separates clinically relevant signals from
task-agnostic noise using parameter-efficient tuning (LoRA) and learnable
prompt tokens. To enhance cross-modal alignment and reduce uncertainty, we
curate high-quality, clinically grounded image-text pairs by generating
captions for a diverse medical dataset. Our approach improves in-distribution
performance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based
methods, while maintaining strong robustness across unseen datasets. Ablation
studies reveal that disentangling task-relevant features and careful alignment
significantly enhance model generalization and reduce unpredictable behavior
under domain shift. These insights contribute toward building safer, more
trustworthy VLMs for clinical use. The code is available at
https://github.com/rumaima/DRiFt.

</details>


### [123] [FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution](https://arxiv.org/abs/2509.09427)
*Yuchan Jie,Yushen Xu,Xiaosong Li,Fuqiang Zhou,Jianming Lv,Huafeng Li*

Main category: cs.CV

TL;DR: FS-Diff proposes a novel joint image fusion and super-resolution method for multimodal images, ensuring better semantic detail and resolution recovery.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of corrupted target and background structures in low-resolution and weak semantic multimodal images, which lead to suboptimal fusion results in existing methods.

Method: FS-Diff integrates image fusion and super-resolution as a conditional generation problem, utilizing a clarity-sensing mechanism, a bidirectional feature 'Mamba,' and a modified U-Net for denoising at different noise levels.

Result: Experiments on six published datasets and the newly constructed AVMS dataset demonstrated that FS-Diff surpasses state-of-the-art methods, providing superior details and semantic recovery at different magnifications.

Conclusion: FS-Diff represents a significant advancement in achieving high-quality image fusion and super-resolution, suitable for applications like military reconnaissance and long-distance detection tasks.

Abstract: As an influential information fusion and low-level vision technique, image
fusion integrates complementary information from source images to yield an
informative fused image. A few attempts have been made in recent years to
jointly realize image fusion and super-resolution. However, in real-world
applications such as military reconnaissance and long-range detection missions,
the target and background structures in multimodal images are easily corrupted,
with low resolution and weak semantic information, which leads to suboptimal
results in current fusion techniques. In response, we propose FS-Diff, a
semantic guidance and clarity-aware joint image fusion and super-resolution
method. FS-Diff unifies image fusion and super-resolution as a conditional
generation problem. It leverages semantic guidance from the proposed clarity
sensing mechanism for adaptive low-resolution perception and cross-modal
feature extraction. Specifically, we initialize the desired fused result as
pure Gaussian noise and introduce the bidirectional feature Mamba to extract
the global features of the multimodal images. Moreover, utilizing the source
images and semantics as conditions, we implement a random iterative denoising
process via a modified U-Net network. This network istrained for denoising at
multiple noise levels to produce high-resolution fusion results with
cross-modal features and abundant semantic information. We also construct a
powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images.
Extensive joint image fusion and super-resolution experiments on six public and
our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art
methods at multiple magnifications and can recover richer details and semantics
in the fused images. The code is available at
https://github.com/XylonXu01/FS-Diff.

</details>


### [124] [Semantic Concentration for Self-Supervised Dense Representations Learning](https://arxiv.org/abs/2509.09429)
*Peisong Wen,Qianqian Xu,Siran Dai,Runmin Cong,Qingming Huang*

Main category: cs.CV

TL;DR: This paper addresses the challenges of learning dense representations for patches in self-supervised learning (SSL) by revealing issues of over-dispersion and introducing methods for explicit semantic concentration using a noise-tolerant ranking loss and object-aware filtering.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the over-dispersion phenomenon in dense SSL, where patches from the same instance scatter and harm downstream performance, in contrast to image-level SSL which benefits from implicit semantic concentration.

Method: The paper proposes distilling patch correspondences to break strict spatial alignment and using a noise-tolerant ranking loss extended from Average Precision loss. Additionally, the authors introduce an object-aware filter to map patch representations to object-based spaces via cross-attention.

Result: Empirical studies demonstrate the effectiveness of the proposed methods across various tasks. Code for replication is provided.

Conclusion: Explicit semantic concentration techniques, such as noise-tolerant ranking loss and object-aware filtering, effectively mitigate the challenges in dense SSL, enabling significant improvements in representation learning.

Abstract: Recent advances in image-level self-supervised learning (SSL) have made
significant progress, yet learning dense representations for patches remains
challenging. Mainstream methods encounter an over-dispersion phenomenon that
patches from the same instance/category scatter, harming downstream performance
on dense tasks. This work reveals that image-level SSL avoids over-dispersion
by involving implicit semantic concentration. Specifically, the non-strict
spatial alignment ensures intra-instance consistency, while shared patterns,
i.e., similar parts of within-class instances in the input space, ensure
inter-image consistency. Unfortunately, these approaches are infeasible for
dense SSL due to their spatial sensitivity and complicated scene-centric data.
These observations motivate us to explore explicit semantic concentration for
dense SSL. First, to break the strict spatial alignment, we propose to distill
the patch correspondences. Facing noisy and imbalanced pseudo labels, we
propose a noise-tolerant ranking loss. The core idea is extending the Average
Precision (AP) loss to continuous targets, such that its decision-agnostic and
adaptive focusing properties prevent the student model from being misled.
Second, to discriminate the shared patterns from complicated scenes, we propose
the object-aware filter to map the output space to an object-based space.
Specifically, patches are represented by learnable prototypes of objects via
cross-attention. Last but not least, empirical studies across various tasks
soundly support the effectiveness of our method. Code is available in
https://github.com/KID-7391/CoTAP.

</details>


### [125] [FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion based on diffusion model](https://arxiv.org/abs/2509.09456)
*Yushen Xu,Xiaosong Li,Yuchun Wang,Xiaoqi Cheng,Huafeng Li,Haishu Tan*

Main category: cs.CV

TL;DR: This paper proposes FlexiD-Fuse, a diffusion-based network for flexible and high-quality multi-modal medical image fusion, surpassing existing methods that handle fixed numbers of modalities.


<details>
  <summary>Details</summary>
Motivation: Existing medical image fusion techniques are limited because they only handle a fixed number of input modalities, making them less practical for clinical applications.

Method: The paper introduces FlexiD-Fuse, a diffusion-based network that integrates Expectation-Maximization with diffusion sampling to accommodate flexible numbers of input modalities for medical image fusion.

Result: Experimental results on Harvard datasets and nine metrics show that FlexiD-Fuse outperformed state-of-the-art methods. Additionally, extensive extensions showed it is effective for various other image fusion tasks.

Conclusion: FlexiD-Fuse is a highly effective and versatile solution for multi-modal medical image fusion, addressing the limitations of fixed-modality fusion methods and demonstrating superior performance in diverse applications.

Abstract: Different modalities of medical images provide unique physiological and
anatomical information for diseases. Multi-modal medical image fusion
integrates useful information from different complementary medical images with
different modalities, producing a fused image that comprehensively and
objectively reflects lesion characteristics to assist doctors in clinical
diagnosis. However, existing fusion methods can only handle a fixed number of
modality inputs, such as accepting only two-modal or tri-modal inputs, and
cannot directly process varying input quantities, which hinders their
application in clinical settings. To tackle this issue, we introduce
FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate
flexible quantities of input modalities. It can end-to-end process two-modal
and tri-modal medical image fusion under the same weight. FlexiD-Fuse
transforms the diffusion fusion problem, which supports only fixed-condition
inputs, into a maximum likelihood estimation problem based on the diffusion
process and hierarchical Bayesian modeling. By incorporating the
Expectation-Maximization algorithm into the diffusion sampling iteration
process, FlexiD-Fuse can generate high-quality fused images with cross-modal
information from source images, independently of the number of input images. We
compared the latest two and tri-modal medical image fusion methods, tested them
on Harvard datasets, and evaluated them using nine popular metrics. The
experimental results show that our method achieves the best performance in
medical image fusion with varying inputs. Meanwhile, we conducted extensive
extension experiments on infrared-visible, multi-exposure, and multi-focus
image fusion tasks with arbitrary numbers, and compared them with the
perspective SOTA methods. The results of the extension experiments consistently
demonstrate the effectiveness and superiority of our method.

</details>


### [126] [Resource-Efficient Glioma Segmentation on Sub-Saharan MRI](https://arxiv.org/abs/2509.09469)
*Freedmore Sidume,Oumayma Soula,Joseph Muthui Wacira,YunFei Zhu,Abbas Rabiu Muhammad,Abderrazek Zeraii,Oluwaseun Kalejaye,Hajer Ibrahim,Olfa Gaddour,Brain Halubanza,Dong Zhang,Udunna C Anazodo,Confidence Raymond*

Main category: cs.CV

TL;DR: This paper proposes a deep learning framework using 3D Attention UNet for glioma segmentation in MRI images from data-scarce Sub-Saharan Africa, achieving high accuracy and efficient deployment.


<details>
  <summary>Details</summary>
Motivation: To address the lack of annotated glioma MRI datasets and computational resources in Sub-Saharan Africa, which hinders the use of advanced segmentation models in clinical workflows.

Method: The study used a 3D Attention UNet architecture enhanced with residual blocks and transfer learning from the BraTS 2021 dataset. The model was evaluated on the BraTS-Africa dataset.

Result: The model achieved Dice scores of 0.76 for Enhancing Tumor (ET), 0.80 for Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding Non-Functional Hemisphere (SNFH) despite limited data.

Conclusion: The framework demonstrates high potential for clinical use in resource-constrained settings and promotes AI solutions for equitable global healthcare in underserved regions.

Abstract: Gliomas are the most prevalent type of primary brain tumors, and their
accurate segmentation from MRI is critical for diagnosis, treatment planning,
and longitudinal monitoring. However, the scarcity of high-quality annotated
imaging data in Sub-Saharan Africa (SSA) poses a significant challenge for
deploying advanced segmentation models in clinical workflows. This study
introduces a robust and computationally efficient deep learning framework
tailored for resource-constrained settings. We leveraged a 3D Attention UNet
architecture augmented with residual blocks and enhanced through transfer
learning from pre-trained weights on the BraTS 2021 dataset. Our model was
evaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma
segmentation in SSA MRI data. Despite the limited data quality and quantity,
our approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80
for Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding
Non-Functional Hemisphere (SNFH). These results demonstrate the
generalizability of the proposed model and its potential to support clinical
decision making in low-resource settings. The compact architecture,
approximately 90 MB, and sub-minute per-volume inference time on consumer-grade
hardware further underscore its practicality for deployment in SSA health
systems. This work contributes toward closing the gap in equitable AI for
global health by empowering underserved regions with high-performing and
accessible medical imaging solutions.

</details>


### [127] [OpenFake: An Open Dataset and Platform Toward Large-Scale Deepfake Detection](https://arxiv.org/abs/2509.09495)
*Victor Livernoche,Akshatha Arodi,Andreea Musulan,Zachary Yang,Adam Salvail,Gaétan Marceau Caron,Jean-François Godbout,Reihaneh Rabbany*

Main category: cs.CV

TL;DR: The paper addresses the challenge of detecting deepfakes, especially in politically-sensitive contexts, and introduces a large, modern dataset and crowdsourced platform to enhance detection methods.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the limitations of existing deepfake detection datasets, including outdated generation methods and low-quality images, which hinder the development of effective detection algorithms against sophisticated modern synthetic media.

Method: Researchers conducted a human perception study to identify how realistic synthetic images have become and created a dataset containing three million real images and 963,000 synthetic images. They used both proprietary and open-source models and designed an adversarial crowdsourcing platform for ongoing dataset updates.

Result: The authors provided a benchmark dataset with millions of images while establishing a dynamic crowdsourced approach to stay ahead of constantly evolving generative techniques.

Conclusion: The paper highlights the need for robust deepfake detection and presents tools enabling adaptive advancements, contributing to the mitigation of misinformation risks in public discourse.

Abstract: Deepfakes, synthetic media created using advanced AI techniques, have
intensified the spread of misinformation, particularly in politically sensitive
contexts. Existing deepfake detection datasets are often limited, relying on
outdated generation methods, low realism, or single-face imagery, restricting
the effectiveness for general synthetic image detection. By analyzing social
media posts, we identify multiple modalities through which deepfakes propagate
misinformation. Furthermore, our human perception study demonstrates that
recently developed proprietary models produce synthetic images increasingly
indistinguishable from real ones, complicating accurate identification by the
general public. Consequently, we present a comprehensive, politically-focused
dataset specifically crafted for benchmarking detection against modern
generative models. This dataset contains three million real images paired with
descriptive captions, which are used for generating 963k corresponding
high-quality synthetic images from a mix of proprietary and open-source models.
Recognizing the continual evolution of generative techniques, we introduce an
innovative crowdsourced adversarial platform, where participants are
incentivized to generate and submit challenging synthetic images. This ongoing
community-driven initiative ensures that deepfake detection methods remain
robust and adaptive, proactively safeguarding public discourse from
sophisticated misinformation threats.

</details>


### [128] [Improving Human Motion Plausibility with Body Momentum](https://arxiv.org/abs/2509.09496)
*Ha Linh Nguyen,Tze Ho Elden Tse,Angela Yao*

Main category: cs.CV

TL;DR: The paper addresses human motion modeling challenges by linking local joint behavior to global displacement using whole-body momentum constraints and proposes a momentum-based loss term to improve motion realism.


<details>
  <summary>Details</summary>
Motivation: Existing models fail to adequately capture the physical coupling between local joint dynamics and global motion, while direct derivation through joint torques and forces is computationally complex.

Method: The authors use linear and angular momentum as constraints and introduce a novel loss term enforcing consistency between generated and ground-truth momentum profiles.

Result: The proposed approach reduces foot sliding, jitter, and improves balance while preserving the accuracy of motion recovery.

Conclusion: Employing momentum-based constraints effectively enhances human motion modeling by linking local and global dynamics, offering improvements in motion realism.

Abstract: Many studies decompose human motion into local motion in a frame attached to
the root joint and global motion of the root joint in the world frame, treating
them separately. However, these two components are not independent. Global
movement arises from interactions with the environment, which are, in turn,
driven by changes in the body configuration. Motion models often fail to
precisely capture this physical coupling between local and global dynamics,
while deriving global trajectories from joint torques and external forces is
computationally expensive and complex. To address these challenges, we propose
using whole-body linear and angular momentum as a constraint to link local
motion with global movement. Since momentum reflects the aggregate effect of
joint-level dynamics on the body's movement through space, it provides a
physically grounded way to relate local joint behavior to global displacement.
Building on this insight, we introduce a new loss term that enforces
consistency between the generated momentum profiles and those observed in
ground-truth data. Incorporating our loss reduces foot sliding and jitter,
improves balance, and preserves the accuracy of the recovered motion. Code and
data are available at the project page https://hlinhn.github.io/momentum_bmvc.

</details>


### [129] [Region-Wise Correspondence Prediction between Manga Line Art Images](https://arxiv.org/abs/2509.09501)
*Yingxuan Li,Jiafeng Mao,Qianru Qiu,Yusuke Matsui*

Main category: cs.CV

TL;DR: The paper proposes a method to predict region-wise correspondence in manga line art images without pre-existing annotations, achieving high patch- and region-level accuracy.


<details>
  <summary>Details</summary>
Motivation: Enable applications like automatic colorization and frame generation for manga, overcoming the lack of segmentation and annotations.

Method: A Transformer-based framework to learn patch-level similarities, combined with edge-aware clustering and a region matching algorithm.

Result: Achieved 96.34% accuracy on patch-level predictions and consistent region-level correspondences across datasets.

Conclusion: The method is effective and practical for applications in manga processing, showing strong potential in real-world scenarios.

Abstract: Understanding region-wise correspondence between manga line art images is a
fundamental task in manga processing, enabling downstream applications such as
automatic line art colorization and in-between frame generation. However, this
task remains largely unexplored, especially in realistic scenarios without
pre-existing segmentation or annotations. In this paper, we introduce a novel
and practical task: predicting region-wise correspondence between raw manga
line art images without any pre-existing labels or masks. To tackle this
problem, we divide each line art image into a set of patches and propose a
Transformer-based framework that learns patch-level similarities within and
across images. We then apply edge-aware clustering and a region matching
algorithm to convert patch-level predictions into coherent region-level
correspondences. To support training and evaluation, we develop an automatic
annotation pipeline and manually refine a subset of the data to construct
benchmark datasets. Experiments on multiple datasets demonstrate that our
method achieves high patch-level accuracy (e.g., 96.34%) and generates
consistent region-level correspondences, highlighting its potential for
real-world manga applications.

</details>


### [130] [Generative Diffusion Contrastive Network for Multi-View Clustering](https://arxiv.org/abs/2509.09527)
*Jian Zhu,Xin Zou,Xi Wang,Ning Zhang,Bian Wu,Yao Yang,Ying Zhou,Lingfang Zeng,Chang Tang,Cheng Luo*

Main category: cs.CV

TL;DR: This paper introduces a new method called Stochastic Generative Diffusion Fusion (SGDF) to address low-quality data issues in Multi-View Clustering (MVC), and extends it with the Generative Diffusion Contrastive Network (GDCN) to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To improve clustering performance in Multi-View Clustering (MVC) by addressing challenges related to noisy and missing data during the multi-view fusion process.

Method: The paper proposes the Stochastic Generative Diffusion Fusion (SGDF) method, which uses a multi-generative mechanism to handle low-quality data in multi-view samples. It also develops the Generative Diffusion Contrastive Network (GDCN) built on SGDF.

Result: The proposed GDCN achieves state-of-the-art performance in deep Multi-View Clustering tasks based on extensive experiments.

Conclusion: The study successfully tackles the multi-view data quality problem using SGDF and GDCN, significantly enhancing clustering analysis capabilities. The source code is openly available for public use.

Abstract: In recent years, Multi-View Clustering (MVC) has been significantly advanced
under the influence of deep learning. By integrating heterogeneous data from
multiple views, MVC enhances clustering analysis, making multi-view fusion
critical to clustering performance. However, there is a problem of low-quality
data in multi-view fusion. This problem primarily arises from two reasons: 1)
Certain views are contaminated by noisy data. 2) Some views suffer from missing
data. This paper proposes a novel Stochastic Generative Diffusion Fusion (SGDF)
method to address this problem. SGDF leverages a multiple generative mechanism
for the multi-view feature of each sample. It is robust to low-quality data.
Building on SGDF, we further present the Generative Diffusion Contrastive
Network (GDCN). Extensive experiments show that GDCN achieves the
state-of-the-art results in deep MVC tasks. The source code is publicly
available at https://github.com/HackerHyper/GDCN.

</details>


### [131] [DualTrack: Sensorless 3D Ultrasound needs Local and Global Context](https://arxiv.org/abs/2509.09530)
*Paul F. R. Wilson,Matteo Ronchetti,Rüdiger Göbl,Viktoria Markova,Sebastian Rosenzweig,Raphael Prevost,Parvin Mousavi,Oliver Zettinig*

Main category: cs.CV

TL;DR: The paper introduces DualTrack, a novel dual-encoder deep learning system for estimating 3D ultrasound probe trajectories using decoupled local and global feature extraction, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations in adopting 3D ultrasound systems due to their cost and complexity by proposing a deep learning-based alternative that separately models local and global features for better performance.

Method: The proposed method, DualTrack, uses a dual-encoder architecture with a local encoder capturing fine-grained features via dense spatiotemporal convolutions and a global encoder for high-level anatomical features using a CNN backbone and temporal attention. The features are fused for trajectory estimation.

Result: Experimental validation on a large benchmark dataset demonstrates superior accuracy and globally consistent 3D ultrasound reconstructions, with reconstruction errors under 5 mm, outperforming prior methods.

Conclusion: DualTrack effectively models complementary local and global features, facilitating accurate and cost-effective 3D ultrasound imaging, paving the way for broader adoption in clinical applications.

Abstract: Three-dimensional ultrasound (US) offers many clinical advantages over
conventional 2D imaging, yet its widespread adoption is limited by the cost and
complexity of traditional 3D systems. Sensorless 3D US, which uses deep
learning to estimate a 3D probe trajectory from a sequence of 2D US images, is
a promising alternative. Local features, such as speckle patterns, can help
predict frame-to-frame motion, while global features, such as coarse shapes and
anatomical structures, can situate the scan relative to anatomy and help
predict its general shape. In prior approaches, global features are either
ignored or tightly coupled with local feature extraction, restricting the
ability to robustly model these two complementary aspects. We propose
DualTrack, a novel dual-encoder architecture that leverages decoupled local and
global encoders specialized for their respective scales of feature extraction.
The local encoder uses dense spatiotemporal convolutions to capture
fine-grained features, while the global encoder utilizes an image backbone
(e.g., a 2D CNN or foundation model) and temporal attention layers to embed
high-level anatomical features and long-range dependencies. A lightweight
fusion module then combines these features to estimate the trajectory.
Experimental results on a large public benchmark show that DualTrack achieves
state-of-the-art accuracy and globally consistent 3D reconstructions,
outperforming previous methods and yielding an average reconstruction error
below 5 mm.

</details>


### [132] [Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders](https://arxiv.org/abs/2509.09547)
*Dohun Lee,Hyeonho Jeong,Jiwook Kim,Duygu Ceylan,Jong Chul Ye*

Main category: cs.CV

TL;DR: This paper introduces Align4Gen, a method to improve video diffusion models by aligning their intermediate features with pre-trained vision encoders, leading to better video generation.


<details>
  <summary>Details</summary>
Motivation: To enhance the feature representation power of video diffusion models, which has received less attention compared to architectural innovations and novel training objectives.

Method: Proposed Align4Gen, a multi-feature fusion and alignment method, integrated with video diffusion model training. The paper includes a new metric and an analysis of vision encoders for discriminability and temporal consistency.

Result: Align4Gen improves both unconditional and class-conditional video generation tasks with better results quantified by various metrics.

Conclusion: Align4Gen enhances video diffusion model training via feature alignment, resulting in higher-quality video generation. Full video results are shared on their project page.

Abstract: Video diffusion models have advanced rapidly in the recent years as a result
of series of architectural innovations (e.g., diffusion transformers) and use
of novel training objectives (e.g., flow matching). In contrast, less attention
has been paid to improving the feature representation power of such models. In
this work, we show that training video diffusion models can benefit from
aligning the intermediate features of the video generator with feature
representations of pre-trained vision encoders. We propose a new metric and
conduct an in-depth analysis of various vision encoders to evaluate their
discriminability and temporal consistency, thereby assessing their suitability
for video feature alignment. Based on the analysis, we present Align4Gen which
provides a novel multi-feature fusion and alignment method integrated into
video diffusion model training. We evaluate Align4Gen both for unconditional
and class-conditional video generation tasks and show that it results in
improved video generation as quantified by various metrics. Full video results
are available on our project page: https://align4gen.github.io/align4gen/

</details>


### [133] [InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation](https://arxiv.org/abs/2509.09555)
*Sirui Xu,Dongting Li,Yucheng Zhang,Xiyan Xu,Qi Long,Ziyin Wang,Yunzhi Lu,Shuchang Dong,Hezi Jiang,Akshat Gupta,Yu-Xiong Wang,Liang-Yan Gui*

Main category: cs.CV

TL;DR: The paper introduces InterAct, a benchmark for 3D human-object interaction (HOI), addressing limitations in existing datasets through data enrichment and quality enhancement methods.


<details>
  <summary>Details</summary>
Motivation: Current challenges in modeling dynamic 3D human-object interactions arise due to limitations in existing datasets, such as lack of high-quality motion, incomplete annotations, and artifacts.

Method: The authors consolidate and standardize 21.81 hours of diverse HOI data, improve quality using a unified optimization framework, and expand the dataset to 30.70 hours by introducing motion variations while preserving human-object contact relationships.

Result: InterAct achieves state-of-the-art performance on six benchmarking tasks and demonstrates the utility of its enriched dataset through extensive experiments.

Conclusion: InterAct serves as a foundational resource for advancing 3D HOI generation and is made publicly available for continued research, with active maintenance provided.

Abstract: While large-scale human motion capture datasets have advanced human motion
generation, modeling and generating dynamic 3D human-object interactions (HOIs)
remain challenging due to dataset limitations. Existing datasets often lack
extensive, high-quality motion and annotation and exhibit artifacts such as
contact penetration, floating, and incorrect hand motions. To address these
issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset
and methodological advancements. First, we consolidate and standardize 21.81
hours of HOI data from diverse sources, enriching it with detailed textual
annotations. Second, we propose a unified optimization framework to enhance
data quality by reducing artifacts and correcting hand motions. Leveraging the
principle of contact invariance, we maintain human-object relationships while
introducing motion variations, expanding the dataset to 30.70 hours. Third, we
define six benchmarking tasks and develop a unified HOI generative modeling
perspective, achieving state-of-the-art performance. Extensive experiments
validate the utility of our dataset as a foundational resource for advancing 3D
human-object interaction generation. To support continued research in this
area, the dataset is publicly available at
https://github.com/wzyabcas/InterAct, and will be actively maintained.

</details>


### [134] [Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in MRI-based Alzheimer's Disease Classification](https://arxiv.org/abs/2509.09558)
*Akshit Achara,Esther Puyol Anton,Alexander Hammers,Andrew P. King*

Main category: cs.CV

TL;DR: The paper investigates shortcut learning and demographic bias in deep learning-based Alzheimer’s disease diagnostic models using brain MRI scans, revealing race and sex biases.


<details>
  <summary>Details</summary>
Motivation: Understanding and mitigating demographic biases in deep learning models for brain MRI analysis is crucial for developing equitable diagnostic tools for diseases like Alzheimer’s.

Method: Researchers tested whether deep learning models can identify race and sex from MRI scans and assessed the impact of training set imbalances. Additionally, they analyzed feature attribution for protected attributes and disease classification using models like ResNet and SwinTransformer across multiple datasets.

Result: Both race and sex-based biases were identified in the diagnostic models, influenced by shortcut learning, as demonstrated through experiments across datasets and model types.

Conclusion: The study highlights the need for addressing bias and shortcut learning in DL-based tools for fairer and more accurate Alzheimer’s disease MRI diagnostics.

Abstract: Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep
learning (DL) algorithms have been proposed to aid in the diagnosis of diseases
such as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can
suffer from shortcut learning, in which spurious features, not directly related
to the output label, are used for prediction. When these features are related
to protected attributes, they can lead to performance bias against
underrepresented protected groups, such as those defined by race and sex. In
this work, we explore the potential for shortcut learning and demographic bias
in DL based AD diagnosis from MRI. We first investigate if DL algorithms can
identify race or sex from 3D brain MRI scans to establish the presence or
otherwise of race and sex based distributional shifts. Next, we investigate
whether training set imbalance by race or sex can cause a drop in model
performance, indicating shortcut learning and bias. Finally, we conduct a
quantitative and qualitative analysis of feature attributions in different
brain regions for both the protected attribute and AD classification tasks.
Through these experiments, and using multiple datasets and DL models (ResNet
and SwinTransformer), we demonstrate the existence of both race and sex based
shortcut learning and bias in DL based AD classification. Our work lays the
foundation for fairer DL diagnostic tools in brain MRI. The code is provided at
https://github.com/acharaakshit/ShortMR

</details>


### [135] [PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection](https://arxiv.org/abs/2509.09572)
*Sijun Dong,Yuxuan Hu,LiBo Wang,Geng Chen,Xiaoliang Meng*

Main category: cs.CV

TL;DR: PeftCD is a change detection framework utilizing Vision Foundation Models with efficient fine-tuning to achieve state-of-the-art performance on remote sensing tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in remote sensing change detection, such as pseudo changes, lack of labeled samples, and cross-domain generalization.

Method: PeftCD employs a Siamese encoder from VFMs, integrating LoRA and Adapter modules, alongside leading backbones like SAM2 and DINOv3, and a lightweight decoder.

Result: PeftCD achieves high accuracy on public datasets, with impressive IoU scores across systems like SYSU-CD, WHUCD, and others, reducing pseudo changes and improving boundary precision.

Conclusion: The proposed framework is efficient, accurate, scalable, and adapts large VFMs effectively for real-world remote sensing applications.

Abstract: To tackle the prevalence of pseudo changes, the scarcity of labeled samples,
and the difficulty of cross-domain generalization in multi-temporal and
multi-source remote sensing imagery, we propose PeftCD, a change detection
framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient
Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese
encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly
integrated. This design enables highly efficient task adaptation by training
only a minimal set of additional parameters. To fully unlock the potential of
VFMs, we investigate two leading backbones: the Segment Anything Model v2
(SAM2), renowned for its strong segmentation priors, and DINOv3, a
state-of-the-art self-supervised representation learner. The framework is
complemented by a deliberately lightweight decoder, ensuring the focus remains
on the powerful feature representations from the backbones. Extensive
experiments demonstrate that PeftCD achieves state-of-the-art performance
across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD
(92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and
LEVIR-CD (85.62%), with notably precise boundary delineation and strong
suppression of pseudo-changes. In summary, PeftCD presents an optimal balance
of accuracy, efficiency, and generalization. It offers a powerful and scalable
paradigm for adapting large-scale VFMs to real-world remote sensing change
detection applications. The code and pretrained models will be released at
https://github.com/dyzy41/PeftCD.

</details>


### [136] [Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis](https://arxiv.org/abs/2509.09595)
*Yikang Ding,Jiwen Liu,Wenyuan Zhang,Zekun Wang,Wentao Hu,Liyuan Cui,Mingming Lao,Yingchao Shao,Hui Liu,Xiaohan Li,Ming Chen,Xiaoqiang Liu,Yu-Shen Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: Kling-Avatar introduces a two-stage framework for creating semantically-rich, photorealistic avatar videos driven by multimodal instructions.


<details>
  <summary>Details</summary>
Motivation: Improve narrative coherence and character expressiveness in audio-driven avatar video generation by integrating instruction understanding.

Method: A cascaded two-stage framework: 1) multimodal large language model (MLLM) director generates blueprint videos, 2) first-last frame strategy synthesizes detailed sub-clips guided by blueprints.

Result: High-quality avatar videos (up to 1080p, 48 fps) with better lip synchronization, emotion expression, instruction controllability, identity fidelity, and cross-domain generalization.

Conclusion: Kling-Avatar sets a new standard for audio-driven avatar synthesis in terms of fidelity, semantics, and scalability, suitable for practical applications like livestreaming and vlogging.

Abstract: Recent advances in audio-driven avatar video generation have significantly
enhanced audio-visual realism. However, existing methods treat instruction
conditioning merely as low-level tracking driven by acoustic or visual cues,
without modeling the communicative purpose conveyed by the instructions. This
limitation compromises their narrative coherence and character expressiveness.
To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that
unifies multimodal instruction understanding with photorealistic portrait
generation. Our approach adopts a two-stage pipeline. In the first stage, we
design a multimodal large language model (MLLM) director that produces a
blueprint video conditioned on diverse instruction signals, thereby governing
high-level semantics such as character motion and emotions. In the second
stage, guided by blueprint keyframes, we generate multiple sub-clips in
parallel using a first-last frame strategy. This global-to-local framework
preserves fine-grained details while faithfully encoding the high-level intent
behind multimodal instructions. Our parallel architecture also enables fast and
stable generation of long-duration videos, making it suitable for real-world
applications such as digital human livestreaming and vlogging. To
comprehensively evaluate our method, we construct a benchmark of 375 curated
samples covering diverse instructions and challenging scenarios. Extensive
experiments demonstrate that Kling-Avatar is capable of generating vivid,
fluent, long-duration videos at up to 1080p and 48 fps, achieving superior
performance in lip synchronization accuracy, emotion and dynamic
expressiveness, instruction controllability, identity preservation, and
cross-domain generalization. These results establish Kling-Avatar as a new
benchmark for semantically grounded, high-fidelity audio-driven avatar
synthesis.

</details>


### [137] [Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth](https://arxiv.org/abs/2509.09610)
*Daria Laslo,Efthymios Georgiou,Marius George Linguraru,Andreas Rauschecker,Sabine Muller,Catherine R. Jutzeler,Sarah Bruningk*

Main category: cs.CV

TL;DR: The paper proposes a hybrid framework for predicting brain tumor progression by combining a mathematical tumor growth model with image synthesis using a guided DDIM model.


<details>
  <summary>Details</summary>
Motivation: Accurate spatio-temporal prediction of brain tumor progression is crucial for improving clinical decision-making in neuro-oncology.

Method: The framework integrates a mathematical tumor growth model (using ordinary differential equations to capture tumor dynamics and radiotherapy effects) with a guided denoising diffusion implicit model to synthesize future MRIs conditioned on predicted tumor growth.

Result: The model, trained on BraTS glioma datasets and tested on pediatric diffuse midline glioma cases, generates anatomically realistic follow-up scans and provides tumor growth probability maps with high spatial resemblance to ground truth.

Conclusion: The method offers a novel, data-efficient approach for generating future tumor progression scenarios, enhancing clinical insights into tumor growth directionality and extent.

Abstract: Predicting the spatio-temporal progression of brain tumors is essential for
guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic
learning framework that combines a mathematical tumor growth model with a
guided denoising diffusion implicit model (DDIM) to synthesize anatomically
feasible future MRIs from preceding scans. The mechanistic model, formulated as
a system of ordinary differential equations, captures temporal tumor dynamics
including radiotherapy effects and estimates future tumor burden. These
estimates condition a gradient-guided DDIM, enabling image synthesis that
aligns with both predicted growth and patient anatomy. We train our model on
the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices
of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our
framework generates realistic follow-up scans based on spatial similarity
metrics. It also introduces tumor growth probability maps, which capture both
clinically relevant extent and directionality of tumor growth as shown by 95th
percentile Hausdorff Distance. The method enables biologically informed image
generation in data-limited scenarios, offering generative-space-time
predictions that account for mechanistic priors.

</details>


### [138] [Measuring Epistemic Humility in Multimodal Large Language Models](https://arxiv.org/abs/2509.09658)
*Bingkui Tong,Jiaer Xia,Sifeng Shang,Kaiyang Zhou*

Main category: cs.CV

TL;DR: HumbleBench is a benchmark for assessing multimodal large language models (MLLMs) on their ability to reject incorrect visual content, enhancing safety-critical evaluations.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in MLLMs can lead to misinformation and unsafe decision-making, emphasizing the need for benchmarks that assess not only recognition accuracy but also epistemic humility.

Method: HumbleBench utilizes a panoptic scene graph dataset, GPT-4-Turbo generated questions, and manual filtering to create a benchmark measuring the ability of MLLMs to identify incorrect answers across object, relation, and attribute hallucination types.

Result: State-of-the-art MLLMs were evaluated, highlighting strengths and weaknesses in their performance on rejecting incorrect options and validating the benchmark's utility.

Conclusion: HumbleBench provides a realistic and critical measure of MLLM reliability, addressing gaps in current evaluation methods and supporting safer applications of these models.

Abstract: Hallucinations in multimodal large language models (MLLMs) -- where the model
generates content inconsistent with the input image -- pose significant risks
in real-world applications, from misinformation in visual question answering to
unsafe errors in decision-making. Existing benchmarks primarily test
recognition accuracy, i.e., evaluating whether models can select the correct
answer among distractors. This overlooks an equally critical capability for
trustworthy AI: recognizing when none of the provided options are correct, a
behavior reflecting epistemic humility. We present HumbleBench, a new
hallucination benchmark designed to evaluate MLLMs' ability to reject plausible
but incorrect answers across three hallucination types: object, relation, and
attribute. Built from a panoptic scene graph dataset, we leverage fine-grained
scene graph annotations to extract ground-truth entities and relations, and
prompt GPT-4-Turbo to generate multiple-choice questions, followed by a
rigorous manual filtering process. Each question includes a "None of the above"
option, requiring models not only to recognize correct visual information but
also to identify when no provided answer is valid. We evaluate a variety of
state-of-the-art MLLMs -- including both general-purpose and specialized
reasoning models -- on HumbleBench and share valuable findings and insights
with the community. By incorporating explicit false-option rejection,
HumbleBench fills a key gap in current evaluation suites, providing a more
realistic measure of MLLM reliability in safety-critical settings. Our code and
dataset are released publicly and can be accessed at
https://github.com/maifoundations/HumbleBench.

</details>


### [139] [Can Understanding and Generation Truly Benefit Together -- or Just Coexist?](https://arxiv.org/abs/2509.09666)
*Zhiyuan Yan,Kaiqing Lin,Zongjian Li,Junyan Ye,Hui Han,Zhendong Wang,Hao Liu,Bin Lin,Hao Li,Xue Xu,Xinyan Xiao,Jingdong Wang,Haifeng Wang,Li Yuan*

Main category: cs.CV

TL;DR: The paper presents a new framework, UAE, to unify multimodal learning by using bidirectional processes of image-to-text (I2T) and text-to-image (T2I) with a focus on reconstruction fidelity.


<details>
  <summary>Details</summary>
Motivation: The authors aim to bridge understanding (image-to-text) and generation (text-to-image) in multimodal learning by creating a unified system that enhances both processes through mutual gains.

Method: They propose the UAE framework, which pre-trains a decoder using large-scale data and employs a three-stage Unified-GRPO reinforcement learning process to fine-tune the encoder and decoder iteratively.

Result: Surprisingly, the encoder autonomously generates more descriptive captions while the decoder improves its ability to understand these descriptions, resulting in high-quality image reconstructions.

Conclusion: The UAE framework demonstrates significant mutual enhancement of image understanding and generation, setting a new benchmark (Unified-Bench) for evaluating unified multimodal models.

Abstract: In this paper, we introduce an insightful paradigm through the Auto-Encoder
lens-understanding as the encoder (I2T) that compresses images into text, and
generation as the decoder (T2I) that reconstructs images from that text. Using
reconstruction fidelity as the unified training objective, we enforce the
coherent bidirectional information flow between the understanding and
generation processes, bringing mutual gains. To implement this, we propose UAE,
a novel framework for unified multimodal learning. We begin by pre-training the
decoder with large-scale long-context image captions to capture fine-grained
semantic and complex spatial relationships. We then propose Unified-GRPO via
reinforcement learning (RL), which covers three stages: (1) A cold-start phase
to gently initialize both encoder and decoder with a semantic reconstruction
loss; (2) Generation for Understanding, where the encoder is trained to
generate informative captions that maximize the decoder's reconstruction
quality, enhancing its visual understanding; (3) Understanding for Generation,
where the decoder is refined to reconstruct from these captions, forcing it to
leverage every detail and improving its long-context instruction following and
generation fidelity. For evaluation, we introduce Unified-Bench, the first
benchmark tailored to assess the degree of unification of the UMMs. A
surprising "aha moment" arises within the multimodal learning domain: as RL
progresses, the encoder autonomously produces more descriptive captions, while
the decoder simultaneously demonstrates a profound ability to understand these
intricate descriptions, resulting in reconstructions of striking fidelity.

</details>


### [140] [Geometric Neural Distance Fields for Learning Human Motion Priors](https://arxiv.org/abs/2509.09667)
*Zhengdi Yu,Simone Foti,Linguang Zhang,Amy Zhao,Cem Keskin,Stefanos Zafeiriou,Tolga Birdal*

Main category: cs.CV

TL;DR: This paper introduces Neural Riemannian Motion Fields (NRMF), a 3D generative human motion model offering robust, consistent, and realistic motion recovery.


<details>
  <summary>Details</summary>
Motivation: Current motion priors lack robustness and fail to ensure temporally consistent, physically plausible motion recovery. A more geometrically rigorous approach is needed to model human motion.

Method: NRMF models human motion as neural distance fields (NDFs) on a product space of joint rotations, velocities, and accelerations. It includes an adaptive-step projection algorithm and a geometric integrator for realistic motion generation.

Result: NRMF achieves significant performance improvements across tasks like denoising, motion in-betweening, and fitting sparse 2D/3D observations, outperforming existing methods.

Conclusion: NRMF provides a physically and temporally consistent framework for 3D motion generation that generalizes effectively across tasks and input modes.

Abstract: We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative
human motion prior that enables robust, temporally consistent, and physically
plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods,
our higher-order motion prior explicitly models the human motion in the zero
level set of a collection of neural distance fields (NDFs) corresponding to
pose, transition (velocity), and acceleration dynamics. Our framework is
rigorous in the sense that our NDFs are constructed on the product space of
joint rotations, their angular velocities, and angular accelerations,
respecting the geometry of the underlying articulations. We further introduce:
(i) a novel adaptive-step hybrid algorithm for projecting onto the set of
plausible motions, and (ii) a novel geometric integrator to "roll out"
realistic motion trajectories during test-time-optimization and generation. Our
experiments show significant and consistent gains: trained on the AMASS
dataset, NRMF remarkably generalizes across multiple input modalities and to
diverse tasks ranging from denoising to motion in-betweening and fitting to
partial 2D / 3D observations.

</details>


### [141] [Locality in Image Diffusion Models Emerges from Data Statistics](https://arxiv.org/abs/2509.09672)
*Artem Lukoianov,Chenyang Yuan,Justin Solomon,Vincent Sitzmann*

Main category: cs.CV

TL;DR: The paper investigates the gap between the optimal diffusion model denoiser and deep diffusion models, showing that the emergent locality in deep diffusion is driven by dataset properties rather than CNN biases, and proposes an improved analytical denoiser.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are intriguing because they have a closed-form optimal denoiser, but this optimality doesn't replicate deep diffusion model behaviors. This paper explores the reasons behind this discrepancy and aims to better understand the performance gaps.

Method: The study examines how locality arises in diffusion models, demonstrating that it stems from datasets' statistical image properties rather than CNN inductive biases. An analytical parametric linear denoiser is proposed alongside theoretical and experimental analysis.

Result: The optimal parametric linear denoiser exhibits locality properties similar to deep neural denoisers, which the study attributes to dataset-driven pixel correlations. The study constructs an improved analytical denoiser aligning more closely with deep diffusion models.

Conclusion: Locality in diffusion models is statistically emergent from image datasets, challenging previous assumptions of it being tied to CNNs' inductive biases. This insight enables the design of a more accurate analytical denoiser for diffusion models.

Abstract: Among generative models, diffusion models are uniquely intriguing due to the
existence of a closed-form optimal minimizer of their training objective, often
referred to as the optimal denoiser. However, diffusion using this optimal
denoiser merely reproduces images in the training set and hence fails to
capture the behavior of deep diffusion models. Recent work has attempted to
characterize this gap between the optimal denoiser and deep diffusion models,
proposing analytical, training-free models that can generate images that
resemble those generated by a trained UNet. The best-performing method
hypothesizes that shift equivariance and locality inductive biases of
convolutional neural networks are the cause of the performance gap, hence
incorporating these assumptions into its analytical model. In this work, we
present evidence that the locality in deep diffusion models emerges as a
statistical property of the image dataset, not due to the inductive bias of
convolutional neural networks. Specifically, we demonstrate that an optimal
parametric linear denoiser exhibits similar locality properties to the deep
neural denoisers. We further show, both theoretically and experimentally, that
this locality arises directly from the pixel correlations present in natural
image datasets. Finally, we use these insights to craft an analytical denoiser
that better matches scores predicted by a deep diffusion model than the prior
expert-crafted alternative.

</details>


### [142] [SpatialVID: A Large-Scale Video Dataset with Spatial Annotations](https://arxiv.org/abs/2509.09676)
*Jiahao Wang,Yufeng Yuan,Rujie Zheng,Youtian Lin,Jian Gao,Lin-Zhuo Chen,Yajie Bao,Yi Zhang,Chang Zeng,Yanxi Zhou,Xiaoxiao Long,Hao Zhu,Zhaoxiang Zhang,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: The paper introduces SpatialVID, a large-scale dataset of diverse real-world videos with dense 3D annotations aimed at improving model scalability and performance in spatial intelligence tasks.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for spatial reconstruction and world exploration are limited in scale, diversity, and annotation detail, particularly for real-world dynamic scenes with ground-truth camera motion, impeding model scalability and real-world application.

Method: The authors collect over 21,000 hours of video, filter it into 2.7 million clips, and annotate these with dense spatial and semantic details such as camera poses, depth maps, dynamic masks, and motion instructions, ensuring rich diversity and fidelity.

Result: SpatialVID consists of 7,089 hours of dynamic content enriched with detailed 3D and semantic annotations. Its diversity and richness enhance model generalization and usability in video and 3D vision tasks.

Conclusion: SpatialVID addresses the limitations of existing datasets by offering a high-quality, large-scale resource that significantly enhances the potential for advancements in spatial intelligence and 3D vision research.

Abstract: Significant progress has been made in spatial intelligence, spanning both
spatial reconstruction and world exploration. However, the scalability and
real-world fidelity of current models remain severely constrained by the
scarcity of large-scale, high-quality training data. While several datasets
provide camera pose information, they are typically limited in scale,
diversity, and annotation richness, particularly for real-world dynamic scenes
with ground-truth camera motion. To this end, we collect \textbf{SpatialVID}, a
dataset consists of a large corpus of in-the-wild videos with diverse scenes,
camera movements and dense 3D annotations such as per-frame camera poses,
depth, and motion instructions. Specifically, we collect more than 21,000 hours
of raw video, and process them into 2.7 million clips through a hierarchical
filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent
annotation pipeline enriches these clips with detailed spatial and semantic
information, including camera poses, depth maps, dynamic masks, structured
captions, and serialized motion instructions. Analysis of SpatialVID's data
statistics reveals a richness and diversity that directly foster improved model
generalization and performance, establishing it as a key asset for the video
and 3D vision research community.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [143] [A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems](https://arxiv.org/abs/2509.08969)
*Nima Karimian Kakolaki*

Main category: cs.DC

TL;DR: The paper evaluates distributed identifier schemes, finding ULIDs excel in performance, collision risks, and efficiency. Source code is provided for reproducibility.


<details>
  <summary>Details</summary>
Motivation: To analyze and identify the most efficient, scalable, and low-collision distributed identifier scheme for distributed systems.

Method: The study uses both mathematical collision probability calculations and empirical experiments in a simulated distributed environment to compare identifier schemes.

Result: ULIDs reduced network overhead by 83.7%, increased generation speed by 97.32%, and exhibited 98.42% lower collision risk compared to UUIDv7, maintaining negligible collision risks even in high-generation scenarios.

Conclusion: The paper concludes that ULIDs are optimal for distributed systems requiring high performance, time-ordering, and scalability, backed by reproducible experiments and public code resources.

Abstract: Distributed systems require robust, scalable identifier schemes to ensure
data uniqueness and efficient indexing across multiple nodes. This paper
presents a comprehensive analysis of the evolution of distributed identifiers,
comparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We
combine mathematical calculation of collision probabilities with empirical
experiments measuring generation speed and network transmission overhead in a
simulated distributed environment. Results demonstrate that ULIDs significantly
outperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing
generation speed by 97.32%. statistical analysis further shows ULIDs offer a
98.42% lower collision risk compared to UUIDv7, while maintaining negligible
collision probabilities even at high generation rates. These findings highlight
ULIDs as an optimal choice for high-performance distributed systems, providing
efficient, time-ordered, and lexicographically sortable identifiers suitable
for scalable applications. All source code, datasets, and analysis scripts
utilized in this research are publicly available in our dedicated repository at
https://github.com/nimakarimiank/uids-comparison. This repository contains
comprehensive documentation of the experimental setup, including configuration
files for the distributed environment, producer and consumer implementations,
and message broker integration. Additionally, it provides the data scripts and
datasets. Researchers and practitioners are encouraged to explore the
repository for full reproducibility of the experiments and to facilitate
further investigation or extension of the presented work.

</details>


### [144] [Optimizing the Variant Calling Pipeline Execution on Human Genomes Using GPU-Enabled Machines](https://arxiv.org/abs/2509.09058)
*Ajay Kumar,Praveen Rao,Peter Sanders*

Main category: cs.DC

TL;DR: The paper introduces an ML-based approach to improve the efficiency of variant calling pipelines on GPU-enabled machines. The method effectively reduces the execution time compared to other approaches.


<details>
  <summary>Details</summary>
Motivation: Human genome variant calling is computationally intensive, and optimizing its execution in cloud environments can significantly reduce cost and resource usage.

Method: The authors use machine learning to predict the execution times of variant calling pipeline stages and optimize workload scheduling using concepts from the job shop scheduling problem.

Result: The approach achieved a 2X speedup over a greedy ML-based approach and a 1.6X speedup over a dynamic resource-based approach on tested genome sequences.

Conclusion: The proposed ML-based approach efficiently schedules workloads, reducing execution time significantly, and demonstrates the potential for broader applications in genomic data analysis.

Abstract: Variant calling is the first step in analyzing a human genome and aims to
detect variants in an individual's genome compared to a reference genome. Due
to the computationally-intensive nature of variant calling, genomic data are
increasingly processed in cloud environments as large amounts of compute and
storage resources can be acquired with the pay-as-you-go pricing model. In this
paper, we address the problem of efficiently executing a variant calling
pipeline for a workload of human genomes on graphics processing unit
(GPU)-enabled machines. We propose a novel machine learning (ML)-based approach
for optimizing the workload execution to minimize the total execution time. Our
approach encompasses two key techniques: The first technique employs ML to
predict the execution times of different stages in a variant calling pipeline
based on the characteristics of a genome sequence. Using the predicted times,
the second technique generates optimal execution plans for the machines by
drawing inspiration from the flexible job shop scheduling problem. The plans
are executed via careful synchronization across different machines. We
evaluated our approach on a workload of publicly available genome sequences
using a testbed with different types of GPU hardware. We observed that our
approach was effective in predicting the execution times of variant calling
pipeline stages using ML on features such as sequence size, read quality,
percentage of duplicate reads, and average read length. In addition, our
approach achieved 2X speedup (on an average) over a greedy approach that also
used ML for predicting the execution times on the tested workload of sequences.
Finally, our approach achieved 1.6X speedup (on an average) over a dynamic
approach that executed the workload based on availability of resources without
using any ML-based time predictions.

</details>


### [145] [Coherence-Aware Task Graph Modeling for Realistic Application](https://arxiv.org/abs/2509.09094)
*Guochu Xiong,Xiangzhong Luo,Weichen Liu*

Main category: cs.DC

TL;DR: CoTAM is a novel framework to model task graphs for multicore systems, explicitly addressing cache coherence, a factor often overlooked in existing methods.


<details>
  <summary>Details</summary>
Motivation: Static task graph approaches fail for dynamic, data-dependent behaviors in realistic workloads, and current methods inadequately address cache coherence.

Method: CoTAM decouples coherence effects from execution, uses a learned weighting scheme to quantify its impact, and generates coherence-aware task graphs.

Result: Extensive experiments prove CoTAM's efficiency, validating its ability to bridge the gap between workload behavior and system designs while modeling cache coherence interactions.

Conclusion: CoTAM enhances accuracy and generality in system-level analysis by explicitly incorporating cache coherence into task graph modeling.

Abstract: As multicore systems continue to scale, cache coherence has emerged as a
critical determinant of system performance, with coherence behavior and task
execution closely intertwined, reshaping inter-task dependencies. Task graph
modeling provides a structured way to capture such dependencies and serves as
the foundation for many system-level design strategies. However, these
strategies typically rely on predefined task graphs, while many real-world
applications lack explicit graphs and exhibit dynamic, data-dependent behavior,
limiting the effectiveness of static approaches. To address this, several task
graph modeling methods for realistic workloads have been developed. Yet, they
either rely on implicit techniques that use application-specific features
without producing explicit graphs, or they generate graphs tailored to fixed
scheduling models, which limits generality. More importantly, they often
overlook coherence interactions, creating a gap between design assumptions and
actual runtime behavior. To overcome these limitations, we propose CoTAM, a
Coherence-Aware Task Graph Modeling framework for realistic workloads that
constructs a unified task graph reflecting runtime behavior. CoTAM analyzes the
impact of coherence by decoupling its effects from overall execution,
quantifies its influence through a learned weighting scheme, and infers
inter-task dependencies for coherence-aware graph generation. Extensive
experiments show that CoTAM outperforms implicit methods, bridging the gap
between dynamic workload behavior and existing designs while demonstrating the
importance of incorporating cache coherence into task graph modeling for
accurate and generalizable system-level analysis.

</details>


### [146] [WebAssembly and Unikernels: A Comparative Study for Serverless at the Edge](https://arxiv.org/abs/2509.09400)
*Valerio Besozzi,Enrico Fiasco,Marco Danelutto,Patrizio Dazzi*

Main category: cs.DC

TL;DR: The paper compares performance of WebAssembly and Firecracker MicroVMs for serverless computing at the edge, focusing on cold start latency and workload execution.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to identify the most suitable lightweight execution environment for minimizing cold start latency in urgent edge computing scenarios, where quick response is crucial.

Method: The study introduces Limes, a WebAssembly runtime built on Wasmtime, and compares its performance to Firecracker MicroVMs using serverless workloads.

Result: WebAssembly provides lower cold start times for lightweight functions, while Firecracker shows stable cold starts and superior performance for I/O-heavy tasks.

Conclusion: WebAssembly is advantageous for lightweight tasks due to its reduced latency, but Firecracker is better suited for complex and I/O-intensive workloads in serverless edge computing.

Abstract: Serverless computing at the edge requires lightweight execution environments
to minimize cold start latency, especially in Urgent Edge Computing (UEC). This
paper compares WebAssembly and unikernel-based MicroVMs for serverless
workloads. We present Limes, a WebAssembly runtime built on Wasmtime, and
evaluate it against the Firecracker-based environment used in SPARE. Results
show that WebAssembly offers lower cold start times for lightweight functions
but suffers with complex workloads, while Firecracker provides higher, but
stable, cold starts and better execution performance, particularly for
I/O-heavy tasks.

</details>


### [147] [Barycentric Coded Distributed Computing with Flexible Recovery Threshold for Collaborative Mobile Edge Computing](https://arxiv.org/abs/2509.09435)
*Houming Qiu,Kun Zhu,Dusit Niyato,Nguyen Cong Luong,Changyan Yi,Chen Dai*

Main category: cs.DC

TL;DR: The paper proposes an improved coded distributed computing (CDC) scheme using barycentric rational interpolation to address straggler issues in mobile edge computing systems, ensuring flexibility, numerical stability, and enhanced performance.


<details>
  <summary>Details</summary>
Motivation: Current CDC schemes for collaborative mobile edge computing face issues such as rigid recovery thresholds and numerical instability due to poles in encoding/decoding functions.

Method: The authors utilize barycentric rational interpolation to develop a CDC scheme that allows decoding with any partial results, supports computations over both finite and real fields, and eliminates poles for enhanced tuning and numerical stability.

Result: Experimental results demonstrate superior waiting time reduction and approximate accuracy compared to existing CDC solutions.

Conclusion: The proposed scheme effectively mitigates straggler effects, accelerates computing tasks, and offers robustness, flexibility, and accuracy improvements over traditional CDC methods.

Abstract: Collaborative mobile edge computing (MEC) has emerged as a promising paradigm
to enable low-capability edge nodes to cooperatively execute
computation-intensive tasks. However, straggling edge nodes (stragglers)
significantly degrade the performance of MEC systems by prolonging computation
latency. While coded distributed computing (CDC) as an effective technique is
widely adopted to mitigate straggler effects, existing CDC schemes exhibit two
critical limitations: (i) They cannot successfully decode the final result
unless the number of received results reaches a fixed recovery threshold, which
seriously restricts their flexibility; (ii) They suffer from inherent poles in
their encoding/decoding functions, leading to decoding inaccuracies and
numerical instability in the computational results. To address these
limitations, this paper proposes an approximated CDC scheme based on
barycentric rational interpolation. The proposed CDC scheme offers several
outstanding advantages. Firstly, it can decode the final result leveraging any
returned results from workers. Secondly, it supports computations over both
finite and real fields while ensuring numerical stability. Thirdly, its
encoding/decoding functions are free of poles, which not only enhances
approximation accuracy but also achieves flexible accuracy tuning. Fourthly, it
integrates a novel BRI-based gradient coding algorithm accelerating the
training process while providing robustness against stragglers. Finally,
experimental results reveal that the proposed scheme is superior to existing
CDC schemes in both waiting time and approximate accuracy.

</details>


### [148] [Weaker Assumptions for Asymmetric Trust](https://arxiv.org/abs/2509.09493)
*Ignacio Amores-Sesar,Christian Cachin,Juan Villacis*

Main category: cs.DC

TL;DR: The paper addresses the challenges in asymmetric trust models for distributed systems by introducing less restrictive assumptions and proposing improved algorithms for reliable broadcast and consensus.


<details>
  <summary>Details</summary>
Motivation: Distributed systems with asymmetric trust face limitations in reliability and consensus when only classical quorum properties are considered. Current methods often use restrictive assumptions, negating the advantages of asymmetric trust.

Method: The authors propose a novel characterization of asymmetric problems and develop algorithms for reliable broadcast and consensus with weaker assumptions than prior solutions.

Result: The proposed approach enables reliable broadcast and consensus in asymmetric trust settings without the need for overly restrictive assumptions, preserving the system's flexibility.

Conclusion: By leveraging the new characterization and algorithms, the paper demonstrates that foundational problems in distributed systems can be solved in asymmetric trust environments with reduced restrictions.

Abstract: In distributed systems with asymmetric trust, each participant is free to
make its own trust assumptions about others, captured by an asymmetric quorum
system. This contrasts with ordinary, symmetric quorum systems and threshold
models, where trust assumptions are uniformly shared among participants.
Fundamental problems like reliable broadcast and consensus are unsolvable in
the asymmetric model if quorum systems satisfy only the classical properties of
consistency and availability. Existing approaches overcome this by introducing
stronger assumptions. We show that some of these assumptions are overly
restrictive, so much so that they effectively eliminate the benefits of
asymmetric trust. To address this, we propose a new approach to characterize
asymmetric problems and, building upon it, present algorithms for reliable
broadcast and consensus that require weaker assumptions than previous
solutions. Our methods are general and can be extended to other core problems
in systems with asymmetric trust.

</details>


### [149] [TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes](https://arxiv.org/abs/2509.09525)
*Jialiang Huang,Teng Ma,Zheng Liu,Sixing Lin,Kang Chen,Jinlei Jiang,Xia Liao,Yingdi Shan,Yongwei Wu,Ning Zhang,Mengting Lu,Tao Ma,Haifeng Gong,Mingxing Zhang*

Main category: cs.DC

TL;DR: TrEnv is a serverless platform tailored for LLM agents, reducing latency and memory overhead compared to current systems.


<details>
  <summary>Details</summary>
Motivation: Emerging workloads like LLM agents have unpredictable patterns, requiring a more efficient serverless platform to address high infrastructure costs.

Method: TrEnv optimizes serverless computing via co-designed environments, repurposable sandboxes, memory templates, browser sharing, and page cache bypassing.

Result: P99 latency reduced by up to 7X and memory usage reduced by 48% in container settings; for VM-based agents, latency decreased by 58% and memory reduced by 61%, outperforming systems like E2B.

Conclusion: TrEnv demonstrates significant efficiency improvements for serverless computing tailored for the unique demands of LLM agents.

Abstract: Serverless computing provides dynamic scalability, but its infrastructure
overhead becomes a bottleneck for emerging workloads such as LLM agents, which
exhibit unpredictable invocation patterns and variable resource demands. Our
analysis shows that for these agents, the cost of running on serverless
platforms can reach up to 70% of the cost of LLM API calls. This finding
motivates the need for a more efficient, high-density serverless platform. We
present TrEnv, a co-designed serverless platform that supports both container-
and VM-based environments, optimized for the unique demands of LLM agents.
TrEnv reduces startup latency and memory usage through repurposable sandboxes
and memory templates, which enable fast reuse and restoration of execution
environments. To further reduce overhead in VM-based agent workloads, TrEnv
leverages browser sharing and a page cache bypassing mechanism. Evaluations
show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in
container-based settings, and achieves up to 58% lower P99 latency and 61%
memory savings for VM-based agents compared to state-of-the-art systems like
E2B.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [150] [Uncertainty Estimation using Variance-Gated Distributions](https://arxiv.org/abs/2509.08846)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: This paper critiques conventional uncertainty decomposition in neural networks and proposes a new framework based on signal-to-noise ratio and ensembles.


<details>
  <summary>Details</summary>
Motivation: The goal is to improve uncertainty quantification for applications with high stakes, as simple additive decomposition methods are now being questioned.

Method: The authors present a framework leveraging the signal-to-noise ratio of class predictions and introduce a variance-gated measure that incorporates ensemble methods.

Result: The variance-gated measure is shown to provide insights into committee machine diversity collapse, improving uncertainty quantification.

Conclusion: This new framework offers a more intuitive and possibly accurate method for estimating and decomposing uncertainty in neural network predictions.

Abstract: Evaluation of per-sample uncertainty quantification from neural networks is
essential for decision-making involving high-risk applications. A common
approach is to use the predictive distribution from Bayesian or approximation
models and decompose the corresponding predictive uncertainty into epistemic
(model-related) and aleatoric (data-related) components. However, additive
decomposition has recently been questioned. In this work, we propose an
intuitive framework for uncertainty estimation and decomposition based on the
signal-to-noise ratio of class probability distributions across different model
predictions. We introduce a variance-gated measure that scales predictions by a
confidence factor derived from ensembles. We use this measure to discuss the
existence of a collapse in the diversity of committee machines.

</details>


### [151] [Instance-Optimal Matrix Multiplicative Weight Update and Its Quantum Applications](https://arxiv.org/abs/2509.08911)
*Weiyuan Gong,Tongyang Li,Xinzhao Wang,Zhiyu Zhang*

Main category: cs.LG

TL;DR: This paper presents an enhanced algorithm for the matrix version of Learning from Expert Advice (LEA), achieving improved instance-optimal regret bounds, while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve upon the minimax-optimal regret bound provided by the Matrix Multiplicative Weight Update (MMWU) algorithm and explore applications in quantum learning theory.

Method: The authors develop a potential-based general framework for matrix LEA, utilize a new Jensen's trace inequality built on Laplace transform techniques, and adopt an optimal potential function inspired by the imaginary error function for vector LEA.

Result: The algorithm achieves an instance-optimal regret bound of $O(\sqrt{T\cdot S(X||d^{-1}I_d)})$, improves upon MMWU without computational overhead, and shows superior performance in various quantum learning scenarios.

Conclusion: The new algorithm improves regret bounds in matrix LEA for free computationally and has promising applications in quantum learning, enabling better handling of noisy and random quantum states as well as prediction of nonlinear quantum properties.

Abstract: The Matrix Multiplicative Weight Update (MMWU) is a seminal online learning
algorithm with numerous applications. Applied to the matrix version of the
Learning from Expert Advice (LEA) problem on the $d$-dimensional spectraplex,
it is well known that MMWU achieves the minimax-optimal regret bound of
$O(\sqrt{T\log d})$, where $T$ is the time horizon. In this paper, we present
an improved algorithm achieving the instance-optimal regret bound of
$O(\sqrt{T\cdot S(X||d^{-1}I_d)})$, where $X$ is the comparator in the regret,
$I_d$ is the identity matrix, and $S(\cdot||\cdot)$ denotes the quantum
relative entropy. Furthermore, our algorithm has the same computational
complexity as MMWU, indicating that the improvement in the regret bound is
``free''.
  Technically, we first develop a general potential-based framework for matrix
LEA, with MMWU being its special case induced by the standard exponential
potential. Then, the crux of our analysis is a new ``one-sided'' Jensen's trace
inequality built on a Laplace transform technique, which allows the application
of general potential functions beyond exponential to matrix LEA. Our algorithm
is finally induced by an optimal potential function from the vector LEA
problem, based on the imaginary error function.
  Complementing the above, we provide a memory lower bound for matrix LEA, and
explore the applications of our algorithm in quantum learning theory. We show
that it outperforms the state of the art for learning quantum states corrupted
by depolarization noise, random quantum states, and Gibbs states. In addition,
applying our algorithm to linearized convex losses enables predicting nonlinear
quantum properties, such as purity, quantum virtual cooling, and R\'{e}nyi-$2$
correlation.

</details>


### [152] [Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates](https://arxiv.org/abs/2509.08933)
*Sreejeet Maity,Aritra Mitra*

Main category: cs.LG

TL;DR: The paper introduces a robust Q-learning algorithm for reinforcement learning in the presence of adversarially corrupted reward signals, ensuring finite-time convergence despite such perturbations.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of learning optimal policies in reinforcement learning settings where reward signals can be disrupted by adversarial corruption, which degrades the performance of classical algorithms.

Method: Propose a variant of Q-learning with provable robustness against reward corruption, establish finite-time convergence rate matching non-adversarial cases, and derive information-theoretic bounds. Explore an adaptation requiring no prior knowledge on reward statistics using refined mathematical tools.

Result: The robust Q-learning variant shows finite-time convergence resilience against adversarial reward corruption, with an unavoidable additive term proportional to corrupted samples. A second variant handles unknown reward statistics effectively.

Conclusion: This work bridges the gap in robust reinforcement learning by providing finite-time robustness guarantees for asynchronous Q-learning, even under adversarial corruption conditions.

Abstract: We consider the problem of learning the optimal policy in a discounted,
infinite-horizon reinforcement learning (RL) setting where the reward signal is
subject to adversarial corruption. Such corruption, which may arise from
extreme noise, sensor faults, or malicious attacks, can severely degrade the
performance of classical algorithms such as Q-learning. To address this
challenge, we propose a new provably robust variant of the Q-learning algorithm
that operates effectively even when a fraction of the observed rewards are
arbitrarily perturbed by an adversary. Under the asynchronous sampling model
with time-correlated data, we establish that despite adversarial corruption,
the finite-time convergence rate of our algorithm matches that of existing
results for the non-adversarial case, up to an additive term proportional to
the fraction of corrupted samples. Moreover, we derive an information-theoretic
lower bound revealing that the additive corruption term in our upper bounds is
unavoidable.
  Next, we propose a variant of our algorithm that requires no prior knowledge
of the statistics of the true reward distributions. The analysis of this
setting is particularly challenging and is enabled by carefully exploiting a
refined Azuma-Hoeffding inequality for almost-martingales, a technical tool
that might be of independent interest. Collectively, our contributions provide
the first finite-time robustness guarantees for asynchronous Q-learning,
bridging a significant gap in robust RL.

</details>


### [153] [Group Distributionally Robust Machine Learning under Group Level Distributional Uncertainty](https://arxiv.org/abs/2509.08942)
*Xenia Konti,Yi Shen,Zifan Wang,Karl Henrik Johansson,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: This paper proposes a framework using Wasserstein-based distributionally robust optimization (DRO) to improve machine learning performance for underrepresented data groups in noisy, evolving environments.


<details>
  <summary>Details</summary>
Motivation: Standard machine learning models often fail to perform well on underrepresented or atypical data groups, especially in heterogeneous and non-stationary environments.

Method: The authors introduce a framework leveraging Wasserstein-based DRO to handle distributional uncertainty within groups, coupled with a gradient descent-ascent algorithm for optimization.

Result: The proposed method effectively improves worst-group performance and shows efficacy on real-world datasets.

Conclusion: The framework successfully addresses challenges with underrepresented groups in noisy, evolving environments by combining DRO with worst-group performance optimization principles.

Abstract: The performance of machine learning (ML) models critically depends on the
quality and representativeness of the training data. In applications with
multiple heterogeneous data generating sources, standard ML methods often learn
spurious correlations that perform well on average but degrade performance for
atypical or underrepresented groups. Prior work addresses this issue by
optimizing the worst-group performance. However, these approaches typically
assume that the underlying data distributions for each group can be accurately
estimated using the training data, a condition that is frequently violated in
noisy, non-stationary, and evolving environments. In this work, we propose a
novel framework that relies on Wasserstein-based distributionally robust
optimization (DRO) to account for the distributional uncertainty within each
group, while simultaneously preserving the objective of improving the
worst-group performance. We develop a gradient descent-ascent algorithm to
solve the proposed DRO problem and provide convergence results. Finally, we
validate the effectiveness of our method on real-world data.

</details>


### [154] [FoundationalECGNet: A Lightweight Foundational Model for ECG-based Multitask Cardiac Analysis](https://arxiv.org/abs/2509.08961)
*Md. Sajeebul Islam Sk.,Md Jobayer,Md Mehedi Hasan Shawon,Md. Golam Raibul Alam*

Main category: cs.LG

TL;DR: This paper presents FoundationalECGNet, an automated ECG classification model that achieves high diagnostic accuracy and provides risk estimations.


<details>
  <summary>Details</summary>
Motivation: Cardiovascular diseases remain a major cause of mortality, highlighting the need for accurate and scalable ECG diagnostic systems to address challenges like noise, class imbalance, and dataset heterogeneity.

Method: The method includes a dual-stage denoising process (using Morlet and Daubechies wavelets), CBAM, GAT, and Time Series Transformers to capture spatial and temporal dependencies in multi-channel ECG signals. It further classifies signals hierarchically into normal or one of five cardiac conditions.

Result: FoundationalECGNet achieves 99% F1-score for Normal vs. Abnormal classification and state-of-the-art performance in multi-class disease detection, including nearly 99% F1-scores for specific conditions like Conduction Disorders and Hypertrophy.

Conclusion: FoundationalECGNet offers a scalable and interpretable framework for ECG analysis, improving diagnostic precision and aiding clinical decision-making, with potential benefits for healthcare.

Abstract: Cardiovascular diseases (CVDs) remain a leading cause of mortality worldwide,
underscoring the importance of accurate and scalable diagnostic systems.
Electrocardiogram (ECG) analysis is central to detecting cardiac abnormalities,
yet challenges such as noise, class imbalance, and dataset heterogeneity limit
current methods. To address these issues, we propose FoundationalECGNet, a
foundational framework for automated ECG classification. The model integrates a
dual-stage denoising by Morlet and Daubechies wavelets transformation,
Convolutional Block Attention Module (CBAM), Graph Attention Networks (GAT),
and Time Series Transformers (TST) to jointly capture spatial and temporal
dependencies in multi-channel ECG signals. FoundationalECGNet first
distinguishes between Normal and Abnormal ECG signals, and then classifies the
Abnormal signals into one of five cardiac conditions: Arrhythmias, Conduction
Disorders, Myocardial Infarction, QT Abnormalities, or Hypertrophy. Across
multiple datasets, the model achieves a 99% F1-score for Normal vs. Abnormal
classification and shows state-of-the-art performance in multi-class disease
detection, including a 99% F1-score for Conduction Disorders and Hypertrophy,
as well as a 98.9% F1-score for Arrhythmias. Additionally, the model provides
risk level estimations to facilitate clinical decision-making. In conclusion,
FoundationalECGNet represents a scalable, interpretable, and generalizable
solution for automated ECG analysis, with the potential to improve diagnostic
precision and patient outcomes in healthcare settings. We'll share the code
after acceptance.

</details>


### [155] [Value bounds and Convergence Analysis for Averages of LRP attributions](https://arxiv.org/abs/2509.08963)
*Alexander Binder,Nastaran Takmil-Homayouni,Urun Dogan*

Main category: cs.LG

TL;DR: The study examines numerical properties of Layer-wise relevance propagation (LRP) attribution methods, deriving bounds for attribution values and constants governing empirical convergence. Notably, LRP-beta constants remain independent of weight norms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the numerical behavior of LRP attribution methods and their implications for empirical convergence, especially in test samples with multiple data augmentations.

Method: The paper analyzes LRP-type methods by representing them via modified gradient matrices and deriving upper bounds for singular values and attribution map components.

Result: The study finds constants that govern the convergence of attributions to expectations, highlighting notable differences between LRP-beta and other approaches like gradient-based methods and LRP-epsilon.

Conclusion: LRP-beta attribution methods offer a distinct advantage with constants unaffected by weight norms, providing insights for applications like Smoothgrad and multiple augmentations on test samples.

Abstract: We analyze numerical properties of Layer-wise relevance propagation
(LRP)-type attribution methods by representing them as a product of modified
gradient matrices. This representation creates an analogy to matrix
multiplications of Jacobi-matrices which arise from the chain rule of
differentiation. In order to shed light on the distribution of attribution
values, we derive upper bounds for singular values. Furthermore we derive
component-wise bounds for attribution map values. As a main result, we apply
these component-wise bounds to obtain multiplicative constants. These constants
govern the convergence of empirical means of attributions to expectations of
attribution maps. This finding has important implications for scenarios where
multiple non-geometric data augmentations are applied to individual test
samples, as well as for Smoothgrad-type attribution methods. In particular, our
analysis reveals that the constants for LRP-beta remain independent of weight
norms, a significant distinction from both gradient-based methods and
LRP-epsilon.

</details>


### [156] [Green Federated Learning via Carbon-Aware Client and Time Slot Scheduling](https://arxiv.org/abs/2509.08980)
*Daniel Richards Arputharaj,Charlotte Rodriguez,Angelo Rodio,Giovanni Neglia*

Main category: cs.LG

TL;DR: The paper explores reducing carbon emissions in Federated Learning through carbon-aware client scheduling and training practices, offering higher model accuracy under stringent carbon limitations.


<details>
  <summary>Details</summary>
Motivation: Federated Learning incurs high carbon emissions due to large-scale distributed training, but leveraging regional and temporal variations in carbon intensity could mitigate this impact.

Method: The authors propose a carbon-aware scheduler integrating slack time, α-fair carbon allocation, and global fine-tuning, balancing carbon reduction and model performance trade-offs.

Result: Experiments on real-world carbon intensity data demonstrated significant accuracy improvements over conventional methods, particularly under strict carbon constraints.

Conclusion: A carefully designed carbon-aware FL framework can achieve substantial emission reductions without compromising model performance, making it suitable under varying carbon budgets.

Abstract: Training large-scale machine learning models incurs substantial carbon
emissions. Federated Learning (FL), by distributing computation across
geographically dispersed clients, offers a natural framework to leverage
regional and temporal variations in Carbon Intensity (CI). This paper
investigates how to reduce emissions in FL through carbon-aware client
selection and training scheduling. We first quantify the emission savings of a
carbon-aware scheduling policy that leverages slack time -- permitting a modest
extension of the training duration so that clients can defer local training
rounds to lower-carbon periods. We then examine the performance trade-offs of
such scheduling which stem from statistical heterogeneity among clients,
selection bias in participation, and temporal correlation in model updates. To
leverage these trade-offs, we construct a carbon-aware scheduler that
integrates slack time, $\alpha$-fair carbon allocation, and a global
fine-tuning phase. Experiments on real-world CI data show that our scheduler
outperforms slack-agnostic baselines, achieving higher model accuracy across a
wide range of carbon budgets, with especially strong gains under tight carbon
constraints.

</details>


### [157] [STRIDE: Scalable and Interpretable XAI via Subset-Free Functional Decomposition](https://arxiv.org/abs/2509.09070)
*Chaeyun Ko*

Main category: cs.LG

TL;DR: STRIDE is a scalable model-agnostic framework for explainable AI (XAI) that addresses inefficiencies in reasoning over feature subsets and limitations of scalar attribution by leveraging orthogonal functional decomposition within RKHS.


<details>
  <summary>Details</summary>
Motivation: Current XAI methods face challenges like high computational cost in subset reasoning and reduced interpretability when effects are represented as single scalar values. STRIDE aims to overcome these obstacles for better scalability and expressiveness.

Method: STRIDE uses functional decomposition in the RKHS and computes functional components via recursive kernel-centering procedures. It avoids subset enumeration and supports both local and global views.

Result: STRIDE achieved a speedup of up to 9.7 times on benchmarks while maintaining high fidelity (R2 ≥ 0.81) and strong rank agreement across datasets.

Conclusion: STRIDE complements XAI scalar attribution methods, introduces structured functional diagnostics, and demonstrates scalability and effectiveness in tabular setups.

Abstract: Most explainable AI (XAI) frameworks face two practical limitations: the
exponential cost of reasoning over feature subsets and the reduced
expressiveness of summarizing effects as single scalar values. We present
STRIDE, a scalable framework that aims to mitigate both issues by framing
explanation as a subset-enumeration-free, orthogonal functional decomposition
in a Reproducing Kernel Hilbert Space (RKHS). Rather than focusing only on
scalar attributions, STRIDE computes functional components f_S(x_S) via an
analytical projection scheme based on a recursive kernel-centering procedure,
avoiding explicit subset enumeration. In the tabular setups we study, the
approach is model-agnostic, provides both local and global views, and is
supported by theoretical results on orthogonality and L^2 convergence under
stated assumptions. On public tabular benchmarks in our environment, we
observed speedups ranging from 0.6 times (slower than TreeSHAP on a small
dataset) to 9.7 times (California), with a median approximate 3.0 times across
10 datasets, while maintaining high fidelity (R^2 between 0.81 and 0.999) and
substantial rank agreement on most datasets. Overall, STRIDE complements scalar
attribution methods by offering a structured functional perspective, enabling
novel diagnostics like 'component surgery' to quantitatively measure the impact
of specific interactions within our experimental scope.

</details>


### [158] [Active Learning and Explainable AI for Multi-Objective Optimization of Spin Coated Polymers](https://arxiv.org/abs/2509.08988)
*Brendan Young,Brendan Alvey,Andreas Werbrouck,Will Murphy,James Keller,Mattias J. Young,Matthew Maschmann*

Main category: cs.LG

TL;DR: This paper discusses a framework combining PyePAL, UMAP visualization, and fuzzy linguistic summaries for optimizing polymer thin film processing parameters.


<details>
  <summary>Details</summary>
Motivation: The goal was to optimize spin coating parameters to achieve specific mechanical properties in polymer thin films, ensuring efficient exploration of a high-dimensional design space.

Method: The authors utilized PyePAL, a Pareto front learning algorithm with Gaussian process models, UMAP for dimensional visualization, and fuzzy linguistic summaries to interpret the optimization process.

Result: The approach effectively identified optimal designs for specific mechanical properties and provided interpretable insights through visualizations and linguistic summaries.

Conclusion: The integrated framework enables efficient parameter optimization and facilitates better understanding of the trade-offs in thin film design, aiding expert decision-making.

Abstract: Spin coating polymer thin films to achieve specific mechanical properties is
inherently a multi-objective optimization problem. We present a framework that
integrates an active Pareto front learning algorithm (PyePAL) with
visualization and explainable AI techniques to optimize processing parameters.
PyePAL uses Gaussian process models to predict objective values (hardness and
elasticity) from the design variables (spin speed, dilution, and polymer
mixture), guiding the adaptive selection of samples toward promising regions of
the design space. To enable interpretable insights into the high-dimensional
design space, we utilize UMAP (Uniform Manifold Approximation and Projection)
for two-dimensional visualization of the Pareto front exploration.
Additionally, we incorporate fuzzy linguistic summaries, which translate the
learned relationships between process parameters and performance objectives
into linguistic statements, thus enhancing the explainability and understanding
of the optimization results. Experimental results demonstrate that our method
efficiently identifies promising polymer designs, while the visual and
linguistic explanations facilitate expert-driven analysis and knowledge
discovery.

</details>


### [159] [ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning](https://arxiv.org/abs/2509.09534)
*Sena Ergisi,Luis Maßny,Rawad Bitar*

Main category: cs.LG

TL;DR: The paper introduces ProDiGy, a Byzantine-robust federated learning (FL) algorithm using a dual scoring system for client gradient evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of federated learning (FL) algorithms to adversarial attacks, especially under non-IID data heterogeneity.

Method: ProDiGy evaluates client gradients using a joint dual scoring system based on proximity and dissimilarity to distinguish honest clients from attackers.

Result: ProDiGy demonstrated superior performance and robustness against adversarial attacks, especially in non-IID data scenarios, outperforming existing defense mechanisms.

Conclusion: The dual perspective approach enhances FL security by promoting natural similarity among honest clients and identifying abnormal uniformity as potential attack signals.

Abstract: Federated Learning (FL) emerged as a widely studied paradigm for distributed
learning. Despite its many advantages, FL remains vulnerable to adversarial
attacks, especially under data heterogeneity. We propose a new Byzantine-robust
FL algorithm called ProDiGy. The key novelty lies in evaluating the client
gradients using a joint dual scoring system based on the gradients' proximity
and dissimilarity. We demonstrate through extensive numerical experiments that
ProDiGy outperforms existing defenses in various scenarios. In particular, when
the clients' data do not follow an IID distribution, while other defense
mechanisms fail, ProDiGy maintains strong defense capabilities and model
accuracy. These findings highlight the effectiveness of a dual perspective
approach that promotes natural similarity among honest clients while detecting
suspicious uniformity as a potential indicator of an attack.

</details>


### [160] [Fast attention mechanisms: a tale of parallelism](https://arxiv.org/abs/2509.09001)
*Jingwen Liu,Hantao Yu,Clayton Sanford,Alexandr Andoni,Daniel Hsu*

Main category: cs.LG

TL;DR: The paper introduces Approximate Nearest Neighbor Attention (ANNA), an efficient attention mechanism for transformers, achieving sub-quadratic complexity without losing representational power.


<details>
  <summary>Details</summary>
Motivation: Transformers struggle with quadratic time complexity, limiting their scalability for tasks requiring massive parallel computation.

Method: The paper designs ANNA, leveraging Approximate Nearest Neighbor techniques to reduce complexity, while retaining the capacity to perform reasoning tasks and simulate low-rank transformers.

Result: ANNA-transformers are demonstrated to match the capabilities of standard attention models in solving reasoning tasks like Match2 and k-hop with near-optimal depth.

Conclusion: The method unifies and extends reasoning approaches for efficient transformer approximations, offering scalable and powerful solutions for computational tasks.

Abstract: Transformers have the representational capacity to simulate Massively
Parallel Computation (MPC) algorithms, but they suffer from quadratic time
complexity, which severely limits their scalability. We introduce an efficient
attention mechanism called Approximate Nearest Neighbor Attention (ANNA) with
sub-quadratic time complexity. We prove that ANNA-transformers (1) retain the
expressive power previously established for standard attention in terms of
matching the capabilities of MPC algorithms, and (2) can solve key reasoning
tasks such as Match2 and $k$-hop with near-optimal depth. Using the MPC
framework, we further prove that constant-depth ANNA-transformers can simulate
constant-depth low-rank transformers, thereby providing a unified way to reason
about a broad class of efficient attention approximations.

</details>


### [161] [Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison](https://arxiv.org/abs/2509.09009)
*Marianna Nezhurina,Taishi Nakamura,Timur Carstensen,Niccolò Ajroldi,Ville Komulainen,David Salinas,Jenia Jitsev*

Main category: cs.LG

TL;DR: The paper introduces a family of dense transformer models, open-sci-ref, designed as research baselines across various scales and datasets and provides tools for standardized comparison.


<details>
  <summary>Details</summary>
Motivation: To provide standardized baselines and tools for researchers to evaluate and compare training approaches and datasets across compute, scale, and performance.

Method: Developed transformer models with 0.13B to 1.7B parameters, trained on 8 open reference datasets, and evaluated on standardized benchmarks with intermediate checkpoints and scaling trend analysis.

Result: Training on NemoTron-CC HQ dataset performs best, beating other datasets like DCLM-baseline and FineWeb-Edu, with results supporting reproducibility and standardization of comparisons.

Conclusion: The work sets crucial baselines, introduces reliable datasets and evaluation protocols, and provides resources for advancing transformer training and scaling studies.

Abstract: We introduce open-sci-ref, a family of dense transformer models trained as
research baselines across multiple model (0.13B to 1.7B parameters) and token
scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on
various standardized benchmarks, our training runs set establishes reference
points that enable researchers to assess the sanity and quality of alternative
training approaches across scales and datasets. Intermediate checkpoints allow
comparison and studying of the training dynamics. The established reference
baselines allow training procedures to be compared through their scaling
trends, aligning them on a common compute axis. Comparison of open reference
datasets reveals that training on NemoTron-CC HQ consistently outperforms other
reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to
intermediate training checkpoints, the release includes logs, code, and
downstream evaluations to simplify reproduction, standardize comparison, and
facilitate future research.

</details>


### [162] [Deep Context-Conditioned Anomaly Detection for Tabular Data](https://arxiv.org/abs/2509.09030)
*Spencer King,Zhilu Zhang,Ruofan Yu,Baris Coskun,Wei Ding,Qian Cui*

Main category: cs.LG

TL;DR: The paper proposes a context-conditional anomaly detection framework for tabular datasets using a deep autoencoder, achieving better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for unsupervised anomaly detection often miss context-specific nuances in heterogeneous tabular data, leading to lower detection accuracy.

Method: The framework uses context feature identification and conditional data distribution modeling with a deep autoencoder to account for contextual variations in tabular datasets.

Result: Experimental results on multiple benchmark datasets show that this approach outperforms current state-of-the-art methods in anomaly detection.

Conclusion: The study highlights the critical role of incorporating context for enhanced anomaly detection in heterogeneous tabular data.

Abstract: Anomaly detection is critical in domains such as cybersecurity and finance,
especially when working with large-scale tabular data. Yet, unsupervised
anomaly detection -- where no labeled anomalies are available -- remains a
significant challenge. Although various deep learning methods have been
proposed to model a dataset's joint distribution, real-world tabular data often
contain heterogeneous contexts (e.g., different users), making globally rare
events normal under certain contexts. Consequently, relying on a single global
distribution can overlook these contextual nuances, degrading detection
performance. In this paper, we present a context-conditional anomaly detection
framework tailored for tabular datasets. Our approach automatically identifies
context features and models the conditional data distribution using a simple
deep autoencoder. Extensive experiments on multiple tabular benchmark datasets
demonstrate that our method outperforms state-of-the-art approaches,
underscoring the importance of context in accurately distinguishing anomalous
from normal instances.

</details>


### [163] [MoWE : A Mixture of Weather Experts](https://arxiv.org/abs/2509.09052)
*Dibyajyoti Chakraborty,Romit Maulik,Peter Harrington,Dallas Foster,Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: This paper proposes a Mixture of Experts (MoWE) approach to improve weather model accuracy by combining outputs from existing models using a Vision Transformer-based gating network.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome a plateau in progress among state-of-the-art data-driven weather models by leveraging existing high-quality models in a novel way instead of building a new forecaster.

Method: A Mixture of Experts (MoWE) model is trained using significantly fewer computational resources, employing a Vision Transformer-based gating network that dynamically weights the contributions of multiple expert models based on each grid point and forecast lead time.

Result: The proposed approach improves weather forecasting accuracy, achieving up to a 10% reduction in RMSE compared to the best-performing AI weather model over a 2-day forecast horizon, outperforming individual models and simple averaging methods.

Conclusion: MoWE offers a computationally efficient and scalable strategy to enhance data-driven weather forecasting accuracy by optimally synthesizing outputs from leading models.

Abstract: Data-driven weather models have recently achieved state-of-the-art
performance, yet progress has plateaued in recent years. This paper introduces
a Mixture of Experts (MoWE) approach as a novel paradigm to overcome these
limitations, not by creating a new forecaster, but by optimally combining the
outputs of existing models. The MoWE model is trained with significantly lower
computational resources than the individual experts. Our model employs a Vision
Transformer-based gating network that dynamically learns to weight the
contributions of multiple "expert" models at each grid point, conditioned on
forecast lead time. This approach creates a synthesized deterministic forecast
that is more accurate than any individual component in terms of Root Mean
Squared Error (RMSE). Our results demonstrate the effectiveness of this method,
achieving up to a 10% lower RMSE than the best-performing AI weather model on a
2-day forecast horizon, significantly outperforming individual experts as well
as a simple average across experts. This work presents a computationally
efficient and scalable strategy to push the state of the art in data-driven
weather prediction by making the most out of leading high-quality forecast
models.

</details>


### [164] [A Scoping Review of Machine Learning Applications in Power System Protection and Disturbance Management](https://arxiv.org/abs/2509.09053)
*Julian Oelhaf,Georg Kordowich,Mehran Pashaei,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: This review evaluates the use of machine learning (ML) in power system protection and disturbance management. It identifies inconsistencies in research methodologies, dataset quality, and evaluation metrics, and suggests standardized practices for future work.


<details>
  <summary>Details</summary>
Motivation: The integration of renewable energy and distributed energy resources necessitates improved protection schemes for power systems. Current protection mechanisms face challenges due to the dynamic nature of grid conditions, prompting exploration into ML solutions.

Method: A comprehensive scoping review of over 100 publications was conducted, guided by the PRISMA framework. The study synthesizes research trends, evaluates ML performance, introduces a taxonomy, and proposes standardized practices.

Result: The study indicates ML models perform well on simulated datasets but lack real-world validation. It identifies fragmented literature, terminological inconsistencies, and methodological gaps that hinder the general applicability of findings.

Conclusion: The paper emphasizes the need for standardized reporting, better dataset documentation, and robust evaluation methods. It calls for prioritizing real-world validation, public benchmarks, and advanced ML architectures to make ML-based protection viable for modern power systems.

Abstract: The integration of renewable and distributed energy resources reshapes modern
power systems, challenging conventional protection schemes. This scoping review
synthesizes recent literature on machine learning (ML) applications in power
system protection and disturbance management, following the PRISMA for Scoping
Reviews framework. Based on over 100 publications, three key objectives are
addressed: (i) assessing the scope of ML research in protection tasks; (ii)
evaluating ML performance across diverse operational scenarios; and (iii)
identifying methods suitable for evolving grid conditions. ML models often
demonstrate high accuracy on simulated datasets; however, their performance
under real-world conditions remains insufficiently validated. The existing
literature is fragmented, with inconsistencies in methodological rigor, dataset
quality, and evaluation metrics. This lack of standardization hampers the
comparability of results and limits the generalizability of findings. To
address these challenges, this review introduces a ML-oriented taxonomy for
protection tasks, resolves key terminological inconsistencies, and advocates
for standardized reporting practices. It further provides guidelines for
comprehensive dataset documentation, methodological transparency, and
consistent evaluation protocols, aiming to improve reproducibility and enhance
the practical relevance of research outcomes. Critical gaps remain, including
the scarcity of real-world validation, insufficient robustness testing, and
limited consideration of deployment feasibility. Future research should
prioritize public benchmark datasets, realistic validation methods, and
advanced ML architectures. These steps are essential to move ML-based
protection from theoretical promise to practical deployment in increasingly
dynamic and decentralized power systems.

</details>


### [165] ["A 6 or a 9?": Ensemble Learning Through the Multiplicity of Performant Models and Explanations](https://arxiv.org/abs/2509.09073)
*Gianlucca Zuin,Adriano Veloso*

Main category: cs.LG

TL;DR: The paper introduces the Rashomon Ensemble method, selecting diverse yet accurate models to improve robustness and generalization, validated through real-world datasets.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of selecting machine learning models that generalize well to unseen data, particularly in cases with the Rashomon Effect, where multiple models achieve similar high performance.

Method: The proposed approach strategically selects models from the Rashomon set, grouping them based on performance and explanation diversity, to form ensembles that are robust to distribution shifts while maintaining accuracy.

Result: The method showed up to 0.20+ AUROC improvements and demonstrated practical applicability in real-world business scenarios where the Rashomon ratio is significant.

Conclusion: The Rashomon Ensemble improves robustness and generalization for machine learning models in diverse and complex real-world settings, offering tangible benefits.

Abstract: Creating models from past observations and ensuring their effectiveness on
new data is the essence of machine learning. However, selecting models that
generalize well remains a challenging task. Related to this topic, the Rashomon
Effect refers to cases where multiple models perform similarly well for a given
learning problem. This often occurs in real-world scenarios, like the
manufacturing process or medical diagnosis, where diverse patterns in data lead
to multiple high-performing solutions. We propose the Rashomon Ensemble, a
method that strategically selects models from these diverse high-performing
solutions to improve generalization. By grouping models based on both their
performance and explanations, we construct ensembles that maximize diversity
while maintaining predictive accuracy. This selection ensures that each model
covers a distinct region of the solution space, making the ensemble more robust
to distribution shifts and variations in unseen data. We validate our approach
on both open and proprietary collaborative real-world datasets, demonstrating
up to 0.20+ AUROC improvements in scenarios where the Rashomon ratio is large.
Additionally, we demonstrate tangible benefits for businesses in various
real-world applications, highlighting the robustness, practicality, and
effectiveness of our approach.

</details>


### [166] [An entropy formula for the Deep Linear Network](https://arxiv.org/abs/2509.09088)
*Govind Menon,Tianmin Yu*

Main category: cs.LG

TL;DR: This paper investigates the Riemannian geometry of Deep Linear Networks as a basis for understanding the learning process using thermodynamics concepts.


<details>
  <summary>Details</summary>
Motivation: To provide a geometric and thermodynamic understanding of deep learning, particularly in overparametrized models.

Method: Uses group actions, Riemannian submersion, and the foliation of balanced manifolds to derive a Riemannian geometry and compute Boltzmann entropy.

Result: An orthonormal basis for the tangent space of the balanced manifold is constructed, connecting parameter space and observable space through geometry.

Conclusion: The use of Riemannian submersion effectively bridges the parameter space and observable space geometry, with implications for thermodynamic interpretations of learning processes.

Abstract: We study the Riemannian geometry of the Deep Linear Network (DLN) as a
foundation for a thermodynamic description of the learning process. The main
tools are the use of group actions to analyze overparametrization and the use
of Riemannian submersion from the space of parameters to the space of
observables. The foliation of the balanced manifold in the parameter space by
group orbits is used to define and compute a Boltzmann entropy. We also show
that the Riemannian geometry on the space of observables defined in [2] is
obtained by Riemannian submersion of the balanced manifold. The main technical
step is an explicit construction of an orthonormal basis for the tangent space
of the balanced manifold using the theory of Jacobi matrices.

</details>


### [167] [Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models](https://arxiv.org/abs/2509.09119)
*Hao Zhang,Bo Huang,Zhenjia Li,Xi Xiao,Hui Yi Leong,Zumeng Zhang,Xinwei Long,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: This paper introduces Sensitivity-LoRA, a technique for efficiently fine-tuning large language models (LLMs) by dynamically allocating ranks to weight matrices using sensitivity analysis.


<details>
  <summary>Details</summary>
Motivation: Adapting LLMs to specialized tasks is challenging due to computational constraints and the inefficiency of existing rank allocation techniques in fine-tuning methods like LoRA.

Method: Sensitivity-LoRA dynamically allocates ranks based on weight sensitivity derived from second-order derivatives, minimizing computational overhead and stabilizing adaptation.

Result: Experimental results showcase the robustness, efficiency, and stability of Sensitivity-LoRA across a variety of tasks and benchmarks.

Conclusion: Sensitivity-LoRA provides a significant advancement in parameter-efficient fine-tuning, addressing limitations of previous methods while maintaining practical efficiency and effectiveness.

Abstract: Large Language Models (LLMs) have transformed both everyday life and
scientific research. However, adapting LLMs from general-purpose models to
specialized tasks remains challenging, particularly in resource-constrained
environments. Low-Rank Adaptation (LoRA), a prominent method within
Parameter-Efficient Fine-Tuning (PEFT), has emerged as a promising approach to
LLMs by approximating model weight updates using low-rank decomposition.
However, LoRA is limited by its uniform rank ( r ) allocation to each
incremental matrix, and existing rank allocation techniques aimed at addressing
this issue remain computationally inefficient, complex, and unstable, hindering
practical applications. To address these limitations, we propose
Sensitivity-LoRA, an efficient fine-tuning method that dynamically allocates
ranks to weight matrices based on both their global and local sensitivities. It
leverages the second-order derivatives (Hessian Matrix) of the loss function to
effectively capture weight sensitivity, enabling optimal rank allocation with
minimal computational overhead. Our experimental results have demonstrated
robust effectiveness, efficiency and stability of Sensitivity-LoRA across
diverse tasks and benchmarks.

</details>


### [168] [Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction](https://arxiv.org/abs/2509.09128)
*Emam Hossain,Md Osman Gani*

Main category: cs.LG

TL;DR: The paper presents a deep learning framework using causal inference techniques to enhance the robustness and interpretability of predictions, applied to Arctic sea ice forecasting.


<details>
  <summary>Details</summary>
Motivation: Correlations in typical machine learning models struggle to distinguish real causative factors from spurious ones, compromising robustness, interpretability, and generalizability.

Method: The framework integrates MVGC and PCMCI+ for selecting causal features, employing a hybrid neural architecture to identify and prioritize direct causal influences using 43 years of Arctic sea ice data.

Result: The use of causal inputs improves prediction accuracy, interpretability, and reduces computational load across different forecasting lead times.

Conclusion: The causality-aware framework proves broadly applicable beyond Arctic forecasting, offering scalable improvements to predictive modeling in dynamic, high-dimensional domains.

Abstract: Conventional machine learning and deep learning models typically rely on
correlation-based learning, which often fails to distinguish genuine causal
relationships from spurious associations, limiting their robustness,
interpretability, and ability to generalize. To overcome these limitations, we
introduce a causality-aware deep learning framework that integrates
Multivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection
within a hybrid neural architecture. Leveraging 43 years (1979-2021) of Arctic
Sea Ice Extent (SIE) data and associated ocean-atmospheric variables at daily
and monthly resolutions, the proposed method identifies causally influential
predictors, prioritizes direct causes of SIE dynamics, reduces unnecessary
features, and enhances computational efficiency. Experimental results show that
incorporating causal inputs leads to improved prediction accuracy and
interpretability across varying lead times. While demonstrated on Arctic SIE
forecasting, the framework is broadly applicable to other dynamic,
high-dimensional domains, offering a scalable approach that advances both the
theoretical foundations and practical performance of causality-informed
predictive modeling.

</details>


### [169] [Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.09135)
*Xuefeng Wang,Lei Zhang,Henglin Pu,Ahmed H. Qureshi,Husheng Li*

Main category: cs.LG

TL;DR: The paper introduces CT-MARL, a continuous-time multi-agent RL framework using physics-informed neural networks to overcome challenges in high-dimensional systems and destabilized policy training.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of continuous-time RL applications in multi-agent scenarios, particularly the issues of dimensionality and policy instability.

Method: Introduce the CT-MARL framework, leveraging PINNs for scalable HJB-based value function approximation and a Value Gradient Iteration module for refining value gradients.

Result: CT-MARL outperforms existing continuous-time RL baselines in benchmarks like MPE and MuJoCo, showcasing scalability and improved policy learning.

Conclusion: CT-MARL effectively extends CTRL methods to multi-agent systems, offering improved scalability and accuracy in continuous-time dynamics.

Abstract: Existing reinforcement learning (RL) methods struggle with complex dynamical
systems that demand interactions at high frequencies or irregular time
intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by
replacing discrete-time Bellman recursion with differential value functions
defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation.
While CTRL has shown promise, its applications have been largely limited to the
single-agent domain. This limitation stems from two key challenges: (i)
conventional solution methods for HJB equations suffer from the curse of
dimensionality (CoD), making them intractable in high-dimensional systems; and
(ii) even with HJB-based learning approaches, accurately approximating
centralized value functions in multi-agent settings remains difficult, which in
turn destabilizes policy training. In this paper, we propose a CT-MARL
framework that uses physics-informed neural networks (PINNs) to approximate
HJB-based value functions at scale. To ensure the value is consistent with its
differential structure, we align value learning with value-gradient learning by
introducing a Value Gradient Iteration (VGI) module that iteratively refines
value gradients along trajectories. This improves gradient fidelity, in turn
yielding more accurate values and stronger policy learning. We evaluate our
method using continuous-time variants of standard benchmarks, including
multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results
demonstrate that our approach consistently outperforms existing continuous-time
RL baselines and scales to complex multi-agent dynamics.

</details>


### [170] [Peering Partner Recommendation for ISPs using Machine Learning](https://arxiv.org/abs/2509.09146)
*Md Ibrahim Ibne Alam,Ankur Senapati,Anindo Mahmood,Murat Yuksel,Koushik Kar*

Main category: cs.LG

TL;DR: The paper proposes machine learning models to automate peering partner selection using publicly available ISP data, achieving 98% accuracy with tree-based models.


<details>
  <summary>Details</summary>
Motivation: To enhance efficiency and optimize ISP operations by automating the complex and lengthy peering partner selection process.

Method: Public data from platforms like PeeringDB and CAIDA were utilized to train different machine learning models—tree-based, neural network-based, and transformer-based. Tree-based models were found most effective.

Result: The XGBoost model showed 98% accuracy in predicting peering partners and exhibited resiliency to variations in data.

Conclusion: ISPs can use the proposed automated approach to streamline the selection of peering partners, leading to a more efficient global Internet ecosystem.

Abstract: Internet service providers (ISPs) need to connect with other ISPs to provide
global connectivity services to their users. To ensure global connectivity,
ISPs can either use transit service(s) or establish direct peering
relationships between themselves via Internet exchange points (IXPs). Peering
offers more room for ISP-specific optimizations and is preferred, but it often
involves a lengthy and complex process. Automating peering partner selection
can enhance efficiency in the global Internet ecosystem. We explore the use of
publicly available data on ISPs to develop a machine learning (ML) model that
can predict whether an ISP pair should peer or not. At first, we explore public
databases, e.g., PeeringDB, CAIDA, etc., to gather data on ISPs. Then, we
evaluate the performance of three broad types of ML models for predicting
peering relationships: tree-based, neural network-based, and transformer-based.
Among these, we observe that tree-based models achieve the highest accuracy and
efficiency in our experiments. The XGBoost model trained with publicly
available data showed promising performance, with a 98% accuracy rate in
predicting peering partners. In addition, the model demonstrated great
resilience to variations in time, space, and missing data. We envision that
ISPs can adopt our method to fully automate the peering partner selection
process, thus transitioning to a more efficient and optimized Internet
ecosystem.

</details>


### [171] [HISPASpoof: A New Dataset For Spanish Speech Forensics](https://arxiv.org/abs/2509.09155)
*Maria Risques,Kratika Bhagtani,Amit Kumar Singh Yadav,Edward J. Delp*

Main category: cs.LG

TL;DR: This paper introduces HISPASpoof, a large-scale dataset for detecting and attributing synthetic Spanish speech, addressing the gap in speech forensics for non-English languages.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in detecting synthetic speech in English and Chinese, Spanish—a major global language—lacks sufficient forensic tools for synthetic speech detection.

Method: Created HISPASpoof, a dataset combining real Spanish speech (with different accents) and synthetic speech generated using six zero-shot TTS systems. Evaluated five detection methods, comparing performance for synthetic speech detection and attribution.

Result: General findings show English-trained detectors fail in Spanish application. However, training on HISPASpoof significantly enhances synthetic speech detection and generation method attribution in Spanish.

Conclusion: HISPASpoof bridges a critical gap in speech forensics, ensuring inclusivity for Spanish and advancing the reliability of detection and attribution systems for synthetic speech.

Abstract: Zero-shot Voice Cloning (VC) and Text-to-Speech (TTS) methods have advanced
rapidly, enabling the generation of highly realistic synthetic speech and
raising serious concerns about their misuse. While numerous detectors have been
developed for English and Chinese, Spanish-spoken by over 600 million people
worldwide-remains underrepresented in speech forensics. To address this gap, we
introduce HISPASpoof, the first large-scale Spanish dataset designed for
synthetic speech detection and attribution. It includes real speech from public
corpora across six accents and synthetic speech generated with six zero-shot
TTS systems. We evaluate five representative methods, showing that detectors
trained on English fail to generalize to Spanish, while training on HISPASpoof
substantially improves detection. We also evaluate synthetic speech attribution
performance on HISPASpoof, i.e., identifying the generation method of synthetic
speech. HISPASpoof thus provides a critical benchmark for advancing reliable
and inclusive speech forensics in Spanish.

</details>


### [172] [Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication](https://arxiv.org/abs/2509.09168)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis*

Main category: cs.LG

TL;DR: This paper introduces a training-free framework for adaptive token merging in pretrained vision transformers to enhance efficiency and performance in resource-constrained 6G networks.


<details>
  <summary>Details</summary>
Motivation: There is a need to overcome the computational barriers of deploying large-scale transformer models in constrained 6G networks for semantic communication systems.

Method: A multi-objective optimization problem is formulated using Bayesian optimization to adaptively reduce token merging, optimizing for both accuracy and computational cost.

Result: The method reduces floating-point operations while maintaining competitive accuracy even across varying SNR conditions, outperforming other baseline methods.

Conclusion: An efficient and scalable approach is provided for deploying transformer-based semantic communication systems, allowing adaptive merging in response to channel conditions, ensuring practical and dynamic applications in edge intelligence.

Abstract: Large-scale transformer models have emerged as a powerful tool for semantic
communication systems, enabling edge devices to extract rich representations
for robust inference across noisy wireless channels. However, their substantial
computational demands remain a major barrier to practical deployment in
resource-constrained 6G networks. In this paper, we present a training-free
framework for adaptive token merging in pretrained vision transformers to
jointly reduce inference time and transmission resource usage. We formulate the
selection of per-layer merging proportions as a multi-objective optimization
problem to balance accuracy and computational cost. We employ Gaussian
process-based Bayesian optimization to construct a Pareto frontier of optimal
configurations, enabling flexible runtime adaptation to dynamic application
requirements and channel conditions. Extensive experiments demonstrate that our
method consistently outperforms other baselines and achieves significant
reductions in floating-point operations while maintaining competitive accuracy
across a wide range of signal-to-noise ratio (SNR) conditions. Additional
results highlight the effectiveness of adaptive policies that adjust merging
aggressiveness in response to channel quality, providing a practical mechanism
to trade off latency and semantic fidelity on demand. These findings establish
a scalable and efficient approach for deploying transformer-based semantic
communication in future edge intelligence systems.

</details>


### [173] [Quantum Machine Learning, Quantitative Trading, Reinforcement Learning, Deep Learning](https://arxiv.org/abs/2509.09176)
*Jun-Hao Chen,Yu-Chien Huang,Yun-Cheng Tsai,Samuel Yen-Chi Chen*

Main category: cs.LG

TL;DR: This paper explores the fusion of quantum-inspired neural networks and deep reinforcement learning for financial trading, delivering improved performance through quantum-enhanced methods.


<details>
  <summary>Details</summary>
Motivation: To enhance financial trading efficiency using quantum-inspired neural networks and deep reinforcement learning approaches specifically targeting the USD/TWD currency pair.

Method: They implemented Quantum Long Short-Term Memory (QLSTM) for short-term trend prediction and Quantum Asynchronous Advantage Actor-Critic (QA3C) for reinforcement learning, training data spanning over 25 years.

Result: The trading agent achieved an 11.87% return over about 5 years, with only a 0.92% maximum drawdown, outperforming several currency ETFs.

Conclusion: Hybrid models using quantum-inspired techniques demonstrate competitive performance in FX trading, with QLSTM effective for small-profit trades under tight risk controls.

Abstract: The convergence of quantum-inspired neural networks and deep reinforcement
learning offers a promising avenue for financial trading. We implemented a
trading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM)
for short-term trend prediction with Quantum Asynchronous Advantage
Actor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained
on data from 2000-01-01 to 2025-04-30 (80\% training, 20\% testing), the
long-only agent achieves 11.87\% return over around 5 years with 0.92\% max
drawdown, outperforming several currency ETFs. We detail state design (QLSTM
features and indicators), reward function for trend-following/risk control, and
multi-core training. Results show hybrid models yield competitive FX trading
performance. Implications include QLSTM's effectiveness for small-profit trades
with tight risk and future enhancements. Key hyperparameters: QLSTM sequence
length$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and
simplified strategy. \footnote{The views expressed in this article are those of
the authors and do not represent the views of Wells Fargo. This article is for
informational purposes only. Nothing contained in this article should be
construed as investment advice. Wells Fargo makes no express or implied
warranties and expressly disclaims all legal, tax, and accounting implications
related to this article.

</details>


### [174] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: FSPO is a sequence-level reinforcement learning method for training large language models, addressing fairness in response lengths by using a novel clipping method based on importance-sampling weights.


<details>
  <summary>Details</summary>
Motivation: To address the mismatch in sequence-level reinforcement learning methods where traditional clipping approaches favor short or long responses unevenly during training of large language models.

Method: FSPO revisits sequence-level RL techniques and introduces a length-fair clipping mechanism using a Gaussian-inspired adjustment to the importance-sampling weight space, adjusting log-IS ratios with a KL-corrected drift that scales proportionally to sequence length.

Result: FSPO demonstrates stabilized training, equalized clip rates across varying response lengths, and superior performance compared with baseline methods across diverse evaluation datasets.

Conclusion: Correcting length-based bias in RL methods for LLM training using FSPO contributes to fairer and more stable optimization processes, producing better results overall.

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [175] [Breaking the Statistical Similarity Trap in Extreme Convection Detection](https://arxiv.org/abs/2509.09195)
*Md Tanveer Hossain Munim*

Main category: cs.LG

TL;DR: Current metrics overvalue statistical similarity, leading to missing rare weather events. DART introduces methods for high-resolution extreme event prediction.


<details>
  <summary>Details</summary>
Motivation: Address deficiencies in weather prediction metrics that prioritize statistical similarity and overlook high-impact rare weather events.

Method: Propose a Dual Architecture (DART) using dual-decoder design, extreme event-decomposition, specialized loss functions, and physically motivated oversampling.

Result: Demonstrates improved prediction accuracy for rare weather events, including better CSI scores and operational flexibility. Real-world validation included.

Conclusion: DART addresses critical gaps in extreme weather detection, enhances accuracy compared to baselines, and supports streamlined integration into meteorological workflows.

Abstract: Current evaluation metrics for deep learning weather models create a
"Statistical Similarity Trap", rewarding blurry predictions while missing rare,
high-impact events. We provide quantitative evidence of this trap, showing
sophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous
convection detection. We introduce DART (Dual Architecture for Regression
Tasks), a framework addressing the challenge of transforming coarse atmospheric
forecasts into high-resolution satellite brightness temperature fields
optimized for extreme convection detection (below 220 K). DART employs
dual-decoder architecture with explicit background/extreme decomposition,
physically motivated oversampling, and task-specific loss functions. We present
four key findings: (1) empirical validation of the Statistical Similarity Trap
across multiple sophisticated baselines; (2) the "IVT Paradox", removing
Integrated Water Vapor Transport, widely regarded as essential for atmospheric
river analysis, improves extreme convection detection by 270%; (3)
architectural necessity demonstrated through operational flexibility (DART
achieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent
CSI), and (4) real-world validation with the August 2023 Chittagong flooding
disaster as a case study. To our knowledge, this is the first work to
systematically address this hybrid conversion-segmentation-downscaling task,
with no direct prior benchmarks identified in existing literature. Our
validation against diverse statistical and deep learning baselines sufficiently
demonstrates DART's specialized design. The framework enables precise
operational calibration through beta-tuning, trains in under 10 minutes on
standard hardware, and integrates seamlessly with existing meteorological
workflows, demonstrating a pathway toward trustworthy AI for extreme weather
preparedness.

</details>


### [176] [Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning](https://arxiv.org/abs/2509.09208)
*Somnath Hazra,Pallab Dasgupta,Soumyajit Dey*

Main category: cs.LG

TL;DR: The paper presents IP3O, a new reinforcement learning algorithm designed to maintain safety constraints while optimizing performance.


<details>
  <summary>Details</summary>
Motivation: Balancing reward maximization with constraint satisfaction in constrained reinforcement learning is challenging due to instability near constraint boundaries.

Method: The authors proposed the IP3O algorithm, which uses an adaptive incentive mechanism and incremental penalties to stabilize training near constraints.

Result: Empirical evaluations show that IP3O outperforms state-of-the-art Safe RL algorithms on benchmark tests.

Conclusion: IP3O effectively handles trade-offs between performance and safety constraints, supported by strong theoretical guarantees.

Abstract: Constrained Reinforcement Learning (RL) aims to maximize the return while
adhering to predefined constraint limits, which represent domain-specific
safety requirements. In continuous control settings, where learning agents
govern system actions, balancing the trade-off between reward maximization and
constraint satisfaction remains a significant challenge. Policy optimization
methods often exhibit instability near constraint boundaries, resulting in
suboptimal training performance. To address this issue, we introduce a novel
approach that integrates an adaptive incentive mechanism in addition to the
reward structure to stay within the constraint bound before approaching the
constraint boundary. Building on this insight, we propose Incrementally
Penalized Proximal Policy Optimization (IP3O), a practical algorithm that
enforces a progressively increasing penalty to stabilize training dynamics.
Through empirical evaluation on benchmark environments, we demonstrate the
efficacy of IP3O compared to the performance of state-of-the-art Safe RL
algorithms. Furthermore, we provide theoretical guarantees by deriving a bound
on the worst-case error of the optimality achieved by our algorithm.

</details>


### [177] [Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach](https://arxiv.org/abs/2509.09214)
*Alka Gadakh,Vidya Kumbhar,Sonal Khosla,Kumar Karunendra*

Main category: cs.LG

TL;DR: The study explores agro-tourism as a tool for rural development, using ML methods to identify growth indicators, with LR achieving high accuracy in predictions.


<details>
  <summary>Details</summary>
Motivation: Agro-tourism is a booming area in tourism that supports rural economies and cultural preservation. The study aims to identify strategies for its growth.

Method: The study involved two phases: a literature review to determine agro-tourism growth indicators and the application of machine learning techniques such as LASSO combined with classifiers like LR, DT, RF, and XGBoost for feature selection.

Result: LASSO combined with LR achieved the highest classification accuracy of 98% (70-30 split) and 99% (80-20 split), outperforming RF, DT, and XGBoost.

Conclusion: Machine learning models are effective in identifying key indicators for agro-tourism growth. LR emerged as the most accurate model, highlighting its suitability for predicting successful strategies.

Abstract: Agro-tourism serves as a strategic economic model designed to facilitate
rural development by diversifying income streams for local communities like
farmers while promoting the conservation of indigenous cultural heritage and
traditional agricultural practices. As a very booming subdomain of tourism,
there is a need to study the strategies for the growth of Agro-tourism in
detail. The current study has identified the important indicators for the
growth and enhancement of agro-tourism. The study is conducted in two phases:
identification of the important indicators through a comprehensive literature
review and in the second phase state-of-the-art techniques were used to
identify the important indicators for the growth of agro-tourism. The
indicators are also called features synonymously, the machine learning models
for feature selection were applied and it was observed that the Least Absolute
Shrinkage and Selection Operator (LASSO) method combined with, the machine
Learning Classifiers such as Logistic Regression (LR), Decision Trees (DT),
Random Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were
used to suggest the growth of the agro-tourism. The results show that with the
LASSO method, LR model gives the highest classification accuracy of 98% in
70-30% train-test data followed by RF with 95% accuracy. Similarly, in the
80-20% train-test data LR maintains the highest accuracy at 99%, while DT and
XGBoost follow with 97% accuracy.

</details>


### [178] [Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement](https://arxiv.org/abs/2509.09219)
*Jakob Nyberg,Pontus Johnson*

Main category: cs.LG

TL;DR: Vejde framework combines data abstraction, graph neural networks, and reinforcement learning to create policies for decision problems with complex states.


<details>
  <summary>Details</summary>
Motivation: To address decision problems involving structured states, like object classes and relations, which are challenging for standard methods.

Method: Vejde uses database facts as MDP states, representing them as bipartite graphs, and applies message passing in graph neural networks. It trains policies using supervised and reinforcement learning.

Result: Testing on eight domains showed Vejde generalizes well to unseen instances, scoring almost on par with MLP agents and Prost planning algorithm.

Conclusion: Vejde demonstrates effective generalization and comparable performance, offering a flexible solution for problems with variable sizes and structures.

Abstract: We present and evaluate Vejde; a framework which combines data abstraction,
graph neural networks and reinforcement learning to produce inductive policy
functions for decision problems with richly structured states, such as object
classes and relations. MDP states are represented as data bases of facts about
entities, and Vejde converts each state to a bipartite graph, which is mapped
to latent states through neural message passing. The factored representation of
both states and actions allows Vejde agents to handle problems of varying size
and structure. We tested Vejde agents on eight problem domains defined in RDDL,
with ten problem instances each, where policies were trained using both
supervised and reinforcement learning. To test policy generalization, we
separate problem instances in two sets, one for training and the other solely
for testing. Test results on unseen instances for the Vejde agents were
compared to MLP agents trained on each problem instance, as well as the online
planning algorithm Prost. Our results show that Vejde policies in average
generalize to the test instances without a significant loss in score.
Additionally, the inductive agents received scores on unseen test instances
that on average were close to the instance-specific MLP agents.

</details>


### [179] [Constructing a Question-Answering Simulator through the Distillation of LLMs](https://arxiv.org/abs/2509.09226)
*Haipeng Liu,Ting Long,Jing Fu*

Main category: cs.LG

TL;DR: This paper introduces LDSim, a QA simulator leveraging knowledge distillation from large language models (LLMs) for effective performance in predicting students' responses.


<details>
  <summary>Details</summary>
Motivation: To improve educational recommendation systems by enabling accurate predictions of student responses while addressing the limitations of LLM-free and LLM-based methods.

Method: Develop LDSim that distills domain knowledge and reasoning capabilities from LLMs, combining efficiency and performance for student response prediction.

Result: Experiments show LDSim outperforms existing methods in simulation tasks and knowledge tracing tasks.

Conclusion: LDSim effectively balances speed, memory efficiency, and performance, making it a strong approach for enhancing student learning prediction models.

Abstract: The question-answering (QA) simulator is a model that mimics real student
learning behaviors and predicts their correctness of their responses to
questions. QA simulators enable educational recommender systems (ERS) to
collect large amounts of training data without interacting with real students,
thereby preventing harmful recommendations made by an undertrained ERS from
undermining actual student learning. Given the QA history, there are two
categories of solutions to predict the correctness, conducting the simulation:
(1) LLM-free methods, which apply a traditional sequential model to transfer
the QA history into a vector representation first, and make predictions based
on the representation; (2) LLM-based methods, which leverage the domain
knowledge and reasoning capability of LLM to enhence the prediction. LLM-free
methods offer fast inference but generally yield suboptimal performance. In
contrast, most LLM-based methods achieve better results, but at the cost of
slower inference speed and higher GPU memory consumption. In this paper, we
propose a method named LLM Distillation based Simulator (LDSim), which distills
domain knowledge and reasoning capability from an LLM to better assist
prediction, thereby improving simulation performance. Extensive experiments
demonstrate that our LDSim achieves strong results on both the simulation task
and the knowledge tracing (KT) task. Our code is publicly available at
https://anonymous.4open.science/r/LDSim-05A9.

</details>


### [180] [Unsupervised Multi-Attention Meta Transformer for Rotating Machinery Fault Diagnosis](https://arxiv.org/abs/2509.09251)
*Hanyang Wang,Yuxuan Yang,Hongjun Wang,Lihui Wang*

Main category: cs.LG

TL;DR: The paper proposes a Multi-Attention Meta Transformer (MMT-FD) for diagnosing rotating machinery faults using minimal labeled data, achieving 99% accuracy with only 1% labeled samples.


<details>
  <summary>Details</summary>
Motivation: Fault diagnosis of rotating mechanical equipment requires extensive labeled data, which is hard and costly to obtain. Existing models lack generalizability for diverse equipment types.

Method: MMT-FD integrates a time-frequency domain encoder and a meta-learning model. It utilizes data augmentation, contrastive learning, and fine-tuning with limited labeled data for fault classification.

Result: Experiments on bearing fault and rotor test bench datasets show the model achieves 99% fault diagnosis accuracy with only 1% labeled data.

Conclusion: MMT-FD exhibits strong fault representation extraction and generalization capabilities, making it highly efficient and applicable across diverse types of rotating machinery.

Abstract: The intelligent fault diagnosis of rotating mechanical equipment usually
requires a large amount of labeled sample data. However, in practical
industrial applications, acquiring enough data is both challenging and
expensive in terms of time and cost. Moreover, different types of rotating
mechanical equipment with different unique mechanical properties, require
separate training of diagnostic models for each case. To address the challenges
of limited fault samples and the lack of generalizability in prediction models
for practical engineering applications, we propose a Multi-Attention Meta
Transformer method for few-shot unsupervised rotating machinery fault diagnosis
(MMT-FD). This framework extracts potential fault representations from
unlabeled data and demonstrates strong generalization capabilities, making it
suitable for diagnosing faults across various types of mechanical equipment.
The MMT-FD framework integrates a time-frequency domain encoder and a
meta-learning generalization model. The time-frequency domain encoder predicts
status representations generated through random augmentations in the
time-frequency domain. These enhanced data are then fed into a meta-learning
network for classification and generalization training, followed by fine-tuning
using a limited amount of labeled data. The model is iteratively optimized
using a small number of contrastive learning iterations, resulting in high
efficiency. To validate the framework, we conducted experiments on a bearing
fault dataset and rotor test bench data. The results demonstrate that the
MMT-FD model achieves 99\% fault diagnosis accuracy with only 1\% of labeled
sample data, exhibiting robust generalization capabilities.

</details>


### [181] [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265)
*Jiawei Wang,Jiacai Liu,Yuqian Fu,Yingru Li,Xintao Wang,Yuan Lin,Yu Yue,Lin Zhang,Yang Wang,Ke Wang*

Main category: cs.LG

TL;DR: Sparse rewards in long-horizon tasks challenge LLM agents, prompting the introduction of Entropy-Modulated Policy Gradients (EMPG) to improve learning dynamics and achieve significant gains.


<details>
  <summary>Details</summary>
Motivation: LLMs face difficulties in assigning credit in tasks with sparse, outcome-based rewards, impeding efficient learning and solution exploration.

Method: EMPG adjusts policy gradient magnitudes based on step-wise uncertainty and task outcomes, amplifying confident correct actions, penalizing errors, and attenuating uncertain updates.

Result: EMPG shows substantial performance improvements, outperforming strong policy gradient baselines in tasks like WebShop, ALFWorld, and Deep Search.

Conclusion: EMPG effectively addresses learning inefficiencies in LLMs for sparse-reward tasks, demonstrating its usefulness for stable exploration and improved performance.

Abstract: In long-horizon tasks, recent agents based on Large Language Models (LLMs)
face a significant challenge that sparse, outcome-based rewards make it
difficult to assign credit to intermediate steps. Previous methods mainly focus
on creating dense reward signals to guide learning, either through traditional
reinforcement learning techniques like inverse reinforcement learning or by
using Process Reward Models for step-by-step feedback. In this paper, we
identify a fundamental problem in the learning dynamics of LLMs: the magnitude
of policy gradients is inherently coupled with the entropy, which leads to
inefficient small updates for confident correct actions and potentially
destabilizes large updates for uncertain ones. To resolve this, we propose
Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the
learning signal based on step-wise uncertainty and the final task outcome. EMPG
amplifies updates for confident correct actions, penalizes confident errors,
and attenuates updates from uncertain steps to stabilize exploration. We
further introduce a bonus term for future clarity that encourages agents to
find more predictable solution paths. Through comprehensive experiments on
three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we
demonstrate that EMPG achieves substantial performance gains and significantly
outperforms strong policy gradient baselines. Project page is at
https://empgseed-seed.github.io/

</details>


### [182] [Data Driven Discovery of Emergent Dynamics in Reaction Diffusion Systems from Sparse and Noisy Observations](https://arxiv.org/abs/2509.09278)
*Saumitra Dwivedi,Ricardo da Silva Torres,Ibrahim A. Hameed,Gunnar Tufte,Anniken Susanne T. Karlsen*

Main category: cs.LG

TL;DR: This paper addresses the challenges of discovering emergent dynamics in reaction-diffusion systems by developing a data-driven method using Soft Artificial Life models, achieving good prediction accuracy even under noisy and sparse conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of identifying reaction-diffusion systems when the underlying physics is unknown, which is crucial for various fields such as neuroscience, ecology, and epidemiology.

Method: The authors propose a conceptual framework, DRSALife, to learn Soft Artificial Life rulesets for reaction-diffusion systems. They conduct experiments using noisy and sparse data to evaluate performance.

Result: Their learned models achieve a prediction accuracy of 74% for emergent dynamics and exhibit robustness against Gaussian noise and temporal sparsity.

Conclusion: This study successfully demonstrates the potential of machine-based Soft ALife ruleset learning for reaction-diffusion dynamics, advancing the field of data-driven system identification without prior physics knowledge.

Abstract: Data-driven discovery of emergent dynamics is gaining popularity,
particularly in the context of reaction-diffusion systems. These systems are
widely studied across various fields, including neuroscience, ecology,
epidemiology, and several other subject areas that deal with emergent dynamics.
A current challenge in the discovery process relates to system identification
when there is no prior knowledge of the underlying physics. We attempt to
address this challenge by learning Soft Artificial Life (Soft ALife) models,
such as Agent-based and Cellular Automata (CA) models, from observed data for
reaction-diffusion systems. In this paper, we present findings on the
applicability of a conceptual framework, the Data-driven Rulesets for Soft
Artificial Life (DRSALife) model, to learn Soft ALife rulesets that accurately
represent emergent dynamics in a reaction-diffusion system from observed data.
This model has demonstrated promising results for Elementary CA Rule 30, Game
of Life, and Vicsek Flocking problems in recent work. To our knowledge, this is
one of the few studies that explore machine-based Soft ALife ruleset learning
and system identification for reaction-diffusion dynamics without any prior
knowledge of the underlying physics. Moreover, we provide comprehensive
findings from experiments investigating the potential effects of using noisy
and sparse observed datasets on learning emergent dynamics. Additionally, we
successfully identify the structure and parameters of the underlying partial
differential equations (PDEs) representing these dynamics. Experimental results
demonstrate that the learned models are able to predict the emergent dynamics
with good accuracy (74%) and exhibit quite robust performance when subjected to
Gaussian noise and temporal sparsity.

</details>


### [183] [MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph Experts](https://arxiv.org/abs/2509.09337)
*Junda Ye,Zhongbao Zhang,Li Sun,Siqiang Luo*

Main category: cs.LG

TL;DR: This paper introduces a novel framework called Mixture of Subgraph Experts (MoSE) to improve the flexibility and expressiveness of subgraph-based representation learning in graph neural networks (GNNs).


<details>
  <summary>Details</summary>
Motivation: Existing graph neural networks (GNNs) rely on local message-passing schemes, which cannot fully capture high-order subgraph patterns due to insufficient structural expressiveness. Even recent methods using random walk kernels face limitations in flexibility and applicability to various graph tasks.

Method: The authors propose the MoSE framework, which employs anonymous walks to extract subgraphs and dynamically routes them to specialized experts based on structural semantics. The framework is designed to capture diverse subgraph patterns with improved adaptability and interpretability.

Result: MoSE demonstrates superior performance compared to competitive baselines in experiments. The model also provides interpretable visualizations that show how subgraph experts learn structural patterns. Theoretical analysis reveals that MoSE is more powerful than the Subgraph Weisfeiler-Lehman (SWL) test in terms of structural expressivity.

Conclusion: The study concludes that MoSE extends the capabilities of GNNs by enhancing their structural expressiveness, flexibility, and interpretability, making it suitable for diverse tasks like node classification and other graph-level problems.

Abstract: While graph neural networks (GNNs) have achieved great success in learning
from graph-structured data, their reliance on local, pairwise message passing
restricts their ability to capture complex, high-order subgraph patterns.
leading to insufficient structural expressiveness. Recent efforts have
attempted to enhance structural expressiveness by integrating random walk
kernels into GNNs. However, these methods are inherently designed for
graph-level tasks, which limits their applicability to other downstream tasks
such as node classification. Moreover, their fixed kernel configurations hinder
the model's flexibility in capturing diverse subgraph structures. To address
these limitations, this paper proposes a novel Mixture of Subgraph Experts
(MoSE) framework for flexible and expressive subgraph-based representation
learning across diverse graph tasks. Specifically, MoSE extracts informative
subgraphs via anonymous walks and dynamically routes them to specialized
experts based on structural semantics, enabling the model to capture diverse
subgraph patterns with improved flexibility and interpretability. We further
provide a theoretical analysis of MoSE's expressivity within the Subgraph
Weisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL.
Extensive experiments, together with visualizations of learned subgraph
experts, demonstrate that MoSE not only outperforms competitive baselines but
also provides interpretable insights into structural patterns learned by the
model.

</details>


### [184] [Robust Non-Linear Correlations via Polynomial Regression](https://arxiv.org/abs/2509.09380)
*Luca Giuliani,Michele Lombardi*

Main category: cs.LG

TL;DR: The paper introduces a novel, robust approach for computing the Hirschfeld-Gebelein-Rényi (HGR) correlation coefficient using polynomial kernels, addressing the limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: There is a need for a reliable method to compute the HGR correlation for broader applications like fairness in algorithms and causal discovery without compromising robustness and real-world applicability.

Method: A new computational approach based on user-configurable polynomial kernels for HGR correlation calculation, improving robustness and speed while maintaining effectiveness.

Result: The approach demonstrated robustness, determinism, and superior applicability in constrained machine learning, providing an effective subgradient for loss regularization.

Conclusion: The proposed method offers a reliable and advantageous alternative to existing HGR computation techniques, suitable for practical, real-world deployment.

Abstract: The Hirschfeld-Gebelein-R\'enyi (HGR) correlation coefficient is an extension
of Pearson's correlation that is not limited to linear correlations, with
potential applications in algorithmic fairness, scientific analysis, and causal
discovery. Recently, novel algorithms to estimate HGR in a differentiable
manner have been proposed to facilitate its use as a loss regularizer in
constrained machine learning applications. However, the inherent
uncomputability of HGR requires a bias-variance trade-off, which can possibly
compromise the robustness of the proposed methods, hence raising technical
concerns if applied in real-world scenarios. We introduce a novel computational
approach for HGR that relies on user-configurable polynomial kernels, offering
greater robustness compared to previous methods and featuring a faster yet
almost equally effective restriction. Our approach provides significant
advantages in terms of robustness and determinism, making it a more reliable
option for real-world applications. Moreover, we present a brief experimental
analysis to validate the applicability of our approach within a constrained
machine learning framework, showing that its computation yields an insightful
subgradient that can serve as a loss regularizer.

</details>


### [185] [MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization](https://arxiv.org/abs/2509.09387)
*Mohammed Tiouti,Mohamed Bal-Ghaoui*

Main category: cs.LG

TL;DR: MetaLLMiX presents a zero-shot hyperparameter optimization framework utilizing meta-learning, explainable AI, and efficient LLM reasoning to tackle challenges in deep learning hyperparameter selection, achieving significant cost and time savings while maintaining competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep learning model and hyperparameter selection are notoriously resource-intensive and often require substantial expertise, which limits accessibility and efficiency. Current automated approaches, like AutoML and LLM-based methods, face a lack of interpretability and generalizability.

Method: The paper introduces MetaLLMiX, which leverages meta-learning and SHAP-based explanations of historical experimental data for hyperparameter recommendation. It integrates local LLM-based reasoning and introduces an LLM-as-judge evaluation to ensure quality control in format, accuracy, and completeness.

Result: MetaLLMiX achieves comparable or better performance than traditional hyperparameter optimization methods, with drastic reductions in computational cost and response times (99.6%-99.9%). It provides fast training speeds (2.4-15.7x faster on specific datasets) and accuracy within 1-5% of the best-performing baselines.

Conclusion: MetaLLMiX effectively reduces resource consumption and training time while delivering competitive or superior performance in hyperparameter optimization for deep learning applications. It outperforms prior API-based approaches in multiple tasks with local deployment benefits.

Abstract: Effective model and hyperparameter selection remains a major challenge in
deep learning, often requiring extensive expertise and computation. While
AutoML and large language models (LLMs) promise automation, current LLM-based
approaches rely on trial and error and expensive APIs, which provide limited
interpretability and generalizability. We propose MetaLLMiX, a zero-shot
hyperparameter optimization framework combining meta-learning, explainable AI,
and efficient LLM reasoning. By leveraging historical experiment outcomes with
SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained
models without additional trials. We further employ an LLM-as-judge evaluation
to control output format, accuracy, and completeness. Experiments on eight
medical imaging datasets using nine open-source lightweight LLMs show that
MetaLLMiX achieves competitive or superior performance to traditional HPO
methods while drastically reducing computational cost. Our local deployment
outperforms prior API-based approaches, achieving optimal results on 5 of 8
tasks, response time reductions of 99.6-99.9%, and the fastest training times
on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of
best-performing baselines.

</details>


### [186] [LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations](https://arxiv.org/abs/2509.09396)
*Harry Mayne,Ryan Othniel Kearns,Yushi Yang,Andrew M. Bean,Eoin Delaney,Chris Russell,Adam Mahdi*

Main category: cs.LG

TL;DR: This paper investigates how effectively language models can produce self-generated counterfactual explanations (SCEs) to explain their decisions, revealing limitations in their validity and minimality when generating such counterfactuals.


<details>
  <summary>Details</summary>
Motivation: The study aims to evaluate language models' potential to provide self-explanations through counterfactual reasoning, crucial for transparent decision-making in human-AI collaboration, especially in high-stakes scenarios.

Method: The authors analyze the ability of large language models (LLMs) to generate SCEs that balance two properties: validity (achieving the intended change in prediction) and minimality (making the smallest possible modification). They test across multiple models, datasets, and settings.

Result: LLMs can generate valid SCEs but fail to meet minimality requirements. When aiming for minimality, the generated counterfactuals often do not change predictions. This validity-minimality trade-off highlights significant shortcomings in their explainability.

Conclusion: SCEs do not offer reliable insight into model behavior and could be misleading. This raises concerns about using LLMs in critical applications, emphasizing the importance of addressing explainability pitfalls.

Abstract: To collaborate effectively with humans, language models must be able to
explain their decisions in natural language. We study a specific type of
self-explanation: self-generated counterfactual explanations (SCEs), where a
model explains its prediction by modifying the input such that it would have
predicted a different outcome. We evaluate whether LLMs can produce SCEs that
are valid, achieving the intended outcome, and minimal, modifying the input no
more than necessary. When asked to generate counterfactuals, we find that LLMs
typically produce SCEs that are valid, but far from minimal, offering little
insight into their decision-making behaviour. Worryingly, when asked to
generate minimal counterfactuals, LLMs typically make excessively small edits
that fail to change predictions. The observed validity-minimality trade-off is
consistent across several LLMs, datasets, and evaluation settings. Our findings
suggest that SCEs are, at best, an ineffective explainability tool and, at
worst, can provide misleading insights into model behaviour. Proposals to
deploy LLMs in high-stakes settings must consider the impact of unreliable
self-explanations on downstream decision-making. Our code is available at
https://github.com/HarryMayne/SCEs.

</details>


### [187] [Kriging prior Regression: A Case for Kriging-Based Spatial Features with TabPFN in Soil Mapping](https://arxiv.org/abs/2509.09408)
*Jonas Schmidinger,Viacheslav Barkov,Sebastian Vogel,Martin Atzmueller,Gerard B M Heuvelink*

Main category: cs.LG

TL;DR: The paper proposes a hybrid framework, 'kriging prior regression' (KpR), combining geostatistics and machine learning for improved spatial prediction of soil properties.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of geostatistics and machine learning in spatially mapping soil properties, particularly the need for accurate predictions and uncertainty estimates.

Method: A hybrid approach was developed that incorporates 'spatial lag' features derived from ordinary kriging into machine learning models, specifically using the TabPFN algorithm.

Result: KpR with TabPFN achieved a 30% average R2 improvement over non-spatial machine learning algorithms and demonstrated reliable uncertainty estimates across six soil datasets.

Conclusion: KpR with TabPFN is a robust and versatile framework for digital soil mapping, particularly effective in settings with small datasets or weak sensing feature relationships, making it valuable for precision agriculture.

Abstract: Machine learning and geostatistics are two fundamentally different frameworks
for predicting and spatially mapping soil properties. Geostatistics leverages
the spatial structure of soil properties, while machine learning captures the
relationship between available environmental features and soil properties. We
propose a hybrid framework that enriches ML with spatial context through
engineering of 'spatial lag' features from ordinary kriging. We call this
approach 'kriging prior regression' (KpR), as it follows the inverse logic of
regression kriging. To evaluate this approach, we assessed both the point and
probabilistic prediction performance of KpR, using the TabPFN model across six
fieldscale datasets from LimeSoDa. These datasets included soil organic carbon,
clay content, and pH, along with features derived from remote sensing and
in-situ proximal soil sensing. KpR with TabPFN demonstrated reliable
uncertainty estimates and more accurate predictions in comparison to several
other spatial techniques (e.g., regression/residual kriging with TabPFN), as
well as to established non-spatial machine learning algorithms (e.g., random
forest). Most notably, it significantly improved the average R2 by around 30%
compared to machine learning algorithms without spatial context. This
improvement was due to the strong prediction performance of the TabPFN
algorithm itself and the complementary spatial information provided by KpR
features. TabPFN is particularly effective for prediction tasks with small
sample sizes, common in precision agriculture, whereas KpR can compensate for
weak relationships between sensing features and soil properties when proximal
soil sensing data are limited. Hence, we conclude that KpR with TabPFN is a
very robust and versatile modelling framework for digital soil mapping in
precision agriculture.

</details>


### [188] [Fused Lasso Improves Accuracy of Co-occurrence Network Inference in Grouped Samples](https://arxiv.org/abs/2509.09413)
*Daniel Agyapong,Briana H. Beatty,Peter G. Kennedy,Toby D. Hocking*

Main category: cs.LG

TL;DR: The paper introduces 'fuser,' an algorithm for microbiome community network inference that considers spatial and temporal dynamics, outperforming existing methods in cross-environment scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of microbiome network algorithms, which often ignore the dynamics of microbial communities adapting to varied ecological conditions and rely on static or single-environment analyses.

Method: The study introduces the Same-All Cross-validation (SAC) framework for evaluating algorithm performance and proposes 'fuser,' an algorithm that preserves environment-specific signals while integrating shared information across niches.

Result: The proposed 'fuser' algorithm shows comparable performance to existing methods in single-environment evaluations (Same) and significantly better performance in cross-environment analyses (All).

Conclusion: 'Fuser' offers an effective and innovative approach for analyzing microbiome networks under varying ecological conditions by generating environment-specific predictive networks, addressing limitations in current methods and improving predictive accuracy.

Abstract: Co-occurrence network inference algorithms have significantly advanced our
understanding of microbiome communities. However, these algorithms typically
analyze microbial associations within samples collected from a single
environmental niche, often capturing only static snapshots rather than dynamic
microbial processes. Previous studies have commonly grouped samples from
different environmental niches together without fully considering how microbial
communities adapt their associations when faced with varying ecological
conditions. Our study addresses this limitation by explicitly investigating
both spatial and temporal dynamics of microbial communities. We analyzed
publicly available microbiome abundance data across multiple locations and time
points, to evaluate algorithm performance in predicting microbial associations
using our proposed Same-All Cross-validation (SAC) framework. SAC evaluates
algorithms in two distinct scenarios: training and testing within the same
environmental niche (Same), and training and testing on combined data from
multiple environmental niches (All). To overcome the limitations of
conventional algorithms, we propose fuser, an algorithm that, while not
entirely new in machine learning, is novel for microbiome community network
inference. It retains subsample-specific signals while simultaneously sharing
relevant information across environments during training. Unlike standard
approaches that infer a single generalized network from combined data, fuser
generates distinct, environment-specific predictive networks. Our results
demonstrate that fuser achieves comparable predictive performance to existing
algorithms such as glmnet when evaluated within homogeneous environments
(Same), and notably reduces test error compared to baseline algorithms in
cross-environment (All) scenarios.

</details>


### [189] [Composable Score-based Graph Diffusion Model for Multi-Conditional Molecular Generation](https://arxiv.org/abs/2509.09451)
*Anjie Qiao,Zhen Wang,Chuan Chen,DeFu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: The paper introduces Composable Score-based Graph Diffusion model (CSGD) to improve molecular graph generation with multi-property constraints, achieving state-of-the-art results in validity, fidelity, and controllability.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges in molecular graph generation, particularly improving controllability in multi-conditional settings, which is essential for material and drug discovery.

Method: The paper introduces CSGD, using score matching for discrete graphs. Techniques like Composable Guidance (CoG) for fine-tuned property control and Probability Calibration (PC) are developed to enhance the training process and model performance.

Result: CSGD delivers an average improvement of 15.3% in controllability compared to prior methods across four molecular datasets, while maintaining high validity and adherence to target molecule distributions.

Conclusion: CSGD demonstrates the potential of score-based modeling for discrete graph generation, offering flexible and effective molecular designs for real-world applications in drug and material innovation.

Abstract: Controllable molecular graph generation is essential for material and drug
discovery, where generated molecules must satisfy diverse property constraints.
While recent advances in graph diffusion models have improved generation
quality, their effectiveness in multi-conditional settings remains limited due
to reliance on joint conditioning or continuous relaxations that compromise
fidelity. To address these limitations, we propose Composable Score-based Graph
Diffusion model (CSGD), the first model that extends score matching to discrete
graphs via concrete scores, enabling flexible and principled manipulation of
conditional guidance. Building on this foundation, we introduce two score-based
techniques: Composable Guidance (CoG), which allows fine-grained control over
arbitrary subsets of conditions during sampling, and Probability Calibration
(PC), which adjusts estimated transition probabilities to mitigate train-test
mismatches. Empirical results on four molecular datasets show that CSGD
achieves state-of-the-art performance, with a 15.3% average improvement in
controllability over prior methods, while maintaining high validity and
distributional fidelity. Our findings highlight the practical advantages of
score-based modeling for discrete graph generation and its capacity for
flexible, multi-property molecular design.

</details>


### [190] [AquaCast: Urban Water Dynamics Forecasting with Precipitation-Informed Multi-Input Transformer](https://arxiv.org/abs/2509.09458)
*Golnoosh Abdollahinejad,Saleh Baghersalimi,Denisa-Andreea Constantinescu,Sergey Shevchik,David Atienza*

Main category: cs.LG

TL;DR: The paper introduces AquaCast, a deep learning model for urban water dynamics forecasting, which integrates endogenous and exogenous variables to achieve enhanced accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: To address the forecasting challenges in urban water systems by improving how temporal and variable dependencies are captured using endogenous and exogenous data.

Method: The study proposes AquaCast, a deep learning model incorporating embedding layers for exogenous inputs, focusing forecast computation only on endogenous variables, tested on real and synthetic datasets.

Result: AquaCast achieves state-of-the-art performance in forecasting urban water variables, surpassing baselines on both realistic and synthetic datasets across varying levels of temporal complexity.

Conclusion: AquaCast is an effective, generalizable, and scalable approach for improving urban water dynamics forecasting through advanced modeling of variable interactions and temporal dependencies.

Abstract: This work addresses the challenge of forecasting urban water dynamics by
developing a multi-input, multi-output deep learning model that incorporates
both endogenous variables (e.g., water height or discharge) and exogenous
factors (e.g., precipitation history and forecast reports). Unlike conventional
forecasting, the proposed model, AquaCast, captures both inter-variable and
temporal dependencies across all inputs, while focusing forecast solely on
endogenous variables. Exogenous inputs are fused via an embedding layer,
eliminating the need to forecast them and enabling the model to attend to their
short-term influences more effectively. We evaluate our approach on the
LausanneCity dataset, which includes measurements from four urban drainage
sensors, and demonstrate state-of-the-art performance when using only
endogenous variables. Performance also improves with the inclusion of exogenous
variables and forecast reports. To assess generalization and scalability, we
additionally test the model on three large-scale synthesized datasets,
generated from MeteoSwiss records, the Lorenz Attractors model, and the Random
Fields model, each representing a different level of temporal complexity across
100 nodes. The results confirm that our model consistently outperforms existing
baselines and maintains a robust and accurate forecast across both real and
synthetic datasets.

</details>


### [191] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: The paper introduces 'Agent-E,' an AI system that identifies academic papers from specific regions and automates subsequent actions like form submission, achieving 100% recall and 99.4% accuracy.


<details>
  <summary>Details</summary>
Motivation: To simplify and expedite scholarly discovery by automating the labor-intensive process of identifying target papers and executing related actions.

Method: Implemented a specialized AI agent ('Agent-E') combined with Robotic Process Automation to locate academic papers matching specific criteria and perform predefined tasks.

Result: Validated the system on 586 papers from five conferences, achieving a recall of 100% and an accuracy of 99.4%.

Conclusion: Task-oriented AI agents, such as 'Agent-E,' can streamline academic workflows by effectively integrating data discovery with action execution.

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [192] [CountTRuCoLa: Rule Confidence Learning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2509.09474)
*Julia Gastinger,Christian Meilicke,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: The paper proposes a new explainable method based on temporal rules for forecasting temporal knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: To improve performance and interpretability in temporal knowledge graph forecasting by leveraging rules based on recency and frequency.

Method: The authors introduce a method that learns four types of temporal rules, combined with a confidence function, to forecast knowledge graph data.

Result: The proposed method achieves comparable or superior performance to eight state-of-the-art models and two baselines across nine datasets.

Conclusion: Their method is effective and provides fully interpretable predictions, addressing a key gap in temporal knowledge graph forecasting research.

Abstract: We address the task of temporal knowledge graph (TKG) forecasting by
introducing a fully explainable method based on temporal rules. Motivated by
recent work proposing a strong baseline using recurrent facts, our approach
learns four simple types of rules with a confidence function that considers
both recency and frequency. Evaluated on nine datasets, our method matches or
surpasses the performance of eight state-of-the-art models and two baselines,
while providing fully interpretable predictions.

</details>


### [193] [Balancing Utility and Privacy: Dynamically Private SGD with Random Projection](https://arxiv.org/abs/2509.09485)
*Zhanhong Jiang,Md Zahid Hasan,Nastaran Saadati,Aditya Balu,Chao Liu,Soumik Sarkar*

Main category: cs.LG

TL;DR: The paper introduces D2P2-SGD, an optimizer that addresses privacy challenges and learning efficiency in stochastic optimization by combining dynamic differential privacy and random projections, achieving sub-linear convergence and improved model utility.


<details>
  <summary>Details</summary>
Motivation: Existing stochastic optimization approaches face challenges such as privacy leakage of model parameters and gradients, static noise mechanisms affecting model performance, and inefficiencies due to increasing model complexity.

Method: The paper proposes D2P2-SGD, which integrates dynamic differential privacy with automated gradient clipping alongside a random projection mechanism in SGD, enabling adaptable trade-offs between model utility and privacy.

Result: The D2P2-SGD optimizer achieves provably sub-linear convergence rates, surpasses other methods in balancing privacy and accuracy, and shows improved utility through extensive experimental validation across diverse datasets.

Conclusion: D2P2-SGD successfully addresses privacy concerns and learning challenges in stochastic optimization, improving accuracy while preserving differential privacy, and offers a theoretical framework with practical implementation.

Abstract: Stochastic optimization is a pivotal enabler in modern machine learning,
producing effective models for various tasks. However, several existing works
have shown that model parameters and gradient information are susceptible to
privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy
concerns, its static noise mechanism impacts the error bounds for model
performance. Additionally, with the exponential increase in model parameters,
efficient learning of these models using stochastic optimizers has become more
challenging. To address these concerns, we introduce the Dynamically
Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we
combine two important ideas: (i) dynamic differential privacy (DDP) with
automatic gradient clipping and (ii) random projection with SGD, allowing
dynamic adjustment of the tradeoff between utility and privacy of the model. It
exhibits provably sub-linear convergence rates across different objective
functions, matching the best available rate. The theoretical analysis further
suggests that DDP leads to better utility at the cost of privacy, while random
projection enables more efficient model learning. Extensive experiments across
diverse datasets show that D2P2-SGD remarkably enhances accuracy while
maintaining privacy. Our code is available here.

</details>


### [194] [PIPES: A Meta-dataset of Machine Learning Pipelines](https://arxiv.org/abs/2509.09512)
*Cynthia Moreira Maia,Lucas B. V. de Amorim,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: The paper proposes PIPES, a diverse collection of machine learning experiments addressing limitations in OpenML regarding pipeline diversity and balance, facilitating more comprehensive meta-learning analyses.


<details>
  <summary>Details</summary>
Motivation: The work aims to address the imbalance and lack of diversity in algorithmic pipelines available in OpenML for meta-learning, which can hinder effective algorithm selection and broader analyses.

Method: PIPES involves experiments from 9,408 pipelines across 300 datasets, ensuring diversity by combining various techniques in pipeline blocks and storing detailed execution data.

Result: PIPES achieved a comprehensive dataset that covers diverse pipelines, training/testing details, error logs, and predictions, which enhances research capabilities in the meta-learning field.

Conclusion: PIPES improves the representation of preprocessing techniques, provides a more balanced dataset for experiments, and offers a foundation for further meta-learning research.

Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face
the challenge of high computational costs associated with evaluating various
algorithms' performances on a given dataset. To mitigate this cost, the
meta-learning field can leverage previously executed experiments shared in
online repositories such as OpenML. OpenML provides an extensive collection of
machine learning experiments. However, an analysis of OpenML's records reveals
limitations. It lacks diversity in pipelines, specifically when exploring data
preprocessing steps/blocks, such as scaling or imputation, resulting in limited
representation. Its experiments are often focused on a few popular techniques
within each pipeline block, leading to an imbalanced sample. To overcome the
observed limitations of OpenML, we propose PIPES, a collection of experiments
involving multiple pipelines designed to represent all combinations of the
selected sets of techniques, aiming at diversity and completeness. PIPES stores
the results of experiments performed applying 9,408 pipelines to 300 datasets.
It includes detailed information on the pipeline blocks, training and testing
times, predictions, performances, and the eventual error messages. This
comprehensive collection of results allows researchers to perform analyses
across diverse and representative pipelines and datasets. PIPES also offers
potential for expansion, as additional data and experiments can be incorporated
to support the meta-learning community further. The data, code, supplementary
material, and all experiments can be found at
https://github.com/cynthiamaia/PIPES.git.

</details>


### [195] [Cough Classification using Few-Shot Learning](https://arxiv.org/abs/2509.09515)
*Yoga Disha Sendhil Kumar,Manas V Shetty,Sudip Vhaduri*

Main category: cs.LG

TL;DR: The paper explores utilizing Prototypical Networks for few-shot learning in respiratory sound classification, achieving competitive performance with limited data, especially in a multi-class setting.


<details>
  <summary>Details</summary>
Motivation: To investigate the application of few-shot learning in medical diagnostics and its potential to achieve accurate results with minimal labeled data, focusing on COVID-19, Flu, and healthy cough sound classification.

Method: The authors use Prototypical Networks combined with spectrogram representations of cough sounds for both multi-class and binary classification tasks, evaluating model effectiveness with varying support examples.

Result: Few-shot learning models achieved up to 74.87% accuracy in multi-class classification with 15 support examples per class and over 70% accuracy in binary classification, with Flu being most distinguishable and Healthy the most challenging.

Conclusion: Few-shot learning proves feasible for medical diagnostics with limited data, and multi-class models perform comparably to binary ones, emphasizing its utility in scenarios lacking large labeled datasets.

Abstract: This paper investigates the effectiveness of few-shot learning for
respiratory sound classification, focusing on coughbased detection of COVID-19,
Flu, and healthy conditions. We leverage Prototypical Networks with spectrogram
representations of cough sounds to address the challenge of limited labeled
data. Our study evaluates whether few-shot learning can enable models to
achieve performance comparable to traditional deep learning approaches while
using significantly fewer training samples. Additionally, we compare
multi-class and binary classification models to assess whether multi-class
models can perform comparably to their binary counterparts. Experimental
findings show that few-shot learning models can achieve competitive accuracy.
Our model attains 74.87% accuracy in multi-class classification with only 15
support examples per class, while binary classification achieves over 70%
accuracy across all class pairs. Class-wise analysis reveals Flu as the most
distinguishable class, and Healthy as the most challenging. Statistical tests
(paired t-test p = 0.149, Wilcoxon p = 0.125) indicate no significant
performance difference between binary and multiclass models, supporting the
viability of multi-class classification in this setting. These results
highlight the feasibility of applying few-shot learning in medical diagnostics,
particularly when large labeled datasets are unavailable.

</details>


### [196] [Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication](https://arxiv.org/abs/2509.09597)
*Maysam Behmanesh,Erkan Turan,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: This paper proposes a novel framework for graph alignment that optimizes node distinctiveness and latent space consistency to outperform current unsupervised baselines in aligning graphs and other representations.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised graph alignment methods struggle with oversmoothing in GNN-based embeddings and misalignment of latent spaces caused by structural noise and feature heterogeneity.

Method: The proposed approach employs a dual-pass encoder combining low-pass and high-pass spectral filters for distinctive embeddings and a geometry-aware functional map module for consistent latent space transformations.

Result: Experimental analysis demonstrates superior performance and robustness in both graph benchmarks and vision-language benchmarks, indicating effective generalization.

Conclusion: The framework successfully enhances node distinctiveness and geometric consistency, proving its applicability in graph and cross-domain alignment tasks.

Abstract: Graph alignment-the problem of identifying corresponding nodes across
multiple graphs-is fundamental to numerous applications. Most existing
unsupervised methods embed node features into latent representations to enable
cross-graph comparison without ground-truth correspondences. However, these
methods suffer from two critical limitations: the degradation of node
distinctiveness due to oversmoothing in GNN-based embeddings, and the
misalignment of latent spaces across graphs caused by structural noise, feature
heterogeneity, and training instability, ultimately leading to unreliable node
correspondences. We propose a novel graph alignment framework that
simultaneously enhances node distinctiveness and enforces geometric consistency
across latent spaces. Our approach introduces a dual-pass encoder that combines
low-pass and high-pass spectral filters to generate embeddings that are both
structure-aware and highly discriminative. To address latent space
misalignment, we incorporate a geometry-aware functional map module that learns
bijective and isometric transformations between graph embeddings, ensuring
consistent geometric relationships across different representations. Extensive
experiments on graph benchmarks demonstrate that our method consistently
outperforms existing unsupervised alignment baselines, exhibiting superior
robustness to structural inconsistencies and challenging alignment scenarios.
Additionally, comprehensive evaluation on vision-language benchmarks using
diverse pretrained models shows that our framework effectively generalizes
beyond graph domains, enabling unsupervised alignment of vision and language
representations.

</details>


### [197] [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](https://arxiv.org/abs/2509.09599)
*Ira J. S. Shokar,Rich R. Kerswell,Peter H. Haynes*

Main category: cs.LG

TL;DR: This paper introduces a deep learning model capable of emulating chaotic and stochastic systems governed by PDEs, with high computational efficiency and the ability to generalize across varying parameters and domain sizes.


<details>
  <summary>Details</summary>
Motivation: To address the computational complexity in exploring parametric and spatial variations of spatio-temporal systems modeled by PDEs, and to enable efficient uncertainty quantification and rare event analysis.

Method: The method involves pre-training a deep learning emulator on a single set of PDE parameter values, then fine-tuning it on a smaller, diverse dataset. The emulator incorporates local attention mechanisms to handle variable domain sizes and resolutions.

Result: The model demonstrates efficient and accurate emulation of chaotic systems such as the Kuramoto-Sivashinsky equation and beta-plane turbulence, showing computational speed-ups and reliable performance at interpolated parameter values.

Conclusion: The proposed emulator not only provides a faster alternative to numerical integration but also facilitates statistical studies, including uncertainty quantification and rare event analysis, showcasing its potential for broader applications.

Abstract: We present a deep learning emulator for stochastic and chaotic
spatio-temporal systems, explicitly conditioned on the parameter values of the
underlying partial differential equations (PDEs). Our approach involves
pre-training the model on a single parameter domain, followed by fine-tuning on
a smaller, yet diverse dataset, enabling generalisation across a broad range of
parameter values. By incorporating local attention mechanisms, the network is
capable of handling varying domain sizes and resolutions. This enables
computationally efficient pre-training on smaller domains while requiring only
a small additional dataset to learn how to generalise to larger domain sizes.
We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky
equation and stochastically-forced beta-plane turbulence, showcasing its
ability to capture phenomena at interpolated parameter values. The emulator
provides significant computational speed-ups over conventional numerical
integration, facilitating efficient exploration of parameter space, while a
probabilistic variant of the emulator provides uncertainty quantification,
allowing for the statistical study of rare events.

</details>


### [198] [ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance](https://arxiv.org/abs/2509.09611)
*Haolan Zheng,Yanlai Chen,Jiequn Han,Yue Yu*

Main category: cs.LG

TL;DR: The paper introduces ReBaNO, a data-efficient operator learning algorithm designed for solving PDEs with multiple inputs, that achieves state-of-the-art generalization and strict discretization invariance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of solving groups of PDEs with multiple distinct inputs while ensuring generalization, computational efficiency, and discretization invariance.

Method: ReBaNO uses a greedy algorithm for adaptive offline network structure building, paired with task-specific activation functions for efficient and physics-embedded learning.

Result: Numerical results show that ReBaNO outperforms PCA-Net, DeepONet, FNO, and CNO in terms of generalization capabilities and achieving strict discretization invariance.

Conclusion: ReBaNO offers a compact, effective, and generalization-enhancing solution for operator learning with minimal computational cost and embedded physics advantages.

Abstract: We propose a novel data-lean operator learning algorithm, the Reduced Basis
Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct
inputs. Inspired by the Reduced Basis Method and the recently introduced
Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a
mathematically rigorous greedy algorithm to build its network structure offline
adaptively from the ground up. Knowledge distillation via task-specific
activation function allows ReBaNO to have a compact architecture requiring
minimal computational cost online while embedding physics. In comparison to
state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,
and CNO, numerical results demonstrate that ReBaNO significantly outperforms
them in terms of eliminating/shrinking the generalization gap for both in- and
out-of-distribution tests and being the only operator learning algorithm
achieving strict discretization invariance.

</details>


### [199] [Explaining Concept Drift through the Evolution of Group Counterfactuals](https://arxiv.org/abs/2509.09616)
*Ignacy Stępka,Jerzy Stefanowski*

Main category: cs.LG

TL;DR: The paper proposes a novel method for explaining concept drift in machine learning models by analyzing the temporal evolution of group-based counterfactual explanations (GCEs).


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding how and why a model's decision-making logic changes in the presence of concept drift.

Method: The method tracks shifts in GCEs' cluster centroids and counterfactual action vectors before and after drift, combining insights from data shifts, prediction disagreements, and explanation levels in a three-layer framework.

Result: The approach enables a comprehensive diagnosis of drift, distinguishing between different root causes, such as spatial data shifts versus concept re-labeling.

Conclusion: By leveraging evolving GCEs, the methodology provides interpretable proxies for structural changes in models, facilitating better understanding and troubleshooting of concept drift.

Abstract: Machine learning models in dynamic environments often suffer from concept
drift, where changes in the data distribution degrade performance. While
detecting this drift is a well-studied topic, explaining how and why the
model's decision-making logic changes still remains a significant challenge. In
this paper, we introduce a novel methodology to explain concept drift by
analyzing the temporal evolution of group-based counterfactual explanations
(GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their
associated counterfactual action vectors before and after a drift. These
evolving GCEs act as an interpretable proxy, revealing structural changes in
the model's decision boundary and its underlying rationale. We operationalize
this analysis within a three-layer framework that synergistically combines
insights from the data layer (distributional shifts), the model layer
(prediction disagreement), and our proposed explanation layer. We show that
such holistic view allows for a more comprehensive diagnosis of drift, making
it possible to distinguish between different root causes, such as a spatial
data shift versus a re-labeling of concepts.

</details>


### [200] [Functional Groups are All you Need for Chemically Interpretable Molecular Property Prediction](https://arxiv.org/abs/2509.09619)
*Roshan Balaji,Joe Bobby,Nirav Pravinbhai Bhatt*

Main category: cs.LG

TL;DR: This paper introduces the Functional Group Representation (FGR) framework for molecular property prediction using functional group-based encodings to improve performance and interpretability of deep learning models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretability in deep learning models for molecular property prediction, which hinders their adoption by chemists.

Method: The paper proposes the Functional Group Representation (FGR) framework, which encodes molecules based on curated and data-mined functional groups, integrating pre-training on large unlabeled datasets and 2D structural descriptors.

Result: The FGR framework achieved state-of-the-art performance across 33 benchmark datasets and provided chemically interpretable predictions by linking them to specific functional groups.

Conclusion: This framework enhances the accuracy and interpretability of deep learning models, promoting their relevance and usability for molecular discovery.

Abstract: Molecular property prediction using deep learning (DL) models has accelerated
drug and materials discovery, but the resulting DL models often lack
interpretability, hindering their adoption by chemists. This work proposes
developing molecule representations using the concept of Functional Groups (FG)
in chemistry. We introduce the Functional Group Representation (FGR) framework,
a novel approach to encoding molecules based on their fundamental chemical
substructures. Our method integrates two types of functional groups: those
curated from established chemical knowledge (FG), and those mined from a large
molecular corpus using sequential pattern mining (MFG). The resulting FGR
framework encodes molecules into a lower-dimensional latent space by leveraging
pre-training on a large dataset of unlabeled molecules. Furthermore, the
proposed framework allows the inclusion of 2D structure-based descriptors of
molecules. We demonstrate that the FGR framework achieves state-of-the-art
performance on a diverse range of 33 benchmark datasets spanning physical
chemistry, biophysics, quantum mechanics, biological activity, and
pharmacokinetics while enabling chemical interpretability. Crucially, the
model's representations are intrinsically aligned with established chemical
principles, allowing chemists to directly link predicted properties to specific
functional groups and facilitating novel insights into structure-property
relationships. Our work presents a significant step toward developing
high-performing, chemically interpretable DL models for molecular discovery.

</details>


### [201] [Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management](https://arxiv.org/abs/2509.09655)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: FG-FARL, a novel offline RL approach, is introduced to improve fairness and safety in decision-making for protected subgroups.


<details>
  <summary>Details</summary>
Motivation: To address fairness and safety in reinforcement learning for scenarios like population health management, where subgroup disparities may cause harm.

Method: FG-FARL calibrates per-group safety thresholds within an offline RL framework to equalize fairness metrics such as coverage or harm across subgroups.

Result: FG-FARL delivered comparable value to existing methods like BC and HACO, while significantly enhancing fairness metrics across protected subgroups.

Conclusion: The method provides a feasible and effective way to implement safer and more equitable decision-making in offline RL applications.

Abstract: We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning
(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds
to reduce harm while equalizing a chosen fairness target (coverage or harm)
across protected subgroups. Using de-identified longitudinal trajectories from
a Medicaid population health management program, we evaluate FG-FARL against
behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global
conformal safety baseline). We report off-policy value estimates with bootstrap
95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL
achieves comparable value to baselines while improving fairness metrics,
demonstrating a practical path to safer and more equitable decision support.

</details>


### [202] [ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms](https://arxiv.org/abs/2509.09679)
*Bingxin Xu,Zhen Dong,Oussama Elachqar,Yuzhang Shang*

Main category: cs.LG

TL;DR: This paper introduces ButterflyQuant, a novel method for 2-bit quantization in large language models using learnable butterfly transforms to handle activation outliers.


<details>
  <summary>Details</summary>
Motivation: To address the memory challenges in deploying large language models on consumer hardware while overcoming performance degradation caused by fixed quantization methods.

Method: The authors replace fixed Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens rotation angles. Orthogonality is enforced to achieve efficient outlier suppression and uniformity in activations.

Result: On LLaMA-2-7B, ButterflyQuant achieves a perplexity of 15.4, significantly better than QuaRot's 22.1, for 2-bit quantization using only minimal calibration samples and computation time.

Conclusion: ButterflyQuant offers an efficient, layer-adaptive quantization approach with improved performance metrics and computational effectiveness for memory-limited deployment of large language models.

Abstract: Large language models require massive memory footprints, severely limiting
deployment on consumer hardware. Quantization reduces memory through lower
numerical precision, but extreme 2-bit quantization suffers from catastrophic
performance loss due to outliers in activations. Rotation-based methods such as
QuIP and QuaRot apply orthogonal transforms to eliminate outliers before
quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} =
(\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these
methods use fixed transforms--Hadamard matrices achieving optimal worst-case
coherence $\mu = 1/\sqrt{n}$--that cannot adapt to specific weight
distributions. We identify that different transformer layers exhibit distinct
outlier patterns, motivating layer-adaptive rotations rather than
one-size-fits-all approaches. We propose ButterflyQuant, which replaces
Hadamard rotations with learnable butterfly transforms parameterized by
continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$
entries that are non-differentiable and prohibit gradient-based learning,
butterfly transforms' continuous parameterization enables smooth optimization
while guaranteeing orthogonality by construction. This orthogonal constraint
ensures theoretical guarantees in outlier suppression while achieving $O(n \log
n)$ computational complexity with only $\frac{n \log n}{2}$ learnable
parameters. We further introduce a uniformity regularization on
post-transformation activations to promote smoother distributions amenable to
quantization. Learning requires only 128 calibration samples and converges in
minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit
quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [203] [Time-Fair Benchmarking for Metaheuristics: A Restart-Fair Protocol for Fixed-Time Comparisons](https://arxiv.org/abs/2509.08986)
*Junbo Jacob Lian*

Main category: cs.NE

TL;DR: The paper emphasizes the importance of using wall-clock time, not just function evaluations (FEs), for benchmarking metaheuristic algorithms, proposing a fixed-time benchmarking protocol and a standardized reporting checklist.


<details>
  <summary>Details</summary>
Motivation: Issues with unfair comparison of metaheuristics arise when additional computational burdens (e.g., intensive iterations or preprocessing) are hidden, even when equivalent function evaluations (FEs) are claimed.

Method: The paper formalizes a benchmarking approach that uses fixed wall-clock time budgets, allows for algorithm restarts and adaptive mechanisms, and introduces standardized metrics for performance evaluations.

Result: The proposed protocol enables fairer and more reproducible metaheuristic evaluations by mitigating concealed overheads and emphasizing practical performance metrics.

Conclusion: Shifting focus to time-based evaluations enhances the credibility and relevance of metaheuristic algorithm assessments. A standardized reporting checklist further improves reproducibility and transparency.

Abstract: Numerous purportedly improved metaheuristics claim superior performance based
on equivalent function evaluations (FEs), yet often conceal additional
computational burdens in more intensive iterations, preprocessing stages, or
hyperparameter tuning. This paper posits that wall-clock time, rather than
solely FEs, should serve as the principal budgetary constraint for equitable
comparisons. We formalize a fixed-time, restart-fair benchmarking protocol
wherein each algorithm is allotted an identical wall-clock time budget per
problem instance, permitting unrestricted utilization of restarts, early
termination criteria, and internal adaptive mechanisms. We advocate for the
adoption of anytime performance curves, expected running time (ERT) metrics,
and performance profiles that employ time as the cost measure, all aimed at
predefined targets. Furthermore, we introduce a concise, reproducible checklist
to standardize reporting practices and mitigate undisclosed computational
overheads. This approach fosters more credible and practically relevant
evaluations of metaheuristic algorithms.

</details>


### [204] [A modified RIME algorithm with covariance learning and diversity enhancement for numerical optimization](https://arxiv.org/abs/2509.09529)
*Shangqing Shi,Luoxiao Zhang,Yuchen Yin,Xiong Yang,Hoileong Lee*

Main category: cs.NE

TL;DR: The paper introduces MRIME-CD, an enhanced version of the RIME metaheuristic algorithm, addressing issues like population diversity loss and local optima stagnation through new strategies to improve optimization performance.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the paper is to tackle the weaknesses of the original RIME algorithm, particularly its rapid loss of population diversity and tendency to fall into local optima, which hinder its exploitation and exploration capabilities.

Method: MRIME-CD incorporates three strategies: covariance learning to assist diversity, bootstrapping for global search enhancement, and stochastic updates for stagnant individuals. It utilizes rigorous statistical tests for validation using datasets like CEC2017 and CEC2022.

Result: The experimental validations highlight MRIME-CD's superior results compared to the original RIME, showing improved solution accuracy, faster convergence, and greater stability.

Conclusion: MRIME-CD effectively resolves the limitations of RIME by improving overall optimization performance while maintaining balance between exploitation and exploration.

Abstract: Metaheuristics are widely applied for their ability to provide more efficient
solutions. The RIME algorithm is a recently proposed physical-based
metaheuristic algorithm with certain advantages. However, it suffers from rapid
loss of population diversity during optimization and is prone to fall into
local optima, leading to unbalanced exploitation and exploration. To address
the shortcomings of RIME, this paper proposes a modified RIME with covariance
learning and diversity enhancement (MRIME-CD). The algorithm applies three
strategies to improve the optimization capability. First, a covariance learning
strategy is introduced in the soft-rime search stage to increase the population
diversity and balance the over-exploitation ability of RIME through the
bootstrapping effect of dominant populations. Second, in order to moderate the
tendency of RIME population to approach the optimal individual in the early
search stage, an average bootstrapping strategy is introduced into the
hard-rime puncture mechanism, which guides the population search through the
weighted position of the dominant populations, thus enhancing the global search
ability of RIME in the early stage. Finally, a new stagnation indicator is
proposed, and a stochastic covariance learning strategy is used to update the
stagnant individuals in the population when the algorithm gets stagnant, thus
enhancing the ability to jump out of the local optimal solution. The proposed
MRIME-CD algorithm is subjected to a series of validations on the CEC2017 test
set, the CEC2022 test set, and the experimental results are analyzed using the
Friedman test, the Wilcoxon rank sum test, and the Kruskal Wallis test. The
results show that MRIME-CD can effectively improve the performance of basic
RIME and has obvious superiorities in terms of solution accuracy, convergence
speed and stability.

</details>


### [205] [An improved educational competition optimizer with multi-covariance learning operators for global optimization problems](https://arxiv.org/abs/2509.09552)
*Baoqi Zhao,Xiong Yang,Hoileong Lee,Bowen Dong*

Main category: cs.NE

TL;DR: The paper enhances the Educational Competition Optimizer (ECO) by introducing a new algorithm, IECO-MCO, which uses multi-covariance learning operators to balance exploration and exploitation, outperforming previous methods in terms of speed, stability, and avoiding local optima.


<details>
  <summary>Details</summary>
Motivation: The original ECO algorithm struggles with imbalances in exploration and exploitation, making it prone to local optima and limiting its utility in complex optimization problems.

Method: The paper introduces IECO-MCO, which implements three multi-covariance learning operators to improve exploration, exploitation, and population convergence. It validates performance on benchmark functions and conducts statistical tests.

Result: IECO-MCO outperforms the original ECO and other algorithms, achieving faster convergence, greater stability, and better avoidance of local optima in multiple benchmark tests.

Conclusion: IECO-MCO is robust and effective for solving complex optimization problems, demonstrating superior performance over existing methods and practical feasibility for constrained real-world scenarios.

Abstract: The educational competition optimizer is a recently introduced metaheuristic
algorithm inspired by human behavior, originating from the dynamics of
educational competition within society. Nonetheless, ECO faces constraints due
to an imbalance between exploitation and exploration, rendering it susceptible
to local optima and demonstrating restricted effectiveness in addressing
complex optimization problems. To address these limitations, this study
presents an enhanced educational competition optimizer (IECO-MCO) utilizing
multi-covariance learning operators. In IECO, three distinct covariance
learning operators are introduced to improve the performance of ECO. Each
operator effectively balances exploitation and exploration while preventing
premature convergence of the population. The effectiveness of IECO is assessed
through benchmark functions derived from the CEC 2017 and CEC 2022 test suites,
and its performance is compared with various basic and improved algorithms
across different categories. The results demonstrate that IECO-MCO surpasses
the basic ECO and other competing algorithms in convergence speed, stability,
and the capability to avoid local optima. Furthermore, statistical analyses,
including the Friedman test, Kruskal-Wallis test, and Wilcoxon rank-sum test,
are conducted to validate the superiority of IECO-MCO over the compared
algorithms. Compared with the basic algorithm (improved algorithm), IECO-MCO
achieved an average ranking of 2.213 (2.488) on the CE2017 and CEC2022 test
suites. Additionally, the practical applicability of the proposed IECO-MCO
algorithm is verified by solving constrained optimization problems. The
experimental outcomes demonstrate the superior performance of IECO-MCO in
tackling intricate optimization problems, underscoring its robustness and
practical effectiveness in real-world scenarios.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [206] [HD-MoE: Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with 3D Near-Memory Processing](https://arxiv.org/abs/2509.09420)
*Haochen Huang,Shuzhang Zhong,Zhe Zhang,Shuangchen Li,Dimin Niu,Hongzhong Zheng,Runsheng Wang,Meng Li*

Main category: cs.PF

TL;DR: This paper proposes HD-MoE, an optimization method for efficiently mapping Mixture-of-Expert (MoE) Large Language Models (LLMs) onto Near-Memory Processing (NMP) accelerators.


<details>
  <summary>Details</summary>
Motivation: Optimize the deployment of resource-efficient Mixture-of-Expert (MoE) LLMs on Near-Memory Processing (NMP) architectures, which face performance bottlenecks due to communication costs and unbalanced computation.

Method: Propose HD-MoE, which includes an offline automatic hybrid parallel mapping algorithm and an online dynamic scheduling strategy to handle MoE computation on NMP accelerators efficiently.

Result: HD-MoE achieves significant inference speedups, ranging from 1.1x to 1.8x against Tensor Parallelism (TP), 1.1x to 1.5x against Expert Parallelism (EP), and 1.0x to 1.4x over compute-balanced hybrid TP-EP strategies.

Conclusion: HD-MoE effectively addresses communication and computation inefficiencies and establishes itself as an advanced mapping solution for MoE LLMs on NMP accelerators.

Abstract: Large Language Models (LLMs) with Mixture-of-Expert (MoE) architectures
achieve superior model performance with reduced computation costs, but at the
cost of high memory capacity and bandwidth requirements. Near-Memory Processing
(NMP) accelerators that stack memory directly on the compute through hybrid
bonding have demonstrated high bandwidth with high energy efficiency, becoming
a promising architecture for MoE models. However, as NMP accelerators comprise
distributed memory and computation, how to map the MoE computation directly
determines the LLM inference efficiency. Existing parallel mapping strategies,
including Tensor Parallelism (TP) and Expert Parallelism (EP), suffer from
either high communication costs or unbalanced computation utilization, leading
to inferior efficiency. The dynamic routing mechanism of MoE LLMs further
aggravates the efficiency challenges. Therefore, in this paper, we propose
HD-MoE to automatically optimize the MoE parallel computation across an NMP
accelerator. HD-MoE features an offline automatic hybrid parallel mapping
algorithm and an online dynamic scheduling strategy to reduce the communication
costs while maximizing the computation utilization. With extensive experimental
results, we demonstrate that HD-MoE achieves a speedup ranging from 1.1x to
1.8x over TP, 1.1x to 1.5x over EP, and 1.0x to 1.4x over the baseline Hybrid
TP-EP with Compute-Balanced parallelism strategies.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [207] [Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs](https://arxiv.org/abs/2509.09019)
*Mohit Tekriwal,John Sarracino*

Main category: cs.PL

TL;DR: This paper uses the Verified LLVM framework to ensure the correctness of floating-point optimizations, focusing on Fused-Multiply-Add (FMA).


<details>
  <summary>Details</summary>
Motivation: To ensure the correctness of aggressive floating-point optimizations applied by compilers for better performance.

Method: The Verified LLVM framework in the Rocq theorem prover is used to formally prove the correctness of FMA optimization.

Result: A correctness proof is presented for a specific optimization involving the arithmetic expression $a * b + c$ at the LLVM IR level.

Conclusion: The authors propose extending this preliminary work to incorporate more program features and fast math optimizations.

Abstract: Scientific computing programs often undergo aggressive compiler optimization
to achieve high performance and efficient resource utilization. While
performance is critical, we also need to ensure that these optimizations are
correct. In this paper, we focus on a specific class of optimizations,
floating-point optimizations, notably due to fast math, at the LLVM IR level.
We present a preliminary work, which leverages the Verified LLVM framework in
the Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)
optimization for a basic block implementing the arithmetic expression $a * b +
c$ . We then propose ways to extend this preliminary results by adding more
program features and fast math floating-point optimizations.

</details>


### [208] [Dependent-Type-Preserving Memory Allocation](https://arxiv.org/abs/2509.09059)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: The paper identifies issues with dependently typed programming languages when linking with external programs, particularly memory safety. A typed intermediate language and a type-preserving compilation process are proposed to address these concerns.


<details>
  <summary>Details</summary>
Motivation: Dependently typed languages lose their specifications after compilation, which can lead to violations when linking with external programs. The authors aim to minimize these specification violations, particularly for memory safety.

Method: The authors propose a typed intermediate language and a dependent-type-preserving compiler pass specifically designed for memory allocation.

Result: The work facilitates type checking during program linking, targeting errors like uninitialized memory pointers, which can undermine specifications.

Conclusion: By preserving types through the compilation process, the approach ensures compatibility and eliminates ill-typed linking, improving the reliability of dependently typed languages.

Abstract: Dependently typed programming languages such as Coq, Agda, Idris, and F*,
allow programmers to write detailed specifications of their programs and prove
their programs meet these specifications. However, these specifications can be
violated during compilation since they are erased after type checking. External
programs linked with the compiled program can violate the specifications of the
original program and change the behavior of the compiled program -- even when
compiled with a verified compiler. For example, since Coq does not allow
explicitly allocating memory, a programmer might link their Coq program with a
C program that can allocate memory. Even if the Coq program is compiled with a
verified compiler, the external C program can still violate the memory-safe
specification of the Coq program by providing an uninitialized pointer to
memory. This error could be ruled out by type checking in a language expressive
enough to indicate whether memory is initialized versus uninitialized. Linking
with a program with an uninitialized pointer could be considered ill-typed, and
our linking process could prevent linking with ill-typed programs. To
facilitate type checking during linking, we can use type-preserving
compilation, which preserves the types through the compilation process. In this
ongoing work, we develop a typed intermediate language that supports dependent
memory allocation, as well as a dependent-type-preserving compiler pass for
memory allocation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [209] [Multi Robot Coordination in Highly Dynamic Environments: Tackling Asymmetric Obstacles and Limited Communication](https://arxiv.org/abs/2509.08859)
*Vincenzo Suriani,Daniele Affinita,Domenico D. Bloisi,Daniele Nardi*

Main category: cs.RO

TL;DR: This paper proposes a distributed coordination method for multi-agent systems operating in low-communication environments with active and asymmetric obstacles.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in coordinating multi-agent systems in environments with limited communication, high dynamics, asymmetric obstacles, and partial observability.

Method: The authors introduced a market-based task assignment approach tailored for low-communication scenarios, accommodating asymmetric obstacles, and validated it using NAO robots in simulations and RoboCup competitions.

Result: The proposed algorithm significantly reduces task overlaps in limited communication settings by 52%.

Conclusion: The method effectively manages coordination challenges in poor communication environments, ensuring better task assignments and reduced overlaps in dynamic asymmetric settings.

Abstract: Coordinating a fully distributed multi-agent system (MAS) can be challenging
when the communication channel has very limited capabilities in terms of
sending rate and packet payload. When the MAS has to deal with active obstacles
in a highly partially observable environment, the communication channel
acquires considerable relevance. In this paper, we present an approach to deal
with task assignments in extremely active scenarios, where tasks need to be
frequently reallocated among the agents participating in the coordination
process. Inspired by market-based task assignments, we introduce a novel
distributed coordination method to orchestrate autonomous agents' actions
efficiently in low communication scenarios. In particular, our algorithm takes
into account asymmetric obstacles. While in the real world, the majority of
obstacles are asymmetric, they are usually treated as symmetric ones, thus
limiting the applicability of existing methods. To summarize, the presented
architecture is designed to tackle scenarios where the obstacles are active and
asymmetric, the communication channel is poor and the environment is partially
observable. Our approach has been validated in simulation and in the real
world, using a team of NAO robots during official RoboCup competitions.
Experimental results show a notable reduction in task overlaps in limited
communication settings, with a decrease of 52% in the most frequent reallocated
task.

</details>


### [210] [Rapid Manufacturing of Lightweight Drone Frames Using Single-Tow Architected Composites](https://arxiv.org/abs/2509.09024)
*Md Habib Ullah Khan,Kaiyue Deng,Ismail Mujtaba Khan,Kelvin Fu*

Main category: cs.RO

TL;DR: This paper introduces 3D Fiber Tethering (3DFiT) for crafting lightweight lattice drone frames that are stronger, lighter, and more durable than conventional materials.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in traditional composite manufacturing methods in aerospace and robotics, including weak points in joins and challenges with fiber reinforcement.

Method: The study develops Face Centered Cubic lattice drone frames using a new 3DFiT process which employs continuous single tow fiber alignment for precision and eliminating weak joints.

Result: The drone frame crafted weighs 260 g, is 10% lighter than a commercial DJI F450 frame, offers 3 minutes of increased flight time, and has specific strength 4–8 times higher than traditional materials.

Conclusion: Single tow lattice truss-based drone frames using 3DFiT are proven scalable, lightweight, and durable, suitable for aerospace and robotics applications.

Abstract: The demand for lightweight and high-strength composite structures is rapidly
growing in aerospace and robotics, particularly for optimized drone frames.
However, conventional composite manufacturing methods struggle to achieve
complex 3D architectures for weight savings and rely on assembling separate
components, which introduce weak points at the joints. Additionally,
maintaining continuous fiber reinforcement remains challenging, limiting
structural efficiency. In this study, we demonstrate the lightweight Face
Centered Cubic (FFC) lattice structured conceptualization of drone frames for
weight reduction and complex topology fabrication through 3D Fiber Tethering
(3DFiT) using continuous single tow fiber ensuring precise fiber alignment,
eliminating weak points associated with traditional composite assembly.
Mechanical testing demonstrates that the fabricated drone frame exhibits a high
specific strength of around four to eight times the metal and thermoplastic,
outperforming other conventional 3D printing methods. The drone frame weighs
only 260 g, making it 10% lighter than the commercial DJI F450 frame, enhancing
structural integrity and contributing to an extended flight time of three
minutes, while flight testing confirms its stability and durability under
operational conditions. The findings demonstrate the potential of single tow
lattice truss-based drone frames, with 3DFiT serving as a scalable and
efficient manufacturing method.

</details>


### [211] [KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning](https://arxiv.org/abs/2509.09074)
*Alice Kate Li,Thales C Silva,Victoria Edwards,Vijay Kumar,M. Ani Hsieh*

Main category: cs.RO

TL;DR: This paper introduces KoopMotion, a method based on Koopman Operators to design motion planning systems that ensure robots smoothly follow desired trajectories and converge to desired endpoints.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing Koopman operator theory in ensuring smooth trajectory following and endpoint convergence in motion planning tasks, especially essential in learning from demonstrations.

Method: KoopMotion represents motion flow fields as dynamical systems parameterized by Koopman Operators, ensuring divergence properties that allow robots to converge to reference trajectories and endpoints.

Result: KoopMotion demonstrated effectiveness on datasets such as LASA handwriting and 3D manipulator trajectories, as well as physical experiments with autonomous robots. It achieved high efficiency, requiring only 3% of LASA data for dense motion plans, and outperformed baselines in modeling spatial and temporal dynamics.

Conclusion: KoopMotion is a sample-efficient solution for motion planning in dynamic environments, offering smooth trajectory tracking and endpoint convergence with significant efficacy improvements over alternative methods.

Abstract: In this work, we propose a novel flow field-based motion planning method that
drives a robot from any initial state to a desired reference trajectory such
that it converges to the trajectory's end point. Despite demonstrated efficacy
in using Koopman operator theory for modeling dynamical systems, Koopman does
not inherently enforce convergence to desired trajectories nor to specified
goals -- a requirement when learning from demonstrations (LfD). We present
KoopMotion which represents motion flow fields as dynamical systems,
parameterized by Koopman Operators to mimic desired trajectories, and leverages
the divergence properties of the learnt flow fields to obtain smooth motion
fields that converge to a desired reference trajectory when a robot is placed
away from the desired trajectory, and tracks the trajectory until the end
point. To demonstrate the effectiveness of our approach, we show evaluations of
KoopMotion on the LASA human handwriting dataset and a 3D manipulator
end-effector trajectory dataset, including spectral analysis. We also perform
experiments on a physical robot, verifying KoopMotion on a miniature autonomous
surface vehicle operating in a non-static fluid flow environment. Our approach
is highly sample efficient in both space and time, requiring only 3\% of the
LASA dataset to generate dense motion plans. Additionally, KoopMotion provides
a significant improvement over baselines when comparing metrics that measure
spatial and temporal dynamics modeling efficacy.

</details>


### [212] [Kinetostatics and Particle-Swarm Optimization of Vehicle-Mounted Underactuated Metamorphic Loading Manipulators](https://arxiv.org/abs/2509.09093)
*Nan Mao,Guanglu Jia,Junpeng Chen,Emmanouil Spyrakos-Papastavridis,Jian S. Dai*

Main category: cs.RO

TL;DR: This paper introduces an underactuated metamorphic loading manipulator (UMLM) integrating a metamorphic arm and adaptively driven gripper for dynamic tasks with efficient performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of fixed degree-of-freedom (DoF) mechanisms, such as excessive actuators, complex control, and restricted adaptability in dynamic environments.

Method: The study proposes a UMLM integrating a metamorphic arm for topology reconfiguration and a passively adaptive gripper. A structural model and kinetostatics analysis are conducted, and the Particle-Swarm Optimization (PSO) is used to optimize gripper parameters.

Result: Simulation results confirm the UMLM's effective control strategy, versatility, and ability to adapt to dynamic environments by grasping diverse objects easily.

Conclusion: This research validates underactuated metamorphic mechanisms' practicality and introduces a scalable modeling and optimization framework applicable to various robotic systems requiring efficiency and flexibility.

Abstract: Fixed degree-of-freedom (DoF) loading mechanisms often suffer from excessive
actuators, complex control, and limited adaptability to dynamic tasks. This
study proposes an innovative mechanism of underactuated metamorphic loading
manipulators (UMLM), integrating a metamorphic arm with a passively adaptive
gripper. The metamorphic arm exploits geometric constraints, enabling the
topology reconfiguration and flexible motion trajectories without additional
actuators. The adaptive gripper, driven entirely by the arm, conforms to
diverse objects through passive compliance. A structural model is developed,
and a kinetostatics analysis is conducted to investigate isomorphic grasping
configurations. To optimize performance, Particle-Swarm Optimization (PSO) is
utilized to refine the gripper's dimensional parameters, ensuring robust
adaptability across various applications. Simulation results validate the
UMLM's easily implemented control strategy, operational versatility, and
effectiveness in grasping diverse objects in dynamic environments. This work
underscores the practical potential of underactuated metamorphic mechanisms in
applications requiring efficient and adaptable loading solutions. Beyond the
specific design, this generalized modeling and optimization framework extends
to a broader class of manipulators, offering a scalable approach to the
development of robotic systems that require efficiency, flexibility, and robust
performance.

</details>


### [213] [LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots](https://arxiv.org/abs/2509.09106)
*Haokai Su,Haoxiang Luo,Shunpeng Yang,Kaiwen Jiang,Wei Zhang,Hua Chen*

Main category: cs.RO

TL;DR: The paper proposes a reward design based on the Linear Inverted Pendulum Model (LIPM) for enabling stable and robust locomotion in bipedal robots on complex terrains.


<details>
  <summary>Details</summary>
Motivation: Bipedal robots struggle with stability and robustness in unstructured, outdoor environments due to terrain complexity and external disturbances.

Method: A novel reward function is designed using LIPM principles, along with a Reward Fusion Module (RFM) for adaptive trade-offs between stability and velocity. A double-critic architecture improves evaluation and training efficiency.

Result: Extensive experiments validate superior performance in terrain adaptability, disturbance rejection, and consistent locomotion over varied speeds and perceptual scenarios.

Conclusion: The proposed approach effectively ensures perceptive and stable locomotion for bipedal robots, demonstrating practical viability in dynamic outdoor settings.

Abstract: Achieving stable and robust perceptive locomotion for bipedal robots in
unstructured outdoor environments remains a critical challenge due to complex
terrain geometry and susceptibility to external disturbances. In this work, we
propose a novel reward design inspired by the Linear Inverted Pendulum Model
(LIPM) to enable perceptive and stable locomotion in the wild. The LIPM
provides theoretical guidance for dynamic balance by regulating the center of
mass (CoM) height and the torso orientation. These are key factors for
terrain-aware locomotion, as they help ensure a stable viewpoint for the
robot's camera. Building on this insight, we design a reward function that
promotes balance and dynamic stability while encouraging accurate CoM
trajectory tracking. To adaptively trade off between velocity tracking and
stability, we leverage the Reward Fusion Module (RFM) approach that prioritizes
stability when needed. A double-critic architecture is adopted to separately
evaluate stability and locomotion objectives, improving training efficiency and
robustness. We validate our approach through extensive experiments on a bipedal
robot in both simulation and real-world outdoor environments. The results
demonstrate superior terrain adaptability, disturbance rejection, and
consistent performance across a wide range of speeds and perceptual conditions.

</details>


### [214] [AEOS: Active Environment-aware Optimal Scanning Control for UAV LiDAR-Inertial Odometry in Complex Scenes](https://arxiv.org/abs/2509.09141)
*Jianping Li,Xinhang Xu,Zhongyuan Liu,Shenghai Yuan,Muqing Cao,Lihua Xie*

Main category: cs.RO

TL;DR: The paper proposes a biologically inspired adaptive LiDAR control framework, AEOS, to improve UAV-based 3D perception by addressing limitations of narrow field-of-view LiDAR sensors.


<details>
  <summary>Details</summary>
Motivation: Traditional LiDAR-based systems for UAVs are hindered by narrow fields of view and hardware constraints, which degrade performance in complex environments.

Method: AEOS uses a hybrid approach combining model predictive control (MPC) and reinforcement learning (RL) to optimize LiDAR-inertial odometry. It features a simulation environment for scalable training and real-world generalization.

Result: AEOS demonstrated better odometry accuracy compared to traditional, optimization-only, and fully learned baselines in both simulated and real-world scenarios.

Conclusion: The framework significantly enhances real-time odometry performance for UAVs while addressing hardware limitations, making it viable for practical deployment.

Abstract: LiDAR-based 3D perception and localization on unmanned aerial vehicles (UAVs)
are fundamentally limited by the narrow field of view (FoV) of compact LiDAR
sensors and the payload constraints that preclude multi-sensor configurations.
Traditional motorized scanning systems with fixed-speed rotations lack scene
awareness and task-level adaptability, leading to degraded odometry and mapping
performance in complex, occluded environments. Inspired by the active sensing
behavior of owls, we propose AEOS (Active Environment-aware Optimal Scanning),
a biologically inspired and computationally efficient framework for adaptive
LiDAR control in UAV-based LiDAR-Inertial Odometry (LIO). AEOS combines model
predictive control (MPC) and reinforcement learning (RL) in a hybrid
architecture: an analytical uncertainty model predicts future pose
observability for exploitation, while a lightweight neural network learns an
implicit cost map from panoramic depth representations to guide exploration. To
support scalable training and generalization, we develop a point cloud-based
simulation environment with real-world LiDAR maps across diverse scenes,
enabling sim-to-real transfer. Extensive experiments in both simulation and
real-world environments demonstrate that AEOS significantly improves odometry
accuracy compared to fixed-rate, optimization-only, and fully learned
baselines, while maintaining real-time performance under onboard computational
constraints. The project page can be found at
https://kafeiyin00.github.io/AEOS/.

</details>


### [215] [Occupancy-aware Trajectory Planning for Autonomous Valet Parking in Uncertain Dynamic Environments](https://arxiv.org/abs/2509.09206)
*Farhad Nawaz,Faizan M. Tariq,Sangjae Bae,David Isele,Avinash Singh,Nadia Figueroa,Nikolai Matni,Jovin D'sa*

Main category: cs.RO

TL;DR: The paper introduces an approach for autonomous valet parking by predicting future parking spot availability and planning dynamic maneuvers to improve efficiency and safety.


<details>
  <summary>Details</summary>
Motivation: Existing methods for autonomous valet parking rely on static assumptions or instantaneous observations, which limit their adaptability in uncertain and dynamic environments.

Method: The authors developed a probabilistic spot occupancy estimator that incorporates partial and noisy data from a limited Field-of-View model and accounts for evolving uncertainty. They combined this with a strategy planner for adaptive parking maneuvers, exploratory navigation, and wait-and-go behaviors.

Result: Simulations in large parking lots showed the approach improved parking efficiency, safety margins, and trajectory smoothness compared to prior methods.

Conclusion: This framework advances autonomous valet parking systems by enabling better prediction, planning, and adaptability in dynamic environments, addressing key limitations of existing solutions.

Abstract: Accurately reasoning about future parking spot availability and integrated
planning is critical for enabling safe and efficient autonomous valet parking
in dynamic, uncertain environments. Unlike existing methods that rely solely on
instantaneous observations or static assumptions, we present an approach that
predicts future parking spot occupancy by explicitly distinguishing between
initially vacant and occupied spots, and by leveraging the predicted motion of
dynamic agents. We introduce a probabilistic spot occupancy estimator that
incorporates partial and noisy observations within a limited Field-of-View
(FoV) model and accounts for the evolving uncertainty of unobserved regions.
Coupled with this, we design a strategy planner that adaptively balances
goal-directed parking maneuvers with exploratory navigation based on
information gain, and intelligently incorporates wait-and-go behaviors at
promising spots. Through randomized simulations emulating large parking lots,
we demonstrate that our framework significantly improves parking efficiency,
safety margins, and trajectory smoothness compared to existing approaches.

</details>


### [216] [RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse](https://arxiv.org/abs/2509.09283)
*Yueqi Zhang,Quancheng Qian,Taixian Hou,Peng Zhai,Xiaoyi Wei,Kangmai Hu,Jiafu Yi,Lihua Zhang*

Main category: cs.RO

TL;DR: This paper introduces a framework called Redundant Estimator Network (RENet) for robust quadruped locomotion in outdoor environments, focusing on overcoming challenges in vision-based depth estimation and environmental prediction.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to enable reliable vision-based locomotion for quadruped robots in outdoor environments where visual perception can be uncertain or noisy.

Method: The proposed methodology involves a dual-estimator architecture that incorporates online estimator adaptation, allowing transitions between modules to handle perception uncertainties and vision failures.

Result: The RENet framework was experimentally validated on a real-world quadruped robot, achieving reliable motion control in complex outdoor environments, especially in conditions of degraded visual perception.

Conclusion: RENet provides a practical solution for robust and stable robotic deployment in challenging field conditions, improving real-world applicability for outdoor locomotion algorithms.

Abstract: Vision-based locomotion in outdoor environments presents significant
challenges for quadruped robots. Accurate environmental prediction and
effective handling of depth sensor noise during real-world deployment remain
difficult, severely restricting the outdoor applications of such algorithms. To
address these deployment challenges in vision-based motion control, this letter
proposes the Redundant Estimator Network (RENet) framework. The framework
employs a dual-estimator architecture that ensures robust motion performance
while maintaining deployment stability during onboard vision failures. Through
an online estimator adaptation, our method enables seamless transitions between
estimation modules when handling visual perception uncertainties. Experimental
validation on a real-world robot demonstrates the framework's effectiveness in
complex outdoor environments, showing particular advantages in scenarios with
degraded visual perception. This framework demonstrates its potential as a
practical solution for reliable robotic deployment in challenging field
conditions. Project website: https://RENet-Loco.github.io/

</details>


### [217] [OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning](https://arxiv.org/abs/2509.09332)
*Yuecheng Liu,Dafeng Chi,Shiguang Wu,Zhanguang Zhang,Yuzheng Zhuang,Bowen Yang,He Zhu,Lingfeng Zhang,Pengwei Xie,David Gamaliel Arcos Bravo,Yingxue Zhang,Jianye Hao,Xingyue Quan*

Main category: cs.RO

TL;DR: OmniEVA addresses gaps in current embodied systems by introducing task-adaptive 3D grounding and embodiment-aware reasoning, significantly enhancing multimodal task planning and execution.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome deficits in multimodal large language models (MLLMs), specifically their lack of adaptability to tasks requiring diverse spatial processing and neglect of real-world robotic constraints.

Method: OmniEVA introduces a Task-Adaptive 3D Grounding mechanism for selective 3D fusion based on context and an Embodiment-Aware Reasoning framework that integrates task goals with embodiment constraints.

Result: Experimental results show state-of-the-art performance in general embodied reasoning and robust planning across diverse tasks and scenarios.

Conclusion: OmniEVA advances embodied reasoning and planning by addressing geometric adaptability and embodiment constraints, proving its capability through comprehensive benchmarking.

Abstract: Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.
Nevertheless, current MLLM-based embodied systems face two critical
limitations. First, Geometric Adaptability Gap: models trained solely on 2D
inputs or with hard-coded 3D geometry injection suffer from either insufficient
spatial information or restricted 2D generalization, leading to poor
adaptability across tasks with diverse spatial demands. Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware
Reasoning framework that jointly incorporates task goals and embodiment
constraints into the reasoning loop, resulting in planning decisions that are
both goal-directed and executable. Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios. Evaluations of a suite of proposed embodied benchmarks,
including both primitive and composite tasks, confirm its robust and versatile
planning capabilities. Project page: https://omnieva.github.io

</details>


### [218] [AGILOped: Agile Open-Source Humanoid Robot for Research](https://arxiv.org/abs/2509.09364)
*Grzegorz Ficht,Luis Denninger,Sven Behnke*

Main category: cs.RO

TL;DR: AGILOped is an open-source humanoid robot combining accessibility with high performance, demonstrated through various physical experiments.


<details>
  <summary>Details</summary>
Motivation: Fill the gap between high-performance humanoid robots and accessibility by offering an open-source platform.

Method: Develop AGILOped using standard components and lightweight design, enabling single-person operation and versatile capabilities.

Result: AGILOped shows successful performance in walking, jumping, impact mitigation, and getting-up experiments.

Conclusion: AGILOped offers researchers an accessible and high-performing humanoid robot platform for various applications.

Abstract: With academic and commercial interest for humanoid robots peaking, multiple
platforms are being developed. Through a high level of customization, they
showcase impressive performance. Most of these systems remain closed-source or
have high acquisition and maintenance costs, however. In this work, we present
AGILOped - an open-source humanoid robot that closes the gap between high
performance and accessibility. Our robot is driven by off-the-shelf
backdrivable actuators with high power density and uses standard electronic
components. With a height of 110 cm and weighing only 14.5 kg, AGILOped can be
operated without a gantry by a single person. Experiments in walking, jumping,
impact mitigation and getting-up demonstrate its viability for use in research.

</details>


### [219] [VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model](https://arxiv.org/abs/2509.09372)
*Yihao Wang,Pengxiang Ding,Lingxiao Li,Can Cui,Zirui Ge,Xinyang Tong,Wenxuan Song,Han Zhao,Wei Zhao,Pengxu Hou,Siteng Huang,Yifan Tang,Wenhui Wang,Ru Zhang,Jianyi Liu,Donglin Wang*

Main category: cs.RO

TL;DR: The paper introduces VLA-Adapter, a lightweight model reducing reliance on large Vision-Language Models (VLMs) for robotic tasks, achieving high performance without extensive pre-training.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action models require large-scale VLMs and significant training, which is costly and resource-intensive.

Method: Proposes VLA-Adapter, a lightweight policy module with Bridge Attention, which integrates optimal vision-language conditions for action predictions, removing the need for large pre-trained VLMs or extensive robotic data.

Result: VLA-Adapter achieves state-of-the-art performance on simulated and real-world robotic benchmarks, executes in fast inference speeds, and can be trained in 8 hours on a consumer GPU.

Conclusion: The approach reduces the computational and resource burden for training VLA models while maintaining high performance, making them more accessible.

Abstract: Vision-Language-Action (VLA) models typically bridge the gap between
perceptual and action spaces by pre-training a large-scale Vision-Language
Model (VLM) on robotic data. While this approach greatly enhances performance,
it also incurs significant training costs. In this paper, we investigate how to
effectively bridge vision-language (VL) representations to action (A). We
introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA
models on large-scale VLMs and extensive pre-training. To this end, we first
systematically analyze the effectiveness of various VL conditions and present
key findings on which conditions are essential for bridging perception and
action spaces. Based on these insights, we propose a lightweight Policy module
with Bridge Attention, which autonomously injects the optimal condition into
the action space. In this way, our method achieves high performance using only
a 0.5B-parameter backbone, without any robotic data pre-training. Extensive
experiments on both simulated and real-world robotic benchmarks demonstrate
that VLA-Adapter not only achieves state-of-the-art level performance, but also
offers the fast inference speed reported to date. Furthermore, thanks to the
proposed advanced bridging paradigm, VLA-Adapter enables the training of a
powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly
lowering the barrier to deploying the VLA model. Project page:
https://vla-adapter.github.io/.

</details>


### [220] [A Hybrid Hinge-Beam Continuum Robot with Passive Safety Capping for Real-Time Fatigue Awareness](https://arxiv.org/abs/2509.09404)
*Tongshun Chen,Zezhou Sun,Yanhan Sun,Yuhao Wang,Dezhen Song,Ke Wu*

Main category: cs.RO

TL;DR: This paper proposes a fatigue-aware cable-driven continuum robot design and real-time fatigue-awareness method to improve durability and long-term reliability.


<details>
  <summary>Details</summary>
Motivation: Current continuum robots suffer from performance degradation and structural failure due to mechanical fatigue, limiting their usability in prolonged operations.

Method: The study introduces a Hybrid Hinge-Beam structure that decouples torsion and bending stresses, a Passive Stopper ensuring safety and enabling torque sensing, and a real-time stiffness estimation method for online fatigue assessment.

Result: Experiments demonstrated a 49% reduction in fatigue accumulation compared to conventional designs while enabling accurate fatigue estimation using motor-side sensing.

Conclusion: The proposed design shows significant potential for enhancing the durability and operational reliability of continuum robots in constrained environments.

Abstract: Cable-driven continuum robots offer high flexibility and lightweight design,
making them well-suited for tasks in constrained and unstructured environments.
However, prolonged use can induce mechanical fatigue from plastic deformation
and material degradation, compromising performance and risking structural
failure. In the state of the art, fatigue estimation of continuum robots
remains underexplored, limiting long-term operation. To address this, we
propose a fatigue-aware continuum robot with three key innovations: (1) a
Hybrid Hinge-Beam structure where TwistBeam and BendBeam decouple torsion and
bending: passive revolute joints in the BendBeam mitigate stress concentration,
while TwistBeam's limited torsional deformation reduces BendBeam stress
magnitude, enhancing durability; (2) a Passive Stopper that safely constrains
motion via mechanical constraints and employs motor torque sensing to detect
corresponding limit torque, ensuring safety and enabling data collection; and
(3) a real-time fatigue-awareness method that estimates stiffness from motor
torque at the limit pose, enabling online fatigue estimation without additional
sensors. Experiments show that the proposed design reduces fatigue accumulation
by about 49% compared with a conventional design, while passive mechanical
limiting combined with motor-side sensing allows accurate estimation of
structural fatigue and damage. These results confirm the effectiveness of the
proposed architecture for safe and reliable long-term operation.

</details>


### [221] [BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object Bagging](https://arxiv.org/abs/2509.09484)
*Peng Zhou,Jiaming Qi,Hongmin Wu,Chen Wang,Yizhou Chen,Zeqing Zhang*

Main category: cs.RO

TL;DR: The paper presents a dual-arm robotic system for adaptive bag manipulation using real-time visual feedback and advanced modeling/planning techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of deformable object manipulation in industrial bagging tasks, where traditional methods struggle due to unpredictable material properties.

Method: The system leverages Gaussian Mixture Models (GMM) for estimating deformable structures, optimizes SOI generation, employs Constrained Bidirectional Rapidly-exploring Random Tree (CBiRRT) for motion planning, and utilizes Model Predictive Control (MPC) for dual-arm coordination.

Result: Extensive experiments showed that the proposed system adapts well to various bagging scenarios, performing robustly and precisely without requiring prior knowledge of bag properties.

Conclusion: The proposed method offers an innovative, efficient approach for automated deformable object manipulation, advancing robotics applications in industrial bagging.

Abstract: Bagging tasks, commonly found in industrial scenarios, are challenging
considering deformable bags' complicated and unpredictable nature. This paper
presents an automated bagging system from the proposed adaptive
Structure-of-Interest (SOI) manipulation strategy for dual robot arms. The
system dynamically adjusts its actions based on real-time visual feedback,
removing the need for pre-existing knowledge of bag properties. Our framework
incorporates Gaussian Mixture Models (GMM) for estimating SOI states,
optimization techniques for SOI generation, motion planning via Constrained
Bidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination
using Model Predictive Control (MPC). Extensive experiments validate the
capability of our system to perform precise and robust bagging across various
objects, showcasing its adaptability. This work offers a new solution for
robotic deformable object manipulation (DOM), particularly in automated bagging
tasks. Video of this work is available at https://youtu.be/6JWjCOeTGiQ.

</details>


### [222] [SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking](https://arxiv.org/abs/2509.09509)
*Pedro Miguel Bastos Soares,Ali Tourani,Miguel Fernandez-Cortizas,Asier Bikandi Noya,Jose Luis Sanchez-Lopez,Holger Voos*

Main category: cs.RO

TL;DR: The paper introduces SMapper, an open-hardware platform for SLAM research integrating multiple synchronized sensors and releasing a public dataset called SMapper-light for evaluation.


<details>
  <summary>Details</summary>
Motivation: SLAM and autonomous navigation research require reliable datasets, yet many existing datasets lack sufficient sensing and environmental diversity, making experiments less reproducible.

Method: The study presents SMapper, a platform with integrated LiDAR, cameras, and inertial sensors, supported by a calibration pipeline. It also releases the SMapper-light dataset with synchronized data and benchmarks SLAM frameworks.

Result: SMapper proved capable of precise multimodal sensing and reproducible experiments, while the SMapper-light dataset provided high-accuracy ground-truth trajectories for SLAM benchmarking.

Conclusion: The work enhances SLAM research capabilities through its open hardware design, reproducible dataset, and benchmark evaluations, fostering progress in algorithm development and comparisons.

Abstract: Advancing research in fields like Simultaneous Localization and Mapping
(SLAM) and autonomous navigation critically depends on reliable and
reproducible multimodal datasets. While several influential datasets have
driven progress in these domains, they often suffer from limitations in sensing
modalities, environmental diversity, and the reproducibility of the underlying
hardware setups. To address these challenges, this paper introduces SMapper, a
novel open-hardware, multi-sensor platform designed explicitly for, though not
limited to, SLAM research. The device integrates synchronized LiDAR,
multi-camera, and inertial sensing, supported by a robust calibration and
synchronization pipeline that ensures precise spatio-temporal alignment across
modalities. Its open and replicable design allows researchers to extend its
capabilities and reproduce experiments across both handheld and robot-mounted
scenarios. To demonstrate its practicality, we additionally release
SMapper-light, a publicly available SLAM dataset containing representative
indoor and outdoor sequences. The dataset includes tightly synchronized
multimodal data and ground-truth trajectories derived from offline LiDAR-based
SLAM with sub-centimeter accuracy, alongside dense 3D reconstructions.
Furthermore, the paper contains benchmarking results on state-of-the-art LiDAR
and visual SLAM frameworks using the SMapper-light dataset. By combining
open-hardware design, reproducible data collection, and comprehensive
benchmarking, SMapper establishes a robust foundation for advancing SLAM
algorithm development, evaluation, and reproducibility.

</details>


### [223] [A Neuromorphic Incipient Slip Detection System using Papillae Morphology](https://arxiv.org/abs/2509.09546)
*Yanhui Lu,Zeyu Deng,Stephen J. Redmond,Efi Psomopoulou,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: The paper presents a neuromorphic tactile sensing system using a spiking convolutional neural network (SCNN) for accurate slip-state classification and early detection of incipient slippage.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting incipient slippage for safer robotic manipulation on energy-constrained edge platforms.

Method: Developed a tactile sensing system combining NeuroTac sensor with spiking convolutional neural network (SCNN) to classify slip states and perform early slip detection.

Result: Achieved over 94% classification accuracy, detected incipient slip at least 360 ms before gross slip in all tested conditions.

Conclusion: The neuromorphic tactile sensing system provides reliable, early incipient slip detection, supporting safer robotic manipulation.

Abstract: Detecting incipient slip enables early intervention to prevent object
slippage and enhance robotic manipulation safety. However, deploying such
systems on edge platforms remains challenging, particularly due to energy
constraints. This work presents a neuromorphic tactile sensing system based on
the NeuroTac sensor with an extruding papillae-based skin and a spiking
convolutional neural network (SCNN) for slip-state classification. The SCNN
model achieves 94.33% classification accuracy across three classes (no slip,
incipient slip, and gross slip) in slip conditions induced by sensor motion.
Under the dynamic gravity-induced slip validation conditions, after temporal
smoothing of the SCNN's final-layer spike counts, the system detects incipient
slip at least 360 ms prior to gross slip across all trials, consistently
identifying incipient slip before gross slip occurs. These results demonstrate
that this neuromorphic system has stable and responsive incipient slip
detection capability.

</details>


### [224] [ObjectReact: Learning Object-Relative Control for Visual Navigation](https://arxiv.org/abs/2509.09594)
*Sourav Garg,Dustin Craggs,Vineeth Bhat,Lachlan Mares,Stefan Podgorski,Madhava Krishna,Feras Dayoub,Ian Reid*

Main category: cs.RO

TL;DR: This paper proposes a novel paradigm, "object-relative" control for visual navigation which uses a topometric map and object-relative representations.


<details>
  <summary>Details</summary>
Motivation: Current methods of visual navigation rely heavily on image-relative approaches tied to agents' pose and embodiment, which restrict flexibility and generalization.

Method: The authors present "ObjectReact," leveraging a relative 3D scene graph and object-level global path planning costs, trained using a high-level "WayObject Costmap" without relying on RGB inputs.

Result: Object-relative control shows greater adaptability across sensor and embodiment variations, performs well on complex navigation tasks, and generalizes from simulation to real-world environments.

Conclusion: Object-relative control surpasses conventional image-relative control in flexibility, generalization, and robustness in visual navigation tasks.

Abstract: Visual navigation using only a single camera and a topological map has
recently become an appealing alternative to methods that require additional
sensors and 3D maps. This is typically achieved through an "image-relative"
approach to estimating control from a given pair of current observation and
subgoal image. However, image-level representations of the world have
limitations because images are strictly tied to the agent's pose and
embodiment. In contrast, objects, being a property of the map, offer an
embodiment- and trajectory-invariant world representation. In this work, we
present a new paradigm of learning "object-relative" control that exhibits
several desirable characteristics: a) new routes can be traversed without
strictly requiring to imitate prior experience, b) the control prediction
problem can be decoupled from solving the image matching problem, and c) high
invariance can be achieved in cross-embodiment deployment for variations across
both training-testing and mapping-execution settings. We propose a topometric
map representation in the form of a "relative" 3D scene graph, which is used to
obtain more informative object-level global path planning costs. We train a
local controller, dubbed "ObjectReact", conditioned directly on a high-level
"WayObject Costmap" representation that eliminates the need for an explicit RGB
input. We demonstrate the advantages of learning object-relative control over
its image-relative counterpart across sensor height variations and multiple
navigation tasks that challenge the underlying spatial understanding
capability, e.g., navigating a map trajectory in the reverse direction. We
further show that our sim-only policy is able to generalize well to real-world
indoor environments. Code and supplementary material are accessible via project
page: https://object-react.github.io/

</details>


### [225] [MOFU: Development of a MOrphing Fluffy Unit with Expansion and Contraction Capabilities and Evaluation of the Animacy of Its Movements](https://arxiv.org/abs/2509.09613)
*Taisei Mogi,Mari Saito,Yoshihiro Nakata*

Main category: cs.RO

TL;DR: This paper introduces a new robot, MOFU, capable of whole-body expansion-contraction movements, which increase perceived animacy compared to traditional robots. It finds that movement features like volume changes enhance animacy.


<details>
  <summary>Details</summary>
Motivation: Robots for therapy and social interaction aim to evoke animacy, but little attention has been paid to volume-changing movements observed in living organisms and their impact on animacy perception.

Method: The researchers developed MOFU with volume-changing capability using a "Jitterbug" structure for smooth expansion and contraction via a single motor and conducted an online survey to evaluate animacy perception based on MOFU's movements.

Result: Expansion-contraction movements significantly increased perceived animacy. Combining expansion-contraction with locomotion further improved animacy ratings. No significant animacy difference arose between single and dual robots.

Conclusion: Design elements like expansion-contraction enhance perceived animacy in robots and should be considered in future robot development to improve human-robot interaction.

Abstract: Robots for therapy and social interaction are often intended to evoke
"animacy" in humans. While many robots imitate appearance and joint movements,
little attention has been given to whole-body expansion-contraction,
volume-changing movements observed in living organisms, and their effect on
animacy perception. We developed a mobile robot called "MOFU (Morphing Fluffy
Unit)," capable of whole-body expansion-contraction with a single motor and
covered with a fluffy exterior. MOFU employs a "Jitterbug" structure, a
geometric transformation mechanism that enables smooth volume change in
diameter from 210 to 280 mm using one actuator. It is also equipped with a
differential two-wheel drive mechanism for locomotion. To evaluate the effect
of expansion-contraction movements, we conducted an online survey using videos
of MOFU's behavior. Participants rated impressions with the Godspeed
Questionnaire Series. First, we compared videos of MOFU in a stationary state
with and without expansion-contraction and turning, finding that
expansion-contraction significantly increased perceived animacy. Second, we
hypothesized that presenting two MOFUs would increase animacy compared with a
single robot; however, this was not supported, as no significant difference
emerged. Exploratory analyses further compared four dual-robot motion
conditions. Third, when expansion-contraction was combined with locomotion,
animacy ratings were higher than locomotion alone. These results suggest that
volume-changing movements such as expansion and contraction enhance perceived
animacy in robots and should be considered an important design element in
future robot development aimed at shaping human impressions.

</details>


### [226] [Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration](https://arxiv.org/abs/2509.09671)
*Sirui Xu,Yu-Wei Chao,Liuyu Bian,Arsalan Mousavian,Yu-Xiong Wang,Liang-Yan Gui,Wei Yang*

Main category: cs.RO

TL;DR: The paper introduces Dexplore, a unified framework that optimizes robot control policies directly from large-scale motion-capture data, addressing inaccuracies and embodiment gaps in traditional approaches.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of using motion-capture data for robot manipulation due to inaccuracies and embodiment differences between human and robot hands.

Method: A single-loop optimization framework that unifies retargeting and tracking to train robot policies using reinforcement learning, leveraging adaptive spatial scopes derived from raw demonstration trajectories.

Result: Dexplore improves noise robustness, scales effectively to large datasets, and generates a skill-conditioned controller with diverse manipulation strategies that generalize across objects for real-world applications.

Conclusion: Dexplore effectively transforms imperfect human demonstrations into valuable training signals for dexterous robotic manipulation, supporting scalable and robust robotic strategies.

Abstract: Hand-object motion-capture (MoCap) repositories offer large-scale,
contact-rich demonstrations and hold promise for scaling dexterous robotic
manipulation. Yet demonstration inaccuracies and embodiment gaps between human
and robot hands limit the straightforward use of these data. Existing methods
adopt a three-stage workflow, including retargeting, tracking, and residual
correction, which often leaves demonstrations underused and compound errors
across stages. We introduce Dexplore, a unified single-loop optimization that
jointly performs retargeting and tracking to learn robot control policies
directly from MoCap at scale. Rather than treating demonstrations as ground
truth, we use them as soft guidance. From raw trajectories, we derive adaptive
spatial scopes, and train with reinforcement learning to keep the policy
in-scope while minimizing control effort and accomplishing the task. This
unified formulation preserves demonstration intent, enables robot-specific
strategies to emerge, improves robustness to noise, and scales to large
demonstration corpora. We distill the scaled tracking policy into a
vision-based, skill-conditioned generative controller that encodes diverse
manipulation skills in a rich latent representation, supporting generalization
across objects and real-world deployment. Taken together, these contributions
position Dexplore as a principled bridge that transforms imperfect
demonstrations into effective training signals for dexterous manipulation.

</details>


### [227] [SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674)
*Haozhan Li,Yuxin Zuo,Jiale Yu,Yuhao Zhang,Zhaohui Yang,Kaiyan Zhang,Xuekai Zhu,Yuchen Zhang,Tianxing Chen,Ganqu Cui,Dehui Wang,Dingxiang Luo,Yuchen Fan,Youbang Sun,Jia Zeng,Jiangmiao Pang,Shanghang Zhang,Yu Wang,Yao Mu,Bowen Zhou,Ning Ding*

Main category: cs.RO

TL;DR: This paper introduces SimpleVLA-RL, an RL framework for Vision-Language-Action (VLA) models, which improves long-horizon action planning and generalization while reducing reliance on large-scale human-operated datasets.


<details>
  <summary>Details</summary>
Motivation: The scarcity and high cost of human-operated robotic trajectories for supervised fine-tuning and the need to address limited generalization of VLA models in tasks involving distribution shifts.

Method: The authors propose SimpleVLA-RL, an RL framework with features like VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation.

Result: SimpleVLA-RL achieves state-of-the-art performance on LIBERO, outperforms baseline $\pi_0$ on RoboTwin 1.0 & 2.0, and enhances exploration strategies. It also discovers previously unseen patterns through a phenomenon called "pushcut."

Conclusion: SimpleVLA-RL demonstrates significant advantages over traditional supervised fine-tuning, enabling robust generalization and more effective long-horizon action planning for VLA models.

Abstract: Vision-Language-Action (VLA) models have recently emerged as a powerful
paradigm for robotic manipulation. Despite substantial progress enabled by
large-scale pretraining and supervised fine-tuning (SFT), these models face two
fundamental challenges: (i) the scarcity and high cost of large-scale
human-operated robotic trajectories required for SFT scaling, and (ii) limited
generalization to tasks involving distribution shift. Recent breakthroughs in
Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can
dramatically enhance step-by-step reasoning capabilities, raising a natural
question: Can RL similarly improve the long-horizon step-by-step action
planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL
framework tailored for VLA models. Building upon veRL, we introduce
VLA-specific trajectory sampling, scalable parallelization, multi-environment
rendering, and optimized loss computation. When applied to OpenVLA-OFT,
SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$
on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce.
SimpleVLA-RL not only reduces dependence on large-scale data and enables robust
generalization, but also remarkably surpasses SFT in real-world tasks.
Moreover, we identify a novel phenomenon ``pushcut'' during RL training,
wherein the policy discovers previously unseen patterns beyond those seen in
the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [228] [Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2509.08843)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: This paper showcases the Python 'glob' module as a foundational tool for pattern-based file access in computational research, emphasizing its utility in scalable workflows.


<details>
  <summary>Details</summary>
Motivation: Pattern-based file access is often under-documented, and this paper seeks to demonstrate how the Python glob module can address this gap by serving as a versatile tool across disciplines.

Method: The authors provide use cases, practical examples, and integration strategies for leveraging the glob module in Python, using libraries like pandas, scikit-learn, and matplotlib.

Result: Examples illustrate how the glob module facilitates efficient file traversal, improves integration into analytical workflows, and supports reproducible research workflows.

Conclusion: The paper positions the Python glob module as a key methodological resource for file pattern matching, encouraging standardized use in Python-based computational research.

Abstract: Pattern-based file access is a fundamental but often under-documented aspect
of computational research. The Python glob module provides a simple yet
powerful way to search, filter, and ingest files using wildcard patterns,
enabling scalable workflows across disciplines. This paper introduces glob as a
versatile tool for data science, business analytics, and artificial
intelligence applications. We demonstrate use cases including large-scale data
ingestion, organizational data analysis, AI dataset construction, and
reproducible research practices. Through concrete Python examples with widely
used libraries such as pandas,scikit-learn, and matplotlib, we show how glob
facilitates efficient file traversal and integration with analytical pipelines.
By situating glob within the broader context of reproducible research and data
engineering, we highlight its role as a methodological building block. Our goal
is to provide researchers and practitioners with a concise reference that
bridges foundational concepts and applied practice, making glob a default
citation for file pattern matching in Python-based research workflows.

</details>


### [229] [A Systematic Mapping Study on Chatbots in Programming Education](https://arxiv.org/abs/2509.08857)
*Marcelino Garcia,Renato Garcia,Arthur Parizotto,Andre Mendes,Pedro Valle,Ricardo Vilela,Renato Balancieri,Williamson Silva*

Main category: cs.SE

TL;DR: A study analyzed 54 out of 3,216 publications on educational chatbots in programming education, revealing their focus on teaching Python and basic programming concepts.


<details>
  <summary>Details</summary>
Motivation: To explore how educational chatbots are developed and applied for programming education, especially in introductory contexts.

Method: Conducted a Systematic Mapping Study (SMS) analyzing 54 publications based on chatbot types, programming languages, educational content, interaction models, and application contexts.

Result: Predominance of Python-focused chatbots that teach fundamental programming concepts using diverse pedagogical approaches and architectures.

Conclusion: Trends in chatbot usage for programming education were identified, offering insights for creating innovative educational tools and addressing literary gaps.

Abstract: Educational chatbots have gained prominence as support tools for teaching
programming, particularly in introductory learning contexts. This paper
presents a Systematic Mapping Study (SMS) that investigated how such agents
have been developed and applied in programming education. From an initial set
of 3,216 publications, 54 studies were selected and analyzed based on five
research subquestions, addressing chatbot types, programming languages used,
educational content covered, interaction models, and application contexts. The
results reveal a predominance of chatbots designed for Python instruction,
focusing on fundamental programming concepts, and employing a wide variety of
pedagogical approaches and technological architectures. In addition to
identifying trends and gaps in the literature, this study provides insights to
inform the development of new educational tools for programming instruction.

</details>


### [230] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: The paper introduces GeoJSON Agents, a multi-agent LLM framework, using Function Calling and Code Generation techniques to automate GIS tasks with improved performance and scalability.


<details>
  <summary>Details</summary>
Motivation: LLMs face limitations in handling GIS tasks due to the complexity of spatial data processing and analysis.

Method: The framework uses Planner agents to convert natural language tasks into GeoJSON commands and Worker agents to process spatial data via function APIs or Python code generation.

Result: The Code Generation approach achieved 97.14% accuracy, outperforming Function Calling (85.71%) and a general-purpose model (48.57%), demonstrating flexibility and stability advantages.

Conclusion: This study establishes a novel LLM-based multi-agent framework for GeoJSON tasks, systematically evaluating two enhancement techniques and offering insights for enhancing GeoAI performance.

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [231] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: TraceRAG is a retrieval-augmented generation framework leveraging large language models to analyze and identify malware in Android applications with high accuracy and human-readable explanations.


<details>
  <summary>Details</summary>
Motivation: Traditional malware detection methods struggle with deeply concealed malicious behaviors in apps and fail to provide understandable justifications for detected threats. This gap necessitates more robust, explainable detection techniques.

Method: The proposed TraceRAG framework uses large language models to generate summaries of method-level code snippets that are stored in a vector database. During query time, relevant code snippets are retrieved for inspection based on behavioral questions, culminating in human-readable reports that correlate detected behaviors with code.

Result: TraceRAG achieves a malware detection accuracy of 96% and behavior identification accuracy of 83.81%, validated by VirusTotal scans and manual assessments. Experts found the generated reports to be practically useful.

Conclusion: TraceRAG effectively bridges the gap between explainable AI and malware analysis, offering a highly accurate and comprehensible solution for detecting and analyzing malicious behaviors in Android applications.

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [232] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: The paper introduces an energy efficiency benchmark for Large Language Models (LLMs) under real-world usage conditions.


<details>
  <summary>Details</summary>
Motivation: Rising deployment and use of LLMs are impacting climate due to substantial energy consumption, necessitating awareness and improved energy efficiency information.

Method: Proposed a benchmark (LLM Efficiency Benchmark) using vLLM backend, evaluating model size, architecture, and concurrent requests' impact on inference efficiency.

Result: Showed that practical energy efficiency benchmarks can offer insights that align more closely with real-world conditions.

Conclusion: The benchmark aids developers in creating more energy-efficient and sustainable AI systems.

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [233] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: CLARA is an open-source browser extension designed to assist with code comprehension, refactoring, and quality detection using an inference model, evaluated for usability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing tools for understanding and analyzing open-source codebases require extensive setup, lack context-awareness, and demand significant manual intervention, making them inefficient.

Method: The authors developed CLARA, a tool using state-of-the-art inference models, and conducted qualitative evaluation using datasets as well as a user study with 10 developers and researchers.

Result: CLARA was found to be accurate, usable, and practical for code-related tasks during evaluation by developers and researchers.

Conclusion: CLARA enhances code comprehension and analysis by addressing gaps in existing tools. It is open-source and demonstrated to be effective and user-friendly.

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [234] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: This paper introduces ReDef, a high-confidence dataset for software defect prediction based on revert commits, evaluates pre-trained language models' reasoning on code modifications, and uncovers limitations in semantic understanding.


<details>
  <summary>Details</summary>
Motivation: To address the issues of noisy labels and low precision in bug-inducing commit identification in software defect prediction datasets.

Method: Construct a reliable defect dataset using revert commits and post-hoc history validation, filter ambiguous cases via GPT-assisted triage, evaluate PLMs using five encoding strategies, and perform counterfactual tests.

Result: ReDef includes 3,164 defective and 10,268 clean modifications with more reliable labeling than existing datasets. Compact diff-style encodings outperform whole-function formats for PLMs, exposing reliance on superficial cues under counterfactual tests.

Conclusion: Current pre-trained language models show limited ability to understand code modifications semantically, relying on superficial patterns instead.

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [235] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: The paper explores a methodology to integrate Large Language Models (LLMs) with traditional software engineering practices, specifically Scenario-Based Programming (SBP), to enhance software development reliability and correctness. A Connect4 agent built using this approach demonstrates promising results.


<details>
  <summary>Details</summary>
Motivation: LLMs offer significant benefits in software development, such as reduced development time and creative code suggestions. However, their tendency to produce confident but incorrect outputs poses risks, necessitating a structured approach to leverage their potential reliably.

Method: The authors propose combining LLMs with the Scenario-Based Programming (SBP) paradigm, allowing developers to incorporate domain knowledge, inspect outputs, and verify program properties. This integration was tested through a case study involving the development of a Connect4 game.

Result: The methodology successfully produced a Connect4 agent capable of defeating strong existing agents. Additionally, some aspects of the agent's correctness were formally verified, demonstrating the approach's effectiveness.

Conclusion: Integrating LLMs with SBP enhances the software development process by reducing errors and improving verification. The proposed methodology shows promise in creating reliable and high-performing software solutions while shedding light on its user-friendliness.

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [236] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: The paper investigates history alterations in Git repositories, identifying potential security and governance concerns, and introduces the GitHistorian tool for detecting such modifications.


<details>
  <summary>Details</summary>
Motivation: To evaluate the impact of Git history alterations on project integrity, reproducibility, and security, particularly when affecting public branches.

Method: Analyzed 111 million Git repositories archived by Software Heritage to identify and categorize history alterations. Conducted case studies on license changes and secret removal.

Result: Found 1.22 million repositories containing history alterations, with 8.7 million rewritten histories. Revealed recurrent issues like license retroactive modifications and secret removal practices.

Conclusion: History alterations pose governance and security risks. The GitHistorian tool helps developers identify such changes to ensure better project management and security practices.

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [237] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: CodeBERT-based deep learning models were evaluated for vulnerability detection across industrial and open-source software. Results indicate domain-specific limitations and class imbalance strategies proved beneficial. A CI/CD-integrated tool called AI-DO was developed for real-world use and assessed positively through surveys.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address gaps in transitioning deep learning vulnerability detection models from academia to industry, focusing on trustworthiness, performance, integration challenges, and applicability to real-world workflows.

Method: Performance of CodeBERT was evaluated for vulnerability detection in cross-domain settings (industrial versus open-source). Additionally, a CI/CD-integrated recommender system called AI-DO was developed using fine-tuned CodeBERT. A survey with IT professionals was conducted to assess usability.

Result: CodeBERT models yielded strong performance in detecting vulnerabilities within their trained domain. Models trained on open-source data performed better on industrial data under certain strategies like undersampling to handle class imbalance.

Conclusion: Deep learning models such as fine-tuned CodeBERT can be adapted effectively for industrial vulnerability detection, especially when integrated into existing workflows like CI/CD pipelines. However, domain-specific training data plays a crucial role in model performance.

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [238] [ORCA: Unveiling Obscure Containers In The Wild](https://arxiv.org/abs/2509.09322)
*Jacopo Bufalino,Agathe Blaise,Stefano Secci*

Main category: cs.SE

TL;DR: The paper identifies limitations in existing Software Composition Analysis (SCA) tools for analyzing obscure container images and proposes an improved solution, ORCA, which achieves superior results.


<details>
  <summary>Details</summary>
Motivation: The growing reliance on open-source libraries and containerized environments in software development carries risks of security vulnerabilities due to outdated or modified components, highlighting the need for robust SCA tools.

Method: The authors analyzed 600 popular containers to assess the limitations of cloud-based and open-source SCA tools with obscure images and designed ORCA, an obscuration-resilient container analysis methodology.

Result: ORCA was found to improve file coverage by a median of 40% over existing tools like Docker Scout and Syft when analyzing obscure containers.

Conclusion: ORCA effectively addresses the limitations of current SCA tools, providing a more reliable solution for analyzing containerized environments and improving security in production workflows.

Abstract: Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.

</details>


### [239] [LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering](https://arxiv.org/abs/2509.09614)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench is a novel benchmark for evaluating long-context language models (LLMs) in complex software development scenarios, focusing on their ability to handle entire codebases and multi-file reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the lack of evaluation tools for long-context LLMs, which struggle with understanding large-scale software systems beyond individual functions or short-context tasks.

Method: LoCoBench systematically generates 8,000 scenarios across 10 programming languages with context lengths of 10K-1M tokens, covering 8 task categories and featuring a 5-phase pipeline for scenario creation. It introduces 17 evaluation metrics combined into a LoCoBench Score (LCBS).

Result: State-of-the-art long-context LLMs were evaluated, highlighting significant performance gaps and the challenges in developing models capable of understanding large-scale codebases.

Conclusion: Long-context language models require substantial advancements to achieve effective understanding and reasoning in realistic software development environments. LoCoBench provides a robust framework to benchmark progress in this area.

Abstract: The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.

</details>


### [240] [I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection](https://arxiv.org/abs/2509.09630)
*Zhenguang Liu,Lixun Ma,Zhongzheng Mu,Chengkun Wei,Xiaojun Xu,Yingying Jiao,Kui Ren*

Main category: cs.SE

TL;DR: This paper introduces SmartDetector, an approach to detect similarity between smart contract functions using explainable methods.


<details>
  <summary>Details</summary>
Motivation: The high reuse of open-source smart contract code aids efficiency but accelerates bug propagation, with limited tools available for precise similarity detection.

Method: SmartDetector dissects ASTs of functions into statement trees to compare at a statement level, using a classifier optimized through a cosine-wise diffusion process.

Result: SmartDetector achieves an average 14.01% improvement in F1-score over current methods, with an average F1-score of 95.88% in experiments on large datasets.

Conclusion: SmartDetector provides interpretable and effective detection of similar smart contract functions, addressing limitations in existing methods with significant performance improvements.

Abstract: Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [241] [Path to Intelligence: Measuring Similarity between Human Brain and Large Language Model Beyond Language Task](https://arxiv.org/abs/2509.08831)
*Doai Ngo,Mingxuan Sun,Zhengji Zhang,Ashwin G Ramayya,Mark Schnitzer,Zhe Zhao*

Main category: q-bio.NC

TL;DR: The paper explores the similarities between human brain activity and the internal states of large language models (LLMs) during sensory-motor tasks, suggesting that LLMs can approximate human neurophysical behavior.


<details>
  <summary>Details</summary>
Motivation: The authors aim to investigate whether LLM internal states exhibit similarities to human brain activity, particularly outside language-specific tasks, and assess LLMs' potential in studying neuroscience topics.

Method: Researchers translated a sensory-motor task into natural language for LLMs, extracted their hidden states during key steps, and compared these states to human intracranial EEG signals to find correlations.

Result: The study identified linear mappings between LLM reactions and human neural activity during sensory-motor tasks, demonstrating shared patterns of behavior between the two systems.

Conclusion: LLMs show potential in aiding neuroscience research, as they can mimic human neurophysical responses beyond language domains when structured appropriately for tasks.

Abstract: Large language models (LLMs) have demonstrated human-like abilities in
language-based tasks. While language is a defining feature of human
intelligence, it emerges from more fundamental neurophysical processes rather
than constituting the basis of intelligence itself. In this work, we study the
similarity between LLM internal states and human brain activity in a
sensory-motor task rooted in anticipatory and visuospatial behavior. These
abilities are essential for cognitive performance that constitute human
intelligence. We translate the sensory-motor task into natural language in
order to replicate the process for LLMs. We extract hidden states from
pre-trained LLMs at key time steps and compare them to human intracranial EEG
signals. Our results reveal that LLM-derived reactions can be linearly mapped
onto human neural activity. These findings suggest that LLMs, with a simple
natural language translation to make them understand temporal-relevant tasks,
can approximate human neurophysical behavior in experiments involving sensory
stimulants. In all, our contribution is two-fold: (1) We demonstrate similarity
between LLM and human brain activity beyond language-based tasks. (2) We
demonstrate that with such similarity, LLMs could help us understand human
brains by enabling us to study topics in neuroscience that are otherwise
challenging to tackle.

</details>


### [242] [A novel cost-effective fabrication of a flexible neural probe for brain signal recording](https://arxiv.org/abs/2509.09213)
*Alireza Irandoost,Amirreza Bahramani,Roya Mohajeri,Faezeh Shahdost-Fard,Ali Ghazizadeh,Mehdi Fardmanesh*

Main category: q-bio.NC

TL;DR: The study presents a cost-effective, flexible neural probe using polyimide film, gold, and SU-8 materials, tested successfully for neural signal recording with zebra finch models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop a biocompatible, implantable neural probe using cost-effective materials and techniques to improve neural signal recording while being suitable for commercial applications.

Method: The process uses microfabrication with polyimide film (Kapton) as the substrate, gold for electrodes and tracks, SU-8 for isolation and stiffness, followed by validation using electrochemical spectroscopy and in vivo neural signal recording.

Result: The neural probe exhibited low impedance and successfully recorded local field potential signals, validated against commercial electrodes using auditory stimuli on zebra finches.

Conclusion: The probe's biocompatibility and effective performance suggest its potential for medical applications and commercialization of flexible neural implantable devices.

Abstract: This study introduces a novel, flexible, and implantable neural probe using a
cost-effective microfabrication process based on a thin polyimide film.
Polyimide film, known as Kapton, serves as a flexible substrate for
microelectrodes, conductive tracks, and contact pads of the probe, which are
made from a thin film of gold (Au). SU-8 is used to cover the corresponding
tracks for electrical isolation and to increase the stiffness of the probe for
better implantation. To evaluate the performance of the fabricated probe,
electrochemical impedance spectroscopy (EIS) and artificial neural signal
recording have been used to characterize its properties. The microelectrode
dimensions have been carefully chosen to provide low impedance characteristics,
which are necessary for acquiring local field potential (LFP) signals. The in
vivo LFP data have been obtained from a male zebra finch presented with
auditory stimuli. By properly filtering the extracellular recordings and
analyzing the data, the obtained results have been validated by comparing them
with the signals acquired with a commercial neural electrode. Due to the use of
Kapton, SU-8, and Au materials with non-toxic and adaptable properties in the
body environment, the fabricated neural probe is considered a promising
biocompatible implantable neural probe that may pave the way for the
fabrication of other neural implantable devices with commercial aims.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [243] [Scalable extensions to given-data Sobol' index estimators](https://arxiv.org/abs/2509.09078)
*Teresa Portone,Bert Debusschere,Samantha Yang,Emiliano Islas-Quinones,T. Patrick Xiao*

Main category: stat.ML

TL;DR: This paper introduces extensions to the Sobol' index computation methods for variance-based sensitivity analysis to handle extremely large models efficiently.


<details>
  <summary>Details</summary>
Motivation: Current Sobol' index methods face limitations in applying to models with a vast number of inputs and preclude processing nonstandard input distributions or maintaining all input-output evaluations in memory.

Method: The paper proposes a general definition for Sobol' index estimation with arbitrary partitions, a streaming algorithm for handling input-output samples in batches, and a heuristic for filtering noise-affected indices.

Result: The proposed methods overcome memory constraints, demonstrate reduced bias compared to earlier equiprobable partition methods, and achieve comparable accuracy and runtimes.

Conclusion: The extensions make Sobol' index computation feasible for large-scale models like neural networks ($>10^4$ inputs) with improved efficiency and reliability.

Abstract: Given-data methods for variance-based sensitivity analysis have significantly
advanced the feasibility of Sobol' index computation for computationally
expensive models and models with many inputs. However, the limitations of
existing methods still preclude their application to models with an extremely
large number of inputs. In this work, we present practical extensions to the
existing given-data Sobol' index method, which allow variance-based sensitivity
analysis to be efficiently performed on large models such as neural networks,
which have $>10^4$ parameterizable inputs. For models of this size, holding all
input-output evaluations simultaneously in memory -- as required by existing
methods -- can quickly become impractical. These extensions also support
nonstandard input distributions with many repeated values, which are not
amenable to equiprobable partitions employed by existing given-data methods.
  Our extensions include a general definition of the given-data Sobol' index
estimator with arbitrary partition, a streaming algorithm to process
input-output samples in batches, and a heuristic to filter out small indices
that are indistinguishable from zero indices due to statistical noise. We show
that the equiprobable partition employed in existing given-data methods can
introduce significant bias into Sobol' index estimates even at large sample
sizes and provide numerical analyses that demonstrate why this can occur. We
also show that our streaming algorithm can achieve comparable accuracy and
runtimes with lower memory requirements, relative to current methods which
process all samples at once. We demonstrate our novel developments on two
application problems in neural network modeling.

</details>


### [244] [Global Optimization of Stochastic Black-Box Functions with Arbitrary Noise Distributions using Wilson Score Kernel Density Estimation](https://arxiv.org/abs/2509.09238)
*Thorbjørn Mosekjær Iversen,Lars Carøe Sørensen,Simon Faarvang Mathiesen,Henrik Gordon Petersen*

Main category: stat.ML

TL;DR: The paper explores using the Wilson Score Kernel Density Estimator (WS-KDE) for Bayesian optimization of stochastic black-box functions, demonstrating its efficacy in simulations and vibrational part feeder design.


<details>
  <summary>Details</summary>
Motivation: Optimizing stochastic black-box functions in robotics is challenging due to their time-expensive evaluations and susceptibility to unmeasurable disturbances.

Method: The authors employ the Wilson Score Kernel Density Estimator (WS-KDE) to provide reliable confidence bounds for Bayesian optimization, effective for functions confined to [0,1], without requiring extensive evaluations or disturbance modeling.

Result: WS-KDE is shown to be a robust confidence estimator for Bayesian optimization, extending its applicability for stable global optimization across more diverse cost functions.

Conclusion: The study concludes that WS-KDE can enhance Bayesian optimization by providing stable and informative confidence bounds, making it suitable for a wider array of stochastic functions.

Abstract: Many optimization problems in robotics involve the optimization of
time-expensive black-box functions, such as those involving complex simulations
or evaluation of real-world experiments. Furthermore, these functions are often
stochastic as repeated experiments are subject to unmeasurable disturbances.
Bayesian optimization can be used to optimize such methods in an efficient
manner by deploying a probabilistic function estimator to estimate with a given
confidence so that regions of the search space can be pruned away.
Consequently, the success of the Bayesian optimization depends on the function
estimator's ability to provide informative confidence bounds. Existing function
estimators require many function evaluations to infer the underlying confidence
or depend on modeling of the disturbances. In this paper, it is shown that the
confidence bounds provided by the Wilson Score Kernel Density Estimator
(WS-KDE) are applicable as excellent bounds to any stochastic function with an
output confined to the closed interval [0;1] regardless of the distribution of
the output. This finding opens up the use of WS-KDE for stable global
optimization on a wider range of cost functions. The properties of WS-KDE in
the context of Bayesian optimization are demonstrated in simulation and applied
to the problem of automated trap design for vibrational part feeders.

</details>


### [245] [Low-degree lower bounds via almost orthonormal bases](https://arxiv.org/abs/2509.09353)
*Alexandra Carpentier,Simone Maria Giancola,Christophe Giraud,Nicolas Verzelen*

Main category: stat.ML

TL;DR: The paper focuses on constructing almost orthonormal polynomial bases for random graph models to better understand statistical-computational gaps and optimize low-degree bounds in high-dimensional testing problems.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for analyzing statistical-computational gaps in detection problems fail for estimation tasks or complex testing scenarios with planted structures, necessitating new techniques.

Method: The authors propose constructing a nearly orthonormal basis of polynomials under specific statistical regimes in random graph models, targeting areas where statistical-computational gaps emerge.

Result: The new basis provides a direct method for low-degree lower bounds, allows identification of optimal low-degree criterion polynomials, and recovers known bounds while establishing new ones for various models.

Conclusion: This approach simplifies proving statistical-computational gaps and has algorithmic implications by revealing the design of optimal polynomial-time methods.

Abstract: Low-degree polynomials have emerged as a powerful paradigm for providing
evidence of statistical-computational gaps across a variety of high-dimensional
statistical models [Wein25]. For detection problems -- where the goal is to
test a planted distribution $\mathbb{P}'$ against a null distribution
$\mathbb{P}$ with independent components -- the standard approach is to bound
the advantage using an $\mathbb{L}^2(\mathbb{P})$-orthonormal family of
polynomials. However, this method breaks down for estimation tasks or more
complex testing problems where $\mathbb{P}$ has some planted structures, so
that no simple $\mathbb{L}^2(\mathbb{P})$-orthogonal polynomial family is
available. To address this challenge, several technical workarounds have been
proposed [SW22,SW25], though their implementation can be delicate. In this
work, we propose a more direct proof strategy. Focusing on random graph models,
we construct a basis of polynomials that is almost orthonormal under
$\mathbb{P}$, in precisely those regimes where statistical-computational gaps
arise. This almost orthonormal basis not only yields a direct route to
establishing low-degree lower bounds, but also allows us to explicitly identify
the polynomials that optimize the low-degree criterion. This, in turn, provides
insights into the design of optimal polynomial-time algorithms. We illustrate
the effectiveness of our approach by recovering known low-degree lower bounds,
and establishing new ones for problems such as hidden subcliques, stochastic
block models, and seriation models.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [246] [Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems](https://arxiv.org/abs/2509.09204)
*Chin Yuen Kwok,Jia Qi Yip,Zhen Qiu,Chi Hung Chi,Kwok Yan Lam*

Main category: cs.SD

TL;DR: The paper tackles the limitations of current Audio Deepfake Detection (ADD) evaluation methods, proposing a novel framework that incorporates diverse bona fide datasets for more balanced and realistic assessments.


<details>
  <summary>Details</summary>
Motivation: Current ADD evaluation methods overly favor synthesizers with more samples and lack diversity in bona fide speech, thus failing to simulate real-world conditions effectively.

Method: The authors propose a 'bona fide cross-testing' approach, which includes a diverse set of bona fide datasets and calculates aggregated EERs for balanced and robust evaluation.

Result: They benchmarked over 150 synthesizers across 9 different bona fide speech types and released a new dataset for further research.

Conclusion: The proposed framework provides a more realistic and balanced assessment method, improving robustness and interpretability in the evaluation of ADD models.

Abstract: Audio deepfake detection (ADD) models are commonly evaluated using datasets
that combine multiple synthesizers, with performance reported as a single Equal
Error Rate (EER). However, this approach disproportionately weights
synthesizers with more samples, underrepresenting others and reducing the
overall reliability of EER. Additionally, most ADD datasets lack diversity in
bona fide speech, often featuring a single environment and speech style (e.g.,
clean read speech), limiting their ability to simulate real-world conditions.
To address these challenges, we propose bona fide cross-testing, a novel
evaluation framework that incorporates diverse bona fide datasets and
aggregates EERs for more balanced assessments. Our approach improves robustness
and interpretability compared to traditional evaluation methods. We benchmark
over 150 synthesizers across nine bona fide speech types and release a new
dataset to facilitate further research at
https://github.com/cyaaronk/audio_deepfake_eval.

</details>


### [247] [DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech](https://arxiv.org/abs/2509.09631)
*Ngoc-Son Nguyen,Hieu-Nghia Huynh-Nguyen,Thanh V. T. Tran,Truong-Son Hy,Van Nguyen*

Main category: cs.SD

TL;DR: DiFlow-TTS introduces a novel approach in zero-shot Text-to-Speech synthesis by utilizing purely Discrete Flow Matching for fast and high-quality speech generation.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot TTS systems face challenges such as slow inference speed and repetition artifacts while attempting to synthesize high-quality speech mimicking unseen speakers. Existing approaches do not fully utilize discrete representations in flow-matching methods.

Method: DiFlow-TTS employs a purely Discrete Flow Matching mechanism, modeling factorized speech attributes through distinct heads for prosody and acoustic details. The model utilizes in-context learning by conditioning the synthesis on textual and prosodic/acoustic attributes extracted from reference speech.

Result: The model achieves improved performance across metrics like speech naturalness, prosody, and speaker style mimicry. Additionally, it offers faster synthesis, generating speech up to 25.8 times quicker than existing baselines.

Conclusion: DiFlow-TTS establishes a novel, efficient, and high-performing method for zero-shot TTS, leveraging discrete generative modeling with low-latency and compact architecture advancements.

Abstract: Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that
mimics the voice of an unseen speaker using only a short reference sample,
requiring not only speaker adaptation but also accurate modeling of prosodic
attributes. Recent approaches based on language models, diffusion, and flow
matching have shown promising results in zero-shot TTS, but still suffer from
slow inference and repetition artifacts. Discrete codec representations have
been widely adopted for speech synthesis, and recent works have begun to
explore diffusion models in purely discrete settings, suggesting the potential
of discrete generative modeling for speech synthesis. However, existing
flow-matching methods typically embed these discrete tokens into a continuous
space and apply continuous flow matching, which may not fully leverage the
advantages of discrete representations. To address these challenges, we
introduce DiFlow-TTS, which, to the best of our knowledge, is the first model
to explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS
explicitly models factorized speech attributes within a compact and unified
architecture. It leverages in-context learning by conditioning on textual
content, along with prosodic and acoustic attributes extracted from a reference
speech, enabling effective attribute cloning in a zero-shot setting. In
addition, the model employs a factorized flow prediction mechanism with
distinct heads for prosody and acoustic details, allowing it to learn
aspect-specific distributions. Experimental results demonstrate that DiFlow-TTS
achieves promising performance in several key metrics, including naturalness,
prosody, preservation of speaker style, and energy control. It also maintains a
compact model size and achieves low-latency inference, generating speech up to
25.8 times faster than the latest existing baselines.

</details>


### [248] [Adaptive Knowledge Distillation using a Device-Aware Teacher for Low-Complexity Acoustic Scene Classification](https://arxiv.org/abs/2509.09262)
*Seung Gyu Jeong,Seong Eun Kim*

Main category: cs.SD

TL;DR: The paper presents a system for acoustic scene classification with low complexity and device robustness using a knowledge distillation framework and device-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of low computational complexity and robust performance on both seen and unseen devices in acoustic scene classification.

Method: The approach uses a knowledge distillation framework with a CP-MobileNet student learning from two teacher models, including a specialized teacher with a DAFA loss for device robustness. Final fine-tuning is performed using test-time device labels.

Result: The system achieves a 57.93% accuracy on the development set, outperforming the official baseline, especially on unseen devices.

Conclusion: The proposed method is effective in delivering significant performance improvements while meeting low complexity and device robustness requirements.

Abstract: In this technical report, we describe our submission for Task 1,
Low-Complexity Device-Robust Acoustic Scene Classification, of the DCASE 2025
Challenge. Our work tackles the dual challenges of strict complexity
constraints and robust generalization to both seen and unseen devices, while
also leveraging the new rule allowing the use of device labels at test time.
Our proposed system is based on a knowledge distillation framework where an
efficient CP-MobileNet student learns from a compact, specialized two-teacher
ensemble. This ensemble combines a baseline PaSST teacher, trained with
standard cross-entropy, and a 'generalization expert' teacher. This expert is
trained using our novel Device-Aware Feature Alignment (DAFA) loss, adapted
from prior work, which explicitly structures the feature space for device
robustness. To capitalize on the availability of test-time device labels, the
distilled student model then undergoes a final device-specific fine-tuning
stage. Our proposed system achieves a final accuracy of 57.93\% on the
development set, demonstrating a significant improvement over the official
baseline, particularly on unseen devices.

</details>


### [249] [Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates](https://arxiv.org/abs/2509.09550)
*Harry Julia,Rachel Beeson,Lohith Konathala,Johanna Ulin,Jiameng Gao*

Main category: cs.SD

TL;DR: NeuCodec, an FSQ-based neural audio codec, demonstrates robust performance in noisy channels and is shown to maintain reconstruction quality despite encoding variations.


<details>
  <summary>Details</summary>
Motivation: Exploring a new FSQ-based approach in neural audio codecs due to baked-in redundancy, aiming to enhance robustness for transmission over noisy channels.

Method: Encoder distillation experiments to compare encoding approaches and noise simulation tests for bit-level perturbation robustness analysis.

Result: FSQ codecs exhibit superior robustness to noisy transmission compared to RVQ codecs, with encoders creating diverse code sequences while preserving reconstruction quality.

Conclusion: FSQ-based NACs, like NeuCodec, hold promise for applications requiring resilience in noisy environments, leveraging FSQ's inherent capabilities.

Abstract: Neural Audio Codecs (NACs) have become increasingly adopted in speech
processing tasks due to their excellent rate-distortion performance and
compatibility with Large Language Models (LLMs) as discrete feature
representations for audio generation. While most existing codecs rely on
Residual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has
recently emerged as a compelling alternative that simplifies training and
natively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,
and show that FSQ encodes baked-in redundancy which produces an encoding which
is robust when transmitted through noisy channels. First, through an encoder
distillation experiment, we show that two different encoders can learn to
encode identical audio into vastly different code sequences whilst maintaining
comparable reconstruction quality with the same quantizer and decoder. Second,
we demonstrate that FSQ has vastly superior bit-level perturbation robustness
by comparing the performance of RVQ and FSQ codecs when simulating the
transmission of code sequences through a noisy channel.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [250] [Personalized Sleep Prediction via Deep Adaptive Spatiotemporal Modeling and Sparse Data](https://arxiv.org/abs/2509.09018)
*Xueyi Wang,C. J. C.,Lamoth,Elisabeth Wilhelm*

Main category: eess.SP

TL;DR: This paper introduces the AdaST-Sleep model for personalized sleep forecasting, leveraging both spatial and temporal data interactions and demonstrating high predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: To create a personalized and accurate model for sleep forecasting that can assist individuals and healthcare providers in improving well-being.

Method: The model combines convolutional layers (spatial features), recurrent neural networks (temporal data), and a domain classifier to make it generalized across various subjects. Multiple input and prediction window sizes were tested.

Result: The model outperformed four baseline models, achieving an RMSE of 0.282 (seven-day input, one-day predicting) and demonstrated reliable multi-day forecasting.

Conclusion: AdaST-Sleep provides a robust solution for sleep forecasting, effectively using sparse wearable device data and adapting well across different users.

Abstract: A sleep forecast allows individuals and healthcare providers to anticipate
and proactively address factors influencing restful rest, ultimately improving
mental and physical well-being. This work presents an adaptive spatial and
temporal model (AdaST-Sleep) for predicting sleep scores. Our proposed model
combines convolutional layers to capture spatial feature interactions between
multiple features and recurrent neural network layers to handle longer-term
temporal health-related data. A domain classifier is further integrated to
generalize across different subjects. We conducted several experiments using
five input window sizes (3, 5, 7, 9, 11 days) and five predicting window sizes
(1, 3, 5, 7, 9 days). Our approach consistently outperformed four baseline
models, achieving its lowest RMSE (0.282) with a seven-day input window and a
one-day predicting window. Moreover, the method maintained strong performance
even when forecasting multiple days into the future, demonstrating its
versatility for real-world applications. Visual comparisons reveal that the
model accurately tracks both the overall sleep score level and daily
fluctuations. These findings prove that the proposed framework provides a
robust and adaptable solution for personalized sleep forecasting using sparse
data from commercial wearable devices and domain adaptation techniques.

</details>


### [251] [A Masked Representation Learning to Model Cardiac Functions Using Multiple Physiological Signals](https://arxiv.org/abs/2509.08830)
*Seong-A Park,Jong-Eui Chae,Sungdong Kim,Hyung-Chul Lee,Hyun-Lim Yang*

Main category: eess.SP

TL;DR: This paper introduces SNUPHY-M, a self-supervised learning model integrating ECG, PPG, and ABP signals to enhance hemodynamic analysis for clinical applications.


<details>
  <summary>Details</summary>
Motivation: Clinical settings require integrated analysis of multiple physiological signals to accurately monitor hemodynamics and improve patient prognosis.

Method: SNUPHY-M uses self-supervised learning to restore masked ECG, PPG, and ABP signals while extracting enriched physiological features useful for clinical predictions.

Result: The SNUPHY-M model outperformed other supervised and SSL models in clinical prediction tasks utilizing non-invasive signals.

Conclusion: SNUPHY-M represents a significant advancement, enabling multi-modal signal analysis without invasiveness, thus enhancing early diagnosis and management in clinical hemodynamics.

Abstract: In clinical settings, monitoring hemodynamics is crucial for managing patient
prognosis, necessitating the integrated analysis of multiple physiological
signals. While recent research has analyzed single signals such as
electrocardiography (ECG) or photoplethysmography (PPG), there has yet to be a
proposal for an approach that encompasses the complex signal analysis required
in actual clinical scenarios. In this study, we introduce the SNUPHY-M (Seoul
National University hospital PHYsiological signal Masked representation
learning) model extracts physiological features reflecting the electrical,
pressure, and fluid characteristics of the cardiac cycle in the process of
restoring three masked physiological signals based on self-supervised learning
(SSL): ECG, PPG, and arterial blood pressure (ABP) signals. By employing
multiple physical characteristics, the model can extract more enriched features
only using non-invasive signals. We evaluated the model's performance in
clinical downstream tasks such as hypotension, stroke volume, systolic blood
pressure, diastolic blood pressure, and age prediction. Our results showed that
the SNUPHY-M significantly outperformed supervised or SSL models, especially in
prediction tasks using non-invasive signals. To the best of our knowledge,
SNUPHY-M is the first model to apply multi-modal SSL to cardiovascular analysis
involving ECG, PPG, and ABP signals. This approach effectively supports
clinical decision-making and enables precise diagnostics, contributing
significantly to the early diagnosis and management of hemodynamics without
invasiveness.

</details>


### [252] [Deploying AI for Signal Processing education: Selected challenges and intriguing opportunities](https://arxiv.org/abs/2509.08950)
*Jarvis Haupt,Qin Lu,Yanning Shen,Jia Chen,Yue Dong,Dan McCreary,Mehmet Akçakaya,Georgios B. Giannakis*

Main category: eess.SP

TL;DR: The paper discusses how emerging AI tools, like large language models and speech generation technologies, can be applied to improve education, especially in signal processing, while addressing challenges such as fairness, inclusivity, and transparency.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to explore the use of advanced AI tools for meaningful global improvements, particularly in enhancing educational experiences in engineering and signal processing.

Method: The paper employs the development of a "smart textbook" as an example to demonstrate principles of fairness, inclusivity, handling AI hallucinations, and promoting transparency and trustworthiness in AI applications in education.

Result: The proposed "smart textbook" serves as a structured and reliable educational tool, addressing core challenges such as resource efficiency and inclusivity while showcasing the practical value of AI in enhancing educational experiences.

Conclusion: AI has immense potential to advance education, but non-technical concerns like fairness and explainability must be addressed. The paper illustrates how these challenges can be mitigated through its application in engineering education.

Abstract: Powerful artificial intelligence (AI) tools that have emerged in recent years
-- including large language models, automated coding assistants, and advanced
image and speech generation technologies -- are the result of monumental human
achievements. These breakthroughs reflect mastery across multiple technical
disciplines and the resolution of significant technological challenges.
However, some of the most profound challenges may still lie ahead. These
challenges are not purely technical but pertain to the fair and responsible use
of AI in ways that genuinely improve the global human condition. This article
explores one promising application aligned with that vision: the use of AI
tools to facilitate and enhance education, with a specific focus on signal
processing (SP). It presents two interrelated perspectives: identifying and
addressing technical limitations, and applying AI tools in practice to improve
educational experiences. Primers are provided on several core technical issues
that arise when using AI in educational settings, including how to ensure
fairness and inclusivity, handle hallucinated outputs, and achieve efficient
use of resources. These and other considerations -- such as transparency,
explainability, and trustworthiness -- are illustrated through the development
of an immersive, structured, and reliable "smart textbook." The article serves
as a resource for researchers and educators seeking to advance AI's role in
engineering education.

</details>


### [253] [Ultrafast Deep Learning-Based Scatter Estimation in Cone-Beam Computed Tomography](https://arxiv.org/abs/2509.08973)
*Harshit Agrawal,Ari Hietanen,Simo Särkkä*

Main category: eess.SP

TL;DR: This paper explores reducing scatter artifacts in CBCT scans using lightweight deep learning techniques, emphasizing downsampling to balance speed and accuracy, with significant improvements in computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Scatter artifacts degrade CBCT image quality, and current deep learning-based solutions are too resource-intensive for mobile CBCT or edge devices. This study aims to enable deployment in such constrained environments by optimizing resolution.

Method: The paper first analyzed reconstruction errors from down-up sampling of CBCT scatter signals across six resolutions using four interpolation methods. Then, a recent model was trained and evaluated across five resolutions for computational efficiency and performance.

Result: The proposed method reduced FLOPs by 78-fold while achieving comparable performance in MAPE (3.85% vs. 4.42%) and MSE (1.34 × 10^-2 vs. 2.01 × 10^-2). Inference time and GPU memory usage were reduced by factors of 16 and 12, respectively. Consistent performance was demonstrated on simulated and real datasets.

Conclusion: Downsampling plays a critical role in deep learning-based scatter correction. The method's computational efficiency and maintained accuracy make it feasible for resource-constrained devices like mobile CBCT systems.

Abstract: Purpose: Scatter artifacts drastically degrade the image quality of cone-beam
computed tomography (CBCT) scans. Although deep learning-based methods show
promise in estimating scatter from CBCT measurements, their deployment in
mobile CBCT systems or edge devices is still limited due to the large memory
footprint of the networks. This study addresses the issue by applying networks
at varying resolutions and suggesting an optimal one, based on speed and
accuracy.
  Methods: First, the reconstruction error in down-up sampling of CBCT scatter
signal was examined at six resolutions by comparing four interpolation methods.
Next, a recent state-of-the-art method was trained across five image
resolutions and evaluated for the reductions in floating-point operations
(FLOPs), inference times, and GPU memory requirements.
  Results: Reducing the input size and network parameters achieved a 78-fold
reduction in FLOPs compared to the baseline method, while maintaining comarable
performance in terms of mean-absolute-percentage-error (MAPE) and
mean-square-error (MSE). Specifically, the MAPE decreased to 3.85% compared to
4.42%, and the MSE decreased to 1.34 \times 10^{-2} compared to 2.01 \times
10^{-2}. Inference time and GPU memory usage were reduced by factors of 16 and
12, respectively. Further experiments comparing scatter-corrected
reconstructions on a large, simulated dataset and real CBCT scans from water
and Sedentex CT phantoms clearly demonstrated the robustness of our method.
  Conclusion: This study highlights the underappreciated role of downsampling
in deep learning-based scatter estimation. The substantial reduction in FLOPs
and GPU memory requirements achieved by our method enables scatter correction
in resource-constrained environments, such as mobile CBCT and edge devices.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [254] [An Integrated Open Source Software System for the Generation and Analysis of Subject-Specific Blood Flow Simulation Ensembles](https://arxiv.org/abs/2509.09392)
*Simon Leistikow,Thomas Miro,Adrian Kummerländer,Ali Nahardani,Katja Grün,Markus Franz,Verena Hoerr,Mathias J. Krause,Lars Linsen*

Main category: physics.med-ph

TL;DR: The paper introduces an interactive tool merging MRI and CFD for improved blood flow analysis, emphasizing comprehensive hemodynamic studies.


<details>
  <summary>Details</summary>
Motivation: To integrate MRI and CFD for subject-specific hemodynamic analysis, addressing cardiovascular disease diagnostics and parameter investigations.

Method: An open-source visual analysis tool was developed for customizable simulation ensembles, enabling detailed examination of simulations and measurements.

Result: The tool was successfully demonstrated in three real-world use cases, co-evaluated with MRI and CFD experts, which improved its features and usability.

Conclusion: The tool effectively combines CFD and MRI to enhance comprehension of hemodynamic parameters, improving the analysis of hemodynamic biomarkers.

Abstract: Background and Objective: Hemodynamic analysis of blood flow through arteries
and veins is critical for diagnosing cardiovascular diseases, such as aneurysms
and stenoses, and for investigating cardiovascular parameters, such as
turbulence and wall shear stress. For subject-specific analyses, the anatomy
and blood flow of the subject can be captured non-invasively using structural
and 4D Magnetic Resonance Imaging (MRI). Computational Fluid Dynamics (CFD), on
the other hand, can be used to generate blood flow simulations by solving the
Navier-Stokes equations. To generate and analyze subject-specific blood flow
simulations, MRI and CFD have to be brought together.
  Methods: We present an interactive, customizable, and user-oriented visual
analysis tool that assists researchers in both medicine and numerical analysis.
Our open-source tool is applicable to domains such as CFD and MRI, and it
facilitates the analysis of simulation results and medical data, especially in
hemodynamic studies. It enables the creation of simulation ensembles with a
high variety of parameters. Furthermore, it allows for the visual and
analytical examination of simulations and measurements through 2D embeddings of
the similarity space.
  Results: To demonstrate the effectiveness of our tool, we applied it to three
real-world use cases, showcasing its ability to configure simulation ensembles
and analyse blood flow dynamics. We evaluated our example cases together with
MRI and CFD experts to further enhance features and increase the usability.
  Conclusions: By combining the strengths of both CFD and MRI, our tool
provides a more comprehensive understanding of hemodynamic parameters,
facilitating more accurate analysis of hemodynamic biomarkers.

</details>


### [255] [Explainable AI for Accelerated Microstructure Imaging: A SHAP-Guided Protocol on the Connectome 2.0 scanner](https://arxiv.org/abs/2509.09513)
*Quentin Uhl,Tommaso Pavan,Julianna Gerold,Kwok-Shing Chan,Yohan Jun,Shohei Fujita,Aneri Bhatt,Yixin Ma,Qiaochu Wang,Hong-Hsi Lee,Susie Y. Huang,Berkin Bilgic,Ileana Jelescu*

Main category: physics.med-ph

TL;DR: The paper presents a reduced acquisition scheme for the Connectome 2.0 scanner that shortens scan time while maintaining model accuracy for Neurite Exchange Imaging.


<details>
  <summary>Details</summary>
Motivation: To address the problem of long scan times in current gray matter microstructure imaging protocols, thereby enhancing practicality and clinical applicability.

Method: Developed a data-driven framework using explainable AI with a guided recursive feature elimination strategy to optimize an 8-feature protocol, reducing features from 15.

Result: The reduced 8-feature protocol produced parameter estimates and cortical maps comparable to the full protocol, with minimal test-retest variability and estimation errors.

Conclusion: The proposed framework achieves accurate neurite exchange imaging in 14 minutes, improving accessibility and providing a generalized approach for efficient imaging protocols.

Abstract: The diffusion MRI Neurite Exchange Imaging model offers a promising framework
for probing gray matter microstructure by estimating parameters such as
compartment sizes, diffusivities, and inter-compartmental water exchange time.
However, existing protocols require long scan times. This study proposes a
reduced acquisition scheme for the Connectome 2.0 scanner that preserves model
accuracy while substantially shortening scan duration. We developed a
data-driven framework using explainable artificial intelligence with a guided
recursive feature elimination strategy to identify an optimal 8-feature subset
from a 15-feature protocol. The performance of this optimized protocol was
validated in vivo and benchmarked against the full acquisition and alternative
reduction strategies. Parameter accuracy, preservation of anatomical contrast,
and test-retest reproducibility were assessed. The reduced protocol yielded
parameter estimates and cortical maps comparable to the full protocol, with low
estimation errors in synthetic data and minimal impact on test-retest
variability. Compared to theory-driven and heuristic reduction schemes, the
optimized protocol demonstrated superior robustness, reducing the deviation in
water exchange time estimates by over two-fold. In conclusion, this hybrid
optimization framework enables viable imaging of neurite exchange in 14 minutes
without loss of parameter fidelity. This approach supports the broader
application of exchange-sensitive diffusion magnetic resonance imaging in
neuroscience and clinical research, and offers a generalizable method for
designing efficient acquisition protocols in biophysical parameter mapping.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [256] [Physics-informed waveform inversion using pretrained wavefield neural operators](https://arxiv.org/abs/2509.08967)
*Xinquan Huang,Fu Wang,Tariq Alkhalifah*

Main category: physics.geo-ph

TL;DR: The paper introduces a physics-informed FWI framework leveraging neural operators to enhance subsurface model accuracy while ensuring computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of traditional FWI methods, namely low-resolution models due to null spaces, and the high computational cost, especially for real-time applications.

Method: The framework integrates a physics constraint term into the loss function of FWI, improving inversion by reducing noise and artifacts. It leverages neural operators and a physics-informed loss based on wave equation compliance and data matching.

Result: Numerical experiments on OpenFWI and Overthrust models reveal that the proposed method achieves cleaner and more accurate subsurface velocity reconstructions compared to traditional FWI approaches.

Conclusion: The physics-informed neural operator-based FWI framework demonstrates superior performance and efficiency, marking a significant advancement for real-time subsurface applications.

Abstract: Full waveform inversion (FWI) is crucial for reconstructing high-resolution
subsurface models, but it is often hindered, considering the limited data, by
its null space resulting in low-resolution models, and more importantly, by its
computational cost, especially if needed for real-time applications. Recent
attempts to accelerate FWI using learned wavefield neural operators have shown
promise in efficiency and differentiability, but typically suffer from noisy
and unstable inversion performance. To address these limitations, we introduce
a novel physics-informed FWI framework to enhance the inversion in accuracy
while maintaining the efficiency of neural operator-based FWI. Instead of
relying only on the L2 norm objective function via automatic differentiation,
resulting in noisy model reconstruction, we integrate a physics constraint term
in the loss function of FWI, improving the quality of the inverted velocity
models. Specifically, starting with an initial model to simulate wavefields and
then evaluating the loss over how much the resulting wavefield obeys the
physical laws (wave equation) and matches the recorded data, we achieve a
reduction in noise and artifacts. Numerical experiments using the OpenFWI and
Overthrust models demonstrate our method's superior performance, offering
cleaner and more accurate subsurface velocity than vanilla approaches.
Considering the efficiency of the approach compared to FWI, this advancement
represents a significant step forward in the practical application of FWI for
real-time subsurface monitoring.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [257] [PerFairX: Is There a Balance Between Fairness and Personality in Large Language Model Recommendations?](https://arxiv.org/abs/2509.08829)
*Chandan Kumar Sah*

Main category: cs.CY

TL;DR: This paper introduces PerFairX, a framework to measure trade-offs between personalization and demographic fairness in recommender systems with LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing psychological alignment through personality traits and demographic fairness in LLM-based recommender systems.

Method: Developed PerFairX framework to evaluate zero-shot personality-based personalization and fairness, benchmarking ChatGPT and DeepSeek using neutral and personality-sensitive prompts on movie and music datasets.

Result: DeepSeek achieves better psychological alignment but is more sensitive to prompts, while ChatGPT provides stable but less personalized recommendations.

Conclusion: PerFairX offers a benchmark to aid in designing equitable and psychologically aware LLM-based recommender systems for inclusive user-centric applications.

Abstract: The integration of Large Language Models (LLMs) into recommender systems has
enabled zero-shot, personality-based personalization through prompt-based
interactions, offering a new paradigm for user-centric recommendations.
However, incorporating user personality traits via the OCEAN model highlights a
critical tension between achieving psychological alignment and ensuring
demographic fairness. To address this, we propose PerFairX, a unified
evaluation framework designed to quantify the trade-offs between
personalization and demographic equity in LLM-generated recommendations. Using
neutral and personality-sensitive prompts across diverse user profiles, we
benchmark two state-of-the-art LLMs, ChatGPT and DeepSeek, on movie (MovieLens
10M) and music (Last.fm 360K) datasets. Our results reveal that
personality-aware prompting significantly improves alignment with individual
traits but can exacerbate fairness disparities across demographic groups.
Specifically, DeepSeek achieves stronger psychological fit but exhibits higher
sensitivity to prompt variations, while ChatGPT delivers stable yet less
personalized outputs. PerFairX provides a principled benchmark to guide the
development of LLM-based recommender systems that are both equitable and
psychologically informed, contributing to the creation of inclusive,
user-centric AI applications in continual learning contexts.

</details>


### [258] [Deep opacity and AI: A threat to XAI and to privacy protection mechanisms](https://arxiv.org/abs/2509.08835)
*Vincent C. Müller*

Main category: cs.CY

TL;DR: Big data analytics and AI create privacy issues due to opacity, making it harder to ensure informed consent or anonymity.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address privacy threats posed by opacity in AI and big data systems, where judgments and actions lack proper justification.

Method: The author differentiates between three types of opacity and analyzes their implications for privacy and informed consent.

Result: Agents involved in AI and big data cannot adequately justify judgments needed to protect privacy, exacerbating privacy risks.

Conclusion: Big data analytics worsens privacy problems while technical solutions may offer partial remedies to address opacity issues.

Abstract: It is known that big data analytics and AI pose a threat to privacy, and that
some of this is due to some kind of "black box problem" in AI. I explain how
this becomes a problem in the context of justification for judgments and
actions. Furthermore, I suggest distinguishing three kinds of opacity: 1) the
subjects do not know what the system does ("shallow opacity"), 2) the analysts
do not know what the system does ("standard black box opacity"), or 3) the
analysts cannot possibly know what the system might do ("deep opacity"). If the
agents, data subjects as well as analytics experts, operate under opacity, then
these agents cannot provide justifications for judgments that are necessary to
protect privacy, e.g., they cannot give "informed consent", or guarantee
"anonymity". It follows from these points that agents in big data analytics and
AI often cannot make the judgments needed to protect privacy. So I conclude
that big data analytics makes the privacy problems worse and the remedies less
effective. As a positive note, I provide a brief outlook on technical ways to
handle this situation.

</details>


### [259] [Safe and Certifiable AI Systems: Concepts, Challenges, and Lessons Learned](https://arxiv.org/abs/2509.08852)
*Kajetan Schweighofer,Barbara Brune,Lukas Gruber,Simon Schmid,Alexander Aufreiter,Andreas Gruber,Thomas Doms,Sebastian Eder,Florian Mayer,Xaver-Paul Stadlbauer,Christoph Schwald,Werner Zellinger,Bernhard Nessler,Sepp Hochreiter*

Main category: cs.CY

TL;DR: The paper introduces the TÜV AUSTRIA Trusted AI framework, an audit methodology to assess AI systems' functional trustworthiness in compliance with the EU AI Act.


<details>
  <summary>Details</summary>
Motivation: There is a growing demand to assess and certify AI systems for safety, legality, and social acceptance, especially in safety-critical applications.

Method: The paper develops an audit catalog based on three pillars: Secure Software Development, Functional Requirements, and Ethics & Data Privacy. This methodology translates the EU AI Act into specific, testable criteria with a focus on risk-based performance requirements and statistical testing.

Result: The framework defines transparent criteria for auditing AI systems and highlights pitfalls like data leakage, biased data, and lack of distribution drift controls from practical applications.

Conclusion: By aligning with EU standards, the framework offers a structured way to ensure AI systems are legally compliant, trustworthy, and certifiable, with future research directions outlined to enhance its utility.

Abstract: There is an increasing adoption of artificial intelligence in safety-critical
applications, yet practical schemes for certifying that AI systems are safe,
lawful and socially acceptable remain scarce. This white paper presents the
T\"UV AUSTRIA Trusted AI framework an end-to-end audit catalog and methodology
for assessing and certifying machine learning systems. The audit catalog has
been in continuous development since 2019 in an ongoing collaboration with
scientific partners. Building on three pillars - Secure Software Development,
Functional Requirements, and Ethics & Data Privacy - the catalog translates the
high-level obligations of the EU AI Act into specific, testable criteria. Its
core concept of functional trustworthiness couples a statistically defined
application domain with risk-based minimum performance requirements and
statistical testing on independently sampled data, providing transparent and
reproducible evidence of model quality in real-world settings. We provide an
overview of the functional requirements that we assess, which are oriented on
the lifecycle of an AI system. In addition, we share some lessons learned from
the practical application of the audit catalog, highlighting common pitfalls we
encountered, such as data leakage scenarios, inadequate domain definitions,
neglect of biases, or a lack of distribution drift controls. We further discuss
key aspects of certifying AI systems, such as robustness, algorithmic fairness,
or post-certification requirements, outlining both our current conclusions and
a roadmap for future research. In general, by aligning technical best practices
with emerging European standards, the approach offers regulators, providers,
and users a practical roadmap for legally compliant, functionally trustworthy,
and certifiable AI systems.

</details>


### [260] [A vibe coding learning design to enhance EFL students' talking to, through, and about AI](https://arxiv.org/abs/2509.08854)
*David James Woo,Kai Guo,Yangyang Yu*

Main category: cs.CY

TL;DR: This paper piloted the use of vibe coding (natural language-based AI programming) for EFL education, revealing the role of meta-languaging in effective AI interaction and coding outcomes.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore teaching methodologies that integrate AI programming (vibe coding) into EFL education to address writing challenges and improve technology-mediated language learning.

Method: A four-hour workshop was designed using backward principles, where students created applications. Data was collected via worksheets, video recordings, think-aloud protocols, screen recordings, and AI output for a case study analysis.

Result: One student successfully created a functional application aligning with her design, while the other faced technical issues, highlighting varied outcomes due to differences in prompt engineering and AI mental models.

Conclusion: Effective vibe coding instruction requires a focus on meta-languaging scaffolding, structured prompt engineering guidance, fostering discussions on authorship, and enhancing understanding of AI mental models.

Abstract: This innovative practice article reports on the piloting of vibe coding
(using natural language to create software applications with AI) for English as
a Foreign Language (EFL) education. We developed a human-AI meta-languaging
framework with three dimensions: talking to AI (prompt engineering), talking
through AI (negotiating authorship), and talking about AI (mental models of
AI). Using backward design principles, we created a four-hour workshop where
two students designed applications addressing authentic EFL writing challenges.
We adopted a case study methodology, collecting data from worksheets and video
recordings, think-aloud protocols, screen recordings, and AI-generated images.
Contrasting cases showed one student successfully vibe coding a functional
application cohering to her intended design, while another encountered
technical difficulties with major gaps between intended design and actual
functionality. Analysis reveals differences in students' prompt engineering
approaches, suggesting different AI mental models and tensions in attributing
authorship. We argue that AI functions as a beneficial languaging machine, and
that differences in how students talk to, through, and about AI explain vibe
coding outcome variations. Findings indicate that effective vibe coding
instruction requires explicit meta-languaging scaffolding, teaching structured
prompt engineering, facilitating critical authorship discussions, and
developing vocabulary for articulating AI mental models.

</details>


### [261] [Investigating Student Interaction Patterns with Large Language Model-Powered Course Assistants in Computer Science Courses](https://arxiv.org/abs/2509.08862)
*Chang Liu,Loc Hoang,Andrew Stolman,Rene F. Kizilcec,Bo Wu*

Main category: cs.CY

TL;DR: Researchers deployed an LLM-powered course assistant to aid 2,000 students across six computer science courses, discovering that it effectively fills support gaps, but faces pedagogical limitations in higher cognitive tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of providing flexible and timely academic support to students when educators aren't available.

Method: Deployed an LLM-powered course assistant across multiple computer science courses, analyzed interaction data, manually annotated 200 conversations per course, and assessed pedagogical implications using metrics like Bloom's taxonomy.

Result: The system was used significantly during evenings, nights, and by novice learners. Most responses were correct, though higher-order cognitive questions were lacking and follow-up questions were often ignored.

Conclusion: LLMs show promise in addressing support gaps but need educator involvement and advancements in higher-level questioning capabilities to improve pedagogical outcomes.

Abstract: Providing students with flexible and timely academic support is a challenge
at most colleges and universities, leaving many students without help outside
scheduled hours. Large language models (LLMs) are promising for bridging this
gap, but interactions between students and LLMs are rarely overseen by
educators. We developed and studied an LLM-powered course assistant deployed
across multiple computer science courses to characterize real-world use and
understand pedagogical implications. By Spring 2024, our system had been
deployed to approximately 2,000 students across six courses at three
institutions. Analysis of the interaction data shows that usage remains strong
in the evenings and nights and is higher in introductory courses, indicating
that our system helps address temporal support gaps and novice learner needs.
We sampled 200 conversations per course for manual annotation: most sampled
responses were judged correct and helpful, with a small share unhelpful or
erroneous; few responses included dedicated examples. We also examined an
inquiry-based learning strategy: only around 11% of sampled conversations
contained LLM-generated follow-up questions, which were often ignored by
students in advanced courses. A Bloom's taxonomy analysis reveals that current
LLM capabilities are limited in generating higher-order cognitive questions.
These patterns suggest opportunities for pedagogically oriented LLM-based
educational systems and greater educator involvement in configuring prompts,
content, and policies.

</details>


### [262] [Decentralising LLM Alignment: A Case for Context, Pluralism, and Participation](https://arxiv.org/abs/2509.08858)
*Oriane Peter,Kate Devlin*

Main category: cs.CY

TL;DR: The paper critiques current alignment methods of large language models (LLMs) for centralizing control and proposes a decentralized approach based on context, pluralism, and participation, supported with concrete use cases.


<details>
  <summary>Details</summary>
Motivation: Current alignment techniques in LLMs disproportionately reflect normative values of a small reference group, consolidating knowledge governance within influential institutions, and creating potential epistemic injustices.

Method: The authors propose decentralizing LLM alignment using three key principles: context, pluralism, and participation. They ground their framework in specific use cases to illustrate its applicability.

Result: The paper highlights the significance of decentralizing LLM alignment and provides practical examples to illustrate context-specific strategies while addressing the nuances of implementation.

Conclusion: LLM alignment, if guided by decentralization principles, can fight against epistemic injustices and support democratic values, though these methods alone cannot address the larger societal inequalities.

Abstract: Large Language Models (LLMs) alignment methods have been credited with the
commercial success of products like ChatGPT, given their role in steering LLMs
towards user-friendly outputs. However, current alignment techniques
predominantly mirror the normative preferences of a narrow reference group,
effectively imposing their values on a wide user base. Drawing on theories of
the power/knowledge nexus, this work argues that current alignment practices
centralise control over knowledge production and governance within already
influential institutions. To counter this, we propose decentralising alignment
through three characteristics: context, pluralism, and participation.
Furthermore, this paper demonstrates the critical importance of delineating the
context-of-use when shaping alignment practices by grounding each of these
features in concrete use cases. This work makes the following contributions:
(1) highlighting the role of context, pluralism, and participation in
decentralising alignment; (2) providing concrete examples to illustrate these
strategies; and (3) demonstrating the nuanced requirements associated with
applying alignment across different contexts of use. Ultimately, this paper
positions LLM alignment as a potential site of resistance against epistemic
injustice and the erosion of democratic processes, while acknowledging that
these strategies alone cannot substitute for broader societal changes.

</details>


### [263] [Incorporating AI Incident Reporting into Telecommunications Law and Policy: Insights from India](https://arxiv.org/abs/2509.09508)
*Avinash Agarwal,Manisha J. Nene*

Main category: cs.CY

TL;DR: The paper emphasizes the unique risks AI poses to telecommunications, highlighting India's regulatory gaps and offering actionable policy suggestions for addressing AI-specific incidents.


<details>
  <summary>Details</summary>
Motivation: To address the emerging risks of integrating AI into telecommunications that are not covered by existing cybersecurity and data protection laws, especially in jurisdictions without horizontal AI regulations, like India.

Method: The paper provides a typology of telecommunications AI incidents, analyzes India's key digital regulations, identifies regulatory gaps, and explores disclosure barriers and limitations of AI incident repositories.

Result: India's regulations like the Telecommunications Act, CERT-In Rules, and Digital Personal Data Protection Act fail to address specific AI risks such as algorithmic bias and performance degradation, revealing a substantial regulatory gap.

Conclusion: The paper suggests specific policy solutions, including mandatory AI incident reporting and designating a government body to handle such incidents, to improve the integration of AI risk management into existing telecom regulatory frameworks.

Abstract: The integration of artificial intelligence (AI) into telecommunications
infrastructure introduces novel risks, such as algorithmic bias and
unpredictable system behavior, that fall outside the scope of traditional
cybersecurity and data protection frameworks. This paper introduces a precise
definition and a detailed typology of telecommunications AI incidents,
establishing them as a distinct category of risk that extends beyond
conventional cybersecurity and data protection breaches. It argues for their
recognition as a distinct regulatory concern. Using India as a case study for
jurisdictions that lack a horizontal AI law, the paper analyzes the country's
key digital regulations. The analysis reveals that India's existing legal
instruments, including the Telecommunications Act, 2023, the CERT-In Rules, and
the Digital Personal Data Protection Act, 2023, focus on cybersecurity and data
breaches, creating a significant regulatory gap for AI-specific operational
incidents, such as performance degradation and algorithmic bias. The paper also
examines structural barriers to disclosure and the limitations of existing AI
incident repositories. Based on these findings, the paper proposes targeted
policy recommendations centered on integrating AI incident reporting into
India's existing telecom governance. Key proposals include mandating reporting
for high-risk AI failures, designating an existing government body as a nodal
agency to manage incident data, and developing standardized reporting
frameworks. These recommendations aim to enhance regulatory clarity and
strengthen long-term resilience, offering a pragmatic and replicable blueprint
for other nations seeking to govern AI risks within their existing sectoral
frameworks.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [264] [The Role of Community Detection Methods in Performance Variations of Graph Mining Tasks](https://arxiv.org/abs/2509.09045)
*Shrabani Ghosh,Erik Saule*

Main category: cs.SI

TL;DR: The paper explores how the choice of community detection algorithms impacts downstream graph mining tasks, using a systematic evaluation framework.


<details>
  <summary>Details</summary>
Motivation: To understand whether the selection of community detection algorithms significantly affects the performance of downstream graph analysis tasks.

Method: A framework integrating various community detection methods was designed to systematically compare and evaluate their effects on downstream applications such as link prediction and node classification.

Result: The analysis shows that certain community detection algorithms perform better in specific applications, emphasizing the importance of method selection.

Conclusion: Community detection method choices significantly impact downstream task performance; thus, careful selection tailored to application needs is crucial.

Abstract: In real-world scenarios, large graphs represent relationships among entities
in complex systems. Mining these large graphs often containing millions of
nodes and edges helps uncover structural patterns and meaningful insights.
Dividing a large graph into smaller subgraphs facilitates complex system
analysis by revealing local information. Community detection extracts clusters
or communities of graphs based on statistical methods and machine learning
models using various optimization techniques. Structure based community
detection methods are more suitable for applying to graphs because they do not
rely heavily on rich node or edge attribute information. The features derived
from these communities can improve downstream graph mining tasks, such as link
prediction and node classification. In real-world applications, we often lack
ground truth community information. Additionally, there is neither a
universally accepted gold standard for community detection nor a single method
that is consistently optimal across diverse applications. In many cases, it is
unclear how practitioners select community detection methods, and choices are
often made without explicitly considering their potential impact on downstream
tasks. In this study, we investigate whether the choice of community detection
algorithm significantly influences the performance of downstream applications.
We propose a framework capable of integrating various community detection
methods to systematically evaluate their effects on downstream task outcomes.
Our comparative analysis reveals that specific community detection algorithms
yield superior results in certain applications, highlighting that method
selection substantially affects performance.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [265] [HARD: A Performance Portable Radiation Hydrodynamics Code based on FleCSI Framework](https://arxiv.org/abs/2509.08971)
*Julien Loiseau,Hyun Lim,Andrés Yagüe López,Mammadbaghir Baghirzade,Shihab Shahriar Khan,Yoonsoo Kim,Sudarshan Neopane,Alexander Strack,Farhana Taiyebah,Benjamin K. Bergen*

Main category: physics.comp-ph

TL;DR: HARD is an open-source tool designed for high-performance simulations, focusing on compressible hydrodynamics coupled with radiation diffusion, leveraging extensible computation frameworks.


<details>
  <summary>Details</summary>
Motivation: To provide a sustainable, open-source platform capable of handling complex radiation hydrodynamics simulations across diverse computing environments.

Method: The paper introduces HARD, using FleCSI for expressing computational tasks, Kokkos for efficient node-level parallelism, and incorporating verification tests against analytical solutions.

Result: HARD showcases efficient performance across various environments, reproduces canonical verification problems, and supports community-focused development with reliable tools.

Conclusion: HARD emerges as a reliable and flexible simulation tool that facilitates advancements in radiation hydrodynamics research, suitable for diverse computational setups.

Abstract: Hydrodynamics And Radiation Diffusion} (HARD) is an open-source application
for high-performance simulations of compressible hydrodynamics with
radiation-diffusion coupling. Built on the FleCSI (Flexible Computational
Science Infrastructure) framework, HARD expresses its computational units as
tasks whose execution can be orchestrated by multiple back-end runtimes,
including Legion, MPI, and HPX. Node-level parallelism is delegated to Kokkos,
providing a single, portable code base that runs efficiently on laptops, small
homogeneous clusters, and the largest heterogeneous supercomputers currently
available. To ensure scientific reliability, HARD includes a regression-test
suite that automatically reproduces canonical verification problems such as the
Sod and LeBlanc shock tubes and the Sedov blast wave, comparing numerical
solutions against known analytical results. The project is distributed under an
OSI-approved license, hosted on GitHub, and accompanied by reproducible build
scripts and continuous integration workflows. This combination of performance
portability, verification infrastructure, and community-focused development
makes HARD a sustainable platform for advancing radiation hydrodynamics
research across multiple domains.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [266] [Representation-Aware Distributionally Robust Optimization: A Knowledge Transfer Framework](https://arxiv.org/abs/2509.09371)
*Zitao Wang,Nian Si,Molei Liu*

Main category: stat.ME

TL;DR: This paper introduces READ, a new framework for robust learning using Wasserstein distributionally robust optimization (DRO), which adapts to predictive representations to handle distributional shifts effectively.


<details>
  <summary>Details</summary>
Motivation: Existing robust learning methods treat all feature perturbations equally, which may overlook the importance of predictive representations. The authors aim to improve robustness by considering representation-aware alignments.

Method: The authors propose embedding a multidimensional alignment parameter into the transport cost of the Wasserstein DRO problem. They theoretically link this to seminorm regularizations, introduce robust Wasserstein radius selection, analyze the estimator geometry, and propose an optimization algorithm for projection estimation.

Result: READ offers robustness against feature variation without compromising the invariant structure. It generates confidence regions with unique geometries and selects robust estimators with optimized representation structures.

Conclusion: Through simulations and a real-world study, READ proves to be a powerful framework for robust estimation, offering the advantage of representation-awareness and enhanced robustness against distributional shifts.

Abstract: We propose REpresentation-Aware Distributionally Robust Estimation (READ), a
novel framework for Wasserstein distributionally robust learning that accounts
for predictive representations when guarding against distributional shifts.
Unlike classical approaches that treat all feature perturbations equally, READ
embeds a multidimensional alignment parameter into the transport cost, allowing
the model to differentially discourage perturbations along directions
associated with informative representations. This yields robustness to feature
variation while preserving invariant structure. Our first contribution is a
theoretical foundation: we show that seminorm regularizations for linear
regression and binary classification arise as Wasserstein distributionally
robust objectives, thereby providing tractable reformulations of READ and
unifying a broad class of regularized estimators under the DRO lens. Second, we
adopt a principled procedure for selecting the Wasserstein radius using the
techniques of robust Wasserstein profile inference. This further enables the
construction of valid, representation-aware confidence regions for model
parameters with distinct geometric features. Finally, we analyze the geometry
of READ estimators as the alignment parameters vary and propose an optimization
algorithm to estimate the projection of the global optimum onto this solution
surface. This procedure selects among equally robust estimators while optimally
constructing a representation structure. We conclude by demonstrating the
effectiveness of our framework through extensive simulations and a real-world
study, providing a powerful robust estimation grounded in learning
representation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [267] [Envy-Free but Still Unfair: Envy-Freeness Up To One Item (EF-1) in Personalized Recommendation](https://arxiv.org/abs/2509.09037)
*Amanda Aird,Ben Armstrong,Nicholas Mattei,Robin Burke*

Main category: cs.IR

TL;DR: The paper discusses envy-freeness (EF) in economics and recommendation systems, questioning its suitability in personalized settings.


<details>
  <summary>Details</summary>
Motivation: To explore and critique the appropriateness of envy-freeness as a fairness metric in personalized settings, such as recommendation systems.

Method: The study overviews the concept of envy-freeness and examines its applications in economics and recommendation systems.

Result: The authors argue that envy, as a measure of fairness, may not align well with scenarios involving personalization.

Conclusion: Envy-freeness, while popular in various fields, may not be suitable for measuring fairness in personalized contexts.

Abstract: Envy-freeness and the relaxation to Envy-freeness up to one item (EF-1) have
been used as fairness concepts in the economics, game theory, and social choice
literatures since the 1960s, and have recently gained popularity within the
recommendation systems communities. In this short position paper we will give
an overview of envy-freeness and its use in economics and recommendation
systems; and illustrate why envy is not appropriate to measure fairness for use
in settings where personalization plays a role.

</details>


### [268] [Generative Engine Optimization: How to Dominate AI Search](https://arxiv.org/abs/2509.08919)
*Mahe Chen,Xiaoxuan Wang,Kaiwen Chen,Nick Koudas*

Main category: cs.IR

TL;DR: The paper investigates the shift in search engines from traditional ranked lists to generative AI-driven answers, coining the term Generative Engine Optimization (GEO) to adapt to this new paradigm.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the impact of generative AI-powered search engines such as ChatGPT on traditional SEO and offer strategies for improving visibility in this transformed search landscape.

Method: The study conducts large-scale, controlled experiments comparing traditional web search with AI search across various dimensions, such as verticals, languages, and paraphrasing, to identify differences in information sourcing and presentation.

Result: Key findings include AI Search's bias toward authoritative third-party sources (earned media), distinct differences between AI search services, and significant deviations from the domain diversity and content balance found in Google search.

Conclusion: The paper proposes a GEO framework with actionable strategies to optimize for AI-driven search engines, focusing on machine-readable content, earned media authority, and overcoming biases against smaller brands.

Abstract: The rapid adoption of generative AI-powered search engines like ChatGPT,
Perplexity, and Gemini is fundamentally reshaping information retrieval, moving
from traditional ranked lists to synthesized, citation-backed answers. This
shift challenges established Search Engine Optimization (SEO) practices and
necessitates a new paradigm, which we term Generative Engine Optimization
(GEO).
  This paper presents a comprehensive comparative analysis of AI Search and
traditional web search (Google). Through a series of large-scale, controlled
experiments across multiple verticals, languages, and query paraphrases, we
quantify critical differences in how these systems source information. Our key
findings reveal that AI Search exhibit a systematic and overwhelming bias
towards Earned media (third-party, authoritative sources) over Brand-owned and
Social content, a stark contrast to Google's more balanced mix. We further
demonstrate that AI Search services differ significantly from each other in
their domain diversity, freshness, cross-language stability, and sensitivity to
phrasing.
  Based on these empirical results, we formulate a strategic GEO agenda. We
provide actionable guidance for practitioners, emphasizing the critical need
to: (1) engineer content for machine scannability and justification, (2)
dominate earned media to build AI-perceived authority, (3) adopt
engine-specific and language-aware strategies, and (4) overcome the inherent
"big brand bias" for niche players. Our work provides the foundational
empirical analysis and a strategic framework for achieving visibility in the
new generative search landscape.

</details>


### [269] [Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations](https://arxiv.org/abs/2509.09651)
*Zakaria El Kassimi,Fares Fourati,Mohamed-Slim Alouini*

Main category: cs.IR

TL;DR: The paper introduces a telecom-specific Retrieval-Augmented Generation (RAG) pipeline for question answering in radio regulations and demonstrates its effectiveness through improved accuracy and evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of question answering in the legally sensitive and high-stakes domain of radio regulations.

Method: The study develops a telecom-specific RAG pipeline, creates a multiple-choice evaluation set, defines a domain-specific retrieval metric, and benchmarks retrieval and generation models using their approach.

Result: The retrieval method achieves 97% accuracy in its domain-specific metric, and their pipeline improves GPT-4o generation accuracy by nearly 12% relative improvement, far outperforming unstructured document insertion.

Conclusion: The research demonstrates that targeted grounding with their RAG pipeline is both a strong baseline and an effective domain-specific solution for regulatory question answering.

Abstract: We study question answering in the domain of radio regulations, a legally
sensitive and high-stakes area. We propose a telecom-specific
Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,
the first multiple-choice evaluation set for this domain, constructed from
authoritative sources using automated filtering and human validation. To assess
retrieval quality, we define a domain-specific retrieval metric, under which
our retriever achieves approximately 97% accuracy. Beyond retrieval, our
approach consistently improves generation accuracy across all tested models. In
particular, while naively inserting documents without structured retrieval
yields only marginal gains for GPT-4o (less than 1%), applying our pipeline
results in nearly a 12% relative improvement. These findings demonstrate that
carefully targeted grounding provides a simple yet strong baseline and an
effective domain-specific solution for regulatory question answering. All code
and evaluation scripts, along with our derived question-answer dataset, are
available at https://github.com/Zakaria010/Radio-RAG.

</details>


### [270] [We're Still Doing It (All) Wrong: Recommender Systems, Fifteen Years Later](https://arxiv.org/abs/2509.09414)
*Alan Said,Maria Soledad Pera,Michael D. Ekstrand*

Main category: cs.IR

TL;DR: This paper revisits longstanding issues in recommender systems research, arguing that fundamental flaws identified in 2011 persist, despite increasing complexity in methodologies.


<details>
  <summary>Details</summary>
Motivation: The research addresses the enduring conceptual, methodological, and infrastructural issues in recommender systems, aiming to critique and refine the field.

Method: The paper draws on recent developments in reproducibility, evaluation methods, environmental impact studies, and participatory design to analyze problems and propose shifts in the research landscape.

Result: Highlighting community-driven initiatives, the study identifies that meaningful changes require fundamental reframing of research goals and methodologies within the field.

Conclusion: Recommender systems research must focus on epistemic humility, prioritize human impact, and adopt sustainable practices instead of compounding complexity on flawed foundations.

Abstract: In 2011, Xavier Amatriain sounded the alarm: recommender systems research was
"doing it all wrong" [1]. His critique, rooted in statistical misinterpretation
and methodological shortcuts, remains as relevant today as it was then. But
rather than correcting course, we added new layers of sophistication on top of
the same broken foundations. This paper revisits Amatriain's diagnosis and
argues that many of the conceptual, epistemological, and infrastructural
failures he identified still persist, in more subtle or systemic forms. Drawing
on recent work in reproducibility, evaluation methodology, environmental
impact, and participatory design, we showcase how the field's accelerating
complexity has outpaced its introspection. We highlight ongoing community-led
initiatives that attempt to shift the paradigm, including workshops, evaluation
frameworks, and calls for value-sensitive and participatory research. At the
same time, we contend that meaningful change will require not only new metrics
or better tooling, but a fundamental reframing of what recommender systems
research is for, who it serves, and how knowledge is produced and validated.
Our call is not just for technical reform, but for a recommender systems
research agenda grounded in epistemic humility, human impact, and sustainable
practice.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [271] [Database Views as Explanations for Relational Deep Learning](https://arxiv.org/abs/2509.09482)
*Agapi Rissaki,Ilias Fountalis,Wolfgang Gatterbauer,Benny Kimelfeld*

Main category: cs.DB

TL;DR: This paper introduces a framework to explain predictions of deep learning models, particularly hetero-GNNs, over relational databases using view definitions that highlight relevant database portions.


<details>
  <summary>Details</summary>
Motivation: The complexity of deep learning models over relational databases makes it difficult to explain predictions in understandable terms.

Method: The approach adapts the notion of determinacy to generate concise and granular explanations while employing model-agnostic and hetero-GNN-specific techniques like learnable masking.

Result: The proposed explanation framework was empirically validated on the RelBench collection, showing usefulness across various tasks and domains along with efficiency.

Conclusion: The framework enhances interpretability of relational database ML models like hetero-GNNs, offering meaningful and efficient explanations.

Abstract: In recent years, there has been significant progress in the development of
deep learning models over relational databases, including architectures based
on heterogeneous graph neural networks (hetero-GNNs) and heterogeneous graph
transformers. In effect, such architectures state how the database records and
links (e.g., foreign-key references) translate into a large, complex numerical
expression, involving numerous learnable parameters. This complexity makes it
hard to explain, in human-understandable terms, how a model uses the available
data to arrive at a given prediction. We present a novel framework for
explaining machine-learning models over relational databases, where
explanations are view definitions that highlight focused parts of the database
that mostly contribute to the model's prediction. We establish such global
abductive explanations by adapting the classic notion of determinacy by Nash,
Segoufin, and Vianu (2010). In addition to tuning the tradeoff between
determinacy and conciseness, the framework allows controlling the level of
granularity by adopting different fragments of view definitions, such as ones
highlighting whole columns, foreign keys between tables, relevant groups of
tuples, and so on. We investigate the realization of the framework in the case
of hetero-GNNs. We develop heuristic algorithms that avoid the exhaustive
search over the space of all databases. We propose techniques that are
model-agnostic, and others that are tailored to hetero-GNNs via the notion of
learnable masking. Our approach is evaluated through an extensive empirical
study on the RelBench collection, covering a variety of domains and different
record-level tasks. The results demonstrate the usefulness of the proposed
explanations, as well as the efficiency of their generation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [272] [Towards A High-Performance Quantum Data Center Network Architecture](https://arxiv.org/abs/2509.09653)
*Yufeng Xin,Liang Zhang*

Main category: quant-ph

TL;DR: The paper proposes a novel three-layer fat-tree network architecture for quantum data centers (QDCs) to address challenges in scalability, entanglement, and memory management.


<details>
  <summary>Details</summary>
Motivation: Large-scale quantum computers are limited by technology and cost, necessitating modular quantum computer clusters for scalable quantum processing.

Method: The authors designed a three-layer fat-tree network leveraging a specialized leaf switch, an advanced spine switch, and a queue scheduling mechanism. Performance was validated through queuing-theoretical models and NetSquid-based simulations.

Result: The new architecture demonstrated both scalability and success in maintaining high entanglement fidelity, ensuring efficient handling of quantum tasks.

Conclusion: The proposed design offers a practical and scalable solution for implementing modular quantum data center networks while addressing critical operational challenges.

Abstract: Quantum Data Centers (QDCs) are needed to support large-scale quantum
processing for both academic and commercial applications. While large-scale
quantum computers are constrained by technological and financial barriers, a
modular approach that clusters small quantum computers offers an alternative.
This approach, however, introduces new challenges in network scalability,
entanglement generation, and quantum memory management. In this paper, we
propose a three-layer fat-tree network architecture for QDCs, designed to
address these challenges. Our architecture features a unique leaf switch and an
advanced swapping spine switch design, optimized to handle high volumes of
entanglement requests as well as a queue scheduling mechanism that efficiently
manages quantum memory to prevent decoherence. Through queuing-theoretical
models and simulations in NetSquid, we demonstrate the proposed architecture's
scalability and effectiveness in maintaining high entanglement fidelity,
offering a practical path forward for modular QDC networks.

</details>


### [273] [Generative quantum advantage for classical and quantum problems](https://arxiv.org/abs/2509.09033)
*Hsin-Yuan Huang,Michael Broughton,Norhan Eassa,Hartmut Neven,Ryan Babbush,Jarrod R. McClean*

Main category: quant-ph

TL;DR: The paper introduces generative quantum models capable of efficiently learning and sampling distributions that are beyond classical computation reach and demonstrates these using a quantum processor.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of efficiently learning and generating samples from classically intractable quantum distributions, establishing generative quantum advantage.

Method: The authors propose trainable quantum models that avoid classical simulation limitations, barren plateaus, and local minima, implementing their approach on a $68$-qubit quantum processor.

Result: They successfully demonstrated learning classically intractable distributions and implementing quantum circuits for faster physical simulations on the quantum processor.

Conclusion: The study proves that generative quantum models can efficiently perform learning and sampling in the beyond-classical regime, showcasing quantum's potential in enhancing generative tasks.

Abstract: Recent breakthroughs in generative machine learning, powered by massive
computational resources, have demonstrated unprecedented human-like
capabilities. While beyond-classical quantum experiments can generate samples
from classically intractable distributions, their complexity has thwarted all
efforts toward efficient learning. This challenge has hindered demonstrations
of generative quantum advantage: the ability of quantum computers to learn and
generate desired outputs substantially better than classical computers. We
resolve this challenge by introducing families of generative quantum models
that are hard to simulate classically, are efficiently trainable, exhibit no
barren plateaus or proliferating local minima, and can learn to generate
distributions beyond the reach of classical computers. Using a $68$-qubit
superconducting quantum processor, we demonstrate these capabilities in two
scenarios: learning classically intractable probability distributions and
learning quantum circuits for accelerated physical simulation. Our results
establish that both learning and sampling can be performed efficiently in the
beyond-classical regime, opening new possibilities for quantum-enhanced
generative models with provable advantage.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [274] [WarpPINN-fibers: improved cardiac strain estimation from cine-MR with physics-informed neural networks](https://arxiv.org/abs/2509.08872)
*Felipe Álvarez Barrientos,Tomás Banduc,Isabeau Sirven,Francisco Sahli Costabal*

Main category: eess.IV

TL;DR: The paper presents WarpPINN-fibers, a physics-informed neural network that leverages cardiac fiber information to improve the accuracy of strain and motion predictions in the heart using cine MRI data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limited accuracy of existing cardiac strain estimation approaches, which fail to incorporate fiber mechanics essential to accurately describing heart function and related pathologies.

Method: The authors developed WarpPINN-fibers, a neural network trained on a hyper-elastic model with a novel loss function combining data similarity, near-incompressibility regularization, and fiber-stretch penalization. This framework uses cine MRI to predict deformation fields in cardiac tissue.

Result: WarpPINN-fibers enhances strain estimation by improving fiber stretch control and outperforms previous methodologies in both synthetic phantom and real-world cine MRI experiments involving 15 healthy volunteers.

Conclusion: The study concludes that WarpPINN-fibers can provide highly accurate deformation fields and strain predictions that align with fiber physiology using standard MRI imaging techniques.

Abstract: The contractile motion of the heart is strongly determined by the
distribution of the fibers that constitute cardiac tissue. Strain analysis
informed with the orientation of fibers allows to describe several pathologies
that are typically associated with impaired mechanics of the myocardium, such
as cardiovascular disease. Several methods have been developed to estimate
strain-derived metrics from traditional imaging techniques. However, the
physical models underlying these methods do not include fiber mechanics,
restricting their capacity to accurately explain cardiac function. In this
work, we introduce WarpPINN-fibers, a physics-informed neural network framework
to accurately obtain cardiac motion and strains enhanced by fiber information.
We train our neural network to satisfy a hyper-elastic model and promote fiber
contraction with the goal to predict the deformation field of the heart from
cine magnetic resonance images. For this purpose, we build a loss function
composed of three terms: a data-similarity loss between the reference and the
warped template images, a regularizer enforcing near-incompressibility of
cardiac tissue and a fiber-stretch penalization that controls strain in the
direction of synthetically produced fibers. We show that our neural network
improves the former WarpPINN model and effectively controls fiber stretch in a
synthetic phantom experiment. Then, we demonstrate that WarpPINN-fibers
outperforms alternative methodologies in landmark-tracking and strain curve
prediction for a cine-MRI benchmark with a cohort of 15 healthy volunteers. We
expect that our method will enable a more precise quantification of cardiac
strains through accurate deformation fields that are consistent with fiber
physiology, without requiring imaging techniques more sophisticated than MRI.

</details>


### [275] [Virtual staining for 3D X-ray histology of bone implants](https://arxiv.org/abs/2509.09235)
*Sarah C. Irvine,Christian Lucas,Diana Krüger,Bianca Guedert,Julian Moosmann,Berit Zeller-Plumhoff*

Main category: eess.IV

TL;DR: The paper proposes virtual staining for 3D X-ray imaging using deep learning, enhancing interpretability without extra sample preparation.


<details>
  <summary>Details</summary>
Motivation: Current X-ray tomography has limited biochemical specificity compared to conventional histology staining techniques.

Method: The authors developed a modified CycleGAN network for cross-modality image translation of synchrotron micro-CT scans into virtually stained slices using matched histological image pairs.

Result: The proposed method outperformed baseline models like Pix2Pix and CycleGAN across multiple metrics (SSIM, PSNR, LPIPS) while preserving structural details.

Conclusion: The approach enables chemically informative, label-free tissue characterization in 3D without invasive sample preparation, although future work is required to refine implant degradation visualization.

Abstract: Three-dimensional X-ray histology techniques offer a non-invasive alternative
to conventional 2D histology, enabling volumetric imaging of biological tissues
without the need for physical sectioning or chemical staining. However, the
inherent greyscale image contrast of X-ray tomography limits its biochemical
specificity compared to traditional histological stains. Within digital
pathology, deep learning-based virtual staining has demonstrated utility in
simulating stained appearances from label-free optical images. In this study,
we extend virtual staining to the X-ray domain by applying cross-modality image
translation to generate artificially stained slices from
synchrotron-radiation-based micro-CT scans. Using over 50 co-registered image
pairs of micro-CT and toluidine blue-stained histology from bone-implant
samples, we trained a modified CycleGAN network tailored for limited paired
data. Whole slide histology images were downsampled to match the voxel size of
the CT data, with on-the-fly data augmentation for patch-based training. The
model incorporates pixelwise supervision and greyscale consistency terms,
producing histologically realistic colour outputs while preserving
high-resolution structural detail. Our method outperformed Pix2Pix and standard
CycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the
model can be applied to full CT volumes to generate virtually stained 3D
datasets, enhancing interpretability without additional sample preparation.
While features such as new bone formation were able to be reproduced, some
variability in the depiction of implant degradation layers highlights the need
for further training data and refinement. This work introduces virtual staining
to 3D X-ray imaging and offers a scalable route for chemically informative,
label-free tissue characterisation in biomedical research.

</details>


### [276] [Dynamic Structural Recovery Parameters Enhance Prediction of Visual Outcomes After Macular Hole Surgery](https://arxiv.org/abs/2509.09227)
*Yinzheng Zhao,Zhihao Zhao,Rundong Jiang,Louisa Sackewitz,Quanmin Liang,Mathias Maier,Daniel Zapp,Peter Charbel Issa,Mohammad Ali Nasseri*

Main category: eess.IV

TL;DR: The paper proposes using dynamic structural parameters within a multimodal deep learning framework to enhance prediction accuracy for visual recovery in macular hole patients.


<details>
  <summary>Details</summary>
Motivation: Current methods lack the precision to predict postoperative visual outcomes for macular hole surgery, necessitating better predictive tools.

Method: The study leveraged publicly available longitudinal OCT data and integrated automated feature extraction with multimodal deep learning techniques alongside logistic regression models.

Result: Dynamic recovery parameters notably improved predictive accuracy in regression models, and the multimodal deep learning model outperformed these, with significant performance boosts at various postoperative stages.

Conclusion: Dynamic parameters integrated within deep learning frameworks present substantial promise for improving personalized postoperative management in macular hole surgeries.

Abstract: Purpose: To introduce novel dynamic structural parameters and evaluate their
integration within a multimodal deep learning (DL) framework for predicting
postoperative visual recovery in idiopathic full-thickness macular hole (iFTMH)
patients. Methods: We utilized a publicly available longitudinal OCT dataset at
five stages (preoperative, 2 weeks, 3 months, 6 months, and 12 months). A stage
specific segmentation model delineated related structures, and an automated
pipeline extracted quantitative, composite, qualitative, and dynamic features.
Binary logistic regression models, constructed with and without dynamic
parameters, assessed their incremental predictive value for best-corrected
visual acuity (BCVA). A multimodal DL model combining clinical variables,
OCT-derived features, and raw OCT images was developed and benchmarked against
regression models. Results: The segmentation model achieved high accuracy
across all timepoints (mean Dice > 0.89). Univariate and multivariate analyses
identified base diameter, ellipsoid zone integrity, and macular hole area as
significant BCVA predictors (P < 0.05). Incorporating dynamic recovery rates
consistently improved logistic regression AUC, especially at the 3-month
follow-up. The multimodal DL model outperformed logistic regression, yielding
higher AUCs and overall accuracy at each stage. The difference is as high as
0.12, demonstrating the complementary value of raw image volume and dynamic
parameters. Conclusions: Integrating dynamic parameters into the multimodal DL
model significantly enhances the accuracy of predictions. This fully automated
process therefore represents a promising clinical decision support tool for
personalized postoperative management in macular hole surgery.

</details>


### [277] [In-Loop Filtering Using Learned Look-Up Tables for Video Coding](https://arxiv.org/abs/2509.09494)
*Zhuoyuan Li,Jiacheng Li,Yao Li,Jialin Li,Li Li,Dong Liu,Feng Wu*

Main category: eess.IV

TL;DR: The paper proposes a look-up table-based in-loop filtering (ILF) solution, LUT-ILF++, to enhance video coding efficiency with reduced time and computational complexity.


<details>
  <summary>Details</summary>
Motivation: Deep neural network-based ILF methods, while effective, are computationally intensive and demand dedicated hardware. The paper aims to address this limitation by exploring a practical solution using look-up tables (LUTs) to reduce this complexity.

Method: The authors train a DNN for ILF and store its output values into LUTs, enabling filtering by retrieving the filtered pixels instead of heavy computations. Innovations include cooperation of multiple filtering LUTs, cross-component indexing for joint filtering of color components, and LUT pruning to reduce storage requirements.

Result: The framework was implemented in VVC reference software and showed significant bitrate reductions (0.82%–4.11%) across different configurations with much lower computational and storage costs compared to DNN-based methods.

Conclusion: The LUT-ILF++ framework demonstrates a practical and efficient solution for video coding, providing a good balance between performance and resource requirements, making it more suitable for general use.

Abstract: In-loop filtering (ILF) is a key technology in video coding standards to
reduce artifacts and enhance visual quality. Recently, neural network-based ILF
schemes have achieved remarkable coding gains, emerging as a powerful candidate
for next-generation video coding standards. However, the use of deep neural
networks (DNN) brings significant computational and time complexity or high
demands for dedicated hardware, making it challenging for general use. To
address this limitation, we study a practical ILF solution by adopting look-up
tables (LUTs). After training a DNN with a restricted reference range for ILF,
all possible inputs are traversed, and the output values of the DNN are cached
into LUTs. During the coding process, the filtering process is performed by
simply retrieving the filtered pixel through locating the input pixels and
interpolating between the cached values, instead of relying on heavy inference
computations. In this paper, we propose a universal LUT-based ILF framework,
termed LUT-ILF++. First, we introduce the cooperation of multiple kinds of
filtering LUTs and propose a series of customized indexing mechanisms to enable
better filtering reference perception with limited storage consumption. Second,
we propose the cross-component indexing mechanism to enable the filtering of
different color components jointly. Third, in order to make our solution
practical for coding uses, we propose the LUT compaction scheme to enable the
LUT pruning, achieving a lower storage cost of the entire solution. The
proposed framework is implemented in the VVC reference software. Experimental
results show that the proposed framework achieves on average 0.82%/2.97%/1.63%
and 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI
and RA configurations, respectively. Compared to DNN-based solutions, our
proposed solution has much lower time complexity and storage cost.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [278] [CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction](https://arxiv.org/abs/2509.08947)
*Yancheng Cai,Robert Wanat,Rafal Mantiuk*

Main category: cs.GR

TL;DR: This paper introduces a system (CameraVDP) combining camera-based image reconstruction with perceptual evaluation, enabling precise measurement of display artifacts and their visibility to human viewers.


<details>
  <summary>Details</summary>
Motivation: Traditional methods fail to capture spatially detailed distortions in modern displays, and camera measurements alone are inaccurate without visual system modeling.

Method: Developed a pipeline for camera-based reconstruction with techniques like HDR stacking, vignetting correction, and paired it with a Visual Difference Predictor to model human visual perception.

Result: Validated the CameraVDP framework across three applications: defective pixel detection, color fringes analysis, and display uniformity evaluation.

Conclusion: CameraVDP effectively combines physical measurements with perceptual modeling, offering robust analysis for display performance and defect detection.

Abstract: Accurate measurement of images produced by electronic displays is critical
for the evaluation of both traditional and computational displays. Traditional
display measurement methods based on sparse radiometric sampling and fitting a
model are inadequate for capturing spatially varying display artifacts, as they
fail to capture high-frequency and pixel-level distortions. While cameras offer
sufficient spatial resolution, they introduce optical, sampling, and
photometric distortions. Furthermore, the physical measurement must be combined
with a model of a visual system to assess whether the distortions are going to
be visible. To enable perceptual assessment of displays, we propose a
combination of a camera-based reconstruction pipeline with a visual difference
predictor, which account for both the inaccuracy of camera measurements and
visual difference prediction. The reconstruction pipeline combines HDR image
stacking, MTF inversion, vignetting correction, geometric undistortion,
homography transformation, and color correction, enabling cameras to function
as precise display measurement instruments. By incorporating a Visual
Difference Predictor (VDP), our system models the visibility of various stimuli
under different viewing conditions for the human visual system. We validate the
proposed CameraVDP framework through three applications: defective pixel
detection, color fringing awareness, and display non-uniformity evaluation. Our
uncertainty analysis framework enables the estimation of the theoretical upper
bound for defect pixel detection performance and provides confidence intervals
for VDP quality scores.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [279] [Towards Confidential and Efficient LLM Inference with Dual Privacy Protection](https://arxiv.org/abs/2509.09091)
*Honglan Yu,Yibin Wang,Feifei Dai,Dong Liu,Haihui Fan,Xiaoyan Gu*

Main category: cs.CR

TL;DR: This paper introduces CMIF, a model inference framework addressing latency, privacy, and performance concerns in deploying large language models within trusted execution environments and GPUs.


<details>
  <summary>Details</summary>
Motivation: Current methods for private inference using TEEs and DP suffer from high latency in TEEs and performance degradation due to random noise application in DP.

Method: CMIF deploys the embedding layer in client-side TEEs and subsequent layers on GPU servers. It also optimizes the Report-Noisy-Max mechanism for protecting sensitive inputs.

Result: Experiments on Llama-series models show CMIF improves latency in TEEs and maintains user data privacy with minimal performance loss.

Conclusion: CMIF effectively balances efficient inference and confidentiality, overcoming limitations in current TEEs and DP methods for large language models.

Abstract: CPU-based trusted execution environments (TEEs) and differential privacy (DP)
have gained wide applications for private inference. Due to high inference
latency in TEEs, researchers use partition-based approaches that offload linear
model components to GPUs. However, dense nonlinear layers of large language
models (LLMs) result in significant communication overhead between TEEs and
GPUs. DP-based approaches apply random noise to protect data privacy, but this
compromises LLM performance and semantic understanding. To overcome the above
drawbacks, this paper proposes CMIF, a Confidential and efficient Model
Inference Framework. CMIF confidentially deploys the embedding layer in the
client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes
the Report-Noisy-Max mechanism to protect sensitive inputs with a slight
decrease in model performance. Extensive experiments on Llama-series models
demonstrate that CMIF reduces additional inference overhead in TEEs while
preserving user data privacy.

</details>


### [280] [DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models](https://arxiv.org/abs/2509.09097)
*Honghui Xu,Shiva Shrestha,Wei Chen,Zhiyuan Li,Zhipeng Cai*

Main category: cs.CR

TL;DR: This paper presents DP-FedLoRA, a method for privacy-preserving fine-tuning of large language models (LLMs) on edge devices using federated learning combined with differential privacy and LoRA adaptation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable advanced language understanding and generation on edge devices while addressing privacy concerns related to user-specific sensitive data in federated learning.

Method: DP-FedLoRA combines LoRA-based model adaptation with differential privacy by locally clipping and perturbing LoRA matrices on each client using Gaussian noise, ensuring ($\epsilon$, $\delta$)-differential privacy in a communication-efficient manner.

Result: The proposed method is theoretically analyzed for unbiased updates and controlled noise variance. Experiments on mainstream benchmarks demonstrate that it achieves competitive performance while maintaining strong privacy guarantees.

Conclusion: DP-FedLoRA provides a scalable, privacy-preserving solution for federated fine-tuning, facilitating the deployment of large language models in on-device environments.

Abstract: As on-device large language model (LLM) systems become increasingly
prevalent, federated fine-tuning enables advanced language understanding and
generation directly on edge devices; however, it also involves processing
sensitive, user-specific data, raising significant privacy concerns within the
federated learning framework. To address these challenges, we propose
DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates
LoRA-based adaptation with differential privacy in a communication-efficient
setting. Each client locally clips and perturbs its LoRA matrices using
Gaussian noise to satisfy ($\epsilon$, $\delta$)-differential privacy. We
further provide a theoretical analysis demonstrating the unbiased nature of the
updates and deriving bounds on the variance introduced by noise, offering
practical guidance for privacy-budget calibration. Experimental results across
mainstream benchmarks show that DP-FedLoRA delivers competitive performance
while offering strong privacy guarantees, paving the way for scalable and
privacy-preserving LLM deployment in on-device environments.

</details>


### [281] [Character-Level Perturbations Disrupt LLM Watermarks](https://arxiv.org/abs/2509.09112)
*Zhaoxi Zhang,Xiaomei Zhang,Yanjun Zhang,He Zhang,Shirui Pan,Bo Liu,Asif Qumer Gill,Leo Yu Zhang*

Main category: cs.CR

TL;DR: The research explores vulnerabilities in watermarking of Large Language Models (LLM) and proposes advanced methods for watermark removal using character-level perturbations and Genetic Algorithm-based attacks, demonstrating their effectiveness against practical defenses.


<details>
  <summary>Details</summary>
Motivation: The paper aims to highlight the limitations of current LLM watermarking schemes and stresses the need for more robust mechanisms to protect copyrights, prevent misuse, and enhance content detection in the face of attacks.

Method: The authors formalized a system model for LLM watermarking, identified two realistic threat models, analyzed the effectiveness of different perturbation approaches, and developed removal attacks guided by the Genetic Algorithm under constrained black-box query situations.

Result: Character-level perturbations proved highly effective at disrupting tokenization and removing watermarks under restrictive threat models. The Genetic Algorithm-based attacks demonstrated strong watermark removal capabilities with limited access to detectors, outperforming existing approaches.

Conclusion: Current LLM watermark schemes face significant risks from advanced perturbation and attack methods, emphasizing the need for adaptive and robust defenses against vulnerabilities highlighted in this study.

Abstract: Large Language Model (LLM) watermarking embeds detectable signals into
generated text for copyright protection, misuse prevention, and content
detection. While prior studies evaluate robustness using watermark removal
attacks, these methods are often suboptimal, creating the misconception that
effective removal requires large perturbations or powerful adversaries.
  To bridge the gap, we first formalize the system model for LLM watermark, and
characterize two realistic threat models constrained on limited access to the
watermark detector. We then analyze how different types of perturbation vary in
their attack range, i.e., the number of tokens they can affect with a single
edit. We observe that character-level perturbations (e.g., typos, swaps,
deletions, homoglyphs) can influence multiple tokens simultaneously by
disrupting the tokenization process. We demonstrate that character-level
perturbations are significantly more effective for watermark removal under the
most restrictive threat model. We further propose guided removal attacks based
on the Genetic Algorithm (GA) that uses a reference detector for optimization.
Under a practical threat model with limited black-box queries to the watermark
detector, our method demonstrates strong removal performance. Experiments
confirm the superiority of character-level perturbations and the effectiveness
of the GA in removing watermarks under realistic constraints. Additionally, we
argue there is an adversarial dilemma when considering potential defenses: any
fixed defense can be bypassed by a suitable perturbation strategy. Motivated by
this principle, we propose an adaptive compound character-level attack.
Experimental results show that this approach can effectively defeat the
defenses. Our findings highlight significant vulnerabilities in existing LLM
watermark schemes and underline the urgency for the development of new robust
mechanisms.

</details>


### [282] [CryptGNN: Enabling Secure Inference for Graph Neural Networks](https://arxiv.org/abs/2509.09107)
*Pritam Sen,Yao Ma,Cristian Borcea*

Main category: cs.CR

TL;DR: CryptGNN introduces a secure method for third-party graph neural network inference in the cloud using secure multi-party computation (SMPC).


<details>
  <summary>Details</summary>
Motivation: The paper aims to address data security concerns when using third-party GNN models and cloud-based ML as a service.

Method: The authors use distributed SMPC techniques for secure message passing and feature transformation layers in CryptGNN.

Result: CryptGNN is secure against collusion of up to P-1 out of P cloud parties and does not rely on a trusted server, as evidenced by both theoretical analysis and experiments.

Conclusion: CryptGNN effectively protects client data, graph structures, and model parameters while maintaining efficiency in cloud-based GNN inference.

Abstract: We present CryptGNN, a secure and effective inference solution for
third-party graph neural network (GNN) models in the cloud, which are accessed
by clients as ML as a service (MLaaS). The main novelty of CryptGNN is its
secure message passing and feature transformation layers using distributed
secure multi-party computation (SMPC) techniques. CryptGNN protects the
client's input data and graph structure from the cloud provider and the
third-party model owner, and it protects the model parameters from the cloud
provider and the clients. CryptGNN works with any number of SMPC parties, does
not require a trusted server, and is provably secure even if P-1 out of P
parties in the cloud collude. Theoretical analysis and empirical experiments
demonstrate the security and efficiency of CryptGNN.

</details>


### [283] [ENSI: Efficient Non-Interactive Secure Inference for Large Language Models](https://arxiv.org/abs/2509.09424)
*Zhiyu He,Maojiang Wang,Xinwen Gao,Yuchuan Luo,Lin Liu,Shaojing Fu*

Main category: cs.CR

TL;DR: ENSI proposes a non-interactive, secure inference framework for LLMs, integrating cryptography protocols with a lightweight LLM architecture to reduce computational complexity.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of integrating cryptographic protocols with large language models (LLMs) for secure and practical privacy-preserving machine learning.

Method: ENSI co-designs cryptographic protocols and LLM architecture. It uses an optimized encoding strategy with the CKKS scheme and a lightweight LLM (BitNet), integrates sigmoid attention as a retraining-free alternative under homomorphic encryption (HE), and embeds Bootstrapping into RMSNorm to efficiently refresh ciphertexts.

Result: The framework achieves an 8x acceleration in encrypted matrix multiplications and a 2.6x speedup in softmax inference compared to current methods, while reducing bootstrapping usage to 1%.

Conclusion: ENSI overcomes critical computational barriers in secure inference for LLMs, offering a significant improvement in efficiency and practicality for privacy-preserving machine learning applications.

Abstract: Secure inference enables privacy-preserving machine learning by leveraging
cryptographic protocols that support computations on sensitive user data
without exposing it. However, integrating cryptographic protocols with large
language models (LLMs) presents significant challenges, as the inherent
complexity of these protocols, together with LLMs' massive parameter scale and
sophisticated architectures, severely limits practical usability. In this work,
we propose ENSI, a novel non-interactive secure inference framework for LLMs,
based on the principle of co-designing the cryptographic protocols and LLM
architecture. ENSI employs an optimized encoding strategy that seamlessly
integrates CKKS scheme with a lightweight LLM variant, BitNet, significantly
reducing the computational complexity of encrypted matrix multiplications. In
response to the prohibitive computational demands of softmax under homomorphic
encryption (HE), we pioneer the integration of the sigmoid attention mechanism
with HE as a seamless, retraining-free alternative. Furthermore, by embedding
the Bootstrapping operation within the RMSNorm process, we efficiently refresh
ciphertexts while markedly decreasing the frequency of costly bootstrapping
invocations. Experimental evaluations demonstrate that ENSI achieves
approximately an 8x acceleration in matrix multiplications and a 2.6x speedup
in softmax inference on CPU compared to state-of-the-art method, with the
proportion of bootstrapping is reduced to just 1%.

</details>


### [284] [Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts](https://arxiv.org/abs/2509.09488)
*Felix Mächtle,Ashwath Shetty,Jonas Sander,Nils Loose,Sören Pirk,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: This paper discusses vulnerabilities in text-to-image diffusion models, focusing on prompt theft and a flaw in noise generation, and proposes solutions.


<details>
  <summary>Details</summary>
Motivation: To address the security and privacy concerns related to prompt theft in text-to-image diffusion models, which hold significant intellectual and economic value.

Method: The authors identify a noise-generation vulnerability in major frameworks caused by PyTorch's seed value limitations and conduct large-scale empirical evaluations. They also develop PromptPirate, a new optimization method leveraging brute-forced seeds, and propose countermeasures.

Result: The authors demonstrate that 95% of images on CivitAI can have their seeds brute-forced within 140 minutes using SeedSnitch. PromptPirate achieves a significant improvement of 8-11% in LPIPS similarity over existing state-of-the-art methods.

Conclusion: The paper highlights a critical vulnerability in text-to-image generation models and offers both enhanced attacking and defending strategies, ensuring models remain secure and privacy-respecting.

Abstract: Diffusion models have significantly advanced text-to-image generation,
enabling the creation of highly realistic images conditioned on textual prompts
and seeds. Given the considerable intellectual and economic value embedded in
such prompts, prompt theft poses a critical security and privacy concern. In
this paper, we investigate prompt-stealing attacks targeting diffusion models.
We reveal that numerical optimization-based prompt recovery methods are
fundamentally limited as they do not account for the initial random noise used
during image generation. We identify and exploit a noise-generation
vulnerability (CWE-339), prevalent in major image-generation frameworks,
originating from PyTorch's restriction of seed values to a range of $2^{32}$
when generating the initial random noise on CPUs. Through a large-scale
empirical analysis conducted on images shared via the popular platform CivitAI,
we demonstrate that approximately 95% of these images' seed values can be
effectively brute-forced in 140 minutes per seed using our seed-recovery tool,
SeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic
algorithm-based optimization method explicitly designed for prompt stealing.
PromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and
CLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity.
Furthermore, we introduce straightforward and effective countermeasures that
render seed stealing, and thus optimization-based prompt stealing, ineffective.
We have disclosed our findings responsibly and initiated coordinated mitigation
efforts with the developers to address this critical vulnerability.

</details>


### [285] [What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion Detection Datasets](https://arxiv.org/abs/2509.09564)
*Meghan Wilkinson,Robert H Thomson*

Main category: cs.CR

TL;DR: The paper investigates whether sub-categories exist within benign traffic in network intrusion detection datasets to potentially enhance multi-classification performance using clustering techniques.


<details>
  <summary>Details</summary>
Motivation: Most intrusion detection research treats benign traffic as a single undifferentiated class, raising concerns that this approach might overlook meaningful substructures within the data.

Method: The authors employ unsupervised clustering techniques, like HDBSCAN and Mean Shift Clustering, to analyze and evaluate benign traffic in datasets such as NSL-KDD, UNSW-NB15, and CIC-IDS 2017.

Result: The study demonstrates that different clustering techniques produce distinct partitions of benign traffic, suggesting the presence of meaningful sub-categories.

Conclusion: Identifying sub-categories within benign traffic using clustering approaches could improve the performance of existing supervised intrusion detection algorithms.

Abstract: Supervised machine learning techniques rely on labeled data to achieve high
task performance, but this requires the labels to capture some meaningful
differences in the underlying data structure. For training network intrusion
detection algorithms, most datasets contain a series of attack classes and a
single large benign class which captures all non-attack network traffic. A
review of intrusion detection papers and guides that explicitly state their
data preprocessing steps identified that the majority took the labeled
categories of the dataset at face value when training their algorithms. The
present paper evaluates the structure of benign traffic in several common
intrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and
determines whether there are meaningful sub-categories within this traffic
which may improve overall multi-classification performance using common machine
learning techniques. We present an overview of some unsupervised clustering
techniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they
differentially cluster the benign traffic space.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [286] [Convexity of Optimization Curves: Local Sharp Thresholds, Robustness Impossibility, and New Counterexamples](https://arxiv.org/abs/2509.08954)
*Le Duc Hieu*

Main category: math.OC

TL;DR: The paper investigates when the optimization curve of gradient descent (GD) and similar methods is convex, focusing on stepsize thresholds for convex L-smooth functions.


<details>
  <summary>Details</summary>
Motivation: To explore the properties of first-order optimization curves, particularly their convexity, in the context of gradient descent for convex L-smooth functions. The study bridges discrete and continuous optimization dynamics.

Method: The authors analyze the behavior of optimization curves produced by gradient descent iterations, deriving critical stepsize thresholds for convexity and monotonicity of gradient norms.

Result: For convex L-smooth functions, the optimization curve is convex if stepsizes are at most 1.75/L, while gradient norms decrease for stepsizes up to 2/L. Continuously, gradient flows always yield convex curves.

Conclusion: The paper provides tight stepsize bounds, enhancing the classical optimization toolbox by linking discrete iterations with continuous dynamics and refining worst-case convex optimization analysis.

Abstract: We study when the \emph{optimization curve} of first-order methods -- the
sequence \${f(x\_n)}*{n\ge0}\$ produced by constant-stepsize iterations -- is
convex, equivalently when the forward differences \$f(x\_n)-f(x*{n+1})\$ are
nonincreasing. For gradient descent (GD) on convex \$L\$-smooth functions, the
curve is convex for all stepsizes \$\eta \le 1.75/L\$, and this threshold is
tight. Moreover, gradient norms are nonincreasing for all \$\eta \le 2/L\$, and
in continuous time (gradient flow) the curve is always convex. These results
complement and refine the classical smooth convex optimization toolbox,
connecting discrete and continuous dynamics as well as worst-case analyses.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [287] [Expressive Power of Deep Networks on Manifolds: Simultaneous Approximation](https://arxiv.org/abs/2509.09362)
*Hanfei Zhou,Lei Shi*

Main category: math.NA

TL;DR: The paper develops a theoretical foundation for approximating functions on intricate manifolds using deep neural networks, demonstrating efficient parameter use and overcoming dimensionality issues.


<details>
  <summary>Details</summary>
Motivation: The complexity of curved geometries in domains makes solving PDEs using traditional methods challenging, necessitating new approaches leveraging machine learning.

Method: The authors created approximation theories showing how constant-depth ReLU networks with bounded weights can approximate functions in Sobolev spaces on manifolds.

Result: The method efficiently achieves approximation rates without suffering from the curse of dimensionality, depending only on intrinsic manifold dimensions. They establish near-optimal bounds for required parameters.

Conclusion: The study provides a theoretical basis for using deep neural networks to learn manifold-based PDEs, taking advantage of sparse architectural designs and low-dimensional geometry properties.

Abstract: A key challenge in scientific machine learning is solving partial
differential equations (PDEs) on complex domains, where the curved geometry
complicates the approximation of functions and their derivatives required by
differential operators. This paper establishes the first simultaneous
approximation theory for deep neural networks on manifolds. We prove that a
constant-depth $\mathrm{ReLU}^{k-1}$ network with bounded weights--a property
that plays a crucial role in controlling generalization error--can approximate
any function in the Sobolev space $\mathcal{W}_p^{k}(\mathcal{M}^d)$ to an
error of $\varepsilon$ in the $\mathcal{W}_p^{s}(\mathcal{M}^d)$ norm, for
$k\geq 3$ and $s<k$, using $\mathcal{O}(\varepsilon^{-d/(k-s)})$ nonzero
parameters, a rate that overcomes the curse of dimensionality by depending only
on the intrinsic dimension $d$. These results readily extend to functions in
H\"older-Zygmund spaces. We complement this result with a matching lower bound,
proving our construction is nearly optimal by showing the required number of
parameters matches up to a logarithmic factor. Our proof of the lower bound
introduces novel estimates for the Vapnik-Chervonenkis dimension and
pseudo-dimension of the network's high-order derivative classes. These
complexity bounds provide a theoretical cornerstone for learning PDEs on
manifolds involving derivatives. Our analysis reveals that the network
architecture leverages a sparse structure to efficiently exploit the manifold's
low-dimensional geometry.

</details>


### [288] [DeepTV: A neural network approach for total variation minimization](https://arxiv.org/abs/2409.05569)
*Andreas Langer,Sara Behnamian*

Main category: math.NA

TL;DR: This paper proposes a neural network-based approach to solve an infinite-dimensional total variation minimization problem, establishes theoretical guarantees, and validates findings with numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To explore the use of neural networks in solving infinite-dimensional total variation minimization problems and address the theoretical gaps in existing approaches.

Method: The authors circumvent the lack of solutions for the neural network problem by creating an auxiliary problem that Γ-converges to the original. They also propose a discrete version of the problem and analyze its connection to finite difference methods.

Result: Both the auxiliary and discrete problems are proven to Γ-converge to the original infinite-dimensional problem. Connections to standard discretization methods are established, and numerical experiments support the theoretical results.

Conclusion: The study advances the link between infinite-dimensional variational problems and neural network solutions, providing a framework for practical and theoretical problem-solving.

Abstract: Neural network approaches have been demonstrated to work quite well to solve
partial differential equations in practice. In this context approaches like
physics-informed neural networks and the Deep Ritz method have become popular.
In this paper, we propose a similar approach to solve an infinite-dimensional
total variation minimization problem using neural networks. We illustrate that
the resulting neural network problem does not have a solution in general. To
circumvent this theoretic issue, we consider an auxiliary neural network
problem, which indeed has a solution, and show that it converges in the sense
of $\Gamma$-convergence to the original problem. For computing a numerical
solution we further propose a discrete version of the auxiliary neural network
problem and again show its $\Gamma$-convergence to the original
infinite-dimensional problem. In particular, the $\Gamma$-convergence proof
suggests a particular discretization of the total variation. Moreover, we
connect the discrete neural network problem to a finite difference
discretization of the infinite-dimensional total variation minimization
problem. Numerical experiments are presented supporting our theoretical
findings.

</details>
