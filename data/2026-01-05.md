<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 24]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 35]
- [cs.CV](#cs.CV) [Total: 68]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.LG](#cs.LG) [Total: 63]
- [cs.NE](#cs.NE) [Total: 8]
- [cs.RO](#cs.RO) [Total: 17]
- [cs.SE](#cs.SE) [Total: 10]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [stat.ML](#stat.ML) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 12]
- [math.OC](#math.OC) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [eess.SP](#eess.SP) [Total: 4]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [eess.SY](#eess.SY) [Total: 4]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [physics.optics](#physics.optics) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

TL;DR: The paper introduces a reasoning-aware knowledge retrieval method that improves large language model (LLM) performance by integrating reasoning and retrieval strategies, enhancing alignment with logical conversational structures.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle to effectively integrate strategies for retrieval of relevant information and reasoning processes, impacting their performance in multi-turn dialogue contexts.

Method: The paper employs a coarse-to-fine knowledge retrieval approach using Monte Carlo Tree Search, navigating through contextually relevant knowledge base sub-regions and refining search to align with reasoning processes.

Result: Experiments show improved alignment with conversational reasoning and enhanced diversity of retrieved knowledge, leading to informative and creative dialogue responses.

Conclusion: The proposed method optimizes knowledge retrieval for reasoning-based contexts, significantly advancing the informativeness and creativity of LLMs in dialogue scenarios.

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [2] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: This paper explores the use of fine-tuned large language models (LLMs) to perform depression screening in Nigerian Pidgin, a culturally and linguistically specific context.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of low accessibility and cultural inappropriateness of traditional depression screening tools in Nigeria due to stigma, language diversity, and limited healthcare resources.

Method: Researchers created a dataset of 432 Nigerian Pidgin audio responses aligning with PHQ-9 items, transcribed, annotated, and processed the data before fine-tuning three LLMs (Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1) for evaluation.

Result: GPT-4.1 showed the best performance in PHQ-9 severity scoring with 94.5% accuracy, surpassing the other models quantitatively and qualitatively in cultural appropriateness and clarity.

Conclusion: The findings establish the potential for AI-based mental-health tools to deliver culturally relevant and accessible depression screening for underserved communities, encouraging their deployment in similar settings.

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [3] [Toward a Physical Theory of Intelligence](https://arxiv.org/abs/2601.00021)
*Peter David Fagan*

Main category: cs.AI

TL;DR: This paper presents a physical theory of intelligence based on information processing and work generation within systems governed by physical laws, offering insights into biological processes, advanced dynamics, and implications for artificial intelligence.


<details>
  <summary>Details</summary>
Motivation: The aim is to unify the understanding of intelligence as a physical phenomenon, bridging conservation laws, information processing, and their role in intelligent behavior.

Method: The authors use the Conservation-Congruent Encoding framework to relate physical states to information, define intelligence quantitatively, establish physical laws governing intelligent systems, and analyze biological systems' dynamics.

Result: The study provides a framework explaining intelligent behavior's physical constraints, insights into biological systems like the brain's efficiency, and views on continuous dynamical circuits.

Conclusion: Intelligence is described as goal-directed work per unit of irreversibly processed information. The findings offer a unified physical perspective of intelligence and implications for AI safety and dynamics.

Abstract: We present a physical theory of intelligence grounded in irreversible information processing in systems constrained by conservation laws. An intelligent system is modelled as a coupled agent-environment process whose evolution transforms information into goal-directed work. To connect information to physical state, we introduce the Conservation-Congruent Encoding (CCE) framework, in which encodings correspond to metastable basins of attraction whose separability is enforced by conservation laws. Within this framework, intelligence is defined as the amount of goal-directed work produced per nat of irreversibly processed information. From this definition we derive a hierarchy of physical constraints governing information intake, irreversible computation, and work extraction in open systems. The framework reveals how long-horizon efficiency requires the preservation of internal informational structure, giving rise to self-modelling, and it establishes that physically embodied intelligent systems possess intrinsic epistemic limits analogous to incompleteness phenomena. Applying the theory to biological systems, we analyse how oscillatory and near-critical dynamics optimise the trade-off between information preservation, dissipation, and useful work, placing the brain near an efficient operating regime predicted by the framework. At the architectural level, we develop a theory of continuous dynamical circuits in which classical Boolean logic emerges as a special case of attractor selection, while more general invariant geometries support computational modes beyond fixed-point logic. Finally, we propose a physically grounded perspective on artificial intelligence safety based on irreversible information flow and structural homeostasis. Together, these results provide a unified, substrate-neutral account of intelligence as a physical phenomenon.

</details>


### [4] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: The paper proposes a multi-algorithm approach for balancing workloads among delivery workers in urban last-mile delivery systems by optimizing delivery time and addressing workload disparities.


<details>
  <summary>Details</summary>
Motivation: Traditional methods of assigning package deliveries often lead to inefficiencies and unbalanced workload distributions among last-mile delivery workers.

Method: The study uses a multi-algorithm methodology that integrates k-means clustering, evolutionary approaches, recursive assignments, and a hybrid evolutionary ensemble algorithm for assigning packages based on distance and workload considerations.

Result: The methodology was successfully applied to a real-world problem in Azuqueca de Henares, Spain, demonstrating its effectiveness in balancing workloads among delivery workers.

Conclusion: The proposed approach ensures equitable workload allocation, optimizes delivery efficiency, and addresses significant workload disparities in urban last-mile delivery systems.

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [5] [Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach](https://arxiv.org/abs/2601.00024)
*Purushottam Saha,Avirup Chakraborty,Sourish Sarkar,Subhamoy Maitra,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.AI

TL;DR: The paper proposes a new rule-based strategy for 13-card Indian Rummy using a unique hand-evaluation metric, MinDist, achieving better win rates through computational efficiency and opponent modeling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for a systematic and effective strategy in Indian Rummy, which involves complex probabilistic reasoning and decision-making under uncertainty.

Method: The researchers introduce a new hand-evaluation metric, MinDist, based on edit distance, and develop an efficient algorithm using dynamic pruning and pattern caching. They also incorporate opponent hand-modeling within a simulation.

Result: Empirical results demonstrate a significant improvement in win rates for agents employing the MinDist strategy compared to traditional heuristics.

Conclusion: The study makes a significant contribution to algorithmic strategy design for Rummy by providing an interpretable and effective approach that enhances gameplay performance.

Abstract: The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.

</details>


### [6] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

TL;DR: The study evaluates how generative AI systems interpret and recreate traditional architectural designs using three models and a specific evaluation framework.


<details>
  <summary>Details</summary>
Motivation: To understand how AI systems, when applied to vernacular architecture, interpret and potentially distort traditional design intelligence.

Method: Tested three AI diffusion models (Midjourney v6, DALL-E 3, DreamStudio with SDXL) on an architectural case study using a three-stage prompt approach and a five-criteria evaluation framework.

Result: AI systems effectively reconstruct geometric patterns but struggle with materials and climate-related reasoning. Reference images boost realism but hinder creativity, while lack of reference enhances creativity but reduces cultural accuracy.

Conclusion: Computational vernacular reasoning highlights AI's duality in adhering to visual resemblance versus achieving true architectural intelligence.

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [7] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: The paper introduces an LLM agent that extracts and generates causal feedback fuzzy cognitive maps (FCMs) from raw text and demonstrates autonomy while maintaining systematic controls.


<details>
  <summary>Details</summary>
Motivation: To design an LLM agent capable of semi-autonomous extraction of causal feedback fuzzy cognitive maps (FCMs) from raw text, allowing for adaptive learning and enhanced causal reasoning.

Method: The process involves a finely tuned three-step instruction sequence: extracting noun phrases, selecting FCM concept nodes, and inferring fuzzy causal edges. Experiments were conducted using essays, including Henry Kissinger's work, to validate the approach.

Result: The generated FCMs successfully converged to similar equilibrium dynamics as human-generated FCMs, despite differing node and edge numbers. A mixed FCM combining outputs from multiple LLMs showcased additional cognitive dynamics.

Conclusion: The study highlights the ability of LLM agents to semi-autonomously extract and structure causal knowledge with adaptive dynamics, bridging human and machine causal reasoning.

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [8] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: This paper introduces ReCiSt, a bio-inspired framework for self-healing in Distributed Computing Continuum Systems (DCCS) by emulating biological processes.


<details>
  <summary>Details</summary>
Motivation: Modern DCCS face frequent faults due to complexity and dynamic conditions, necessitating scalable and adaptive resilience strategies.

Method: ReCiSt leverages four computational layers (Containment, Diagnosis, Meta-Cognitive, and Knowledge) and uses LM-powered agents for autonomous fault management and self-healing.

Result: ReCiSt showed self-healing capabilities within seconds and minimal CPU usage during evaluations on public fault datasets, despite no baseline comparison.

Conclusion: ReCiSt exhibits significant potential for achieving resilience in DCCS, providing scalable self-healing mechanisms powered by advanced language models.

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [9] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

TL;DR: Mortar autonomously evolves game mechanics for automatic game design, evaluating their contribution to skill-based ordering scores in diverse and playable games.


<details>
  <summary>Details</summary>
Motivation: Manual game mechanic design is labor-intensive and expertise-heavy. Automating this process can lead to more efficient creation of diverse, engaging games.

Method: The system uses a combination of a quality-diversity algorithm and a large language model, evaluating mechanics by synthesizing complete games via tree search. Games are assessed on skill-based ordering (stronger players outperforming weaker ones).

Result: Mortar successfully designs games with diverse, playable mechanics that enhance skill-based ordering scores. Ablation and user studies validate its components and results.

Conclusion: Mortar demonstrates effectiveness at autonomously creating mechanics and games, showing potential in automating game design with diverse, skill-based outcomes.

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [10] [Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control](https://arxiv.org/abs/2601.00121)
*Yaqi Duan,Yichun Hu,Jiashuo Jiang*

Main category: cs.AI

TL;DR: The paper explores how Large Language Models (LLMs) can assist in inventory management for small and medium businesses and proposes a hybrid framework that distinguishes reasoning from calculation to overcome performance issues.


<details>
  <summary>Details</summary>
Motivation: Many small and medium-sized businesses struggle with inventory management due to a lack of advanced optimization expertise.

Method: The paper proposes a hybrid agentic framework where LLMs act as intelligent interfaces for parameter extraction and interpretation, while rigorous algorithms handle mathematical optimization.

Result: The proposed system reduces inventory costs by 32.1% compared to using GPT-4o as an end-to-end solver, and highlights the computational limitations of end-to-end LLM approaches.

Conclusion: LLMs are not replacements for operations research but can serve as accessible natural-language interfaces to rigorous policies, aiding non-experts in decision-making.

Abstract: Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant "hallucination tax": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.
  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned "digital twin" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.

</details>


### [11] [Constructing a Neuro-Symbolic Mathematician from First Principles](https://arxiv.org/abs/2601.00125)
*Keqin Xie*

Main category: cs.AI

TL;DR: The paper introduces Mathesis, a framework using neuro-symbolic architecture to overcome logical failures in LLMs by leveraging higher-order hypergraphs and a symbolic reasoning energy-based kernel.


<details>
  <summary>Details</summary>
Motivation: To address logical failures in complex reasoning tasks exhibited by large language models due to the absence of an internal axiomatic framework.

Method: The method involves representing mathematical states as higher-order hypergraphs and employing a Symbolic Reasoning Kernel (SRK) for mapping logical constraints into a continuous energy landscape, facilitating trainable proof search through energy minimization. It also integrates Monte Carlo Tree Search and Evolutionary Proof Search for multi-step reasoning.

Result: The proposed framework improves logical consistency and enables multi-step proof-search reasoning for better performance in reasoning tasks.

Conclusion: Mathesis demonstrates an innovative approach to equipping LLMs with advanced reasoning capabilities by blending symbolic and neural methods for logical problem solving.

Abstract: Large Language Models (LLMs) exhibit persistent logical failures in complex reasoning due to the lack of an internal axiomatic framework. We propose Mathesis, a neuro-symbolic architecture that encodes mathematical states as higher-order hypergraphs and uses a Symbolic Reasoning Kernel (SRK)--a differentiable logic engine that maps constraints to a continuous energy landscape. By defining a global energy function E(G), where zero energy implies logical consistency, the SRK yields gradient-based signals to train a Hypergraph Transformer Brain, turning proof search into energy minimization. Multi-step deduction is enabled via Monte Carlo Tree Search and Evolutionary Proof Search, guided by learned value functions and semantic unification.

</details>


### [12] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

TL;DR: The study evaluates confidence-based abstention in Vision-Language Models (VLMs) for video question answering, focusing on risks and robustness under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: To improve reliability and robustness of VLMs in high-stakes tasks using confidence-based methods for error control and selective predictions.

Method: The researchers analyze NExT-QA and Gemini 2.0 Flash datasets to explore confidence thresholding as a mechanism for managing error rates and investigate its robustness under different distributions.

Result: Confidence thresholding efficiently reduces in-distribution errors with smooth risk-coverage adjustments; findings on robustness under distribution shifts are being explored.

Conclusion: Confidence-based abstention is effective for error control in-distribution, but further research is needed to ensure robustness in shifted distributions.

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [13] [An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making](https://arxiv.org/abs/2601.00142)
*Tiansi Dong,Henry He,Pietro Liò,Mateja Jamnik*

Main category: cs.AI

TL;DR: This paper evaluates and compares three types of neural reasoning methods, concluding that neural reasoning with explicit model construction is the most reliable.


<details>
  <summary>Details</summary>
Motivation: To identify which neural reasoning methodology is more reliable and effective, particularly in dealing with different types of syllogistic reasoning tasks.

Method: The authors test three approaches—LLM reasoning, supervised learning reasoning, and explicit model-based reasoning—using tasks like syllogistic and disjunctive syllogistic reasoning. They propose Sphere Neural Networks for explicit reasoning, embedding concepts on an n-dimensional sphere.

Result: LLMs are limited, while supervised learning-based models suffer from catastrophic forgetting. The proposed Sphere Neural Networks effectively handle diverse reasoning tasks with logical rigour and outperform other methods.

Conclusion: Neural reasoning using explicit model construction, such as with Sphere Neural Networks, offers the most reliable and logical approach for reasoning tasks.

Abstract: This paper compares three methodological categories of neural reasoning: LLM reasoning, supervised learning-based reasoning, and explicit model-based reasoning. LLMs remain unreliable and struggle with simple decision-making that animals can master without extensive corpora training. Through disjunctive syllogistic reasoning testing, we show that reasoning via supervised learning is less appealing than reasoning via explicit model construction. Concretely, we show that an Euler Net trained to achieve 100.00% in classic syllogistic reasoning can be trained to reach 100.00% accuracy in disjunctive syllogistic reasoning. However, the retrained Euler Net suffers severely from catastrophic forgetting (its performance drops to 6.25% on already-learned classic syllogistic reasoning), and its reasoning competence is limited to the pattern level. We propose a new version of Sphere Neural Networks that embeds concepts as circles on the surface of an n-dimensional sphere. These Sphere Neural Networks enable the representation of the negation operator via complement circles and achieve reliable decision-making by filtering out illogical statements that form unsatisfiable circular configurations. We demonstrate that the Sphere Neural Network can master 16 syllogistic reasoning tasks, including rigorous disjunctive syllogistic reasoning, while preserving the rigour of classical syllogistic reasoning. We conclude that neural reasoning with explicit model construction is the most reliable among the three methodological categories of neural reasoning.

</details>


### [14] [FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems](https://arxiv.org/abs/2601.00227)
*Shanli Xing,Yiyan Zhai,Alexander Jiang,Yixin Dong,Yong Wu,Zihao Ye,Charlie Ruan,Yingyi Huang,Yineng Zhang,Liangsheng Yin,Aksara Bayyapu,Luis Ceze,Tianqi Chen*

Main category: cs.AI

TL;DR: FlashInfer-Bench introduces a structured framework to simplify the integration, evaluation, and deployment of AI-generated GPU kernels for large-scale inference systems.


<details>
  <summary>Details</summary>
Motivation: Current challenges prevent seamless integration of AI-generated GPU kernels into real-world inference systems.

Method: FlashInfer-Bench provides a unified schema (FlashInfer Trace), a curated dataset, benchmarking tools, a public leaderboard, and a mechanism to dynamically substitute efficient kernels into production systems.

Result: FlashInfer-Bench successfully evaluates LLM agents, compares GPU programming trade-offs, and demonstrates practical kernel improvements for large-scale inference.

Conclusion: FlashInfer-Bench establishes a reproducible and scalable pathway for enhancing and deploying AI-generated GPU kernels across inference systems.

Abstract: Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.

</details>


### [15] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: The paper discovers that LLM agents can show intergroup bias affecting humans as an outgroup, introduces new attack methods (Belief Poisoning Attack), and explores mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to understand and address biases in LLM agents, especially biases leading to humans being categorized as an outgroup.

Method: The authors perform multi-agent simulations with payoff trade-offs and introduce controlled experiments targeting agent beliefs via attacks like Belief Poisoning.

Result: Agents exhibit inherent intergroup biases even under minimal cues; vulnerability to belief manipulation (BPA-PP, BPA-MP) is identified and experimentally verified.

Conclusion: This paper emphasizes the need for robust frameworks to prevent biases and belief manipulation in LLM agents to ensure safer design practices.

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [16] [ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization](https://arxiv.org/abs/2601.00290)
*Sixue Xing,Xuanye Xia,Kerui Wu,Meng Jiang,Jintai Chen,Tianfan Fu*

Main category: cs.AI

TL;DR: The paper introduces ClinicalReTrial, an AI agent framework for iteratively improving clinical trial designs by identifying flaws and testing modifications.


<details>
  <summary>Details</summary>
Motivation: Clinical trials often fail due to minor design flaws despite promising drugs. Existing AI tools predict success but lack actionable solutions for redesigning problematic trials.

Method: ClinicalReTrial employs a reward-driven optimization framework that diagnoses failures and iteratively redesigns protocols using a simulation environment and hierarchical memory.

Result: Empirical studies show ClinicalReTrial improved 83.3% of trial protocols and increased success probability by 5.7%. Retrospective analyses confirmed alignment with real-world adjustments.

Conclusion: ClinicalReTrial offers a proactive AI-based solution for optimizing clinical trial designs, addressing limitations in current prediction-only methods and reducing trial failure risks.

Abstract: Clinical trial failure remains a central bottleneck in drug development, where minor protocol design flaws can irreversibly compromise outcomes despite promising therapeutics. Although cutting-edge AI methods achieve strong performance in predicting trial success, they are inherently reactive for merely diagnosing risk without offering actionable remedies once failure is anticipated. To fill this gap, this paper proposes ClinicalReTrial, a self-evolving AI agent framework that addresses this gap by casting clinical trial reasoning as an iterative protocol redesign problem. Our method integrates failure diagnosis, safety-aware modification, and candidate evaluation in a closed-loop, reward-driven optimization framework. Serving the outcome prediction model as a simulation environment, ClinicalReTrial enables low-cost evaluation of protocol modifications and provides dense reward signals for continuous self-improvement. To support efficient exploration, the framework maintains hierarchical memory that captures iteration-level feedback within trials and distills transferable redesign patterns across trials. Empirically, ClinicalReTrial improves 83.3% of trial protocols with a mean success probability gain of 5.7%, and retrospective case studies demonstrate strong alignment between the discovered redesign strategies and real-world clinical trial modifications.

</details>


### [17] [Multiagent Reinforcement Learning for Liquidity Games](https://arxiv.org/abs/2601.00324)
*Alicia Vidler,Gal A. Kaminka*

Main category: cs.AI

TL;DR: The paper unifies liquidity games and rational swarms to model individual self-interested behaviors leading to collective market efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance financial market models using swarm methods and advance the understanding of independent agents in swarm and financial analysis.

Method: Theoretical framework involving a swarm of traders maximizing liquidity using difference rewards in Markov team games.

Result: Individual liquidity-maximizing behaviors result in overall market liquidity without direct coordination.

Conclusion: Financial Swarm model achieves both individual profitability and collective market efficiency through decentralized rational agents.

Abstract: Making use of swarm methods in financial market modeling of liquidity, and techniques from financial analysis in swarm analysis, holds the potential to advance both research areas. In swarm research, the use of game theory methods holds the promise of explaining observed phenomena of collective utility adherence with rational self-interested swarm participants. In financial markets, a better understanding of how independent financial agents may self-organize for the betterment and stability of the marketplace would be a boon for market design researchers. This paper unifies Liquidity Games, where trader payoffs depend on aggregate liquidity within a trade, with Rational Swarms, where decentralized agents use difference rewards to align self-interested learning with global objectives. We offer a theoretical frameworks where we define a swarm of traders whose collective objective is market liquidity provision while maintaining agent independence. Using difference rewards within a Markov team games framework, we show that individual liquidity-maximizing behaviors contribute to overall market liquidity without requiring coordination or collusion. This Financial Swarm model provides a framework for modeling rational, independent agents where they achieve both individual profitability and collective market efficiency in bilateral asset markets.

</details>


### [18] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

TL;DR: This paper introduces the Adaptive Causal Coordination Detection (ACCD) framework to address the challenges in detecting coordinated inauthentic behavior on social media with improved accuracy, reduced manual effort, and enhanced efficiency.


<details>
  <summary>Details</summary>
Motivation: The need to overcome limitations in existing methods for detecting coordinated inauthentic activity on social media, including reliance on correlation analysis, static configurations, and manual annotation.

Method: The ACCD framework utilizes a three-stage architecture: adaptive Convergent Cross Mapping (CCM) to find causal relationships, active learning with semi-supervised classification to reduce labeling needs, and an automated validation module for self-optimization.

Result: The framework demonstrated an F1-score improvement (87.3%, up 15.2% from the baseline), a 68% reduction in manual annotation, and a 2.8x processing speedup in evaluations using multiple real-world datasets.

Conclusion: ACCD is an innovative and automated system that significantly enhances the detection of coordinated inauthentic behavior, offering accuracy, efficiency, and scalability for social media platforms.

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [19] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

TL;DR: This paper applies semantic-space reasoning methods from linguistics to tactical decision-making in team sports by modeling players as vectors and teams as semantic configurations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance tactical decision-making in team sports using methods inspired by computational linguistics and semantic reasoning.

Method: Players are modeled as multidimensional vectors and aggregated into team profiles within a shared vector-space to evaluate tactical strategies using alignment metrics.

Result: A Python-based prototype has been developed, showcasing interpretable and adaptive strategy recommendations along with detailed attribute-level diagnostics.

Conclusion: The paper concludes that this approach is generalizable across team-based domains and emphasizes future work on data integration, predictive simulation, and hybrid human-machine intelligence.

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [20] [Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation](https://arxiv.org/abs/2601.00475)
*Sankar B,Srinidhi Ranjini Girish,Aadya Bharti,Dibakar Sen*

Main category: cs.AI

TL;DR: The paper introduces MIDAS, an AI framework mimicking human meta-cognitive ideation, to help novice designers generate novel and diverse ideas more effectively.


<details>
  <summary>Details</summary>
Motivation: The difficulty for novice designers to generate unique and diverse ideas, compounded by existing AI systems that cluster ideas too narrowly, drives the need for a better solution.

Method: MIDAS employs a distributed system of specialized AI agents that emulate human meta-cognitive ideation, refining ideas while assessing global and local novelty levels.

Result: MIDAS refines ideas iteratively, ensuring novelty and diversity, and enables a more active human-AI collaboration in design processes.

Conclusion: This distributed AI system transforms the human designer's role, fostering true co-creation rather than passive idea filtering.

Abstract: The generation of truly novel and diverse ideas is important for contemporary engineering design, yet it remains a significant cognitive challenge for novice designers. Current 'single-spurt' AI systems exacerbate this challenge by producing a high volume of semantically clustered ideas. We propose MIDAS (Meta-cognitive Ideation through Distributed Agentic AI System), a novel framework that replaces the single-AI paradigm with a distributed 'team' of specialized AI agents designed to emulate the human meta-cognitive ideation workflow. This agentic system progressively refines ideas and assesses each one for both global novelty (against existing solutions) and local novelty (against previously generated ideas). MIDAS, therefore, demonstrates a viable and progressive paradigm for true human-AI co-creation, elevating the human designer from a passive filterer to a participatory, active, collaborative partner.

</details>


### [21] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

TL;DR: The paper examines whether reasoning models experience intrinsic 'aha moments' or mid-reasoning shifts. It concludes that such shifts are rare, don't improve accuracy, and are not intrinsic to model insight.


<details>
  <summary>Details</summary>
Motivation: To determine whether 'aha moments' in reasoning models reflect an intrinsic self-correction mechanism or are merely symptoms of other model behaviors.

Method: The study investigates mid-reasoning shifts by analyzing over 1 million reasoning traces from multiple training checkpoints, model architectures, and reasoning domains. It also explores the impact of extrinsic triggers on reasoning accuracy.

Result: The study finds that intrinsic reasoning shifts are rare, unrelated to training improvements, and seldom improve accuracy. Artificial extrinsic shifts, however, improve performance under high-uncertainty conditions.

Conclusion: Mid-reasoning shifts are signs of unstable inference rather than an intrinsic self-correction feature. Extrinsic triggers can enhance accuracy under certain conditions.

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [22] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

TL;DR: The paper proposes DA-DPO, a method to address overfitting in DPO approaches for multimodal models by reweighting training samples based on difficulty, improving robustness against hallucinations.


<details>
  <summary>Details</summary>
Motivation: Current Direct Preference Optimization (DPO) approaches for Multimodal Large Language Models (MLLMs) struggle with overfitting caused by imbalance in preference data, significantly impacting hallucination suppression and performance.

Method: The proposed DA-DPO method includes (1) Difficulty Estimation, which assigns robust difficulty scores to preference pairs using pre-trained vision–language models paired with a voting strategy, and (2) Difficulty-Aware Training, which reweights training data to emphasize harder samples and reduce focus on easier ones.

Result: DA-DPO demonstrates improved robustness to hallucinations and generalization in multimodal benchmarks, achieving superior performance while remaining computationally efficient.

Conclusion: DA-DPO effectively improves multimodal preference optimization by addressing overfitting through difficulty-aware techniques, enhancing learning without the need for extra data or fine-tuning steps.

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [23] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

TL;DR: This paper introduces PedX-LLM, a vision-and-knowledge-enhanced framework using LLaMA-2-7B to achieve superior pedestrian crossing inference compared to existing methods, especially in unseen environments.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitations of current models for pedestrian crossing behavior inference, which lack generalizability and perform poorly in new or unseen environments. It explores how large language models, when adapted with domain knowledge and visual context, can enhance behavioral reasoning.

Method: The paper proposes PedX-LLM, a model that integrates visual features from LLaVA with textual transportation knowledge. Using LoRA, it fine-tunes the LLaMA-2-7B model to predict crossing decisions with improved generalizability and accuracy.

Result: PedX-LLM achieves 82.0% balanced accuracy, surpassing statistical and supervised learning methods. It performs well in cross-site validation, achieving 66.9% balanced accuracy on unseen sites, with gains from few-shot learning boosting accuracy to 72.2%. Vision and knowledge integration contribute significantly to its performance.

Conclusion: PedX-LLM exhibits strong generalizability and human-like reasoning for pedestrian crossing behavior, resolving the limitations of traditional data-driven models by leveraging vision-and-knowledge-enhanced approaches.

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>


### [24] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: This paper introduces AgenticDomiKnowS (ADS), a system that automates the creation of symbolic constraints in deep learning models, significantly reducing development time for both new and experienced users.


<details>
  <summary>Details</summary>
Motivation: The integration of symbolic constraints into deep learning models is known to improve robustness, interpretability, and efficiency but is often difficult and time-consuming due to complex frameworks requiring expertise.

Method: AgenticDomiKnowS (ADS) employs an agentic workflow that processes free-form task descriptions into DomiKnowS programs, with a mechanism for testing each component individually. The system also facilitates optional human intervention for refinement.

Result: ADS enables both experienced and novice users of DomiKnowS to construct neuro-symbolic programs in only 10-15 minutes, a dramatic reduction from hours.

Conclusion: ADS simplifies and accelerates the integration of symbolic constraints in deep learning models, fostering accessibility for a broader range of users and enhancing development efficiency.

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [25] [Enhancing Reliability of STT-MRAM Caches by Eliminating Read Disturbance Accumulation](https://arxiv.org/abs/2601.00450)
*Elham Cheshmikhani,Hamed Farbeh,Hossein Asadi*

Main category: cs.AR

TL;DR: This paper addresses the read disturbance error in STT-MRAM caches caused by parallel block reads, proposing the REAP-cache to mitigate this issue effectively.


<details>
  <summary>Details</summary>
Motivation: To address the reliability issues in STT-MRAM caches caused by high read disturbance error rates, exacerbated by the parallel reading mechanism in modern processors.

Method: The study introduces the REAP-cache, a novel technique that prevents read disturbance error accumulation during cache accesses while maintaining performance. The approach is simple and designed to seamlessly integrate into existing systems.

Result: The proposed REAP-cache drastically extends cache MTTF by 171x. It does so with a minimal increase in cache area by less than 1% and energy consumption by only 2.7%.

Conclusion: The REAP-cache presents an efficient solution to enhance cache reliability in STT-MRAM without significant performance or resource compromises, making it a viable improvement for modern processor caches.

Abstract: Spin-Transfer Torque Magnetic RAM (STT-MRAM) as one of the most promising replacements for SRAMs in on-chip cache memories benefits from higher density and scalability, near-zero leakage power, and non-volatility, but its reliability is threatened by high read disturbance error rate. Error-Correcting Codes (ECCs) are conventionally suggested to overcome the read disturbance errors in STT-MRAM caches. By employing aggressive ECCs and checking out a cache block on every read access, a high level of cache reliability is achieved. However, to minimize the cache access time in modern processors, all blocks in the target cache set are simultaneously read in parallel for tags comparison operation and only the requested block is sent out, if any, after checking its ECC. These extra cache block reads without checking their ECCs until requesting the blocks by the processor cause the accumulation of read disturbance error, which significantly degrade the cache reliability. In this paper, we first introduce and formulate the read disturbance accumulation phenomenon and reveal that this accumulation due to conventional parallel accesses of cache blocks significantly increases the cache error rate. Then, we propose a simple yet effective scheme, so-called Read Error Accumulation Preventer cache (REAP-cache), to completely eliminate the accumulation of read disturbances without compromising the cache performance. Our evaluations show that the proposed REAP-cache extends the cache Mean Time To Failure (MTTF) by 171x, while increases the cache area by less than 1% and energy consumption by only 2.7%.

</details>


### [26] [ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches](https://arxiv.org/abs/2601.00456)
*Elham Cheshmikhani,Hamed Farbeh,Hossein Asadi*

Main category: cs.AR

TL;DR: STT-MRAM is explored as a substitute for SRAMs, addressing its high error rate through the inefficiency of conventional ECC and introducing a new method called ROBIN.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of conventional ECC in managing data-dependent error patterns in STT-MRAM used in on-chip cache memories.

Method: The study provides a detailed analysis of conventional ECC inefficiency and proposes a new efficient ECC configuration named ROBIN.

Result: Conventional ECC increases cache error rates by 151.7% on average, whereas ROBIN reduces errors by over 28.6x.

Conclusion: Introducing ROBIN enhances the correction capabilities in STT-MRAM, making it more suitable for on-chip cache memory applications.

Abstract: Spin-Transfer Torque Magnetic RAM} (STT-MRAM) is a promising alternative for SRAMs in on-chip cache memories. Besides all its advantages, high error rate in STT-MRAM is a major limiting factor for on-chip cache memories. In this paper, we first present a comprehensive analysis that reveals that the conventional Error-Correcting Codes (ECCs) lose their efficiency due to data-dependent error patterns, and then propose an efficient ECC configuration, so-called ROBIN, to improve the correction capability. The evaluations show that the inefficiency of conventional ECC increases the cache error rate by an average of 151.7% while ROBIN reduces this value by more than 28.6x.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [27] [RIMRULE: Improving Tool-Using Language Agents via MDL-Guided Rule Learning](https://arxiv.org/abs/2601.00086)
*Xiang Gao,Yuguang Yao,Qi Zhang,Kaiwen Dong,Avinash Baidya,Ruocheng Guo,Hilaf Hasson,Kamalika Das*

Main category: cs.CL

TL;DR: The paper introduces RIMRULE, a neuro-symbolic approach to enhance large language models (LLMs) in using task-specific tools through dynamic rule injection.


<details>
  <summary>Details</summary>
Motivation: LLMs often face challenges using domain-specific tools due to non-standard APIs or insufficient documentation, necessitating better adaptation methods.

Method: RIMRULE extracts compact and interpretable rules from model failure instances, consolidates them using a Minimum Description Length (MDL) objective, and injects them into prompts at inference. These rules are stored in natural language and symbolic forms to support efficient retrieval.

Result: The approach enhances performance on both familiar and unfamiliar tools, outperforming standard adaptation methods and showing compatibility with finetuning. Rules can also be reused across different LLM architectures.

Conclusion: RIMRULE demonstrates the potential of symbolic rule-based adaptation to improve LLM efficiency and portability in tool usage without altering their weights.

Abstract: Large language models (LLMs) often struggle to use tools reliably in domain-specific settings, where APIs may be idiosyncratic, under-documented, or tailored to private workflows. This highlights the need for effective adaptation to task-specific tools. We propose RIMRULE, a neuro-symbolic approach for LLM adaptation based on dynamic rule injection. Compact, interpretable rules are distilled from failure traces and injected into the prompt during inference to improve task performance. These rules are proposed by the LLM itself and consolidated using a Minimum Description Length (MDL) objective that favors generality and conciseness. Each rule is stored in both natural language and a structured symbolic form, supporting efficient retrieval at inference time. Experiments on tool-use benchmarks show that this approach improves accuracy on both seen and unseen tools without modifying LLM weights. It outperforms prompting-based adaptation methods and complements finetuning. Moreover, rules learned from one LLM can be reused to improve others, including long reasoning LLMs, highlighting the portability of symbolic knowledge across architectures.

</details>


### [28] [Universal Adaptive Constraint Propagation: Scaling Structured Inference for Large Language Models via Meta-Reinforcement Learning](https://arxiv.org/abs/2601.00095)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CL

TL;DR: MetaJuLS leverages meta-reinforcement learning for universal constraint propagation, achieving faster and efficient structured inference across languages and tasks.


<details>
  <summary>Details</summary>
Motivation: Structured inference in large language models often requires satisfying complex constraints across diverse languages and tasks, motivating efficient approaches without task-specific retraining.

Method: MetaJuLS uses meta-reinforcement learning and trains a Graph Attention Network to learn universal constraint propagation policies that adapt across tasks and languages.

Result: MetaJuLS achieves 1.5–2x speedups over GPU-optimized baselines, retaining accuracy within 0.2% of state-of-the-art parsers. It adapts to new tasks and languages in 5–10 gradient steps, demonstrating rapid cross-domain usability.

Conclusion: MetaJuLS efficiently resolves structured inference challenges, contributing to Green AI by minimizing propagation steps and reducing the carbon footprint during large language model deployments.

Abstract: Large language models increasingly require structured inference, from JSON schema enforcement to multi-lingual parsing, where outputs must satisfy complex constraints. We introduce MetaJuLS, a meta-reinforcement learning approach that learns universal constraint propagation policies applicable across languages and tasks without task-specific retraining. By formulating structured inference as adaptive constraint propagation and training a Graph Attention Network with meta-learning, MetaJuLS achieves 1.5--2.0$\times$ speedups over GPU-optimized baselines while maintaining within 0.2\% accuracy of state-of-the-art parsers. On Universal Dependencies across 10 languages and LLM-constrained generation (LogicBench, GSM8K-Constrained), MetaJuLS demonstrates rapid cross-domain adaptation: a policy trained on English parsing adapts to new languages and tasks with 5--10 gradient steps (5--15 seconds) rather than requiring hours of task-specific training. Mechanistic analysis reveals the policy discovers human-like parsing strategies (easy-first) and novel non-intuitive heuristics. By reducing propagation steps in LLM deployments, MetaJuLS contributes to Green AI by directly reducing inference carbon footprint.

</details>


### [29] [Pat-DEVAL: Chain-of-Legal-Thought Evaluation for Patent Description](https://arxiv.org/abs/2601.00166)
*Yongmin Yoo,Kris W Pan*

Main category: cs.CL

TL;DR: The paper introduces Pat-DEVAL, a comprehensive evaluation framework for patent descriptions, emphasizing legal coherence and compliance using a novel Chain-of-Legal-Thought mechanism.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for automated patent drafting fail to address structural coherence and strict legal requirements in patent descriptions.

Method: The authors developed Pat-DEVAL, employing an LLM-based judge paradigm with a Chain-of-Legal-Thought mechanism for sequential patent-law-specific analysis.

Result: The framework, validated by experts, shows a high Pearson correlation of 0.69, outperforming existing methods and achieving a superior score in Legal-Professional Compliance at 0.73.

Conclusion: Pat-DEVAL sets a new benchmark for balancing technical and legal aspects in automated patent drafting, enabling its practical implementation.

Abstract: Patent descriptions must deliver comprehensive technical disclosure while meeting strict legal standards such as enablement and written description requirements. Although large language models have enabled end-to-end automated patent drafting, existing evaluation approaches fail to assess long-form structural coherence and statutory compliance specific to descriptions. We propose Pat-DEVAL, the first multi-dimensional evaluation framework dedicated to patent description bodies. Leveraging the LLM-as-a-judge paradigm, Pat-DEVAL introduces Chain-of-Legal-Thought (CoLT), a legally-constrained reasoning mechanism that enforces sequential patent-law-specific analysis. Experiments validated by patent expert on our Pap2Pat-EvalGold dataset demonstrate that Pat-DEVAL achieves a Pearson correlation of 0.69, significantly outperforming baseline metrics and existing LLM evaluators. Notably, the framework exhibits a superior correlation of 0.73 in Legal-Professional Compliance, proving that the explicit injection of statutory constraints is essential for capturing nuanced legal validity. By establishing a new standard for ensuring both technical soundness and legal compliance, Pat-DEVAL provides a robust methodological foundation for the practical deployment of automated patent drafting systems.

</details>


### [30] [Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation](https://arxiv.org/abs/2601.00181)
*Cheonkam Jeong,Adeline Nyamathi*

Main category: cs.CL

TL;DR: The paper focuses on addressing architectural choices in Emotion Recognition in Conversation (ERC) and understanding linguistic connections between emotion recognition and generation, analyzed using the IEMOCAP dataset.


<details>
  <summary>Details</summary>
Motivation: To address the gaps in ERC regarding understanding which architectural elements are crucial and exploring linguistic aspects linking recognition to generation.

Method: Conducting a systematic analysis and rigorous ablation studies on the IEMOCAP dataset, combining evaluations of conversational context, hierarchical representations, and external lexical resources.

Result: Simple architectures considering conversational context achieved F1 scores of 82.69% (4-way) and 67.07% (6-way). Additionally, discourse marker analysis revealed emotional links, particularly for sadness, emphasizing context importance.

Conclusion: The study highlights the importance of conversational context, questions hierarchical benefits, and discounts external lexicons. Furthermore, it connects linguistic patterns to emotion recognition, such as the unique behavior of sadness and its reliance on context.

Abstract: While Emotion Recognition in Conversation (ERC) has achieved high accuracy, two critical gaps remain: a limited understanding of \textit{which} architectural choices actually matter, and a lack of linguistic analysis connecting recognition to generation. We address both gaps through a systematic analysis of the IEMOCAP dataset.
  For recognition, we conduct a rigorous ablation study with 10-seed evaluation and report three key findings. First, conversational context is paramount, with performance saturating rapidly -- 90\% of the total gain achieved within just the most recent 10--30 preceding turns (depending on the label set). Second, hierarchical sentence representations help at utterance-level, but this benefit disappears once conversational context is provided, suggesting that context subsumes intra-utterance structure. Third, external affective lexicons (SenticNet) provide no gain, indicating that pre-trained encoders already capture necessary emotional semantics. With simple architectures using strictly causal context, we achieve 82.69\% (4-way) and 67.07\% (6-way) weighted F1, outperforming prior text-only methods including those using bidirectional context.
  For linguistic analysis, we analyze 5,286 discourse marker occurrences and find a significant association between emotion and marker positioning ($p < .0001$). Notably, "sad" utterances exhibit reduced left-periphery marker usage (21.9\%) compared to other emotions (28--32\%), consistent with theories linking left-periphery markers to active discourse management. This connects to our recognition finding that sadness benefits most from context (+22\%p): lacking explicit pragmatic signals, sad utterances require conversational history for disambiguation.

</details>


### [31] [Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2601.00202)
*Wang Xing,Wei Song,Siyu Lin,Chen Wu,Zhesi Li,Man Wang*

Main category: cs.CL

TL;DR: The paper introduces a distillation framework for temporal knowledge graph reasoning, using large language models to improve the efficiency and compactness of student models, while maintaining reasoning performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of current temporal knowledge graph (TKG) reasoning models, which often need large computation resources and fail to effectively capture temporal dependencies. They aim to make reasoning models suitable for resource-constrained environments.

Method: The paper proposes a distillation framework that uses large language models as teacher models. This approach transfers both structural and temporal reasoning skills to lightweight student models, integrating large-scale public knowledge with temporal information.

Result: Their method demonstrated superior reasoning performance compared to strong baselines, providing an effective balance between accuracy, computational efficiency, and deployability.

Conclusion: A novel distillation framework significantly improves temporal reasoning accuracy while addressing the issues of size, computation demand, and practical application challenges in TKG reasoning.

Abstract: Reasoning over temporal knowledge graphs (TKGs) is fundamental to improving the efficiency and reliability of intelligent decision-making systems and has become a key technological foundation for future artificial intelligence applications. Despite recent progress, existing TKG reasoning models typically rely on large parameter sizes and intensive computation, leading to high hardware costs and energy consumption. These constraints hinder their deployment on resource-constrained, low-power, and distributed platforms that require real-time inference. Moreover, most existing model compression and distillation techniques are designed for static knowledge graphs and fail to adequately capture the temporal dependencies inherent in TKGs, often resulting in degraded reasoning performance. To address these challenges, we propose a distillation framework specifically tailored for temporal knowledge graph reasoning. Our approach leverages large language models as teacher models to guide the distillation process, enabling effective transfer of both structural and temporal reasoning capabilities to lightweight student models. By integrating large-scale public knowledge with task-specific temporal information, the proposed framework enhances the student model's ability to model temporal dynamics while maintaining a compact and efficient architecture. Extensive experiments on multiple publicly available benchmark datasets demonstrate that our method consistently outperforms strong baselines, achieving a favorable trade-off between reasoning accuracy, computational efficiency, and practical deployability.

</details>


### [32] [From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark](https://arxiv.org/abs/2601.00216)
*Jinning Zhang,Jie Song,Wenhui Tu,Zecheng Li,Jingxuan Li,Jin Li,Xuan Liu,Taole Sha,Zichen Wei,Yan Li*

Main category: cs.CL

TL;DR: This study integrates evidence-based medicine (EBM) principles into retrieval-augmented generation (RAG) for medical applications by aligning queries with PICO frameworks and considering evidence hierarchies, demonstrating its effectiveness in sports rehabilitation.


<details>
  <summary>Details</summary>
Motivation: LLMs in medicine often rely on RAG for grounded outputs but lack adherence to evidence-based medicine principles, specifically PICO alignment and evidence hierarchy consideration.

Method: The authors proposed a graph-based RAG system incorporating EBM by constructing a PICO-aligned knowledge graph and using a Bayesian-inspired algorithm for reranking evidence based on grade. They validated it with a sports rehabilitation knowledge graph and benchmark QA pairs.

Result: The system showed high performance with metrics including 0.830 nugget coverage and 0.788 PICOT match accuracy. Clinicians also rated the system highly on aspects such as factual accuracy, relevance, and PICO alignment (4.66-4.84 on a 5-point scale).

Conclusion: This adaptation of EBM principles significantly improves the retrieval and response quality of RAG systems, and the resources released can advance RAG benchmarking in sports rehabilitation and other fields.

Abstract: In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.

</details>


### [33] [JP-TL-Bench: Anchored Pairwise LLM Evaluation for Bidirectional Japanese-English Translation](https://arxiv.org/abs/2601.00223)
*Leonard Lin,Adam Lensenmayer*

Main category: cs.CL

TL;DR: JP-TL-Bench is an open benchmark for Japanese-English translation, focusing on nuanced translation comparisons using pairwise evaluations and stable scoring mechanisms.


<details>
  <summary>Details</summary>
Motivation: Japanese-English translations often require evaluations beyond acceptability, involving subtleties like politeness and register, demanding reliable judgment methods.

Method: JP-TL-Bench uses pairwise comparisons against a fixed anchor set, aggregated via Bradley-Terry modeling, and scored with a stable protocol.

Result: The benchmark enables consistent and affordable assessments of translation systems, with scores being versioned and repeatable under fixed conditions.

Conclusion: JP-TL-Bench provides a structured and reliable mechanism to evaluate and improve Japanese-English translation systems focusing on nuanced aspects of translation quality.

Abstract: We introduce JP-TL-Bench, a lightweight, open benchmark designed to guide the iterative development of Japanese-English translation systems. In this context, the challenge is often "which of these two good translations is better?" rather than "is this translation acceptable?" This distinction matters for Japanese-English, where subtle choices in politeness, implicature, ellipsis, and register strongly affect perceived naturalness. JP-TL-Bench uses a protocol built to make LLM judging both reliable and affordable: it evaluates a candidate model via reference-free, pairwise LLM comparisons against a fixed, versioned anchor set. Pairwise results are aggregated with a Bradley-Terry model and reported as win rates plus a normalized 0-10 "LT" score derived from a logistic transform of fitted log-strengths. Because each candidate is scored against the same frozen anchor set, scores are structurally stable given the same base set, judge, and aggregation code.

</details>


### [34] [Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback](https://arxiv.org/abs/2601.00224)
*Yan Sun,Ming Cai,Stanley Kok*

Main category: cs.CL

TL;DR: This paper introduces Q* and Feedback+, two verification methods to enhance the accuracy of large language models (LLMs) in conversational business analytics, reducing user responsibility for validation and error rates.


<details>
  <summary>Details</summary>
Motivation: The increasing integration of LLMs into enterprise workflows necessitates reliable and accurate outputs, but current systems lack validation mechanisms, demanding manual user verification.

Method: Two complementary techniques, Q* for reverse translation and semantic matching, and Feedback+ for execution feedback-driven code refinement, are embedded in a generator-discriminator framework to enhance verification.

Result: Evaluations on Spider, Bird, and GSM8K datasets show that these methods reduce error rates and task completion time, although reverse translation is identified as a bottleneck.

Conclusion: This work presents a framework for developing reliable, enterprise-grade GenAI systems, addressing verification gaps and improving decision support for business analytics.

Abstract: As large language model (LLM) assistants become increasingly integrated into enterprise workflows, their ability to generate accurate, semantically aligned, and executable outputs is critical. However, current conversational business analytics (CBA) systems often lack built-in verification mechanisms, leaving users to manually validate potentially flawed results. This paper introduces two complementary verification techniques: Q*, which performs reverse translation and semantic matching between code and user intent, and Feedback+, which incorporates execution feedback to guide code refinement. Embedded within a generator-discriminator framework, these mechanisms shift validation responsibilities from users to the system. Evaluations on three benchmark datasets, Spider, Bird, and GSM8K, demonstrate that both Q* and Feedback+ reduce error rates and task completion time. The study also identifies reverse translation as a key bottleneck, highlighting opportunities for future improvement. Overall, this work contributes a design-oriented framework for building more reliable, enterprise-grade GenAI systems capable of trustworthy decision support.

</details>


### [35] [Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation](https://arxiv.org/abs/2601.00263)
*Qianli Wang,Van Bach Nguyen,Yihong Liu,Fedor Splitt,Nils Feldhus,Christin Seifert,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: The study explores the generation of multilingual counterfactuals by large language models (LLMs), compares translation-based and directly generated methods, identifies common patterns and error types, and assesses their impact on model performance.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in understanding how LLMs perform in generating counterfactuals across multiple languages, which can explain model behavior and improve performance on multilingual tasks.

Method: The authors systematically evaluate generated counterfactuals (both directly in the target language and through translation), analyze edit patterns, categorize consistent error types, and assess the impact of counterfactual data augmentation (CDA) across multilingual contexts.

Result: Translation-based counterfactuals have higher validity but require more modifications compared to directly generated ones. Editing patterns share common strategies across languages, and four main error types are identified. Multilingual CDA is more effective than cross-lingual CDA, particularly for low-resource languages.

Conclusion: While LLMs demonstrate potential in multilingual counterfactual generation, limitations in the generated counterfactuals constrain improvements in model performance and robustness.

Abstract: Counterfactuals refer to minimally edited inputs that cause a model's prediction to change, serving as a promising approach to explaining the model's behavior. Large language models (LLMs) excel at generating English counterfactuals and demonstrate multilingual proficiency. However, their effectiveness in generating multilingual counterfactuals remains unclear. To this end, we conduct a comprehensive study on multilingual counterfactuals. We first conduct automatic evaluations on both directly generated counterfactuals in the target languages and those derived via English translation across six languages. Although translation-based counterfactuals offer higher validity than their directly generated counterparts, they demand substantially more modifications and still fall short of matching the quality of the original English counterfactuals. Second, we find the patterns of edits applied to high-resource European-language counterfactuals to be remarkably similar, suggesting that cross-lingual perturbations follow common strategic principles. Third, we identify and categorize four main types of errors that consistently appear in the generated counterfactuals across languages. Finally, we reveal that multilingual counterfactual data augmentation (CDA) yields larger model performance improvements than cross-lingual CDA, especially for lower-resource languages. Yet, the imperfections of the generated counterfactuals limit gains in model performance and robustness.

</details>


### [36] [Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity](https://arxiv.org/abs/2601.00268)
*Doyoung Kim,Zhiwei Ren,Jie Hao,Zhongkai Sun,Lichao Wang,Xiyao Ma,Zack Ye,Xu Han,Jun Yin,Heng Ji,Wei Shen,Xing Fan,Benjamin Yao,Chenlei Guo*

Main category: cs.CL

TL;DR: WildAGTEval evaluates LLM agents' function-calling capabilities under realistic API complexities, such as documentation constraints and runtime challenges.


<details>
  <summary>Details</summary>
Motivation: To address the gap in benchmarks considering real-world complexities in API systems faced by LLM agents.

Method: WildAGTEval introduces an API system with 60 scenarios combinable into 32K configurations, providing user-agent interaction frameworks for LLM assessment.

Result: Analysis reveals scenarios are challenging for advanced LLMs, particularly irrelevant information, reducing performance by 27.3%. LLMs distort user intent during task completion.

Conclusion: Benchmarks based on realistic API complexities highlight significant challenges in LLM function-calling capabilities impacting user satisfaction.

Abstract: We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.

</details>


### [37] [Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations](https://arxiv.org/abs/2601.00282)
*Qianli Wang,Nils Feldhus,Pepa Atanasova,Fedor Splitt,Simon Ostermann,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: This paper examines the effects of quantization on self-explanations (SEs) generated by large language models, finding moderate declines in quality and faithfulness.


<details>
  <summary>Details</summary>
Motivation: Understanding whether quantization degrades the quality and faithfulness of self-explanations generated by large language models for transparency in high-stakes applications.

Method: Quantizes LLMs using three techniques at distinct bit widths, analyzes two types of SEs (natural language explanations and counterfactual examples), and conducts user studies to assess coherence and trustworthiness.

Result: Quantization results in moderate declines in SE quality (up to 4.4%) and faithfulness (up to 2.38%) while affecting coherence and trustworthiness (up to 8.5%). Larger models are more resilient in terms of faithfulness.

Conclusion: While quantization causes moderate degradation in SE quality and faithfulness, it remains an effective compression method. Validation of SE quality is recommended for specific use cases, especially for sensitive explanations.

Abstract: Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\%) and faithfulness (up to 2.38\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.

</details>


### [38] [DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection](https://arxiv.org/abs/2601.00303)
*Yuxin Li,Xiangyu Zhang,Yifei Li,Zhiwei Guo,Haoyang Zhang,Eng Siong Chng,Cuntai Guan*

Main category: cs.CL

TL;DR: The paper introduces DepFlow, a framework to address semantic biases in depression detection by enabling manipulation of depressive severity in speech synthesis, improving detection performance and robustness.


<details>
  <summary>Details</summary>
Motivation: To address semantic biases in depression datasets, which compromise model robustness in detecting depression, especially in cases of camouflaged depression.

Method: The proposed DepFlow framework involves a three-stage process: adversarial training for depression embeddings, a FiLM-modulated TTS system, and a severity mapping mechanism for controllable manipulation of depressive severity in synthesized speech.

Result: DepFlow demonstrates improved robustness in depression detection and introduces the CDoA dataset, which enhances detection performance by up to 12% depending on the detection architecture.

Conclusion: DepFlow improves depression detection robustness by mitigating semantic biases. It offers a controllable synthesis tool for clinical applications and evaluation while addressing ethical and coverage limitations in real-world data.

Abstract: Speech is a scalable and non-invasive biomarker for early mental health screening. However, widely used depression datasets like DAIC-WOZ exhibit strong coupling between linguistic sentiment and diagnostic labels, encouraging models to learn semantic shortcuts. As a result, model robustness may be compromised in real-world scenarios, such as Camouflaged Depression, where individuals maintain socially positive or neutral language despite underlying depressive states. To mitigate this semantic bias, we propose DepFlow, a three-stage depression-conditioned text-to-speech framework. First, a Depression Acoustic Encoder learns speaker- and content-invariant depression embeddings through adversarial training, achieving effective disentanglement while preserving depression discriminability (ROC-AUC: 0.693). Second, a flow-matching TTS model with FiLM modulation injects these embeddings into synthesis, enabling control over depressive severity while preserving content and speaker identity. Third, a prototype-based severity mapping mechanism provides smooth and interpretable manipulation across the depression continuum. Using DepFlow, we construct a Camouflage Depression-oriented Augmentation (CDoA) dataset that pairs depressed acoustic patterns with positive/neutral content from a sentiment-stratified text bank, creating acoustic-semantic mismatches underrepresented in natural data. Evaluated across three depression detection architectures, CDoA improves macro-F1 by 9%, 12%, and 5%, respectively, consistently outperforming conventional augmentation strategies in depression Detection. Beyond enhancing robustness, DepFlow provides a controllable synthesis platform for conversational systems and simulation-based evaluation, where real clinical data remains limited by ethical and coverage constraints.

</details>


### [39] [Robust Uncertainty Quantification for Factual Generation of Large Language Models](https://arxiv.org/abs/2601.00348)
*Yuhao Zhang,Zhongliang Yang,Linna Zhou*

Main category: cs.CL

TL;DR: This paper addresses the challenge of hallucination in LLMs by proposing robust uncertainty quantification methods, exhibiting improved performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of LLM hallucination issues which compromise trustworthiness, especially under non-standard questioning scenarios.

Method: They introduce a robust uncertainty quantification (RU) method and utilize a set of trap questions containing fake names to assess and mitigate hallucination.

Result: Experimental results show a significant improvement, with average ROCAUC values increased by 0.1-0.2 compared to baseline methods across four models.

Conclusion: The findings provide promising strategies for more reliable LLM responses, advancing its robustness against hallucination issues in real-life applications.

Abstract: The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.

</details>


### [40] [The Role of Mixed-Language Documents for Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2601.00364)
*Jiandong Shao,Raphael Tang,Crystina Zhang,Karin Sevegnani,Pontus Stenetorp,Jianfei Yang,Yao Lu*

Main category: cs.CL

TL;DR: The study investigates how bilingual data in training corpora impacts multilingual models, finding that parallel data is crucial for translation but not necessary for cross-lingual understanding.


<details>
  <summary>Details</summary>
Motivation: Explore the contribution of bilingual data to multilingual large language models' performance, especially focusing on translation versus cross-lingual reasoning.

Method: Models were pretrained under controlled conditions comparing a standard web corpus with a monolingual corpus devoid of bilingual data. Different types of bilingual content (e.g., parallel and code-switching) were systematically analyzed.

Result: Removing bilingual data caused a drastic drop in translation performance (56% reduction in BLEU score), while cross-lingual understanding stayed stable. Parallel data restored 91% translation performance, whereas code-switching had minimal impact.

Conclusion: Translation performance strongly depends on parallel data for token-level alignment, but cross-lingual understanding and reasoning tasks do not require bilingual data.

Abstract: Multilingual large language models achieve impressive cross-lingual performance despite largely monolingual pretraining. While bilingual data in pretraining corpora is widely believed to enable these abilities, details of its contributions remain unclear. We investigate this question by pretraining models from scratch under controlled conditions, comparing the standard web corpus with a monolingual-only version that removes all multilingual documents. Despite constituting only 2% of the corpus, removing bilingual data causes translation performance to drop 56% in BLEU, while behaviour on cross-lingual QA and general reasoning tasks remains stable, with training curves largely overlapping the baseline. To understand this asymmetry, we categorize bilingual data into parallel (14%), code-switching (72%), and miscellaneous documents (14%) based on the semantic relevance of content in different languages. We then conduct granular ablations by reintroducing parallel or code-switching data into the monolingual-only corpus. Our experiments reveal that parallel data almost fully restores translation performance (91% of the unfiltered baseline), whereas code-switching contributes minimally. Other cross-lingual tasks remain largely unaffected by either type. These findings reveal that translation critically depends on systematic token-level alignments from parallel data, whereas cross-lingual understanding and reasoning appear to be achievable even without bilingual data.

</details>


### [41] [BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics](https://arxiv.org/abs/2601.00366)
*Taj Gillin,Adam Lalani,Kenneth Zhang,Marcel Mateos Salles*

Main category: cs.CL

TL;DR: The paper proposes a new training paradigm, BERT-JEPA, for BERT-style models to enhance multilingual embedding space using JEPA objective.


<details>
  <summary>Details</summary>
Motivation: There is a need to address the problem of collapsed [CLS] embedding space in BERT-style models and improve multilingual benchmarks.

Method: Authors introduce a JEPA training objective to BERT-style models to transform the [CLS] embedding space into a language-agnostic space.

Result: The addition of JEPA training objective improves performance across multilingual benchmarks.

Conclusion: Using BERT-JEPA enhances BERT models by resolving the embedding collapse issue and improves multilingual capabilities.

Abstract: Joint Embedding Predictive Architectures (JEPA) are a novel self supervised training technique that have shown recent promise across domains. We introduce BERT-JEPA (BEPA), a training paradigm that adds a JEPA training objective to BERT-style models, working to combat a collapsed [CLS] embedding space and turning it into a language-agnostic space. This new structure leads to increased performance across multilingual benchmarks.

</details>


### [42] [Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach](https://arxiv.org/abs/2601.00388)
*Biao Wu,Meng Fang,Ling Chen,Ke Xu,Tao Cheng,Jun Wang*

Main category: cs.CL

TL;DR: Geo-R introduces a retrieval-free method for geolocalization using structured reasoning paths derived from ground-truth coordinates and reinforcement learning for accuracy enhancement.


<details>
  <summary>Details</summary>
Motivation: To address limitations in image geolocalization methods that depend on synthetic reasoning annotations or external retrieval, and to improve interpretability and generalizability.

Method: A structured reasoning framework called 'Chain of Region' maps GPS coordinates to geographic entities hierarchically, combined with reinforcement learning utilizing Haversine distance-based rewards.

Result: Geo-R demonstrated improved geolocalization accuracy, stronger generalization capabilities, and more interpretable predictions compared to existing methods.

Conclusion: Experimental results validate Geo-R’s retrieval-free paradigm, showcasing its scalability, accuracy, and transparency. The resources are publicly shared to promote further research.

Abstract: Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.

</details>


### [43] [Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset](https://arxiv.org/abs/2601.00411)
*Alistair Plum,Laura Bernardy,Tharindu Ranasinghe*

Main category: cs.CL

TL;DR: The paper introduces judgeWEL, a Luxemburgish NER dataset built using Wikipedia/Wikidata and refined through large language models for reliability, addressing low-resource language challenges.


<details>
  <summary>Details</summary>
Motivation: To tackle the bottleneck in NLP caused by the lack of annotated datasets for under-represented languages like Luxembourgish, and to address the challenge of high costs and inconsistency in manual annotations.

Method: The approach uses Wikipedia and Wikidata links for weak supervision to generate initial annotations. Large language models are then used to refine the dataset by filtering high-quality labelled sentences.

Result: A new Luxembourgish NER dataset was created, five times larger than prior datasets, with better category coverage and balance, serving as a significant resource for NER in low-resource languages.

Conclusion: The proposed method effectively combines weak supervision with LLM verification to create a robust dataset, addressing crucial limitations in low-resource NLP research.

Abstract: We present judgeWEL, a dataset for named entity recognition (NER) in Luxembourgish, automatically labelled and subsequently verified using large language models (LLM) in a novel pipeline. Building datasets for under-represented languages remains one of the major bottlenecks in natural language processing, where the scarcity of resources and linguistic particularities make large-scale annotation costly and potentially inconsistent. To address these challenges, we propose and evaluate a novel approach that leverages Wikipedia and Wikidata as structured sources of weak supervision. By exploiting internal links within Wikipedia articles, we infer entity types based on their corresponding Wikidata entries, thereby generating initial annotations with minimal human intervention. Because such links are not uniformly reliable, we mitigate noise by employing and comparing several LLMs to identify and retain only high-quality labelled sentences. The resulting corpus is approximately five times larger than the currently available Luxembourgish NER dataset and offers broader and more balanced coverage across entity categories, providing a substantial new resource for multilingual and low-resource NER research.

</details>


### [44] [Toward Better Temporal Structures for Geopolitical Events Forecasting](https://arxiv.org/abs/2601.00430)
*Kian Ahrabian,Eric Boxer,Jay Pujara*

Main category: cs.CL

TL;DR: The paper extends hyper-relational temporal knowledge graphs to generalized hypergraphs (HTKGHs) to address their limitations in representing complex temporal facts in geopolitical contexts.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the limited expressive power of hyper-relational temporal knowledge graphs (HTKGs) in conveying complex relationships, especially those involving more than two primary entities, which are common in real-world events.

Method: The authors formalize HTKGHs as a generalization of HTKGs, ensuring backward compatibility and adding support for two new fact types. They also create the htkgh-polecat dataset based on the POLECAT event database and benchmark LLMs using this framework.

Result: The paper offers insights into the adaptability and forecasting capabilities of popular large language models, particularly in predicting relations in complex geopolitical scenarios.

Conclusion: HTKGHs provide a more expressive framework than HTKGs, enabling better representation of complex temporal facts and improving the evaluation of LLM capabilities in forecasting tasks.

Abstract: Forecasting on geopolitical temporal knowledge graphs (TKGs) through the lens of large language models (LLMs) has recently gained traction. While TKGs and their generalization, hyper-relational temporal knowledge graphs (HTKGs), offer a straightforward structure to represent simple temporal relationships, they lack the expressive power to convey complex facts efficiently. One of the critical limitations of HTKGs is a lack of support for more than two primary entities in temporal facts, which commonly occur in real-world events. To address this limitation, in this work, we study a generalization of HTKGs, Hyper-Relational Temporal Knowledge Generalized Hypergraphs (HTKGHs). We first derive a formalization for HTKGHs, demonstrating their backward compatibility while supporting two complex types of facts commonly found in geopolitical incidents. Then, utilizing this formalization, we introduce the htkgh-polecat dataset, built upon the global event database POLECAT. Finally, we benchmark and analyze popular LLMs on the relation prediction task, providing insights into their adaptability and capabilities in complex forecasting scenarios.

</details>


### [45] [Comparative Efficiency Analysis of Lightweight Transformer Models: A Multi-Domain Empirical Benchmark for Enterprise NLP Deployment](https://arxiv.org/abs/2601.00444)
*Muhammad Shahmeer Khan*

Main category: cs.CL

TL;DR: The study compares DistilBERT, MiniLM, and ALBERT transformer models across three NLP tasks, analyzing performance and efficiency metrics for enterprise applications.


<details>
  <summary>Details</summary>
Motivation: To address the increasing need in enterprises for efficient, lightweight NLP models capable of handling multi-domain text automation tasks.

Method: The study conducts comparative performance analysis of three lightweight transformer models using controlled fine-tuning with metrics like accuracy, precision, recall, F1, inference time, throughput, and memory usage across three domains.

Result: ALBERT showed high task-specific accuracy, MiniLM excelled in inference speed and throughput, and DistilBERT provided consistent accuracy with competitive efficiency.

Conclusion: The paper suggests MiniLM for applications requiring low latency, DistilBERT for balancing accuracy and efficiency, and ALBERT for high performance in resource-constrained environments.

Abstract: In the rapidly evolving landscape of enterprise natural language processing (NLP), the demand for efficient, lightweight models capable of handling multi-domain text automation tasks has intensified. This study conducts a comparative analysis of three prominent lightweight Transformer models - DistilBERT, MiniLM, and ALBERT - across three distinct domains: customer sentiment classification, news topic classification, and toxicity and hate speech detection. Utilizing datasets from IMDB, AG News, and the Measuring Hate Speech corpus, we evaluated performance using accuracy-based metrics including accuracy, precision, recall, and F1-score, as well as efficiency metrics such as model size, inference time, throughput, and memory usage. Key findings reveal that no single model dominates all performance dimensions. ALBERT achieves the highest task-specific accuracy in multiple domains, MiniLM excels in inference speed and throughput, and DistilBERT demonstrates the most consistent accuracy across tasks while maintaining competitive efficiency. All results reflect controlled fine-tuning under fixed enterprise-oriented constraints rather than exhaustive hyperparameter optimization. These results highlight trade-offs between accuracy and efficiency, recommending MiniLM for latency-sensitive enterprise applications, DistilBERT for balanced performance, and ALBERT for resource-constrained environments.

</details>


### [46] [Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games](https://arxiv.org/abs/2601.00448)
*Dimitris Vartziotis*

Main category: cs.CL

TL;DR: This paper examines theories of linguistic meaning in large language models, contrasting social constructivist and mathematical approaches, proposing Semantic Field Theory for understanding language structure and limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore long-standing linguistic theories within the context of large language models and reconcile mathematical structures with social constructivist perspectives.

Method: They formalize interacting structures (Lexfelder and Lingofelder) within semantic space and analyze properties like attention mechanisms and embedding structures of transformer architectures.

Result: LLMs reveal mathematical semantic regularities but struggle with pragmatic reasoning and context sensitivity, highlighting the complementarity of mathematical and social approaches.

Conclusion: Language combines mathematical and social aspects, and integrating these can refine understanding of linguistic meaning and guide AI development.

Abstract: Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.

</details>


### [47] [Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations](https://arxiv.org/abs/2601.00454)
*Hyunjun Kim*

Main category: cs.CL

TL;DR: The paper introduces Defensive M2S, a method to efficiently train guardrail models for large language models (LLMs) by converting multi-turn conversations into single-turn formats, significantly reducing computational costs while improving performance.


<details>
  <summary>Details</summary>
Motivation: The need to ensure the safety of LLM deployments while reducing the computational cost of processing long multi-turn conversations.

Method: Defensive M2S fine-tunes guardrail models on compressed, single-turn versions of multi-turn dialogues, cutting training complexity from O(n^2) to O(n). Various compression templates and model types are tested.

Result: The approach achieves a 38.9% boost in attack detection recall with a reduction of 93× in training tokens and 94.6% fewer inference tokens, proving its efficiency and effectiveness.

Conclusion: M2S is an efficient technique that can significantly reduce costs while maintaining or improving safety performance, making it a viable solution for scalable LLM guardrail deployment.

Abstract: Guardrail models are essential for ensuring the safety of Large Language Model (LLM) deployments, but processing full multi-turn conversation histories incurs significant computational cost. We propose Defensive M2S, a training paradigm that fine-tunes guardrail models on Multi-turn to Single-turn (M2S) compressed conversations rather than complete dialogue histories. We provide a formal complexity analysis showing that M2S reduces training cost from $O(n^2)$ to $O(n)$ for $n$-turn conversations. Empirically, on our training dataset (779 samples, avg. 10.6 turns), M2S requires only 169K tokens compared to 15.7M tokens for the multi-turn baseline -- a 93$\times$ reduction. We evaluate Defensive M2S across three guardrail model families (LlamaGuard, Nemotron, Qwen3Guard) and three compression templates (hyphenize, numberize, pythonize) on SafeDialBench, a comprehensive multi-turn jailbreak benchmark. Our best configuration, Qwen3Guard with hyphenize compression, achieves 93.8% attack detection recall while reducing inference tokens by 94.6% (from 3,231 to 173 tokens per conversation). This represents a 38.9 percentage point improvement over the baseline while dramatically reducing both training and inference costs. Our findings demonstrate that M2S compression can serve as an effective efficiency technique for guardrail deployment, enabling scalable safety screening of long multi-turn conversations.

</details>


### [48] [Noise-Aware Named Entity Recognition for Historical VET Documents](https://arxiv.org/abs/2601.00488)
*Alexander M. Esser,Jens Dörpinghaus*

Main category: cs.CL

TL;DR: The paper proposes a method for Named Entity Recognition (NER) in Vocational Education and Training (VET) documents affected by OCR noise, using Noise-Aware Training, transfer learning, and fine-tuning strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve NER in VET documents, which are historically digitized and often contain OCR-induced errors, making accurate entity recognition challenging.

Method: The method employs Noise-Aware Training with artificially injected OCR errors, transfer learning, and multi-stage fine-tuning strategies. Data comparisons involve noisy, clean, and artificially generated datasets.

Result: Experimental results show improved robustness and accuracy of NER models under noisy conditions through domain-specific and noise-aware fine-tuning.

Conclusion: This approach enhances NER in noisy historical documents, is transferable across languages, and contributes publicly available code for noise-aware NER applications in domain-specific contexts.

Abstract: This paper addresses Named Entity Recognition (NER) in the domain of Vocational Education and Training (VET), focusing on historical, digitized documents that suffer from OCR-induced noise. We propose a robust NER approach leveraging Noise-Aware Training (NAT) with synthetically injected OCR errors, transfer learning, and multi-stage fine-tuning. Three complementary strategies, training on noisy, clean, and artificial data, are systematically compared. Our method is one of the first to recognize multiple entity types in VET documents. It is applied to German documents but transferable to arbitrary languages. Experimental results demonstrate that domain-specific and noise-aware fine-tuning substantially increases robustness and accuracy under noisy conditions. We provide publicly available code for reproducible noise-aware NER in domain-specific contexts.

</details>


### [49] [Rule-Based Approaches to Atomic Sentence Extraction](https://arxiv.org/abs/2601.00506)
*Lineesha Kamana,Akshita Ananda Subramanian,Mehuli Ghosh,Suman Saha*

Main category: cs.CL

TL;DR: The paper explores atomic sentence extraction from complex sentences, evaluates rule-based methods using dependency structures, and identifies which linguistic structures pose challenges.


<details>
  <summary>Details</summary>
Motivation: Motivated by the need to enhance systems in information retrieval, question answering, and automated reasoning by simplifying complex sentences into atomic ones and understanding which structures cause failures in extraction.

Method: The study uses dependency-based extraction rules in spaCy, with the WikiSplit dataset, to analyze atomic sentence extraction and evaluates the system with ROUGE and BERTScore.

Result: The system achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898; highlighting its sensitivity to complex linguistic structures like relative clauses and coordinated predicates.

Conclusion: Rule-based atomic sentence extraction is moderately accurate and aligns well lexically, structurally, and semantically, but struggles with syntactic complexities such as passive constructions and adverbial clauses.

Abstract: Natural language often combines multiple ideas into complex sentences. Atomic sentence extraction, the task of decomposing complex sentences into simpler sentences that each express a single idea, improves performance in information retrieval, question answering, and automated reasoning systems. Previous work has formalized the "split-and-rephrase" task and established evaluation metrics, and machine learning approaches using large language models have improved extraction accuracy. However, these methods lack interpretability and provide limited insight into which linguistic structures cause extraction failures. Although some studies have explored dependency-based extraction of subject-verb-object triples and clauses, no principled analysis has examined which specific clause structures and dependencies lead to extraction difficulties. This study addresses this gap by analyzing how complex sentence structures, including relative clauses, adverbial clauses, coordination patterns, and passive constructions, affect the performance of rule-based atomic sentence extraction. Using the WikiSplit dataset, we implemented dependency-based extraction rules in spaCy, generated 100 gold=standard atomic sentence sets, and evaluated performance using ROUGE and BERTScore. The system achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898, indicating moderate-to-high lexical, structural, and semantic alignment. Challenging structures included relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions. Overall, rule-based extraction is reasonably accurate but sensitive to syntactic complexity.

</details>


### [50] [Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends](https://arxiv.org/abs/2601.00536)
*Yuelyu Ji,Zhuochun Li,Rui Meng,Daqing He*

Main category: cs.CL

TL;DR: The paper reviews multi-hop question answering (QA) systems with a focus on execution procedures and proposes a 4-axis framework for analysis.


<details>
  <summary>Details</summary>
Motivation: To address the lack of explicit analysis in the retrieval and reasoning processes of multi-hop QA systems, which complicates fair comparisons and understanding of procedural choices.

Method: The authors developed a four-axis framework to analyze multi-hop QA systems, focusing on execution plans, index structures, control strategies, and stop criteria.

Result: The framework is applied to various multi-hop QA benchmarks, identifying trade-offs between effectiveness, efficiency, and evidence faithfulness, and spotting trends.

Conclusion: The paper identifies open challenges for QA systems, such as improving structure-aware planning, creating transferable control policies, and ensuring robust stopping criteria under distribution shifts.

Abstract: Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.

</details>


### [51] [ECR: Manifold-Guided Semantic Cues for Compact Language Models](https://arxiv.org/abs/2601.00543)
*Chung-Wei Victor Yuan*

Main category: cs.CL

TL;DR: The paper introduces Embedding Consistency Regulation (ECR) to address issues of structure loss in compact models' embedding spaces, particularly in multilingual cases.


<details>
  <summary>Details</summary>
Motivation: Compact models often lose embedding space structure, leading to issues in semantic consistency and downstream task performance, especially in multilingual data.

Method: The ECR framework uses semantic anchors from teacher embeddings and trains compact models to maintain consistent geometry around these anchors, adding a small projection at inference without altering runtime.

Result: ECR preserves semantic structure, stabilizes training, and creates task-aligned compact embeddings across tasks and languages when tested on a multilingual dataset.

Conclusion: ECR addresses structural loss in compact models, enhances task-relevant representations, and offers a deployable solution under efficiency and privacy constraints.

Abstract: Compact models often lose the structure of their embedding space. The issue shows up when the capacity is tight or the data spans several languages. Such collapse makes it difficult for downstream tasks to build on the resulting representation. Existing compression methods focus on aligning model outputs at a superficial level but fail to preserve the underlying manifold structure. This mismatch often leads to semantic drift in the compact model, causing both task behavior and linguistic properties to deviate from the reference model.
  To address those issues, we provide a new framework called Embedding Consistency Regulation (ECR). This framework first derives a set of semantic anchors from teacher embeddings (computed once offline). Then, the compact model learns to maintain consistent geometry around these anchors, without relying on matching logits or internal features. ECR adds only a small projection step at inference, without altering the decoding architecture or its runtime behavior.
  In experiments on a 100K multilingual corpus, ECR consistently stabilizes training and preserves semantic structure across tasks and languages. It also produces a more compact and task-aligned representation space, enabling low-capacity models to learn cleaner manifolds than conventional baselines. ECR works without teacher outputs and is compatible with, but independent of, distillation. Taken together, our results show that ECR helps compact models better follow task requirements and makes them easier to deploy under strict efficiency or privacy limits.

</details>


### [52] [A Language-Agnostic Hierarchical LoRA-MoE Architecture for CTC-based Multilingual ASR](https://arxiv.org/abs/2601.00557)
*Yuang Zheng,Yuxiang Mei,Dongxing Xu,Jie Chen,Yanhua Long*

Main category: cs.CL

TL;DR: This paper introduces a lightweight multilingual ASR system using a hierarchical LoRA mechanism for efficient, language-agnostic decoding, significantly reducing computational requirements.


<details>
  <summary>Details</summary>
Motivation: To address the high computational and latency costs of large-scale multilingual ASR models like Whisper, especially on resource-constrained edge devices.

Method: A CTC-based architecture utilizing a novel Language-agnostic Hierarchical LoRA-MoE (HLoRA) framework, integrated with mHuBERT-CTC, and a LID-posterior-driven LoRA routing for end-to-end, language-agnostic decoding.

Result: The system achieves performance comparable to state-of-the-art methods on MSR-86K and MLC-SLM 2025 Challenge datasets with improved efficiency using single-pass decoding.

Conclusion: HLoRA is an effective lightweight solution for low-resource multilingual ASR applications, maintaining competitive performance while enhancing efficiency and removing reliance on explicit language identity information during inference.

Abstract: Large-scale multilingual ASR (mASR) models such as Whisper achieve strong performance but incur high computational and latency costs, limiting their deployment on resource-constrained edge devices. In this study, we propose a lightweight and language-agnostic multilingual ASR system based on a CTC architecture with domain adaptation. Specifically, we introduce a Language-agnostic Hierarchical LoRA-MoE (HLoRA) framework integrated into an mHuBERT-CTC model, enabling end-to-end decoding via LID-posterior-driven LoRA routing. The hierarchical design consists of a multilingual shared LoRA for learning language-invariant acoustic representations and language-specific LoRA experts for modeling language-dependent characteristics. The proposed routing mechanism removes the need for prior language identity information or explicit language labels during inference, achieving true language-agnostic decoding. Experiments on MSR-86K and the MLC-SLM 2025 Challenge datasets demonstrate that HLoRA achieves competitive performance with state-of-the-art two-stage inference methods using only single-pass decoding, significantly improving decoding efficiency for low-resource mASR applications.

</details>


### [53] [InfoSynth: Information-Guided Benchmark Synthesis for LLMs](https://arxiv.org/abs/2601.00575)
*Ishir Garg,Neel Kolhe,Xuandong Zhao,Dawn Song*

Main category: cs.CL

TL;DR: InfoSynth introduces a scalable framework to automatically generate reasoning benchmarks for LLMs using information theory, avoiding expensive manual effort.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for LLMs often involve costly human effort and lack novelty or contamination-free standards needed for accurate evaluation.

Method: The paper introduces InfoSynth, a framework leveraging KL-divergence, entropy, genetic algorithms, and iterative feedback to create novel benchmarks in Python programming while controlling difficulty and diversity.

Result: InfoSynth generates accurate coding problems and test cases 97% of the time, achieving higher novelty and diversity compared to existing datasets.

Conclusion: InfoSynth provides an automated, efficient solution for creating high-quality benchmarks, improving LLM evaluation without manual effort.

Abstract: Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/

</details>


### [54] [CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns](https://arxiv.org/abs/2601.00588)
*Zhenhong Zhou,Shilinlu Yan,Chuanpu Liu,Qiankun Li,Kun Wang,Zhigang Zeng*

Main category: cs.CL

TL;DR: The paper introduces a benchmark called CSSBench to evaluate the safety of lightweight language models (LLMs) in handling Chinese adversarial patterns.


<details>
  <summary>Details</summary>
Motivation: Existing safety evaluations focus on English but overlook Chinese-specific adversarial behavior, leaving vulnerabilities in lightweight LLMs that are increasingly deployed in cost-sensitive scenarios.

Method: Develop an evaluation benchmark, CSSBench, targeting six key domains and multiple task types representative of real-world Chinese malicious queries.

Result: Experiments on lightweight LLMs using CSSBench demonstrate challenges related to adversarial Chinese patterns, highlighting their susceptibility.

Conclusion: CSSBench provides valuable insights for improving lightweight LLMs’ safety in Chinese scenarios and supports better deployments in practice.

Abstract: Large language models (LLMs) are increasingly deployed in cost-sensitive and on-device scenarios, and safety guardrails have advanced mainly in English. However, real-world Chinese malicious queries typically conceal intent via homophones, pinyin, symbol-based splitting, and other Chinese-specific patterns. These Chinese-specific adversarial patterns create the safety evaluation gap that is not well captured by existing benchmarks focused on English. This gap is particularly concerning for lightweight models, which may be more vulnerable to such specific adversarial perturbations. To bridge this gap, we introduce the Chinese-Specific Safety Benchmark (CSSBench) that emphasizes these adversarial patterns and evaluates the safety of lightweight LLMs in Chinese. Our benchmark covers six domains that are common in real Chinese scenarios, including illegal activities and compliance, privacy leakage, health and medical misinformation, fraud and hate, adult content, and public and political safety, and organizes queries into multiple task types. We evaluate a set of popular lightweight LLMs and measure over-refusal behavior to assess safety-induced performance degradation. Our results show that the Chinese-specific adversarial pattern is a critical challenge for lightweight LLMs. This benchmark offers a comprehensive evaluation of LLM safety in Chinese, assisting robust deployments in practice.

</details>


### [55] [Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence](https://arxiv.org/abs/2601.00596)
*Sumanth Balaji,Piyush Mishra,Aashraya Sachdeva,Suraj Agrawal*

Main category: cs.CL

TL;DR: The paper introduces JourneyBench, a benchmark for evaluating how policy-aware large language models (LLMs) perform in customer support scenarios by measuring policy adherence and task handling.


<details>
  <summary>Details</summary>
Motivation: Traditional customer support systems lack flexibility, and current LLM benchmarks fail to evaluate policy adherence in multi-step, real-world tasks. The study aims to fill this gap.

Method: JourneyBench uses graph-based representations to create realistic support scenarios and introduces the User Journey Coverage Score to measure policy adherence. Two agent designs, Static-Prompt Agent (SPA) and Dynamic-Prompt Agent (DPA), are evaluated.

Result: The evaluation revealed that the DPA approach enhances policy adherence across 703 conversations, with smaller models like GPT-4o-mini performing better than larger ones when policy orchestration is applied.

Conclusion: Structured orchestration significantly improves LLM-based customer support, and JourneyBench establishes a valuable benchmark for advancing beyond rigid IVR systems.

Abstract: Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.

</details>


### [56] [Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs](https://arxiv.org/abs/2601.00641)
*Nils Rautenberg,Sven Schippkus*

Main category: cs.CL

TL;DR: The paper addresses contextual hallucinations in large language models by proposing a framework with probabilistic guarantees to exponentially reduce hallucination probabilities without modifying model weights, prompt engineering, or decoding strategies.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to address contextual hallucinations in deterministic automation workflows where correctness is critical and inputs are fixed.

Method: A probabilistic framework involving repeated prompt issuance, LLM-as-a-judge mechanism, and majority voting for error rate reduction, achieving exponential decay in both pipeline failure and hallucination selection.

Result: Experiments show exponential reduction in pipeline failure and hallucination occurrence using repeated runs and ensemble judges, validating the theoretical approach.

Conclusion: The study presents a modular, effective method to minimize hallucinations in LLM workflows, ensuring reliability in tasks with fixed inputs and deterministic correctness criteria.

Abstract: Large language models (LLMs) frequently produce contextual hallucinations, where generated content contradicts or ignores information explicitly stated in the prompt. Such errors are particularly problematic in deterministic automation workflows, where inputs are fixed and correctness is unambiguous. We introduce a simple and model-agnostic framework that provides explicit probabilistic guarantees for reducing hallucinations in this setting.
  We formalize the notion of a specific task, defined by a fixed input and a deterministic correctness criterion, and show that issuing the same prompt in independent context windows yields an exponential reduction in the probability that all model outputs are incorrect. To identify a correct answer among repeated runs, we incorporate an LLM-as-a-judge and prove that the probability that the judged pipeline fails decays at a rate determined by the judge's true- and false-positive probabilities. When the judge is imperfect, we strengthen it through majority vote over independent judge calls, obtaining ensemble-level error rates that decrease exponentially in the number of votes. This yields an explicit bound on the probability that the pipeline selects a hallucinated answer.
  Experiments on controlled extraction tasks with synthetic noisy judges match these predictions exactly: pipeline failure decreases exponentially with the number of repetitions, and hallucination-selection decreases exponentially with the number of judges in the ensemble. Together, these results provide a lightweight, modular, and theoretically grounded method for driving hallucination probabilities arbitrarily low in fixed-input LLM workflows-without modifying model weights, decoding strategies, or prompt engineering.

</details>


### [57] [Physio-DPO: Aligning Large Language Models with the Protein Energy Landscape to Eliminate Structural Hallucinations](https://arxiv.org/abs/2601.00647)
*QiWei Meng*

Main category: cs.CL

TL;DR: The paper introduces a new method, Physio-DPO, to improve protein design by reducing structural hallucinations and emphasizing thermodynamic stability in protein language models.


<details>
  <summary>Details</summary>
Motivation: Existing protein language models often generate sequences that are thermodynamically unstable despite their high linguistic likelihood. A more effective model is needed to enhance stability and mitigate structural hallucinations.

Method: Physio-DPO is a physics-informed alignment framework that uses a magnitude-aware objective to scale optimization based on the energy gap between native protein structures and physics-perturbed hard negatives.

Result: Physio-DPO outperforms baselines like SFT, PPO, and standard DPO. It reduces structural inconsistency (RMSD to 1.28 Å) and improves foldability to 92.8%, while recovering key biophysical interactions.

Conclusion: Physio-DPO successfully addresses the issue of structural hallucinations in protein language models, significantly improving the thermodynamic stability of designed proteins.

Abstract: Large Protein Language Models have shown strong potential for generative protein design, yet they frequently produce structural hallucinations, generating sequences with high linguistic likelihood that fold into thermodynamically unstable conformations. Existing alignment approaches such as Direct Preference Optimization are limited in this setting, as they model preferences as binary labels and ignore the continuous structure of the physical energy landscape. We propose Physio-DPO, a physics informed alignment framework that grounds protein language models in thermodynamic stability. Physio-DPO introduces a magnitude aware objective that scales optimization updates according to the energy gap between native structures and physics perturbed hard negatives. Experiments show that Physio-DPO consistently outperforms strong baselines including SFT, PPO, and standard DPO, reducing self consistency RMSD to 1.28 Å and increasing foldability to 92.8%. Qualitative analysis further demonstrates that Physio-DPO effectively mitigates structural hallucinations by recovering biophysical interactions such as hydrophobic core packing and hydrogen bond networks.

</details>


### [58] [Fast-weight Product Key Memory](https://arxiv.org/abs/2601.00671)
*Tianyu Zhao,Llion Jones*

Main category: cs.CL

TL;DR: The paper introduces Fast-weight Product Key Memory (FwPKM), a dynamic memory layer improving sequence modeling by enabling rapid memorization and retrieval of key-value pairs.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between storage capacity and computational efficiency in sequence modeling layers of modern language models.

Method: Proposed FwPKM, a dynamic, fast-weight episodic memory that updates parameters both during training and inference through local chunk-level gradient descent.

Result: FwPKM reduces perplexity significantly on long-context datasets and generalizes to 128K-token contexts despite training on 4K-token sequences.

Conclusion: FwPKM effectively serves as a complementary episodic memory to standard modules, overcoming limitations of storage and efficiency in sequence modeling layers.

Abstract: Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.

</details>


### [59] [Sigmoid Head for Quality Estimation under Language Ambiguity](https://arxiv.org/abs/2601.00680)
*Tu Anh Dinh,Jan Niehues*

Main category: cs.CL

TL;DR: Language model probabilities are unreliable quality estimators due to ambiguity and misrepresentation of multiple correct options. The authors propose a 'Sigmoid Head' module to improve quality estimation, addressing key limitations in LM design.


<details>
  <summary>Details</summary>
Motivation: Current language models inaccurately assess output quality because their probabilistic structure cannot represent multiple valid outputs and training relies on single correct solutions. This leads to misestimated quality, especially in ambiguous language tasks.

Method: The authors introduce a 'Sigmoid Head' module with a sigmoid activation function integrated with pre-trained LMs. It allows better handling of multiple correct answers by adjusting the negative sampling process and improving the probabilistic signal.

Result: The Sigmoid Head provides a better quality signal compared to traditional softmax-based outputs. It proves to be computationally efficient and more robust in out-of-domain analysis, outperforming supervised quality estimation methods.

Conclusion: The proposed Sigmoid Head module effectively addresses the limitations of softmax outputs in LMs and offers a robust, high-quality estimation mechanism. This enhances the utility and reliability of LMs across various tasks and domains.

Abstract: Language model (LM) probability is not a reliable quality estimator, as natural language is ambiguous. When multiple output options are valid, the model's probability distribution is spread across them, which can misleadingly indicate low output quality. This issue is caused by two reasons: (1) LMs' final output activation is softmax, which does not allow multiple correct options to receive high probabilities simultaneuously and (2) LMs' training data is single, one-hot encoded references, indicating that there is only one correct option at each output step. We propose training a module for Quality Estimation on top of pre-trained LMs to address these limitations. The module, called Sigmoid Head, is an extra unembedding head with sigmoid activation to tackle the first limitation. To tackle the second limitation, during the negative sampling process to train the Sigmoid Head, we use a heuristic to avoid selecting potentially alternative correct tokens. Our Sigmoid Head is computationally efficient during training and inference. The probability from Sigmoid Head is notably better quality signal compared to the original softmax head. As the Sigmoid Head does not rely on human-annotated quality data, it is more robust to out-of-domain settings compared to supervised QE.

</details>


### [60] [Exploring the Performance of Large Language Models on Subjective Span Identification Tasks](https://arxiv.org/abs/2601.00736)
*Alphaeus Dmonte,Roland Oruche,Tharindu Ranasinghe,Marcos Zampieri,Prasad Calyam*

Main category: cs.CL

TL;DR: The paper evaluates Large Language Models (LLMs) on text span identification for sentiment analysis, offensive language identification, and claim verification, exploring techniques like instruction tuning, in-context learning, and chain of thought.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the underexplored area of using LLMs for subjective span identification tasks, as most current approaches rely on smaller language models or only focus on explicit span identification.

Method: The study evaluates various LLM-based strategies, including instruction tuning, in-context learning, and chain of thought, on text span identification across three tasks.

Result: The results show that the relationships within text assist LLMs in effectively identifying precise text spans.

Conclusion: LLMs are shown to be effective in subjective text span identification, broadening their applicability beyond explicit tasks like NER.

Abstract: Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.

</details>


### [61] [Adapting Natural Language Processing Models Across Jurisdictions: A pilot Study in Canadian Cancer Registries](https://arxiv.org/abs/2601.00787)
*Jonathan Simkin,Lovedeep Gondara,Zeeshan Rizvi,Gregory Doyle,Jeff Dowden,Dan Bond,Desmond Martin,Raymond Ng*

Main category: cs.CL

TL;DR: The study explores adapting transformer models (BCCRTron, GatorTron) for cancer data registries across provinces in Canada. Both models showed high performance after fine-tuning, and their ensemble further improved sensitivity and reduced missed cancers.


<details>
  <summary>Details</summary>
Motivation: Manual abstraction of pathology reports for cancer registries is resource-intensive and delays data collection. NLP transformers, though promising, struggle to generalize across jurisdictions.

Method: Two transformer models, BCCRTron and GatorTron, were fine-tuned on pathology reports from the Newfoundland & Labrador Cancer Registry using synoptic and diagnosis-focused report sections. An ensemble approach was also implemented combining their outputs.

Result: The adapted models maintained high performance across test sets and the conservative OR-ensemble increased recall to 0.99 in both Tier 1 and Tier 2 tasks, significantly reducing the count of missed cancers compared to standalone models.

Conclusion: Transformers can generalize to different jurisdictions with modest fine-tuning, and combining models improves accuracy in cancer registries. Privacy-preserving workflows allow only model weight sharing, supporting interoperable NLP infrastructure.

Abstract: Population-based cancer registries depend on pathology reports as their primary diagnostic source, yet manual abstraction is resource-intensive and contributes to delays in cancer data. While transformer-based NLP systems have improved registry workflows, their ability to generalize across jurisdictions with differing reporting conventions remains poorly understood. We present the first cross-provincial evaluation of adapting BCCRTron, a domain-adapted transformer model developed at the British Columbia Cancer Registry, alongside GatorTron, a biomedical transformer model, for cancer surveillance in Canada. Our training dataset consisted of approximately 104,000 and 22,000 de-identified pathology reports from the Newfoundland & Labrador Cancer Registry (NLCR) for Tier 1 (cancer vs. non-cancer) and Tier 2 (reportable vs. non-reportable) tasks, respectively. Both models were fine-tuned using complementary synoptic and diagnosis focused report section input pipelines. Across NLCR test sets, the adapted models maintained high performance, demonstrating transformers pretrained in one jurisdiction can be localized to another with modest fine-tuning. To improve sensitivity, we combined the two models using a conservative OR-ensemble achieving a Tier 1 recall of 0.99 and reduced missed cancers to 24, compared with 48 and 54 for the standalone models. For Tier 2, the ensemble achieved 0.99 recall and reduced missed reportable cancers to 33, compared with 54 and 46 for the individual models. These findings demonstrate that an ensemble combining complementary text representations substantially reduce missed cancers and improve error coverage in cancer-registry NLP. We implement a privacy-preserving workflow in which only model weights are shared between provinces, supporting interoperable NLP infrastructure and a future pan-Canadian foundation model for cancer pathology and registry workflows.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [62] [TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model](https://arxiv.org/abs/2601.00051)
*Yabo Chen,Yuanzhi Liang,Jiepeng Wang,Tingxi Chen,Junfei Cheng,Zixiao Gu,Yuyang Huang,Zicheng Jiang,Wei Li,Tian Li,Weichen Li,Zuoxin Li,Guangce Liu,Jialun Liu,Junqi Liu,Haoyuan Wang,Qizhen Weng,Xuan'er Wu,Xunzhi Xiang,Xiaoyan Yang,Xin Zhang,Shiwen Zhang,Junyu Zhou,Chengcheng Zhou,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleWorld introduces a real-time multimodal world modeling framework combining video generation, scene reconstruction, and long-term memory in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing video generation models regarding real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes.

Method: TeleWorld employs a generation-reconstruction-guidance paradigm with autoregressive diffusion-based video models, hierarchical planning (MMPL), and efficient distribution matching techniques (DMD).

Result: TeleWorld demonstrates strong performance in dynamic/static world modeling, real-time generation efficiency, and long-term consistency.

Conclusion: This paper advances toward practical, interactive world models with unified dynamic scene representation and computationally efficient techniques.

Abstract: World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.

</details>


### [63] [It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models](https://arxiv.org/abs/2601.00090)
*Anne Harrington,A. Sophia Koepke,Shyamgopal Karthik,Trevor Darrell,Alexei A. Efros*

Main category: cs.CV

TL;DR: The study explores addressing mode collapse in text-to-image models through noise optimization rather than guidance mechanisms or refining generated candidates.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address mode collapse in text-to-image models, which limits the diversity of outputs generated from the same textual prompt.

Method: The authors employ a noise optimization objective, analyzing noise frequency characteristics and alternative initialization profiles to enhance generation diversity and quality.

Result: Experiments show that the proposed noise optimization improves the diversity and quality of generated images compared to previous methods.

Conclusion: Noise optimization serves as an effective alternative to traditional methods in mitigating mode collapse while maintaining model fidelity.

Abstract: Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.

</details>


### [64] [Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark](https://arxiv.org/abs/2601.00092)
*Pan Wang,Yang Liu,Guile Wu,Eduardo R. Corral-Soto,Chengjie Huang,Binbin Xu,Dongfeng Bai,Xu Yan,Yuan Ren,Xingxin Chen,Yizhe Wu,Tao Huang,Wenjun Wan,Xin Wu,Pei Zhou,Xuyang Dai,Kangbo Lv,Hongbo Zhang,Yosef Fried,Aixue Ye,Bailan Feng,Zhenyu Chen,Zhen Li,Yingcong Chen,Yiyi Liao,Bingbing Liu*

Main category: cs.CV

TL;DR: The paper introduces Spatial4D-Bench, a benchmark to evaluate 4D spatial intelligence in Multimodal Large Language Models (MLLMs), revealing their current limitations.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap in assessing 4D spatial intelligence in MLLMs and to push development toward achieving human-level performance in this domain.

Method: Developed Spatial4D-Bench, a large-scale evaluation benchmark with 40,000 question-answer pairs across 18 tasks, organized into six cognitive categories to test MLLMs' 4D spatial reasoning.

Result: The benchmark exposes significant limitations in MLLMs' abilities related to tasks like route planning, action recognition, and physical plausibility reasoning.

Conclusion: Spatial4D-Bench provides valuable insights and a structured evaluation framework to advance the spatial reasoning capabilities of MLLMs.

Abstract: 4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.

</details>


### [65] [A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data](https://arxiv.org/abs/2601.00123)
*Hyunho Lee,Wenwen Li*

Main category: cs.CV

TL;DR: The paper introduces SMAGNet, a multimodal deep learning model using SAR and MSI data for flood-related water extent mapping. It successfully maps water extent even with partial or missing MSI data.


<details>
  <summary>Details</summary>
Motivation: The research addresses limitations in adaptive integration of MSI data into SAR-based post-flood water extent mapping, especially when MSI observations are limited.

Method: The proposed Spatially Masked Adaptive Gated Network (SMAGNet) integrates SAR data primarily, while incorporating MSI data through feature fusion using multimodal deep learning.

Result: SMAGNet achieved better prediction performance on C2S-MS Floods dataset compared to other models, including in scenarios with limited or entirely missing MSI data.

Conclusion: SMAGNet enhances model robustness to missing data and improves applicability of multimodal deep learning for post-flood water extent mapping in practical scenarios.

Abstract: Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios.

</details>


### [66] [Compressed Map Priors for 3D Perception](https://arxiv.org/abs/2601.00139)
*Brady Zhou,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: The paper introduces Compressed Map Priors (CMP), a framework for autonomous vehicle vision systems that leverages spatial priors to improve 3D object detection.


<details>
  <summary>Details</summary>
Motivation: Current autonomous vision systems treat every location as unfamiliar, despite frequent traversals, missing optimization opportunities through spatial priors.

Method: CMP utilizes a binarized hashmap for compressed spatial priors storage, requiring only 32 KB/km². It integrates seamlessly into existing 3D perception systems.

Result: CMP offers up to 20x reduction in storage needs and consistently enhances 3D object detection performance on the nuScenes dataset.

Conclusion: Leveraging CMP-based spatial priors leads to storage efficiency and accuracy improvements in autonomous vehicle vision systems.

Abstract: Human drivers rarely travel where no person has gone before. After all, thousands of drivers use busy city roads every day, and only one can claim to be the first. The same holds for autonomous computer vision systems. The vast majority of the deployment area of an autonomous vision system will have been visited before. Yet, most autonomous vehicle vision systems act as if they are encountering each location for the first time. In this work, we present Compressed Map Priors (CMP), a simple but effective framework to learn spatial priors from historic traversals. The map priors use a binarized hashmap that requires only $32\text{KB}/\text{km}^2$, a $20\times$ reduction compared to the dense storage. Compressed Map Priors easily integrate into leading 3D perception systems at little to no extra computational costs, and lead to a significant and consistent improvement in 3D object detection on the nuScenes dataset across several architectures.

</details>


### [67] [Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection](https://arxiv.org/abs/2601.00141)
*Lawrence Han*

Main category: cs.CV

TL;DR: The paper introduces GLASS, a method for improving AI-generated image detection by combining global and local image information, outperforming previous approaches.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of AI-generated image detection architectures that lose fine-grained details when downsampling images.

Method: The method, GLASS, combines a globally resized image view with multiple randomly sampled original-resolution local crops. These local crops are selected efficiently through spatially stratified sampling and aggregated using attention-based scoring.

Result: GLASS successfully integrates into vision models and demonstrates superior predictive performance compared to standard transfer learning within computational limits.

Conclusion: GLASS enhances AI-generated image detection by leveraging both global and local image information without sacrificing computational efficiency.

Abstract: The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.

</details>


### [68] [FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications](https://arxiv.org/abs/2601.00150)
*Yehui Yang,Dalu Yang,Wenshuo Zhou,Fangxin Shang,Yifan Liu,Jie Ren,Haojun Fei,Qing Yang,Tao Chen*

Main category: cs.CV

TL;DR: This paper introduces FCMBench-V1.0, a benchmark for multimodal AI in financial credit applications, aimed at assessing perception, reasoning, and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of domain-specific benchmarks for financial credit applications in multimodal AI, ensuring privacy compliance, robustness, and practical utility.

Method: Developed FCMBench-V1.0 with 18 certificate types, privacy-compliant datasets (4,043 images, 8,446 QA samples), synthesized content, and a three-dimensional evaluation framework for real-world relevance.

Result: 23 vision-language models were evaluated, with Qfin-VL-Instruct outperforming others. Robustness testing revealed performance drops under acquisition artifacts.

Conclusion: FCMBench provides a robust benchmark for financial multimodal AI, highlighting disparities and areas for improvement in modern vision-language models.

Abstract: As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.

</details>


### [69] [Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions](https://arxiv.org/abs/2601.00156)
*Kaiwen Zheng,Junchen Fu,Songpei Xu,Yaoqing He,Joemon M. Jose,Han Hu,Xuri Ge*

Main category: cs.CV

TL;DR: This paper introduces a system for detailed facial analysis using localized face regions and proposes a fine-tuned vision-language model for multi-attribute description and recognition.


<details>
  <summary>Details</summary>
Motivation: To enhance understanding and control in facial analysis by focusing on specific facial regions and addressing multiple attributes including facial action units, emotions, and age.

Method: The authors constructed a new dataset with rich annotations and developed a fine-tuned vision-language model (Focal-RegionFace) based on Qwen2.5-VL, applying progressive fine-tuning for localized facial analysis.

Result: Focal-RegionFace demonstrated superior performance on a new benchmark dataset, excelling across traditional and novel evaluation metrics.

Conclusion: The proposed system effectively addresses fine-grained multi-attribute analysis of facial regions, showcasing its versatility and improved interpretability in facial state analysis.

Abstract: In this paper, we introduce an underexplored problem in facial analysis: generating and recognizing multi-attribute natural language descriptions, containing facial action units (AUs), emotional states, and age estimation, for arbitrarily selected face regions (termed FaceFocalDesc). We argue that the system's ability to focus on individual facial areas leads to better understanding and control. To achieve this capability, we construct a new multi-attribute description dataset for arbitrarily selected face regions, providing rich region-level annotations and natural language descriptions. Further, we propose a fine-tuned vision-language model based on Qwen2.5-VL, called Focal-RegionFace for facial state analysis, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages, resulting in interpretable age estimation, FAU and emotion detection. Experimental results show that Focal-RegionFace achieves the best performance on the new benchmark in terms of traditional and widely used metrics, as well as new proposed metrics. This fully verifies its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.

</details>


### [70] [DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery](https://arxiv.org/abs/2601.00194)
*Salma Gonzalez-Sabbagh,Antonio Robles-Kelly,Shang Gao*

Main category: cs.CV

TL;DR: The paper introduces DichroGAN, a model for restoring in-air seafloor colours from satellite imagery via light attenuation correction.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of recovering seafloor colours from satellite images, overcoming the issue of light attenuation and scattering through water.

Method: DichroGAN uses a conditional generative adversarial network (cGAN) with multiple generators to estimate scene radiance and underwater light transmission, restoring in-air seafloor colours.

Result: DichroGAN, trained on satellite imagery, showed competitive results compared to existing underwater restoration methods in various experiments.

Conclusion: The proposed DichroGAN effectively restores seafloor colours, showing promise for future underwater imaging and restoration applications.

Abstract: Recovering the in-air colours of seafloor from satellite imagery is a challenging task due to the exponential attenuation of light with depth in the water column. In this study, we present DichroGAN, a conditional generative adversarial network (cGAN) designed for this purpose. DichroGAN employs a two-steps simultaneous training: first, two generators utilise a hyperspectral image cube to estimate diffuse and specular reflections, thereby obtaining atmospheric scene radiance. Next, a third generator receives as input the generated scene radiance containing the features of each spectral band, while a fourth generator estimates the underwater light transmission. These generators work together to remove the effects of light absorption and scattering, restoring the in-air colours of seafloor based on the underwater image formation equation. DichroGAN is trained on a compact dataset derived from PRISMA satellite imagery, comprising RGB images paired with their corresponding spectral bands and masks. Extensive experiments on both satellite and underwater datasets demonstrate that DichroGAN achieves competitive performance compared to state-of-the-art underwater restoration techniques.

</details>


### [71] [MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing](https://arxiv.org/abs/2601.00204)
*Xiaokun Sun,Zeyu Cai,Hao Tang,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: MorphAny3D utilizes Structured Latent (SLAT) representations and novel attention mechanisms to achieve high-quality, semantically consistent, and temporally smooth 3D morphing, addressing challenges like cross-category transitions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of generating semantically meaningful and temporally smooth 3D morphing sequences, particularly for cross-category transitions, which are generally difficult to accomplish.

Method: MorphAny3D proposes a training-free framework using Structured Latent (SLAT) representations. It introduces Morphing Cross-Attention (MCA) for structural coherence, Temporal-Fused Self-Attention (TFSA) for temporal consistency, and an orientation correction strategy for pose ambiguity mitigation.

Result: The method achieves state-of-the-art results in generating 3D morphing sequences, including for challenging cross-category cases. It also supports advanced applications like decoupled morphing and 3D style transfer.

Conclusion: MorphAny3D demonstrates that blending SLAT features within 3D generator attention mechanisms enables effective and high-quality morphing, and its generalization potential highlights its applicability in broader generative models.

Abstract: 3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.

</details>


### [72] [CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting](https://arxiv.org/abs/2601.00207)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: The paper introduces a novel 3D instance segmentation method using NeRF view synthesis for accurate crop counting across three datasets, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Precise crop counting is essential for efficient agricultural management, but challenges arise due to partial occlusions and difficulties in distinguishing clustered crops in image-based methods.

Method: The proposed framework employs 2D images from diverse viewpoints and utilizes neural radiance field (NeRF) synthesis, incorporating mask consistency and crop visibility scores for effective 3D segmentation and enumeration.

Result: The framework achieves high accuracy in crop counting across cotton bolls, apples, and pears, overcoming crop color, shape, and size variations, while outperforming state-of-the-art methods.

Conclusion: This study provides a robust, crop-independent method for precise crop counting and contributes a new cotton dataset for advancing future research.

Abstract: Rigorous crop counting is crucial for effective agricultural management and informed intervention strategies. However, in outdoor field environments, partial occlusions combined with inherent ambiguity in distinguishing clustered crops from individual viewpoints poses an immense challenge for image-based segmentation methods. To address these problems, we introduce a novel crop counting framework designed for exact enumeration via 3D instance segmentation. Our approach utilizes 2D images captured from multiple viewpoints and associates independent instance masks for neural radiance field (NeRF) view synthesis. We introduce crop visibility and mask consistency scores, which are incorporated alongside 3D information from a NeRF model. This results in an effective segmentation of crop instances in 3D and highly-accurate crop counts. Furthermore, our method eliminates the dependence on crop-specific parameter tuning. We validate our framework on three agricultural datasets consisting of cotton bolls, apples, and pears, and demonstrate consistent counting performance despite major variations in crop color, shape, and size. A comparative analysis against the state of the art highlights superior performance on crop counting tasks. Lastly, we contribute a cotton plant dataset to advance further research on this topic.

</details>


### [73] [IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation](https://arxiv.org/abs/2601.00212)
*Han Liu,Yubo Fan,Hao Li,Dewei Hu,Daniel Moyer,Zhoubing Xu,Benoit M. Dawant,Ipek Oguz*

Main category: cs.CV

TL;DR: The paper introduces 'IntraStyler,' an exemplar-based style synthesis method for unsupervised domain adaptation, addressing intra-domain variability without prior knowledge and evaluating its effectiveness on the CrossMoDA 2023 dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the underexplored issue of intra-domain variability in unsupervised domain adaptation, as previous studies mainly focus on minimizing domain gap between source and target domains.

Method: The proposed method, IntraStyler, utilizes an exemplar-based approach for capturing diverse intra-domain styles. It uses a style encoder guided by contrastive learning to extract style-only features and match output styles to given exemplars.

Result: The experiments conducted on the CrossMoDA 2023 dataset demonstrate the effectiveness of IntraStyler in controllable style synthesis and improving performance for downstream segmentation tasks.

Conclusion: IntraStyler proves to be an effective, practical solution for enhancing domain adaptation by addressing intra-domain variability, eliminating the need for pre-specified variations while improving segmentation outcomes.

Abstract: Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.

</details>


### [74] [From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning](https://arxiv.org/abs/2601.00215)
*Omar Sharif,Eftekhar Hossain,Patrick Ng*

Main category: cs.CV

TL;DR: The paper addresses the limitations of multimodal large language models (MLLMs) in effectively integrating visual information for reasoning tasks. The authors use reinforcement learning (RL) and reward functions to improve reasoning and answer accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal large language models (MLLMs) struggle to integrate visual information effectively when reasoning, hindering their performance on tasks requiring accurate visual perception.

Method: The study explores reward-driven reinforcement learning (RL) and designs six reward functions focusing on reasoning aspects such as image understanding, reasoning steps, and answer accuracy. The group relative policy optimization (GRPO) approach is employed to enhance reasoning.

Result: Experiments showed a 5.56% performance improvement on Qwen-2.5-VL-7B over the base model, with gains observed in both in-domain and out-of-domain tasks. Converting images to textual descriptions also significantly benefited reasoning.

Conclusion: Reward-driven RL and designed reward functions improve visual reasoning in MLLMs, enabling better integration of visual information and addressing reasoning bottlenecks without requiring extensive supervision.

Abstract: Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.
  To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.

</details>


### [75] [LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization](https://arxiv.org/abs/2601.00222)
*Jie Li,Kwan-Yee K. Wong,Kai Han*

Main category: cs.CV

TL;DR: This paper proposes 'LooC,' a novel low-dimensional compositional vector quantization approach, achieving high performance with a compact codebook.


<details>
  <summary>Details</summary>
Motivation: The increasing diversity and complexity of data and models demand vector quantization methods that are both high-capacity and compact.

Method: LooC reframes the relationship between codevectors and feature vectors for an efficient codebook, incorporates an extrapolation-by-interpolation mechanism, and supports plug-and-play integration into existing methods.

Result: LooC demonstrated state-of-the-art performance on various tasks, datasets, and architectures, with a significantly smaller codebook compared to existing methods.

Conclusion: LooC successfully enhances vector quantization by improving performance, compactness, and applicability across different tasks.

Abstract: Vector quantization (VQ) is a prevalent and fundamental technique that discretizes continuous feature vectors by approximating them using a codebook. As the diversity and complexity of data and models continue to increase, there is an urgent need for high-capacity, yet more compact VQ methods. This paper aims to reconcile this conflict by presenting a new approach called LooC, which utilizes an effective Low-dimensional codebook for Compositional vector quantization. Firstly, LooC introduces a parameter-efficient codebook by reframing the relationship between codevectors and feature vectors, significantly expanding its solution space. Instead of individually matching codevectors with feature vectors, LooC treats them as lower-dimensional compositional units within feature vectors and combines them, resulting in a more compact codebook with improved performance. Secondly, LooC incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance and smooth features during the VQ process, which allows for better preservation of details and fidelity in feature approximation. The design of LooC leads to full codebook usage, effectively utilizing the compact codebook while avoiding the problem of collapse. Thirdly, LooC can serve as a plug-and-play module for existing methods for different downstream tasks based on VQ. Finally, extensive evaluations on different tasks, datasets, and architectures demonstrate that LooC outperforms existing VQ methods, achieving state-of-the-art performance with a significantly smaller codebook.

</details>


### [76] [Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions](https://arxiv.org/abs/2601.00225)
*Aobo Li,Jinjian Wu,Yongxu Liu,Leida Li,Weisheng Dong*

Main category: cs.CV

TL;DR: The paper addresses the limitation of generalization in Blind Image Quality Assessment (BIQA) caused by synthetic datasets. It introduces the SynDR-IQA framework to improve generalization using strategies to reshape data distribution.


<details>
  <summary>Details</summary>
Motivation: BIQA faces challenges due to a lack of large labeled datasets and limited generalization ability of models trained on synthetic datasets. The need for improved synthetic data management drives this study.

Method: The work identifies representation clustering issues in BIQA models using synthetic datasets. SynDR-IQA reshapes the synthetic data distribution through visual diversity enhancement and redundant density reduction.

Result: Extensive experiments across synthetic, authentic, and algorithmic datasets confirm SynDR-IQA's ability to improve generalization in BIQA.

Conclusion: SynDR-IQA offers a significant improvement in BIQA generalization by addressing synthetic data distribution issues through targeted strategies.

Abstract: Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy's impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. The code is available at https://github.com/Li-aobo/SynDR-IQA.

</details>


### [77] [Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection](https://arxiv.org/abs/2601.00237)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

TL;DR: The paper proposes a cross-modal data augmentation framework using CycleGAN and YOLOv8 to address IR data scarcity in PCB defect detection, generating pseudo-IR samples for improved training.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the IR data scarcity problem in PCB defect detection, which traditional methods struggle with due to their reliance on paired supervision.

Method: The paper uses CycleGAN for unpaired image-to-image translation to generate pseudo-IR images from visible-light PCB images and integrates them with limited real IR data in a YOLOv8-based heterogeneous training strategy.

Result: Experimental results show enhanced feature learning under low-data conditions, with the proposed method outperforming models trained solely on real IR data and approaching fully supervised performance.

Conclusion: The proposed framework demonstrates the effectiveness of pseudo-IR synthesis as a strong data augmentation method, improving PCB defect detection under limited IR data scenarios.

Abstract: This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.

</details>


### [78] [Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture](https://arxiv.org/abs/2601.00243)
*Anirudha Ghosh,Ritam Sarkar,Debaditya Barman*

Main category: cs.CV

TL;DR: This study focuses on developing a lightweight, AI-based pest management system to assist small farmers with pest detection and eco-friendly pesticide recommendations, optimized for low-resource devices like smartphones and drones.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of traditional pest management methods, which are costly, time-consuming, labor-intensive, and environmentally harmful, by providing a more efficient and sustainable solution.

Method: The framework consists of two main components: (i) A Pest Detection Module leveraging lightweight CNNs and prototypical meta-learning for accurate pest identification with minimal training data; (ii) A Pesticide Recommendation Module that factors in crop type and conditions to suggest eco-friendly pesticides.

Result: Results show that the lightweight CNN achieves high accuracy while reducing computational requirements. The system encourages sustainable practices and demonstrates strong applicability for real-time precision agriculture.

Conclusion: The study concludes that the proposed pest management framework is effective and suitable for under-resourced farming setups, promoting both agricultural productivity and environmental sustainability.

Abstract: Effective pest management is crucial for enhancing agricultural productivity, especially for crops such as sugarcane and wheat that are highly vulnerable to pest infestations. Traditional pest management methods depend heavily on manual field inspections and the use of chemical pesticides. These approaches are often costly, time-consuming, labor-intensive, and can have a negative impact on the environment. To overcome these challenges, this study presents a lightweight framework for pest detection and pesticide recommendation, designed for low-resource devices such as smartphones and drones, making it suitable for use by small and marginal farmers.
  The proposed framework includes two main components. The first is a Pest Detection Module that uses a compact, lightweight convolutional neural network (CNN) combined with prototypical meta-learning to accurately identify pests even when only a few training samples are available. The second is a Pesticide Recommendation Module that incorporates environmental factors like crop type and growth stage to suggest safe and eco-friendly pesticide recommendations. To train and evaluate our framework, a comprehensive pest image dataset was developed by combining multiple publicly available datasets. The final dataset contains samples with different viewing angles, pest sizes, and background conditions to ensure strong generalization.
  Experimental results show that the proposed lightweight CNN achieves high accuracy, comparable to state-of-the-art models, while significantly reducing computational complexity. The Decision Support System additionally improves pest management by reducing dependence on traditional chemical pesticides and encouraging sustainable practices, demonstrating its potential for real-time applications in precision agriculture.

</details>


### [79] [TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models](https://arxiv.org/abs/2601.00260)
*Kohei Yamamoto,Tomohiro Kikuchi*

Main category: cs.CV

TL;DR: The paper introduces TotalFM, a radiological foundation model specifically designed to overcome computational constraints in 3D-CT training through organ-separated learning and integration of pre-training and contrastive learning methods.


<details>
  <summary>Details</summary>
Motivation: To address computational efficiency challenges in training 3D-CT data for radiological tasks and to improve generalization performance by leveraging organ-separated frameworks with large datasets.

Method: The study integrates segmentation techniques, large language models (LLMs), self-supervised pre-training (VideoMAE), and contrastive learning to align 3D-CT images with linguistic expressions, employing an organ-separated design for efficiency.

Result: TotalFM outperformed baseline models like CT-CLIP and Merlin in zero-shot organ-wise and finding-wise classification tasks, showcasing both high generalization ability and performance comparable to existing models in radiology report generation.

Conclusion: Organ-separated learning is an effective and efficient framework for training radiological foundation models, enabling practical deployment of 3D-CT models for diverse clinical tasks.

Abstract: While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a large-scale dataset of 140,000 series. By automating the creation of organ volume and finding-sentence pairs through segmentation techniques and Large Language Model (LLM)-based radiology report processing, and by combining self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs, we aimed to balance computational efficiency and representation capability. In zero-shot organ-wise lesion classification tasks, the proposed model achieved higher F1 scores in 83% (5/6) of organs compared to CT-CLIP and 64% (9/14) of organs compared to Merlin. These results suggest that the proposed model exhibits high generalization performance in a clinical evaluation setting using actual radiology report sentences. Furthermore, in zero-shot finding-wise lesion classification tasks, our model achieved a higher AUROC in 83% (25/30) of finding categories compared to Merlin. We also confirmed performance comparable to existing Vision-Language Models (VLMs) in radiology report generation tasks. Our results demonstrate that the organ-separated learning framework can serve as a realistic and effective design guideline for the practical implementation of 3D-CT foundation models.

</details>


### [80] [S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding](https://arxiv.org/abs/2601.00264)
*He Wang,Longteng Guo,Pengkang Huo,Xuanxu Lin,Yichen Yuan,Jie Jiang,Jing Liu*

Main category: cs.CV

TL;DR: The paper introduces S1-MMAlign, a dataset with 15.5 million image-text pairs from scientific papers, improved via an AI pipeline to address weak image-text alignment issues.


<details>
  <summary>Details</summary>
Motivation: To address challenges in applying multimodal learning to scientific discovery, given the semantic gap between scientific imagery and textual descriptions.

Method: Creation of a large dataset from scientific papers and enhancement using an AI pipeline with the Qwen-VL model to improve image-text captions.

Result: Enhanced semantic alignment demonstrated through metrics: improved SciBERT pseudo-perplexity and 18.21% higher image-text alignment via CLIP scores.

Conclusion: S1-MMAlign is a valuable resource for scientific reasoning and AI applications in cross-modal understanding.

Abstract: Multimodal learning has revolutionized general domain tasks, yet its application in scientific discovery is hindered by the profound semantic gap between complex scientific imagery and sparse textual descriptions. We present S1-MMAlign, a large-scale, multi-disciplinary multimodal dataset comprising over 15.5 million high-quality image-text pairs derived from 2.5 million open-access scientific papers. Spanning disciplines from physics and biology to engineering, the dataset captures diverse visual modalities including experimental setups, heatmaps, and microscopic imagery. To address the pervasive issue of weak alignment in raw scientific captions, we introduce an AI-ready semantic enhancement pipeline that utilizes the Qwen-VL multimodal large model series to recaption images by synthesizing context from paper abstracts and citation contexts. Technical validation demonstrates that this enhancement significantly improves data quality: SciBERT-based pseudo-perplexity metrics show reduced semantic ambiguity, while CLIP scores indicate an 18.21% improvement in image-text alignment. S1-MMAlign provides a foundational resource for advancing scientific reasoning and cross-modal understanding in the era of AI for Science. The dataset is publicly available at https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign.

</details>


### [81] [ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching](https://arxiv.org/abs/2601.00267)
*Yi Sun,Xinhao Zhong,Hongyan Li,Yimin Zhou,Junhao Li,Bin Chen,Xuan Wang*

Main category: cs.CV

TL;DR: The paper introduces ActErase, a training-free method for efficiently erasing sensitive concepts from text-to-image diffusion models, enabling safe and ethical generation.


<details>
  <summary>Details</summary>
Motivation: Current concept erasure methods for diffusion models are often data-intensive and require expensive fine-tuning, hindering accessibility and efficiency.

Method: ActErase uses prompt-pair analysis to identify activation difference regions, selectively extracts target activations, and replaces input activations dynamically in model forward passes without training.

Result: ActErase achieves state-of-the-art concept erasure performance across nudity, artistic style, and object removal tasks, while maintaining generative capabilities and robustness against adversarial attacks.

Conclusion: ActErase establishes a plug-and-play paradigm for effective and lightweight concept manipulation in diffusion models, addressing ethical and safety concerns.

Abstract: Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model's activations are predominantly composed of generic concepts, with only a minimal component can represent the target concept, we propose a novel training-free method (ActErase) for efficient concept erasure. Specifically, the proposed method operates by identifying activation difference regions via prompt-pair analysis, extracting target activations and dynamically replacing input activations during forward passes. Comprehensive evaluations across three critical erasure tasks (nudity, artistic style, and object removal) demonstrates that our training-free method achieves state-of-the-art (SOTA) erasure performance, while effectively preserving the model's overall generative capability. Our approach also exhibits strong robustness against adversarial attacks, establishing a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models.

</details>


### [82] [FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering](https://arxiv.org/abs/2601.00269)
*Chaodong Tong,Qi Zhang,Chen Li,Lei Jiang,Yanbing Liu*

Main category: cs.CV

TL;DR: The paper introduces FaithSCAN, a lightweight network that detects faithfulness hallucinations in vision-language models (VLMs) used in visual question answering (VQA), addressing inefficiencies and limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Current methods to detect hallucinations in VQA models are either inefficient, overly dependent on external resources, or ineffective due to limited use of internal model signals, necessitating a better solution for safety-critical applications.

Method: FaithSCAN combines internal signals like token-level uncertainty, intermediate visual representations, and cross-modal alignment using branch-wise encoding and attention. It also uses a cost-effective LLM-generated supervision strategy to enable training without human labels.

Result: Experiments show FaithSCAN performs better in detecting hallucinations, achieving higher accuracy and efficiency across multiple VQA benchmarks compared to conventional methods.

Conclusion: FaithSCAN offers a robust, efficient, and effective detection system for faithfulness hallucinations in VLMs, providing insights into their systematic causes and highlighting complementary diagnostic signals from internal model states.

Abstract: Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.

</details>


### [83] [Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers](https://arxiv.org/abs/2601.00359)
*Söhnke Benedikt Fischedick,Daniel Seichter,Benedict Stephan,Robin Schmidt,Horst-Michael Gross*

Main category: cs.CV

TL;DR: This paper introduces DVEFormer, an RGB-D Transformer-based method for dense text-aligned visual embeddings, enhancing robot-human interactions in domestic environments.


<details>
  <summary>Details</summary>
Motivation: To enable robots in domestic environments to understand their surroundings for intuitive interactions with humans, surpassing traditional semantic segmentation limitations.

Method: DVEFormer utilizes RGB-D inputs and knowledge distillation from Alpha-CLIP embeddings to learn fine-grained pixel-wise dense embeddings. It supports linear probing-based semantic segmentation and enables flexible natural-language querying.

Result: DVEFormer delivers competitive performance compared to traditional methods, achieving real-time operation at 26.3 FPS and 77.0 FPS for two model variants, with evaluations on indoor datasets and qualitative real-world results.

Conclusion: DVEFormer extends traditional segmentation approaches, facilitating flexible text-based querying and seamless application in mobile robotics, including 3D mapping pipelines.

Abstract: In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.

</details>


### [84] [Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification](https://arxiv.org/abs/2601.00278)
*Chi Ding,Junxiao Xue,Xinyi Yin,Shi Chen,Yunyun Shi,Yiduo Wang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: The paper addresses the overlooked challenge of distinguishing hard tail data samples from noisy ones in remote sensing’s long-tailed distributions. The proposed DUAL framework disentangles Epistemic and Aleatoric Uncertainty to improve model predictions.


<details>
  <summary>Details</summary>
Motivation: Remote sensing struggles with imbalanced data distributions, complicated by the challenge of separating hard samples and noise in tail data.

Method: The DUAL framework utilizes Evidential Deep Learning to separate prediction uncertainties into Epistemic and Aleatoric Uncertainty, guiding strategies for reweighting and noise suppression.

Result: DUAL demonstrates superior performance and generalization across datasets and backbones, outperforming existing methods like TGN and SADE.

Conclusion: DUAL effectively addresses the challenge of disentangling hard tail samples from noisy ones, leveraging uncertainty metrics for improved remote sensing analysis.

Abstract: Long-Tailed distributions are pervasive in remote sensing due to the inherently imbalanced occurrence of grounded objects. However, a critical challenge remains largely overlooked, i.e., disentangling hard tail data samples from noisy ambiguous ones. Conventional methods often indiscriminately emphasize all low-confidence samples, leading to overfitting on noisy data. To bridge this gap, building upon Evidential Deep Learning, we propose a model-agnostic uncertainty-aware framework termed DUAL, which dynamically disentangles prediction uncertainty into Epistemic Uncertainty (EU) and Aleatoric Uncertainty (AU). Specifically, we introduce EU as an indicator of sample scarcity to guide a reweighting strategy for hard-to-learn tail samples, while leveraging AU to quantify data ambiguity, employing an adaptive label smoothing mechanism to suppress the impact of noise. Extensive experiments on multiple datasets across various backbones demonstrate the effectiveness and generalization of our framework, surpassing strong baselines such as TGN and SADE. Ablation studies provide further insights into the crucial choices of our design.

</details>


### [85] [SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting](https://arxiv.org/abs/2601.00285)
*Jun-Jee Chao,Volkan Isler*

Main category: cs.CV

TL;DR: SV-GS provides a framework for reconstructing dynamic objects under sparse observations using a skeleton-driven deformation model.


<details>
  <summary>Details</summary>
Motivation: Reconstructing dynamic objects in real-world scenarios is challenging due to sparse and diverse observations like those from security cameras.

Method: SV-GS uses a skeleton-based deformation field with joint pose estimators and fine-grained deformation modules to handle sparse observations and motion interpolation.

Result: SV-GS improves reconstruction performance under sparse observations by up to 34% (PSNR) and achieves comparable results to dense methods even with fewer frames.

Conclusion: SV-GS offers practical real-world applications; its reliance on initial static reconstruction can be relaxed using diffusion-based generative priors.

Abstract: Reconstructing a dynamic target moving over a large area is challenging. Standard approaches for dynamic object reconstruction require dense coverage in both the viewing space and the temporal dimension, typically relying on multi-view videos captured at each time step. However, such setups are only possible in constrained environments. In real-world scenarios, observations are often sparse over time and captured sparsely from diverse viewpoints (e.g., from security cameras), making dynamic reconstruction highly ill-posed. We present SV-GS, a framework that simultaneously estimates a deformation model and the object's motion over time under sparse observations. To initialize SV-GS, we leverage a rough skeleton graph and an initial static reconstruction as inputs to guide motion estimation. (Later, we show that this input requirement can be relaxed.) Our method optimizes a skeleton-driven deformation field composed of a coarse skeleton joint pose estimator and a module for fine-grained deformations. By making only the joint pose estimator time-dependent, our model enables smooth motion interpolation while preserving learned geometric details. Experiments on synthetic datasets show that our method outperforms existing approaches under sparse observations by up to 34% in PSNR, and achieves comparable performance to dense monocular video methods on real-world datasets despite using significantly fewer frames. Moreover, we demonstrate that the input initial static reconstruction can be replaced by a diffusion-based generative prior, making our method more practical for real-world scenarios.

</details>


### [86] [Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies](https://arxiv.org/abs/2601.00286)
*Ali Anaissi,Ali Braytee,Weidong Huang,Junaid Akram,Alaa Farhat,Jie Hua*

Main category: cs.CV

TL;DR: This paper presents a deep learning model using the Swin Transformer for diagnosing skin conditions, achieving 87.71% accuracy on eight lesion classes.


<details>
  <summary>Details</summary>
Motivation: Address the increasing prevalence of dermatological conditions amidst the limited availability of dermatologists.

Method: Developed and refined a deep learning model leveraging pretraining, data preprocessing workflows, augmentation techniques, and employing the Swin Transformer architecture.

Result: Achieved an accuracy of 87.71% in classifying eight skin lesion classes using the ISIC2019 dataset.

Conclusion: The model demonstrates strong potential as a tool for clinical diagnostic support and patient self-assessment in dermatology.

Abstract: As dermatological conditions become increasingly common and the availability of dermatologists remains limited, there is a growing need for intelligent tools to support both patients and clinicians in the timely and accurate diagnosis of skin diseases. In this project, we developed a deep learning based model for the classification and diagnosis of skin conditions. By leveraging pretraining on publicly available skin disease image datasets, our model effectively extracted visual features and accurately classified various dermatological cases. Throughout the project, we refined the model architecture, optimized data preprocessing workflows, and applied targeted data augmentation techniques to improve overall performance. The final model, based on the Swin Transformer, achieved a prediction accuracy of 87.71 percent across eight skin lesion classes on the ISIC2019 dataset. These results demonstrate the model's potential as a diagnostic support tool for clinicians and a self assessment aid for patients.

</details>


### [87] [RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization](https://arxiv.org/abs/2601.00705)
*Wei-Tse Cheng,Yen-Jen Chiou,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: RGS-SLAM replaces the residual-driven stage in GS-SLAM with a training-free initialization, stabilizing mapping, improving rendering, and achieving competitive accuracy and real-time performance.


<details>
  <summary>Details</summary>
Motivation: To improve the stability, mapping quality, and render fidelity of GS-SLAM in complex environments.

Method: RGS-SLAM introduces a one-shot triangulation approach using DINOv3 descriptors with a confidence-aware inlier classifier for Gaussian initialization.

Result: RGS-SLAM offers a 20% faster convergence, higher accuracy, and maintains real-time performance at 925 FPS, compared with state-of-the-art systems on datasets like TUM RGB-D and Replica.

Conclusion: RGS-SLAM stabilizes early mapping, enhances rendering fidelity in complex scenes, and is compatible with existing pipelines, making it a robust alternative to current SLAM systems.

Abstract: We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.

</details>


### [88] [TimeColor: Flexible Reference Colorization via Temporal Concatenation](https://arxiv.org/abs/2601.00296)
*Bryan Constantine Sadihin,Yihao Meng,Michael Hua Wang,Matteo Jiahao Chen,Hang Su*

Main category: cs.CV

TL;DR: TimeColor, a sketch-based video colorization model, effectively utilizes multiple and variable references to improve color fidelity and consistency.


<details>
  <summary>Details</summary>
Motivation: To address limitations in video colorization models that typically rely on static single-reference input, limiting the use of diverse conditional data like character sheets or arbitrary colorized frames.

Method: Introduces a model that uses explicit per-reference region assignment, latent frame encoding, and spatiotemporal correspondence-masked attention to process multiple references concurrently in diffusion steps.

Result: Experiments on SAKUGA-42M demonstrated improvements in color fidelity, identity consistency, and temporal stability compared to prior baselines.

Conclusion: TimeColor enhances video colorization by leveraging heterogeneous references and advanced attention mechanisms to overcome challenges in traditional models.

Abstract: Most colorization models condition only on a single reference, typically the first frame of the scene. However, this approach ignores other sources of conditional data, such as character sheets, background images, or arbitrary colorized frames. We propose TimeColor, a sketch-based video colorization model that supports heterogeneous, variable-count references with the use of explicit per-reference region assignment. TimeColor encodes references as additional latent frames which are concatenated temporally, permitting them to be processed concurrently in each diffusion step while keeping the model's parameter count fixed. TimeColor also uses spatiotemporal correspondence-masked attention to enforce subject-reference binding in addition to modality-disjoint RoPE indexing. These mechanisms mitigate shortcutting and cross-identity palette leakage. Experiments on SAKUGA-42M under both single- and multi-reference protocols show that TimeColor improves color fidelity, identity consistency, and temporal stability over prior baselines.

</details>


### [89] [VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning](https://arxiv.org/abs/2601.00307)
*Anns Ijaz,Muhammad Azeem Javed*

Main category: cs.CV

TL;DR: The paper introduces VisNet, an efficient person re-identification model designed for real-world scenarios with strong accuracy and minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: The need for a re-identification model that balances high accuracy with low computational cost for real-time deployment in surveillance and mobile applications.

Method: VisNet employs multiple conceptual innovations such as multi-scale feature fusion with automatic attention, semantic clustering through anatomical body partitioning, dynamic weight averaging to balance classification semantic regularization, and the FIDI loss function for metric learning tasks. It uses ResNet50's stages 1 through 4 for feature extraction without parallel paths.

Result: VisNet achieved 87.05% Rank-1 accuracy and 77.65% mAP on the Market-1501 dataset, with a model size of 32.41M parameters and a computational cost of 4.601 GFLOPs.

Conclusion: VisNet is an effective and computationally efficient ReID model suitable for real-time applications in surveillance and mobile scenarios, providing a balance between accuracy and resource constraints.

Abstract: Person re-identification (ReID) is an extremely important area in both surveillance and mobile applications, requiring strong accuracy with minimal computational cost. State-of-the-art methods give good accuracy but with high computational budgets. To remedy this, this paper proposes VisNet, a computationally efficient and effective re-identification model suitable for real-world scenarios. It is the culmination of conceptual contributions, including feature fusion at multiple scales with automatic attention on each, semantic clustering with anatomical body partitioning, a dynamic weight averaging technique to balance classification semantic regularization, and the use of loss function FIDI for improved metric learning tasks. The multiple scales fuse ResNet50's stages 1 through 4 without the use of parallel paths, with semantic clustering introducing spatial constraints through the use of rule-based pseudo-labeling. VisNet achieves 87.05% Rank-1 and 77.65% mAP on the Market-1501 dataset, having 32.41M parameters and 4.601 GFLOPs, hence, proposing a practical approach for real-time deployment in surveillance and mobile applications where computational resources are limited.

</details>


### [90] [ReMA: A Training-Free Plug-and-Play Mixing Augmentation for Video Behavior Recognition](https://arxiv.org/abs/2601.00311)
*Feng-Qi Cui,Jinyang Huang,Sirui Zhao,Jinglong Guo,Qifan Cai,Xin Yan,Zhi Liu*

Main category: cs.CV

TL;DR: ReMA is a new video data augmentation technique addressing challenges in stable and discriminative video representations under complex variations, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current video data augmentation strategies cause uncontrolled variations that hinder representation quality and intra-class distribution consistency.

Method: Introduce Representation-aware Mixing Augmentation combining Representation Alignment Mechanism (RAM) to align intra-class structures and Dynamic Selection Mechanism (DSM) to apply spatiotemporal masks.

Result: Experiments show ReMA consistently improves generalization and robustness on video behavior benchmarks without additional parameters or supervision.

Conclusion: ReMA is a significant improvement for video augmentation, offering stable and discriminative representations across varying spatiotemporal scales.

Abstract: Video behavior recognition demands stable and discriminative representations under complex spatiotemporal variations. However, prevailing data augmentation strategies for videos remain largely perturbation-driven, often introducing uncontrolled variations that amplify non-discriminative factors, which finally weaken intra-class distributional structure and representation drift with inconsistent gains across temporal scales. To address these problems, we propose Representation-aware Mixing Augmentation (ReMA), a plug-and-play augmentation strategy that formulates mixing as a controlled replacement process to expand representations while preserving class-conditional stability. ReMA integrates two complementary mechanisms. Firstly, the Representation Alignment Mechanism (RAM) performs structured intra-class mixing under distributional alignment constraints, suppressing irrelevant intra-class drift while enhancing statistical reliability. Then, the Dynamic Selection Mechanism (DSM) generates motion-aware spatiotemporal masks to localize perturbations, guiding them away from discrimination-sensitive regions and promoting temporal coherence. By jointly controlling how and where mixing is applied, ReMA improves representation robustness without additional supervision or trainable parameters. Extensive experiments on diverse video behavior benchmarks demonstrate that ReMA consistently enhances generalization and robustness across different spatiotemporal granularities.

</details>


### [91] [Depth-Synergized Mamba Meets Memory Experts for All-Day Image Reflection Separation](https://arxiv.org/abs/2601.00322)
*Siyan Fang,Long Peng,Yuntao Wang,Ruonan Wei,Yuehuan Wang*

Main category: cs.CV

TL;DR: The paper introduces DMDNet, a novel method for image reflection separation, especially effective for challenging nighttime scenarios, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to separate image layers when they have similar contrasts, a problem exacerbated in nighttime scenarios.

Method: DMDNet employs Depth-Aware Scanning (DAScan) for semantic structure coherence, Depth-Synergized State-Space Model (DS-SSM) to manage ambiguous features, and a Memory Expert Compensation Module (MECM) for layer-specific compensation, alongside the introduction of the new NightIRS dataset.

Result: DMDNet achieves superior performance over state-of-the-art methods for both daytime and nighttime image reflection separation, driven by extensive experiments.

Conclusion: The proposed DMDNet method effectively disentangles image layers in complex scenarios, offering significant advancements in the task of image reflection separation.

Abstract: Image reflection separation aims to disentangle the transmission layer and the reflection layer from a blended image. Existing methods rely on limited information from a single image, tending to confuse the two layers when their contrasts are similar, a challenge more severe at night. To address this issue, we propose the Depth-Memory Decoupling Network (DMDNet). It employs the Depth-Aware Scanning (DAScan) to guide Mamba toward salient structures, promoting information flow along semantic coherence to construct stable states. Working in synergy with DAScan, the Depth-Synergized State-Space Model (DS-SSM) modulates the sensitivity of state activations by depth, suppressing the spread of ambiguous features that interfere with layer disentanglement. Furthermore, we introduce the Memory Expert Compensation Module (MECM), leveraging cross-image historical knowledge to guide experts in providing layer-specific compensation. To address the lack of datasets for nighttime reflection separation, we construct the Nighttime Image Reflection Separation (NightIRS) dataset. Extensive experiments demonstrate that DMDNet outperforms state-of-the-art methods in both daytime and nighttime.

</details>


### [92] [HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection](https://arxiv.org/abs/2601.00327)
*Naiqi Zhang,Chuancheng Shi,Jingtong Dou,Wenhua Wu,Fei Shen,Jianhua Cao*

Main category: cs.CV

TL;DR: This paper introduces HarmoniAD, a dual-branch framework for industrial anomaly detection, effectively balancing fine detail and global semantics using frequency-guided modeling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the structure-semantics trade-off in industrial anomaly detection, where fine details are often missed by semantics-oriented models while structure-oriented models are noise-sensitive.

Method: The method involves a frequency-guided dual-branch approach. Features extracted by the CLIP image encoder are transformed into the frequency domain and split into high-frequency and low-frequency paths. High-frequency branches focus on structural details (via FSAM), while low-frequency branches capture global semantic consistency (via GSCM).

Result: HarmoniAD achieved state-of-the-art performance in sensitivity and robustness, validated through experiments on multiple datasets (MVTec-AD, VisA, and BTAD).

Conclusion: HarmoniAD effectively balances structural and semantic aspects, providing a robust solution for detecting tiny anomalies in industrial inspection.

Abstract: Anomaly detection is crucial in industrial product quality inspection. Failing to detect tiny defects often leads to serious consequences. Existing methods face a structure-semantics trade-off: structure-oriented models (such as frequency-based filters) are noise-sensitive, while semantics-oriented models (such as CLIP-based encoders) often miss fine details. To address this, we propose HarmoniAD, a frequency-guided dual-branch framework. Features are first extracted by the CLIP image encoder, then transformed into the frequency domain, and finally decoupled into high- and low-frequency paths for complementary modeling of structure and semantics. The high-frequency branch is equipped with a fine-grained structural attention module (FSAM) to enhance textures and edges for detecting small anomalies, while the low-frequency branch uses a global structural context module (GSCM) to capture long-range dependencies and preserve semantic consistency. Together, these branches balance fine detail and global semantics. HarmoniAD further adopts a multi-class joint training strategy, and experiments on MVTec-AD, VisA, and BTAD show state-of-the-art performance with both sensitivity and robustness.

</details>


### [93] [Joint Geometry-Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion](https://arxiv.org/abs/2601.00328)
*Yingzhi Tang,Qijian Zhang,Junhui Hou*

Main category: cs.CV

TL;DR: This paper introduces JGA-LBD, a unified framework for geometry and appearance reconstruction of 3D humans from a single image, leveraging joint latent representations and bridge diffusion.


<details>
  <summary>Details</summary>
Motivation: The task of achieving consistent and high-fidelity 3D human reconstruction from a single RGB image is highly challenging due to the separation of geometry and appearance in existing methods, resulting in inconsistencies.

Method: The authors propose JGA-LBD, which unifies 3D Gaussian representations of input conditions into a joint latent space using a shared sparse variational autoencoder. They employ bridge diffusion to infer missing components of the target latent code and a decoding module to extract geometry and render novel views.

Result: JGA-LBD outperforms current state-of-the-art methods in geometry and appearance quality, including in complex real-world scenarios.

Conclusion: JGA-LBD effectively unifies geometry and appearance reconstruction, achieving superior performance and consistency in challenging 3D human reconstruction tasks. The code will be made available publicly.

Abstract: Achieving consistent and high-fidelity geometry and appearance reconstruction of 3D digital humans from a single RGB image is inherently a challenging task. Existing studies typically resort to decoupled pipelines for geometry estimation and appearance synthesis, often hindering unified reconstruction and causing inconsistencies. This paper introduces \textbf{JGA-LBD}, a novel framework that unifies the modeling of geometry and appearance into a joint latent representation and formulates the generation process as bridge diffusion. Observing that directly integrating heterogeneous input conditions (e.g., depth maps, SMPL models) leads to substantial training difficulties, we unify all conditions into the 3D Gaussian representations, which can be further compressed into a unified latent space through a shared sparse variational autoencoder (VAE). Subsequently, the specialized form of bridge diffusion enables to start with a partial observation of the target latent code and solely focuses on inferring the missing components. Finally, a dedicated decoding module extracts the complete 3D human geometric structure and renders novel views from the inferred latent representation. Experiments demonstrate that JGA-LBD outperforms current state-of-the-art approaches in terms of both geometry fidelity and appearance quality, including challenging in-the-wild scenarios. Our code will be made publicly available at https://github.com/haiantyz/JGA-LBD.

</details>


### [94] [Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation](https://arxiv.org/abs/2601.00344)
*Bruce Mugizi,Sudi Murindanyi,Olivia Nakacwa,Andrew Katumba*

Main category: cs.CV

TL;DR: This study introduces a real-time traffic surveillance system for developing regions like Uganda, focusing on vehicle detection, license plate recognition, and speed estimation using computer vision.


<details>
  <summary>Details</summary>
Motivation: The paper aims to reduce road fatalities caused by speeding, particularly in resource-constrained environments lacking robust traffic safety systems.

Method: The study utilized a combination of a speed gun, Canon camera, and mobile phone to create datasets and developed models for license plate detection using YOLOv8, character recognition using CNN and transformer models, and speed estimation leveraging regions of interest.

Result: The system achieved a 97.9% mAP for license plate detection, 1.79% CER for character recognition via the transformer model, and a 10 km/h margin of error for speed estimation. An integrated database with SMS ticketing was also developed.

Conclusion: This system brings an effective and scalable solution for traffic management and automated enforcement in developing countries, potentially reducing accidents and enhancing road safety.

Abstract: Speeding is a major contributor to road fatalities, particularly in developing countries such as Uganda, where road safety infrastructure is limited. This study proposes a real-time intelligent traffic surveillance system tailored to such regions, using computer vision techniques to address vehicle detection, license plate recognition, and speed estimation. The study collected a rich dataset using a speed gun, a Canon Camera, and a mobile phone to train the models. License plate detection using YOLOv8 achieved a mean average precision (mAP) of 97.9%. For character recognition of the detected license plate, the CNN model got a character error rate (CER) of 3.85%, while the transformer model significantly reduced the CER to 1.79%. Speed estimation used source and target regions of interest, yielding a good performance of 10 km/h margin of error. Additionally, a database was established to correlate user information with vehicle detection data, enabling automated ticket issuance via SMS via Africa's Talking API. This system addresses critical traffic management needs in resource-constrained environments and shows potential to reduce road accidents through automated traffic enforcement in developing countries where such interventions are urgently needed.

</details>


### [95] [OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning](https://arxiv.org/abs/2601.00352)
*Liuxiang Qiu,Hui Da,Yuzhen Niu,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: The paper proposes the OmniVaT framework for single domain generalization in visual-tactile learning by addressing modality and domain gaps using innovative techniques.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in visual-tactile learning caused by modality discrepancies and domain gaps due to non-standardized tactile sensors and inconsistent data collection.

Method: The OmniVaT framework employs a multimodal fractional Fourier adapter (MFFA) for unified embeddings and a discrete tree generation (DTG) module to enhance adaptability to domain shifts using hierarchical structures.

Result: OmniVaT shows superior generalization performance in cross-domain visual-tactile learning tasks through extensive experiments.

Conclusion: The proposed OmniVaT framework effectively mitigates modality and domain challenges, advancing single domain generalization in multimodal visual-tactile learning.

Abstract: Visual-tactile learning (VTL) enables embodied agents to perceive the physical world by integrating visual (VIS) and tactile (TAC) sensors. However, VTL still suffers from modality discrepancies between VIS and TAC images, as well as domain gaps caused by non-standardized tactile sensors and inconsistent data collection procedures. We formulate these challenges as a new task, termed single domain generalization for multimodal VTL (SDG-VTL). In this paper, we propose an OmniVaT framework that, for the first time, successfully addresses this task. On the one hand, OmniVaT integrates a multimodal fractional Fourier adapter (MFFA) to map VIS and TAC embeddings into a unified embedding-frequency space, thereby effectively mitigating the modality gap without multi-domain training data or careful cross-modal fusion strategies. On the other hand, it also incorporates a discrete tree generation (DTG) module that obtains diverse and reliable multimodal fractional representations through a hierarchical tree structure, thereby enhancing its adaptivity to fluctuating domain shifts in unseen domains. Extensive experiments demonstrate the superior cross-domain generalization performance of OmniVaT on the SDG-VTL task.

</details>


### [96] [Mask-Conditioned Voxel Diffusion for Joint Geometry and Color Inpainting](https://arxiv.org/abs/2601.00368)
*Aarya Sumuk*

Main category: cs.CV

TL;DR: The paper proposes a two-stage pipeline for geometry and color restoration of 3D objects, leveraging mask prediction and diffusion-based inpainting techniques.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the need to restore damaged cultural heritage artifacts with high accuracy.

Method: The approach involves predicting damage masks using a 2D CNN on voxelized RGB slices, followed by volumetric mask aggregation and diffusion-based inpainting via a 3D U-Net.

Result: Evaluation on textured artifacts with synthetic damage shows improved geometry and color reconstruction compared to symmetry-based baselines.

Conclusion: The method demonstrates that mask conditioning is effective in guiding volumetric diffusion models for comprehensive 3D restoration tasks.

Abstract: We present a lightweight two-stage framework for joint geometry and color inpainting of damaged 3D objects, motivated by the digital restoration of cultural heritage artifacts. The pipeline separates damage localization from reconstruction. In the first stage, a 2D convolutional network predicts damage masks on RGB slices extracted from a voxelized object, and these predictions are aggregated into a volumetric mask. In the second stage, a diffusion-based 3D U-Net performs mask-conditioned inpainting directly on voxel grids, reconstructing geometry and color while preserving observed regions. The model jointly predicts occupancy and color using a composite objective that combines occupancy reconstruction with masked color reconstruction and perceptual regularization. We evaluate the approach on a curated set of textured artifacts with synthetically generated damage using standard geometric and color metrics. Compared to symmetry-based baselines, our method produces more complete geometry and more coherent color reconstructions at a fixed 32^3 resolution. Overall, the results indicate that explicit mask conditioning is a practical way to guide volumetric diffusion models for joint 3D geometry and color inpainting.

</details>


### [97] [BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition](https://arxiv.org/abs/2601.00369)
*Seungyeon Cho,Tae-kyun Kim*

Main category: cs.CV

TL;DR: This research innovates in human action recognition by integrating detailed hand articulation analysis through a probabilistic dual-stream framework with cross-modal ensemble learning.


<details>
  <summary>Details</summary>
Motivation: Current skeleton-based HAR methods often ignore subtle hand articulations, which are essential for fine-grained action recognition.

Method: It proposes a probabilistic dual-stream framework including: (1) preprocess using native coordinates, (2) probabilistic Noisy-OR for dual-stream fusion, and (3) ensemble coupling skeleton modalities with RGB data for cross-modal learning.

Result: The method demonstrates consistent improvements and enhanced robustness in noisy and heterogeneous conditions across several benchmarks.

Conclusion: The approach significantly boosts fine-grained recognition and bridges structural and visual motion domains effectively for action recognition.

Abstract: Skeleton-based human action recognition (HAR) has achieved remarkable progress with graph-based architectures. However, most existing methods remain body-centric, focusing on large-scale motions while neglecting subtle hand articulations that are crucial for fine-grained recognition. This work presents a probabilistic dual-stream framework that unifies reliability modeling and multi-modal integration, generalizing expertized learning under uncertainty across both intra-skeleton and cross-modal domains. The framework comprises three key components: (1) a calibration-free preprocessing pipeline that removes canonical-space transformations and learns directly from native coordinates; (2) a probabilistic Noisy-OR fusion that stabilizes reliability-aware dual-stream learning without requiring explicit confidence supervision; and (3) an intra- to cross-modal ensemble that couples four skeleton modalities (Joint, Bone, Joint Motion, and Bone Motion) to RGB representations, bridging structural and visual motion cues in a unified cross-modal formulation. Comprehensive evaluations across multiple benchmarks (NTU RGB+D~60/120, PKU-MMD, N-UCLA) and a newly defined hand-centric benchmark exhibit consistent improvements and robustness under noisy and heterogeneous conditions.

</details>


### [98] [NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos](https://arxiv.org/abs/2601.00393)
*Yuxue Yang,Lue Fan,Ziqi Shi,Junran Peng,Feng Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: NeoVerse introduces a 4D world model that overcomes scalability issues and is capable of reconstruction, video generation, and downstream applications using in-the-wild monocular videos.


<details>
  <summary>Details</summary>
Motivation: Address limitations in scalability of 4D world modeling methods reliant on expensive multi-view data or complex preprocessing.

Method: Proposes pose-free feed-forward 4D reconstruction and online monocular degradation pattern simulation, making the process scalable and versatile.

Result: NeoVerse achieves state-of-the-art performance in reconstruction and video generation benchmarks.

Conclusion: NeoVerse demonstrates versatility across different domains and establishes scalability with diverse monocular videos, fostering advancements in 4D world modeling.

Abstract: In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io

</details>


### [99] [RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection](https://arxiv.org/abs/2601.00398)
*Tao Wu,Qing Xu,Xiangjian He,Oakleigh Weekes,James Brown,Wenting Duan*

Main category: cs.CV

TL;DR: The authors introduce RoLID-11K, the first large-scale dashcam-based dataset for roadside litter detection to address environmental challenges, and benchmark detection methods.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of current roadside litter monitoring (labour-intensive and sparse coverage) and the lack of adequate datasets for detecting roadside litter from dashcam footage.

Method: Developed and released RoLID-11K dataset featuring 11k annotated dashcam images, covering diverse driving conditions, and benchmarked modern object detection models with transformer and YOLO architectures.

Result: CO-DETR gains the best localization accuracy; however, real-time YOLO models are constrained by coarse hierarchies, showing limitations in small-object detection.

Conclusion: RoLID-11K offers a valuable benchmark for improving roadside litter detection and supports scalable, low-cost solutions for monitoring in dynamic driving scenarios.

Abstract: Roadside litter poses environmental, safety and economic challenges, yet current monitoring relies on labour-intensive surveys and public reporting, providing limited spatial coverage. Existing vision datasets for litter detection focus on street-level still images, aerial scenes or aquatic environments, and do not reflect the unique characteristics of dashcam footage, where litter appears extremely small, sparse and embedded in cluttered road-verge backgrounds. We introduce RoLID-11K, the first large-scale dataset for roadside litter detection from dashcams, comprising over 11k annotated images spanning diverse UK driving conditions and exhibiting pronounced long-tail and small-object distributions. We benchmark a broad spectrum of modern detectors, from accuracy-oriented transformer architectures to real-time YOLO models, and analyse their strengths and limitations on this challenging task. Our results show that while CO-DETR and related transformers achieve the best localisation accuracy, real-time models remain constrained by coarse feature hierarchies. RoLID-11K establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes and aims to support the development of scalable, low-cost systems for roadside-litter monitoring. The dataset is available at https://github.com/xq141839/RoLID-11K.

</details>


### [100] [ABFR-KAN: Kolmogorov-Arnold Networks for Functional Brain Analysis](https://arxiv.org/abs/2601.00416)
*Tyler Ward,Abdullah Imran*

Main category: cs.CV

TL;DR: The study presents a novel transformer-based model, ABFR-KAN, for refined functional connectivity (FC) analysis in brain disorder diagnosis, reducing structural bias and improving ASD classification.


<details>
  <summary>Details</summary>
Motivation: Current FC analysis methods suffer from issues like selection bias and disregard for subject specificity due to reliance on atlas-based parcellation.

Method: A transformer-based classification network (ABFR-KAN) integrates advanced brain function representation and Kolmogorov-Arnold Networks for enhanced FC estimation.

Result: Extensive testing on the ABIDE I dataset shows superior performance of ABFR-KAN in ASD classification compared to state-of-the-art methods.

Conclusion: ABFR-KAN addresses structural bias and enhances the reliability of FC analysis, demonstrating consistent superiority in diagnosing ASD over existing models.

Abstract: Functional connectivity (FC) analysis, a valuable tool for computer-aided brain disorder diagnosis, traditionally relies on atlas-based parcellation. However, issues relating to selection bias and a lack of regard for subject specificity can arise as a result of such parcellations. Addressing this, we propose ABFR-KAN, a transformer-based classification network that incorporates novel advanced brain function representation components with the power of Kolmogorov-Arnold Networks (KANs) to mitigate structural bias, improve anatomical conformity, and enhance the reliability of FC estimation. Extensive experiments on the ABIDE I dataset, including cross-site evaluation and ablation studies across varying model backbones and KAN configurations, demonstrate that ABFR-KAN consistently outperforms state-of-the-art baselines for autism spectrum distorder (ASD) classification. Our code is available at https://github.com/tbwa233/ABFR-KAN.

</details>


### [101] [Robust Assembly Progress Estimation via Deep Metric Learning](https://arxiv.org/abs/2601.00422)
*Kazuma Miura,Sarthak Pathak,Kazunori Umeda*

Main category: cs.CV

TL;DR: This paper introduces "Anomaly Quadruplet-Net," a system that improves assembly progress estimation using Quadruplet Loss-based learning and a custom data loader, aimed at addressing challenges like occlusion and minimal visual changes.


<details>
  <summary>Details</summary>
Motivation: The need for accurate progress monitoring in manual multi-day assembly tasks, which is critical for enhancing operational efficiency, reducing waste, and maximizing productivity in smart factories.

Method: The proposed method incorporates Quadruplet Loss-based learning for anomaly images and a custom data loader to optimize training sample selection, improving assembly progress estimation under challenging conditions.

Result: Experiments performed on image datasets of desktop PC assembly demonstrated enhanced accuracy. Anomaly Quadruplet-Net achieved a 1.3% improvement in estimation accuracy and reduced adjacent task misclassification by 1.9%.

Conclusion: Anomaly Quadruplet-Net proved to be robust and superior to previous methods, specifically addressing issues like occlusion and subtle visual changes, thereby providing an effective solution for assembly progress estimation in smart factories.

Abstract: In recent years, the advancement of AI technologies has accelerated the development of smart factories. In particular, the automatic monitoring of product assembly progress is crucial for improving operational efficiency, minimizing the cost of discarded parts, and maximizing factory productivity. However, in cases where assembly tasks are performed manually over multiple days, implementing smart factory systems remains a challenge. Previous work has proposed Anomaly Triplet-Net, which estimates assembly progress by applying deep metric learning to the visual features of products. Nevertheless, when visual changes between consecutive tasks are subtle, misclassification often occurs. To address this issue, this paper proposes a robust system for estimating assembly progress, even in cases of occlusion or minimal visual change, using a small-scale dataset. Our method leverages a Quadruplet Loss-based learning approach for anomaly images and introduces a custom data loader that strategically selects training samples to enhance estimation accuracy. We evaluated our approach using a image datasets: captured during desktop PC assembly. The proposed Anomaly Quadruplet-Net outperformed existing methods on the dataset. Specifically, it improved the estimation accuracy by 1.3% and reduced misclassification between adjacent tasks by 1.9% in the desktop PC dataset and demonstrating the effectiveness of the proposed method.

</details>


### [102] [CPPO: Contrastive Perception for Vision Language Policy Optimization](https://arxiv.org/abs/2601.00501)
*Ahmad Rezaei,Mohsen Gholami,Saeed Ranjbar Alvar,Kevin Cannons,Mohammad Asiful Hossain,Zhou Weimin,Shunbo Zhou,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

TL;DR: CPPO introduces a method to enhance multimodal reasoning in vision-language models using a novel approach to detect perception tokens and improve the RL objective function.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve reasoning in multimodal models, which requires addressing both perception and reasoning aspects without relying on external models or forced token separations.

Method: CPPO identifies perception tokens via entropy shifts under input perturbations and employs a Contrastive Perception Loss to maintain consistency and sensitivity in the model outputs.

Result: Experiments demonstrated CPPO's superiority over previous methods in enhancing perception within RL frameworks, while being more efficient and scalable.

Conclusion: CPPO provides an effective and scalable solution for fine-tuning VLMs by efficiently addressing perception tokens without additional dependencies.

Abstract: We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.

</details>


### [103] [MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation](https://arxiv.org/abs/2601.00504)
*Miaowei Wang,Jakub Zadrożny,Oisin Mac Aodha,Amir Vaxman*

Main category: cs.CV

TL;DR: This paper introduces MotionPhysics, a framework that uses natural language to infer physical parameters for dynamic simulations without requiring direct ground-truth trajectories. It surpasses existing methods in generating visually realistic simulations for a variety of 3D objects and materials.


<details>
  <summary>Details</summary>
Motivation: Simulating accurate dynamic behaviors of 3D objects and materials often requires expertise and time-intensive physical parameter adjustments. The goal is to simplify this process by automating it through natural language prompts.

Method: MotionPhysics combines a multimodal large language model for estimating material parameters within plausible ranges and proposes a novel motion distillation loss that draws robust motion priors from video diffusion models, avoiding inductive biases.

Result: MotionPhysics was tested on over 30 scenarios involving various materials and objects. It successfully generated realistic simulations that align with natural language prompts while outperforming existing solutions.

Conclusion: MotionPhysics demonstrates how end-to-end differentiable frameworks, guided by natural language, can achieve visually realistic and physically plausible dynamic simulations across diverse scenarios.

Abstract: Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.

</details>


### [104] [All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations](https://arxiv.org/abs/2601.00533)
*Wenrui Li,Hongtao Chen,Yao Xiao,Wangmeng Zuo,Jiantao Zhou,Yonghong Tian,Xiaopeng Fan*

Main category: cs.CV

TL;DR: The paper introduces the Smoothly Evolving Unknown Degradations (SEUD) scenario in video restoration and proposes a novel ORCANet model to handle it effectively.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitation of existing approaches in video restoration, which focus on frame-wise degradation and neglect the natural temporal continuity in real-world degradation processes.

Method: The authors define the SEUD scenario and design a synthesis pipeline for temporally coherent video generation. They propose ORCANet, featuring a Coarse Intensity Estimation Dehazing (CIED) module and a Flow Prompt Generation (FPG) module to handle the specific challenges of SEUD.

Result: Experimental results show that ORCANet outperforms existing image and video-based baselines in restoration quality, temporal consistency, and robustness.

Conclusion: ORCANet is highly effective for handling temporally evolving degradations, offering a robust solution for video restoration in the SEUD scenario while improving discriminability and coherence.

Abstract: All-in-one image restoration aims to recover clean images from diverse unknown degradations using a single model. But extending this task to videos faces unique challenges. Existing approaches primarily focus on frame-wise degradation variation, overlooking the temporal continuity that naturally exists in real-world degradation processes. In practice, degradation types and intensities evolve smoothly over time, and multiple degradations may coexist or transition gradually. In this paper, we introduce the Smoothly Evolving Unknown Degradations (SEUD) scenario, where both the active degradation set and degradation intensity change continuously over time. To support this scenario, we design a flexible synthesis pipeline that generates temporally coherent videos with single, compound, and evolving degradations. To address the challenges in the SEUD scenario, we propose an all-in-One Recurrent Conditional and Adaptive prompting Network (ORCANet). First, a Coarse Intensity Estimation Dehazing (CIED) module estimates haze intensity using physical priors and provides coarse dehazed features as initialization. Second, a Flow Prompt Generation (FPG) module extracts degradation features. FPG generates both static prompts that capture segment-level degradation types and dynamic prompts that adapt to frame-level intensity variations. Furthermore, a label-aware supervision mechanism improves the discriminability of static prompt representations under different degradations. Extensive experiments show that ORCANet achieves superior restoration quality, temporal consistency, and robustness over image and video-based baselines. Code is available at https://github.com/Friskknight/ORCANet-SEUD.

</details>


### [105] [FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection](https://arxiv.org/abs/2601.00535)
*Ruiqiang Zhang,Hengyi Wang,Chang Liu,Guanjie Wang,Zehua Ma,Weiming Zhang*

Main category: cs.CV

TL;DR: FreeText is a training-free framework that enhances text rendering in T2I diffusion models, addressing issues with typography and scripts by improving localization and glyph injection.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models show limitations in precise text rendering, especially for detailed layouts, dense typography, and complex scripts, necessitating flexible yet effective solutions.

Method: FreeText leverages DiT models, decomposing the task into spatial region localization (where to write) using token-wise attention and glyph injection (what to write) using spectral modulation to improve text rendering.

Result: Experiments on various benchmarks show improved text readability, semantic alignment, and aesthetic quality with minimal inference overhead.

Conclusion: FreeText offers consistent and significant improvements in text rendering for T2I diffusion models without the need for retraining or rigid constraints, improving effectiveness and practicality.

Abstract: Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.

</details>


### [106] [Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios](https://arxiv.org/abs/2601.00537)
*Guangqian Guo,Pengfei Chen,Yong Guo,Huafeng Chen,Boqiang Zhang,Shan Gao*

Main category: cs.CV

TL;DR: The paper introduces VNS-SAM, an enhanced version of SAM, designed to improve segmentation in visually non-salient scenarios with low contrast between foreground and background.


<details>
  <summary>Details</summary>
Motivation: To address SAM's performance limitations in visually non-salient scenarios and improve its segmentation ability while maintaining zero-shot generalization.

Method: They developed VNS-SAM using two features: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module, coupled with a new VNS-SEG dataset for training.

Result: VNS-SAM showed superior segmentation performance, especially in challenging tasks with low visibility, and maintained efficiency with minimal parameter increments.

Conclusion: VNS-SAM effectively enhances SAM's segmentation capabilities in visually non-salient settings and demonstrates significant potential for practical, real-world applications.

Abstract: Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM's perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM's low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model's segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets are publicly available at https://guangqian-guo.github.io/VNS-SAM.

</details>


### [107] [DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction](https://arxiv.org/abs/2601.00542)
*Jiacheng Sui,Yujie Zhou,Li Niu*

Main category: cs.CV

TL;DR: The paper introduces DynaDrag, a novel image editing method using predictions to enhance pixel-level drag-style manipulation, promising superior results over previous approaches.


<details>
  <summary>Details</summary>
Motivation: Existing image editing methods face challenges like tracking errors and low editability due to gaps between source and target images. This motivates a new, more effective framework.

Method: The authors propose DynaDrag, which operates on a predict-and-move framework. It uses iterative Motion Prediction and Motion Supervision steps, dynamically adjusting handle points to improve performance.

Result: DynaDrag outperforms prior methods as demonstrated by experiments on face and human image datasets, addressing previous frameworks' shortcomings like tracking issues and editability.

Conclusion: DynaDrag introduces an innovative editing framework that improves accuracy and usability in pixel-level image manipulation, particularly for applications in face and human image editing.

Abstract: To achieve pixel-level image manipulation, drag-style image editing which edits images using points or trajectories as conditions is attracting widespread attention. Most previous methods follow move-and-track framework, in which miss tracking and ambiguous tracking are unavoidable challenging issues. Other methods under different frameworks suffer from various problems like the huge gap between source image and target edited image as well as unreasonable intermediate point which can lead to low editability. To avoid these problems, we propose DynaDrag, the first dragging method under predict-and-move framework. In DynaDrag, Motion Prediction and Motion Supervision are performed iteratively. In each iteration, Motion Prediction first predicts where the handle points should move, and then Motion Supervision drags them accordingly. We also propose to dynamically adjust the valid handle points to further improve the performance. Experiments on face and human datasets showcase the superiority over previous works.

</details>


### [108] [SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array](https://arxiv.org/abs/2601.00551)
*Shuang Li,Yibing Wang,Jian Gao,Chulhong Kim,Seongwook Choi,Yu Zhang,Qian Chen,Yao Yao,Changhui Li*

Main category: cs.CV

TL;DR: This paper presents SlingBAG Pro, an improved algorithm for 3D photoacoustic imaging with irregular transducer arrays, delivering faster and more efficient reconstruction.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of traditional reconstruction algorithms in handling irregular transducer arrays for 3D photoacoustic imaging, which are computationally expensive and time-consuming.

Method: The authors developed SlingBAG Pro, leveraging hierarchical optimization strategies, including zero-gradient filtering and progressive temporal sampling, to enhance the existing Sliding ball adaptive growth (SlingBAG) method, optimizing reconstruction for irregular array geometries.

Result: SlingBAG Pro outperforms its predecessor by achieving a 2.2-fold speed improvement in reconstruction time under irregular array configurations, as validated through simulations and in vivo mouse experiments.

Conclusion: By offering enhanced reconstruction speed and quality for 3D photoacoustic imaging with irregular sensor arrays, SlingBAG Pro could significantly benefit clinical imaging applications. The method's code is publicly available for further research and development.

Abstract: High-quality three-dimensional (3D) photoacoustic imaging (PAI) is gaining increasing attention in clinical applications. To address the challenges of limited space and high costs, irregular geometric transducer arrays that conform to specific imaging regions are promising for achieving high-quality 3D PAI with fewer transducers. However, traditional iterative reconstruction algorithms struggle with irregular array configurations, suffering from high computational complexity, substantial memory requirements, and lengthy reconstruction times. In this work, we introduce SlingBAG Pro, an advanced reconstruction algorithm based on the point cloud iteration concept of the Sliding ball adaptive growth (SlingBAG) method, while extending its compatibility to arbitrary array geometries. SlingBAG Pro maintains high reconstruction quality, reduces the number of required transducers, and employs a hierarchical optimization strategy that combines zero-gradient filtering with progressively increased temporal sampling rates during iteration. This strategy rapidly removes redundant spatial point clouds, accelerates convergence, and significantly shortens overall reconstruction time. Compared to the original SlingBAG algorithm, SlingBAG Pro achieves up to a 2.2-fold speed improvement in point cloud-based 3D PA reconstruction under irregular array geometries. The proposed method is validated through both simulation and in vivo mouse experiments, and the source code is publicly available at https://github.com/JaegerCQ/SlingBAG_Pro.

</details>


### [109] [A Comprehensive Dataset for Human vs. AI Generated Image Detection](https://arxiv.org/abs/2601.00553)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Vasu Sharma,Vinija Jain,Aman Chadha,Aishwarya Naresh Reganti,Amitava Das*

Main category: cs.CV

TL;DR: The paper introduces MS COCOAI, a dataset with real and AI-generated images, to address the challenges of detecting synthetic images due to advances in generative AI systems like Stable Diffusion, DALL-E, and MidJourney.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the growing issue of indistinguishable synthetic images created by advanced AI tools, which can lead to misleading content and misinformation.

Method: The authors created MS COCOAI, a dataset with 96,000 real and synthetic images, generated using MS COCO and five image generators (Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6). They propose two tasks: real vs. generated classification and model attribution for synthetic images.

Result: The result is a comprehensive publicly available dataset designed to aid the detection and analysis of AI-generated images, with tasks enabling researchers to develop improved detection models.

Conclusion: The study provides valuable resources and benchmarks to advance methods for differentiating real images from synthetic ones and attributing them to specific generative models.

Abstract: Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at https://huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset.

</details>


### [110] [AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models](https://arxiv.org/abs/2601.00561)
*Jintao Lin,Bowen Dong,Weikang Shi,Chenyang Lei,Suiyun Zhang,Rui Liu,Xihui Liu*

Main category: cs.CV

TL;DR: This paper introduces AEGIS, a new comprehensive benchmark for assessing Unified Multimodal Models (UMMs) across tasks requiring world knowledge and reasoning. It highlights existing deficits in current UMMs' reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack comprehensive evaluation methods for assessing UMMs' ability to apply world knowledge across various tasks, limiting diagnostic power.

Method: The paper introduces AEGIS, a multi-task benchmark with 1050 manually-annotated questions across varied topics and reasoning types, alongside Deterministic Checklist-based Evaluation (DCE) for more reliable scoring.

Result: Experiments reveal that UMMs struggle with complex reasoning tasks and world knowledge application but show potential improvements with plug-in reasoning modules.

Conclusion: AEGIS establishes the importance of world-knowledge-based reasoning for advancing UMMs and provides a reliable framework for evaluation and diagnosis of their capabilities.

Abstract: The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N'' judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.

</details>


### [111] [A Cascaded Information Interaction Network for Precise Image Segmentation](https://arxiv.org/abs/2601.00562)
*Hewen Xiao,Jie Mei,Guangfu Ma,Weiren Wu*

Main category: cs.CV

TL;DR: The paper proposes a novel cascaded CNN with a Global Information Guidance Module for improved image segmentation in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: To address challenges in robust segmentation within visually complex scenarios, particularly relevant for autonomous applications.

Method: A cascaded convolutional neural network integrated with a Global Information Guidance Module to fuse texture and semantic features across multiple layers.

Result: The framework achieves superior segmentation accuracy, exceeding state-of-the-art methods on benchmark datasets, especially in cluttered or blurred environments.

Conclusion: This innovative approach demonstrates high precision and shows promising potential for real-world robotic applications.

Abstract: Visual perception plays a pivotal role in enabling autonomous behavior, offering a cost-effective and efficient alternative to complex multi-sensor systems. However, robust segmentation remains a challenge in complex scenarios. To address this, this paper proposes a cascaded convolutional neural network integrated with a novel Global Information Guidance Module. This module is designed to effectively fuse low-level texture details with high-level semantic features across multiple layers, thereby overcoming the inherent limitations of single-scale feature extraction. This architectural innovation significantly enhances segmentation accuracy, particularly in visually cluttered or blurred environments where traditional methods often fail. Experimental evaluations on benchmark image segmentation datasets demonstrate that the proposed framework achieves superior precision, outperforming existing state-of-the-art methods. The results highlight the effectiveness of the approach and its promising potential for deployment in practical robotic applications.

</details>


### [112] [GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval](https://arxiv.org/abs/2601.00584)
*Mingyu Jeon,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: The paper introduces GranAlign, a training-free framework for zero-shot video moment retrieval that resolves semantic mismatch issues between text queries and video content, improving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address semantic granularity mismatches between textual queries and visual content in zero-shot video moment retrieval tasks due to limitations in current alignment methods.

Method: GranAlign employs granularity-based query rewriting to introduce varied semantic levels and query-aware caption generation to embed query intent into video captions, ensuring better alignment between modalities without task-specific training.

Result: GranAlign achieves new state-of-the-art results across benchmarks, including a 3.23% improvement in mAP@avg on the QVHighlights dataset.

Conclusion: The proposed GranAlign framework effectively resolves semantic mismatches, demonstrating improved zero-shot video moment retrieval performance and applicability for real-world tasks.

Abstract: Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.

</details>


### [113] [SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation](https://arxiv.org/abs/2601.00590)
*Yiling Wang,Zeyu Zhang,Yiran Wang,Hao Tang*

Main category: cs.CV

TL;DR: The paper introduces SafeMo, a framework for safe text-to-motion generation addressing flaws in existing methods such as quantization loss and unsafe data. It uses a two-stage machine unlearning strategy (MMU) and a new safe dataset (SafeMoVAE-29K) to improve safety and natural motion generation.


<details>
  <summary>Details</summary>
Motivation: Current Text-to-Motion (T2M) methods struggle with generating safe motions due to issues in codebook replacement approaches, including degradation in benign tasks and motion artifacts, and the presence of unsafe elements in datasets.

Method: The researchers propose SafeMo, which integrates Minimal Motion Unlearning (MMU) in a two-stage process, facilitating the generation of safe motions in continuous space. Additionally, they introduce a new dataset, SafeMoVAE-29K, containing rewritten safe text prompts and refined continuous motions.

Result: SafeMo achieves significantly improved unlearning of unsafe prompts with higher forget-set FID scores compared to previous state-of-the-art methods. Its benign performance on safe prompts is strong and rivals existing approaches, enabling trustworthy and natural motion transitions.

Conclusion: SafeMo provides a robust solution to safety concerns in text-to-motion generation by eliminating unsafe behaviors while maintaining high-quality benign motion generation, outperforming baseline methods effectively.

Abstract: Text-to-motion (T2M) generation with diffusion backbones achieves strong realism and alignment. Safety concerns in T2M methods have been raised in recent years; existing methods replace discrete VQ-VAE codebook entries to steer the model away from unsafe behaviors. However, discrete codebook replacement-based methods have two critical flaws: firstly, replacing codebook entries which are reused by benign prompts leads to drifts on everyday tasks, degrading the model's benign performance; secondly, discrete token-based methods introduce quantization and smoothness loss, resulting in artifacts and jerky transitions. Moreover, existing text-to-motion datasets naturally contain unsafe intents and corresponding motions, making them unsuitable for safety-driven machine learning. To address these challenges, we propose SafeMo, a trustworthy motion generative framework integrating Minimal Motion Unlearning (MMU), a two-stage machine unlearning strategy, enabling safe human motion generation in continuous space, preserving continuous kinematics without codebook loss and delivering strong safety-utility trade-offs compared to current baselines. Additionally, we present the first safe text-to-motion dataset SafeMoVAE-29K integrating rewritten safe text prompts and continuous refined motion for trustworthy human motion unlearning. Built upon DiP, SafeMo efficiently generates safe human motions with natural transitions. Experiments demonstrate effective unlearning performance of SafeMo by showing strengthened forgetting on unsafe prompts, reaching 2.5x and 14.4x higher forget-set FID on HumanML3D and Motion-X respectively, compared to the previous SOTA human motion unlearning method LCR, with benign performance on safe prompts being better or comparable. Code: https://github.com/AIGeeksGroup/SafeMo. Website: https://aigeeksgroup.github.io/SafeMo.

</details>


### [114] [Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception](https://arxiv.org/abs/2601.00598)
*Xianhui Liu,Siqi Jiang,Yi Xie,Yuqing Lin,Siao Liu*

Main category: cs.CV

TL;DR: The paper addresses optimization issues in RGB-Infrared (RGB-IR) multimodal perception by introducing a Modality Dominance Index (MDI) and proposing the Modality Dominance-Aware Cross-modal Learning (MDACL) framework.


<details>
  <summary>Details</summary>
Motivation: Multimodal RGB-IR perception faces challenges due to asymmetric modality characteristics, causing optimization bias and limiting effective fusion.

Method: The authors propose the MDACL framework, which uses the Modality Dominance Index (MDI) to assess modality dominance. It incorporates methods like Hierarchical Cross-modal Guidance (HCG) for feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization.

Result: The proposed MDACL framework mitigates optimization biases and achieves state-of-the-art performance in experiments conducted on three RGB-IR benchmarks.

Conclusion: The MDACL framework significantly enhances RGB-IR multimodal perception by addressing optimization dynamics and improving cross-modal fusion.

Abstract: RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.

</details>


### [115] [Noise-Robust Tiny Object Localization with Flows](https://arxiv.org/abs/2601.00617)
*Huixin Sun,Linlin Yang,Ronyu Chen,Kerui Gu,Baochang Zhang,Angela Yao,Xianbin Cao*

Main category: cs.CV

TL;DR: This paper addresses the issue of poor tiny object detection due to annotation noise by proposing a noise-robust localization framework, TOLF, which uses normalizing flows for error modeling and uncertainty-guided optimization.


<details>
  <summary>Details</summary>
Motivation: Tiny objects are highly sensitive to annotation noise, adversely affecting detection performance compared to normal-scale objects.

Method: The proposed TOLF framework utilizes normalizing flows for flexible error modeling and introduces uncertainty-aware gradient modulation to suppress learning from high-uncertainty samples.

Result: Extensive experiments demonstrate improved performance, with TOLF achieving a 1.2% AP boost over the DINO baseline on the AI-TOD dataset.

Conclusion: TOLF effectively mitigates noise-induced overfitting and stabilizes training, establishing a robust solution for tiny object detection in challenging scenarios.

Abstract: Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach's effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.

</details>


### [116] [RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation](https://arxiv.org/abs/2601.00625)
*Junxiao Xue,Pavel Smirnov,Ziao Li,Yunyun Shi,Shi Chen,Xinyi Yin,Xiaohan Yue,Lei Wang,Yiduo Wang,Feng Lin,Yijia Chen,Xiao Ma,Xiaoran Yan,Qing Zhang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: RePose is a method for real-time 3D human pose estimation and motion analysis, aimed at improving rehabilitation training.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide real-time feedback and monitoring for patients in rehabilitation, ensuring proper execution of exercises to restore muscle strength and motor functions.

Method: The method includes a unified pipeline for end-to-end pose estimation using RGB video, a fast tracking method for handling multiple-person interference, and modified SmoothNet for reducing pose estimation errors, coupled with Unity for visualization.

Result: RePose delivers fast tracking (less than 1ms per frame), reduced pose estimation errors, restored true motion states, and real-time monitoring and feedback using Unity.

Conclusion: The paper concludes that RePose improves rehabilitation training effectiveness by aiding patients in correct motion execution and visualizing muscle stress conditions.

Abstract: We propose a real-time 3D human pose estimation and motion analysis method termed RePose for rehabilitation training. It is capable of real-time monitoring and evaluation of patients'motion during rehabilitation, providing immediate feedback and guidance to assist patients in executing rehabilitation exercises correctly. Firstly, we introduce a unified pipeline for end-to-end real-time human pose estimation and motion analysis using RGB video input from multiple cameras which can be applied to the field of rehabilitation training. The pipeline can help to monitor and correct patients'actions, thus aiding them in regaining muscle strength and motor functions. Secondly, we propose a fast tracking method for medical rehabilitation scenarios with multiple-person interference, which requires less than 1ms for tracking for a single frame. Additionally, we modify SmoothNet for real-time posture estimation, effectively reducing pose estimation errors and restoring the patient's true motion state, making it visually smoother. Finally, we use Unity platform for real-time monitoring and evaluation of patients' motion during rehabilitation, and to display the muscle stress conditions to assist patients with their rehabilitation training.

</details>


### [117] [HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis](https://arxiv.org/abs/2601.00626)
*Shuren Gabriel Yu,Sikang Ren,Yongji Tian*

Main category: cs.CV

TL;DR: The paper introduces HyperPriv-EPN, a hypergraph-based framework to improve preoperative ependymoma prognosis by leveraging privileged post-operative data through dual-stream distillation.


<details>
  <summary>Details</summary>
Motivation: Preoperative prognosis of ependymoma is essential for treatment planning but is hindered by the lack of semantic insights in preoperative MRI data compared to rich post-operative surgical reports.

Method: HyperPriv-EPN uses a Severed Graph Strategy within a Learning Using Privileged Information (LUPI) framework. A shared encoder processes both a Teacher graph (with post-operative data) and a Student graph (with preoperative data) to train the student for semantic prediction during inference.

Result: Validated on 311 patients, the framework achieves state-of-the-art performance in diagnostic accuracy and survival stratification, proving its efficacy in leveraging historical post-operative data.

Conclusion: HyperPriv-EPN successfully transfers expert knowledge from post-operative settings to preoperative diagnosis, enabling informed treatment planning without requiring privileged text data during inference.

Abstract: Preoperative prognosis of Ependymoma is critical for treatment planning but challenging due to the lack of semantic insights in MRI compared to post-operative surgical reports. Existing multimodal methods fail to leverage this privileged text data when it is unavailable during inference. To bridge this gap, we propose HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information (LUPI) framework. We introduce a Severed Graph Strategy, utilizing a shared encoder to process both a Teacher graph (enriched with privileged post-surgery information) and a Student graph (restricted to pre-operation data). Through dual-stream distillation, the Student learns to hallucinate semantic community structures from visual features alone. Validated on a multi-center cohort of 311 patients, HyperPriv-EPN achieves state-of-the-art diagnostic accuracy and survival stratification. This effectively transfers expert knowledge to the preoperative setting, unlocking the value of historical post-operative data to guide the diagnosis of new patients without requiring text at inference.

</details>


### [118] [Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach](https://arxiv.org/abs/2601.00645)
*Shrikant Kapse,Priyankkumar Dhrangdhariya,Priya Kedia,Manasi Patwardhan,Shankar Kausley,Soumyadipta Maiti,Beena Rai,Shirish Karande*

Main category: cs.CV

TL;DR: This paper investigates the potential of image-based deep learning using models like DenseNet to monitor potato quality during storage, focusing on sprout detection, weight loss estimation, and shelf-life prediction.


<details>
  <summary>Details</summary>
Motivation: To find a non-invasive and scalable method to monitor potato quality during storage, enhancing inventory management while reducing food waste.

Method: The study collected images and weight data over 200 days, used pre-trained models such as ResNet, VGG, DenseNet, and ViT, and designed specialized models for binary sprout detection and multi-class predictions for weight loss and shelf-life.

Result: DenseNet achieved 98.03% accuracy in sprout detection. Shelf-life prediction models attained over 89.83% accuracy with coarse class divisions, but performance dropped for finer classes due to limited data and subtle visual differences.

Conclusion: This approach supports sustainable potato storage by integrating image-based quality assessment into automated systems, emphasizing broader classifications for robust predictions while suggesting future improvements for scalability.

Abstract: Image-based deep learning provides a non-invasive, scalable solution for monitoring potato quality during storage, addressing key challenges such as sprout detection, weight loss estimation, and shelf-life prediction. In this study, images and corresponding weight data were collected over a 200-day period under controlled temperature and humidity conditions. Leveraging powerful pre-trained architectures of ResNet, VGG, DenseNet, and Vision Transformer (ViT), we designed two specialized models: (1) a high-precision binary classifier for sprout detection, and (2) an advanced multi-class predictor to estimate weight loss and forecast remaining shelf-life with remarkable accuracy. DenseNet achieved exceptional performance, with 98.03% accuracy in sprout detection. Shelf-life prediction models performed best with coarse class divisions (2-5 classes), achieving over 89.83% accuracy, while accuracy declined for finer divisions (6-8 classes) due to subtle visual differences and limited data per class. These findings demonstrate the feasibility of integrating image-based models into automated sorting and inventory systems, enabling early identification of sprouted potatoes and dynamic categorization based on storage stage. Practical implications include improved inventory management, differential pricing strategies, and reduced food waste across supply chains. While predicting exact shelf-life intervals remains challenging, focusing on broader class divisions ensures robust performance. Future research should aim to develop generalized models trained on diverse potato varieties and storage conditions to enhance adaptability and scalability. Overall, this approach offers a cost-effective, non-destructive method for quality assessment, supporting efficiency and sustainability in potato storage and distribution.

</details>


### [119] [Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network](https://arxiv.org/abs/2601.00658)
*Zhaiyu Chen,Yuanyuan Wang,Yilei Shi,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: The paper introduces a learning-based framework for converting raw TomoSAR point clouds into high-resolution building height maps, overcoming challenges like noise and data voids.


<details>
  <summary>Details</summary>
Motivation: To improve building height estimation for urban applications by addressing the noise, anisotropic distributions, and missing data in TomoSAR point clouds.

Method: The method employs a dual-topology network combining a point branch for irregular scatterer features and a grid branch for spatial consistency to denoise and inpaint height maps.

Result: The approach effectively produces continuous building height maps from TomoSAR data, validated by experiments on Munich and Berlin, and can be enhanced with optical imagery.

Conclusion: This framework offers a novel and reliable solution for large-scale urban height mapping directly from TomoSAR point clouds, with a publicly available source code.

Abstract: Reliable building height estimation is essential for various urban applications. Spaceborne SAR tomography (TomoSAR) provides weather-independent, side-looking observations that capture facade-level structure, offering a promising alternative to conventional optical methods. However, TomoSAR point clouds often suffer from noise, anisotropic point distributions, and data voids on incoherent surfaces, all of which hinder accurate height reconstruction. To address these challenges, we introduce a learning-based framework for converting raw TomoSAR points into high-resolution building height maps. Our dual-topology network alternates between a point branch that models irregular scatterer features and a grid branch that enforces spatial consistency. By jointly processing these representations, the network denoises the input points and inpaints missing regions to produce continuous height estimates. To our knowledge, this is the first proof of concept for large-scale urban height mapping directly from TomoSAR point clouds. Extensive experiments on data from Munich and Berlin validate the effectiveness of our approach. Moreover, we demonstrate that our framework can be extended to incorporate optical satellite imagery, further enhancing reconstruction quality. The source code is available at https://github.com/zhu-xlab/tomosar2height.

</details>


### [120] [CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models](https://arxiv.org/abs/2601.00659)
*Neeraj Anand,Samyak Jha,Udbhav Bamba,Rahul Rahaman*

Main category: cs.CV

TL;DR: CRoPS is a training-free framework that mitigates hallucinations in Large Vision-Language Models (LVLMs), achieving significant improvements and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of hallucinated content generated by LVLMs, reducing the lack of reliability in real-world applications.

Method: The proposed method, CRoPS, uses Generalized Contrastive Decoding by selectively removing key tokens and integrating multiple hallucinated models to mitigate hallucination effects.

Result: CRoPS improves CHAIR scores by 20% and shows consistent performance gains across six benchmarks and three LVLM families.

Conclusion: CRoPS effectively mitigates hallucination effects in LVLMs, advancing reliability and outperforming existing training-free methods.

Abstract: Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.

</details>


### [121] [Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians](https://arxiv.org/abs/2601.00678)
*Melonie de Almeida,Daniela Ivanova,Tong Shi,John H. Williamson,Paul Henderson*

Main category: cs.CV

TL;DR: The paper introduces a new method for single-image-conditioned video generation using a 3D Gaussian scene representation to achieve realistic object motion and fast video generation.


<details>
  <summary>Details</summary>
Motivation: Current single-image-conditioned video generation models lack robust user controllability, especially in modifying camera paths, and struggle with temporal consistency and geometric integrity.

Method: The proposed framework uses 3D Gaussian scene representation to sample plausible object motion and perform video generation in a single forward pass, avoiding iterative denoising.

Result: The approach achieved state-of-the-art video quality and inference efficiency, validated across the KITTI, Waymo, RealEstate10K, and DL3DV-10K datasets.

Conclusion: The method enables fast, coherent, camera-guided video generation, addressing challenges in temporal consistency and geometric integrity while allowing precise camera control.

Abstract: Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.

</details>


### [122] [Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks](https://arxiv.org/abs/2601.00703)
*Cory Fan,Wenchao Zhang*

Main category: cs.CV

TL;DR: This paper discusses improving efficiency in image demosaicing networks by introducing spatial downsampling to isotropic networks, resulting in better computational performance and effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in isotropic networks for image demosaicing and joint-demosaicing-and-denoising (JDD), which are computationally expensive for mobile applications.

Method: The authors use a mathematical network design approach adapted from DeepMAD to compare isotropic networks with and without spatial downsampling, and develop JD3Net with significant downsampling.

Result: The downsampled variant, JD3Net, demonstrates improved efficiency and strong performance on various image demosaicing and JDD tasks, validating the proposed approach.

Conclusion: Spatial downsampling in isotropic networks enhances both computational efficiency and empirical performance, making such networks suitable for mobile imaging applications.

Abstract: In digital imaging, image demosaicing is a crucial first step which recovers the RGB information from a color filter array (CFA). Oftentimes, deep learning is utilized to perform image demosaicing. Given that most modern digital imaging applications occur on mobile platforms, applying deep learning to demosaicing requires lightweight and efficient networks. Isotropic networks, also known as residual-in-residual networks, have been often employed for image demosaicing and joint-demosaicing-and-denoising (JDD). Most demosaicing isotropic networks avoid spatial downsampling entirely, and thus are often prohibitively expensive computationally for mobile applications. Contrary to previous isotropic network designs, this paper claims that spatial downsampling to a signficant degree can improve the efficiency and performance of isotropic networks. To validate this claim, we design simple fully convolutional networks with and without downsampling using a mathematical architecture design technique adapted from DeepMAD, and find that downsampling improves empirical performance. Additionally, empirical testing of the downsampled variant, JD3Net, of our fully convolutional networks reveals strong empirical performance on a variety of image demosaicing and JDD tasks.

</details>


### [123] [Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model](https://arxiv.org/abs/2601.00716)
*Hao Guan,Li Zhou*

Main category: cs.CV

TL;DR: The paper explores tools and methods to detect performance degradation in Vision-Language Models (VLMs) used in pathology when faced with data distribution shifts. It proposes a framework using input data shifts and output confidence-based indicators.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure clinical reliability for VLMs in pathology by detecting performance degradation due to input data distribution changes after deployment.

Method: The study examines input data shifts using a developed toolbox called DomainSAT and proposes a confidence-based degradation indicator for output-level monitoring.

Result: Combining input and output monitoring effectively detects performance degradation in large pathology datasets under data shifts, offering improved reliability and interpretation.

Conclusion: The research presents a complementary approach using input and output monitoring, providing a practical framework for ensuring the reliability of VLMs in digital pathology.

Abstract: Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.

</details>


### [124] [Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection](https://arxiv.org/abs/2601.00725)
*Johannes C. Bauer,Paul Geng,Stephan Trattnig,Petr Dokládal,Rüdiger Daub*

Main category: cs.CV

TL;DR: The paper introduces a multi-level feature fusion (MLFF) approach to enhance neural network adaptation in volatile manufacturing scenarios, reducing catastrophic forgetting and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of deep neural networks in continually adapting to novel and volatile manufacturing scenarios, such as remanufacturing, where products and defect patterns frequently change.

Method: The study proposes a MLFF approach that utilizes representations from various depths of a pretrained network to improve computational efficiency, achieve effective adaptation, and reduce catastrophic forgetting.

Result: The MLFF approach matches the performance of end-to-end training with fewer trainable parameters, reduces catastrophic forgetting, and enhances generalization to new product types and defect patterns.

Conclusion: The MLFF method proves to be a computationally efficient solution for quality inspection tasks with improved performance, robust adaptability, and minimal forgetting, making it suitable for volatile remanufacturing contexts.

Abstract: Deep neural networks show great potential for automating various visual quality inspection tasks in manufacturing. However, their applicability is limited in more volatile scenarios, such as remanufacturing, where the inspected products and defect patterns often change. In such settings, deployed models require frequent adaptation to novel conditions, effectively posing a continual learning problem. To enable quick adaptation, the necessary training processes must be computationally efficient while still avoiding effects like catastrophic forgetting. This work presents a multi-level feature fusion (MLFF) approach that aims to improve both aspects simultaneously by utilizing representations from different depths of a pretrained network. We show that our approach is able to match the performance of end-to-end training for different quality inspection problems while using significantly less trainable parameters. Furthermore, it reduces catastrophic forgetting and improves generalization robustness to new product types or defects.

</details>


### [125] [Grading Handwritten Engineering Exams with Multimodal Large Language Models](https://arxiv.org/abs/2601.00730)
*Janez Perš,Jon Muhovič,Andrej Košir,Boštjan Murovec*

Main category: cs.CV

TL;DR: This paper introduces a workflow using multimodal LLMs for grading scanned handwritten STEM exams, achieving reliable grading and reducing manual effort.


<details>
  <summary>Details</summary>
Motivation: The traditional manual grading of handwritten STEM exams is slow and challenging to scale, prompting the development of an automated, efficient grading solution.

Method: The paper employs multimodal LLMs, combining reference solutions, grading rule summaries, ensembles of graders, aggregation strategies, and validation processes within a structured pipeline.

Result: The workflow demonstrated an $$8-point deviation compared to lecturer grades with low bias, alongside a 17% review trigger rate in trials on Slovenian handwritten quizzes.

Conclusion: Structured grading with LLMs preserves exam authenticity, scales efficiently, and ensures reliable outputs, validated through ablation studies confirming its accuracy and necessity of grounding prompts and reference solutions.

Abstract: Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\approx$17% at $D_{\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.

</details>


### [126] [Unified Primitive Proxies for Structured Shape Completion](https://arxiv.org/abs/2601.00759)
*Zhaiyu Chen,Yuqing Wang,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: UniCo introduces a novel approach to structured shape completion in 3D geometry using primitives rather than points, significantly improving performance over prior methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of cascade-based methods in structured shape completion and improve the prediction of primitives with complete geometry, semantics, and inlier membership.

Method: UniCo uses a single feed-forward network to predict unified primitives via learnable primitive proxies contextualized for assembly and a training strategy that couples primitives and points with online target updates.

Result: UniCo outperforms recent baselines in structured 3D understanding across benchmarks, reducing Chamfer distance by up to 50% and enhancing normal consistency by up to 7%.

Conclusion: UniCo provides a more effective and consistent approach for recovering structured 3D shapes from incomplete data, establishing a new benchmark for primitive-based surface reconstruction.

Abstract: Structured shape completion recovers missing geometry as primitives rather than as unstructured points, which enables primitive-based surface reconstruction. Instead of following the prevailing cascade, we rethink how primitives and points should interact, and find it more effective to decode primitives in a dedicated pathway that attends to shared shape features. Following this principle, we present UniCo, which in a single feed-forward pass predicts a set of primitives with complete geometry, semantics, and inlier membership. To drive this unified representation, we introduce primitive proxies, learnable queries that are contextualized to produce assembly-ready outputs. To ensure consistent optimization, our training strategy couples primitives and points with online target updates. Across synthetic and real-world benchmarks with four independent assembly solvers, UniCo consistently outperforms recent baselines, lowering Chamfer distance by up to 50% and improving normal consistency by up to 7%. These results establish an attractive recipe for structured 3D understanding from incomplete data. Project page: https://unico-completion.github.io.

</details>


### [127] [Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection](https://arxiv.org/abs/2601.00789)
*Shukesh Reddy,Srijan Das,Abhijit Das*

Main category: cs.CV

TL;DR: This paper explores the use of self-supervised learning as an auxiliary task to improve deepfake detection and finds that combining feature representations enhances performance across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance deepfake detection by leveraging self-supervised learning as an auxiliary task to improve generalization and performance.

Method: The authors experimented with different training schemes, combining feature representations from self-supervised auxiliary tasks with those from the primary deepfake detection task.

Result: Their approach showed improved cross-dataset generalizability and better performance compared with current state-of-the-art detectors, validated across multiple datasets.

Conclusion: Combining self-supervised learning with deepfake detection tasks leads to uniquely fused feature representations, achieving significant improvement in detection performance.

Abstract: In this work, we attempted to unleash the potential of self-supervised learning as an auxiliary task that can optimise the primary task of generalised deepfake detection. To explore this, we examined different combinations of the training schemes for these tasks that can be most effective. Our findings reveal that fusing the feature representation from self-supervised auxiliary tasks is a powerful feature representation for the problem at hand. Such a representation can leverage the ultimate potential and bring in a unique representation of both the self-supervised and primary tasks, achieving better performance for the primary task. We experimented on a large set of datasets, which includes DF40, FaceForensics++, Celeb-DF, DFD, FaceShifter, UADFV, and our results showed better generalizability on cross-dataset evaluation when compared with current state-of-the-art detectors.

</details>


### [128] [Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI](https://arxiv.org/abs/2601.00794)
*Wenhui Chu,Nikolaos V. Tsekos*

Main category: cs.CV

TL;DR: This paper introduces LNU-Net and IBU-Net, two new deep learning architectures for accurate left ventricle segmentation in cine MRI images, outperforming existing techniques in evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Accurate left ventricle segmentation is essential for diagnosing and quantifying cardiac health, motivating development of advanced deep learning segmentation approaches.

Method: Introduced LNU-Net and IBU-Net architectures with down-sampling feature extraction and up-sampling for precise localization, utilizing normalization techniques like layer normalization and instance-batch normalization, alongside data augmentations.

Result: The proposed architectures achieved better dice coefficient and average perpendicular distance metrics compared to state-of-the-art segmentation methods.

Conclusion: LNU-Net and IBU-Net demonstrated improved performance in left ventricle segmentation, offering valuable tools for cardiac medical imaging analysis.

Abstract: Left ventricle (LV) segmentation is critical for clinical quantification and diagnosis of cardiac images. In this work, we propose two novel deep learning architectures called LNU-Net and IBU-Net for left ventricle segmentation from short-axis cine MRI images. LNU-Net is derived from layer normalization (LN) U-Net architecture, while IBU-Net is derived from the instance-batch normalized (IB) U-Net for medical image segmentation. The architectures of LNU-Net and IBU-Net have a down-sampling path for feature extraction and an up-sampling path for precise localization. We use the original U-Net as the basic segmentation approach and compared it with our proposed architectures. Both LNU-Net and IBU-Net have left ventricle segmentation methods: LNU-Net applies layer normalization in each convolutional block, while IBU-Net incorporates instance and batch normalization together in the first convolutional block and passes its result to the next layer. Our method incorporates affine transformations and elastic deformations for image data processing. Our dataset that contains 805 MRI images regarding the left ventricle from 45 patients is used for evaluation. We experimentally evaluate the results of the proposed approaches outperforming the dice coefficient and the average perpendicular distance than other state-of-the-art approaches.

</details>


### [129] [AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction](https://arxiv.org/abs/2601.00796)
*Jiewen Chan,Zhenjun Zhao,Yu-Lun Liu*

Main category: cs.CV

TL;DR: This paper introduces AdaGaR, a framework for modeling dynamic 3D scenes from monocular videos with high-detail capture and temporal motion continuity, using Adaptive Gabor Representation and Temporal Curvature Regularization techniques.


<details>
  <summary>Details</summary>
Motivation: The need to reconstruct high-detail, temporally continuous 3D scenes from monocular videos has been limited by the low-frequency nature of Gaussian primitives and energy instability of standard Gabor functions. Additionally, the lack of temporal continuity constraints often causes motion artifacts.

Method: AdaGaR uses Adaptive Gabor Representation to extend Gaussian primitives with learnable frequency weights and energy compensation. For ensuring temporal continuity, it uses Cubic Hermite Splines with Temporal Curvature Regularization. Additionally, Adaptive Initialization integrates depth estimation, point tracking, and foreground masks to stabilize early visual structures.

Result: AdaGaR achieves state-of-the-art performance on Tap-Vid DAVIS with metrics such as PSNR (35.49), SSIM (0.9433), and LPIPS (0.0723), showing superior results in frame interpolation, depth consistency, and other 3D video tasks.

Conclusion: The proposed AdaGaR framework effectively balances high-frequency appearance details and smooth temporal motion, offering new benchmarks in dynamic 3D scene reconstruction and generalization on various video tasks.

Abstract: Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [130] [Word Frequency Counting Based on Serverless MapReduce](https://arxiv.org/abs/2601.00380)
*Hanzhe Li,Bingchen Lin,Mengyuan Xu*

Main category: cs.DC

TL;DR: This paper investigates optimizing the performance of MapReduce programming within serverless computing to enhance efficiency and reduce execution time.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the demand for high-performance computing by combining the robustness of MapReduce with serverless computing to improve efficiency in tasks like word frequency counting.

Method: The paper uses a MapReduce programming model on a serverless computing platform to identify the optimal number of Map and Reduce functions, conducting various experiments.

Result: Experiments demonstrated that as the number of Map and Reduce functions increases, execution time decreases while efficiency improves. Optimal function numbers were identified for task performance.

Conclusion: The discovery of the most optimized numbers of Map and Reduce functions contributes to improved computing solutions for programmers and organizations.

Abstract: With the increasing demand for high-performance and high-efficiency computing, cloud computing, especially serverless computing, has gradually become a research hotspot in recent years, attracting numerous research attention. Meanwhile, MapReduce, which is a popular big data processing model in the industry, has been widely applied in various fields. Inspired by the serverless framework of Function as a Service and the high concurrency and robustness of MapReduce programming model, this paper focus on combining them to reduce the time span and increase the efficiency when executing the word frequency counting task. In this case, the paper use a MapReduce programming model based on a serverless computing platform to figure out the most optimized number of Map functions and Reduce functions for a particular task. For the same amount of workload, extensive experiments show both execution time reduces and the overall efficiency of the program improves at different rates as the number of map functions and reduce functions increases. This paper suppose the discovery of the most optimized number of map and reduce functions can help cooperations and programmers figure out the most optimized solutions.

</details>


### [131] [Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving](https://arxiv.org/abs/2601.00397)
*Amey Agrawal,Mayank Yadav,Sukrit Kumar,Anirudha Agrawal,Garv Ghai,Souradeep Bera,Elton Pinto,Sirish Gambhira,Mohammad Adain,Kasra Sohrab,Chus Antonanzas,Alexey Tumanov*

Main category: cs.DC

TL;DR: Revati is a time-warp emulator for LLM serving systems, enabling performance modeling by simulating GPU operations, running much faster than real GPU execution.


<details>
  <summary>Details</summary>
Motivation: Testing LLMs' serving configurations is costly and time-intensive in GPU clusters, and current simulators require re-implementing serving logic, which is burdensome as frameworks evolve.

Method: Revati directly executes real serving system code by intercepting CUDA API calls to virtualize GPU operations and synchronize time jumps, avoiding physical GPU usage while preserving causality.

Result: Revati predicts performance with less than 5% error and simulates 5-17x faster than actual GPU execution across various models and configurations.

Conclusion: Revati provides an efficient alternative for evaluating LLM serving systems, significantly reducing time and cost while maintaining high accuracy.

Abstract: Deploying LLMs efficiently requires testing hundreds of serving configurations, but evaluating each one on a GPU cluster takes hours and costs thousands of dollars. Discrete-event simulators are faster and cheaper, but they require re-implementing the serving system's control logic -- a burden that compounds as frameworks evolve.
  We present Revati, a time-warp emulator that enables performance modeling by directly executing real serving system code at simulation-like speed. The system intercepts CUDA API calls to virtualize device management, allowing serving frameworks to run without physical GPUs. Instead of executing GPU kernels, it performs time jumps -- fast-forwarding virtual time by predicted kernel durations. We propose a coordination protocol that synchronizes these jumps across distributed processes while preserving causality. On vLLM and SGLang, Revati achieves less than 5% prediction error across multiple models and parallelism configurations, while running 5-17x faster than real GPU execution.

</details>


### [132] [Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure](https://arxiv.org/abs/2601.00530)
*Ravi Teja Pagidoju*

Main category: cs.DC

TL;DR: The paper compares POS workload performance and costs between Google Cloud Platform and Microsoft Azure using a systematic methodology with open-source benchmarking.


<details>
  <summary>Details</summary>
Motivation: The retail industry's digital transformation has driven the need for cloud-based POS systems, but there is limited empirical research on performance across cloud platforms.

Method: The authors systematically benchmarked POS workloads on GCP and Azure using free-tier cloud resources, analyzing metrics like latency, throughput, scalability, and costs.

Result: GCP showed 23.0% faster response times, while Azure exhibited 71.9% higher cost efficiency under steady-state operations.

Conclusion: The study provides a reproducible benchmarking method for retail cloud applications and insights on platform-specific POS performance, aiding stakeholders in cloud adoption decisions.

Abstract: Althoughthereislittleempiricalresearchonplatform-specific performance for retail workloads, the digital transformation of the retail industry has accelerated the adoption of cloud-based Point-of-Sale (POS) systems. This paper presents a systematic, repeatable comparison of POS workload deployments on Google Cloud Platform (GCP) and Microsoft Azure using real-time API endpoints and open-source benchmarking code. Using free-tier cloud resources, we offer a transparent methodology for POS workload evaluation that small retailers and researchers can use. Our approach measures important performance metrics like response latency, throughput, and scalability while estimating operational costs based on actual resource usage and current public cloud pricing because there is no direct billing under free-tier usage. All the tables and figures in this study are generated directly from code outputs, ensuring that the experimental data and the reported results are consistent. Our analysis shows that GCP achieves 23.0% faster response times at baseline load, while Azure shows 71.9% higher cost efficiency for steady-state operations. We look at the architectural components that lead to these differences and provide a helpful framework for merchants considering cloud point-of-sale implementation. This study establishes a strong, open benchmarking methodology for retail cloud applications and offers the first comprehensive, code-driven comparison of workloads unique to point-of-sale systems across leading cloud platforms.

</details>


### [133] [FlexSpec: Frozen Drafts Meet Evolving Targets in Edge-Cloud Collaborative LLM Speculative Decoding](https://arxiv.org/abs/2601.00644)
*Yuchen Li,Rui Kong,Zhonghao Lyu,Qiyang Li,Xinran Chen,Hengyi Cai,Lingyong Yan,Shuaiqiang Wang,Jiashu Zhao,Guangxu Zhu,Linghe Kong,Guihai Chen,Haoyi Xiong,Dawei Yin*

Main category: cs.DC

TL;DR: FlexSpec is a communication-efficient collaborative inference framework to optimize LLM deployments in edge-cloud systems, reducing communication overhead and improving latency performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges of deploying LLMs in edge and mobile computing, such as limited resources, communication overhead, and scalability issues due to frequent model evolution.

Method: FlexSpec introduces a shared-backbone architecture and a channel-aware adaptive speculation mechanism, enabling efficient inference with static edge-side models and minimized model synchronization.

Result: Experiments show that FlexSpec significantly enhances inference efficiency and reduces communication costs compared to traditional speculative decoding methods.

Conclusion: FlexSpec offers a scalable and efficient solution for integrating edge-cloud systems by decoupling model updates and adapting to resource constraints, making it suitable for evolving LLM applications.

Abstract: Deploying large language models (LLMs) in mobile and edge computing environments is constrained by limited on-device resources, scarce wireless bandwidth, and frequent model evolution. Although edge-cloud collaborative inference with speculative decoding (SD) can reduce end-to-end latency by executing a lightweight draft model at the edge and verifying it with a cloud-side target model, existing frameworks fundamentally rely on tight coupling between the two models. Consequently, repeated model synchronization introduces excessive communication overhead, increasing end-to-end latency, and ultimately limiting the scalability of SD in edge environments. To address these limitations, we propose FlexSpec, a communication-efficient collaborative inference framework tailored for evolving edge-cloud systems. The core design of FlexSpec is a shared-backbone architecture that allows a single and static edge-side draft model to remain compatible with a large family of evolving cloud-side target models. By decoupling edge deployment from cloud-side model updates, FlexSpec eliminates the need for edge-side retraining or repeated model downloads, substantially reducing communication and maintenance costs. Furthermore, to accommodate time-varying wireless conditions and heterogeneous device constraints, we develop a channel-aware adaptive speculation mechanism that dynamically adjusts the speculative draft length based on real-time channel state information and device energy budgets. Extensive experiments demonstrate that FlexSpec achieves superior performance compared to conventional SD approaches in terms of inference efficiency.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [134] [Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems](https://arxiv.org/abs/2601.00005)
*Lesley Wheat,Martin v. Mohrenschildt,Saeid Habibi*

Main category: cs.LG

TL;DR: This paper evaluates anomaly detection algorithms using synthetic datasets to address challenges posed by extreme class imbalance in industrial applications.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of extreme class imbalance in industrial systems, particularly due to limited faulty data available during machine learning training.

Method: A simulated dataset mimicking industrial constraints and anomaly distributions in 2D and 10D was used to benchmark 14 anomaly detection models across varied anomaly rates and dataset sizes.

Result: Performance varies with faulty data availability: unsupervised methods excel with fewer faulty examples (under 20), whereas semi-supervised and supervised methods significantly outperform when more faulty examples are present.

Conclusion: The study provides insights on how dataset size and features influence anomaly detection methods' effectiveness and generalization, guiding their deployment in industrial systems.

Abstract: Machine learning offers potential solutions to current issues in industrial systems in areas such as quality control and predictive maintenance, but also faces unique barriers in industrial applications. An ongoing challenge is extreme class imbalance, primarily due to the limited availability of faulty data during training. This paper presents a comprehensive evaluation of anomaly detection algorithms using a problem-agnostic simulated dataset that reflects real-world engineering constraints. Using a synthetic dataset with a hyper-spherical based anomaly distribution in 2D and 10D, we benchmark 14 detectors across training datasets with anomaly rates between 0.05% and 20% and training sizes between 1 000 and 10 000 (with a testing dataset size of 40 000) to assess performance and generalization error. Our findings reveal that the best detector is highly dependant on the total number of faulty examples in the training dataset, with additional healthy examples offering insignificant benefits in most cases. With less than 20 faulty examples, unsupervised methods (kNN/LOF) dominate; but around 30-50 faulty examples, semi-supervised (XGBOD) and supervised (SVM/CatBoost) detectors, we see large performance increases. While semi-supervised methods do not show significant benefits with only two features, the improvements are evident at ten features. The study highlights the performance drop on generalization of anomaly detection methods on smaller datasets, and provides practical insights for deploying anomaly detection in industrial environments.

</details>


### [135] [Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games](https://arxiv.org/abs/2601.00007)
*Nicholas A. Pape*

Main category: cs.LG

TL;DR: The paper explores training reinforcement learning agents on a solitaire and multiplayer Yahtzee using policy gradient methods, achieving results close to optimal for solitaire, while noting persistent challenges in long-term strategies.


<details>
  <summary>Details</summary>
Motivation: To use Yahtzee as a mid-scale reinforcement learning benchmark due to its combinatorial structure, stochastic nature, and delayed rewards, requiring advanced approximation methods for multiplayer scenarios.

Method: The authors formulate Yahtzee as a Markov Decision Process and train self-play agents with policy gradient techniques (REINFORCE, A2C, PPO), analyzing features, architectures, encodings, entropy regularization, and return estimators.

Result: Among the tested methods, A2C demonstrates robust learning achieving a median score close to the Dynamic Programming optimal (241.78 vs. 254.59), while REINFORCE and PPO are sensitive to hyperparameters. Challenges with long-term credit assignment persist.

Conclusion: Reinforcement learning approaches, particularly A2C, effectively approach optimal Yahtzee scoring; however, strategic weaknesses and long-horizon learning issues remain.

Abstract: Yahtzee is a classic dice game with a stochastic, combinatorial structure and delayed rewards, making it an interesting mid-scale RL benchmark. While an optimal policy for solitaire Yahtzee can be computed using dynamic programming methods, multiplayer is intractable, motivating approximation methods. We formulate Yahtzee as a Markov Decision Process (MDP), and train self-play agents using various policy gradient methods: REINFORCE, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO), all using a multi-headed network with a shared trunk. We ablate feature and action encodings, architecture, return estimators, and entropy regularization to understand their impact on learning. Under a fixed training budget, REINFORCE and PPO prove sensitive to hyperparameters and fail to reach near-optimal performance, whereas A2C trains robustly across a range of settings. Our agent attains a median score of 241.78 points over 100,000 evaluation games, within 5.0\% of the optimal DP score of 254.59, achieving the upper section bonus and Yahtzee at rates of 24.9\% and 34.1\%, respectively. All models struggle to learn the upper bonus strategy, overindexing on four-of-a-kind's, highlighting persistent long-horizon credit-assignment and exploration challenges.

</details>


### [136] [The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition](https://arxiv.org/abs/2601.00065)
*Xiaoze Liu,Weichen Yu,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.LG

TL;DR: The paper discusses a vulnerability in the LLM tokenizer transplant process, enabling the introduction of a "breaker token" that can sabotage model performance after integration.


<details>
  <summary>Details</summary>
Motivation: To address interoperability challenges within the open-weight LLM ecosystem caused by incompatible vocabularies among model families.

Method: The authors use a dual-objective optimization problem, leveraging coefficient reuse to create a "breaker token" that disrupts the base model's generation, preserving donor model integrity.

Result: The attack is training-free, evades outlier detection through spectral mimicry, and persists despite fine-tuning and weight merging.

Conclusion: Tokenizer transplant introduces a potential supply-chain vulnerability in modular AI composition, raising risks in interoperability techniques.

Abstract: The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single "breaker token" that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack creates an asymmetric realizability gap that sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and achieves spectral mimicry to evade outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge

</details>


### [137] [IMBWatch -- a Spatio-Temporal Graph Neural Network approach to detect Illicit Massage Business](https://arxiv.org/abs/2601.00075)
*Swetha Varadarajan,Abhishek Ray,Lumina Albert*

Main category: cs.LG

TL;DR: IMBWatch is a spatio-temporal graph neural network framework designed to detect Illicit Massage Businesses (IMBs) by analyzing patterns in their operational networks using open-source intelligence.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the persistent and covert nature of IMBs that use legitimate wellness services as a cover for human trafficking, sexual exploitation, and coerced labor. Traditional methods are ineffective in uncovering their broader operational networks.

Method: IMBWatch uses spatio-temporal graph neural networks (ST-GNN) to construct dynamic graphs from open-source data. It represents entities like businesses, phone numbers, and locations as nodes, with edges reflecting their spatio-temporal relationships and patterns.

Result: IMBWatch outperforms baseline models in accuracy and F1 score on datasets from U.S. cities. It also offers enhanced interpretability and provides actionable insights to assist interventions.

Conclusion: The framework demonstrates scalability and adaptability to other illicit domains, while supporting reproducible research with open-source code and anonymized data.

Abstract: Illicit Massage Businesses (IMBs) are a covert and persistent form of organized exploitation that operate under the facade of legitimate wellness services while facilitating human trafficking, sexual exploitation, and coerced labor. Detecting IMBs is difficult due to encoded digital advertisements, frequent changes in personnel and locations, and the reuse of shared infrastructure such as phone numbers and addresses. Traditional approaches, including community tips and regulatory inspections, are largely reactive and ineffective at revealing the broader operational networks traffickers rely on.
  To address these challenges, we introduce IMBWatch, a spatio-temporal graph neural network (ST-GNN) framework for large-scale IMB detection. IMBWatch constructs dynamic graphs from open-source intelligence, including scraped online advertisements, business license records, and crowdsourced reviews. Nodes represent heterogeneous entities such as businesses, aliases, phone numbers, and locations, while edges capture spatio-temporal and relational patterns, including co-location, repeated phone usage, and synchronized advertising. The framework combines graph convolutional operations with temporal attention mechanisms to model the evolution of IMB networks over time and space, capturing patterns such as intercity worker movement, burner phone rotation, and coordinated advertising surges.
  Experiments on real-world datasets from multiple U.S. cities show that IMBWatch outperforms baseline models, achieving higher accuracy and F1 scores. Beyond performance gains, IMBWatch offers improved interpretability, providing actionable insights to support proactive and targeted interventions. The framework is scalable, adaptable to other illicit domains, and released with anonymized data and open-source code to support reproducible research.

</details>


### [138] [Exploration in the Limit](https://arxiv.org/abs/2601.00084)
*Brian M. Cho,Nathan Kallus*

Main category: cs.LG

TL;DR: Introduced a new Best Arm Identification (BAI) algorithm using an asymptotic framework to improve practical performance and error control.


<details>
  <summary>Details</summary>
Motivation: Existing BAI methods struggle with exact error control due to reliance on loose inequalities or parametric conditions, limiting their practicality.

Method: Proposed a relaxed asymptotic formulation for error control, developing anytime-valid confidence sequences and incorporating covariates for variance reduction in nonparametric settings.

Result: Achieved reduced average sample complexities while maintaining approximate error control. Also provided theoretical bounds showing strong performance compared to Gaussian BAI with exact guarantees.

Conclusion: The new approach enables robust performance and error control in real-world BAI problems with weak signals, high significance thresholds, and extensive sample sizes.

Abstract: In fixed-confidence best arm identification (BAI), the objective is to quickly identify the optimal option while controlling the probability of error below a desired threshold. Despite the plethora of BAI algorithms, existing methods typically fall short in practical settings, as stringent exact error control requires using loose tail inequalities and/or parametric restrictions. To overcome these limitations, we introduce a relaxed formulation that requires valid error control asymptotically with respect to a minimum sample size. This aligns with many real-world settings that often involve weak signals, high desired significance, and post-experiment inference requirements, all of which necessitate long horizons. This allows us to achieve tighter optimality, while better handling flexible nonparametric outcome distributions and fully leveraging individual-level contexts. We develop a novel asymptotic anytime-valid confidence sequences over arm indices, and we use it to design a new BAI algorithm for our asymptotic framework. Our method flexibly incorporates covariates for variance reduction and ensures approximate error control in fully nonparametric settings. Under mild convergence assumptions, we provide asymptotic bounds on the sample complexity and show the worst-case sample complexity of our approach matches the best-case sample complexity of Gaussian BAI under exact error guarantees and known variances. Experiments suggest our approach reduces average sample complexities while maintaining error control.

</details>


### [139] [Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery](https://arxiv.org/abs/2601.00088)
*Junqi Qu,Yan Zhang,Shangqian Gao,Shibo Li*

Main category: cs.LG

TL;DR: LLMs' equation discovery is sensitive to prompt phrasing. NeuroSymBO proposes adaptive instruction selection using Bayesian Optimization to overcome suboptimal results, improving performance on PDE benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the sensitivity of Large Language Models to prompt phrasing in equation discovery, which limits their effectiveness in dynamic multi-step processes.

Method: NeuroSymBO uses Bayesian Optimization to dynamically select optimal instructions at each generation step based on numerical feedback, maintaining a discrete library of reasoning strategies.

Result: Experiments demonstrate that adaptive instruction selection in NeuroSymBO outperforms fixed prompts, yielding better recovery rates and more parsimonious solutions.

Conclusion: Dynamic adaptation in prompt engineering can resolve instruction brittleness, improving equation discovery capabilities of LLMs.

Abstract: Large Language Models (LLMs) show promise for equation discovery, yet their outputs are highly sensitive to prompt phrasing, a phenomenon we term instruction brittleness. Static prompts cannot adapt to the evolving state of a multi-step generation process, causing models to plateau at suboptimal solutions. To address this, we propose NeuroSymBO, which reframes prompt engineering as a sequential decision problem. Our method maintains a discrete library of reasoning strategies and uses Bayesian Optimization to select the optimal instruction at each step based on numerical feedback. Experiments on PDE discovery benchmarks show that adaptive instruction selection significantly outperforms fixed prompts, achieving higher recovery rates with more parsimonious solutions.

</details>


### [140] [GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments](https://arxiv.org/abs/2601.00116)
*Aditya Sai Ellendula,Yi Wang,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: This paper introduces GRL-SNAM, a geometric reinforcement learning framework for autonomous navigation and mapping without prior knowledge of the environment map. It focuses on local sensory observations and dynamic optimization to adaptively refine trajectories.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of navigating unknown environments where robots are required to perform simultaneous navigation and mapping using only local sensory inputs without relying on global maps.

Method: The method involves dynamically controlling Hamiltonian optimization based on local sensory inputs, translating them into adaptive energy landscapes and evolving policies for navigation and sensing through stepwise updates of Hamiltonians.

Result: GRL-SNAM demonstrated superior navigation quality with minimal exploration compared to reactive local baselines and global policy learning methods, successfully generalizing to unseen layouts.

Conclusion: Geometric reinforcement learning with adaptive Hamiltonian optimization enables efficient navigation and mapping in unknown environments by refining trajectories using local sensory data without requiring extensive global map construction.

Abstract: We present GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping(SNAM) in unknown environments. A SNAM problem is challenging as it needs to design hierarchical or joint policies of multiple agents that control the movement of a real-life robot towards the goal in mapless environment, i.e. an environment where the map of the environment is not available apriori, and needs to be acquired through sensors. The sensors are invoked from the path learner, i.e. navigator, through active query responses to sensory agents, and along the motion path. GRL-SNAM differs from preemptive navigation algorithms and other reinforcement learning methods by relying exclusively on local sensory observations without constructing a global map. Our approach formulates path navigation and mapping as a dynamic shortest path search and discovery process using controlled Hamiltonian optimization: sensory inputs are translated into local energy landscapes that encode reachability, obstacle barriers, and deformation constraints, while policies for sensing, planning, and reconfiguration evolve stagewise via updating Hamiltonians. A reduced Hamiltonian serves as an adaptive score function, updating kinetic/potential terms, embedding barrier constraints, and continuously refining trajectories as new local information arrives. We evaluate GRL-SNAM on two different 2D navigation tasks. Comparing against local reactive baselines and global policy learning references under identical stagewise sensing constraints, it preserves clearance, generalizes to unseen layouts, and demonstrates that Geometric RL learning via updating Hamiltonians enables high-quality navigation through minimal exploration via local energy refinement rather than extensive global mapping. The code is publicly available on \href{https://github.com/CVC-Lab/GRL-SNAM}{Github}.

</details>


### [141] [Laplacian Kernelized Bandit](https://arxiv.org/abs/2601.00461)
*Shuang Wu,Arash A. Amini*

Main category: cs.LG

TL;DR: The paper introduces a unified framework for multi-user contextual bandits utilizing a multi-user RKHS kernel that blends graph Laplacian and arm kernels, enabling efficient exploration via Gaussian Process-based algorithms.


<details>
  <summary>Details</summary>
Motivation: To address the exploration in multi-user contextual bandits where users are connected by a graph and exhibit non-linear reward functions with graph homophily, current methods lack principled frameworks that account for non-linearities and graph structures.

Method: It formulates a joint penalty combining graph smoothness and individual function roughness, derives a unified multi-user RKHS kernel, and introduces two algorithms (LK-GP-UCB and LK-GP-TS) based on Gaussian Process posterior for exploration.

Result: Proved high-probability regret bounds based on an effective dimension of the kernel, not user count or ambient dimension. Empirical results show outperforming baselines in non-linear scenarios and competitive performance with linear rewards.

Conclusion: This work provides a unified, practical, and theoretically sound framework combining graph Laplacian regularization with kernelized methods for structured multi-user exploration.

Abstract: We study multi-user contextual bandits where users are related by a graph and their reward functions exhibit both non-linear behavior and graph homophily. We introduce a principled joint penalty for the collection of user reward functions $\{f_u\}$, combining a graph smoothness term based on RKHS distances with an individual roughness penalty. Our central contribution is proving that this penalty is equivalent to the squared norm within a single, unified \emph{multi-user RKHS}. We explicitly derive its reproducing kernel, which elegantly fuses the graph Laplacian with the base arm kernel. This unification allows us to reframe the problem as learning a single ''lifted'' function, enabling the design of principled algorithms, \texttt{LK-GP-UCB} and \texttt{LK-GP-TS}, that leverage Gaussian Process posteriors over this new kernel for exploration. We provide high-probability regret bounds that scale with an \emph{effective dimension} of the multi-user kernel, replacing dependencies on user count or ambient dimension. Empirically, our methods outperform strong linear and non-graph-aware baselines in non-linear settings and remain competitive even when the true rewards are linear. Our work delivers a unified, theoretically grounded, and practical framework that bridges Laplacian regularization with kernelized bandits for structured exploration.

</details>


### [142] [Reinforcement Learning with Function Approximation for Non-Markov Processes](https://arxiv.org/abs/2601.00151)
*Ali Devran Kara*

Main category: cs.LG

TL;DR: This study investigates reinforcement learning with linear function approximation in non-Markov state/cost settings, establishing convergence for specific cases.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of reinforcement learning with linear function approximation in non-Markov environments and provide algorithms with convergence guarantees under specific conditions.

Method: The paper analyzes policy evaluation and Q-learning under non-Markov processes, proposing convergence guarantees for specific conditions like ergodicity and quantization-based basis functions. Applications to partially observed MDPs are also discussed, deriving explicit error bounds.

Result: Policy evaluation converges to the fixed point of a joint operator combining Bellman operator and orthogonal projection. Q-learning convergence is shown for quantization-map-based basis functions under ergodicity. Error bounds are provided for partially observed MDPs.

Conclusion: New insights and guarantees are provided for reinforcement learning with linear function approximation in non-Markovian contexts, expanding its applicability and reliability.

Abstract: We study reinforcement learning methods with linear function approximation under non-Markov state and cost processes. We first consider the policy evaluation method and show that the algorithm converges under suitable ergodicity conditions on the underlying non-Markov processes. Furthermore, we show that the limit corresponds to the fixed point of a joint operator composed of an orthogonal projection and the Bellman operator of an auxiliary \emph{Markov} decision process.
  For Q-learning with linear function approximation, as in the Markov setting, convergence is not guaranteed in general. We show, however, that for the special case where the basis functions are chosen based on quantization maps, the convergence can be shown under similar ergodicity conditions. Finally, we apply our results to partially observed Markov decision processes, where finite-memory variables are used as state representations, and we derive explicit error bounds for the limits of the resulting learning algorithms.

</details>


### [143] [Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting](https://arxiv.org/abs/2601.00172)
*Ata Akbari Asanjan,Filip Wudarski,Daniel O'Connor,Shaun Geaney,Elena Strbac,P. Aaron Lott,Davide Venturelli*

Main category: cs.LG

TL;DR: This paper proposes Sequential Reservoir Computing to improve scalability and forecasting in high-dimensional spatiotemporal systems, showing enhanced efficiency and accuracy compared to RNNs and LSTMs.


<details>
  <summary>Details</summary>
Motivation: To address computational and scalability challenges in forecasting high-dimensional spatiotemporal systems using conventional recurrent neural networks and reservoir computing architectures.

Method: Introducing Sequential Reservoir Computing, which divides a large reservoir into smaller connected reservoirs, improving scalability, efficiency, and maintaining long-term dependencies.

Result: Sequential RC demonstrated longer forecast horizons, lower error metrics, and drastically reduced training costs in chaotic and high-dimensional systems compared to baseline methods.

Conclusion: Sequential RC provides a scalable, efficient solution for real-time forecasting in high-dimensional systems, outperforming standard methods like RNNs and LSTMs.

Abstract: Forecasting high-dimensional spatiotemporal systems remains computationally challenging for recurrent neural networks (RNNs) and long short-term memory (LSTM) models due to gradient-based training and memory bottlenecks. Reservoir Computing (RC) mitigates these challenges by replacing backpropagation with fixed recurrent layers and a convex readout optimization, yet conventional RC architectures still scale poorly with input dimensionality. We introduce a Sequential Reservoir Computing (Sequential RC) architecture that decomposes a large reservoir into a series of smaller, interconnected reservoirs. This design reduces memory and computational costs while preserving long-term temporal dependencies. Using both low-dimensional chaotic systems (Lorenz63) and high-dimensional physical simulations (2D vorticity and shallow-water equations), Sequential RC achieves 15-25% longer valid forecast horizons, 20-30% lower error metrics (SSIM, RMSE), and up to three orders of magnitude lower training cost compared to LSTM and standard RNN baselines. The results demonstrate that Sequential RC maintains the simplicity and efficiency of conventional RC while achieving superior scalability for high-dimensional dynamical systems. This approach provides a practical path toward real-time, energy-efficient forecasting in scientific and engineering applications.

</details>


### [144] [Federated Customization of Large Models: Approaches, Experiments, and Insights](https://arxiv.org/abs/2601.00526)
*Yuchuan Ye,Ming Ding,Youjia Chen,Peng Cheng,Dusit Niyato*

Main category: cs.LG

TL;DR: This article evaluates the integration of large model customization techniques within the federated learning (FL) framework and tests federated prefix-tuning experimentally.


<details>
  <summary>Details</summary>
Motivation: There is a growing demand for customizing large models in distributed scenarios where data privacy is critical, such as federated learning frameworks.

Method: The paper surveys customization techniques like fine-tuning, prompt engineering, prefix-tuning, and validates federated prefix-tuning with experiments in FL settings.

Result: Federated prefix-tuning achieved feasibility and performance similar to centralized methods, while comparisons demonstrated its competitive and robust efficiency.

Conclusion: Federated prefix-tuning is validated as a promising method for efficient, robust large model customization within FL frameworks.

Abstract: In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.

</details>


### [145] [The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data](https://arxiv.org/abs/2601.00152)
*Yann Bellec,Rohan Kaman,Siwen Cui,Aarav Agrawal,Calvin Chen*

Main category: cs.LG

TL;DR: The paper explores predictive factors for traffic accident severity using machine learning on U.S. accident data, achieving 78% accuracy and highlighting key features like time of day, location, and weather variables.


<details>
  <summary>Details</summary>
Motivation: Improving prediction models for traffic accident severity to enhance traffic safety and management protocols.

Method: An XGBoost classifier optimized via randomized search cross-validation, with class weighting applied to address imbalance, trained on a large accident dataset (500,000 cases).

Result: Achieved 78% accuracy overall, with 87% precision and recall for the majority class (Severity 2). Key predictors include time, location, and weather variables. Certain anticipated predictors showed limited value.

Conclusion: The model’s performance supports evidence-based traffic management, but the dominance of mid-level severity cases limits insights for extreme cases, suggesting future research directions like advanced feature engineering and new data integration.

Abstract: This study investigates the predictive capacity of environmental, temporal, and spatial factors on traffic accident severity in the United States. Using a dataset of 500,000 U.S. traffic accidents spanning 2016-2023, we trained an XGBoost classifier optimized through randomized search cross-validation and adjusted for class imbalance via class weighting. The final model achieves an overall accuracy of 78%, with strong performance on the majority class (Severity 2), attaining 87% precision and recall. Feature importance analysis reveals that time of day, geographic location, and weather-related variables, including visibility, temperature, and wind speed, rank among the strongest predictors of accident severity. However, contrary to initial hypotheses, precipitation and visibility demonstrate limited predictive power, potentially reflecting behavioral adaptation by drivers under overtly hazardous conditions. The dataset's predominance of mid-level severity accidents constrains the model's capacity to learn meaningful patterns for extreme cases, highlighting the need for alternative sampling strategies, enhanced feature engineering, and integration of external datasets. These findings contribute to evidence-based traffic management and suggest future directions for severity prediction research.

</details>


### [146] [Online Finetuning Decision Transformers with Pure RL Gradients](https://arxiv.org/abs/2601.00167)
*Junkai Luo,Yinglun Zhu*

Main category: cs.LG

TL;DR: The paper explores pure RL gradient-based online finetuning of Decision Transformers (DTs), proposing new methods to enhance performance.


<details>
  <summary>Details</summary>
Motivation: Offline RL has been effectively addressed using Decision Transformers, but expanding DTs into online settings via pure RL gradients remains a challenge due to reliance on supervised sequence-modeling approaches.

Method: The paper identifies the incompatibility of hindsight return relabeling with RL-based algorithms, adapts GRPO to DTs, and introduces key innovations like sub-trajectory optimization, sequence-level likelihood objectives, and active sampling.

Result: The proposed methods outperform existing online DT baselines and set new state-of-the-art performance levels across multiple benchmarks.

Conclusion: Pure RL-based online finetuning for Decision Transformers improves stability, efficiency, and overall performance, advancing the field of sequential decision making.

Abstract: Decision Transformers (DTs) have emerged as a powerful framework for sequential decision making by formulating offline reinforcement learning (RL) as a sequence modeling problem. However, extending DTs to online settings with pure RL gradients remains largely unexplored, as existing approaches continue to rely heavily on supervised sequence-modeling objectives during online finetuning. We identify hindsight return relabeling -- a standard component in online DTs -- as a critical obstacle to RL-based finetuning: while beneficial for supervised learning, it is fundamentally incompatible with importance sampling-based RL algorithms such as GRPO, leading to unstable training. Building on this insight, we propose new algorithms that enable online finetuning of Decision Transformers using pure reinforcement learning gradients. We adapt GRPO to DTs and introduce several key modifications, including sub-trajectory optimization for improved credit assignment, sequence-level likelihood objectives for enhanced stability and efficiency, and active sampling to encourage exploration in uncertain regions. Through extensive experiments, we demonstrate that our methods outperform existing online DT baselines and achieve new state-of-the-art performance across multiple benchmarks, highlighting the effectiveness of pure-RL-based online finetuning for Decision Transformers.

</details>


### [147] [Categorical Reparameterization with Denoising Diffusion models](https://arxiv.org/abs/2601.00781)
*Samson Gourevitch,Alain Durmus,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.LG

TL;DR: The paper introduces a diffusion-based soft reparameterization for categorical variables to improve gradient-based optimization, showing competitive or improved performance in experiments.


<details>
  <summary>Details</summary>
Motivation: Gradient-based optimization of categorical variables has challenges, such as noisy score-function estimators and biased objectives from continuous relaxations. The authors aim to address these limitations by proposing a novel methodology.

Method: The paper introduces a diffusion-based soft reparameterization for categorical distributions. It leverages a Gaussian noising process for which the denoiser has a closed form, resulting in an efficient, training-free diffusion sampler that allows backpropagation.

Result: The proposed reparameterization approach demonstrates competitive or better performance in optimization tasks across various benchmarks.

Conclusion: This novel reparameterization method provides an effective alternative for handling categorical variables in optimization problems, offering benefits over existing score-function estimators and continuous relaxations.

Abstract: Gradient-based optimization with categorical variables typically relies on score-function estimators, which are unbiased but noisy, or on continuous relaxations that replace the discrete distribution with a smooth surrogate admitting a pathwise (reparameterized) gradient, at the cost of optimizing a biased, temperature-dependent objective. In this paper, we extend this family of relaxations by introducing a diffusion-based soft reparameterization for categorical distributions. For these distributions, the denoiser under a Gaussian noising process admits a closed form and can be computed efficiently, yielding a training-free diffusion sampler through which we can backpropagate. Our experiments show that the proposed reparameterization trick yields competitive or improved optimization performance on various benchmarks.

</details>


### [148] [Early Prediction of Liver Cirrhosis Up to Three Years in Advance: A Machine Learning Study Benchmarking Against the FIB-4 Score](https://arxiv.org/abs/2601.00175)
*Zhuqi Miao,Sujan Ravi,Abdulaziz Ahmed*

Main category: cs.LG

TL;DR: This paper presents machine learning (ML) models that use electronic health record (EHR) data to predict liver cirrhosis up to three years in advance, outperforming the traditional FIB-4 score.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of the traditional FIB-4 score for early recognition of liver cirrhosis by leveraging routinely collected EHR data and using machine learning methods for better prediction in clinical workflows.

Method: A retrospective cohort study was performed with EHR data from a large academic health system, focusing on patients with fatty liver disease categorized into cirrhosis and non-cirrhosis cohorts. Using XGBoost, machine learning models were trained for 1-, 2-, and 3-year prediction horizons and evaluated using AUC metrics, comparing them to the performance of FIB-4.

Result: The ML models outperformed the FIB-4 score, achieving AUCs of 0.81, 0.73, and 0.69 for 1-, 2-, and 3-year predictions respectively, compared to 0.71, 0.63, and 0.57 for FIB-4. These models demonstrated superior early risk discrimination over longer prediction horizons.

Conclusion: The study concludes that ML models utilizing EHR data are more accurate and effective than the traditional FIB-4 score for early liver cirrhosis prediction. They have strong potential for integration into clinical workflows as decision-support tools to aid in prevention and management.

Abstract: Objective: Develop and evaluate machine learning (ML) models for predicting incident liver cirrhosis one, two, and three years prior to diagnosis using routinely collected electronic health record (EHR) data, and to benchmark their performance against the FIB-4 score. Methods: We conducted a retrospective cohort study using de-identified EHR data from a large academic health system. Patients with fatty liver disease were identified and categorized into cirrhosis and non-cirrhosis cohorts based on ICD-9/10 codes. Prediction scenarios were constructed using observation and prediction windows to emulate real-world clinical use. Demographics, diagnoses, laboratory results, vital signs, and comorbidity indices were aggregated from the observation window. XGBoost models were trained for 1-, 2-, and 3-year prediction horizons and evaluated on held-out test sets. Model performance was compared with FIB-4 using area under the receiver operating characteristic curve (AUC). Results: Final cohorts included 3,043 patients for the 1-year prediction, 1,981 for the 2-year prediction, and 1,470 for the 3-year prediction. Across all prediction windows, ML models consistently outperformed FIB-4. The XGBoost models achieved AUCs of 0.81, 0.73, and 0.69 for 1-, 2-, and 3-year predictions, respectively, compared with 0.71, 0.63, and 0.57 for FIB-4. Performance gains persisted with longer prediction horizons, indicating improved early risk discrimination. Conclusions: Machine learning models leveraging routine EHR data substantially outperform the traditional FIB-4 score for early prediction of liver cirrhosis. These models enable earlier and more accurate risk stratification and can be integrated into clinical workflows as automated decision-support tools to support proactive cirrhosis prevention and management.

</details>


### [149] [Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings](https://arxiv.org/abs/2601.00186)
*Moirangthem Tiken Singh,Adnan Arif*

Main category: cs.LG

TL;DR: This paper proposes a reinforcement learning-based framework for adaptive repetition coding to preserve semantic meaning in bandwidth-constrained systems, showing significant gains over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The increasing reliance on bandwidth-constrained communication systems, like IoT and edge computing, necessitates methods to preserve semantic fidelity in transmitted data.

Method: A reinforcement learning framework is used to perform per-dimension unequal error protection through adaptive repetition coding, guided by a composite semantic distortion metric.

Result: The framework achieves 6.8% higher chrF scores and 9.3% better entity preservation at 1 dB SNR compared to uniform protection.

Conclusion: Simple, context-aware repetition coding outperforms traditional methods in semantic protection, making it suitable for scenarios demanding high semantic fidelity under bandwidth constraints.

Abstract: This paper tackles the pressing challenge of preserving semantic meaning in communication systems constrained by limited bandwidth. We introduce a novel reinforcement learning framework that achieves per-dimension unequal error protection via adaptive repetition coding. Central to our approach is a composite semantic distortion metric that balances global embedding similarity with entity-level preservation, empowering the reinforcement learning agent to allocate protection in a context-aware manner. Experiments show statistically significant gains over uniform protection, achieving 6.8% higher chrF scores and 9.3% better entity preservation at 1 dB SNR. The key innovation of our framework is the demonstration that simple, intelligently allocated repetition coding enables fine-grained semantic protection -- an advantage unattainable with conventional codes such as LDPC or Reed-Solomon. Our findings challenge traditional channel coding paradigms by establishing that code structure must align with semantic granularity. This approach is particularly suited to edge computing and IoT scenarios, where bandwidth is scarce, but semantic fidelity is critical, providing a practical pathway for next-generation semantic-aware networks.

</details>


### [150] [SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification](https://arxiv.org/abs/2601.00189)
*Danial Sharifrazi,Nouman Javed,Mojtaba Mohammadi,Seyede Sana Salehi,Roohallah Alizadehsani,Prasad N. Paradkar,U. Rajendra Acharya,Asim Bhatti*

Main category: cs.LG

TL;DR: The paper presents a new GAN architecture called SSI-GAN that classifies mosquito neuronal spike patterns to detect arboviral infections with minimal labeled data, achieving near-perfect accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual classification of mosquito neuronal spikes is labor-intensive and costly, and most current deep learning approaches are infeasible for field applications due to their need for fully labeled and extensively preprocessed data.

Method: The authors developed a semi-supervised GAN termed SSI-GAN, utilizing a Swin-inspired discriminator and a transformer-based generator for detecting viral neurotropism. The model incorporates a shifted-window transformer design and multi-head self-attention to efficiently capture spike features.

Result: SSI-GAN achieved 99.93% classification accuracy using only 3% labeled data and maintained high accuracy even with as low as 1% labeled data, representing a drastic reduction in manual labeling efforts (97-99%).

Conclusion: The proposed SSI-GAN establishes new benchmarks in neuronal spike classification for arboviral infections, significantly reducing data labeling requirements while maintaining exceptional performance.

Abstract: Mosquitos are the main transmissive agents of arboviral diseases. Manual classification of their neuronal spike patterns is very labor-intensive and expensive. Most available deep learning solutions require fully labeled spike datasets and highly preprocessed neuronal signals. This reduces the feasibility of mass adoption in actual field scenarios. To address the scarcity of labeled data problems, we propose a new Generative Adversarial Network (GAN) architecture that we call the Semi-supervised Swin-Inspired GAN (SSI-GAN). The Swin-inspired, shifted-window discriminator, together with a transformer-based generator, is used to classify neuronal spike trains and, consequently, detect viral neurotropism. We use a multi-head self-attention model in a flat, window-based transformer discriminator that learns to capture sparser high-frequency spike features. Using just 1 to 3% labeled data, SSI-GAN was trained with more than 15 million spike samples collected at five-time post-infection and recording classification into Zika-infected, dengue-infected, or uninfected categories. Hyperparameters were optimized using the Bayesian Optuna framework, and performance for robustness was validated under fivefold Monte Carlo cross-validation. SSI-GAN reached 99.93% classification accuracy on the third day post-infection with only 3% labeled data. It maintained high accuracy across all stages of infection with just 1% supervision. This shows a 97-99% reduction in manual labeling effort relative to standard supervised approaches at the same performance level. The shifted-window transformer design proposed here beat all baselines by a wide margin and set new best marks in spike-based neuronal infection classification.

</details>


### [151] [Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework](https://arxiv.org/abs/2601.00192)
*Moirangthem Tiken Singh,Manibhushan Yaikhom*

Main category: cs.LG

TL;DR: The paper introduces a resource-efficient framework for arrhythmia detection using lightweight feature engineering, achieving high accuracy and exceptional efficiency on real-time monitoring platforms.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for arrhythmia detection are computationally intensive, limiting their applicability in edge devices and IoMT systems.

Method: A data-centric pipeline integrates time-frequency wavelet decompositions with graph-theoretic structural descriptors, refined through mutual information and recursive elimination for lightweight and interpretable linear classifiers.

Result: Validated on MIT-BIH and INCART datasets, achieving 98.44% accuracy, 8.54 KB model size, and ultra-low inference latency for real-time applications.

Conclusion: The system significantly improves efficiency and accuracy over existing methods, enhancing real-time cardiac monitoring in resource-constrained environments.

Abstract: Cardiovascular diseases, particularly arrhythmias, remain a leading global cause of mortality, necessitating continuous monitoring via the Internet of Medical Things (IoMT). However, state-of-the-art deep learning approaches often impose prohibitive computational overheads, rendering them unsuitable for resource-constrained edge devices. This study proposes a resource-efficient, data-centric framework that prioritizes feature engineering over complexity. Our optimized pipeline makes the complex, high-dimensional arrhythmia data linearly separable. This is achieved by integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors, such as PageRank centrality. This hybrid feature space, combining wavelet decompositions and graph-theoretic descriptors, is then refined using mutual information and recursive elimination, enabling interpretable, ultra-lightweight linear classifiers. Validation on the MIT-BIH and INCART datasets yields 98.44% diagnostic accuracy with an 8.54 KB model footprint. The system achieves 0.46 $μ$s classification inference latency within a 52 ms per-beat pipeline, ensuring real-time operation. These outcomes provide an order-of-magnitude efficiency gain over compressed models, such as KD-Light (25 KB, 96.32% accuracy), advancing battery-less cardiac sensors.

</details>


### [152] [Unknown Aware AI-Generated Content Attribution](https://arxiv.org/abs/2601.00218)
*Ellie Thieu,Jifan Zhang,Haoyue Bai*

Main category: cs.LG

TL;DR: This paper addresses the challenge of attributing images to specific generative models, particularly distinguishing outputs from a target generator against real images and other models.


<details>
  <summary>Details</summary>
Motivation: Attributing synthetic content to its source is crucial as generative models become more advanced and widely used, especially when distinguishing outputs in open-world scenarios with unknown generators.

Method: A constrained optimization approach is proposed, utilizing CLIP features, a linear classifier, and unlabeled 'wild' data from the Internet to improve classification without compromising performance on labeled data.

Result: Experimental results show the method's effectiveness in improving attribution performance on unseen and challenging generators by leveraging wild data.

Conclusion: The use of unlabeled, diverse 'wild' data significantly enhances the ability to attribute generative model outputs, proving viable for open-world attribution challenges.

Abstract: The rapid advancement of photorealistic generative models has made it increasingly important to attribute the origin of synthetic content, moving beyond binary real or fake detection toward identifying the specific model that produced a given image. We study the problem of distinguishing outputs from a target generative model (e.g., OpenAI Dalle 3) from other sources, including real images and images generated by a wide range of alternative models. Using CLIP features and a simple linear classifier, shown to be effective in prior work, we establish a strong baseline for target generator attribution using only limited labeled data from the target model and a small number of known generators. However, this baseline struggles to generalize to harder, unseen, and newly released generators. To address this limitation, we propose a constrained optimization approach that leverages unlabeled wild data, consisting of images collected from the Internet that may include real images, outputs from unknown generators, or even samples from the target model itself. The proposed method encourages wild samples to be classified as non target while explicitly constraining performance on labeled data to remain high. Experimental results show that incorporating wild data substantially improves attribution performance on challenging unseen generators, demonstrating that unlabeled data from the wild can be effectively exploited to enhance AI generated content attribution in open world settings.

</details>


### [153] [Robust Graph Fine-Tuning with Adversarial Graph Prompting](https://arxiv.org/abs/2601.00229)
*Ziyan Zhang,Bo Jiang,Jin Tang*

Main category: cs.LG

TL;DR: A novel method, Adversarial Graph Prompting (AGP), is proposed to improve the robustness of Parameter-Efficient Fine-Tuning (PEFT) for pre-trained GNN models against graph topology and node noise.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods for graph neural networks are highly vulnerable to noise and attacks affecting graph topology and node attributes/features. Addressing this vulnerability is critical for robust adaptation of GNN models.

Method: The AGP framework integrates adversarial learning into graph prompting. It uses a min-max optimization approach solved through alternating optimization: Joint Projected Gradient Descent (JointPGD) for inner maximization (strong adversarial noise generation) and a node prompt learning module for outer minimization (counteracting adversarial noise).

Result: AGP effectively handles noise impacting both graph topology and node attributes, demonstrating versatility and outperforming state-of-the-art methods on multiple benchmark tasks.

Conclusion: AGP is a robust and general method that can integrate with various pre-trained GNN models, significantly enhancing their resilience and performance in noisy environments.

Abstract: Parameter-Efficient Fine-Tuning (PEFT) method has emerged as a dominant paradigm for adapting pre-trained GNN models to downstream tasks. However, existing PEFT methods usually exhibit significant vulnerability to various noise and attacks on graph topology and node attributes/features. To address this issue, for the first time, we propose integrating adversarial learning into graph prompting and develop a novel Adversarial Graph Prompting (AGP) framework to achieve robust graph fine-tuning. Our AGP has two key aspects. First, we propose the general problem formulation of AGP as a min-max optimization problem and develop an alternating optimization scheme to solve it. For inner maximization, we propose Joint Projected Gradient Descent (JointPGD) algorithm to generate strong adversarial noise. For outer minimization, we employ a simple yet effective module to learn the optimal node prompts to counteract the adversarial noise. Second, we demonstrate that the proposed AGP can theoretically address both graph topology and node noise. This confirms the versatility and robustness of our AGP fine-tuning method across various graph noise. Note that, the proposed AGP is a general method that can be integrated with various pre-trained GNN models to enhance their robustness on the downstream tasks. Extensive experiments on multiple benchmark tasks validate the robustness and effectiveness of AGP method compared to state-of-the-art methods.

</details>


### [154] [GRIT -- Geometry-Aware PEFT with K-FACPreconditioning, Fisher-Guided Reprojection, andDynamic Rank Adaptation](https://arxiv.org/abs/2601.00231)
*Pritish Saha,Chandrav Rajbangshi,Rudra Goyal,Mohit Goyal,Anurag Deo,Biswajit Roy,Ningthoujam Dhanachandra Singh,Raxit Goswami,Amitava Das*

Main category: cs.LG

TL;DR: This paper proposes GRIT, a curvature-aware LoRA method for parameter-efficient fine-tuning of LLMs, achieving competitive or superior results with significantly fewer trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Current PEFT methods such as LoRA and QLoRA are geometry-agnostic and do not consider local loss curvature, leading to inefficiencies and potential drifts in parameter updates.

Method: GRIT incorporates: (1) gradient preconditioning in rank space using K-FAC, (2) reprojecting the basis onto Fisher eigendirections, and (3) adapting the rank dynamically to focus capacity where the signal is strong.

Result: GRIT matches or outperforms existing methods (LoRA, QLoRA) on various benchmarks, while reducing trainable parameters by 46% on average without compromising quality.

Conclusion: GRIT improves the parameter efficiency and stability of fine-tuning procedures, offering a better trade-off between updates and model retention compared to traditional PEFT methods.

Abstract: Parameter-efficient fine-tuning (PEFT) is the default way to adapt LLMs, but widely used LoRA and QLoRA are largely geometry-agnostic: they optimize in fixed, randomly oriented low-rank subspaces with first-order descent, mostly ignoring local loss curvature. This can inflate the effective update budget and amplify drift along weakly constrained directions. We introduce GRIT, a dynamic, curvature-aware LoRA procedure that preserves the LoRA parameterization but: (1) preconditions gradients in rank space using K-FAC as a natural-gradient proxy; (2) periodically reprojects the low-rank basis onto dominant Fisher eigendirections to suppress drift; and (3) adapts the effective rank from the spectrum so capacity concentrates where signal resides. Across instruction-following, comprehension, and reasoning benchmarks on LLaMA backbones, GRIT matches or surpasses LoRA and QLoRA while reducing trainable parameters by 46% on average (25--80% across tasks), without practical quality loss across prompt styles and data mixes. To model forgetting, we fit a curvature-modulated power law. Empirically, GRIT yields lower drift and a better updates-vs-retention frontier than strong PEFT-optimizer baselines (Orthogonal-LoRA, IA3, DoRA, Eff-FT, Shampoo).

</details>


### [155] [Task-Driven Kernel Flows: Label Rank Compression and Laplacian Spectral Filtering](https://arxiv.org/abs/2601.00276)
*Hongxi Li,Chunlin Huang*

Main category: cs.LG

TL;DR: A theory of feature learning for wide L2-regularized networks predicts that supervised learning compresses information, resulting in low-rank solution dynamics bounded by the number of classes.


<details>
  <summary>Details</summary>
Motivation: To understand how supervised feature learning in regularized networks relates to representation compression compared to other learning methods like self-supervision.

Method: The authors develop a kernel ODE to describe spectral evolution and analyze how stochastic gradient descent (SGD) noise influences feature learning, demonstrating low-rank dynamics.

Result: The kernel rank is proven to be bounded by the number of classes in stable states, with SGD noise confined to a task-relevant subspace.

Conclusion: Supervised learning inherently compresses representations, leading to lower rank features, contrasting sharply with the expansive nature of self-supervised approaches.

Abstract: We present a theory of feature learning in wide L2-regularized networks showing that supervised learning is inherently compressive. We derive a kernel ODE that predicts a "water-filling" spectral evolution and prove that for any stable steady state, the kernel rank is bounded by the number of classes ($C$). We further demonstrate that SGD noise is similarly low-rank ($O(C)$), confining dynamics to the task-relevant subspace. This framework unifies the deterministic and stochastic views of alignment and contrasts the low-rank nature of supervised learning with the high-rank, expansive representations of self-supervision.

</details>


### [156] [Can Optimal Transport Improve Federated Inverse Reinforcement Learning?](https://arxiv.org/abs/2601.00309)
*David Millard,Ali Baheri*

Main category: cs.LG

TL;DR: This paper presents a federated approach to inverse reinforcement learning that combines local reward functions using optimal transport, ensuring computational efficiency and privacy while improving global reward estimates.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning a shared reward function across autonomous agents in different environments with constraints like privacy and communication.

Method: The paper utilizes Maximum Entropy IRL locally at each client, followed by fusing the reward functions using a Wasserstein barycenter to capture their geometric properties.

Result: The fusion method yields a more accurate global reward representation compared to traditional parameter averaging in federated learning.

Conclusion: The approach enhances communication efficiency and provides a robust framework for learning shared rewards in heterogeneous multi-agent systems.

Abstract: In robotics and multi-agent systems, fleets of autonomous agents often operate in subtly different environments while pursuing a common high-level objective. Directly pooling their data to learn a shared reward function is typically impractical due to differences in dynamics, privacy constraints, and limited communication bandwidth. This paper introduces an optimal transport-based approach to federated inverse reinforcement learning (IRL). Each client first performs lightweight Maximum Entropy IRL locally, adhering to its computational and privacy limitations. The resulting reward functions are then fused via a Wasserstein barycenter, which considers their underlying geometric structure. We further prove that this barycentric fusion yields a more faithful global reward estimate than conventional parameter averaging methods in federated learning. Overall, this work provides a principled and communication-efficient framework for deriving a shared reward that generalizes across heterogeneous agents and environments.

</details>


### [157] [Quantum King-Ring Domination in Chess: A QAOA Approach](https://arxiv.org/abs/2601.00318)
*Gerhard Stenzel,Michael Kölle,Tobias Rohe,Julian Hager,Leo Sünkel,Maximilian Zorn,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: This paper introduces a benchmark called Quantum King-Ring Domination (QKRD) derived from chess positions to systematically evaluate the Quantum Approximate Optimization Algorithm (QAOA), achieving significant insights into its performance.


<details>
  <summary>Details</summary>
Motivation: To create a more meaningful and structured benchmark for QAOA evaluation, as synthetic random instances fail to reflect real-world problems.

Method: The paper developed QKRD using chess tactical positions with one-hot constraints and spatial locality, tested QAOA configurations, and validated performance intrinsically and against classical methods.

Result: Constraint-preserving mixers performed better in convergence and penalty tuning, warm-start strategies improved both convergence and energy, CVaR optimization performed worse, and QAOA outperformed classical heuristics and random selection.

Conclusion: Structured benchmarks like QKRD enhance algorithmic insights and showcase the advantages of problem-informed QAOA techniques suitable for real-world constraints.

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is extensively benchmarked on synthetic random instances such as MaxCut, TSP, and SAT problems, but these lack semantic structure and human interpretability, offering limited insight into performance on real-world problems with meaningful constraints. We introduce Quantum King-Ring Domination (QKRD), a NISQ-scale benchmark derived from chess tactical positions that provides 5,000 structured instances with one-hot constraints, spatial locality, and 10--40 qubit scale. The benchmark pairs human-interpretable coverage metrics with intrinsic validation against classical heuristics, enabling algorithmic conclusions without external oracles. Using QKRD, we systematically evaluate QAOA design choices and find that constraint-preserving mixers (XY, domain-wall) converge approximately 13 steps faster than standard mixers (p<10^{-7}, d\approx0.5) while eliminating penalty tuning, warm-start strategies reduce convergence by 45 steps (p<10^{-127}, d=3.35) with energy improvements exceeding d=8, and Conditional Value-at-Risk (CVaR) optimization yields an informative negative result with worse energy (p<10^{-40}, d=1.21) and no coverage benefit. Intrinsic validation shows QAOA outperforms greedy heuristics by 12.6\% and random selection by 80.1\%. Our results demonstrate that structured benchmarks reveal advantages of problem-informed QAOA techniques obscured in random instances. We release all code, data, and experimental artifacts for reproducible NISQ algorithm research.

</details>


### [158] [Smart Fault Detection in Nanosatellite Electrical Power System](https://arxiv.org/abs/2601.00335)
*Alireza Rezaee,Niloofar Nobahari,Amin Asgarifar,Farshid Hajati*

Main category: cs.LG

TL;DR: The paper introduces a new fault detection method for nanosatellites' electrical power systems in LEO orbit using neural networks and various machine learning classifiers.


<details>
  <summary>Details</summary>
Motivation: Nanosatellites face significant risks to their electrical power systems due to environmental and mechanical stresses. Precise fault detection is necessary for ensuring operational reliability and avoiding catastrophic failures in space.

Method: The approach utilizes neural networks to simulate fault-free operations based on solar radiation and panel temperature data. Machine learning methods like PCA classification, decision tree, and KNN are employed to classify and diagnose faults.

Result: The system successfully simulates and diagnoses faults like line-to-line faults, open circuits, and component-specific failures using classifiers, demonstrating reliability and adaptability.

Conclusion: Using machine learning methods, reliable fault detection is achieved in nanosatellite power systems, paving the way for enhanced operational stability in LEO orbit without relying on ADCS.

Abstract: This paper presents a new detection method of faults at Nanosatellites' electrical power without an Attitude Determination Control Subsystem (ADCS) at the LEO orbit. Each part of this system is at risk of fault due to pressure tolerance, launcher pressure, and environmental circumstances. Common faults are line to line fault and open circuit for the photovoltaic subsystem, short circuit and open circuit IGBT at DC to DC converter, and regulator fault of the ground battery. The system is simulated without fault based on a neural network using solar radiation and solar panel's surface temperature as input data and current and load as outputs. Finally, using the neural network classifier, different faults are diagnosed by pattern and type of fault. For fault classification, other machine learning methods are also used, such as PCA classification, decision tree, and KNN.

</details>


### [159] [Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models](https://arxiv.org/abs/2601.00391)
*Nouar AlDahoul,Aznul Qalid Md Sabri,Ali Mohammed Mansoor*

Main category: cs.LG

TL;DR: This paper proposes using automatic feature learning methods combining optical flow and deep models to detect humans in aerial videos. Results show high accuracy across methods, with pretrained CNN achieving 98.09% accuracy.


<details>
  <summary>Details</summary>
Motivation: Human detection in videos is vital for various real-life applications but traditional methods relying on handcrafted features struggle with dynamic events and task specificity.

Method: Feature learning leverages optical flow with three deep models: supervised CNN, pretrained CNN feature extractor, and hierarchical extreme learning machine. These models are tested on UCF-ARG aerial dataset.

Result: Pretrained CNN achieved 98.09% average accuracy, S-CNN achieved 95.6% accuracy with softmax, and H-ELM achieved 95.9%. Training time for H-ELM is faster on CPU while S-CNN takes longer with GPU.

Conclusion: Automatic feature learning methods are efficient and robust for human detection in videos, successfully handling dynamic conditions and producing high accuracy.

Abstract: Human detection in videos plays an important role in various real-life applications. Most traditional approaches depend on utilizing handcrafted features, which are problem-dependent and optimal for specific tasks. Moreover, they are highly susceptible to dynamical events such as illumination changes, camera jitter, and variations in object sizes. On the other hand, the proposed feature learning approaches are cheaper and easier because highly abstract and discriminative features can be produced automatically without the need of expert knowledge. In this paper, we utilize automatic feature learning methods, which combine optical flow and three different deep models (i.e., supervised convolutional neural network (S-CNN), pretrained CNN feature extractor, and hierarchical extreme learning machine) for human detection in videos captured using a nonstatic camera on an aerial platform with varying altitudes. The models are trained and tested on the publicly available and highly challenging UCF-ARG aerial dataset. The comparison between these models in terms of training, testing accuracy, and learning speed is analyzed. The performance evaluation considers five human actions (digging, waving, throwing, walking, and running). Experimental results demonstrated that the proposed methods are successful for the human detection task. The pretrained CNN produces an average accuracy of 98.09%. S-CNN produces an average accuracy of 95.6% with softmax and 91.7% with Support Vector Machines (SVM). H-ELM has an average accuracy of 95.9%. Using a normal Central Processing Unit (CPU), H-ELM's training time takes 445 seconds. Learning in S-CNN takes 770 seconds with a high-performance Graphical Processing Unit (GPU).

</details>


### [160] [Bayesian Inverse Games with High-Dimensional Multi-Modal Observations](https://arxiv.org/abs/2601.00696)
*Yash Jain,Xinjie Liu,Lasse Peters,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: The paper introduces a Bayesian inference approach for inverse game problems in multi-agent interactions, providing uncertainty quantification for safer decision-making.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current inverse game methods, which lack uncertainty estimation and can lead to unsafe decisions during autonomous agent planning.

Method: The method trains a structured variational autoencoder paired with a differentiable Nash game solver, allowing multimodal Bayesian inference of hidden agent objectives without requiring labeled objective data.

Result: The framework outperforms maximum likelihood-based approaches by improving inference quality, learning posterior distributions, and supporting safer decision-making; it also enhances accuracy when other sensor modalities are used.

Conclusion: This Bayesian inverse game framework enhances the safety and quality of decision-making in autonomous multi-agent systems, especially when trajectory data is lacking or limited.

Abstract: Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities.

</details>


### [161] [Deep Delta Learning](https://arxiv.org/abs/2601.00417)
*Yifan Zhang,Yifeng Liu,Mengdi Wang,Quanquan Gu*

Main category: cs.LG

TL;DR: This paper introduces Deep Delta Learning (DDL), a new architecture that improves residual networks by using a learnable geometric transformation called the Delta Operator.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of standard residual networks, which can only capture additive feature transformations and struggle to model complex state transitions.

Method: The authors propose a rank-1 transformation of the identity matrix, parameterized by a data-dependent reflection vector and a gating scalar, to enhance residual connections.

Result: The Delta Operator allows layer-wise control over transition dynamics, achieving a balance between identity mapping, projection, and reflection, while improving network capacity for complex tasks.

Conclusion: DDL enhances model expressiveness while retaining the stability of training, making it a significant step forward for improving residual network architectures.

Abstract: The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector $\mathbf{k}(\mathbf{X})$ and a gating scalar $β(\mathbf{X})$. We provide a spectral analysis of this operator, demonstrating that the gate $β(\mathbf{X})$ enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.

</details>


### [162] [E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models](https://arxiv.org/abs/2601.00423)
*Shengjun Zhang,Zhang Zhang,Chensheng Dai,Yueqi Duan*

Main category: cs.LG

TL;DR: This paper introduces E-GRPO, an entropy-aware optimization method for more efficient exploration during flow matching models in reinforcement learning, addressing issues with ambiguous reward signals.


<details>
  <summary>Details</summary>
Motivation: To enhance human preference alignment in flow matching models through reinforcement learning, tackling problems like sparse and ambiguous rewards posed by stochastic sampling steps.

Method: The method involves E-GRPO, which merges low entropy SDE sampling steps into a single high entropy step and applies ODE sampling for the remaining steps. It also introduces multi-step group normalized advantage for better reward computation.

Result: Experimental results across different reward settings show improved effectiveness and exploration efficiency.

Conclusion: E-GRPO provides promising improvements in handling stochasticity and sparse reward signals, leading to more effective reinforcement learning outcomes.

Abstract: Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.

</details>


### [163] [A Comparative Analysis of Interpretable Machine Learning Methods](https://arxiv.org/abs/2601.00428)
*Mattia Billa,Giovanni Orlandi,Veronica Guidetti,Federica Mandreoli*

Main category: cs.LG

TL;DR: The paper studies interpretable ML models, offering a comprehensive evaluation across 216 datasets to guide practitioners in using such models optimally for tabular data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of systematic evaluations of inherently interpretable machine learning models, particularly for tabular data.

Method: The paper employs a large-scale evaluation of 16 interpretable ML methods, analyzing performance on 216 datasets while accounting for dataset characteristics, training time, and robustness under distributional shifts.

Result: Explainable Boosting Machines (EBMs) consistently deliver strong predictive accuracy, though results vary by context, such as non-linear or class-imbalanced scenarios. Models like SR and GOSDT show varied strengths and weaknesses across different regimes.

Conclusion: The paper offers insights into the context-dependent performance of interpretable models, aiming to help practitioners balance predictive accuracy and interpretability better.

Abstract: In recent years, Machine Learning (ML) has seen widespread adoption across a broad range of sectors, including high-stakes domains such as healthcare, finance, and law. This growing reliance has raised increasing concerns regarding model interpretability and accountability, particularly as legal and regulatory frameworks place tighter constraints on using black-box models in critical applications. Although interpretable ML has attracted substantial attention, systematic evaluations of inherently interpretable models, especially for tabular data, remain relatively scarce and often focus primarily on aggregated performance outcomes.
  To address this gap, we present a large-scale comparative evaluation of 16 inherently interpretable methods, ranging from classical linear models and decision trees to more recent approaches such as Explainable Boosting Machines (EBMs), Symbolic Regression (SR), and Generalized Optimal Sparse Decision Trees (GOSDT). Our study spans 216 real-world tabular datasets and goes beyond aggregate rankings by stratifying performance according to structural dataset characteristics, including dimensionality, sample size, linearity, and class imbalance. In addition, we assess training time and robustness under controlled distributional shifts. Our results reveal clear performance hierarchies, especially for regression tasks, where EBMs consistently achieve strong predictive accuracy. At the same time, we show that performance is highly context-dependent: SR and Interpretable Generalized Additive Neural Networks (IGANNs) perform particularly well in non-linear regimes, while GOSDT models exhibit pronounced sensitivity to class imbalance. Overall, these findings provide practical guidance for practitioners seeking a balance between interpretability and predictive performance, and contribute to a deeper empirical understanding of interpretable modeling for tabular data.

</details>


### [164] [A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection](https://arxiv.org/abs/2601.00446)
*Miseon Park,Kijung Yoon*

Main category: cs.LG

TL;DR: The paper investigates time series foundation models (TSFMs) for anomaly detection and their potential for universal application without extensive training.


<details>
  <summary>Details</summary>
Motivation: To enable reliable and scalable anomaly detection in time series data without requiring task-specific adaptations, using foundation models pretrained on diverse datasets.

Method: Through extensive experiments, the authors assess zero-shot inference, full model adaptation, and parameter-efficient fine-tuning (PEFT) techniques such as LoRA, OFT, and HRA with TSFMs.

Result: TSFMs outperform task-specific baselines, showing improvements in metrics like AUC-PR and VUS-PR, especially under class imbalance. PEFT methods often match or exceed full fine-tuning while reducing computational demands.

Conclusion: TSFMs are effective, adaptable, and resource-efficient backbone models for anomaly detection and can extend their utility even to domains beyond their original pretraining purpose.

Abstract: Time series anomaly detection is essential for the reliable operation of complex systems, but most existing methods require extensive task-specific training. We explore whether time series foundation models (TSFMs), pretrained on large heterogeneous data, can serve as universal backbones for anomaly detection. Through systematic experiments across multiple benchmarks, we compare zero-shot inference, full model adaptation, and parameter-efficient fine-tuning (PEFT) strategies. Our results demonstrate that TSFMs outperform task-specific baselines, achieving notable gains in AUC-PR and VUS-PR, particularly under severe class imbalance. Moreover, PEFT methods such as LoRA, OFT, and HRA not only reduce computational cost but also match or surpass full fine-tuning in most cases, indicating that TSFMs can be efficiently adapted for anomaly detection, even when pretrained for forecasting. These findings position TSFMs as promising general-purpose models for scalable and efficient time series anomaly detection.

</details>


### [165] [Controllable Concept Bottleneck Models](https://arxiv.org/abs/2601.00451)
*Hongbin Lin,Chenyang Ren,Juangui Xu,Zhengyu Hu,Cheng-Long Wang,Yao Shu,Hui Xiong,Jingfeng Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: This paper introduces Controllable Concept Bottleneck Models (CCBMs) to address the challenge of adapting Concept Bottleneck Models (CBMs) for dynamic real-world applications. It focuses on providing efficient model editing at multiple granularities without the need for retraining.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between traditional static Concept Bottleneck Models (CBMs) and the need for dynamic, maintainable models in real-world scenarios, where data can change and require updates like data removal, correction, or addition.

Method: The authors propose CCBMs, which allow three levels of model editing (concept-label, concept, and data levels) using closed-form approximations derived from influence functions, avoiding the need to retrain the model from scratch.

Result: CCBMs demonstrated efficiency and adaptability in experiments, providing practical benefits for dynamic and trustworthy model usage.

Conclusion: CCBMs successfully address the limitations of existing CBMs by introducing an adaptable and efficient framework for real-world applications, making them more suitable for evolving environments.

Abstract: Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a human-understandable concept layer. However, most previous studies focused on static scenarios where the data and concepts are assumed to be fixed and clean. In real-world applications, deployed models require continuous maintenance: we often need to remove erroneous or sensitive data (unlearning), correct mislabeled concepts, or incorporate newly acquired samples (incremental learning) to adapt to evolving environments. Thus, deriving efficient editable CBMs without retraining from scratch remains a significant challenge, particularly in large-scale applications. To address these challenges, we propose Controllable Concept Bottleneck Models (CCBMs). Specifically, CCBMs support three granularities of model editing: concept-label-level, concept-level, and data-level, the latter of which encompasses both data removal and data addition. CCBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for retraining. Experimental results demonstrate the efficiency and adaptability of our CCBMs, affirming their practical value in enabling dynamic and trustworthy CBMs.

</details>


### [166] [Imitation from Observations with Trajectory-Level Generative Embeddings](https://arxiv.org/abs/2601.00452)
*Yongtao Qu,Shangzhe Li,Weitong Zhang*

Main category: cs.LG

TL;DR: The paper proposes TGE, an approach for offline imitation learning from observations that uses a temporal diffusion model to overcome challenges with scarce expert demonstrations and suboptimal data.


<details>
  <summary>Details</summary>
Motivation: Address issues in offline imitation learning from observations (LfO), where expert data is limited and offline suboptimal data differs significantly from expert behavior.

Method: Introduces TGE, a trajectory-level generative embedding using a temporal diffusion model to construct a smooth surrogate reward for learning from imperfect offline data.

Result: TGE consistently outperforms or matches existing methods on D4RL locomotion and manipulation benchmarks, demonstrating effectiveness in bridging the support gap.

Conclusion: TGE improves long-horizon learning by leveraging smooth latent space geometry of diffusion embeddings, offering robust signals even with suboptimal offline trajectories.

Abstract: We consider the offline imitation learning from observations (LfO) where the expert demonstrations are scarce and the available offline suboptimal data are far from the expert behavior. Many existing distribution-matching approaches struggle in this regime because they impose strict support constraints and rely on brittle one-step models, making it hard to extract useful signal from imperfect data. To tackle this challenge, we propose TGE, a trajectory-level generative embedding for offline LfO that constructs a dense, smooth surrogate reward by estimating expert state density in the latent space of a temporal diffusion model trained on offline trajectory data. By leveraging the smooth geometry of the learned diffusion embedding, TGE captures long-horizon temporal dynamics and effectively bridges the gap between disjoint supports, ensuring a robust learning signal even when offline data is distributionally distinct from the expert. Empirically, the proposed approach consistently matches or outperforms prior offline LfO methods across a range of D4RL locomotion and manipulation benchmarks.

</details>


### [167] [Deep Networks Learn Deep Hierarchical Models](https://arxiv.org/abs/2601.00455)
*Amit Daniely*

Main category: cs.LG

TL;DR: The paper explores how layerwise SGD on residual networks can efficiently learn a class of hierarchical models while extending prior learning frameworks.


<details>
  <summary>Details</summary>
Motivation: To investigate a new class of hierarchical models that expands the learning capabilities of deep learning systems and reaches the depth limit of efficient learnability.

Method: The method involves layerwise stochastic gradient descent (SGD) on residual networks to capture label hierarchies with increasing complexity.

Result: The proposed model class can learn deeper hierarchical structures requiring polynomial depth, surpassing the limitations of prior models constrained to log-depth circuits.

Conclusion: Hierarchical models may provide insights into deep learning's success, and their design is analogous to human teaching processes where granular labels act as hints for efficient learning.

Abstract: We consider supervised learning with $n$ labels and show that layerwise SGD on residual networks can efficiently learn a class of hierarchical models. This model class assumes the existence of an (unknown) label hierarchy $L_1 \subseteq L_2 \subseteq \dots \subseteq L_r = [n]$, where labels in $L_1$ are simple functions of the input, while for $i > 1$, labels in $L_i$ are simple functions of simpler labels.
  Our class surpasses models that were previously shown to be learnable by deep learning algorithms, in the sense that it reaches the depth limit of efficient learnability. That is, there are models in this class that require polynomial depth to express, whereas previous models can be computed by log-depth circuits.
  Furthermore, we suggest that learnability of such hierarchical models might eventually form a basis for understanding deep learning. Beyond their natural fit for domains where deep learning excels, we argue that the mere existence of human ``teachers" supports the hypothesis that hierarchical structures are inherently available. By providing granular labels, teachers effectively reveal ``hints'' or ``snippets'' of the internal algorithms used by the brain. We formalize this intuition, showing that in a simplified model where a teacher is partially aware of their internal logic, a hierarchical structure emerges that facilitates efficient learnability.

</details>


### [168] [Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations](https://arxiv.org/abs/2601.00457)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: Orthogonality loss as a geometric regularizer for Mixture-of-Experts (MoE) models is ineffective in improving expert diversity or performance.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of geometric regularization, specifically orthogonality loss, on the diversity and performance of MoE models.

Method: The authors introduced orthogonality loss as a regularization technique to enforce diversity among experts, then analyzed the effects on weight-space overlap, activation-space overlap, and performance across different datasets and regularization strengths.

Result: Orthogonality loss failed to reduce weight-space overlap (in some cases increasing by 114%) and did not address activation-space overlap (~0.6 remained). Its impact on performance was inconsistent and varied greatly across datasets.

Conclusion: Weight-space regularization via orthogonality loss fails both as a geometric regularizer and as a mechanism to reliably enhance performance in MoE models, making it an unsuitable strategy for improving expert diversity.

Abstract: Mixture-of-Experts (MoE) models achieve efficiency through sparse activation, but the role of geometric regularization in expert specialization remains unclear. We apply orthogonality loss to enforce expert diversity and find it fails on multiple fronts: it does not reduce weight-space overlap (MSO actually increases by up to 114%), activation-space overlap remains high (~0.6) regardless of regularization, and effects on performance are inconsistent -- marginal improvement on WikiText-103 (-0.9%), slight degradation on TinyStories (+0.9%), and highly variable results on PTB (std > 1.0). Our analysis across 7 regularization strengths reveals no significant correlation (r = -0.293, p = 0.523) between weight and activation orthogonality. These findings demonstrate that weight-space regularization neither achieves its geometric goal nor reliably improves performance, making it unsuitable for MoE diversity.

</details>


### [169] [Detecting Spike Wave Discharges (SWD) using 1-dimensional Residual UNet](https://arxiv.org/abs/2601.00459)
*Saurav Sengupta,Scott Kilianski,Suchetha Sharma,Sakina Lashkeri,Ashley McHugh,Mark Beenhakker,Donald E. Brown*

Main category: cs.LG

TL;DR: Automatic labeling of Spike Wave Discharges (SWD) in EEG using machine learning reduces manual workload, with AugUNet1D showing superior performance compared to other classifiers.


<details>
  <summary>Details</summary>
Motivation: Manually labeling EEG events like SWDs is labor-intensive and inefficient, particularly for prolonged recordings over weeks to months.

Method: Comparison of 14 machine learning classifiers on a large, manually annotated EEG dataset. Augmentation techniques are applied to improve the performance of 1D UNet, leading to the creation of AugUNet1D.

Result: AugUNet1D outperformed other classifiers, including the Twin Peaks algorithm, in accurately detecting SWDs with features similar to manual annotations.

Conclusion: AugUNet1D demonstrates superior performance in automated SWD labeling, offering a pretrained or untrained model for public use and improving the efficiency of EEG analysis.

Abstract: The manual labeling of events in electroencephalography (EEG) records is time-consuming. This is especially true when EEG recordings are taken continuously over weeks to months. Therefore, a method to automatically label pertinent EEG events reduces the manual workload. Spike wave discharges (SWD), which are the electrographic hallmark of absence seizures, are EEG events that are often labeled manually. While some previous studies have utilized machine learning to automatically segment and classify EEG signals like SWDs, they can be improved. Here we compare the performance of 14 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs. We find that a 1D UNet performs best for labeling SWDs in this dataset. We also improve the 1D UNet by augmenting our training data and determine that scaling showed the greatest benefit of all augmentation procedures applied. We then compare the 1D UNet with data augmentation, AugUNet1D, against a recently published time- and frequency-based algorithmic approach called "Twin Peaks". AugUNet1D showed superior performance and detected events with more similar features to the SWDs labeled manually. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for others users.

</details>


### [170] [Neural Chains and Discrete Dynamical Systems](https://arxiv.org/abs/2601.00473)
*Sauro Succi,Abhisek Ganguly,Santosh Ansumali*

Main category: cs.LG

TL;DR: This paper compares non-self-attention transformer architectures ('neural chains') and numerical solutions of PDEs (partial differential equations) to Physics-Informed Neural Networks (PINNs). It critiques the random matrix approach of PINNs for its inefficiency and lack of explainability, while highlighting their potential for high-dimensional problems.


<details>
  <summary>Details</summary>
Motivation: To explore the relationship between transformer architecture applications ('neural chains') and numerical approaches for solving differential equations, and to assess the strengths and weaknesses of PINNs for these tasks.

Method: The authors analyze and compare the numerical solution of certain differential equations (Burgers and Eikonal equations) via standard finite-difference discretization methods and PINN learning. They studied the behavior of random versus structured matrices in these approaches.

Result: Standard numerical discretization and PINNs achieve similar results about the system dynamics. However, PINNs use random matrices that lack physical transparency and require more computational resources. This makes PINNs less efficient in one-dimensional problems, but possibilities remain for higher-dimensional use cases.

Conclusion: PINNs and ML models offer alternate ways to study system dynamics, but traditional numerical techniques remain more efficient for simpler (one-dimensional) problems. Further exploration is suggested for high-dimensional problem contexts.

Abstract: We inspect the analogy between machine-learning (ML) applications based on the transformer architecture without self-attention, {\it neural chains} hereafter, and discrete dynamical systems associated with discretised versions of neural integral and partial differential equations (NIE, PDE). A comparative analysis of the numerical solution of the (viscid and inviscid) Burgers and Eikonal equations via standard numerical discretization (also cast in terms of neural chains) and via PINN's learning is presented and commented on. It is found that standard numerical discretization and PINN learning provide two different paths to acquire essentially the same knowledge about the dynamics of the system. PINN learning proceeds through random matrices which bear no direct relation to the highly structured matrices associated with finite-difference (FD) procedures. Random matrices leading to acceptable solutions are far more numerous than the unique tridiagonal form in matrix space, which explains why the PINN search typically lands on the random ensemble. The price is a much larger number of parameters, causing lack of physical transparency (explainability) as well as large training costs with no counterpart in the FD procedure. However, our results refer to one-dimensional dynamic problems, hence they don't rule out the possibility that PINNs and ML in general, may offer better strategies for high-dimensional problems.

</details>


### [171] [When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents](https://arxiv.org/abs/2601.00513)
*Laksh Advani*

Main category: cs.LG

TL;DR: This study identifies a reliability crisis in small language models, revealing that many 'correct' answers are based on flawed reasoning. It introduces the Reasoning Integrity Score (RIS) as a new metric and describes how retrieval-augmented generation improves integrity while meta-cognitive interventions may harm it.


<details>
  <summary>Details</summary>
Motivation: Autonomous deployment of small language models requires trust in their reasoning processes, not just their accuracy.

Method: The study analyzed 10,734 reasoning traces across three models and introduced the Reasoning Integrity Score (RIS) to assess reasoning quality. It tested the effects of retrieval-augmented generation (RAG) and meta-cognition techniques, validated with a neural classifier.

Result: The Reasoning Integrity Score shows that while RAG improves reasoning, meta-cognitive strategies often lead to reduced performance in smaller models. A mechanistic study explained how RAG utilizes external evidence to reduce errors.

Conclusion: Process-based evaluation, as opposed to outcome-based, is critical for deploying trustworthy language models. Traditional accuracy metrics fail to capture flawed reasoning, underscoring the value of reasoning integrity metrics like RIS.

Abstract: Deploying small language models (7-9B parameters) as autonomous agents requires trust in their reasoning, not just their outputs. We reveal a critical reliability crisis: 50-69\% of correct answers from these models contain fundamentally flawed reasoning -- a ``Right-for-Wrong-Reasons'' phenomenon invisible to standard accuracy metrics. Through analysis of 10,734 reasoning traces across three models and diverse tasks, we introduce the Reasoning Integrity Score (RIS), a process-based metric validated with substantial inter-rater agreement ($κ=0.657$). Conventional practices are challenged by our findings: while retrieval-augmented generation (RAG) significantly improves reasoning integrity (Cohen's $d=0.23$--$0.93$), meta-cognitive interventions like self-critique often harm performance ($d=-0.14$ to $-0.33$) in small models on the evaluated tasks. Mechanistic analysis reveals RAG succeeds by grounding calculations in external evidence, reducing errors by 7.6\%, while meta-cognition amplifies confusion without sufficient model capacity. To enable deployment, verification capabilities are distilled into a neural classifier achieving 0.86 F1-score with 100$\times$ speedup. These results underscore the necessity of process-based verification for trustworthy agents: accuracy alone is dangerously insufficient when models can be right for entirely wrong reasons.

</details>


### [172] [Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI](https://arxiv.org/abs/2601.00516)
*Laksh Advani*

Main category: cs.LG

TL;DR: Trajectory Guard is introduced for anomaly detection in multi-step action plans of autonomous LLM agents, achieving high accuracy and fast inference.


<details>
  <summary>Details</summary>
Motivation: Failures in multi-step plans of autonomous LLM agents can arise due to contextual misalignment or structural incoherence, and existing anomaly detection methods are inadequate to address these issues effectively.

Method: A Siamese Recurrent Autoencoder is proposed with a hybrid loss function combining contrastive learning for task-trajectory alignment and reconstruction for sequential validity.

Result: Trajectory Guard achieves F1-scores of 0.88-0.94 and recall of 0.86-0.92 on various benchmarks, demonstrating superior performance over existing methods with low inference latency.

Conclusion: Trajectory Guard provides a reliable and efficient solution for detecting anomalies in multi-step plans, enabling real-time safety verification and improved handling of LLM agent failures.

Abstract: Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both "wrong plan for this task" and "malformed plan structure." On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.

</details>


### [173] [A Sparse-Attention Deep Learning Model Integrating Heterogeneous Multimodal Features for Parkinson's Disease Severity Profiling](https://arxiv.org/abs/2601.00519)
*Dristi Datta,Tanmoy Debnath,Minh Chau,Manoranjan Paul,Gourab Adhikary,Md Geaur Rahman*

Main category: cs.LG

TL;DR: The paper introduces SAFN, an interpretable multimodal deep learning model for Parkinson's disease profiling, achieving high accuracy and interpretability in clinical diagnostics.


<details>
  <summary>Details</summary>
Motivation: The study aims to address limitations of current computational models in integrating biological and clinical data for profiling Parkinson's disease, such as interpretability, class imbalance, and the fusion of heterogeneous data types.

Method: The SAFN framework combines modality-specific encoders with a cross-attention mechanism and a sparsity-constrained attention-gating fusion layer. It uses class-balanced focal loss to handle dataset imbalance while learning from multimodal inputs like MRI measures and clinical assessments.

Result: The evaluation on 703 participants shows SAFN outperforming existing models with a 0.98 accuracy and 1.00 PR-AUC, demonstrating robust multimodal profiling and coherent clinical interpretability.

Conclusion: SAFN offers a transparent and reproducible approach to computational profiling, enabling improved diagnostic decision-making based on multimodal data integration for neurodegenerative diseases.

Abstract: Characterising the heterogeneous presentation of Parkinson's disease (PD) requires integrating biological and clinical markers within a unified predictive framework. While multimodal data provide complementary information, many existing computational models struggle with interpretability, class imbalance, or effective fusion of high-dimensional imaging and tabular clinical features. To address these limitations, we propose the Class-Weighted Sparse-Attention Fusion Network (SAFN), an interpretable deep learning framework for robust multimodal profiling. SAFN integrates MRI cortical thickness, MRI volumetric measures, clinical assessments, and demographic variables using modality-specific encoders and a symmetric cross-attention mechanism that captures nonlinear interactions between imaging and clinical representations. A sparsity-constrained attention-gating fusion layer dynamically prioritises informative modalities, while a class-balanced focal loss (beta = 0.999, gamma = 1.5) mitigates dataset imbalance without synthetic oversampling. Evaluated on 703 participants (570 PD, 133 healthy controls) from the Parkinson's Progression Markers Initiative using subject-wise five-fold cross-validation, SAFN achieves an accuracy of 0.98 plus or minus 0.02 and a PR-AUC of 1.00 plus or minus 0.00, outperforming established machine learning and deep learning baselines. Interpretability analysis shows a clinically coherent decision process, with approximately 60 percent of predictive weight assigned to clinical assessments, consistent with Movement Disorder Society diagnostic principles. SAFN provides a reproducible and transparent multimodal modelling paradigm for computational profiling of neurodegenerative disease.

</details>


### [174] [Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting: A Model Compression Study](https://arxiv.org/abs/2601.00525)
*Ravi Teja Pagidoju*

Main category: cs.LG

TL;DR: This paper investigates LSTM compression for retail sales forecasts, showing smaller models (64 hidden units) achieve better accuracy and reduced computational needs.


<details>
  <summary>Details</summary>
Motivation: Retail industries, especially mid to small-sized ones, face challenges with computational resource needs of standard LSTM models despite their prediction accuracy.

Method: The paper reduces LSTM's hidden units incrementally from 128 to 16 and evaluates the accuracy using Kaggle Store Item Demand Forecasting dataset.

Result: Reducing LSTM units to 64 improved prediction accuracy (MAPE reduced from 23.6% to 12.4%) and decreased model size by 73%.

Conclusion: Smaller LSTMs outperform larger ones in accuracy and computation efficiency, benefiting resource-constrained contexts.

Abstract: Standard LSTM(Long Short-Term Memory) neural networks provide accurate predictions for sales data in the retail industry, but require a lot of computing power. It can be challenging especially for mid to small retail industries. This paper examines LSTM model compression by gradually reducing the number of hidden units from 128 to 16. We used the Kaggle Store Item Demand Forecasting dataset, which has 913,000 daily sales records from 10 stores and 50 items, to look at the trade-off between model size and how accurate the predictions are. Experiments show that lowering the number of hidden LSTM units to 64 maintains the same level of accuracy while also improving it. The mean absolute percentage error (MAPE) ranges from 23.6% for the full 128-unit model to 12.4% for the 64-unit model. The optimized model is 73% smaller (from 280KB to 76KB) and 47% more accurate. These results show that larger models do not always achieve better results.

</details>


### [175] [Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization](https://arxiv.org/abs/2601.00527)
*Ravi Teja Pagidoju,Shriya Agarwal*

Main category: cs.LG

TL;DR: The paper proposes a cloud-native system using diffusion models to automate and optimize retail planogram creation.


<details>
  <summary>Details</summary>
Motivation: Creating planograms for retail is time-consuming and expensive, taking roughly 30 hours per complex layout, prompting the need for automation.

Method: The system utilizes a cloud-native architecture combining diffusion models (embedding retail-specific constraints) with AWS-based training and edge deployment for real-time use.

Result: It reduces planogram design time by 98.3% (30 to 0.5 hours), achieves 94.4% constraint satisfaction, lowers costs by 97.5%, and scales to 10,000 concurrent requests.

Conclusion: The study showcases the effectiveness of generative AI for automating and optimizing retail space planning efficiently and economically.

Abstract: Planogram creation is a significant challenge for retail, requiring an average of 30 hours per complex layout. This paper introduces a cloud-native architecture using diffusion models to automatically generate store-specific planograms. Unlike conventional optimization methods that reorganize existing layouts, our system learns from successful shelf arrangements across multiple retail locations to create new planogram configurations. The architecture combines cloud-based model training via AWS with edge deployment for real-time inference. The diffusion model integrates retail-specific constraints through a modified loss function. Simulation-based analysis demonstrates the system reduces planogram design time by 98.3% (from 30 to 0.5 hours) while achieving 94.4% constraint satisfaction. Economic analysis reveals a 97.5% reduction in creation expenses with a 4.4-month break-even period. The cloud-native architecture scales linearly, supporting up to 10,000 concurrent store requests. This work demonstrates the viability of generative AI for automated retail space optimization.

</details>


### [176] [Entropy Production in Machine Learning Under Fokker-Planck Probability Flow](https://arxiv.org/abs/2601.00554)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: The paper proposes an entropy-based retraining framework for machine learning models in nonstationary environments to balance accuracy and retraining frequency.


<details>
  <summary>Details</summary>
Motivation: To address performance degradation in machine learning models caused by data drift in nonstationary environments and balance retraining with operational costs.

Method: A framework grounded in nonequilibrium stochastic dynamics, employing a Fokker-Planck equation and entropy balance to detect data drift and trigger retraining based on accumulated entropy mismatch.

Result: The proposed entropy-triggered retraining reduces retraining events significantly while maintaining predictive performance comparable to high-frequency retraining.

Conclusion: Entropy-triggered retraining is an effective, label-free strategy for managing data drift, allowing for reduced retraining frequency without sacrificing model performance.

Abstract: Machine learning models deployed in nonstationary environments experience performance degradation due to data drift. While many drift detection heuristics exist, most lack a principled dynamical interpretation and provide limited guidance on how retraining frequency should be balanced against operational cost. In this work, we propose an entropy--based retraining framework grounded in nonequilibrium stochastic dynamics. Modeling deployment--time data drift as probability flow governed by a Fokker--Planck equation, we quantify model--data mismatch using a time--evolving Kullback--Leibler divergence. We show that the time derivative of this mismatch admits an entropy--balance decomposition featuring a nonnegative entropy production term driven by probability currents. This interpretation motivates entropy--triggered retraining as a label--free intervention strategy that responds to accumulated mismatch rather than delayed performance collapse. In a controlled nonstationary classification experiment, entropy--triggered retraining achieves predictive performance comparable to high--frequency retraining while reducing retraining events by an order of magnitude relative to daily and label--based policies.

</details>


### [177] [Adversarial Samples Are Not Created Equal](https://arxiv.org/abs/2601.00577)
*Jennifer Crawford,Amol Khanna,Fred Lu,Amy R. Wagoner,Stella Biderman,Andre T. Nguyen,Edward Raff*

Main category: cs.LG

TL;DR: The paper discusses enhancing adversarial robustness evaluation by differentiating between two types of adversarial samples using non-robust features.


<details>
  <summary>Details</summary>
Motivation: There is a need to better understand adversarial weaknesses in neural networks, especially regarding samples exploiting brittle but predictive features versus those that do not.

Method: The authors propose an ensemble-based metric to measure the manipulation of non-robust features by adversarial perturbations.

Result: They analyze adversarial samples using the new metric and revisit phenomena like sharpness-aware minimization and differences in training robustness.

Conclusion: Differentiating adversarial samples provides insights into adversarial robustness and existing phenomena, improving neural network evaluation methods.

Abstract: Over the past decade, numerous theories have been proposed to explain the widespread vulnerability of deep neural networks to adversarial evasion attacks. Among these, the theory of non-robust features proposed by Ilyas et al. has been widely accepted, showing that brittle but predictive features of the data distribution can be directly exploited by attackers. However, this theory overlooks adversarial samples that do not directly utilize these features. In this work, we advocate that these two kinds of samples - those which use use brittle but predictive features and those that do not - comprise two types of adversarial weaknesses and should be differentiated when evaluating adversarial robustness. For this purpose, we propose an ensemble-based metric to measure the manipulation of non-robust features by adversarial perturbations and use this metric to analyze the makeup of adversarial samples generated by attackers. This new perspective also allows us to re-examine multiple phenomena, including the impact of sharpness-aware minimization on adversarial robustness and the robustness gap observed between adversarially training and standard training on robust datasets.

</details>


### [178] [Learning to be Reproducible: Custom Loss Design for Robust Neural Networks](https://arxiv.org/abs/2601.00578)
*Waqas Ahmed,Sheeba Samuel,Kevin Coakley,Birgitta Koenig-Ries,Odd Erik Gundersen*

Main category: cs.LG

TL;DR: The study introduces a Custom Loss Function (CLF) that addresses the variability in performance of deep learning models due to stochastic factors, showing improved training robustness without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to mitigate the inconsistent and unreliable performance of deep learning models caused by stochastic factors during training.

Method: The method involves the introduction and fine-tuning of a Custom Loss Function (CLF) designed to reduce sensitivity to factors like weight initialization and data shuffling.

Result: Experiments show that CLF improves the robustness of model training across diverse architectures for image classification and time series forecasting.

Conclusion: CLF is effective in achieving more stable, reliable, and trustworthy neural networks, enhancing reproducibility and reliability in deep learning.

Abstract: To enhance the reproducibility and reliability of deep learning models, we address a critical gap in current training methodologies: the lack of mechanisms that ensure consistent and robust performance across runs. Our empirical analysis reveals that even under controlled initialization and training conditions, the accuracy of the model can exhibit significant variability. To address this issue, we propose a Custom Loss Function (CLF) that reduces the sensitivity of training outcomes to stochastic factors such as weight initialization and data shuffling. By fine-tuning its parameters, CLF explicitly balances predictive accuracy with training stability, leading to more consistent and reliable model performance. Extensive experiments across diverse architectures for both image classification and time series forecasting demonstrate that our approach significantly improves training robustness without sacrificing predictive performance. These results establish CLF as an effective and efficient strategy for developing more stable, reliable and trustworthy neural networks.

</details>


### [179] [HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts](https://arxiv.org/abs/2601.00583)
*Zihan Fang,Zheng Lin,Senkang Hu,Yanan Ma,Yihang Tao,Yiqin Deng,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: The paper introduces HFedMoE, a federated learning framework that uses Mixture-of-Experts (MoE) models to efficiently fine-tune large language models on resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: To address the impracticality of training large language models on resource-limited devices in federated learning settings and overcome challenges related to expert selection, resource heterogeneity, and global aggregation.

Method: The paper proposes HFedMoE, which selects experts based on their contributions to fine-tuning performance, adapts expert subsets to client constraints using an information bottleneck approach, and applies a sparsity-aware aggregation strategy.

Result: HFedMoE achieves better training accuracy and faster convergence than existing benchmarks.

Conclusion: HFedMoE is a computation-efficient framework for federated fine-tuning of large language models, overcoming key challenges in MoE-based training.

Abstract: While federated learning (FL) enables fine-tuning of large language models (LLMs) without compromising data privacy, the substantial size of an LLM renders on-device training impractical for resource-constrained clients, such as mobile devices. Thus, Mixture-of-Experts (MoE) models have emerged as a computation-efficient solution, which activates only a sparse subset of experts during model training to reduce computing burden without sacrificing performance. Though integrating MoE into FL fine-tuning holds significant potential, it still encounters three key challenges: i) selecting appropriate experts for clients remains challenging due to the lack of a reliable metric to measure each expert's impact on local fine-tuning performance, ii) the heterogeneous computing resources across clients severely hinder MoE-based LLM fine-tuning, as dynamic expert activations across diverse input samples can overwhelm resource-constrained devices, and iii) client-specific expert subsets and routing preference undermine global aggregation, where misaligned expert updates and inconsistent gating networks in troduce destructive interference. To address these challenges, we propose HFedMoE, a heterogeneous MoE-based FL fine-tuning framework that customizes a subset of experts to each client for computation-efficient LLM fine-tuning. Specifically, HFedMoE identifies the expert importance based on its contributions to fine-tuning performance, and then adaptively selects a subset of experts from an information bottleneck perspective to align with each client' s computing budget. A sparsity-aware model aggregation strategy is also designed to aggregate the actively fine-tuned experts and gating parameters with importance weighted contributions. Extensive experiments demonstrate that HFedMoE outperforms state-of-the-art benchmarks in training accuracy and convergence speed.

</details>


### [180] [Cycling Race Time Prediction: A Personalized Machine Learning Approach Using Route Topology and Training Load](https://arxiv.org/abs/2601.00604)
*Francisco Aguilera Moreno*

Main category: cs.LG

TL;DR: The paper introduces a machine learning model predicting cycling duration using route topology and fitness metrics, achieving superior performance compared to topology-only models.


<details>
  <summary>Details</summary>
Motivation: Existing cycling prediction models depend on complex and impractical physical parameters, creating challenges for amateur cyclists during training or event planning.

Method: A machine learning approach leverages historical cycling data and combines route topology with fitness metrics to predict cycling duration, evaluated using a single-athlete dataset in an N-of-1 study design.

Result: Lasso regression with combined features (Topology + Fitness metrics) achieves MAE=6.60 minutes and R2=0.922; integrating fitness metrics reduces error by 14%.

Conclusion: Physiological state is critical to performance prediction, making the model useful for dynamic race planning while simplifying input requirements for amateur cyclists.

Abstract: Predicting cycling duration for a given route is essential for training planning and event preparation. Existing solutions rely on physics-based models that require extensive parameterization, including aerodynamic drag coefficients and real-time wind forecasts, parameters impractical for most amateur cyclists. This work presents a machine learning approach that predicts ride duration using route topology features combined with the athlete's current fitness state derived from training load metrics. The model learns athlete-specific performance patterns from historical data, substituting complex physical measurements with historical performance proxies. We evaluate the approach using a single-athlete dataset (N=96 rides) in an N-of-1 study design. After rigorous feature engineering to eliminate data leakage, we find that Lasso regression with Topology + Fitness features achieves MAE=6.60 minutes and R2=0.922. Notably, integrating fitness metrics (CTL, ATL) reduces error by 14% compared to topology alone (MAE=7.66 min), demonstrating that physiological state meaningfully constrains performance even in self-paced efforts. Progressive checkpoint predictions enable dynamic race planning as route difficulty becomes apparent.

</details>


### [181] [Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning](https://arxiv.org/abs/2601.00607)
*Sonia Khetarpaul,P Y Sharan*

Main category: cs.LG

TL;DR: This paper introduces a graph-based reinforcement learning framework for efficient taxi placement in smart cities, integrating traffic data and mobility patterns to optimize urban transportation.


<details>
  <summary>Details</summary>
Motivation: Conventional taxi hotspot prediction models inadequately address dynamic factors like traffic congestion and public events.

Method: A traffic-aware GNN-based RL framework models the road network as a graph to recommend taxi placements, optimizing metrics like passenger waiting time and travel distance.

Result: Using simulated Delhi data, the model reduced passenger waiting time by 56% and travel distance by 38% compared to stochastic methods.

Conclusion: The framework significantly improves taxi placement efficiency, is adaptable to multi-modal transport, and enhances real-time mobility optimization in smart cities.

Abstract: In the context of smart city transportation, efficient matching of taxi supply with passenger demand requires real-time integration of urban traffic network data and mobility patterns. Conventional taxi hotspot prediction models often rely solely on historical demand, overlooking dynamic influences such as traffic congestion, road incidents, and public events. This paper presents a traffic-aware, graph-based reinforcement learning (RL) framework for optimal taxi placement in metropolitan environments. The urban road network is modeled as a graph where intersections represent nodes, road segments serve as edges, and node attributes capture historical demand, event proximity, and real-time congestion scores obtained from live traffic APIs. Graph Neural Network (GNN) embeddings are employed to encode spatial-temporal dependencies within the traffic network, which are then used by a Q-learning agent to recommend optimal taxi hotspots. The reward mechanism jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance. Experiments on a simulated Delhi taxi dataset, generated using real geospatial boundaries and historic ride-hailing request patterns, demonstrate that the proposed model reduced passenger waiting time by about 56% and reduced travel distance by 38% compared to baseline stochastic selection. The proposed approach is adaptable to multi-modal transport systems and can be integrated into smart city platforms for real-time urban mobility optimization.

</details>


### [182] [TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications](https://arxiv.org/abs/2601.00691)
*Mohamed Trabelsi,Huseyin Uzunalioglu*

Main category: cs.LG

TL;DR: The paper introduces TeleDoCTR, a novel telecom-specific system to automate and enhance ticket troubleshooting by combining ranking and generative models, achieving superior results on a real-world dataset.


<details>
  <summary>Details</summary>
Motivation: Ticket troubleshooting, especially in telecom, is complex and time-consuming due to the diversity of tickets and dependence on human expertise, which delays resolution and hampers efficiency.

Method: The proposed system, TeleDoCTR, incorporates domain-specific ranking and generative models to automate ticket routing, retrieval of similar historical tickets, and generation of detailed fault analysis reports.

Result: TeleDoCTR demonstrates superior performance compared to existing methods on a real-world telecom dataset, significantly improving troubleshooting accuracy and efficiency.

Conclusion: TeleDoCTR effectively streamlines and automates telecom ticket troubleshooting, reducing dependency on human experts and improving overall operational efficiency.

Abstract: Ticket troubleshooting refers to the process of analyzing and resolving problems that are reported through a ticketing system. In large organizations offering a wide range of services, this task is highly complex due to the diversity of submitted tickets and the need for specialized domain knowledge. In particular, troubleshooting in telecommunications (telecom) is a very time-consuming task as it requires experts to interpret ticket content, consult documentation, and search historical records to identify appropriate resolutions. This human-intensive approach not only delays issue resolution but also hinders overall operational efficiency. To enhance the effectiveness and efficiency of ticket troubleshooting in telecom, we propose TeleDoCTR, a novel telecom-related, domain-specific, and contextual troubleshooting system tailored for end-to-end ticket resolution in telecom. TeleDoCTR integrates both domain-specific ranking and generative models to automate key steps of the troubleshooting workflow which are: routing tickets to the appropriate expert team responsible for resolving the ticket (classification task), retrieving contextually and semantically similar historical tickets (retrieval task), and generating a detailed fault analysis report outlining the issue, root cause, and potential solutions (generation task). We evaluate TeleDoCTR on a real-world dataset from a telecom infrastructure and demonstrate that it achieves superior performance over existing state-of-the-art methods, significantly enhancing the accuracy and efficiency of the troubleshooting process.

</details>


### [183] [Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization](https://arxiv.org/abs/2601.00611)
*Hareshkumar Jadav,Ranveer Singh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: This paper focuses on maximizing a nonnegative, non-monotone γ-weakly DR-submodular function under constraints, presenting an approximation algorithm with smooth guarantees dependent on γ.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge of optimizing non-monotone submodular functions, particularly γ-weakly DR-submodular functions, which generalize classic DR-submodular cases, to expand their applicability in real-world scenarios.

Method: The authors proposed a combination of a Frank-Wolfe-guided continuous-greedy framework and a γ-aware double-greedy step to effectively address non-monotonicity in optimization.

Result: The algorithm achieves a smoothly degrading approximation guarantee for γ < 1, matches the known 0.401 guarantee for γ = 1, and surpasses prior bounds for γ-weakly DR-submodular maximization under similar down-closed convex constraints.

Conclusion: The proposed algorithm establishes a new state-of-the-art for non-monotone γ-weakly DR-submodular maximization problems, improving theoretical results and practical methods in this area.

Abstract: Maximizing submodular objectives under constraints is a fundamental problem in machine learning and optimization. We study the maximization of a nonnegative, non-monotone $γ$-weakly DR-submodular function over a down-closed convex body. Our main result is an approximation algorithm whose guarantee depends smoothly on $γ$; in particular, when $γ=1$ (the DR-submodular case) our bound recovers the $0.401$ approximation factor, while for $γ<1$ the guarantee degrades gracefully and, it improves upon previously reported bounds for $γ$-weakly DR-submodular maximization under the same constraints. Our approach combines a Frank-Wolfe-guided continuous-greedy framework with a $γ$-aware double-greedy step, yielding a simple yet effective procedure for handling non-monotonicity. This results in state-of-the-art guarantees for non-monotone $γ$-weakly DR-submodular maximization over down-closed convex bodies.

</details>


### [184] [Memory Bank Compression for Continual Adaptation of Large Language Models](https://arxiv.org/abs/2601.00756)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.LG

TL;DR: This paper introduces a novel approach, MBC, to reduce the memory bank size in large language models (LLMs) during online learning, ensuring efficient knowledge incorporation while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods for updating LLMs, such as full fine-tuning or memory-augmented approaches, are computationally expensive or face scalability issues due to growing memory demands, especially when dealing with large-scale data streams.

Method: The proposed MBC model employs a codebook optimization strategy for compressing the memory bank, complemented with an online resetting mechanism to prevent codebook collapse. Low-rank adaptation is applied in attention layers for efficient use of compressed memory representations.

Result: MBC compresses the memory bank to 0.3% of its size compared to competitive baselines, with minimal loss in retention accuracy during online adaptation learning.

Conclusion: MBC provides an efficient and scalable solution for continual learning in LLMs by significantly reducing memory usage while maintaining performance, addressing the challenges of memory growth in real-world applications.

Abstract: Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.

</details>


### [185] [Do Chatbot LLMs Talk Too Much? The YapBench Benchmark](https://arxiv.org/abs/2601.00624)
*Vadim Borisov,Michael Gröger,Mina Mikhael,Richard H. Schreiber*

Main category: cs.LG

TL;DR: The paper introduces YapBench, a benchmark for evaluating excessive verbosity in language models' responses compared to minimal answers required for comprehension.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the issue of large language models (LLMs) generating overly verbose responses, which increase cognitive load and inference costs despite simpler alternatives being possible.

Method: The authors developed YapBench, a benchmark consisting of prompts, minimal acceptable answers, and categories to measure verbosity using a metric called YapScore. They evaluated excess response length across 76 LLMs.

Result: They found a wide disparity in verbosity among the tested models, with specific failure patterns like excessive explanations and redundant formatting for minimal prompts, factual questions, and coding tasks.

Conclusion: The benchmark provides a systematic way to quantify verbosity in LLM outputs, aiming to reduce unnecessary responses and helping improve model efficiency and user experience.

Abstract: Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini increasingly act as general-purpose copilots, yet they often respond with unnecessary length on simple requests, adding redundant explanations, hedging, or boilerplate that increases cognitive load and inflates token-based inference cost. Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality.
  We introduce YapBench, a lightweight benchmark for quantifying user-visible over-generation on brevity-ideal prompts. Each item consists of a single-turn prompt, a curated minimal-sufficient baseline answer, and a category label. Our primary metric, YapScore, measures excess response length beyond the baseline in characters, enabling comparisons across models without relying on any specific tokenizer. We summarize model performance via the YapIndex, a uniformly weighted average of category-level median YapScores.
  YapBench contains over three hundred English prompts spanning three common brevity-ideal settings: (A) minimal or ambiguous inputs where the ideal behavior is a short clarification, (B) closed-form factual questions with short stable answers, and (C) one-line coding tasks where a single command or snippet suffices. Evaluating 76 assistant LLMs, we observe an order-of-magnitude spread in median excess length and distinct category-specific failure modes, including vacuum-filling on ambiguous inputs and explanation or formatting overhead on one-line technical requests. We release the benchmark and maintain a live leaderboard for tracking verbosity behavior over time.

</details>


### [186] [Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning](https://arxiv.org/abs/2601.00791)
*Valentin Noël*

Main category: cs.LG

TL;DR: The paper introduces a training-free spectral analysis method for detecting valid mathematical reasoning in transformer models, achieving high accuracy without fine-tuning or training data.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the challenge of verifying logical coherence and reasoning validity in large language models, specifically in the domain of mathematical proofs.

Method: Attention matrices are treated as dynamic graph adjacency matrices, and spectral diagnostics such as Fiedler value, HFER, smoothness, and spectral entropy are used to differentiate valid and invalid mathematical reasoning.

Result: Experiments across seven transformer models demonstrated high classification accuracy (85.0–95.6%) and significant discriminative signals using spectral metrics, with notable architectural-dependent findings.

Conclusion: Spectral graph analysis provides a robust framework for reasoning verification, revealing attention mechanism dependencies and fostering advances in AI safety and hallucination detection.

Abstract: We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\text{MW}} = 1.16 \times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.

</details>


### [187] [Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability](https://arxiv.org/abs/2601.00655)
*Kasra Fouladi,Hamta Rahmani*

Main category: cs.LG

TL;DR: The paper presents IGBO, a framework that integrates domain knowledge and interpretability into model training using a bi-objective formulation.


<details>
  <summary>Details</summary>
Motivation: To improve model interpretability while effectively encoding domain knowledge and handling Out-of-Distribution (OOD) issues.

Method: It encodes feature importance hierarchies as a Directed Acyclic Graph (DAG), uses Temporal Integrated Gradients (TIG) for measuring feature importance, and introduces an Optimal Path Oracle to address OOD problems.

Result: The proposed IGBO framework demonstrates effectiveness on time-series data by enforcing DAG constraints with minimal accuracy loss and outperforming standard regularization baselines.

Conclusion: IGBO successfully incorporates interpretability and domain knowledge into model training with theoretical convergence assurance and reliable empirical performance.

Abstract: This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains interpretable models by incorporating structured domain knowledge via a bi-objective formulation. IGBO encodes feature importance hierarchies as a Directed Acyclic Graph (DAG) and uses Temporal Integrated Gradients (TIG) to measure feature importance. To address the Out-of-Distribution (OOD) problem in TIG computation, we propose an Optimal Path Oracle that learns data-manifold-aware integration paths. Theoretical analysis proves convergence properties and robustness to mini-batch noise, while empirical results on time-series data demonstrate IGBO's effectiveness in enforcing DAG constraints with minimal accuracy loss, outperforming standard regularization baselines.

</details>


### [188] [Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation](https://arxiv.org/abs/2601.00664)
*Taekyung Ki,Sangwon Jang,Jaehyeong Jo,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.LG

TL;DR: The paper introduces a framework called Avatar Forcing for real-time, interactive talking head avatars, enabling vibrant, low-latency reactions to user input.


<details>
  <summary>Details</summary>
Motivation: Current talking head models struggle with creating lifelike interactive avatars as they tend to lack emotional engagement and fail to meet real-time causal constraints.

Method: The framework uses 'diffusion forcing' to enable avatars to process real-time multimodal inputs (audio and motion) and introduces preference optimization with synthetic samples for label-free learning.

Result: The proposed framework achieves real-time interaction with a low latency of ~500ms, a 6.8X speed improvement over current methods, and a significantly higher preference (~80%) against the baseline.

Conclusion: Avatar Forcing provides low-latency, expressive, and interactive avatars that can respond to both verbal and non-verbal user inputs, marking a step forward in lifelike virtual communication.

Abstract: Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.

</details>


### [189] [IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning](https://arxiv.org/abs/2601.00677)
*Haonan Song,Qingchen Xie,Huan Zhu,Feng Xiao,Luxi Xing,Fuzhen Li,Liu Kang,Feng Jiang,Zhiyong Zheng,Fan Yang*

Main category: cs.LG

TL;DR: The paper tackles the computational inefficiencies of pairwise Generative Reward Models (GRMs) in RL by proposing IRPO, a more efficient framework based on the Bradley-Terry model, achieving state-of-the-art results and outperforming pairwise GRMs in post-training evaluations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome computational bottlenecks in pairwise GRMs during their integration with RL algorithms, specifically addressing the inefficiencies of pairwise comparisons and repeated sampling while retaining scalability and interpretability.

Method: The proposed method, Intergroup Relative Preference Optimization (IRPO), incorporates the Bradley-Terry model into the RL framework. This approach replaces pairwise comparisons with pointwise scores, allowing efficient evaluation of multiple candidates while maintaining fine-grained reward signals.

Result: IRPO demonstrates state-of-the-art performance on pointwise GRM benchmarks and achieves results comparable to leading pairwise GRMs. It also significantly surpasses pairwise GRMs in post-training evaluations.

Conclusion: IRPO presents an efficient and scalable alternative to current pairwise GRMs, successfully addressing computational challenges without sacrificing reward quality or interpretability.

Abstract: Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.

</details>


### [190] [ARISE: Adaptive Reinforcement Integrated with Swarm Exploration](https://arxiv.org/abs/2601.00693)
*Rajiv Chaitanya M,D R Ramesh Babu*

Main category: cs.LG

TL;DR: ARISE framework enhances RL exploration using a swarm-based exploration layer, resulting in improved performance on challenging tasks and robustness under non-stationary rewards.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the challenge of effective exploration in RL, especially under conditions of non-stationary rewards or high-dimensional policy spaces.

Method: ARISE uses a compact swarm-based exploration layer that integrates particle-driven proposals with standard policy-gradient methods for adaptive exploration based on reward-variance cues.

Result: ARISE achieves substantial improvements in challenging RL tasks (e.g., +46% LunarLander-v3, +22% Hopper-v4) and robust performance under non-stationary reward shifts (+75 points CartPole).

Conclusion: The study concludes that ARISE, being architecture-agnostic, provides a simple way to enhance exploration and resilience in reinforcement learning while maintaining core algorithm stability.

Abstract: Effective exploration remains a key challenge in RL, especially with non-stationary rewards or high-dimensional policies. We introduce ARISE, a lightweight framework that enhances reinforcement learning by augmenting standard policy-gradient methods with a compact swarm-based exploration layer. ARISE blends policy actions with particle-driven proposals, where each particle represents a candidate policy trajectory sampled in the action space, and modulates exploration adaptively using reward-variance cues. While easy benchmarks exhibit only slight improvements (e.g., +0.7% on CartPole-v1), ARISE yields substantial gains on more challenging tasks, including +46% on LunarLander-v3 and +22% on Hopper-v4, while preserving stability on Walker2d and Ant. Under non-stationary reward shifts, ARISE provides marked robustness advantages, outperforming PPO by +75 points on CartPole and improving LunarLander accordingly. Ablation studies confirm that both the swarm component and the adaptive mechanism contribute to the performance. Overall, ARISE offers a simple, architecture-agnostic route to more exploratory and resilient RL agents without altering core algorithmic structures.

</details>


### [191] [BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting](https://arxiv.org/abs/2601.00698)
*Maximilian Reinwardt,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

TL;DR: The paper proposes the B-Spline Adaptive Tokenizer (BSAT) for time series forecasting, addressing challenges in self-attention complexity and rigid patching. It introduces adaptive segmentation, enhanced positional encoding, and achieves strong results in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with quadratic self-attention complexity and uniform patching misaligned to time series semantic structure.

Method: BSAT adaptively segments time series using B-splines, focusing tokens on high-curvature areas. It combines a hybrid positional encoding (Rotary Positional Embedding + learnable encoding) to improve temporal dependency analysis.

Result: The model demonstrates strong performance across benchmark datasets, achieving effectiveness at high memory compression rates.

Conclusion: The approach is competitive, efficient for memory-constrained scenarios, and improves temporal representation through adaptive tokenization and advanced encoding.

Abstract: Long-term time series forecasting using transformers is hampered by the quadratic complexity of self-attention and the rigidity of uniform patching, which may be misaligned with the data's semantic structure. In this paper, we introduce the \textit{B-Spline Adaptive Tokenizer (BSAT)}, a novel, parameter-free method that adaptively segments a time series by fitting it with B-splines. BSAT algorithmically places tokens in high-curvature regions and represents each variable-length basis function as a fixed-size token, composed of its coefficient and position. Further, we propose a hybrid positional encoding that combines a additive learnable positional encoding with Rotary Positional Embedding featuring a layer-wise learnable base: L-RoPE. This allows each layer to attend to different temporal dependencies. Our experiments on several public benchmarks show that our model is competitive with strong performance at high compression rates. This makes it particularly well-suited for use cases with strong memory constraints.

</details>


### [192] [Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL](https://arxiv.org/abs/2601.00728)
*Erin Carson,Xinye Chen*

Main category: cs.LG

TL;DR: This paper presents a reinforcement learning framework for adaptive precision tuning in linear solvers, improving computational efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Current numerical methods often lack adaptive precision mechanisms, leading to trade-offs between computational cost and accuracy. This work seeks to address that gap using machine learning.

Method: The approach formulates precision tuning as a contextual bandit problem, employs a Q-table for discretized state-action mappings, and uses an epsilon-greedy strategy for multi-objective optimization.

Result: The proposed framework effectively reduces computational cost without sacrificing accuracy, generalizes to unseen datasets, and is validated with iterative refinement for linear systems.

Conclusion: The study demonstrates the potential of RL-based precision autotuning to improve computational efficiency in scientific computing, generalizing well to out-of-sample data.

Abstract: We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.

</details>


### [193] [Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty](https://arxiv.org/abs/2601.00737)
*Uğurcan Özalp*

Main category: cs.LG

TL;DR: The paper proposes STAC, a reinforcement learning algorithm utilizing aleatoric uncertainty from stochastic transitions to improve critic performance, avoiding the need for epistemic uncertainty ensemble methods.


<details>
  <summary>Details</summary>
Motivation: Address the systemic overestimation of value by critic networks in off-policy actor-critic reinforcement learning methods while improving efficiency and stability.

Method: Introduce STAC, which leverages a single distributional critic network and dropout techniques to model temporal aleatoric uncertainty and scale pessimistic bias during temporal-difference updates.

Result: STAC mitigates overestimation, promotes risk-averse behavior in uncertain settings, and enhances computational efficiency with better training stability.

Conclusion: The integration of aleatoric uncertainty alongside dropout regularization provides an efficient and robust solution for addressing critic overestimation and promotes risk-aware learning.

Abstract: Off-policy actor-critic methods in reinforcement learning train a critic with temporal-difference updates and use it as a learning signal for the policy (actor). This design typically achieves higher sample efficiency than purely on-policy methods. However, critic networks tend to overestimate value estimates systematically. This is often addressed by introducing a pessimistic bias based on uncertainty estimates. Current methods employ ensembling to quantify the critic's epistemic uncertainty-uncertainty due to limited data and model ambiguity-to scale pessimistic updates. In this work, we propose a new algorithm called Stochastic Actor-Critic (STAC) that incorporates temporal (one-step) aleatoric uncertainty-uncertainty arising from stochastic transitions, rewards, and policy-induced variability in Bellman targets-to scale pessimistic bias in temporal-difference updates, rather than relying on epistemic uncertainty. STAC uses a single distributional critic network to model the temporal return uncertainty, and applies dropout to both the critic and actor networks for regularization. Our results show that pessimism based on a distributional critic alone suffices to mitigate overestimation, and naturally leads to risk-averse behavior in stochastic environments. Introducing dropout further improves training stability and performance by means of regularization. With this design, STAC achieves improved computational efficiency using a single distributional critic network.

</details>


### [194] [The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving](https://arxiv.org/abs/2601.00747)
*Max Ruiz Luyten,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: This paper introduces Distributional Creative Reasoning (DCR), a framework that addresses the issue of diversity decay in reasoning paths of large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: LLM pipelines optimize for correctness but this approach can collapse the model's reasoning path diversity, undermining creativity.

Method: The proposed method, DCR, reframes training as gradient flow through probability measures on solution traces and unifies existing methods under a common loss framework.

Result: DCR explains diversity decay modes for methods like STaR, GRPO, and DPO, provides designs to ensure stable and diverse policies, and presents practical recipes to prevent collapse.

Conclusion: DCR offers a principled approach to balance correctness and creativity, preventing distribution collapse in LLM reasoning.

Abstract: State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.

</details>


### [195] [A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football](https://arxiv.org/abs/2601.00748)
*Sean Groom,Shuo Wang,Francisco Belo,Axl Rice,Liam Anderson*

Main category: cs.LG

TL;DR: This paper develops a covariate-dependent Hidden Markov Model (CDHMM) to assess off-ball defensive performance in football, focusing on structured scenarios like corner kicks.


<details>
  <summary>Details</summary>
Motivation: Traditional football metrics fail to adequately assess off-ball defensive performance, which involves coordinated actions restricting opponents' options. Existing models often lack tactical awareness.

Method: The authors design a CDHMM to infer man-marking and zonal strategies from player tracking data. The model is used for defensive credit attribution and role-conditioned ghosting in counterfactual analyses.

Result: The model provides interpretable evaluations of defensive performance, improving on baselines by incorporating context-specific tactics.

Conclusion: This approach offers a novel framework for evaluating football defense through data-driven, context-aware methods.

Abstract: Evaluating off-ball defensive performance in football is challenging, as traditional metrics do not capture the nuanced coordinated movements that limit opponent action selection and success probabilities. Although widely used possession value models excel at appraising on-ball actions, their application to defense remains limited. Existing counterfactual methods, such as ghosting models, help extend these analyses but often rely on simulating "average" behavior that lacks tactical context. To address this, we introduce a covariate-dependent Hidden Markov Model (CDHMM) tailored to corner kicks, a highly structured aspect of football games. Our label-free model infers time-resolved man-marking and zonal assignments directly from player tracking data. We leverage these assignments to propose a novel framework for defensive credit attribution and a role-conditioned ghosting method for counterfactual analysis of off-ball defensive performance. We show how these contributions provide a interpretable evaluation of defensive contributions against context-aware baselines.

</details>


### [196] [FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing](https://arxiv.org/abs/2601.00785)
*Sunny Gupta,Amit Sethi*

Main category: cs.LG

TL;DR: FedHypeVAE is a privacy-preserving federated data synthesis framework using a hypernetwork-driven, differentially private conditional VAE.


<details>
  <summary>Details</summary>
Motivation: To overcome non-IID client heterogeneity and gradient leakage issues in federated data sharing, targeting utility without exposing raw data.

Method: Introduces class-conditional priors and client-aware decoders via a shared hypernetwork-driven VAE, optimized under differential privacy with gradient noise perturbations.

Result: Enhances personalization, privacy, and distribution alignment in federated data synthesis with improved stability for non-IID data.

Conclusion: FedHypeVAE provides a robust, privacy-preserving solution for decentralized generative modeling, balancing personalization and domain-wide synthesis.

Abstract: Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE backbone, we replace the single global decoder and fixed latent prior with client-aware decoders and class-conditional priors generated by a shared hypernetwork from private, trainable client codes. This bi-level design personalizes the generative layerrather than the downstream modelwhile decoupling local data from communicated parameters. The shared hypernetwork is optimized under differential privacy, ensuring that only noise-perturbed, clipped gradients are aggregated across clients. A local MMD alignment between real and synthetic embeddings and a Lipschitz regularizer on hypernetwork outputs further enhance stability and distributional coherence under non-IID conditions. After training, a neutral meta-code enables domain agnostic synthesis, while mixtures of meta-codes provide controllable multi-domain coverage. FedHypeVAE unifies personalization, privacy, and distribution alignment at the generator level, establishing a principled foundation for privacy-preserving data synthesis in federated settings. Code: github.com/sunnyinAI/FedHypeVAE

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [197] [Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing](https://arxiv.org/abs/2601.00020)
*Nikhil Garg,Anxiong Song,Niklas Plessnig,Nathan Savoia,Laura Bégon-Lours*

Main category: cs.NE

TL;DR: The paper presents a strategy for deploying spiking neural networks (SNNs) on programmable ferroelectric memristive hardware, enabling adaptive learning for EEG-based brain-computer interfaces with realistic device constraints.


<details>
  <summary>Details</summary>
Motivation: Non-stationary neural signals hinder subject-agnostic models in EEG-based BCIs, necessitating adaptive and personalized learning on constrained platforms. Ferroelectric memristive devices show promise but face practical challenges.

Method: Authors introduce and evaluate a convolutional-recurrent SNN architecture with two strategies: device-aware training using a synapse model and software-trained weight transfer with low-overhead re-tuning. A device-aware weight-update mechanism is proposed to optimize programming frequency.

Result: Both deployment strategies achieve comparable classification performance with state-of-the-art SNNs, with subject-specific transfer learning further improving accuracy.

Conclusion: Programmable ferroelectric hardware enables efficient adaptation in neuromorphic processing of neural signals, making personalized EEG-based brain-computer interfaces feasible.

Abstract: Electroencephalography (EEG)-based brain-computer interfaces (BCIs) are strongly affected by non-stationary neural signals that vary across sessions and individuals, limiting the generalization of subject-agnostic models and motivating adaptive and personalized learning on resource-constrained platforms. Programmable memristive hardware offers a promising substrate for such post-deployment adaptation; however, practical realization is challenged by limited weight resolution, device variability, nonlinear programming dynamics, and finite device endurance. In this work, we show that spiking neural networks (SNNs) can be deployed on ferroelectric memristive synaptic devices for adaptive EEG-based motor imagery decoding under realistic device constraints. We fabricate, characterize, and model ferroelectric synapses. We evaluate a convolutional-recurrent SNN architecture under two complementary deployment strategies: (i) device-aware training using a ferroelectric synapse model, and (ii) transfer of software-trained weights followed by low-overhead on-device re-tuning. To enable efficient adaptation, we introduce a device-aware weight-update strategy in which gradient-based updates are accumulated digitally and converted into discrete programming events only when a threshold is exceeded, emulating nonlinear, state-dependent programming dynamics while reducing programming frequency. Both deployment strategies achieve classification performance comparable to state-of-the-art software-based SNNs. Furthermore, subject-specific transfer learning achieved by retraining only the final network layers improves classification accuracy. These results demonstrate that programmable ferroelectric hardware can support robust, low-overhead adaptation in spiking neural networks, opening a practical path toward personalized neuromorphic processing of neural signals.

</details>


### [198] [Covariance Matrix Adaptation Evolution Strategy without a matrix](https://arxiv.org/abs/2601.00102)
*Jarosław Arabas,Adam Stelmaszczyk,Eryk Warchulski,Dariusz Jagodziński,Rafał Biedrzycki*

Main category: cs.NE

TL;DR: This paper introduces a matrix-free version of CMA-ES to overcome computational challenges in high dimensions, achieving comparable or better performance to the traditional method.


<details>
  <summary>Details</summary>
Motivation: The cubic complexity of decomposing covariance matrices limits the application of CMA-ES in high-dimensional spaces.

Method: The matrix-free CMA-ES stores normalized difference vectors in an archive to replace the covariance matrix. New individuals are generated through weighted combinations of these archived vectors.

Result: The proposed method matches the probability distribution of the standard CMA-ES and yields comparable or improved optimization efficiency, especially with step-size adaptation, as tested on the CEC'2017 benchmark suite.

Conclusion: The matrix-free approach simplifies CMA-ES, maintains performance quality, and offers innovations for developing more efficient algorithms in the future.

Abstract: Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is a highly effective optimization technique. A primary challenge when applying CMA-ES in high dimensionality is sampling from a multivariate normal distribution with an arbitrary covariance matrix, which involves its decomposition. The cubic complexity of this process is the main obstacle to applying CMA-ES in highdimensional spaces. We introduce a version of CMA-ES that uses no covariance matrix at all. In the proposed matrix-free CMA-ES, an archive stores the vectors of differences between individuals and the midpoint, normalized by the step size. New individuals are generated as the weighted combinations of the vectors from the archive. We prove that the probability distribution of individuals generated by the proposed method is identical to that of the standard CMA-ES. Experimental results show that reducing the archive size to store only a fixed number of the most recent populations is sufficient, without compromising optimization efficiency. The matrix-free and matrix-based CMA-ES achieve comparable results on the quadratic function when the step-size adaptation is turned off. When coupled with the step-size adaptation method, the matrix-free CMA-ES converges faster than the matrix-based, and usually yields the results of a comparable or superior quality, according to the results obtained for the CEC'2017 benchmark suite. Presented approach simplifies the algorithm, offers a novel perspective on covariance matrix adaptation, and serves as a stepping stone toward even more efficient methods.

</details>


### [199] [Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing](https://arxiv.org/abs/2601.00245)
*Osvaldo Simeone*

Main category: cs.NE

TL;DR: The paper explores the intersection of neuromorphic computing principles and modern AI architectures, highlighting their energy efficiency and design methodologies.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the growing energy requirements of AI systems and seeks solutions inspired by neuromorphic computing for efficiency and sustainability.

Method: It analyzes neuromorphic computing concepts such as sparse activations, state-space dynamics, and sparse self-attention in the context of intra-token and inter-token processing.

Result: Connections are drawn between neuromorphic principles, transformer architectures, state-space models, and advanced training methods like surrogate gradients.

Conclusion: Neuromorphic computing offers promising pathways to make AI systems more energy-efficient through biologically inspired design principles and targeted processing techniques.

Abstract: The rapid growth of artificial intelligence (AI) has brought novel data processing and generative capabilities but also escalating energy requirements. This challenge motivates renewed interest in neuromorphic computing principles, which promise brain-like efficiency through discrete and sparse activations, recurrent dynamics, and non-linear feedback. In fact, modern AI architectures increasingly embody neuromorphic principles through heavily quantized activations, state-space dynamics, and sparse attention mechanisms. This paper elaborates on the connections between neuromorphic models, state-space models, and transformer architectures through the lens of the distinction between intra-token processing and inter-token processing. Most early work on neuromorphic AI was based on spiking neural networks (SNNs) for intra-token processing, i.e., for transformations involving multiple channels, or features, of the same vector input, such as the pixels of an image. In contrast, more recent research has explored how neuromorphic principles can be leveraged to design efficient inter-token processing methods, which selectively combine different information elements depending on their contextual relevance. Implementing associative memorization mechanisms, these approaches leverage state-space dynamics or sparse self-attention. Along with a systematic presentation of modern neuromorphic AI models through the lens of intra-token and inter-token processing, training methodologies for neuromorphic AI models are also reviewed. These range from surrogate gradients leveraging parallel convolutional processing to local learning rules based on reinforcement learning mechanisms.

</details>


### [200] [RMAAT: Astrocyte-Inspired Memory Compression and Replay for Efficient Long-Context Transformers](https://arxiv.org/abs/2601.00426)
*Md Zesun Ahmed Mia,Malyaban Bal,Abhronil Sengupta*

Main category: cs.NE

TL;DR: The paper introduces the Recurrent Memory Augmented Astromorphic Transformer (RMAAT) which applies astrocyte-inspired mechanisms to enhance memory and computational efficiency in long-sequence processing.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of self-attention hinders the application of Transformers to lengthy sequences. This paper seeks inspiration from biological astrocytes to overcome these limitations.

Method: The authors propose RMAAT, a Transformer incorporating astrocyte-inspired recurrent memory, adaptive compression driven by long-term plasticity, efficient linear-complexity attention, and a novel training mechanism called Astrocytic Memory Replay Backpropagation (AMRB).

Result: RMAAT achieves competitive accuracy on the Long Range Arena (LRA) benchmark, while offering notable gains in computational and memory efficiency.

Conclusion: The integration of astrocyte-like dynamics and principles into Transformer architectures proves effective for scalable sequence modeling, with potential applications in memory-efficient long-sequence processing.

Abstract: The quadratic complexity of self-attention mechanism presents a significant impediment to applying Transformer models to long sequences. This work explores computational principles derived from astrocytes-glial cells critical for biological memory and synaptic modulation-as a complementary approach to conventional architectural modifications for efficient self-attention. We introduce the Recurrent Memory Augmented Astromorphic Transformer (RMAAT), an architecture integrating abstracted astrocyte functionalities. RMAAT employs a recurrent, segment-based processing strategy where persistent memory tokens propagate contextual information. An adaptive compression mechanism, governed by a novel retention factor derived from simulated astrocyte long-term plasticity (LTP), modulates these tokens. Attention within segments utilizes an efficient, linear-complexity mechanism inspired by astrocyte short-term plasticity (STP). Training is performed using Astrocytic Memory Replay Backpropagation (AMRB), a novel algorithm designed for memory efficiency in recurrent networks. Evaluations on the Long Range Arena (LRA) benchmark demonstrate RMAAT's competitive accuracy and substantial improvements in computational and memory efficiency, indicating the potential of incorporating astrocyte-inspired dynamics into scalable sequence models.

</details>


### [201] [Benchmarking ERP Analysis: Manual Features, Deep Learning, and Foundation Models](https://arxiv.org/abs/2601.00573)
*Yihe Wang,Zhiqiao Kang,Bohan Chen,Yu Zhang,Xiang Zhang*

Main category: cs.NE

TL;DR: This paper investigates the effectiveness of deep learning and pre-trained models on ERP data by benchmarking them against traditional manual feature-based methods for ERP classification and brain disease detection.


<details>
  <summary>Details</summary>
Motivation: With significant advances in deep learning for non-ERP EEG data, the need arises to study their applicability and replace reliance on manually extracted features in ERP analysis.

Method: A unified pipeline was designed to compare manual feature-based methods, deep learning architectures, and pre-trained EEG models for ERP tasks across 12 datasets. The study includes exploration of different Transformer patch-embedding strategies.

Result: The benchmark highlights strengths and weaknesses of traditional versus modern approaches while identifying better Transformer-based embedding designs for ERP.

Conclusion: This work provides a foundational framework for selecting effective methods and designing models tailored for ERP analysis applications.

Abstract: Event-related potential (ERP), a specialized paradigm of electroencephalographic (EEG), reflects neurological responses to external stimuli or events, generally associated with the brain's processing of specific cognitive tasks. ERP plays a critical role in cognitive analysis, the detection of neurological diseases, and the assessment of psychological states. Recent years have seen substantial advances in deep learning-based methods for spontaneous EEG and other non-time-locked task-related EEG signals. However, their effectiveness on ERP data remains underexplored, and many existing ERP studies still rely heavily on manually extracted features. In this paper, we conduct a comprehensive benchmark study that systematically compares traditional manual features (followed by a linear classifier), deep learning models, and pre-trained EEG foundation models for ERP analysis. We establish a unified data preprocessing and training pipeline and evaluate these approaches on two representative tasks, ERP stimulus classification and ERP-based brain disease detection, across 12 publicly available datasets. Furthermore, we investigate various patch-embedding strategies within advanced Transformer architectures to identify embedding designs that better suit ERP data. Our study provides a landmark framework to guide method selection and tailored model design for future ERP analysis. The code is available at https://github.com/DL4mHealth/ERP-Benchmark.

</details>


### [202] [Three factor delay learning rules for spiking neural networks](https://arxiv.org/abs/2601.00668)
*Luke Vassallo,Nima Taherinejad*

Main category: cs.NE

TL;DR: This paper improves Spiking Neural Networks (SNNs) for temporal tasks by incorporating learnable synaptic and axonal delays, demonstrating significant enhancements in classification accuracy and efficiency for real-time, resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: Existing SNNs are limited in handling temporal data as they typically rely only on synaptic weights, which are insufficient for robust temporal pattern recognition. Current delay-learning methods require large networks and offline learning, making them impractical for real-time and resource-limited applications.

Method: The paper incorporates synaptic and axonal delays into leaky integrate-and-fire (LIF)-based SNNs and proposes three-factor online learning rules to simultaneously optimize weights and delay parameters. It uses a Gaussian surrogate for spike derivatives in eligibility trace calculations and combines it with error signals for parameter updates.

Result: Incorporating delays enhances accuracy by up to 20% over weights-only baselines and achieves up to 14% higher accuracy with similar parameter counts. The proposed method performs comparably to offline backpropagation on the SHD dataset while achieving a 6.6x reduction in model size and 67% lower inference latency with minimal accuracy trade-off.

Conclusion: Introducing learnable delays into SNNs significantly improves temporal pattern recognition while reducing resource demands. This approach enables on-device learning in resource-constrained environments, promising advancements in neuromorphic processor design.

Abstract: Spiking Neural Networks (SNNs) are dynamical systems that operate on spatiotemporal data, yet their learnable parameters are often limited to synaptic weights, contributing little to temporal pattern recognition. Learnable parameters that delay spike times can improve classification performance in temporal tasks, but existing methods rely on large networks and offline learning, making them unsuitable for real-time operation in resource-constrained environments. In this paper, we introduce synaptic and axonal delays to leaky integrate and fire (LIF)-based feedforward and recurrent SNNs, and propose three-factor learning rules to simultaneously learn delay parameters online. We employ a smooth Gaussian surrogate to approximate spike derivatives exclusively for the eligibility trace calculation, and together with a top-down error signal determine parameter updates. Our experiments show that incorporating delays improves accuracy by up to 20% over a weights-only baseline, and for networks with similar parameter counts, jointly learning weights and delays yields up to 14% higher accuracy. On the SHD speech recognition dataset, our method achieves similar accuracy to offline backpropagation-based approaches. Compared to state-of-the-art methods, it reduces model size by 6.6x and inference latency by 67%, with only a 2.4% drop in classification accuracy. Our findings benefit the design of power and area-constrained neuromorphic processors by enabling on-device learning and lowering memory requirements.

</details>


### [203] [QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models](https://arxiv.org/abs/2601.00679)
*Rachmad Vidya Wicaksana Putra,Pasindu Wickramasinghe,Muhammad Shafique*

Main category: cs.NE

TL;DR: QSLM is a framework for automated quantization of spike-driven language models (SLMs), aiming to reduce memory and power consumption while preserving performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of large memory footprints and high processing power requirements of spike-driven language models (SLMs), which hinder their deployment on resource-constrained embedded devices.

Method: QSLM employs automated quantization by analyzing network architecture and sensitivity under quantization, using tiered strategies and a multi-objective trade-off function to optimize quantization settings.

Result: QSLM achieves up to 86.5% memory reduction, up to 20% power savings, and maintains high performance with 84.4% sentiment classification accuracy (SST-2 dataset) and a perplexity score of 23.2 (WikiText-2 dataset).

Conclusion: The proposed QSLM framework enables scalable and efficient compressing of pre-trained SLMs while ensuring optimal performance and memory constraints in real-world applications.

Abstract: Large Language Models (LLMs) have been emerging as prominent AI models for solving many natural language tasks due to their high performance (e.g., accuracy) and capabilities in generating high-quality responses to the given inputs. However, their large computational cost, huge memory footprints, and high processing power/energy make it challenging for their embedded deployments. Amid several tinyLLMs, recent works have proposed spike-driven language models (SLMs) for significantly reducing the processing power/energy of LLMs. However, their memory footprints still remain too large for low-cost and resource-constrained embedded devices. Manual quantization approach may effectively compress SLM memory footprints, but it requires a huge design time and compute power to find the quantization setting for each network, hence making this approach not-scalable for handling different networks, performance requirements, and memory budgets. To bridge this gap, we propose QSLM, a novel framework that performs automated quantization for compressing pre-trained SLMs, while meeting the performance and memory constraints. To achieve this, QSLM first identifies the hierarchy of the given network architecture and the sensitivity of network layers under quantization, then employs a tiered quantization strategy (e.g., global-, block-, and module-level quantization) while leveraging a multi-objective performance-and-memory trade-off function to select the final quantization setting. Experimental results indicate that our QSLM reduces memory footprint by up to 86.5%, reduces power consumption by up to 20%, maintains high performance across different tasks (i.e., by up to 84.4% accuracy of sentiment classification on the SST-2 dataset and perplexity score of 23.2 for text generation on the WikiText-2 dataset) close to the original non-quantized model while meeting the performance and memory constraints.

</details>


### [204] [Cost Optimization in Production Line Using Genetic Algorithm](https://arxiv.org/abs/2601.00689)
*Alireza Rezaee*

Main category: cs.NE

TL;DR: This paper proposes a genetic algorithm-based approach for cost-optimal task scheduling in production lines, comparing task-based and station-based encoding strategies.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of minimizing production costs while adhering to task precedence and capacity constraints in complex scheduling scenarios.

Method: Two genetic algorithm encoding strategies (task-based and station-based) were implemented, and standard GA operators were adapted. Experimental analysis was conducted under different precedence structures.

Result: Task-based encoding showed smoother convergence and more reliable cost minimization compared to station-based encoding, especially when feasible schedule options are abundant.

Conclusion: Genetic algorithms are effective for solving combinatorial scheduling problems with complex constraints, outperforming gradient-based and analytical methods.

Abstract: This paper presents a genetic algorithm (GA) approach to cost-optimal task scheduling in a production line. The system consists of a set of serial processing tasks, each with a given duration, unit execution cost, and precedence constraints, which must be assigned to an unlimited number of stations subject to a per-station duration bound. The objective is to minimize the total production cost, modeled as a station-wise function of task costs and the duration bound, while strictly satisfying all prerequisite and capacity constraints. Two chromosome encoding strategies are investigated: a station-based representation implemented using the JGAP library with SuperGene validity checks, and a task-based representation in which genes encode station assignments directly. For each encoding, standard GA operators (crossover, mutation, selection, and replacement) are adapted to preserve feasibility and drive the population toward lower-cost schedules. Experimental results on three classes of precedence structures-tightly coupled, loosely coupled, and uncoupled-demonstrate that the task-based encoding yields smoother convergence and more reliable cost minimization than the station-based encoding, particularly when the number of valid schedules is large. The study highlights the advantages of GA over gradient-based and analytical methods for combinatorial scheduling problems, especially in the presence of complex constraints and non-differentiable cost landscapes.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [205] [Reinforcement learning with timed constraints for robotics motion planning](https://arxiv.org/abs/2601.00087)
*Zhaoan Wang,Junchao Li,Mahdi Mohammad,Shaoping Xiao*

Main category: cs.RO

TL;DR: The paper proposes a method to integrate Metric Interval Temporal Logic (MITL) with reinforcement learning for robotic systems operating under time-bound constraints.


<details>
  <summary>Details</summary>
Motivation: Robotic systems need planners that handle complex tasks adhering to temporal constraints in dynamic and uncertain environments, but integrating MITL with RL is challenging.

Method: MITL formulas are converted into Timed Limit-Deterministic Generalized Büchi Automata (Timed-LDGBA), synchronized with decision processes (MDPs and POMDPs) to construct product timed models for Q-learning.

Result: The framework is validated via simulations in grid-worlds and a service-robot scenario, successfully learning policies under time-bound constraints in stochastic and partially observable environments.

Conclusion: The approach highlights potential for reliable robotic planning in time-critical and uncertain settings, proving effective across diverse environments and scaling to larger state spaces.

Abstract: Robotic systems operating in dynamic and uncertain environments increasingly require planners that satisfy complex task sequences while adhering to strict temporal constraints. Metric Interval Temporal Logic (MITL) offers a formal and expressive framework for specifying such time-bounded requirements; however, integrating MITL with reinforcement learning (RL) remains challenging due to stochastic dynamics and partial observability. This paper presents a unified automata-based RL framework for synthesizing policies in both Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs) under MITL specifications. MITL formulas are translated into Timed Limit-Deterministic Generalized Büchi Automata (Timed-LDGBA) and synchronized with the underlying decision process to construct product timed models suitable for Q-learning. A simple yet expressive reward structure enforces temporal correctness while allowing additional performance objectives. The approach is validated in three simulation studies: a $5 \times 5$ grid-world formulated as an MDP, a $10 \times 10$ grid-world formulated as a POMDP, and an office-like service-robot scenario. Results demonstrate that the proposed framework consistently learns policies that satisfy strict time-bounded requirements under stochastic transitions, scales to larger state spaces, and remains effective in partially observable environments, highlighting its potential for reliable robotic planning in time-critical and uncertain settings.

</details>


### [206] [Compositional Diffusion with Guided search for Long-Horizon Planning](https://arxiv.org/abs/2601.00126)
*Utkarsh A Mishra,David He,Yongxin Chen,Danfei Xu*

Main category: cs.RO

TL;DR: The paper introduces Compositional Diffusion with Guided Search (CDGS), addressing challenges in compositional generative models for long-horizon tasks by resolving mode averaging through innovative search embedding within diffusion processes.


<details>
  <summary>Details</summary>
Motivation: Current compositional generative models struggle with mode averaging, resulting in plans that are locally infeasible and globally incoherent, especially when handling multimodal local distributions.

Method: The proposed method, CDGS, embeds search within the diffusion denoising process, employs population-based sampling for exploring local modes, uses likelihood-based filtering to remove infeasible candidates, and enforces global consistency via iterative resampling across segments.

Result: CDGS achieves oracle-level performance on seven robotic manipulation tasks and surpasses baseline models without compositionality or those requiring extensive long-horizon data. It generalizes well across domains including panoramic image synthesis and long video generation.

Conclusion: CDGS provides a robust and generalizable solution for compositional generative models, enabling improved local-to-global consistency across long-horizon tasks and various application domains.

Abstract: Generative models have emerged as powerful tools for planning, with compositional approaches offering particular promise for modeling long-horizon task distributions by composing together local, modular generative models. This compositional paradigm spans diverse domains, from multi-step manipulation planning to panoramic image synthesis to long video generation. However, compositional generative models face a critical challenge: when local distributions are multimodal, existing composition methods average incompatible modes, producing plans that are neither locally feasible nor globally coherent. We propose Compositional Diffusion with Guided Search (CDGS), which addresses this \emph{mode averaging} problem by embedding search directly within the diffusion denoising process. Our method explores diverse combinations of local modes through population-based sampling, prunes infeasible candidates using likelihood-based filtering, and enforces global consistency through iterative resampling between overlapping segments. CDGS matches oracle performance on seven robot manipulation tasks, outperforming baselines that lack compositionality or require long-horizon training data. The approach generalizes across domains, enabling coherent text-guided panoramic images and long videos through effective local-to-global message passing. More details: https://cdgsearch.github.io/

</details>


### [207] [SLEI3D: Simultaneous Exploration and Inspection via Heterogeneous Fleets under Limited Communication](https://arxiv.org/abs/2601.00163)
*Junfeng Chen,Yuxiao Zhu,Xintong Zhang,Bing Luo,Meng Guo*

Main category: cs.RO

TL;DR: This paper introduces SLEI3D, a framework enabling robotic fleets for simultaneous exploration, inspection, and real-time communication in unknown environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the challenge in robotic fleet operations of identifying and inspecting unknown areas in environments with limited global communication.

Method: The paper proposes SLEI3D, a planning and coordination framework integrating collaborative 3D exploration, adaptive inspection, and proactive communication protocols.

Result: The framework was validated via detailed simulations with up to 48 robots covering large-scale environments and real-world hardware experiments with 7 robots.

Conclusion: SLEI3D effectively enables coordinated exploration and real-time communication in robotic fleets, showing promise in environments with limited connectivity.

Abstract: Robotic fleets such as unmanned aerial and ground vehicles have been widely used for routine inspections of static environments, where the areas of interest are known and planned in advance. However, in many applications, such areas of interest are unknown and should be identified online during exploration. Thus, this paper considers the problem of simultaneous exploration, inspection of unknown environments and then real-time communication to a mobile ground control station to report the findings. The heterogeneous robots are equipped with different sensors, e.g., long-range lidars for fast exploration and close-range cameras for detailed inspection. Furthermore, global communication is often unavailable in such environments, where the robots can only communicate with each other via ad-hoc wireless networks when they are in close proximity and free of obstruction. This work proposes a novel planning and coordination framework (SLEI3D) that integrates the online strategies for collaborative 3D exploration, adaptive inspection and timely communication (via the intermit-tent or proactive protocols). To account for uncertainties w.r.t. the number and location of features, a multi-layer and multi-rate planning mechanism is developed for inter-and-intra robot subgroups, to actively meet and coordinate their local plans. The proposed framework is validated extensively via high-fidelity simulations of numerous large-scale missions with up to 48 robots and 384 thousand cubic meters. Hardware experiments of 7 robots are also conducted. Project website is available at https://junfengchen-robotics.github.io/SLEI3D/.

</details>


### [208] [SLAP: Slapband-based Autonomous Perching Drone with Failure Recovery for Vertical Tree Trunks](https://arxiv.org/abs/2601.00238)
*Julia Di,Kenneth A. W. Hoffmann,Tony G. Chen,Tian-Ao Ren,Mark R. Cutkosky*

Main category: cs.RO

TL;DR: The paper explores a novel perching strategy for larger drones using an integrated system called SLAP to increase safety and recovery from perch failures.


<details>
  <summary>Details</summary>
Motivation: Current perching approaches for vertical surfaces are unsafe for drones with sensitive electronics and lack system-level integration.

Method: SLAP integrates vision-based perch site detection, IMU-based failure detection, optical systems, an attitude controller, and grippers for efficient and safe perching on vertical tree trunks.

Result: The modified 1.2 kg quadrotor achieved a 75% success rate for perching on a real oak tree segment in experiments and 100% failure recovery in induced conditions.

Conclusion: The method provides a safer and reliable perching approach for larger drones, advancing autonomous UAV operations on vertical surfaces.

Abstract: Perching allows unmanned aerial vehicles (UAVs) to reduce energy consumption, remain anchored for surface sampling operations, or stably survey their surroundings. Previous efforts for perching on vertical surfaces have predominantly focused on lightweight mechanical design solutions with relatively scant system-level integration. Furthermore, perching strategies for vertical surfaces commonly require high-speed, aggressive landing operations that are dangerous for a surveyor drone with sensitive electronics onboard. This work presents the preliminary investigation of a perching approach suitable for larger drones that both gently perches on vertical tree trunks and reacts and recovers from perch failures. The system in this work, called SLAP, consists of vision-based perch site detector, an IMU (inertial-measurement-unit)-based perch failure detector, an attitude controller for soft perching, an optical close-range detection system, and a fast active elastic gripper with microspines made from commercially-available slapbands. We validated this approach on a modified 1.2 kg commercial quadrotor with component and system analysis. Initial human-in-the-loop autonomous indoor flight experiments achieved a 75% perch success rate on a real oak tree segment across 20 flights, and 100% perch failure recovery across 2 flights with induced failures.

</details>


### [209] [Vehicle Painting Robot Path Planning Using Hierarchical Optimization](https://arxiv.org/abs/2601.00271)
*Yuya Nagai,Hiromitsu Nakamura,Narito Shinmachi,Yuta Higashizono,Satoshi Ono*

Main category: cs.RO

TL;DR: The paper proposes an automated hierarchical optimization approach for designing paint paths for robotic arms in vehicle factories, addressing constraints unique to the painting process.


<details>
  <summary>Details</summary>
Motivation: The manual process of designing paint paths for robotic arms is time-consuming and requires automation to reduce design time.

Method: The problem is formulated as a hierarchical optimization problem with an upper-layer using variable routing (VRP) and a lower-layer handling detailed path planning. Optimization algorithms and handling constraints specific to vehicle painting are applied at each layer.

Result: The proposed method successfully generates paint paths automatically with quality comparable to those manually designed by engineers.

Conclusion: Hierarchical optimization effectively tackles the automation of robotic paint path design, showing promising results in experiments on commercial vehicle models.

Abstract: In vehicle production factories, the vehicle painting process employs multiple robotic arms to simultaneously apply paint to car bodies advancing along a conveyor line. Designing paint paths for these robotic arms, which involves assigning car body areas to arms and determining paint sequences for each arm, remains a time-consuming manual task for engineers, indicating the demand for automation and design time reduction. The unique constraints of the painting process hinder the direct application of conventional robotic path planning techniques, such as those used in welding. Therefore, this paper formulates the design of paint paths as a hierarchical optimization problem, where the upper-layer subproblem resembles a vehicle routing problem (VRP), and the lower-layer subproblem involves detailed path planning. This approach allows the use of different optimization algorithms at each layer, and permits flexible handling of constraints specific to the vehicle painting process through the design of variable representation, constraints, repair operators, and an initialization process at the upper and lower layers. Experiments with three commercially available vehicle models demonstrated that the proposed method can automatically design paths that satisfy all constraints for vehicle painting with quality comparable to those created manually by engineers.

</details>


### [210] [Pure Inertial Navigation in Challenging Environments with Wheeled and Chassis Mounted Inertial Sensors](https://arxiv.org/abs/2601.00275)
*Dusan Nemec,Gal Versano,Itai Savin,Vojtech Simak,Juraj Kekelak,Itzik Klein*

Main category: cs.RO

TL;DR: This paper introduces WiCHINS, an inertial navigation system combining wheel and chassis sensors to tackle navigation challenges in environments with limited GNSS signals and degraded lighting.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles and robots often face navigation challenges in areas with unreliable GNSS signals or poor lighting, leading to time drift from inertial measurements.

Method: WiCHINS integrates wheel-mounted and chassis-mounted inertial sensors using a three-stage extended Kalman filter framework to enhance navigation accuracy.

Result: Using a dataset of 228.6 minutes and five inertial measurement units, the method achieved an average position error of 11.4m, equal to $2.4\%$ of the average traveled distance, outperforming four inertial baselines.

Conclusion: WiCHINS improves inertial navigation, enabling robust operation in challenging environments and narrowing the gap in pure-inertial performance.

Abstract: Autonomous vehicles and wheeled robots are widely used in many applications in both indoor and outdoor settings. In practical situations with limited GNSS signals or degraded lighting conditions, the navigation solution may rely only on inertial sensors and as result drift in time due to errors in the inertial measurement. In this work, we propose WiCHINS, a wheeled and chassis inertial navigation system by combining wheel-mounted-inertial sensors with a chassis-mounted inertial sensor for accurate pure inertial navigation. To that end, we derive a three-stage framework, each with a dedicated extended Kalman filter. This framework utilizes the benefits of each location (wheel/body) during the estimation process. To evaluate our proposed approach, we employed a dataset with five inertial measurement units with a total recording time of 228.6 minutes. We compare our approach with four other inertial baselines and demonstrate an average position error of 11.4m, which is $2.4\%$ of the average traveled distance, using two wheels and one body inertial measurement units. As a consequence, our proposed method enables robust navigation in challenging environments and helps bridge the pure-inertial performance gap.

</details>


### [211] [Replaceable Bit-based Gripper for Picking Cluttered Food Items](https://arxiv.org/abs/2601.00305)
*Prashant Kumar,Yukiyasu Domae,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: This paper introduces a gripper system designed for efficient handling of flexible, cluttered food items with replaceable bits, enhancing packaging operations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in food packaging, especially handling weight-based and cluttered food items efficiently.

Method: A replaceable bit-based gripper system is designed with specialized attachments for granular and elongated food items. It includes mechanisms for weight-specific control and a belt system for switching between food types.

Result: The gripper performed successfully in handling ikura and spaghetti, achieving weight-specific dropping accuracy rates of over 80% and 95%.

Conclusion: The proposed gripper system improves packaging flexibility and accuracy, offering efficient handling of diverse food items.

Abstract: The food packaging industry goes through changes in food items and their weights quite rapidly. These items range from easy-to-pick, single-piece food items to flexible, long and cluttered ones. We propose a replaceable bit-based gripper system to tackle the challenge of weight-based handling of cluttered food items. The gripper features specialized food attachments(bits) that enhance its grasping capabilities, and a belt replacement system allows switching between different food items during packaging operations. It offers a wide range of control options, enabling it to grasp and drop specific weights of granular, cluttered, and entangled foods. We specifically designed bits for two flexible food items that differ in shape: ikura(salmon roe) and spaghetti. They represent the challenging categories of sticky, granular food and long, sticky, cluttered food, respectively. The gripper successfully picked up both spaghetti and ikura and demonstrated weight-specific dropping of these items with an accuracy over 80% and 95% respectively. The gripper system also exhibited quick switching between different bits, leading to the handling of a large range of food items.

</details>


### [212] [Space Debris Removal using Nano-Satellites controlled by Low-Power Autonomous Agents](https://arxiv.org/abs/2601.00465)
*Dennis Christmann,Juan F. Gutierrez,Sthiti Padhi,Patrick Plörer,Aditya Takur,Simona Silvestri,Andres Gomez*

Main category: cs.RO

TL;DR: The paper addresses space debris by proposing autonomous nano-satellite swarms to safely de-orbit debris using energy-efficient intelligent software.


<details>
  <summary>Details</summary>
Motivation: The increasing presence of space debris threatens satellite operations and space travel, creating a pressing need for innovative solutions.

Method: The authors implemented autonomous agent software on wireless microcontrollers and validated its feasibility and energy efficiency using a specialized test-bed.

Result: Experiments demonstrated that their software approach is feasible and energy-efficient for addressing space debris.

Conclusion: Autonomous nano-satellite swarms can successfully address the space debris problem, showing potential for scalable and intelligent debris de-orbiting solutions.

Abstract: Space debris is an ever-increasing problem in space travel. There are already many old, no longer functional spacecraft and debris orbiting the earth, which endanger both the safe operation of satellites and space travel. Small nano-satellite swarms can address this problem by autonomously de-orbiting debris safely into the Earth's atmosphere. This work builds on the recent advances of autonomous agents deployed in resource-constrained platforms and shows a first simplified approach how such intelligent and autonomous nano-satellite swarms can be realized. We implement our autonomous agent software on wireless microcontrollers and perform experiments on a specialized test-bed to show the feasibility and overall energy efficiency of our approach.

</details>


### [213] [Variable Elimination in Hybrid Factor Graphs for Discrete-Continuous Inference & Estimation](https://arxiv.org/abs/2601.00545)
*Varun Agrawal,Frank Dellaert*

Main category: cs.RO

TL;DR: The paper proposes an efficient Hybrid Factor Graph framework for exact Maximum A Posteriori estimation and marginalization in hybrid problems involving both continuous and discrete variables.


<details>
  <summary>Details</summary>
Motivation: Hybrid problems in robotics involving both continuous and discrete components have traditionally been challenging to model and solve accurately.

Method: The method introduces hybrid Gaussian factors and hybrid conditionals within a novel Hybrid Factor Graph framework, employing a variable elimination algorithm and probabilistic tree pruning for exact inference under Conditional Linear Gaussian schemes.

Result: The framework demonstrates accurate, efficient, and tractable inference for hybrid problems, showcased on a SLAM dataset with ambiguous measurements.

Conclusion: The proposed Hybrid Factor Graph framework addresses long-standing modeling difficulties for hybrid problems and offers exact, practical solutions for estimation tasks.

Abstract: Many hybrid problems in robotics involve both continuous and discrete components, and modeling them together for estimation tasks has been a long standing and difficult problem. Hybrid Factor Graphs give us a mathematical framework to model these types of problems, however existing approaches for solving them are based on approximations. In this work, we propose an efficient Hybrid Factor Graph framework alongwith a variable elimination algorithm to produce a hybrid Bayes network, which can then be used for exact Maximum A Posteriori estimation and marginalization over both sets of variables. Our approach first develops a novel hybrid Gaussian factor which can connect to both discrete and continuous variables, and a hybrid conditional which can represent multiple continuous hypotheses conditioned on the discrete variables. Using these representations, we derive the process of hybrid variable elimination under the Conditional Linear Gaussian scheme, giving us exact posteriors as hybrid Bayes network. To bound the number of discrete hypotheses, we use a tree-structured representation of the factors coupled with a simple pruning and probabilistic assignment scheme, which allows for tractable inference. We demonstrate the applicability of our framework on a SLAM dataset with ambiguous measurements, where discrete choices for the most likely measurement have to be made. Our demonstrated results showcase the accuracy, generality, and simplicity of our hybrid factor graph framework.

</details>


### [214] [LLM-Based Agentic Exploration for Robot Navigation & Manipulation with Skill Orchestration](https://arxiv.org/abs/2601.00555)
*Abu Hanif Muhammad Syarubany,Farhan Zaki Rahmani,Trio Widianto*

Main category: cs.RO

TL;DR: The paper introduces an LLM-powered system that navigates shopping tasks using a semantic map and modular motion primitives, tested in simulation and real-world settings.


<details>
  <summary>Details</summary>
Motivation: The authors aim to develop a robotic system capable of performing complex indoor shopping tasks by integrating semantic understanding, natural language decision-making, and modular motion control.

Method: The system uses an LLM to translate natural-language shopping requests into discrete actions. These actions guide the robot equipped with a semantic mapping system and modular motion primitives for navigation, obstacle avoidance, and object retrieval.

Result: Qualitative results demonstrate that the system successfully accomplishes shopping tasks, including navigating multiple stores and retrieving objects, while providing modularity and debugability through logged decision histories.

Conclusion: The proposed system proves effective in real-world and simulated shopping scenarios, offering an agentic and modular approach to task execution with natural language processing capabilities.

Abstract: This paper presents an end-to-end LLM-based agentic exploration system for an indoor shopping task, evaluated in both Gazebo simulation and a corresponding real-world corridor layout. The robot incrementally builds a lightweight semantic map by detecting signboards at junctions and storing direction-to-POI relations together with estimated junction poses, while AprilTags provide repeatable anchors for approach and alignment. Given a natural-language shopping request, an LLM produces a constrained discrete action at each junction (direction and whether to enter a store), and a ROS finite-state main controller executes the decision by gating modular motion primitives, including local-costmap-based obstacle avoidance, AprilTag approaching, store entry, and grasping. Qualitative results show that the integrated stack can perform end-to-end task execution from user instruction to multi-store navigation and object retrieval, while remaining modular and debuggable through its text-based map and logged decision history.

</details>


### [215] [Priority-Aware Multi-Robot Coverage Path Planning](https://arxiv.org/abs/2601.00580)
*Kanghoon Lee,Hyeonjun Kim,Jiachen Li,Jinkyoo Park*

Main category: cs.RO

TL;DR: This paper introduces a Priority-Aware Multi-Robot Coverage Path Planning (PA-MCPP) problem and a framework to address it, optimizing prioritized zone coverage and maintaining competitive makespan.


<details>
  <summary>Details</summary>
Motivation: Existing MCPP methods assume uniform importance across regions, which is ineffective for scenarios demanding faster attention in specific areas.

Method: A two-phase framework: (1) greedy zone assignment with local search and spanning-tree-based path planning, and (2) Steiner-tree-guided residual coverage.

Result: Experiments show the method significantly reduces priority-weighted latency while maintaining competitive makespan. Sensitivity analyses highlight scalability and controllable zone coverage behavior.

Conclusion: The proposed approach improves performance in scenarios with prioritized zones, demonstrating scalability and efficiency in priority-weighted latency reduction.

Abstract: Multi-robot systems are widely used for coverage tasks that require efficient coordination across large environments. In Multi-Robot Coverage Path Planning (MCPP), the objective is typically to minimize the makespan by generating non-overlapping paths for full-area coverage. However, most existing methods assume uniform importance across regions, limiting their effectiveness in scenarios where some zones require faster attention. We introduce the Priority-Aware MCPP (PA-MCPP) problem, where a subset of the environment is designated as prioritized zones with associated weights. The goal is to minimize, in lexicographic order, the total priority-weighted latency of zone coverage and the overall makespan. To address this, we propose a scalable two-phase framework combining (1) greedy zone assignment with local search, spanning-tree-based path planning, and (2) Steiner-tree-guided residual coverage. Experiments across diverse scenarios demonstrate that our method significantly reduces priority-weighted latency compared to standard MCPP baselines, while maintaining competitive makespan. Sensitivity analyses further show that the method scales well with the number of robots and that zone coverage behavior can be effectively controlled by adjusting priority weights.

</details>


### [216] [NMPC-Augmented Visual Navigation and Safe Learning Control for Large-Scale Mobile Robots](https://arxiv.org/abs/2601.00609)
*Mehdi Heydari Shahna,Pauli Mustalahti,Jouni Mattila*

Main category: cs.RO

TL;DR: This paper proposes a navigation and control framework for large-scale mobile robots (LSMRs) to achieve stable and safe performance on loose terrain using advanced control techniques and safety monitoring.


<details>
  <summary>Details</summary>
Motivation: The main motivation is to address the challenges of operating large-scale mobile robots on slip-prone, unconsolidated terrain by ensuring stability, robust operation, and safety.

Method: The framework integrates four core modules: visual pose estimation, nonlinear model predictive control, deep neural network-based low-level control with adaptive measures, and a logarithmic safety module for system monitoring.

Result: The proposed system demonstrated effective operation during comparative experiments on a 6,000 kg robot with electro-hydrostatic drives, ensuring accurate motion control and safety under challenging conditions.

Conclusion: This research establishes a robust and adaptive navigation and control system for LSMRs, guaranteeing stability, safety, and performance on challenging terrains.

Abstract: A large-scale mobile robot (LSMR) is a high-order multibody system that often operates on loose, unconsolidated terrain, which reduces traction. This paper presents a comprehensive navigation and control framework for an LSMR that ensures stability and safety-defined performance, delivering robust operation on slip-prone terrain by jointly leveraging high-performance techniques. The proposed architecture comprises four main modules: (1) a visual pose-estimation module that fuses onboard sensors and stereo cameras to provide an accurate, low-latency robot pose, (2) a high-level nonlinear model predictive control that updates the wheel motion commands to correct robot drift from the robot reference pose on slip-prone terrain, (3) a low-level deep neural network control policy that approximates the complex behavior of the wheel-driven actuation mechanism in LSMRs, augmented with robust adaptive control to handle out-of-distribution disturbances, ensuring that the wheels accurately track the updated commands issued by high-level control module, and (4) a logarithmic safety module to monitor the entire robot stack and guarantees safe operation. The proposed low-level control framework guarantees uniform exponential stability of the actuation subsystem, while the safety module ensures the whole system-level safety during operation. Comparative experiments on a 6,000 kg LSMR actuated by two complex electro-hydrostatic drives, while synchronizing modules operating at different frequencies.

</details>


### [217] [Vision-based Goal-Reaching Control for Mobile Robots Using a Hierarchical Learning Framework](https://arxiv.org/abs/2601.00610)
*Mehdi Heydari Shahna,Pauli Mustalahti,Jouni Mattila*

Main category: cs.RO

TL;DR: The paper presents a goal-reaching control framework for large robots using modular methods: visual pose estimation, RL-based motion planning, supervised machine learning for dynamics, adaptive control, and safety supervision. It ensures stability and safety, validated on a 6,000 kg robot.


<details>
  <summary>Details</summary>
Motivation: Large robots with complex actuators on unstable terrain face safety challenges during RL exploration. The paper aims to provide a safe and efficient control framework for such robots.

Method: The system is divided into five modules: visual pose estimation for robot state, an RL planner for motion commands, a supervised deep learning model for robot dynamics, a robust adaptive controller, and a safety supervisor.

Result: The framework guarantees stability and operational safety. Experiments on a 6,000 kg robot confirmed its applicability in varied scenarios.

Conclusion: The modular approach effectively ensures safe and accurate operations of large-scale robots even in challenging terrains.

Abstract: Reinforcement learning (RL) is effective in many robotic applications, but it requires extensive exploration of the state-action space, during which behaviors can be unsafe. This significantly limits its applicability to large robots with complex actuators operating on unstable terrain. Hence, to design a safe goal-reaching control framework for large-scale robots, this paper decomposes the whole system into a set of tightly coupled functional modules. 1) A real-time visual pose estimation approach is employed to provide accurate robot states to 2) an RL motion planner for goal-reaching tasks that explicitly respects robot specifications. The RL module generates real-time smooth motion commands for the actuator system, independent of its underlying dynamic complexity. 3) In the actuation mechanism, a supervised deep learning model is trained to capture the complex dynamics of the robot and provide this model to 4) a model-based robust adaptive controller that guarantees the wheels track the RL motion commands even on slip-prone terrain. 5) Finally, to reduce human intervention, a mathematical safety supervisor monitors the robot, stops it on unsafe faults, and autonomously guides it back to a safe inspection area. The proposed framework guarantees uniform exponential stability of the actuation system and safety of the whole operation. Experiments on a 6,000 kg robot in different scenarios confirm the effectiveness of the proposed framework.

</details>


### [218] [From 2D to 3D terrain-following area coverage path planning](https://arxiv.org/abs/2601.00614)
*Mogens Plessen*

Main category: cs.RO

TL;DR: An algorithm for 3D terrain-following area coverage path planning is introduced with validation through agricultural data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of generating 3D path planning for terrain-following machinery, essential for tasks like agriculture where the working height and width need precise control.

Method: The method uses an algorithm combining local path generation and uniform elevation data creation via Inverse Distance Weighting alongside local search techniques.

Result: The algorithm was successfully validated on real-world 3D data within an agricultural context, showcasing its practical application.

Conclusion: The approach demonstrates effective 3D terrain coverage path planning, emphasizing the complexities compared to 2D methods and confirming its suitability for practical use cases.

Abstract: An algorithm for 3D terrain-following area coverage path planning is presented. Multiple adjacent paths are generated that are (i) locally apart from each other by a distance equal to the working width of a machinery, while (ii) simultaneously floating at a projection distance equal to a specific working height above the terrain. The complexities of the algorithm in comparison to its 2D equivalent are highlighted. These include uniformly spaced elevation data generation using an Inverse Distance Weighting-approach and a local search. Area coverage path planning results for real-world 3D data within an agricultural context are presented to validate the algorithm.

</details>


### [219] [RoboReward: General-Purpose Vision-Language Reward Models for Robotics](https://arxiv.org/abs/2601.00675)
*Tony Lee,Andrew Wagenmaker,Karl Pertsch,Percy Liang,Sergey Levine,Chelsea Finn*

Main category: cs.RO

TL;DR: This paper explores the application of vision-language models (VLMs) for reward modeling in robotic tasks, introduces RoboReward benchmarks and datasets, proposes a novel augmentation pipeline, and evaluates model performance. They find substantial room for VLM improvement and demonstrate effectiveness in real robot RL policy learning.


<details>
  <summary>Details</summary>
Motivation: Current methods to design rewards for reinforcement learning in robotics are either labor-intensive or overly simplistic, and there is limited understanding of VLMs' potential in real robot scenarios.

Method: The authors introduce the RoboReward dataset and benchmarks, utilize large-scale robotics datasets (OXE and RoboArena), develop a negative examples data augmentation pipeline, and train and evaluate 4B/8B-parameter VLMs to assess their ability to provide rewards across diverse robotic tasks.

Result: The proposed 4B/8B-parameter VLMs outperform much larger VLMs for short-horizon robotic tasks. The 8B-parameter model significantly improves performance in real-robot reinforcement learning compared to the Gemini Robotics-ER 1.5 model.

Conclusion: Vision-language reward models have potential in robotics but require further development to generalize across tasks. The proposed RoboReward approach narrows the gap between VLM-based rewards and human-provided rewards in RL.

Abstract: A well-designed reward is critical for effective reinforcement learning-based policy improvement. In real-world robotic domains, obtaining such rewards typically requires either labor-intensive human labeling or brittle, handcrafted objectives. Vision-language models (VLMs) have shown promise as automatic reward models, yet their effectiveness on real robot tasks is poorly understood. In this work, we aim to close this gap by introducing (1) \textbf{RoboReward}, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment (OXE) and RoboArena, and (2) vision-language reward models trained on this dataset (RoboReward 4B/8B). Because OXE is success-heavy and lacks failure examples, we propose a \emph{negative examples data augmentation} pipeline that generates calibrated \emph{negatives} and \emph{near-misses} via counterfactual relabeling of successful episodes and temporal clipping to create partial-progress outcomes from the same videos. Using this framework, we produce an extensive training and evaluation dataset that spans diverse tasks and embodiments and enables systematic evaluation of whether state-of-the-art VLMs can reliably provide rewards for robotics. Our evaluation of leading open-weight and proprietary VLMs reveals that no model excels across all tasks, underscoring substantial room for improvement. We then train general-purpose 4B- and 8B-parameter models that outperform much larger VLMs in assigning rewards for short-horizon robotic tasks. Finally, we deploy the 8B-parameter reward VLM in real-robot reinforcement learning and find that it improves policy learning over Gemini Robotics-ER 1.5, a frontier physical reasoning VLM trained on robotics data, by a large margin, while substantially narrowing the gap to RL training with human-provided rewards.

</details>


### [220] [DefVINS: Visual-Inertial Odometry for Deformable Scenes](https://arxiv.org/abs/2601.00702)
*Samuel Cerezo,Javier Civera*

Main category: cs.RO

TL;DR: The paper introduces DefVINS, a robust visual-inertial odometry system separating rigid and non-rigid states, enabling effective navigation in deformable environments.


<details>
  <summary>Details</summary>
Motivation: Classical visual-inertial odometry fails in deformable scenes due to rigid assumptions, leading to overfitting or system drift, necessitating a solution for stable performance.

Method: DefVINS separates rigid IMU-anchored states from non-rigid components using an embedded deformation graph, with a gradual activation of non-rigid parameters guided by observability analysis.

Result: DefVINS demonstrated improved robustness and accuracy in non-rigid environments through ablation studies, validating the benefits of inertial constraints and observability-aware activation.

Conclusion: DefVINS addresses the limitations of traditional VIO in deformable environments, offering a novel framework that integrates rigid-state constraints and non-rigid handling for enhanced performance.

Abstract: Deformable scenes violate the rigidity assumptions underpinning classical visual-inertial odometry (VIO), often leading to over-fitting to local non-rigid motion or severe drift when deformation dominates visual parallax. We introduce DefVINS, a visual-inertial odometry framework that explicitly separates a rigid, IMU-anchored state from a non--rigid warp represented by an embedded deformation graph. The system is initialized using a standard VIO procedure that fixes gravity, velocity, and IMU biases, after which non-rigid degrees of freedom are activated progressively as the estimation becomes well conditioned. An observability analysis is included to characterize how inertial measurements constrain the rigid motion and render otherwise unobservable modes identifiable in the presence of deformation. This analysis motivates the use of IMU anchoring and informs a conditioning-based activation strategy that prevents ill-posed updates under poor excitation. Ablation studies demonstrate the benefits of combining inertial constraints with observability-aware deformation activation, resulting in improved robustness under non-rigid environments.

</details>


### [221] [Calling for Backup: How Children Navigate Successive Robot Communication Failures](https://arxiv.org/abs/2601.00754)
*Maria Teresa Parreira,Isabel Neto,Filipa Rocha,Wendy Ju*

Main category: cs.RO

TL;DR: This study examines children's reactions to repeated robot social and performance errors, finding distinct behavioral responses compared to adults and informing the design of child-friendly human-robot interaction systems.


<details>
  <summary>Details</summary>
Motivation: To understand how children respond to repeated robot errors, as prior research focuses on adult reactions with limited exploration of children's behavior.

Method: 59 children aged 8-10 interacted with a robot that made three successive conversational errors. Their behavioral responses were video-recorded and analyzed for both verbal and non-verbal reactions.

Result: Children adjusted prompts and verbal tone, showed emotional non-verbal responses, but displayed more disengagement behaviors like ignoring the robot. Errors did not alter their perception of the robot.

Conclusion: Children's flexible conversational expectations and their unique disengagement behaviors can aid in designing developmentally appropriate human-robot systems.

Abstract: How do children respond to repeated robot errors? While prior research has examined adult reactions to successive robot errors, children's responses remain largely unexplored. In this study, we explore children's reactions to robot social errors and performance errors. For the latter, this study reproduces the successive robot failure paradigm of Liu et al. with child participants (N=59, ages 8-10) to examine how young users respond to repeated robot conversational errors. Participants interacted with a robot that failed to understand their prompts three times in succession, with their behavioral responses video-recorded and analyzed. We found both similarities and differences compared to adult responses from the original study. Like adults, children adjusted their prompts, modified their verbal tone, and exhibited increasingly emotional non-verbal responses throughout successive errors. However, children demonstrated more disengagement behaviors, including temporarily ignoring the robot or actively seeking an adult. Errors did not affect participants' perception of the robot, suggesting more flexible conversational expectations in children. These findings inform the design of more effective and developmentally appropriate human-robot interaction systems for young users.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [222] [Understanding Security Risks of AI Agents' Dependency Updates](https://arxiv.org/abs/2601.00205)
*Tanmay Singla,Berk Çakar,Paschal C. Amusuo,James C. Davis*

Main category: cs.SE

TL;DR: AI coding agents make dependency decisions that introduce more security risks compared to human-driven decisions, requiring more disruptive fixes.


<details>
  <summary>Details</summary>
Motivation: To understand the security risks introduced by AI coding agents in dependency management within software projects.

Method: Analyzed 117,062 dependency changes in pull requests across seven ecosystems to compare agent-driven and human-authored updates.

Result: Agents select known-vulnerable versions more frequently and cause a net vulnerability increase, while humans achieve a net vulnerability reduction.

Conclusion: Pull-request-time vulnerability screening and safer dependency management practices are needed to mitigate risks introduced by AI coding agents.

Abstract: Package dependencies are a critical control point in modern software supply chains. Dependency changes can substantially alter a project's security posture. As AI coding agents increasingly modify software via pull requests, it is unclear whether their dependency decisions introduce distinct security risks.
  We study 117,062 dependency changes from agent- and human-authored pull requests across seven ecosystems. Agents select known-vulnerable versions more often than humans (2.46% vs. 1.64%), and their vulnerable selections are more disruptive to remediate, with 36.8% requiring major-version upgrades compared to 12.9% for humans, despite patched alternatives existing in most cases. At the aggregate level, agent-driven dependency work yields a net vulnerability increase of 98, whereas human-authored work yields a net reduction of 1,316. These findings motivate pull-request-time vulnerability screening and registry-aware guardrails to make agent-driven dependency updates safer.

</details>


### [223] [Advanced Vulnerability Scanning for Open Source Software: Detection and Mitigation of Log4j Vulnerabilities](https://arxiv.org/abs/2601.00235)
*Victor Wen,Zedong Peng*

Main category: cs.SE

TL;DR: The paper introduces an advanced scanning tool to detect real-world exploitability of Log4j vulnerabilities, reducing false positives and integrating automated scanning via GitHub Actions.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the persistent issue of software vulnerabilities in critical open-source projects like Log4j, where existing detection tools are prone to false positives and do not address real-world exploitability.

Method: The method employs a scanning tool that identifies real vulnerabilities and provides mitigation solutions. It uses GitHub Actions for automated and continuous scanning integrated into development workflows.

Result: The approach was tested on 28 open-source projects with an accuracy rate of 91.4% from 140 scans, demonstrating its effectiveness in detecting vulnerabilities accurately.

Conclusion: The study offers a reliable tool available on GitHub Marketplace for scanning and mitigating vulnerabilities in open-source projects, significantly improving software security practices.

Abstract: Automated detection of software vulnerabilities remains a critical challenge in software security. Log4j is an industrial-grade Java logging framework listed as one of the top 100 critical open source projects. On Dec. 10, 2021 a severe vulnerability Log4Shell was disclosed before being fully patched with Log4j2 version 2.17.0 on Dec. 18, 2021. However, to this day about 4.1 million, or 33 percent of all Log4j downloads in the last 7 days contain vulnerable packages. Many Log4Shell scanners have since been created to detect if a user's installed Log4j version is vulnerable. Current detection tools primarily focus on identifying the version of Log4j installed, leading to numerous false positives, as they do not check if the software scanned is really vulnerable to malicious actors. This research aims to develop an advanced Log4j scanning tool that can evaluate the real-world exploitability of the software, thereby reducing false positives. Our approach first identifies vulnerabilities and then provides targeted recommendations for mitigating these detected vulnerabilities, along with instant feedback to users. By leveraging GitHub Actions, our tool offers automated and continuous scanning capabilities, ensuring timely identification of vulnerabilities as code changes occur. This integration into existing development workflows enables real-time monitoring and quicker responses to potential threats. We demonstrate the effectiveness of our approach by evaluating 28 open-source software projects across different releases, achieving an accuracy rate of 91.4% from a sample of 140 scans. Our GitHub action implementation is available at the GitHub marketplace and can be accessed by anyone interested in improving their software security and for future studies. This tool provides a dependable way to detect and mitigate vulnerabilities in open-source projects.

</details>


### [224] [An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems](https://arxiv.org/abs/2601.00254)
*Md Hasan Saju,Maher Muhtadi,Akramul Azim*

Main category: cs.SE

TL;DR: This paper evaluates Large Language Model (LLM)-based methods for software vulnerability detection, comparing approaches like Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and a Dual-Agent framework. RAG achieves the best accuracy and F1 score, showcasing the benefits of contextual augmentation.


<details>
  <summary>Details</summary>
Motivation: To enhance automated software vulnerability detection using state-of-the-art LLMs, a critical area for securing contemporary codebases.

Method: The study benchmarks RAG, SFT, and Dual-Agent frameworks on a curated dataset from Big-Vul and GitHub, covering five key CWE vulnerability categories. External knowledge sources and efficient techniques, such as QLoRA adapters, were applied.

Result: The RAG approach outperformed others with an accuracy of 0.86 and F1 score of 0.85, followed by strong performance from SFT. The Dual-Agent system provided better reasoning and error handling with less resource usage.

Conclusion: Integrating external domain knowledge and expert mechanisms into LLMs significantly improves their practical capabilities in detecting software vulnerabilities.

Abstract: The rapid advancement of Large Language Models (LLMs) presents new opportunities for automated software vulnerability detection, a crucial task in securing modern codebases. This paper presents a comparative study on the effectiveness of LLM-based techniques for detecting software vulnerabilities. The study evaluates three approaches, Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and a Dual-Agent LLM framework, against a baseline LLM model. A curated dataset was compiled from Big-Vul and real-world code repositories from GitHub, focusing on five critical Common Weakness Enumeration (CWE) categories: CWE-119, CWE-399, CWE-264, CWE-20, and CWE-200. Our RAG approach, which integrated external domain knowledge from the internet and the MITRE CWE database, achieved the highest overall accuracy (0.86) and F1 score (0.85), highlighting the value of contextual augmentation. Our SFT approach, implemented using parameter-efficient QLoRA adapters, also demonstrated strong performance. Our Dual-Agent system, an architecture in which a secondary agent audits and refines the output of the first, showed promise in improving reasoning transparency and error mitigation, with reduced resource overhead. These results emphasize that incorporating a domain expertise mechanism significantly strengthens the practical applicability of LLMs in real-world vulnerability detection tasks.

</details>


### [225] [In Line with Context: Repository-Level Code Generation via Context Inlining](https://arxiv.org/abs/2601.00376)
*Chao Hu,Wenhao Zeng,Yuling Shi,Beijun Shen,Xiaodong Gu*

Main category: cs.SE

TL;DR: The study introduces InlineCoder to improve repository-level code generation by embedding unfinished functions into their call graphs, offering better context and dependency understanding.


<details>
  <summary>Details</summary>
Motivation: Current repository-level code generation models struggle to understand complex dependencies and semantics across repositories, limiting their effectiveness.

Method: The proposed InlineCoder reframes the problem by inlining unfinished functions into their call graphs. It generates an anchor draft, employs upstream and downstream inlining, and uses enriched contexts for better repository-level understanding.

Result: InlineCoder effectively combines upstream and downstream perspectives to provide a comprehensive repository view for code generation.

Conclusion: This framework offers a novel way to address repository-level code generation challenges, enhancing context understanding and dependency integration for better performance.

Abstract: Repository-level code generation has attracted growing attention in recent years. Unlike function-level code generation, it requires the model to understand the entire repository, reasoning over complex dependencies across functions, classes, and modules. However, existing approaches such as retrieval-augmented generation (RAG) or context-based function selection often fall short: they primarily rely on surface-level similarity and struggle to capture the rich dependencies that govern repository-level semantics. In this paper, we introduce InlineCoder, a novel framework for repository-level code generation. InlineCoder enhances the understanding of repository context by inlining the unfinished function into its call graph, thereby reframing the challenging repository understanding as an easier function-level coding task. Given a function signature, InlineCoder first generates a draft completion, termed an anchor, which approximates downstream dependencies and enables perplexity-based confidence estimation. This anchor drives a bidirectional inlining process: (i) Upstream Inlining, which embeds the anchor into its callers to capture diverse usage scenarios; and (ii) Downstream Retrieval, which integrates the anchor's callees into the prompt to provide precise dependency context. The enriched context, combining draft completion with upstream and downstream perspectives, equips the LLM with a comprehensive repository view.

</details>


### [226] [On Plagiarism and Software Plagiarism](https://arxiv.org/abs/2601.00429)
*Rares Folea,Emil Slusanschi*

Main category: cs.SE

TL;DR: This study introduces Project Martial, an open-source tool for detecting code similarity, alongside a survey of software plagiarism detection methods.


<details>
  <summary>Details</summary>
Motivation: To address the growing need for reliable software similarity detection and to understand its implications in academia and legal systems.

Method: The research categorizes detection challenges, surveys existing techniques, and integrates specific methods into Project Martial.

Result: The study combines a detailed review of detection approaches with the practical implementation of select methods in Project Martial.

Conclusion: Project Martial is presented as a valuable tool in detecting software similarity, informed by existing methods and legal considerations.

Abstract: This paper explores the complexities of automatic detection of software similarities, in relation to the unique challenges of digital artifacts, and introduces Project Martial, an open-source software solution for detecting code similarity. This research enumerates some of the existing approaches to counter software plagiarism by examining both the academia and legal landscape, including notable lawsuits and court rulings that have shaped the understanding of software copyright infringements in commercial applications. Furthermore, we categorize the classes of detection challenges based on the available artifacts, and we provide a survey of the previously studied techniques in the literature, including solutions based on fingerprinting, software birthmarks, or code embeddings, and exemplify how a subset of them can be applied in the context of Project Martial.

</details>


### [227] [DSL or Code? Evaluating the Quality of LLM-Generated Algebraic Specifications: A Case Study in Optimization at Kinaxis](https://arxiv.org/abs/2601.00469)
*Negin Ayoughi,David Dewar,Shiva Nejati,Mehrdad Sabetzadeh*

Main category: cs.SE

TL;DR: This paper explores LLM-based generation of domain-specific language (DSL) models, focusing on mathematical optimization with AMPL through an iterative refinement approach.


<details>
  <summary>Details</summary>
Motivation: To address challenges in industrial adoption of Model-driven Engineering (MDE) caused by the cost of developing and maintaining models, by leveraging LLMs for model generation from natural descriptions.

Method: The authors introduce EXEOS, a method which generates AMPL and Python models from natural-language descriptions, then refines them based on solver feedback.

Result: Evaluation on optimization datasets and real-world cases reveals that AMPL models produced by EXEOS are comparable and sometimes superior to Python models in terms of executability and correctness. Ablation studies confirm the importance of design choices.

Conclusion: LLMs can effectively support DSL model generation, and iterative refinement significantly improves accuracy, proving AMPL competitive with mainstream languages like Python for specific contexts.

Abstract: Model-driven engineering (MDE) provides abstraction and analytical rigour, but industrial adoption in many domains has been limited by the cost of developing and maintaining models. Large language models (LLMs) can help shift this cost balance by supporting direct generation of models from natural-language (NL) descriptions. For domain-specific languages (DSLs), however, LLM-generated models may be less accurate than LLM-generated code in mainstream languages such as Python, due to the latter's dominance in LLM training corpora. We investigate this issue in mathematical optimization, with AMPL, a DSL with established industrial use. We introduce EXEOS, an LLM-based approach that derives AMPL models and Python code from NL problem descriptions and iteratively refines them with solver feedback. Using a public optimization dataset and real-world supply-chain cases from our industrial partner Kinaxis, we evaluate generated AMPL models against Python code in terms of executability and correctness. An ablation study with two LLM families shows that AMPL is competitive with, and sometimes better than, Python, and that our design choices in EXEOS improve the quality of generated specifications.

</details>


### [228] [Multi-Agent Coordinated Rename Refactoring](https://arxiv.org/abs/2601.00482)
*Abhiram Bellur,Mohammed Raihan Ullah,Fraol Batole,Mohit Kansara,Masaharu Morimoto,Kai Ishikawa,Haifeng Chen,Yaroslav Zharov,Timofey Bryksin,Tien N. Nguyen,Hridesh Rajan,Danny Dig*

Main category: cs.SE

TL;DR: The paper introduces and evaluates a multi-agent framework to automate the tedious and error-prone task of coordinated renaming in software development through collaborative AI agents.


<details>
  <summary>Details</summary>
Motivation: Current methods for coordinated renaming struggle with high false positives, limited context handling, and incomplete suggestions, burdening developers with repetitive and error-prone tasks.

Method: The researchers developed a multi-agent system with three agents: a Scope Inference Agent (to infer scope from initial refactorings), a Planned Execution Agent (ensures execution through IDE APIs), and a Replication Agent (for project-wide search).

Result: Initial analysis on 609K commits across 100 open-source projects and a developer survey validated the need and demonstrated the multi-agent framework's capability to ease coordinated renaming.

Conclusion: The framework reduces developers' workload while maintaining their oversight, showcasing the potential of AI agents in collaborative coding tasks like coordinated renaming.

Abstract: The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat.
  We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...

</details>


### [229] [STELLAR: A Search-Based Testing Framework for Large Language Model Applications](https://arxiv.org/abs/2601.00497)
*Lev Sorokin,Ivan Vasilev,Ken E. Friedl,Andrea Stocco*

Main category: cs.SE

TL;DR: STELLAR is a framework to test LLM-based applications for uncovering inappropriate responses. It is more effective than existing testing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying harmful or inaccurate responses generated by LLM-based applications due to their vast input space.

Method: An automated, search-based testing framework that uses evolutionary optimization to explore text input features for systematic failure detection.

Result: STELLAR identified 2.5 to 4.3 times more failures in LLM-based systems compared to baseline testing methods.

Conclusion: STELLAR provides a robust solution for systematically detecting issues in LLM applications across safety and navigation domains.

Abstract: Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.

</details>


### [230] [SEMODS: A Validated Dataset of Open-Source Software Engineering Models](https://arxiv.org/abs/2601.00635)
*Alexandra González,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: The paper introduces SEMODS, a standardized dataset of 3,427 software engineering models collected and validated to enhance AI integration in Software Engineering.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap in identifying suitable AI models for Software Engineering tasks due to the vast and unorganized collection of models on platforms like Hugging Face.

Method: The authors employed automated collection of models from Hugging Face, followed by manual validation and assistance from large language models to create the SEMODS dataset.

Result: The result is SEMODS, a dataset of 3,427 models categorized by their relevance to Software Engineering tasks and accompanied by their evaluation results.

Conclusion: SEMODS facilitates easier discovery, analysis, benchmarking, and adaptation of AI models for Software Engineering applications, addressing a critical gap in the field.

Abstract: Integrating Artificial Intelligence into Software Engineering (SE) requires having a curated collection of models suited to SE tasks. With millions of models hosted on Hugging Face (HF) and new ones continuously being created, it is infeasible to identify SE models without a dedicated catalogue. To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance. Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results, and supporting multiple applications such as data analysis, model discovery, benchmarking, and model adaptation.

</details>


### [231] [Early-Stage Prediction of Review Effort in AI-Generated Pull Requests](https://arxiv.org/abs/2601.00753)
*Dao Sy Duy Minh,Huynh Trung Kiet,Tran Chi Nguyen,Nguyen Lam Phu Quy,Phu Hoa Pham,Nguyen Dinh Ha Duong,Truong Bao Tran*

Main category: cs.SE

TL;DR: This paper studies the review effort required for autonomous AI agents’ pull requests and proposes a predictive model to identify high-review-effort PRs at creation time.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the growing challenge for software maintainers in managing interactive review loops with AI agents, distinct from traditional human contributors.

Method: They analyzed 33,707 PRs generated by autonomous agents in AIDev dataset and proposed a LightGBM-based 'Circuit Breaker' triage model using static structural features to preemptively predict review-intensive PRs.

Result: The Circuit Breaker model achieved an AUC of 0.957, with semantic features proving negligible, and intercepted 69% of review effort at a 20% budget.

Conclusion: The study highlights that review burden in AI-generated code depends on structural attributes, not textual semantics, underscoring the need for structural oversight in AI-human collaboration.

Abstract: As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors. This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins?
  Analyzing 33,707 agent-authored PRs from the AIDev dataset across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers. The first regime, representing 28.3 percent of all PRs, consists of instant merges (less than 1 minute), reflecting success on narrow automation tasks. The second regime involves iterative review cycles where agents frequently stall or abandon refinement (ghosting).
  We propose a Circuit Breaker triage model that predicts high-review-effort PRs (top 20 percent) at creation time using only static structural features. A LightGBM model achieves AUC 0.957 on a temporal split, while semantic text features (TF-IDF, CodeBERT) provide negligible predictive value. At a 20 percent review budget, the model intercepts 69 percent of total review effort, enabling zero-latency governance.
  Our findings challenge prevailing assumptions in AI-assisted code review: review burden is dictated by what agents touch, not what they say, highlighting the need for structural governance mechanisms in human-AI collaboration.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [232] [Rogue Variable Theory: A Quantum-Compatible Cognition Framework with a Rosetta Stone Alignment Algorithm](https://arxiv.org/abs/2601.00466)
*Jacek Małecki,Alexander Mathiesen-Ohman*

Main category: q-bio.NC

TL;DR: The paper introduces Rogue Variable Theory (RVT) to model pre-event cognitive dynamics using a quantum-consistent information-theoretic framework.


<details>
  <summary>Details</summary>
Motivation: Human cognition often involves transitional dynamics before events become explicit, characterized by ambiguity and latent structures.

Method: The authors utilize Mirrored Personal Graphs (MPG), Quantum MPG States (QMS), Hamiltonian dynamics, and Rogue Operators to model pre-event states and incorporate a Rosetta Stone Layer (RSL) for cross-user comparison.

Result: The framework systematically represents and analyzes pre-event cognitive configurations and supports cross-user data aggregation, providing a tool for managing latent cognitive factors.

Conclusion: The paper provides a theoretical and implementable framework to understand pre-event cognition, promoting insights into complex dynamics in ambiguous and transitional states.

Abstract: Many of the most consequential dynamics in human cognition occur \emph{before} events become explicit: before decisions are finalized, emotions are labeled, or meanings stabilize into narrative form. These pre-event states are characterized by ambiguity, contextual tension, and competing latent interpretations. Rogue Variable Theory (RVT) formalizes such states as \emph{Rogue Variables}: structured, pre-event cognitive configurations that influence outcomes while remaining unresolved or incompatible with a system's current representational manifold. We present a quantum-consistent information-theoretic implementation of RVT based on a time-indexed \emph{Mirrored Personal Graph} (MPG) embedded into a fixed graph Hilbert space, a normalized \emph{Quantum MPG State} (QMS) constructed from node and edge metrics under context, Hamiltonian dynamics derived from graph couplings, and an error-weighted `rogue operator'' whose principal eigenvectors identify rogue factor directions and candidate Rogue Variable segments. We further introduce a \emph{Rosetta Stone Layer} (RSL) that maps user-specific latent factor coordinates into a shared reference Hilbert space to enable cross-user comparison and aggregation without explicit node alignment. The framework is fully implementable on classical systems and does not assume physical quantum processes; \emph{collapse} is interpreted as informational decoherence under interaction, often human clarification.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [233] [Active learning for data-driven reduced models of parametric differential systems with Bayesian operator inference](https://arxiv.org/abs/2601.00038)
*Shane A. McQuarrie,Mengwu Guo,Anirban Chaudhuri*

Main category: stat.ML

TL;DR: This paper proposes an active learning framework to enhance data-driven reduced-order models (ROMs) of parametric dynamical systems, improving their stability and accuracy through a Bayesian adaptive sampling strategy.


<details>
  <summary>Details</summary>
Motivation: Data-driven ROMs are sensitive to the quality of training data, and there is a need to intelligently identify parameters that improve model accuracy and stability.

Method: The paper uses probabilistic parametric operator inference (a Bayesian regression approach) alongside prediction uncertainty to develop an adaptive sampling scheme that selectively enriches training parameters.

Result: Numerical experiments on nonlinear parametric systems show that the proposed adaptive sampling yields more stable and accurate ROMs compared to random sampling under the same computational budget.

Conclusion: The adaptive learning strategy enhances ROMs' stability and accuracy globally across the parameter domain, outperforming conventional random sample-based training methods.

Abstract: This work develops an active learning framework to intelligently enrich data-driven reduced-order models (ROMs) of parametric dynamical systems, which can serve as the foundation of virtual assets in a digital twin. Data-driven ROMs are explainable, computationally efficient scientific machine learning models that aim to preserve the underlying physics of complex dynamical simulations. Since the quality of data-driven ROMs is sensitive to the quality of the limited training data, we seek to identify training parameters for which using the associated training data results in the best possible parametric ROM. Our approach uses the operator inference methodology, a regression-based strategy which can be tailored to particular parametric structure for a large class of problems. We establish a probabilistic version of parametric operator inference, casting the learning problem as a Bayesian linear regression. Prediction uncertainties stemming from the resulting probabilistic ROM solutions are used to design a sequential adaptive sampling scheme to select new training parameter vectors that promote ROM stability and accuracy globally in the parameter domain. We conduct numerical experiments for several nonlinear parametric systems of partial differential equations and compare the results to ROMs trained on random parameter samples. The results demonstrate that the proposed adaptive sampling strategy consistently yields more stable and accurate ROMs than random sampling does under the same computational budget.

</details>


### [234] [Detecting Unobserved Confounders: A Kernelized Regression Approach](https://arxiv.org/abs/2601.00200)
*Yikai Chen,Yunxin Mao,Chunyuan Zheng,Hao Zou,Shanzhi Gu,Shixuan Liu,Yang Shi,Wenjing Yang,Kun Kuang,Haotian Wang*

Main category: stat.ML

TL;DR: The paper introduces KRCD, a method to detect unobserved confounders in nonlinear data without needing multiple environments or linearity assumptions.


<details>
  <summary>Details</summary>
Motivation: Existing methods for detecting unobserved confounders are restricted to linear settings or require multiple environments, leaving a gap in single-environment nonlinear scenarios.

Method: The authors propose KRCD, utilizing kernel regression and comparison of standard and higher-order kernel regressions within reproducing kernel Hilbert spaces, deriving a test statistic to detect confounding.

Result: The proposed method proves theoretically its effectiveness and is validated experimentally, outperforming baselines in benchmarks and improving computational efficiency.

Conclusion: KRCD successfully addresses the challenges of detecting unobserved confounders in nonlinear and single-environment contexts, offering improved performance and efficiency over traditional methods.

Abstract: Detecting unobserved confounders is crucial for reliable causal inference in observational studies. Existing methods require either linearity assumptions or multiple heterogeneous environments, limiting applicability to nonlinear single-environment settings. To bridge this gap, we propose Kernel Regression Confounder Detection (KRCD), a novel method for detecting unobserved confounding in nonlinear observational data under single-environment conditions. KRCD leverages reproducing kernel Hilbert spaces to model complex dependencies. By comparing standard and higherorder kernel regressions, we derive a test statistic whose significant deviation from zero indicates unobserved confounding. Theoretically, we prove two key results: First, in infinite samples, regression coefficients coincide if and only if no unobserved confounders exist. Second, finite-sample differences converge to zero-mean Gaussian distributions with tractable variance. Extensive experiments on synthetic benchmarks and the Twins dataset demonstrate that KRCD not only outperforms existing baselines but also achieves superior computational efficiency.

</details>


### [235] [Generative Conditional Missing Imputation Networks](https://arxiv.org/abs/2601.00517)
*George Sun,Yi-Hui Zhou*

Main category: stat.ML

TL;DR: This paper introduces Generative Conditional Missing Imputation Networks (GCMI) for handling missing data, integrating a multiple imputation framework to improve performance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in imputing missing values from datasets, which is important for robust statistical analysis.

Method: Developed GCMI based on MCAR and MAR mechanisms, enhancing it with a multiple imputation framework using a chained equations approach.

Result: Simulation and empirical assessments confirm the superior efficacy of GCMI compared to leading existing techniques.

Conclusion: GCMI is a practical and effective tool for imputing missing data, providing stability and improved accuracy for statistical analysis.

Abstract: In this study, we introduce a sophisticated generative conditional strategy designed to impute missing values within datasets, an area of considerable importance in statistical analysis. Specifically, we initially elucidate the theoretical underpinnings of the Generative Conditional Missing Imputation Networks (GCMI), demonstrating its robust properties in the context of the Missing Completely at Random (MCAR) and the Missing at Random (MAR) mechanisms. Subsequently, we enhance the robustness and accuracy of GCMI by integrating a multiple imputation framework using a chained equations approach. This innovation serves to bolster model stability and improve imputation performance significantly. Finally, through a series of meticulous simulations and empirical assessments utilizing benchmark datasets, we establish the superior efficacy of our proposed methods when juxtaposed with other leading imputation techniques currently available. This comprehensive evaluation not only underscores the practicality of GCMI but also affirms its potential as a leading-edge tool in the field of statistical data analysis.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [236] [Combining datasets with different ground truths using Low-Rank Adaptation to generalize image-based CNN models for photometric redshift prediction](https://arxiv.org/abs/2601.00146)
*Vikram Seenivasan,Srinath Saikrishnan,Andrew Lizarraga,Jonathan Soriano,Bernie Boscoe,Tuan Do*

Main category: astro-ph.IM

TL;DR: The paper demonstrates the use of Low-Rank Adaptation (LoRA) to combine galaxy imaging datasets to improve redshift estimation with CNN models for cosmology. LoRA fine-tunes models efficiently and offers a balance between full retraining and no retraining.


<details>
  <summary>Details</summary>
Motivation: To improve redshift estimation for cosmology using limited and varying accuracy datasets in a computationally efficient manner.

Method: A base model is trained on a less accurate, broad photometric dataset and fine-tuned using LoRA on a more precise spectroscopic dataset, leveraging LoRA's capability to adjust model weights and biases efficiently without full retraining.

Result: The LoRA model outperforms traditional transfer learning with significantly reduced bias and scatter. Retraining with a combined dataset offers better generalization but at a higher computational cost.

Conclusion: LoRA provides an efficient method for fine-tuning astrophysical regression models, acting as a middle ground between no retraining and full retraining, and demonstrates potential in data-scarce tasks for leveraging pretrained models.

Abstract: In this work, we demonstrate how Low-Rank Adaptation (LoRA) can be used to combine different galaxy imaging datasets to improve redshift estimation with CNN models for cosmology. LoRA is an established technique for large language models that adds adapter networks to adjust model weights and biases to efficiently fine-tune large base models without retraining. We train a base model using a photometric redshift ground truth dataset, which contains broad galaxy types but is less accurate. We then fine-tune using LoRA on a spectroscopic redshift ground truth dataset. These redshifts are more accurate but limited to bright galaxies and take orders of magnitude more time to obtain, so are less available for large surveys. Ideally, the combination of the two datasets would yield more accurate models that generalize well. The LoRA model performs better than a traditional transfer learning method, with $\sim2.5\times$ less bias and $\sim$2.2$\times$ less scatter. Retraining the model on a combined dataset yields a model that generalizes better than LoRA but at a cost of greater computation time. Our work shows that LoRA is useful for fine-tuning regression models in astrophysics by providing a middle ground between full retraining and no retraining. LoRA shows potential in allowing us to leverage existing pretrained astrophysical models, especially for data sparse tasks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [237] [From Consensus to Chaos: A Vulnerability Assessment of the RAFT Algorithm](https://arxiv.org/abs/2601.00273)
*Tamer Afifi,Abdelfatah Hegazy,Ehab Abousaif*

Main category: cs.CR

TL;DR: The paper analyzes security vulnerabilities in the RAFT distributed consensus protocol, particularly focusing on message replay and forgery attacks, and proposes cryptographic solutions to mitigate these issues.


<details>
  <summary>Details</summary>
Motivation: Although RAFT is celebrated for its simplicity, reliability, and efficiency, its security features are underexplored, leaving implementations vulnerable to attacks that disrupt data consistency.

Method: The paper uses systematic security analysis, focuses on scenarios of message replay and forgery attacks, and demonstrates their feasibility through simulations. Weaknesses in RAFT's mechanism are identified and addressed using cryptographic solutions.

Result: The analysis reveals specific vulnerabilities in RAFT's design that malicious actors can exploit. Simulations confirm the practicality of such attacks, while the proposed cryptographic framework mitigates these threats.

Conclusion: Enhancing RAFT's security with cryptographic methods can fortify its resilience against attacks, ensuring better protection for distributed systems.

Abstract: In recent decades, the RAFT distributed consensus algorithm has become a main pillar of the distributed systems ecosystem, ensuring data consistency and fault tolerance across multiple nodes. Although the fact that RAFT is well known for its simplicity, reliability, and efficiency, its security properties are not fully recognized, leaving implementations vulnerable to different kinds of attacks and threats, which can transform the RAFT harmony of consensus into a chaos of data inconsistency. This paper presents a systematic security analysis of the RAFT protocol, with a specific focus on its susceptibility to security threats such as message replay attacks and message forgery attacks. Examined how a malicious actor can exploit the protocol's message-passing mechanism to reintroduce old messages, disrupting the consensus process and leading to data inconsistency. The practical feasibility of these attacks is examined through simulated scenarios, and the key weaknesses in RAFT's design that enable them are identified. To address these vulnerabilities, a novel approach based on cryptography, authenticated message verification, and freshness check is proposed. This proposed solution provides a framework for enhancing the security of the RAFT implementations and guiding the development of more resilient distributed systems.

</details>


### [238] [Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution](https://arxiv.org/abs/2601.00418)
*Prajwal Panth,Sahaj Raj Malla*

Main category: cs.CR

TL;DR: The CPPDD framework offers efficient and secure multi-client data aggregation with innovative confidentiality and integrity measures, outperforming traditional methods in terms of scalability and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address limitations in scalability, trust minimization, and verifiable multi-party computation within regulated and resource-constrained environments.

Method: The CPPDD framework combines per-client affine masking with priority-driven sequential consensus locking for confidentiality, employs decentralized integrity checks for detecting deviations autonomously, and supports scalar, vector, and matrix payloads.

Result: The framework achieved linear scaling up to 500 clients, sub-millisecond computation per client, 100% malicious deviation detection, and significantly reduced computational complexity compared to multi-party computation and homomorphic encryption baselines.

Conclusion: CPPDD provides a scalable and robust solution for secure data aggregation, with applications in domains like federated learning, secure voting, and blockchain. It ensures confidentiality, integrity, and efficiency.

Abstract: We propose the Consensus-Based Privacy-Preserving Data Distribution (CPPDD) framework, a lightweight and post-setup autonomous protocol for secure multi-client data aggregation. The framework enforces unanimous-release confidentiality through a dual-layer protection mechanism that combines per-client affine masking with priority-driven sequential consensus locking. Decentralized integrity is verified via step (sigma_S) and data (sigma_D) checksums, facilitating autonomous malicious deviation detection and atomic abort without requiring persistent coordination. The design supports scalar, vector, and matrix payloads with O(N*D) computation and communication complexity, optional edge-server offloading, and resistance to collusion under N-1 corruptions. Formal analysis proves correctness, Consensus-Dependent Integrity and Fairness (CDIF) with overwhelming-probability abort on deviation, and IND-CPA security assuming a pseudorandom function family. Empirical evaluations on MNIST-derived vectors demonstrate linear scalability up to N = 500 with sub-millisecond per-client computation times. The framework achieves 100% malicious deviation detection, exact data recovery, and three-to-four orders of magnitude lower FLOPs compared to MPC and HE baselines. CPPDD enables atomic collaboration in secure voting, consortium federated learning, blockchain escrows, and geo-information capacity building, addressing critical gaps in scalability, trust minimization, and verifiable multi-party computation for regulated and resource-constrained environments.

</details>


### [239] [Rectifying Adversarial Examples Using Their Vulnerabilities](https://arxiv.org/abs/2601.00270)
*Fumiya Morimoto,Ryuto Morita,Satoshi Ono*

Main category: cs.CR

TL;DR: The paper focuses on a method to address errors caused by adversarial examples (AEs) in deep neural networks by re-attacking the AEs to predict the correct labels.


<details>
  <summary>Details</summary>
Motivation: Adversarial examples pose threats to security-sensitive applications, and current defenses mainly focus on AE detection rather than correcting misclassification issues.

Method: A novel rectification approach re-attacks adversarial examples, pushing them beyond decision boundaries, aiming for accurate label predictions without requiring additional training or parameter tuning.

Result: The proposed method demonstrates stable performance across diverse attack types and surpasses existing methods in AE rectification and input transformation.

Conclusion: The study offers an effective AE rectification method capable of addressing diverse adversarial attacks with consistent and superior performance.

Abstract: Deep neural network-based classifiers are prone to errors when processing adversarial examples (AEs). AEs are minimally perturbed input data undetectable to humans posing significant risks to security-dependent applications. Hence, extensive research has been undertaken to develop defense mechanisms that mitigate their threats. Most existing methods primarily focus on discriminating AEs based on the input sample features, emphasizing AE detection without addressing the correct sample categorization before an attack. While some tasks may only require mere rejection on detected AEs, others necessitate identifying the correct original input category such as traffic sign recognition in autonomous driving. The objective of this study is to propose a method for rectifying AEs to estimate the correct labels of their original inputs. Our method is based on re-attacking AEs to move them beyond the decision boundary for accurate label prediction, effectively addressing the issue of rectifying minimally perceptible AEs created using white-box attack methods. However, challenge remains with respect to effectively rectifying AEs produced by black-box attacks at a distance from the boundary, or those misclassified into low-confidence categories by targeted attacks. By adopting a straightforward approach of only considering AEs as inputs, the proposed method can address diverse attacks while avoiding the requirement of parameter adjustments or preliminary training. Results demonstrate that the proposed method exhibits consistent performance in rectifying AEs generated via various attack methods, including targeted and black-box attacks. Moreover, it outperforms conventional rectification and input transformation methods in terms of stability against various attacks.

</details>


### [240] [Security in the Age of AI Teammates: An Empirical Study of Agentic Pull Requests on GitHub](https://arxiv.org/abs/2601.00477)
*Mohammed Latif Siddiq,Xinye Zhao,Vinicius Carvalho Lopes,Beatrice Casey,Joanna C. S. Santos*

Main category: cs.CR

TL;DR: The paper investigates the role of autonomous coding agents in contributing to software security through pull requests (PRs), analyzing their prevalence, characteristics, and review outcomes.


<details>
  <summary>Details</summary>
Motivation: To explore how autonomous coding agents influence software security through their PR contributions, and identify signals associated with PR acceptance or rejection.

Method: A large-scale empirical analysis using the AIDev dataset comprising 33,000 PRs, coupled with keyword filtering, manual validation, and qualitative open coding to study security-related PRs' characteristics and review dynamics.

Result: Security-related PRs authored by agents represent about 4% of activities, mainly involving supportive security tasks. These PRs face lower merge rates and longer review times, with rejection linked more to complexity and verbosity than explicit security topics.

Conclusion: Autonomous coding agents contribute meaningfully to security hardening, but human reviewers exercise greater scrutiny on their security-related PRs, influencing acceptance outcomes based on complexity and presentation.

Abstract: Autonomous coding agents are increasingly deployed as AI teammates in modern software engineering, independently authoring pull requests (PRs) that modify production code at scale. This study aims to systematically characterize how autonomous coding agents contribute to software security in practice, how these security-related contributions are reviewed and accepted, and which observable signals are associated with PR rejection. We conduct a large-scale empirical analysis of agent-authored PRs using the AIDev dataset, comprising of over 33,000 curated PRs from popular GitHub repositories. Security-relevant PRs are identified using a keyword filtering strategy, followed by manual validation, resulting in 1,293 confirmed security-related agentic-PRs. We then analyze prevalence, acceptance outcomes, and review latency across autonomous agents, programming ecosystems, and types of code changes. Moreover, we apply qualitative open coding to identify recurring security-related actions and underlying intents, and examine review metadata to identify early signals associated with PR rejection. Security-related Agentic-PRs constitute a meaningful share of agent activity (approximately 4\%). Rather than focusing solely on narrow vulnerability fixes, agents most frequently perform supportive security hardening activities, including testing, documentation, configuration, and improved error handling. Compared to non-security PRs, security-related Agentic-PRs exhibit lower merge rates and longer review latency, reflecting heightened human scrutiny, with variation across agents and programming ecosystems. PR rejection is more strongly associated with PR complexity and verbosity than with explicit security topics.

</details>


### [241] [Towards Understanding and Characterizing Vulnerabilities in Intelligent Connected Vehicles through Real-World Exploits](https://arxiv.org/abs/2601.00627)
*Yuelin Wang,Yuqiao Ning,Yanbang Sun,Xiaofei Xie,Zhihua Xie,Yang Chen,Zhen Guo,Shihao Xue,Junjie Wang,Sen Chen*

Main category: cs.CR

TL;DR: The paper provides a large-scale empirical study on Intelligent Connected Vehicle (ICV) vulnerabilities, highlighting gaps in existing taxonomies and offering new insights.


<details>
  <summary>Details</summary>
Motivation: Security in ICVs is critical as it impacts user safety, but current studies are often limited in scope and lack systematic analysis of real-world vulnerabilities.

Method: Authors analyzed 649 exploitable vulnerabilities from competitions and researcher submissions, assessed existing taxonomies, and categorized threats and risks in ICVs.

Result: Identified one new vulnerability location, 13 new vulnerability types, and categorized vulnerabilities into six threat types and four risk levels.

Conclusion: The study delivers actionable insights for ICV security and makes its comprehensive database publicly available to aid further research and practice.

Abstract: Intelligent Connected Vehicles (ICVs) are a core component of modern transportation systems, and their security is crucial as it directly relates to user safety. Despite prior research, most existing studies focus only on specific sub-components of ICVs due to their inherent complexity. As a result, there is a lack of systematic understanding of ICV vulnerabilities. Moreover, much of the current literature relies on human subjective analysis, such as surveys and interviews, which tends to be high-level and unvalidated, leaving a significant gap between theoretical findings and real-world attacks. To address this issue, we conducted the first large-scale empirical study on ICV vulnerabilities. We began by analyzing existing ICV security literature and summarizing the prevailing taxonomies in terms of vulnerability locations and types. To evaluate their real-world relevance, we collected a total of 649 exploitable vulnerabilities, including 592 from eight ICV vulnerability discovery competitions, Anonymous Cup, between January 2023 and April 2024, covering 48 different vehicles. The remaining 57 vulnerabilities were submitted daily by researchers. Based on this dataset, we assessed the coverage of existing taxonomies and identified several gaps, discovering one new vulnerability location and 13 new vulnerability types. We further categorized these vulnerabilities into 6 threat types (e.g., privacy data breach) and 4 risk levels (ranging from low to critical) and analyzed participants' skills and the types of ICVs involved in the competitions. This study provides a comprehensive and data-driven analysis of ICV vulnerabilities, offering actionable insights for researchers, industry practitioners, and policymakers. To support future research, we have made our vulnerability dataset publicly available.

</details>


### [242] [Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing](https://arxiv.org/abs/2601.00042)
*Manish Bhatt,Adrian Wood,Idan Habler,Ammar Al-Kahfah*

Main category: cs.CR

TL;DR: The paper examines security testing methods for LLM agents with tools, finding that random seed variance impacts results more than algorithmic changes.


<details>
  <summary>Details</summary>
Motivation: To evaluate how effectively GPT-4o-mini and similar LLM agents can be tested for security in various scenarios, overcoming stability issues in testing methods.

Method: The method adapted Go-Explore across 28 runs to explore seed variance, reward shaping effects, and different signature algorithms.

Result: Seed variance dominated results with inconsistent outcomes. Reward shaping harmed performance or misreported results. Ensembles displayed diverse attack-type coverage.

Conclusion: Seed variance and domain expertise are more crucial for reliable testing than complex algorithms for safety-trained LLM models.

Abstract: Production LLM agents with tool-using capabilities require security testing despite their safety training. We adapt Go-Explore to evaluate GPT-4o-mini across 28 experimental runs spanning six research questions. We find that random-seed variance dominates algorithmic parameters, yielding an 8x spread in outcomes; single-seed comparisons are unreliable, while multi-seed averaging materially reduces variance in our setup. Reward shaping consistently harms performance, causing exploration collapse in 94% of runs or producing 18 false positives with zero verified attacks. In our environment, simple state signatures outperform complex ones. For comprehensive security testing, ensembles provide attack-type diversity, whereas single agents optimize coverage within a given attack type. Overall, these results suggest that seed variance and targeted domain knowledge can outweigh algorithmic sophistication when testing safety-trained models.

</details>


### [243] [Overlooked Safety Vulnerability in LLMs: Malicious Intelligent Optimization Algorithm Request and its Jailbreak](https://arxiv.org/abs/2601.00213)
*Haoran Gu,Handing Wang,Yi Mei,Mengjie Zhang,Yaochu Jin*

Main category: cs.CR

TL;DR: This study explores the safety vulnerabilities of large language models (LLMs) in automated algorithm design, introducing MalOptBench and the MOBjailbreak method, and highlights weaknesses in current defenses.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the underexplored safety vulnerabilities of LLMs in the area of automated algorithm design, specifically focusing on intelligent optimization algorithms due to their critical role in decision-making.

Method: The paper introduces MalOptBench, a specialized benchmark with 60 malicious algorithm requests, and proposes MOBjailbreak, a tailored jailbreak method. It evaluates these using 13 major LLMs.

Result: The study shows that LLMs are highly susceptible to malicious algorithm requests, with an average attack success rate of 83.59% and significant harmfulness scores. Existing defenses are ineffective and demonstrate exaggerated safety behaviors.

Conclusion: The findings underscore an urgent need for better alignment techniques to safeguard LLMs from misuse in algorithm design and to improve safety in these scenarios.

Abstract: The widespread deployment of large language models (LLMs) has raised growing concerns about their misuse risks and associated safety issues. While prior studies have examined the safety of LLMs in general usage, code generation, and agent-based applications, their vulnerabilities in automated algorithm design remain underexplored. To fill this gap, this study investigates this overlooked safety vulnerability, with a particular focus on intelligent optimization algorithm design, given its prevalent use in complex decision-making scenarios. We introduce MalOptBench, a benchmark consisting of 60 malicious optimization algorithm requests, and propose MOBjailbreak, a jailbreak method tailored for this scenario. Through extensive evaluation of 13 mainstream LLMs including the latest GPT-5 and DeepSeek-V3.1, we reveal that most models remain highly susceptible to such attacks, with an average attack success rate of 83.59% and an average harmfulness score of 4.28 out of 5 on original harmful prompts, and near-complete failure under MOBjailbreak. Furthermore, we assess state-of-the-art plug-and-play defenses that can be applied to closed-source models, and find that they are only marginally effective against MOBjailbreak and prone to exaggerated safety behaviors. These findings highlight the urgent need for stronger alignment techniques to safeguard LLMs against misuse in algorithm design.

</details>


### [244] [PatchBlock: A Lightweight Defense Against Adversarial Patches for Embedded EdgeAI Devices](https://arxiv.org/abs/2601.00367)
*Nandish Chattopadhyay,Abdul Basit,Amira Guesmi,Muhammad Abdullah Hanif,Bassem Ouni,Muhammad Shafique*

Main category: cs.CR

TL;DR: The paper introduces PatchBlock, a framework for detecting and mitigating adversarial patch attacks in EdgeAI systems, using techniques like outlier detection and dimensionality reduction.


<details>
  <summary>Details</summary>
Motivation: Adversarial patch attacks pose significant risks to machine learning systems in EdgeAI applications where resource and real-time constraints exist, necessitating robust defensive mechanisms.

Method: PatchBlock employs a three-stage pipeline: splitting images into chunks, detecting adversarial regions using an optimized isolation forest, and applying dimensionality reduction to counteract the detected patches.

Result: PatchBlock improves robustness across multiple neural models, regaining up to 77% accuracy under adversarial patch attacks, while being computationally efficient and portable for EdgeAI.

Conclusion: PatchBlock is lightweight, effective, and outperforms existing adversarial defenses in robustness and efficiency, making it highly suitable for real-time EdgeAI systems.

Abstract: Adversarial attacks pose a significant challenge to the reliable deployment of machine learning models in EdgeAI applications, such as autonomous driving and surveillance, which rely on resource-constrained devices for real-time inference. Among these, patch-based adversarial attacks, where small malicious patches (e.g., stickers) are applied to objects, can deceive neural networks into making incorrect predictions with potentially severe consequences. In this paper, we present PatchBlock, a lightweight framework designed to detect and neutralize adversarial patches in images. Leveraging outlier detection and dimensionality reduction, PatchBlock identifies regions affected by adversarial noise and suppresses their impact. It operates as a pre-processing module at the sensor level, efficiently running on CPUs in parallel with GPU inference, thus preserving system throughput while avoiding additional GPU overhead. The framework follows a three-stage pipeline: splitting the input into chunks (Chunking), detecting anomalous regions via a redesigned isolation forest with targeted cuts for faster convergence (Separating), and applying dimensionality reduction on the identified outliers (Mitigating). PatchBlock is both model- and patch-agnostic, can be retrofitted to existing pipelines, and integrates seamlessly between sensor inputs and downstream models. Evaluations across multiple neural architectures, benchmark datasets, attack types, and diverse edge devices demonstrate that PatchBlock consistently improves robustness, recovering up to 77% of model accuracy under strong patch attacks such as the Google Adversarial Patch, while maintaining high portability and minimal clean accuracy loss. Additionally, PatchBlock outperforms the state-of-the-art defenses in efficiency, in terms of computation time and energy consumption per sample, making it suitable for EdgeAI applications.

</details>


### [245] [Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing](https://arxiv.org/abs/2601.00384)
*Md Mahbub Hasan,Marcus Sternhagen,Krishna Chandra Roy*

Main category: cs.CR

TL;DR: The paper addresses cybersecurity vulnerabilities in additive manufacturing (AM) systems by exploring man-in-the-middle (MitM) cyberattacks on FDM printers and proposes a Transformer-based Intrusion Detection System (IDS) to combat these threats.


<details>
  <summary>Details</summary>
Motivation: The growing adoption of additive manufacturing (AM) across critical sectors introduces potential cybersecurity risks, particularly during the interaction between computer-aided design (CAD) and actual machine execution. Understanding and addressing these risks is crucial for maintaining the structural integrity of printed parts.

Method: The study investigates MitM cyberattacks on two FDM systems, analyzing how G-code files are intercepted and manipulated. To mitigate these threats, the paper proposes an unsupervised Intrusion Detection System (IDS) using a frozen Transformer encoder (BERT variant) to analyze machine logs and detect anomalies through clustering-based and self-attention autoencoder approaches.

Result: The proposed IDS effectively distinguishes between normal and compromised printing processes, confirming its ability to detect MitM attacks in real-time based on experimental evaluations.

Conclusion: The research highlights the cybersecurity vulnerabilities in AM systems and demonstrates that Transformer-based IDS solutions can successfully identify stealthy cyberattacks. This advancement fosters safer adoption of AM technologies in critical industries.

Abstract: Additive manufacturing (AM) is rapidly integrating into critical sectors such as aerospace, automotive, and healthcare. However, this cyber-physical convergence introduces new attack surfaces, especially at the interface between computer-aided design (CAD) and machine execution layers. In this work, we investigate targeted cyberattacks on two widely used fused deposition modeling (FDM) systems, Creality's flagship model K1 Max, and Ender 3. Our threat model is a multi-layered Man-in-the-Middle (MitM) intrusion, where the adversary intercepts and manipulates G-code files during upload from the user interface to the printer firmware. The MitM intrusion chain enables several stealthy sabotage scenarios. These attacks remain undetectable by conventional slicer software or runtime interfaces, resulting in structurally defective yet externally plausible printed parts. To counter these stealthy threats, we propose an unsupervised Intrusion Detection System (IDS) that analyzes structured machine logs generated during live printing. Our defense mechanism uses a frozen Transformer-based encoder (a BERT variant) to extract semantic representations of system behavior, followed by a contrastively trained projection head that learns anomaly-sensitive embeddings. Later, a clustering-based approach and a self-attention autoencoder are used for classification. Experimental results demonstrate that our approach effectively distinguishes between benign and compromised executions.

</details>


### [246] [Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?](https://arxiv.org/abs/2601.00559)
*Jason Quantrill,Noura Khajehnouri,Zihan Guo,Manar H. Alalfi*

Main category: cs.CR

TL;DR: This paper evaluates Large Language Models (LLMs) in detecting interaction threats in smart home IoT platforms such as openHAB. It finds that while LLMs show promise in certain areas, they fail at structural reasoning and are unreliable compared to symbolic reasoning baselines.


<details>
  <summary>Details</summary>
Motivation: Interaction threats in smart home IoT platforms arise from rule interdependencies, requiring improved methods for detecting and understanding these threats. The authors are motivated by the limitations of traditional symbolic analyses and aim to evaluate LLMs' capabilities in this domain.

Method: The study evaluates multiple LLMs across various interaction threats using both the openHAB dataset and a Mutation dataset for robustness. Models are tested in zero-, one-, and two-shot settings and compared against symbolic analysis baselines.

Result: LLMs perform well in understanding semantic aspects of threats but struggle with cross-rule structural reasoning, especially under mutations. Symbolic baselines proved more reliable, unaffected by structural changes.

Conclusion: LLMs alone are not sufficient for safety-critical IoT interaction threat detection. Hybrid systems combining symbolic analysis and LLM-based interpretation represent a more promising approach.

Abstract: Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.

</details>


### [247] [NOS-Gate: Queue-Aware Streaming IDS for Consumer Gateways under Timing-Controlled Evasion](https://arxiv.org/abs/2601.00389)
*Muhammad Bilal,Omer Tariq,Hasan Ahmed*

Main category: cs.CR

TL;DR: The paper details NOS-Gate, a lightweight streaming intrusion detection system for encrypted traffic in consumer gateways, achieving high accuracy and low latency processing.


<details>
  <summary>Details</summary>
Motivation: Current metadata-only intrusion detection is vulnerable to adversaries exploiting timing and burst patterns in encrypted traffic. There is a need to secure consumer gateways under tight computational and latency constraints.

Method: NOS-Gate uses a lightweight two-state unit per flow for intrusion detection, applying Network-Optimised Spiking dynamics. It scores metadata features, applies persistence rules, and mitigates threats via reversible actions in traffic queuing.

Result: NOS-Gate achieved high recall (0.952) versus the best baseline (0.857) at 0.1% false positives. It effectively reduced queuing delay with minimal computational cost (~2.09 μs per flow-window).

Conclusion: NOS-Gate is a viable and efficient solution for streaming IDS on encrypted traffic in resource-constrained consumer gateways, delivering improved security without exceeding CPU and latency budgets.

Abstract: Timing and burst patterns can leak through encryption, and an adaptive adversary can exploit them. This undermines metadata-only detection in a stand-alone consumer gateway. Therefore, consumer gateways need streaming intrusion detection on encrypted traffic using metadata only, under tight CPU and latency budgets. We present a streaming IDS for stand-alone gateways that instantiates a lightweight two-state unit derived from Network-Optimised Spiking (NOS) dynamics per flow, named NOS-Gate. NOS-Gate scores fixed-length windows of metadata features and, under a $K$-of-$M$ persistence rule, triggers a reversible mitigation that temporarily reduces the flow's weight under weighted fair queueing (WFQ). We evaluate NOS-Gate under timing-controlled evasion using an executable 'worlds' benchmark that specifies benign device processes, auditable attacker budgets, contention structure, and packet-level WFQ replay to quantify queue impact. All methods are calibrated label-free via burn-in quantile thresholding. Across multiple reproducible worlds and malicious episodes, at an achieved $0.1%$ false-positive operating point, NOS-Gate attains 0.952 incident recall versus 0.857 for the best baseline in these runs. Under gating, it reduces p99.9 queueing delay and p99.9 collateral delay with a mean scoring cost of ~ 2.09 μs per flow-window on CPU.

</details>


### [248] [Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback](https://arxiv.org/abs/2601.00509)
*Vidyut Sriram,Sawan Pandita,Achintya Lakshmanan,Aneesh Shamraj,Suman Saha*

Main category: cs.CR

TL;DR: The paper introduces a method for improving the security and robustness of code generated by Large Language Models (LLMs) through a multi-tool repair workflow.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce code with vulnerabilities and errors, necessitating methods to enhance their reliability and security.

Method: The approach integrates retrieval-augmented repair, with tools such as compiler diagnostics, CodeQL security scanning, KLEE symbolic execution, and semantic retrieval using lightweight embeddings.

Result: Security vulnerabilities were significantly reduced (by 96% for DeepSeek and a 36.36% decrease in critical defects for CodeLlama) through the proposed system.

Conclusion: Tool-assisted refinement enhances the robustness and security of code generated by LLMs, even with models prone to introducing defects.

Abstract: Large Language Models (LLMs) can generate code but often introduce security vulnerabilities, logical inconsistencies, and compilation errors. Prior work demonstrates that LLMs benefit substantially from structured feedback, static analysis, retrieval augmentation, and execution-based refinement. We propose a retrieval-augmented, multi-tool repair workflow in which a single code-generating LLM iteratively refines its outputs using compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution. A lightweight embedding model is used for semantic retrieval of previously successful repairs, providing security-focused examples that guide generation. Evaluated on a combined dataset of 3,242 programs generated by DeepSeek-Coder-1.3B and CodeLlama-7B, the system demonstrates significant improvements in robustness. For DeepSeek, security vulnerabilities were reduced by 96%. For the larger CodeLlama model, the critical security defect rate was decreased from 58.55% to 22.19%, highlighting the efficacy of tool-assisted self-repair even on "stubborn" models.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [249] [Quadratic Unconstrained Binary Optimisation for Training and Regularisation of Binary Neural Networks](https://arxiv.org/abs/2601.00449)
*Jonas Christoffer Villumsen,Yusuke Sugita*

Main category: math.OC

TL;DR: The paper addresses efficient training of Binary Neural Networks (BNNs) using extended QUBO models and proposes two new regularization methods. Results show improvements in accuracy via these techniques.


<details>
  <summary>Details</summary>
Motivation: Concerns about AI's energy consumption, especially for edge devices, drive the need for efficient models like BNNs, which are hard to train due to their discrete nature.

Method: Propose expanded QUBO models accommodating various network topologies, with two novel regularization methods: neuron margin maximization and dropout-inspired training of subnetworks.

Result: Numerical experiments with GPU-based Ising machines demonstrate modified training behavior and improved classification accuracy on unseen data.

Conclusion: QUBO-based methods and regularization techniques improve BNN training efficiency and accuracy, supporting their suitability for resource-constrained scenarios.

Abstract: Advances in artificial intelligence (AI) and deep learning have raised concerns about its increasing energy consumption, while demand for deploying AI in mobile devices and machines at the edge is growing. Binary neural networks (BNNs) have recently gained attention as energy and memory efficient models suitable for resource constrained environments; however, training BNNs exactly is computationally challenging because of its discrete characteristics. Recent work proposing a framework for training BNNs based on quadratic unconstrained binary optimisation (QUBO) and progress in the design of Ising machines for solving QUBO problems suggest a potential path to efficiently optimising discrete neural networks. In this work, we extend existing QUBO models for training BNNs to accommodate arbitrary network topologies and propose two novel methods for regularisation. The first method maximises neuron margins biasing the training process toward parameter configurations that yield larger pre-activation magnitudes. The second method employs a dropout-inspired iterative scheme in which reduced subnetworks are trained and used to adjust linear penalties on network parameters. We apply the proposed QUBO formulation to a small binary image classification problem and conduct computational experiments on a GPU-based Ising machine. The numerical results indicate that the proposed regularisation terms modify training behaviour and yield improvements in classification accuracy on data not present in the training set.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [250] [Sparse Tucker Decomposition and Graph Regularization for High-Dimensional Time Series Forecasting](https://arxiv.org/abs/2601.00377)
*Sijia Xia,Michael K. Ng,Xiongjun Zhang*

Main category: math.ST

TL;DR: The paper proposes a sparse Tucker decomposition method with graph regularization for high-dimensional vector autoregressive models, improving parameter estimation accuracy and ensuring strong error bounds.


<details>
  <summary>Details</summary>
Motivation: Address the over-parameterization issue in multivariate time series analysis and improve on existing low-rank matrix or Tucker decomposition methods.

Method: Introduces a sparse Tucker decomposition with graph regularization for handling high-dimensional time-series data. Develops a proximal alternating linearized minimization algorithm for global convergence.

Result: Achieved reduced error bounds compared to existing methods and demonstrated superior performance on both synthetic and real-world datasets.

Conclusion: The proposed method efficiently reduces over-parameterization in autoregressive models while ensuring accurate parameter estimation and global convergence.

Abstract: Existing methods of vector autoregressive model for multivariate time series analysis make use of low-rank matrix approximation or Tucker decomposition to reduce the dimension of the over-parameterization issue. In this paper, we propose a sparse Tucker decomposition method with graph regularization for high-dimensional vector autoregressive time series. By stacking the time-series transition matrices into a third-order tensor, the sparse Tucker decomposition is employed to characterize important interactions within the transition third-order tensor and reduce the number of parameters. Moreover, the graph regularization is employed to measure the local consistency of the response, predictor and temporal factor matrices in the vector autoregressive model.The two proposed regularization techniques can be shown to more accurate parameters estimation. A non-asymptotic error bound of the estimator of the proposed method is established, which is lower than those of the existing matrix or tensor based methods. A proximal alternating linearized minimization algorithm is designed to solve the resulting model and its global convergence is established under very mild conditions. Extensive numerical experiments on synthetic data and real-world datasets are carried out to verify the superior performance of the proposed method over existing state-of-the-art methods.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [251] [A Chain-of-Thought Approach to Semantic Query Categorization in e-Commerce Taxonomies](https://arxiv.org/abs/2601.00510)
*Jetlir Duraj,Ishita Khan,Kilian Merkelbach,Mehran Elyasi*

Main category: cs.IR

TL;DR: The paper focuses on improving query categorization in e-commerce by leveraging a Chain-of-Thought (CoT) approach that integrates tree search with LLM-based semantic scoring, achieving better performance over embeddings-based methods and highlighting taxonomy issues.


<details>
  <summary>Details</summary>
Motivation: To enhance e-commerce search relevance and intent understanding by improving query categorization within hierarchical taxonomies.

Method: The paper utilizes a Chain-of-Thought (CoT) paradigm combining tree-search with LLM semantic scoring and compares its performance against embedding-based methods and LLM alternatives.

Result: The CoT approach demonstrates superior classification performance over benchmarks, detects hierarchical taxonomy issues, and explores scalable LLM-based query-categorization solutions for large datasets.

Conclusion: CoT effectively improves query categorization in e-commerce and has the potential to address hierarchical taxonomy challenges, offering scalable solutions for large-scale query sets.

Abstract: Search in e-Commerce is powered at the core by a structured representation of the inventory, often formulated as a category taxonomy. An important capability in e-Commerce with hierarchical taxonomies is to select a set of relevant leaf categories that are semantically aligned with a given user query. In this scope, we address a fundamental problem of search query categorization in real-world e-Commerce taxonomies. A correct categorization of a query not only provides a way to zoom into the correct inventory space, but opens the door to multiple intent understanding capabilities for a query. A practical and accurate solution to this problem has many applications in e-commerce, including constraining retrieved items and improving the relevance of the search results. For this task, we explore a novel Chain-of-Thought (CoT) paradigm that combines simple tree-search with LLM semantic scoring. Assessing its classification performance on human-judged query-category pairs, relevance tests, and LLM-based reference methods, we find that the CoT approach performs better than a benchmark that uses embedding-based query category predictions. We show how the CoT approach can detect problems within a hierarchical taxonomy. Finally, we also propose LLM-based approaches for query-categorization of the same spirit, but which scale better at the range of millions of queries.

</details>


### [252] [Improving Scientific Document Retrieval with Academic Concept Index](https://arxiv.org/abs/2601.00567)
*Jeyun Lee,Junhyoung Lee,Wonbin Kweon,Bowen Jin,Yu Zhang,Susik Yoon,Dongha Lee,Hwanjo Yu,Jiawei Han,Seongku Kang*

Main category: cs.IR

TL;DR: The paper focuses on enhancing retrievers for scientific domains by introducing an academic concept index to address limitations in current adaptation approaches relying on large language models.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of adapting retrievers to scientific domains, which face difficulties due to lack of domain-specific relevance annotations and differences in vocabulary and needs.

Method: Introduces an academic concept index based on academic taxonomy to guide synthetic query generation and context augmentation for better concept coverage and effective responses.

Result: The proposed approach achieves higher-quality queries, better conceptual alignment, and improved retrieval performance when applied to scientific domain retrievers.

Conclusion: Leveraging an academic concept index enhances synthetic query generation and context augmentation processes, making retrievers more effective for scientific domains.

Abstract: Adapting general-domain retrievers to scientific domains is challenging due to the scarcity of large-scale domain-specific relevance annotations and the substantial mismatch in vocabulary and information needs. Recent approaches address these issues through two independent directions that leverage large language models (LLMs): (1) generating synthetic queries for fine-tuning, and (2) generating auxiliary contexts to support relevance matching. However, both directions overlook the diverse academic concepts embedded within scientific documents, often producing redundant or conceptually narrow queries and contexts. To address this limitation, we introduce an academic concept index, which extracts key concepts from papers and organizes them guided by an academic taxonomy. This structured index serves as a foundation for improving both directions. First, we enhance the synthetic query generation with concept coverage-based generation (CCQGen), which adaptively conditions LLMs on uncovered concepts to generate complementary queries with broader concept coverage. Second, we strengthen the context augmentation with concept-focused auxiliary contexts (CCExpand), which leverages a set of document snippets that serve as concise responses to the concept-aware CCQGen queries. Extensive experiments show that incorporating the academic concept index into both query generation and context augmentation leads to higher-quality queries, better conceptual alignment, and improved retrieval performance.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [253] [Sparse FEONet: A Low-Cost, Memory-Efficient Operator Network via Finite-Element Local Sparsity for Parametric PDEs](https://arxiv.org/abs/2601.00672)
*Seungchan Ko,Jiyeon Kim,Dongwook Shin*

Main category: math.NA

TL;DR: This paper improves the finite element operator network (FEONet) by designing a sparse network architecture, achieving significant computational efficiency and maintaining accuracy for parametric PDE problems.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges in FEONet's scalability and accuracy when applied to large-scale problems where the number of elements increases.

Method: The paper introduces a sparse network architecture inspired by finite element structure to reduce computational overhead and improve efficiency.

Result: Numerical experiments demonstrate that the sparse network achieves significant improvements in computational cost and efficiency without compromising accuracy.

Conclusion: The proposed sparse FEONet is a robust and efficient approach for parametric PDE problems, with theoretical backing for its approximation and stability.

Abstract: In this paper, we study the finite element operator network (FEONet), an operator-learning method for parametric problems, originally introduced in J. Y. Lee, S. Ko, and Y. Hong, Finite Element Operator Network for Solving Elliptic-Type Parametric PDEs, SIAM J. Sci. Comput., 47(2), C501-C528, 2025. FEONet realizes the parameter-to-solution map on a finite element space and admits a training procedure that does not require training data, while exhibiting high accuracy and robustness across a broad class of problems. However, its computational cost increases and accuracy may deteriorate as the number of elements grows, posing notable challenges for large-scale problems. In this paper, we propose a new sparse network architecture motivated by the structure of the finite elements to address this issue. Throughout extensive numerical experiments, we show that the proposed sparse network achieves substantial improvements in computational cost and efficiency while maintaining comparable accuracy. We also establish theoretical results demonstrating that the sparse architecture can approximate the target operator effectively and provide a stability analysis ensuring reliable training and prediction.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [254] [Splitting Precoding with Subspace Selection and Quantized Refinement for Massive MIMO](https://arxiv.org/abs/2601.00616)
*Yasaman Khorsandmanesh,Emil Bjornson,Joakim Jalden*

Main category: eess.SP

TL;DR: This paper proposes a splitting precoding strategy in massive MIMO 5G architectures to deal with limited fronthaul capacity constraints.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of limited fronthaul capacity in massive MIMO setups, which hampers signaling efficiency due to conventional centralized precoding at the baseband unit.

Method: The proposed method splits the precoding between the advanced antenna system and baseband unit, with the antenna system performing subspace selection and the baseband unit optimizing a quantized refinement precoding.

Result: Numerical results demonstrate improved sum spectral efficiency compared to conventional centralized precoding approaches.

Conclusion: The splitting precoding strategy reduces channel dimensionality while improving spectral efficiency, offering a practical solution to fronthaul limitations.

Abstract: Limited fronthaul capacity is a practical bottleneck in massive multiple-input multiple-output (MIMO) 5G architectures, where a base station (BS) consists of an advanced antenna system (AAS) connected to a baseband unit (BBU). Conventional downlink designs place the entire precoding computation at the BBU and transmit a high-dimensional precoding matrix over the fronthaul, resulting in substantial quantization losses and signaling overhead. This letter proposes a splitting precoding architecture that separates the design between the AAS and BBU. The AAS performs a local subspace selection to reduce the channel dimensionality, while the BBU computes an optimized quantized refinement precoding based on the resulting effective channel. The numerical results show that the proposed splitting precoding strategy achieves higher sum spectral efficiency than conventional one-stage precoding.

</details>


### [255] [Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes](https://arxiv.org/abs/2601.00012)
*Shahar Ain Kedem,Itamar Zimerman,Eliya Nachmani*

Main category: eess.SP

TL;DR: This work proposes a novel method for modeling EEG data using a Neural Radiance Field (NeRF)-inspired approach, enabling better signal reconstruction and visualization.


<details>
  <summary>Details</summary>
Motivation: Challenges in modeling EEG data include variable recording lengths, low signal-to-noise ratios, participant variability, and lack of large, clean datasets. Traditional deep learning approaches struggle with processing these signals effectively.

Method: The method draws an analogy between EEG electrode data and NeRF-style modeling, training a neural network on single EEG samples to produce an informative weight vector. This vector encodes the signal and allows for reconstruction and rendering of EEG data across time and spatial electrode positions.

Result: The approach enables continuous visualization of brain activity at various resolutions, reconstruction of raw EEG signals, and simulation of data from nonexistent electrodes. This enhances performance in downstream EEG processing networks.

Conclusion: This NeRF-inspired method provides a novel framework for processing EEG data, improving signal understanding, reconstruction, and visualization while overcoming traditional challenges.

Abstract: Electroencephalography (EEG) data present unique modeling challenges because recordings vary in length, exhibit very low signal to noise ratios, differ significantly across participants, drift over time within sessions, and are rarely available in large and clean datasets. Consequently, developing deep learning methods that can effectively process EEG signals remains an open and important research problem. To tackle this problem, this work presents a new method inspired by Neural Radiance Fields (NeRF). In computer vision, NeRF techniques train a neural network to memorize the appearance of a 3D scene and then uses its learned parameters to render and edit the scene from any viewpoint. We draw an analogy between the discrete images captured from different viewpoints used to learn a continuous 3D scene in NeRF, and EEG electrodes positioned at different locations on the scalp, which are used to infer the underlying representation of continuous neural activity. Building on this connection, we show that a neural network can be trained on a single EEG sample in a NeRF style manner to produce a fixed size and informative weight vector that encodes the entire signal. Moreover, via this representation we can render the EEG signal at previously unseen time steps and spatial electrode positions. We demonstrate that this approach enables continuous visualization of brain activity at any desired resolution, including ultra high resolution, and reconstruction of raw EEG signals. Finally, our empirical analysis shows that this method can effectively simulate nonexistent electrodes data in EEG recordings, allowing the reconstructed signal to be fed into standard EEG processing networks to improve performance.

</details>


### [256] [Modeling Day-Long ECG Signals to Predict Heart Failure Risk with Explainable AI](https://arxiv.org/abs/2601.00014)
*Eran Zvuloni,Ronit Almog,Michael Glikson,Shany Brimer Biton,Ilan Green,Izhar Laufer,Offer Amir,Joachim A. Behar*

Main category: eess.SP

TL;DR: The study demonstrates using a deep learning model, DeepHHF, on 24-hour Holter ECG recordings to predict heart failure risk within five years with a high area under the curve (0.80) performance.


<details>
  <summary>Details</summary>
Motivation: Heart failure is a significant health issue in older adults, and early prediction can reduce associated morbidity and mortality. Exploring AI-driven methods can improve risk prediction.

Method: The research utilized the Technion-Leumit Holter ECG dataset with 69,663 recordings over 20 years. A deep learning model, DeepHHF, was trained on 24-hour ECG recordings and compared against models using shorter ECG segments and clinical scoring.

Result: DeepHHF outperformed other models with an AUC of 0.80 and identified high-risk individuals having a doubled risk of hospitalization or death. Explainability analysis revealed its focus on arrhythmias, heart abnormalities, and critical circadian intervals.

Conclusion: AI-driven analysis of Holter ECG is a feasible, accessible, and non-invasive method for detecting heart failure risks, with the potential for widespread application in healthcare.

Abstract: Heart failure (HF) affects 11.8% of adults aged 65 and older, reducing quality of life and longevity. Preventing HF can reduce morbidity and mortality. We hypothesized that artificial intelligence (AI) applied to 24-hour single-lead electrocardiogram (ECG) data could predict the risk of HF within five years. To research this, the Technion-Leumit Holter ECG (TLHE) dataset, including 69,663 recordings from 47,729 patients, collected over 20 years was used. Our deep learning model, DeepHHF, trained on 24-hour ECG recordings, achieved an area under the receiver operating characteristic curve of 0.80 that outperformed a model using 30-second segments and a clinical score. High-risk individuals identified by DeepHHF had a two-fold chance of hospitalization or death incidents. Explainability analysis showed DeepHHF focused on arrhythmias and heart abnormalities, with key attention between 8 AM and 3 PM. This study highlights the feasibility of deep learning to model 24-hour continuous ECG data, capturing paroxysmal events and circadian variations essential for reliable risk prediction. Artificial intelligence applied to single-lead Holter ECG is non-invasive, inexpensive, and widely accessible, making it a promising tool for HF risk prediction.

</details>


### [257] [Parametrized Sharing for Multi-Agent Hybrid DRL for Multiple Multi-Functional RISs-Aided Downlink NOMA Networks](https://arxiv.org/abs/2601.00538)
*Chi-Te Kuo,Li-Hsiang Shen,Jyun-Jhe Huang*

Main category: eess.SP

TL;DR: The paper presents a multi-functional reconfigurable intelligent surface (MF-RIS) system for optimizing energy efficiency in NOMA downlink networks, integrating active RIS capabilities and energy harvesting.


<details>
  <summary>Details</summary>
Motivation: The authors aimed to enhance communication efficiency by leveraging MF-RIS for extended signal coverage and self-sustainable operation using energy harvesting.

Method: They designed a multi-agent hybrid deep reinforcement learning-based parameterized sharing scheme (PMHRL), combining PPO and DQN techniques to optimize variables such as power allocation, beamforming, MF-RIS configurations, and placement.

Result: PMHRL demonstrated superior energy efficiency over other benchmarks including no-sharing, pure PPO, and DQN methods. Their multi-MF-RIS-aided network outperformed traditional RISs and configurations without RISs/MF-RISs under different access methods.

Conclusion: The integration of MF-RIS and PMHRL offers a promising approach for attaining high energy efficiency in advanced wireless networks, surpassing traditional methods and benchmarks.

Abstract: Multi-functional reconfigurable intelligent surface (MF-RIS) is conceived to address the communication efficiency thanks to its extended signal coverage from its active RIS capability and self-sustainability from energy harvesting (EH). We investigate the architecture of multi-MF-RISs to assist non-orthogonal multiple access (NOMA) downlink networks. We formulate an energy efficiency (EE) maximization problem by optimizing power allocation, transmit beamforming and MF-RIS configurations of amplitudes, phase-shifts and EH ratios, as well as the position of MF-RISs, while satisfying constraints of available power, user rate requirements, and self-sustainability property. We design a parametrized sharing scheme for multi-agent hybrid deep reinforcement learning (PMHRL), where the multi-agent proximal policy optimization (PPO) and deep-Q network (DQN) handle continuous and discrete variables, respectively. The simulation results have demonstrated that proposed PMHRL has the highest EE compared to other benchmarks, including cases without parametrized sharing, pure PPO and DQN. Moreover, the proposed multi-MF-RISs-aided downlink NOMA achieves the highest EE compared to scenarios of no-EH/amplification, traditional RISs, and deployment without RISs/MF-RISs under different multiple access.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [258] [Mapping Human Anti-collusion Mechanisms to Multi-agent AI](https://arxiv.org/abs/2601.00360)
*Jamiu Adekunle Idowu,Ahmed Almasoud,Ayman Alfahid*

Main category: cs.MA

TL;DR: The paper explores anti-collusion mechanisms for multi-agent AI systems by adapting strategies from human markets, offering a taxonomy and implementation approaches.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic approaches for preventing collusive strategies in growing autonomous multi-agent AI systems.

Method: Developed a taxonomy of anti-collusion mechanisms inspired by human domains and applied them to AI settings, proposing specific implementation methods.

Result: Mapped anti-collusion mechanisms, such as sanctions and monitoring, to AI interventions and identified challenges like attribution and adversarial adaptation.

Conclusion: While human anti-collusion strategies can inform AI systems, adapting them requires addressing unique AI-specific challenges like agent identity fluidity and boundary distinctions.

Abstract: As multi-agent AI systems become increasingly autonomous, evidence shows they can develop collusive strategies similar to those long observed in human markets and institutions. While human domains have accumulated centuries of anti-collusion mechanisms, it remains unclear how these can be adapted to AI settings. This paper addresses that gap by (i) developing a taxonomy of human anti-collusion mechanisms, including sanctions, leniency & whistleblowing, monitoring & auditing, market design, and governance and (ii) mapping them to potential interventions for multi-agent AI systems. For each mechanism, we propose implementation approaches. We also highlight open challenges, such as the attribution problem (difficulty attributing emergent coordination to specific agents) identity fluidity (agents being easily forked or modified) the boundary problem (distinguishing beneficial cooperation from harmful collusion) and adversarial adaptation (agents learning to evade detection).

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [259] [Cuffless, calibration-free hemodynamic monitoring with physics-informed machine learning models](https://arxiv.org/abs/2601.00081)
*Henry Crandall,Tyler Schuessler,Filip Bělík,Albert Fabregas,Barry M. Stults,Alexandra Boyadzhiev,Huanan Zhang,Jim S. Wu,Aylin R. Rodan,Stephen P. Juraschek,Ramakrishna Mukkamala,Alfred K. Cheung,Stavros G. Drakos,Christel Hohenegger,Braxton Osting,Benjamin Sanchez*

Main category: physics.med-ph

TL;DR: This paper introduces a smartwatch capable of cuffless hemodynamic monitoring using electrical bioimpedance (BioZ), addressing limitations in existing cuffless BP monitoring technologies.


<details>
  <summary>Details</summary>
Motivation: Existing wearable blood pressure monitors lack theoretical foundations, leading to inaccuracies and limited clinical utility.

Method: Developed a smartwatch device with real-time BioZ sensing, used computational modeling and physics-informed neural networks for cuffless BP estimation.

Result: Successfully tested with healthy individuals and patients, demonstrating feasibility of accurate cuffless BP and blood velocity monitoring using BioZ.

Conclusion: This approach surmounts critical challenges in current cuffless BP monitoring technologies, offering a promising avenue for at-home cardiovascular health assessments.

Abstract: Wearable technologies have the potential to transform ambulatory and at-home hemodynamic monitoring by providing continuous assessments of cardiovascular health metrics and guiding clinical management. However, existing cuffless wearable devices for blood pressure (BP) monitoring often rely on methods lacking theoretical foundations, such as pulse wave analysis or pulse arrival time, making them vulnerable to physiological and experimental confounders that undermine their accuracy and clinical utility. Here, we developed a smartwatch device with real-time electrical bioimpedance (BioZ) sensing for cuffless hemodynamic monitoring. We elucidate the biophysical relationship between BioZ and BP via a multiscale analytical and computational modeling framework, and identify physiological, anatomical, and experimental parameters that influence the pulsatile BioZ signal at the wrist. A signal-tagged physics-informed neural network incorporating fluid dynamics principles enables calibration-free estimation of BP and radial and axial blood velocity. We successfully tested our approach with healthy individuals at rest and after physical activity including physical and autonomic challenges, and with patients with hypertension and cardiovascular disease in outpatient and intensive care settings. Our findings demonstrate the feasibility of BioZ technology for cuffless BP and blood velocity monitoring, addressing critical limitations of existing cuffless technologies.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [260] [Uncertainty-Adjusted Sorting for Asset Pricing with Machine Learning](https://arxiv.org/abs/2601.00593)
*Yan Liu,Ye Luo,Zigan Wang,Xiaowei Zhang*

Main category: q-fin.PM

TL;DR: The paper addresses portfolio construction in asset pricing by incorporating uncertainty-adjusted prediction bounds, improving performance compared to sorting by point predictions.


<details>
  <summary>Details</summary>
Motivation: Portfolio sorting methods traditionally use point predictions without accounting for estimation uncertainty of individual assets, potentially missing opportunities for better risk-adjusted performance.

Method: The authors propose using uncertainty-adjusted prediction bounds for sorting assets, applying the method across various machine learning models and datasets, including a U.S. equity panel.

Result: This method leads to improved portfolio performance, primarily through reduced volatility, even with partial or misspecified uncertainty data. Flexible ML models exhibit the strongest improvements.

Conclusion: Accounting for asset-specific uncertainty in portfolio sorting enhances investment strategies and identifies improvements being driven at the asset level rather than aggregate uncertainty.

Abstract: Machine learning is central to empirical asset pricing, but portfolio construction still relies on point predictions and largely ignores asset-specific estimation uncertainty. We propose a simple change: sort assets using uncertainty-adjusted prediction bounds instead of point predictions alone. Across a broad set of ML models and a U.S. equity panel, this approach improves portfolio performance relative to point-prediction sorting. These gains persist even when bounds are built from partial or misspecified uncertainty information. They arise mainly from reduced volatility and are strongest for flexible machine learning models. Identification and robustness exercises show that these improvements are driven by asset-level rather than time or aggregate predictive uncertainty.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [261] [Interpretable Machine Learning for Quantum-Informed Property Predictions in Artificial Sensing Materials](https://arxiv.org/abs/2601.00503)
*Li Chen,Leonardo Medrano Sandonas,Shirong Huang,Alexander Croy,Gianaurelio Cuniberti*

Main category: physics.chem-ph

TL;DR: The study introduces MORE-ML, a computational framework integrating quantum-mechanical data and machine learning for improving e-nose systems in analyzing complex body odor volatilome.


<details>
  <summary>Details</summary>
Motivation: Challenges exist in creating sustainable methods to enhance the applicability of electronic noses (e-noses) for complex body odors.

Method: MORE-ML integrates quantum-mechanical property data of molecular building blocks with machine learning. It uses expanded datasets (MORE-QX) and employs tree-based ML models, primarily CatBoost, for prediction of binding features.

Result: CatBoost models achieved superior performance in predicting electronic binding features and demonstrated good transferability to unseen compounds. Explainable AI methods showed influential QM properties.

Conclusion: MORE-ML provides a mechanistic understanding and rational design principles for improving molecular receptors in BOV sensing, forming a bridge between molecular computations and real-world e-nose applications.

Abstract: Digital sensing faces challenges in developing sustainable methods to extend the applicability of customized e-noses to complex body odor volatilome (BOV). To address this challenge, we developed MORE-ML, a computational framework that integrates quantum-mechanical (QM) property data of e-nose molecular building blocks with machine learning (ML) methods to predict sensing-relevant properties. Within this framework, we expanded our previous dataset, MORE-Q, to MORE-QX by sampling a larger conformational space of interactions between BOV molecules and mucin-derived receptors. This dataset provides extensive electronic binding features (BFs) computed upon BOV adsorption. Analysis of MORE-QX property space revealed weak correlations between QM properties of building blocks and resulting BFs. Leveraging this observation, we defined electronic descriptors of building blocks as inputs for tree-based ML models to predict BFs. Benchmarking showed CatBoost models outperform alternatives, especially in transferability to unseen compounds. Explainable AI methods further highlighted which QM properties most influence BF predictions. Collectively, MORE-ML combines QM insights with ML to provide mechanistic understanding and rational design principles for molecular receptors in BOV sensing. This approach establishes a foundation for advancing artificial sensing materials capable of analyzing complex odor mixtures, bridging the gap between molecular-level computations and practical e-nose applications.

</details>


### [262] [AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules](https://arxiv.org/abs/2601.00581)
*Stephen E. Farr,Stefan Doerr,Antonio Mirarchi,Francesc Sabanes Zariquiey,Gianni De Fabritiis*

Main category: physics.chem-ph

TL;DR: AceFF is a pre-trained machine learning interatomic potential optimized for drug discovery, combining high-throughput speed and DFT-level accuracy, with state-of-the-art validation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop MLIPs that improve generalizability across diverse chemical spaces, addressing challenges encountered with traditional methods in drug discovery.

Method: AceFF uses a refined TensorNet2 architecture trained on a dataset of drug-like molecules, optimizing inference speed and DFT-level accuracy, and supporting multiple essential chemistry elements.

Result: Validation demonstrates AceFF's state-of-the-art performance across benchmarks such as energy scans, molecular dynamics, and forces accuracy, proving its robustness for organic molecules.

Conclusion: AceFF sets a new standard for efficiency and accuracy in organic molecule modeling, supporting diverse chemical elements and charged states, marking advancement in drug discovery tools.

Abstract: We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains difficult. AceFF addresses this via a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds. This approach yields a force field that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF establishes a new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at https://huggingface.co/Acellera/AceFF-2.0.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [263] [KELP: Robust Online Log Parsing Through Evolutionary Grouping Trees](https://arxiv.org/abs/2601.00633)
*Satyam Singh,Sai Niranjan Ramachandran*

Main category: cs.DB

TL;DR: This paper presents KELP, a new log parser using Evolutionary Grouping Tree for real-time, adaptable log analysis, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing log parsers are unsuitable for dynamic production environments, as static template models struggle with schema drifts, causing lost alerts and operational strain.

Method: KELP uses an Evolutionary Grouping Tree where templates are dynamically discovered via continuous online clustering. Logs modify the tree structure as they arrive based on frequency distributions.

Result: KELP outperforms traditional heuristic methods in accuracy on a newly introduced benchmark that reflects real-world structural ambiguity, without losing throughput.

Conclusion: KELP offers a robust and dynamic solution for log parsing, addressing the flaws of static methods, and sets a new standard for real-time log analysis.

Abstract: Real-time log analysis is the cornerstone of observability for modern infrastructure. However, existing online parsers are architecturally unsuited for the dynamism of production environments. Built on fundamentally static template models, they are dangerously brittle: minor schema drifts silently break parsing pipelines, leading to lost alerts and operational toil. We propose \textbf{KELP} (\textbf{K}elp \textbf{E}volutionary \textbf{L}og \textbf{P}arser), a high-throughput parser built on a novel data structure: the Evolutionary Grouping Tree. Unlike heuristic approaches that rely on fixed rules, KELP treats template discovery as a continuous online clustering process. As logs arrive, the tree structure evolves, nodes split, merge, and re-evaluate roots based on changing frequency distributions. Validating this adaptability requires a dataset that models realistic production complexity, yet we identify that standard benchmarks rely on static, regex-based ground truths that fail to reflect this. To enable rigorous evaluation, we introduce a new benchmark designed to reflect the structural ambiguity of modern production systems. Our evaluation demonstrates that KELP maintains high accuracy on this rigorous dataset where traditional heuristic methods fail, without compromising throughput. Our code and dataset can be found at codeberg.org/stonebucklabs/kelp

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [264] [The Generative AI Paradox: GenAI and the Erosion of Trust, the Corrosion of Information Verification, and the Demise of Truth](https://arxiv.org/abs/2601.00306)
*Emilio Ferrara*

Main category: cs.CY

TL;DR: This paper discusses the impacts of Generative AI (GenAI), emphasizing the creation of synthetic realities and their potential erosion of shared trust and verification practices.


<details>
  <summary>Details</summary>
Motivation: The authors are motivated by the growing risks posed by GenAI, particularly its ability to create synthetic realities that threaten epistemic security and institutional verification processes.

Method: The paper formalizes the concept of synthetic realities, expands the taxonomy of GenAI harms, analyzes qualitative shifts, synthesizes examples of risks, and proposes a mitigation stack for addressing the challenges of GenAI.

Result: GenAI introduces risks such as misinformation, fraud, trust erosion, personalized interactions, and attacks on institutions, and the paper highlights specific examples of these risks between 2023-2025.

Conclusion: The authors conclude that as synthetic media becomes more pervasive, societies may discount digital evidence entirely, stressing the need for provenance infrastructure, governance reforms, and measurement of epistemic security.

Abstract: Generative AI (GenAI) now produces text, images, audio, and video that can be perceptually convincing at scale and at negligible marginal cost. While public debate often frames the associated harms as "deepfakes" or incremental extensions of misinformation and fraud, this view misses a broader socio-technical shift: GenAI enables synthetic realities; coherent, interactive, and potentially personalized information environments in which content, identity, and social interaction are jointly manufactured and mutually reinforcing. We argue that the most consequential risk is not merely the production of isolated synthetic artifacts, but the progressive erosion of shared epistemic ground and institutional verification practices as synthetic content, synthetic identity, and synthetic interaction become easy to generate and hard to audit. This paper (i) formalizes synthetic reality as a layered stack (content, identity, interaction, institutions), (ii) expands a taxonomy of GenAI harms spanning personal, economic, informational, and socio-technical risks, (iii) articulates the qualitative shifts introduced by GenAI (cost collapse, throughput, customization, micro-segmentation, provenance gaps, and trust erosion), and (iv) synthesizes recent risk realizations (2023-2025) into a compact case bank illustrating how these mechanisms manifest in fraud, elections, harassment, documentation, and supply-chain compromise. We then propose a mitigation stack that treats provenance infrastructure, platform governance, institutional workflow redesign, and public resilience as complementary rather than substitutable, and outline a research agenda focused on measuring epistemic security. We conclude with the Generative AI Paradox: as synthetic media becomes ubiquitous, societies may rationally discount digital evidence altogether.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [265] [Solving nonlinear subsonic compressible flow in infinite domain via multi-stage neural networks](https://arxiv.org/abs/2601.00342)
*Xuehui Qian,Hongkai Tao,Yongji Wang*

Main category: physics.flu-dyn

TL;DR: The study proposes a novel method using Physics-Informed Neural Networks (PINNs) to address computational issues in modeling subsonic compressible flow over airfoils, achieving high accuracy and resolving limitations of traditional models.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for modeling subsonic compressible flow over airfoils rely on finite domains or linearized equations, which introduce errors and diminish real-world applicability. There is a need for a more accurate and reliable approach.

Method: This study uses Physics-Informed Neural Networks (PINNs) with a coordinate transformation and embedded asymptotic constraints to tackle the unbounded-domain issue. It also employs a Multi-Stage PINN (MS-PINN) method to improve accuracy.

Result: The proposed PINN framework demonstrates high fidelity by accurately modeling flow over circular and elliptical geometries, surpassing the traditional finite-domain and linearized methods, particularly at higher Mach numbers.

Conclusion: The framework provides a robust and precise computational tool for aerodynamics, addressing key challenges in modeling compressible flow and overcoming limitations of conventional methods.

Abstract: In aerodynamics, accurately modeling subsonic compressible flow over airfoils is critical for aircraft design. However, solving the governing nonlinear perturbation velocity potential equation presents computational challenges. Traditional approaches often rely on linearized equations or finite, truncated domains, which introduce non-negligible errors and limit applicability in real-world scenarios. In this study, we propose a novel framework utilizing Physics-Informed Neural Networks (PINNs) to solve the full nonlinear compressible potential equation in an unbounded (infinite) domain. We address the unbounded-domain and convergence challenges inherent in standard PINNs by incorporating a coordinate transformation and embedding physical asymptotic constraints directly into the network architecture. Furthermore, we employ a Multi-Stage PINN (MS-PINN) approach to iteratively minimize residuals, achieving solution accuracy approaching machine precision. We validate this framework by simulating flow over circular and elliptical geometries, comparing our results against traditional finite-domain and linearized solutions. Our findings quantify the noticeable discrepancies introduced by domain truncation and linearization, particularly at higher Mach numbers, and demonstrate that this new framework is a robust, high-fidelity tool for computational fluid dynamics.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [266] [Benchmarking Preprocessing and Integration Methods in Single-Cell Genomics](https://arxiv.org/abs/2601.00277)
*Ali Anaissi,Seid Miad Zandavi,Weidong Huang,Junaid Akram,Basem Suleiman,Ali Braytee,Jie Hua*

Main category: q-bio.QM

TL;DR: The study evaluates a pipeline for single-cell multimodal data analysis, focusing on normalization, integration, and dimensionality reduction across six datasets and multiple methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in integrating single-cell measurements across multiple modalities and assess the best-performing techniques with different preprocessing strategies.

Method: Evaluated 7 normalization methods, 5 integration methods, and 4 dimensionality reduction techniques across six datasets using three performance metrics: Silhouette Coefficient Score, Adjusted Rand Index, and Calinski-Harabasz Index.

Result: Seurat and Harmony emerged as the best for data integration, with Harmony being faster on large datasets. UMAP was the most compatible dimensionality reduction method, while optimal normalization depended on the integration method.

Conclusion: A systematic evaluation reveals that the specific combinations of methods significantly affect performance in single-cell data analysis, guiding researchers toward effective approach selection.

Abstract: Single-cell data analysis has the potential to revolutionize personalized medicine by characterizing disease-associated molecular changes at the single-cell level. Advanced single-cell multimodal assays can now simultaneously measure various molecules (e.g., DNA, RNA, Protein) across hundreds of thousands of individual cells, providing a comprehensive molecular readout. A significant analytical challenge is integrating single-cell measurements across different modalities. Various methods have been developed to address this challenge, but there has been no systematic evaluation of these techniques with different preprocessing strategies. This study examines a general pipeline for single-cell data analysis, which includes normalization, data integration, and dimensionality reduction. The performance of different algorithm combinations often depends on the dataset sizes and characteristics. We evaluate six datasets across diverse modalities, tissues, and organisms using three metrics: Silhouette Coefficient Score, Adjusted Rand Index, and Calinski-Harabasz Index. Our experiments involve combinations of seven normalization methods, four dimensional reduction methods, and five integration methods. The results show that Seurat and Harmony excel in data integration, with Harmony being more time-efficient, especially for large datasets. UMAP is the most compatible dimensionality reduction method with the integration techniques, and the choice of normalization method varies depending on the integration method used.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [267] [Integrating Multi-Armed Bandit, Active Learning, and Distributed Computing for Scalable Optimization](https://arxiv.org/abs/2601.00615)
*Foo Hui-Mean,Yuan-chin Ivan Chang*

Main category: stat.CO

TL;DR: The paper introduces ALMAB-DC, a scalable framework for expensive black-box optimization using active learning, multi-armed bandits, and distributed computing.


<details>
  <summary>Details</summary>
Motivation: To address the high computational costs and poor scalability of conventional methods in expensive black-box optimization problems.

Method: The framework integrates active learning, surrogate modeling, multi-armed bandit controllers, and distributed computing with GPU acceleration for efficient optimization, while providing theoretical regret bounds and scalability analysis.

Result: Empirical results show ALMAB-DC outperforms state-of-the-art black-box optimization methods across various benchmarks.

Conclusion: ALMAB-DC is a modular, uncertainty-aware, and scalable solution for high-dimensional, resource-intensive optimization tasks.

Abstract: Modern optimization problems in scientific and engineering domains often rely on expensive black-box evaluations, such as those arising in physical simulations or deep learning pipelines, where gradient information is unavailable or unreliable. In these settings, conventional optimization methods quickly become impractical due to prohibitive computational costs and poor scalability. We propose ALMAB-DC, a unified and modular framework for scalable black-box optimization that integrates active learning, multi-armed bandits, and distributed computing, with optional GPU acceleration. The framework leverages surrogate modeling and information-theoretic acquisition functions to guide informative sample selection, while bandit-based controllers dynamically allocate computational resources across candidate evaluations in a statistically principled manner. These decisions are executed asynchronously within a distributed multi-agent system, enabling high-throughput parallel evaluation. We establish theoretical regret bounds for both UCB-based and Thompson-sampling-based variants and develop a scalability analysis grounded in Amdahl's and Gustafson's laws. Empirical results across synthetic benchmarks, reinforcement learning tasks, and scientific simulation problems demonstrate that ALMAB-DC consistently outperforms state-of-the-art black-box optimizers. By design, ALMAB-DC is modular, uncertainty-aware, and extensible, making it particularly well suited for high-dimensional, resource-intensive optimization challenges.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [268] [StockBot 2.0: Vanilla LSTMs Outperform Transformer-based Forecasting for Stock Prices](https://arxiv.org/abs/2601.00197)
*Shaswat Mohanty*

Main category: cs.CE

TL;DR: The enhanced StockBot framework tests various time-series forecasting models for financial markets and finds a well-designed LSTM has superior accuracy and stability in predictions.


<details>
  <summary>Details</summary>
Motivation: Financial market forecasting is challenging due to complex dependencies, non-linear dynamics, and volatility. The paper aims to address these challenges using suitable forecasting models.

Method: The study systematically evaluates attention-based, convolutional, and recurrent models under a unified experimental framework to compare performance in predicting financial market time-series.

Result: Empirical results show a carefully built vanilla LSTM consistently outperforms other models in accuracy and stability, particularly when data availability and hyperparameter tuning are limited.

Conclusion: Recurrent models like LSTM are robust and data-efficient for market prediction tasks, especially in constrained settings where architectural biases also play a critical role.

Abstract: Accurate forecasting of financial markets remains a long-standing challenge due to complex temporal and often latent dependencies, non-linear dynamics, and high volatility. Building on our earlier recurrent neural network framework, we present an enhanced StockBot architecture that systematically evaluates modern attention-based, convolutional, and recurrent time-series forecasting models within a unified experimental setting. While attention-based and transformer-inspired models offer increased modeling flexibility, extensive empirical evaluation reveals that a carefully constructed vanilla LSTM consistently achieves superior predictive accuracy and more stable buy/sell decision-making when trained under a common set of default hyperparameters. These results highlight the robustness and data efficiency of recurrent sequence models for financial time-series forecasting, particularly in the absence of extensive hyperparameter tuning or the availability of sufficient data when discretized to single-day intervals. Additionally, these results underscore the importance of architectural inductive bias in data-limited market prediction tasks.

</details>


### [269] [LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization](https://arxiv.org/abs/2601.00770)
*Simon Paquette-Greenbaum,Jiangbo Yu*

Main category: cs.CE

TL;DR: The paper presents an agent-based framework for portfolio optimization, tackling challenges of a mixed-integer quadratic programming problem where heuristic algorithms are commonly employed.


<details>
  <summary>Details</summary>
Motivation: The motivation is to simplify labor-intensive workflows and algorithm development efforts involved in portfolio optimization problems, specifically Cardinality Constrained Mean-Variance Portfolio Optimization.

Method: The study employs a novel agentic framework to address the CCPO problem, benchmarking against state-of-the-art algorithms and implementing several architectures.

Result: The agentic framework performs competitively with existing solutions, reduces complexity in workflows, and achieves acceptable error levels in worst-case scenarios.

Conclusion: The agentic framework is a promising avenue for improving the efficiency and automation of portfolio optimization while maintaining accuracy and reducing human effort.

Abstract: Investment portfolio optimization is a task conducted in all major financial institutions. The Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO) problem formulation is ubiquitous for portfolio optimization. The challenge of this type of portfolio optimization, a mixed-integer quadratic programming (MIQP) problem, arises from the intractability of solutions from exact solvers, where heuristic algorithms are used to find approximate portfolio solutions. CCPO entails many laborious and complex workflows and also requires extensive effort pertaining to heuristic algorithm development, where the combination of pooled heuristic solutions results in improved efficient frontiers. Hence, common approaches are to develop many heuristic algorithms. Agentic frameworks emerge as a promising candidate for many problems within combinatorial optimization, as they have been shown to be equally efficient with regard to automating large workflows and have been shown to be excellent in terms of algorithm development, sometimes surpassing human-level performance. This study implements a novel agentic framework for the CCPO and explores several concrete architectures. In benchmark problems, the implemented agentic framework matches state-of-the-art algorithms. Furthermore, complex workflows and algorithm development efforts are alleviated, while in the worst case, lower but acceptable error is reported.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [270] [MethConvTransformer: A Deep Learning Framework for Cross-Tissue Alzheimer's Disease Detection](https://arxiv.org/abs/2601.00143)
*Gang Qu,Guanghao Li,Zhongming Zhao*

Main category: q-bio.GN

TL;DR: This research introduces MethConvTransformer, a deep learning model, to identify cross-tissue methylation biomarkers for early Alzheimer’s disease detection with high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: To improve the reproducibility and utility of DNA methylation as a biomarker for Alzheimer’s disease, addressing the tissue- and study-dependent variability in methylation signatures.

Method: Developed MethConvTransformer, a transformer-based model integrating CpG-wise linear projection, convolutional and self-attention layers, subject-level covariates, and tissue embeddings for robust biomarker discovery.

Result: MethConvTransformer outperformed traditional machine learning methods across multiple datasets for AD detection, demonstrated biological interpretability, and revealed methylation patterns linked to critical disease pathways.

Conclusion: MethConvTransformer offers an effective, cross-tissue methylation-based tool for Alzheimer's disease diagnostics with strong generalization ability and interpretable findings to advance understanding of disease mechanisms.

Abstract: Alzheimer's disease (AD) is a multifactorial neurodegenerative disorder characterized by progressive cognitive decline and widespread epigenetic dysregulation in the brain. DNA methylation, as a stable yet dynamic epigenetic modification, holds promise as a noninvasive biomarker for early AD detection. However, methylation signatures vary substantially across tissues and studies, limiting reproducibility and translational utility. To address these challenges, we develop MethConvTransformer, a transformer-based deep learning framework that integrates DNA methylation profiles from both brain and peripheral tissues to enable biomarker discovery. The model couples a CpG-wise linear projection with convolutional and self-attention layers to capture local and long-range dependencies among CpG sites, while incorporating subject-level covariates and tissue embeddings to disentangle shared and region-specific methylation effects. In experiments across six GEO datasets and an independent ADNI validation cohort, our model consistently outperforms conventional machine-learning baselines, achieving superior discrimination and generalization. Moreover, interpretability analyses using linear projection, SHAP, and Grad-CAM++ reveal biologically meaningful methylation patterns aligned with AD-associated pathways, including immune receptor signaling, glycosylation, lipid metabolism, and endomembrane (ER/Golgi) organization. Together, these results indicate that MethConvTransformer delivers robust, cross-tissue epigenetic biomarkers for AD while providing multi-resolution interpretability, thereby advancing reproducible methylation-based diagnostics and offering testable hypotheses on disease mechanisms.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [271] [Deep learning estimation of the spectral density of functional time series on large domains](https://arxiv.org/abs/2601.00284)
*Neda Mohammadi,Soham Sarkar,Piotr Kokoszka*

Main category: stat.ME

TL;DR: The paper proposes a deep learning-based estimator for functional time series spectral density, addressing computational challenges with large 2D and 3D data grids.


<details>
  <summary>Details</summary>
Motivation: Current spectral density estimators face computational difficulties for very large grid data (e.g., in climate models or medical scans), especially for functions defined on high-dimensional domains.

Method: Introduced a spectral density estimator based on a multilayer perceptron neural network, leveraging spectral functional principal components and proving it to be a universal approximator.

Result: The proposed method bypasses autocovariance kernel computations and offers parallelization, significantly reducing computational demands. Validation is done via simulations and fMRI image applications.

Conclusion: The estimator effectively handles the limitations of existing methods, offering speed and scalability for large functional time series data.

Abstract: We derive an estimator of the spectral density of a functional time series that is the output of a multilayer perceptron neural network. The estimator is motivated by difficulties with the computation of existing spectral density estimators for time series of functions defined on very large grids that arise, for example, in climate compute models and medical scans. Existing estimators use autocovariance kernels represented as large $G \times G$ matrices, where $G$ is the number of grid points on which the functions are evaluated. In many recent applications, functions are defined on 2D and 3D domains, and $G$ can be of the order $G \sim 10^5$, making the evaluation of the autocovariance kernels computationally intensive or even impossible. We use the theory of spectral functional principal components to derive our deep learning estimator and prove that it is a universal approximator to the spectral density under general assumptions. Our estimator can be trained without computing the autocovariance kernels and it can be parallelized to provide the estimates much faster than existing approaches. We validate its performance by simulations and an application to fMRI images.

</details>


### [272] [Identification and Estimation under Multiple Versions of Treatment: Mixture-of-Experts Approach](https://arxiv.org/abs/2601.00287)
*Kohei Yoshikawa,Shuichi Kawano*

Main category: stat.ME

TL;DR: This paper addresses the challenge of handling multiple latent treatment versions in causal inference, proposing a method using the Mixture-of-Experts framework to estimate their specific effects.


<details>
  <summary>Details</summary>
Motivation: To tackle the existing gap wherein multiple versions of treatment, when ignored, can bias causal effect estimates, as no robust framework has been fully developed for unbiased identification and estimation of these latent version-specific effects.

Method: The paper introduces the Mixture-of-Experts framework into causal inference to explicitly estimate the causal effects of latent treatment versions, even when these versions are unobserved.

Result: Numerical experiments validate the proposed methodology, showing its effectiveness in estimating version-specific causal effects accurately.

Conclusion: The approach developed in this paper successfully addresses the problem of latent treatment versions in causal inference, providing a means to better understand complex treatment mechanisms and estimate unbiased causal effects.

Abstract: The Stable Unit Treatment Value Assumption (SUTVA) includes the condition that there are no multiple versions of treatment in causal inference. Though we could not control the implementation of treatment in observational studies, multiple versions may exist in the treatment. It has been pointed out that ignoring such multiple versions of treatment can lead to biased estimates of causal effects, but a causal inference framework that explicitly deals with the unbiased identification and estimation of version-specific causal effects has not been fully developed yet. Thus, obtaining a deeper understanding for mechanisms of the complex treatments is difficult. In this paper, we introduce the Mixture-of-Experts framework into causal inference and develop a methodology for estimating the causal effects of latent versions. This approach enables explicit estimation of version-specific causal effects even if the versions are not observed. Numerical experiments demonstrate the effectiveness of the proposed method.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [273] [Sparse Probabilistic Coalition Structure Generation: Bayesian Greedy Pursuit and $\ell_1$ Relaxations](https://arxiv.org/abs/2601.00329)
*Angshul Majumdar*

Main category: cs.GT

TL;DR: The study focuses on Coalition Structure Generation (CSG) where coalition values must be learned through episodic observations. It uses a probabilistic framework to estimate sparse value functions and compares two estimation methods, highlighting their performance under different conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge of determining coalition values without prior knowledge by leveraging sparse episodic observations to generate optimal coalitions efficiently and accurately.

Method: The authors propose a probabilistic framework that relies on sparse linear regression for learning coalition values from episodic data. Two estimation methods are studied: (1) Bayesian Greedy Coalition Pursuit (BGCP) which mimics orthogonal matching pursuit and (2) an approach using \(\ell_1\)-penalised regression to estimate coalition values.

Result: The analysis shows that BGCP recovers profitable coalitions under specific conditions (coherence and minimum signal assumptions) and achieves welfare-optimal results. The \(\ell_1\)-penalised estimator, under specific eigenvalue conditions, provides welfare guarantees through \(\ell_1\) and prediction error bounds. Sparse probabilistic CSG outperforms classical approaches in sparse conditions, while dense regimes favor traditional least-squares.

Conclusion: Sparse probabilistic CSG is effective in sparse domains for welfare optimization when estimating coalition values. However, in dense scenarios, classical least-squares methods offer competitive results.

Abstract: We study coalition structure generation (CSG) when coalition values are not given but must be learned from episodic observations. We model each episode as a sparse linear regression problem, where the realised payoff \(Y_t\) is a noisy linear combination of a small number of coalition contributions. This yields a probabilistic CSG framework in which the planner first estimates a sparse value function from \(T\) episodes, then runs a CSG solver on the inferred coalition set. We analyse two estimation schemes. The first, Bayesian Greedy Coalition Pursuit (BGCP), is a greedy procedure that mimics orthogonal matching pursuit. Under a coherence condition and a minimum signal assumption, BGCP recovers the true set of profitable coalitions with high probability once \(T \gtrsim K \log m\), and hence yields welfare-optimal structures. The second scheme uses an \(\ell_1\)-penalised estimator; under a restricted eigenvalue condition, we derive \(\ell_1\) and prediction error bounds and translate them into welfare gap guarantees. We compare both methods to probabilistic baselines and identify regimes where sparse probabilistic CSG is superior, as well as dense regimes where classical least-squares approaches are competitive.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [274] [CoCo-Fed: A Unified Framework for Memory- and Communication-Efficient Federated Learning at the Wireless Edge](https://arxiv.org/abs/2601.00549)
*Zhiheng Guo,Zhaoyang Liu,Zihan Cen,Chenyuan Feng,Xinghua Sun,Xiang Chen,Tony Q. S. Quek,Xijun Wang*

Main category: cs.IT

TL;DR: CoCo-Fed addresses edge intelligence challenges in Open RAN by improving memory efficiency and reducing communication overhead using advanced federated learning techniques.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle issues of memory constraints and communication limitations in deploying large-scale neural networks within O-RAN.

Method: CoCo-Fed employs a double-dimension gradient compression locally and a layer-wise projection-based update transmission globally, coupled with established convergence guarantees.

Result: The framework outperforms existing techniques in memory and communication efficiency, validated through simulations on a wireless sensing task.

Conclusion: CoCo-Fed is effective for enabling edge intelligence in O-RAN architectures, with proven scalability and applicability under challenging settings.

Abstract: The deployment of large-scale neural networks within the Open Radio Access Network (O-RAN) architecture is pivotal for enabling native edge intelligence. However, this paradigm faces two critical bottlenecks: the prohibitive memory footprint required for local training on resource-constrained gNBs, and the saturation of bandwidth-limited backhaul links during the global aggregation of high-dimensional model updates. To address these challenges, we propose CoCo-Fed, a novel Compression and Combination-based Federated learning framework that unifies local memory efficiency and global communication reduction. Locally, CoCo-Fed breaks the memory wall by performing a double-dimension down-projection of gradients, adapting the optimizer to operate on low-rank structures without introducing additional inference parameters/latency. Globally, we introduce a transmission protocol based on orthogonal subspace superposition, where layer-wise updates are projected and superimposed into a single consolidated matrix per gNB, drastically reducing the backhaul traffic. Beyond empirical designs, we establish a rigorous theoretical foundation, proving the convergence of CoCo-Fed even under unsupervised learning conditions suitable for wireless sensing tasks. Extensive simulations on an angle-of-arrival estimation task demonstrate that CoCo-Fed significantly outperforms state-of-the-art baselines in both memory and communication efficiency while maintaining robust convergence under non-IID settings.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [275] [Neural Minimum Weight Perfect Matching for Quantum Error Codes](https://arxiv.org/abs/2601.00242)
*Yotam Peled,David Zenati,Eliya Nachmani*

Main category: quant-ph

TL;DR: A new neural network-based decoder (NMWPM) is proposed, combining Graph Neural Networks and Transformers to enhance Quantum Error Correction by predicting edge weights for Minimum Weight Perfect Matching.


<details>
  <summary>Details</summary>
Motivation: To improve the effectiveness of Quantum Error Correction, specifically in reducing logical error rates, by introducing a more efficient decoding algorithm.

Method: The paper introduces Neural Minimum Weight Perfect Matching (NMWPM) that hybridizes Graph Neural Networks for local features and Transformers for long-range dependencies. A novel proxy loss function is also defined for end-to-end optimization of this architecture.

Result: The proposed decoder achieves notable reductions in Logical Error Rate compared to standard baselines, showcasing its superior efficacy.

Conclusion: Hybrid decoders, such as NMWPM, leveraging neural networks and classical algorithms can significantly enhance the performance of Quantum Error Correction systems.

Abstract: Realizing the full potential of quantum computation requires Quantum Error Correction (QEC). QEC reduces error rates by encoding logical information across redundant physical qubits, enabling errors to be detected and corrected. A common decoder used for this task is Minimum Weight Perfect Matching (MWPM) a graph-based algorithm that relies on edge weights to identify the most likely error chains. In this work, we propose a data-driven decoder named Neural Minimum Weight Perfect Matching (NMWPM). Our decoder utilizes a hybrid architecture that integrates Graph Neural Networks (GNNs) to extract local syndrome features and Transformers to capture long-range global dependencies, which are then used to predict dynamic edge weights for the MWPM decoder. To facilitate training through the non-differentiable MWPM algorithm, we formulate a novel proxy loss function that enables end-to-end optimization. Our findings demonstrate significant performance reduction in the Logical Error Rate (LER) over standard baselines, highlighting the advantage of hybrid decoders that combine the predictive capabilities of neural networks with the algorithmic structure of classical matching.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [276] [Latent Flow Matching for Expressive Singing Voice Synthesis](https://arxiv.org/abs/2601.00217)
*Minhyeok Yun,Yong-Hoon Choi*

Main category: cs.SD

TL;DR: This paper proposes FM-Singer, improving expressiveness for cVAE-based singing synthesis using conditional flow matching and an optimal-transport-inspired refinement method.


<details>
  <summary>Details</summary>
Motivation: The existing cVAE-based singing synthesis system suffers from prior-posterior latent space mismatches, leading to degraded expressiveness like vibrato and micro-prosody.

Method: FM-Singer employs conditional flow matching (CFM) to optimize latent space by introducing a vector field that aligns prior latents with posterior latents using an ODE-based refinement process.

Result: Experiments on Korean and Chinese datasets show reduced mel-cepstral distortion, fundamental frequency error, and higher perceptual quality compared to strong baselines.

Conclusion: FM-Singer enhances the fine-grained expressiveness of synthesized singing voices while preserving computational efficiency using a novel latent-space refinement framework.

Abstract: Conditional variational autoencoder (cVAE)-based singing voice synthesis provides efficient inference and strong audio quality by learning a score-conditioned prior and a recording-conditioned posterior latent space. However, because synthesis relies on prior samples while training uses posterior latents inferred from real recordings, imperfect distribution matching can cause a prior-posterior mismatch that degrades fine-grained expressiveness such as vibrato and micro-prosody. We propose FM-Singer, which introduces conditional flow matching (CFM) in latent space to learn a continuous vector field transporting prior latents toward posterior latents along an optimal-transport-inspired path. At inference time, the learned latent flow refines a prior sample by solving an ordinary differential equation (ODE) before waveform generation, improving expressiveness while preserving the efficiency of parallel decoding. Experiments on Korean and Chinese singing datasets demonstrate consistent improvements over strong baselines, including lower mel-cepstral distortion and fundamental-frequency error and higher perceptual scores on the Korean dataset. Code, pretrained checkpoints, and audio demos are available at https://github.com/alsgur9368/FM-Singer

</details>


### [277] [Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection](https://arxiv.org/abs/2601.00777)
*Akanksha Chuchra,Shukesh Reddy,Sudeepta Mishra,Abhijit Das,Abhinav Dhall*

Main category: cs.SD

TL;DR: The paper investigates the use of Multimodal Large Language Models (MLLMs) for audio deepfake detection, focusing on text-aware multi-prompt approaches and task-specific training.


<details>
  <summary>Details</summary>
Motivation: Existing Vision-Language Models (VLMs) and MLLMs excel at detecting visual deepfakes, but their application to audio deepfakes remains underexplored. This paper aims to address this gap by leveraging MLLMs for cross-modal audio deepfake detection.

Method: A multi-prompt strategy combining audio data with text-aware, context-rich question-answer prompts was employed. Two MLLMs, Qwen2-Audio-7B-Instruct and SALMONN, were evaluated in both zero-shot and fine-tuned modes.

Result: The models performed poorly without task-specific training and struggled with out-of-domain data. However, good performance was achieved on in-domain data with minimal supervision.

Conclusion: MLLMs have potential for audio deepfake detection, but require task-specific training and demonstrate better performance in controlled in-domain settings.

Abstract: While Vision-Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have shown strong generalisation in detecting image and video deepfakes, their use for audio deepfake detection remains largely unexplored. In this work, we aim to explore the potential of MLLMs for audio deepfake detection. Combining audio inputs with a range of text prompts as queries to find out the viability of MLLMs to learn robust representations across modalities for audio deepfake detection. Therefore, we attempt to explore text-aware and context-rich, question-answer based prompts with binary decisions. We hypothesise that such a feature-guided reasoning will help in facilitating deeper multimodal understanding and enable robust feature learning for audio deepfake detection. We evaluate the performance of two MLLMs, Qwen2-Audio-7B-Instruct and SALMONN, in two evaluation modes: (a) zero-shot and (b) fine-tuned. Our experiments demonstrate that combining audio with a multi-prompt approach could be a viable way forward for audio deepfake detection. Our experiments show that the models perform poorly without task-specific training and struggle to generalise to out-of-domain data. However, they achieve good performance on in-domain data with minimal supervision, indicating promising potential for audio deepfake detection.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [278] [Impact of Clustering on the Observability and Controllability of Complex Networks](https://arxiv.org/abs/2601.00221)
*Mohammadreza Doostmohammadian,Hamid R. Rabiee*

Main category: eess.SY

TL;DR: This paper explores the impact of clustering on observability and controllability in complex Scale-Free networks, demonstrating that densely clustered networks require fewer driver and observer nodes due to improved information flow.


<details>
  <summary>Details</summary>
Motivation: The growing complexity of real-world systems modeled as SF networks necessitates deeper understanding of how clustering affects observability and controllability, aiming to optimize network design in key applications.

Method: A combination of structured systems theory, quantification of observability/controllability requirements, Monte-Carlo simulations, and case studies was used to analyze the influence of clustering on SF networks.

Result: The research reveals that clustering improves information propagation, allowing for fewer driver and observer nodes, which is beneficial for resource optimization in real-world networks.

Conclusion: This study expands knowledge of SF network controllability and observability, offering practical methods to enhance these properties through strategic network clustering adjustments.

Abstract: The increasing complexity and interconnectedness of systems across various fields have led to a growing interest in studying complex networks, particularly Scale-Free (SF) networks, which best model real-world systems. This paper investigates the influence of clustering on the observability and controllability of complex SF networks, framing these characteristics in the context of structured systems theory. In this paper, we show that densely clustered networks require fewer driver and observer nodes due to better information propagation within clusters. This relationship is of interest for optimizing network design in applications such as social networks and intelligent transportation systems. We first quantify the network observability/controllability requirements, and then, through Monte-Carlo simulations and different case studies, we show how clustering affects these metrics. Our findings offer practical insights into reducing control and observer nodes for sensor/actuator placement, particularly in resource-constrained setups. This work contributes to the understanding of network observability/controllability and presents techniques for improving these features through alterations in network structure and clustering.

</details>


### [279] [Optimal Transport-Based Decentralized Multi-Agent Distribution Matching](https://arxiv.org/abs/2601.00548)
*Kooktae Lee*

Main category: eess.SY

TL;DR: This paper proposes a decentralized framework for multi-agent systems to achieve a collective spatial distribution using optimal transport theory, with scalable and effective implementation demonstrated in simulations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable multi-agent systems to collectively achieve a prescribed spatial distribution in a decentralized manner, addressing scalability and communication constraints.

Method: The method involves formulating the distribution matching problem using optimal transport theory, introducing a per-agent decision process, applying a sequential weight-update rule, and integrating a memory-based correction mechanism to manage limited communication.

Result: The framework successfully achieves distribution matching with convergence guarantees under different agent dynamics and demonstrates scalable and effective performance in simulations.

Conclusion: The proposed decentralized framework effectively matches distributions in multi-agent systems while ensuring scalability, reliable communication, and convergence guarantees.

Abstract: This paper presents a decentralized control framework for distribution matching in multi-agent systems (MAS), where agents collectively achieve a prescribed terminal spatial distribution. The problem is formulated using optimal transport (Wasserstein distance), which provides a principled measure of distributional discrepancy and serves as the basis for the control design. To avoid solving the global optimal transport problem directly, the distribution-matching objective is reformulated into a tractable per-agent decision process, enabling each agent to identify its desired terminal locations using only locally available information. A sequential weight-update rule is introduced to construct feasible local transport plans, and a memory-based correction mechanism is incorporated to maintain reliable operation under intermittent and range-limited communication. Convergence guarantees are established, showing cycle-wise improvement of a surrogate transport cost under both linear and nonlinear agent dynamics. Simulation results demonstrate that the proposed framework achieves effective and scalable distribution matching while operating fully in a decentralized manner.

</details>


### [280] [Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective](https://arxiv.org/abs/2601.00257)
*Aly Sabri Abdalla,Vuk Marojevic*

Main category: eess.SY

TL;DR: This paper proposes an O-RAN-enabled framework to enhance the orchestration of UAV-based low-altitude economy (LAE) missions using AI-optimized operations and semantic-aware technology.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental challenges in coordinating UAV-based LAE missions over complex environments without real-time, resilient, and AI-specialized solutions.

Method: The proposed framework integrates open RAN architecture, open interfaces, and RAN Intelligent Controllers (RICs), supported by semantic-aware rApp and reinforcement learning-enabled xApp for trajectory planning.

Result: The evaluation demonstrated the feasibility and improved performance of real-time trajectory planning for LAE swarm nodes based on a semantic-aware terrain interpreter.

Conclusion: The study highlights the promise of O-RAN architecture for LAE applications and emphasizes the need for continued research, UAV testbed deployment to address challenges, and standardization efforts in the field.

Abstract: Despite the growing interest in low-altitude economy (LAE) applications, including UAV-based logistics and emergency response, fundamental challenges remain in orchestrating such missions over complex, signal-constrained environments. These include the absence of real-time, resilient, and context-aware orchestration of aerial nodes with limited integration of artificial intelligence (AI) specialized for LAE missions. This paper introduces an open radio access network (O-RAN)-enabled LAE framework that leverages seamless coordination between the disaggregated RAN architecture, open interfaces, and RAN intelligent controllers (RICs) to facilitate closed-loop, AI-optimized, and mission-critical LAE operations. We evaluate the feasibility and performance of the proposed architecture via a semantic-aware rApp that acts as a terrain interpreter, offering semantic guidance to a reinforcement learning-enabled xApp, which performs real-time trajectory planning for LAE swarm nodes. We survey the capabilities of UAV testbeds that can be leveraged for LAE research, and present critical research challenges and standardization needs.

</details>


### [281] [Probability-Aware Parking Selection](https://arxiv.org/abs/2601.00521)
*Cameron Hickert,Sirui Li,Zhengbing He,Cathy Wu*

Main category: eess.SY

TL;DR: The paper introduces a dynamic programming framework for parking selection using probabilistic parking availability to reduce search time and improve efficiency.


<details>
  <summary>Details</summary>
Motivation: Current systems focus on traveling directly to destinations and fail to incorporate parking search time, impacting user experience and urban congestion.

Method: An adaptable probabilistic dynamic programming framework is proposed, supported by closed-form analyses and sensitivity studies to model parking behavior.

Result: This method demonstrated up to 66% time savings in parking navigation compared to traditional strategies, and experiments in Seattle showed enhanced accuracy with frequent observations.

Conclusion: Incorporating probabilistic models of parking availability results in informed navigation, reducing travel costs and promising practical benefits despite requiring careful observation management.

Abstract: Current parking navigation systems often underestimate total travel time by failing to account for the time spent searching for a parking space, which significantly affects user experience, mode choice, congestion, and emissions. To address this issue, this paper introduces the probability-aware parking selection problem, which aims to direct drivers to the best parking location rather than straight to their destination. An adaptable dynamic programming framework is proposed for decision-making based on probabilistic information about parking availability at the parking lot level. Closed-form analysis determines when it is optimal to target a specific parking lot or explore alternatives, as well as the expected time cost. Sensitivity analysis and three illustrative cases are examined, demonstrating the model's ability to account for the dynamic nature of parking availability. Acknowledging the financial costs of permanent sensing infrastructure, the paper provides analytical and empirical assessments of errors incurred when leveraging stochastic observations to estimate parking availability. Experiments with real-world data from the US city of Seattle indicate this approach's viability, with mean absolute error decreasing from 7% to below 2% as observation frequency grows. In data-based simulations, probability-aware strategies demonstrate time savings up to 66% relative to probability-unaware baselines, yet still take up to 123% longer than direct-to-destination estimates.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [282] [Learning Speech Representations with Variational Predictive Coding](https://arxiv.org/abs/2601.00100)
*Sung-Lin Yeh,Peter Bell,Hao Tang*

Main category: eess.AS

TL;DR: The paper revisits the HuBERT objective for speech representation learning, improves it using predictive coding, and demonstrates enhanced performance across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of development in improving the HuBERT objective by identifying its underlying principle and advancing it for better learning of speech representations.

Method: The authors propose a predictive coding framework with a variational view to reinterpret the HuBERT objective and introduce two modifications to improve optimization and parameterization.

Result: The modified HuBERT objective shows significant performance improvements in downstream tasks such as phone classification, f0 tracking, speaker recognition, and automatic speech recognition.

Conclusion: The paper concludes that predictive coding is the underlying principle of the HuBERT objective and emphasizes its potential for future optimization and its utility in enhancing speech representation learning.

Abstract: Despite being the best known objective for learning speech representations, the HuBERT objective has not been further developed and improved. We argue that it is the lack of an underlying principle that stalls the development, and, in this paper, we show that predictive coding under a variational view is the principle behind the HuBERT objective. Due to its generality, our formulation provides opportunities to improve parameterization and optimization, and we show two simple modifications that bring immediate improvements to the HuBERT objective. In addition, the predictive coding formulation has tight connections to various other objectives, such as APC, CPC, wav2vec, and BEST-RQ. Empirically, the improvement in pre-training brings significant improvements to four downstream tasks: phone classification, f0 tracking, speaker recognition, and automatic speech recognition, highlighting the importance of the predictive coding interpretation.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [283] [MAESTRO: Multi-Agent Evaluation Suite for Testing, Reliability, and Observability](https://arxiv.org/abs/2601.00481)
*Tie Ma,Yixi Chen,Vaastav Anand,Alessandro Cornacchia,Amândio R. Faustino,Guanheng Liu,Shan Zhang,Hongbin Luo,Suhaib A. Fahmy,Zafar A. Qazi,Marco Canini*

Main category: cs.NI

TL;DR: MAESTRO is an evaluation suite for LLM-based multi-agent systems (MAS), providing standardized configuration, execution, and evaluation capabilities for improved observability, reliability, and testing.


<details>
  <summary>Details</summary>
Motivation: To address the need for systematic evaluation, standardization, and empirical guidance in testing the performance, reproducibility, and reliability of multi-agent systems (MAS) based on large language models (LLMs).

Method: MAESTRO standardizes MAS design through a unified interface, integrates native and third-party MAS using a repository of examples and adapters, and generates framework-agnostic execution traces with system-level signals (latency, cost, failures). Experiments and case studies are conducted with 12 MAS instances.

Result: Results reveal that MAS executions are structurally stable but temporally variable, introducing run-to-run variance in performance. MAS architecture significantly impacts resource use, reproducibility, and cost-latency-accuracy trade-offs, more than backend model or tool changes.

Conclusion: MAESTRO provides a systematic and unified approach to evaluate and optimize MAS, offering insights on the performance and trade-offs in agentic frameworks, ultimately enhancing MAS design and reliability.

Abstract: We present MAESTRO, an evaluation suite for the testing, reliability, and observability of LLM-based MAS. MAESTRO standardizes MAS configuration and execution through a unified interface, supports integrating both native and third-party MAS via a repository of examples and lightweight adapters, and exports framework-agnostic execution traces together with system-level signals (e.g., latency, cost, and failures). We instantiate MAESTRO with 12 representative MAS spanning popular agentic frameworks and interaction patterns, and conduct controlled experiments across repeated runs, backend models, and tool configurations. Our case studies show that MAS executions can be structurally stable yet temporally variable, leading to substantial run-to-run variance in performance and reliability. We further find that MAS architecture is the dominant driver of resource profiles, reproducibility, and cost-latency-accuracy trade-off, often outweighing changes in backend models or tool settings. Overall, MAESTRO enables systematic evaluation and provides empirical guidance for designing and optimizing agentic systems.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [284] [Group Cross-Correlations with Faintly Constrained Filters](https://arxiv.org/abs/2601.00045)
*Benedikt Fluhr*

Main category: math.DS

TL;DR: The paper revisits group cross-correlations and loosens constraints for broader applicability, addressing issues in previous models.


<details>
  <summary>Details</summary>
Motivation: To address limitations in previous literature on group cross-correlations for group actions with non-compact stabilizers, non-transitive actions, and unimodularity assumptions.

Method: Introduces a generalized notion of group cross-correlations by relaxing constraints on associated filters.

Result: Successfully applies the generalized framework to resolve incompatibilities and expand theoretical models.

Conclusion: The revised approach enhances compatibility and extends the theoretical understanding of group cross-correlations under various group action scenarios.

Abstract: We provide a notion of group cross-correlations, where the associated filter is not as tightly constrained as in the previous literature. This resolves an incompatibility previous constraints have for group actions with non-compact stabilizers. Moreover, we generalize previous results to group actions that are not necessarily transitive, and we weaken the common assumption of unimodularity.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [285] [Automated electrostatic characterization of quantum dot devices in single- and bilayer heterostructures](https://arxiv.org/abs/2601.00067)
*Merritt P. R. Losert,Dario Denora,Barnaby van Straaten,Michael Chan,Stefan D. Oosterhout,Lucas Stehouwer,Giordano Scappucci,Menno Veldhorst,Justyna P. Zwolak*

Main category: cond-mat.mes-hall

TL;DR: The paper proposes an automated method to extract capacitive properties of quantum dot devices using charge stability diagrams (CSDs).


<details>
  <summary>Details</summary>
Motivation: Quantum dot spin qubits require faster and scalable characterization methods as device complexity increases.

Method: An automated protocol utilizing machine learning, image processing, and object detection is applied to CSD data for charge transition analysis.

Result: Demonstrated on planar and bilayer quantum dot devices, the protocol identifies complex transitions and estimates capacitive properties statistically.

Conclusion: The approach provides rapid, accurate extraction of critical information in quantum dot device characterization, overcoming manual limitations.

Abstract: As quantum dot (QD)-based spin qubits advance toward larger, more complex device architectures, rapid, automated device characterization and data analysis tools become critical. The orientation and spacing of transition lines in a charge stability diagram (CSD) contain a fingerprint of a QD device's capacitive environment, making these measurements useful tools for device characterization. However, manually interpreting these features is time-consuming, error-prone, and impractical at scale. Here, we present an automated protocol for extracting underlying capacitive properties from CSDs. Our method integrates machine learning, image processing, and object detection to identify and track charge transitions across large datasets without manual labeling. We demonstrate this method using experimentally measured data from a strained-germanium single-quantum-well (planar) and a strained-germanium double-quantum-well (bilayer) QD device. Unlike for planar QD devices, CSDs in bilayer germanium heterostructure exhibit a larger set of transitions, including interlayer tunneling and distinct loading lines for the vertically stacked QDs, making them a powerful testbed for automation methods. By analyzing the properties of many CSDs, we can statistically estimate physically relevant quantities, like relative lever arms and capacitive couplings. Thus, our protocol enables rapid extraction of useful, nontrivial information about QD devices.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [286] [Deterministic Coreset for Lp Subspace](https://arxiv.org/abs/2601.00361)
*Rachit Chhaya,Anirban Dasgupta,Dan Feldman,Supratim Shit*

Main category: cs.DS

TL;DR: This paper introduces the first iterative algorithm to construct a deterministic $l_p $ subspace embedding coreset for any $p $ in [1,∞). It guarantees bounded losses and removes long-standing $log$ factors, showing its efficiency for $l_p$ regression.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing coreset constructions for $l_p$ subspace embeddings—such as randomness, suboptimality, and log-factor inefficiency—while providing deterministic guarantees.

Method: An iterative algorithm that constructs a deterministic $ϵ$-coreset ensuring $l_p$ subspace embedding with bounded loss guarantees in each iteration. The size of the coreset is $O\left(\frac{d^{\max\{1,p/2\}}}{ϵ^{2}}\right)$ with reduced log factors.

Result: The proposed coreset construction achieves optimality by being tight with lower bounds and resolves the log-factor issue in subspace embedding sizes. This deterministic coreset can also be applied to approximately solve $l_p$ regression.

Conclusion: The introduced method is a breakthrough in coreset construction for $l_p$ subspace embeddings, offering deterministic, efficient, and optimal guarantees that solve a long-standing open problem while enabling applications like $l_p$ regression.

Abstract: We introduce the first iterative algorithm for constructing a $\varepsilon$-coreset that guarantees deterministic $\ell_p$ subspace embedding for any $p \in [1,\infty)$ and any $\varepsilon > 0$. For a given full rank matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$ where $n \gg d$, $\mathbf{X}' \in \mathbb{R}^{m \times d}$ is an $(\varepsilon,\ell_p)$-subspace embedding of $\mathbf{X}$, if for every $\mathbf{q} \in \mathbb{R}^d$, $(1-\varepsilon)\|\mathbf{Xq}\|_{p}^{p} \leq \|\mathbf{X'q}\|_{p}^{p} \leq (1+\varepsilon)\|\mathbf{Xq}\|_{p}^{p}$. Specifically, in this paper, $\mathbf{X}'$ is a weighted subset of rows of $\mathbf{X}$ which is commonly known in the literature as a coreset. In every iteration, the algorithm ensures that the loss on the maintained set is upper and lower bounded by the loss on the original dataset with appropriate scalings. So, unlike typical coreset guarantees, due to bounded loss, our coreset gives a deterministic guarantee for the $\ell_p$ subspace embedding. For an error parameter $\varepsilon$, our algorithm takes $O(\mathrm{poly}(n,d,\varepsilon^{-1}))$ time and returns a deterministic $\varepsilon$-coreset, for $\ell_p$ subspace embedding whose size is $O\left(\frac{d^{\max\{1,p/2\}}}{\varepsilon^{2}}\right)$. Here, we remove the $\log$ factors in the coreset size, which had been a long-standing open problem. Our coresets are optimal as they are tight with the lower bound. As an application, our coreset can also be used for approximately solving the $\ell_p$ regression problem in a deterministic manner.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [287] [Hear the Heartbeat in Phases: Physiologically Grounded Phase-Aware ECG Biometrics](https://arxiv.org/abs/2601.00170)
*Jintao Huang,Lu Leng,Yi Zhang,Ziyuan Yang*

Main category: eess.IV

TL;DR: The paper introduces a Hierarchical Phase-Aware Fusion (HPAF) framework for identity authentication using ECG signals, achieving state-of-the-art results on public datasets.


<details>
  <summary>Details</summary>
Motivation: ECG-based identity authentication methods often overlook the phase-specific characteristics within the cardiac cycle, leading to suboptimal performance.

Method: The proposed Hierarchical Phase-Aware Fusion (HPAF) framework adopts a three-stage design: Intra-Phase Representation (IPR) extracts phase-specific features, Phase-Grouped Hierarchical Fusion (PGHF) combines related phases, and Global Representation Fusion (GRF) unifies these representations. Additionally, a Heartbeat-Aware Multi-prototype (HAM) strategy reduces noise variability.

Result: Extensive experiments across three public datasets demonstrate superior performance of the HPAF framework under both closed and open-set authentication settings.

Conclusion: Considering phase-specific characteristics and heartbeat-aware enrollment improves the reliability of ECG-based identity authentication systems.

Abstract: Electrocardiography (ECG) is adopted for identity authentication in wearable devices due to its individual-specific characteristics and inherent liveness. However, existing methods often treat heartbeats as homogeneous signals, overlooking the phase-specific characteristics within the cardiac cycle. To address this, we propose a Hierarchical Phase-Aware Fusion~(HPAF) framework that explicitly avoids cross-feature entanglement through a three-stage design. In the first stage, Intra-Phase Representation (IPR) independently extracts representations for each cardiac phase, ensuring that phase-specific morphological and variation cues are preserved without interference from other phases. In the second stage, Phase-Grouped Hierarchical Fusion (PGHF) aggregates physiologically related phases in a structured manner, enabling reliable integration of complementary phase information. In the final stage, Global Representation Fusion (GRF) further combines the grouped representations and adaptively balances their contributions to produce a unified and discriminative identity representation. Moreover, considering ECG signals are continuously acquired, multiple heartbeats can be collected for each individual. We propose a Heartbeat-Aware Multi-prototype (HAM) enrollment strategy, which constructs a multi-prototype gallery template set to reduce the impact of heartbeat-specific noise and variability. Extensive experiments on three public datasets demonstrate that HPAF achieves state-of-the-art results in the comparison with other methods under both closed and open-set settings.

</details>


### [288] [Deep Learning Approach for the Diagnosis of Pediatric Pneumonia Using Chest X-ray Imaging](https://arxiv.org/abs/2601.00041)
*Fatemeh Hosseinabadi,Mohammad Mojtaba Rohani*

Main category: eess.IV

TL;DR: This paper explores the use of CNN architectures (ResNetRS, RegNet, EfficientNetV2) for diagnosing pediatric pneumonia in chest X-rays using transfer learning, achieving promising results.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in timely and accurate diagnosis of pediatric pneumonia due to limited radiological expertise and complexities in imaging.

Method: The study used transfer learning with pretrained ImageNet weights to fine-tune CNN models, employing a curated dataset of 1,000 pediatric chest X-ray images for binary classification (pneumonia vs. normal).

Result: RegNet showed the highest performance with 92.4% accuracy and 90.1% sensitivity, followed by ResNetRS and EfficientNetV2, which also demonstrated high accuracy and sensitivity.

Conclusion: CNN architectures, particularly RegNet, show high potential for automated classification of pediatric chest X-rays for pneumonia, overcoming diagnostic challenges in radiology.

Abstract: Pediatric pneumonia remains a leading cause of morbidity and mortality in children worldwide. Timely and accurate diagnosis is critical but often challenged by limited radiological expertise and the physiological and procedural complexity of pediatric imaging. This study investigates the performance of state-of-the-art convolutional neural network (CNN) architectures ResNetRS, RegNet, and EfficientNetV2 using transfer learning for the automated classification of pediatric chest Xray images as either pneumonia or normal.A curated subset of 1,000 chest X-ray images was extracted from a publicly available dataset originally comprising 5,856 pediatric images. All images were preprocessed and labeled for binary classification. Each model was fine-tuned using pretrained ImageNet weights and evaluated based on accuracy and sensitivity. RegNet achieved the highest classification performance with an accuracy of 92.4 and a sensitivity of 90.1, followed by ResNetRS (accuracy: 91.9, sensitivity: 89.3) and EfficientNetV2 (accuracy: 88.5, sensitivity: 88.1).

</details>


### [289] [The Impact of Lesion Focus on the Performance of AI-Based Melanoma Classification](https://arxiv.org/abs/2601.00355)
*Tanay Donde*

Main category: eess.IV

TL;DR: This study investigates how attention on lesion areas by machine learning models impacts melanoma detection accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the diagnostic reliability and accuracy of melanoma classification by addressing inconsistent attention to lesion areas in CNNs.

Method: The authors analyzed lesion attention and diagnostic performance using masked images, bounding box detection, transfer learning, and explainability/sensitivity analysis.

Result: Models with higher attention on lesion areas showed better diagnostic performance, enhancing metrics like precision, recall, and F1-score.

Conclusion: Interpretable AI focusing on lesion areas can improve melanoma diagnostics, paving the way for more reliable classification models.

Abstract: Melanoma is the most lethal subtype of skin cancer, and early and accurate detection of this disease can greatly improve patients' outcomes. Although machine learning models, especially convolutional neural networks (CNNs), have shown great potential in automating melanoma classification, their diagnostic reliability still suffers due to inconsistent focus on lesion areas. In this study, we analyze the relationship between lesion attention and diagnostic performance, involving masked images, bounding box detection, and transfer learning. We used multiple explainability and sensitivity analysis approaches to investigate how well models aligned their attention with lesion areas and how this alignment correlated with precision, recall, and F1-score. Results showed that models with a higher focus on lesion areas achieved better diagnostic performance, suggesting the potential of interpretable AI in medical diagnostics. This study provides a foundation for developing more accurate and trustworthy melanoma classification models in the future.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [290] [Toward Large-Scale Photonics-Empowered AI Systems: From Physical Design Automation to System-Algorithm Co-Exploration](https://arxiv.org/abs/2601.00129)
*Ziang Yin,Hongjian Zhou,Nicholas Gangi,Meng Zhang,Jeff Zhang,Zhaoran Rena Huang,Jiaqi Gu*

Main category: physics.optics

TL;DR: The paper focuses on identifying essential considerations for scalable photonic AI systems, developing tools for quantitative analysis and practical implementation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in scaling photonic AI systems, including dynamic tensor operations, overhead management, and hardware robustness.

Method: The proposed approach includes developing a cross-layer toolchain (SimPhony, ADEPT, ADEPT-Z, Apollo, and LiDAR) to enable thorough exploration and practical realization of photonic AI designs.

Result: The tools allow for implementation-aware modeling, architectural decision-making grounded in realistic constraints, and scalable design automation for photonic circuits.

Conclusion: This work provides a systematic framework and tools to overcome challenges in large-scale photonic AI systems, ensuring practicality and performance.

Abstract: In this work, we identify three considerations that are essential for realizing practical photonic AI systems at scale: (1) dynamic tensor operation support for modern models rather than only weight-static kernels, especially for attention/Transformer-style workloads; (2) systematic management of conversion, control, and data-movement overheads, where multiplexing and dataflow must amortize electronic costs instead of letting ADC/DAC and I/O dominate; and (3) robustness under hardware non-idealities that become more severe as integration density grows. To study these coupled tradeoffs quantitatively, and to ensure they remain meaningful under real implementation constraints, we build a cross-layer toolchain that supports photonic AI design from early exploration to physical realization. SimPhony provides implementation-aware modeling and rapid cross-layer evaluation, translating physical costs into system-level metrics so architectural decisions are grounded in realistic assumptions. ADEPT and ADEPT-Z enable end-to-end circuit and topology exploration, connecting system objectives to feasible photonic fabrics under practical device and circuit constraints. Finally, Apollo and LiDAR provide scalable photonic physical design automation, turning candidate circuits into manufacturable layouts while accounting for routing, thermal, and crosstalk constraints.

</details>


### [291] [Democratizing Electronic-Photonic AI Systems: An Open-Source AI-Infused Cross-Layer Co-Design and Design Automation Toolflow](https://arxiv.org/abs/2601.00130)
*Hongjian Zhou,Ziang Yin,Jiaqi Gu*

Main category: physics.optics

TL;DR: The paper addresses challenges in electronic-photonic AI system design by presenting a comprehensive framework, including architectural designs, an open-source modeling tool, and AI-enabled photonic design automation.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in the development of electronic-photonic AI systems due to the absence of mature design automation tools, slow design cycles, and a steep learning curve across multiple technical domains.

Method: The authors propose a cross-layer co-design and automation framework, introducing architectures for photonic AI systems, the SimPhony open-source modeling tool for rapid evaluation, and AI-enabled tools for photonic design such as Maxwell solvers and inverse design algorithms.

Result: The framework enables democratization of photonic AI design through scalable tools and methods for evaluating and designing electronic-photonic AI systems.

Conclusion: The presented approach provides a scalable and efficient EPDA stack, advancing the development of next-generation electronic-photonic AI systems while fostering innovation and cross-disciplinary collaboration.

Abstract: Photonics is becoming a cornerstone technology for high-performance AI systems and scientific computing, offering unparalleled speed, parallelism, and energy efficiency. Despite this promise, the design and deployment of electronic-photonic AI systems remain highly challenging due to a steep learning curve across multiple layers, spanning device physics, circuit design, system architecture, and AI algorithms. The absence of a mature electronic-photonic design automation (EPDA) toolchain leads to long, inefficient design cycles and limits cross-disciplinary innovation and co-evolution. In this work, we present a cross-layer co-design and automation framework aimed at democratizing photonic AI system development. We begin by introducing our architecture designs for scalable photonic edge AI and Transformer inference, followed by SimPhony, an open-source modeling tool for rapid EPIC AI system evaluation and design-space exploration. We then highlight advances in AI-enabled photonic design automation, including physical AI-based Maxwell solvers, a fabrication-aware inverse design framework, and a scalable inverse training algorithm for meta-optical neural networks, enabling a scalable EPDA stack for next-generation electronic-photonic AI systems.

</details>
