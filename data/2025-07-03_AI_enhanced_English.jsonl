{"id": "2507.01145", "pdf": "https://arxiv.org/pdf/2507.01145", "abs": "https://arxiv.org/abs/2507.01145", "authors": ["Xuesi Chen", "Leo Han", "Anvita Bhagavathula", "Udit Gupta"], "title": "CarbonClarity: Understanding and Addressing Uncertainty in Embodied Carbon for Sustainable Computing", "categories": ["cs.AR"], "comment": null, "summary": "Embodied carbon footprint modeling has become an area of growing interest due\nto its significant contribution to carbon emissions in computing. However, the\ndeterministic nature of the existing models fail to account for the spatial and\ntemporal variability in the semiconductor supply chain. The absence of\nuncertainty modeling limits system designers' ability to make informed,\ncarbon-aware decisions. We introduce CarbonClarity, a probabilistic framework\ndesigned to model embodied carbon footprints through distributions that reflect\nuncertainties in energy-per-area, gas-per-area, yield, and carbon intensity\nacross different technology nodes. Our framework enables a deeper understanding\nof how design choices, such as chiplet architectures and new vs. old technology\nnode selection, impact emissions and their associated uncertainties. For\nexample, we show that the gap between the mean and 95th percentile of embodied\ncarbon per cm$^2$ can reach up to 1.6X for the 7nm technology node.\nAdditionally, we demonstrate through case studies that: (i) CarbonClarity is a\nvaluable resource for device provisioning, help maintaining performance under a\ntight carbon budget; and (ii) chiplet technology and mature nodes not only\nreduce embodied carbon but also significantly lower its associated uncertainty,\nachieving an 18% reduction in the 95th percentile compared to monolithic\ndesigns for the mobile application.", "AI": {"tldr": "This paper introduces CarbonClarity, a probabilistic framework for modeling uncertainties in embodied carbon footprint in semiconductor design, highlighting the potential for improved carbon-aware decision-making.", "motivation": "The study is motivated by the lack of models that incorporate spatial and temporal variability in the semiconductor supply chain, limiting system designers' ability to address carbon emissions effectively.", "method": "The authors developed CarbonClarity, a probabilistic framework that models embodied carbon footprints by accounting for uncertainties in factors like energy-per-area, yield, and carbon intensity across various technology nodes.", "result": "The framework reveals variability in embodied carbon emissions, with up to 1.6X gap between mean and 95th percentile values for the 7nm node. It also shows that chiplet technology and mature nodes significantly reduce both emissions and uncertainty.", "conclusion": "CarbonClarity helps system designers make carbon-aware decisions, emphasizing the value of chiplet architecture and mature nodes to lower emissions and uncertainties for better carbon budgeting."}}
{"id": "2507.01113", "pdf": "https://arxiv.org/pdf/2507.01113", "abs": "https://arxiv.org/abs/2507.01113", "authors": ["Vairavan Palaniappan", "Adam H. Ross", "Amit Ranjan Trivedi", "Debjit Pal"], "title": "HERCULES: Hardware accElerator foR stoChastic schedULing in hEterogeneous Systems", "categories": ["cs.DC", "cs.SY", "eess.SY"], "comment": "10 pages, 10 figures, accepted for publication in in Int'l Conference\n  on Computer Aided Design (ICCAD) 2025", "summary": "Efficient workload scheduling is a critical challenge in modern heterogeneous\ncomputing environments, particularly in high-performance computing (HPC)\nsystems. Traditional software-based schedulers struggle to efficiently balance\nworkload distribution due to high scheduling overhead, lack of adaptability to\ndynamic workloads, and suboptimal resource utilization. These pitfalls are\ncompounded in heterogeneous systems, where differing computational elements can\nhave vastly different performance profiles. To resolve these hindrances, we\npresent a novel FPGA-based accelerator for stochastic online scheduling (SOS).\nWe modify a greedy cost selection assignment policy by adapting existing cost\nequations to engage with discretized time before implementing them into a\nhardware accelerator design. Our design leverages hardware parallelism,\nprecalculation, and precision quantization to reduce job scheduling latency. By\nintroducing a hardware-accelerated approach to real-time scheduling, this paper\nestablishes a new paradigm for adaptive scheduling mechanisms in heterogeneous\ncomputing systems. The proposed design achieves high throughput, low latency,\nand energy-efficient operation, offering a scalable alternative to traditional\nsoftware scheduling methods. Experimental results demonstrate consistent\nworkload distribution, fair machine utilization, and up to 1060x speedup over\nsingle-threaded software scheduling policy implementations. This makes the SOS\naccelerator a strong candidate for deployment in high-performance computing\nsystem, deeplearning pipelines, and other performance-critical applications.", "AI": {"tldr": "The paper proposes an FPGA-based accelerator for stochastic online scheduling (SOS) to enhance workload scheduling in heterogeneous computing environments, achieving up to 1060x speedup over traditional software methods.", "motivation": "Traditional software-based schedulers face challenges in high scheduling overhead, lack of adaptability, and poor resource utilization, especially in heterogeneous computing systems with diverse performance profiles.", "method": "The authors presented an SOS accelerator by modifying a greedy cost-selection policy to include discretized time, implemented in hardware to leverage parallelism, precalculation, and precision quantization to optimize scheduling.", "result": "The proposed hardware accelerator achieves high throughput, low latency, and energy efficiency, demonstrating consistent workload distribution and fair machine utilization with significant speedups (up to 1060x).", "conclusion": "The SOS accelerator is a scalable and efficient alternative to traditional software-based schedulers, suitable for HPC systems, deep learning pipelines, and performance-critical applications."}}
{"id": "2507.01309", "pdf": "https://arxiv.org/pdf/2507.01309", "abs": "https://arxiv.org/abs/2507.01309", "authors": ["Zhican Wang", "Guanghui He", "Hongxiang Fan"], "title": "SD-Acc: Accelerating Stable Diffusion through Phase-aware Sampling and Hardware Co-Optimizations", "categories": ["cs.AR"], "comment": "Under Review", "summary": "The emergence of diffusion models has significantly advanced generative AI,\nimproving the quality, realism, and creativity of image and video generation.\nAmong them, Stable Diffusion (StableDiff) stands out as a key model for\ntext-to-image generation and a foundation for next-generation multi-modal\nalgorithms. However, its high computational and memory demands hinder inference\nspeed and energy efficiency. To address these challenges, we identify three\ncore issues: (1) intensive and often redundant computations, (2) heterogeneous\noperations involving convolutions and attention mechanisms, and (3) diverse\nweight and activation sizes.\n  We present SD-Acc, a novel algorithm and hardware co-optimization framework.\nAt the algorithm level, we observe that high-level features in certain\ndenoising phases show significant similarity, enabling approximate computation.\nLeveraging this, we propose an adaptive, phase-aware sampling strategy that\nreduces compute and memory loads. This framework automatically balances image\nquality and complexity based on the StableDiff model and user requirements. At\nthe hardware level, we design an address-centric dataflow to efficiently handle\nheterogeneous operations within a simple systolic array. We address the\nbottleneck of nonlinear functions via a two-stage streaming architecture and a\nreconfigurable vector processing unit. Additionally, we implement adaptive\ndataflow optimizations by combining dynamic reuse and operator fusion tailored\nto StableDiff workloads, significantly reducing memory access. Across multiple\nStableDiff models, our method achieves up to a 3x reduction in computational\ndemand without compromising image quality. Combined with our optimized hardware\naccelerator, SD-Acc delivers higher speed and energy efficiency than\ntraditional CPU and GPU implementations.", "AI": {"tldr": "The paper introduces SD-Acc, a framework to optimize computation and hardware for text-to-image generation in Stable Diffusion models, improving speed and energy efficiency while reducing computational demands by up to 3x.", "motivation": "Stable Diffusion models, despite their advanced generative capabilities, are hindered by high computational and memory demands, limiting their speed and energy efficiency.", "method": "SD-Acc employs both algorithmic and hardware optimizations. Algorithmically, it uses an adaptive, phase-aware sampling strategy to reduce redundant computations and balance image quality with complexity. Hardware-wise, it incorporates an address-centric dataflow system, a two-stage streaming architecture for nonlinear functions, and adaptive dataflow optimizations tailored to StableDiff workloads.", "result": "The framework achieves up to a 3x reduction in computational demands across StableDiff models without degrading image quality. When paired with the optimized hardware accelerator, it outperforms conventional CPU and GPU setups in speed and energy efficiency.", "conclusion": "SD-Acc effectively balances the trade-off between computational efficiency and image quality in Stable Diffusion models. Its co-optimization approach streamlines both software and hardware processes, making text-to-image generation faster and more energy-efficient."}}
{"id": "2507.01224", "pdf": "https://arxiv.org/pdf/2507.01224", "abs": "https://arxiv.org/abs/2507.01224", "authors": ["Wenqi Jia", "Ying Huang", "Jian Xu", "Zhewen Hu", "Sian Jin", "Jiannan Tian", "Yuede Ji", "Miao Yin"], "title": "FLARE: A Dataflow-Aware and Scalable Hardware Architecture for Neural-Hybrid Scientific Lossy Compression", "categories": ["cs.DC"], "comment": null, "summary": "Scientific simulation leveraging high-performance computing (HPC) systems is\ncrucial for modeling complex systems and phenomena in fields such as\nastrophysics, climate science, and fluid dynamics, generating massive datasets\nthat often reach petabyte to exabyte scales. However, managing these vast data\nvolumes introduces significant I/O and network bottlenecks, limiting practical\nperformance and scalability. While cutting-edge lossy compression frameworks\npowered by deep neural networks (DNNs) have demonstrated superior compression\nratios by capturing complex data correlations, their integration into HPC\nworkflows poses substantial challenges due to the hybrid non-neural and neural\ncomputation patterns, causing excessive memory access overhead, large\nsequential stalls, and limited adaptability to varying data sizes and workloads\nin existing hardware platforms. To overcome these challenges and push the limit\nof high-performance scientific computing, we for the first time propose FLARE,\na dataflow-aware and scalable hardware architecture for neural-hybrid\nscientific lossy compression. FLARE minimizes off-chip data access, reduces\nbubble overhead through efficient dataflow, and adopts a modular design that\nprovides both scalability and flexibility, significantly enhancing throughput\nand energy efficiency on modern HPC systems. Particularly, the proposed FLARE\nachieves runtime speedups ranging from $3.50 \\times$ to $96.07 \\times$, and\nenergy efficiency improvements ranging from $24.51 \\times$ to $520.68 \\times$,\nacross various datasets and hardware platforms.", "AI": {"tldr": "This paper introduces FLARE, a hardware architecture addressing scalability, energy efficiency, and runtime bottlenecks in HPC workflows using neural-hybrid lossy compression.", "motivation": "The need to manage substantial I/O and network bottlenecks in processing massive scientific datasets while leveraging advanced lossy compression frameworks.", "method": "FLARE employs a dataflow-aware scalable hardware architecture minimizing memory access overhead and implementing modular design for adaptability and efficiency.", "result": "FLARE demonstrated significant runtime speedups (up to 96.07x) and energy efficiency improvements (up to 520.68x) across varied datasets and hardware.", "conclusion": "FLARE is a groundbreaking solution that enhances HPC systems' capability to handle scientific lossy compression efficiently, addressing critical bottlenecks and promoting scalability."}}
{"id": "2507.01429", "pdf": "https://arxiv.org/pdf/2507.01429", "abs": "https://arxiv.org/abs/2507.01429", "authors": ["Benjamin Chen Ming Choong", "Tao Luo", "Cheng Liu", "Bingsheng He", "Wei Zhang", "Joey Tianyi Zhou"], "title": "Hardware-software co-exploration with racetrack memory based in-memory computing for CNN inference in embedded systems", "categories": ["cs.ET", "cs.AI", "cs.AR"], "comment": null, "summary": "Deep neural networks generate and process large volumes of data, posing\nchallenges for low-resource embedded systems. In-memory computing has been\ndemonstrated as an efficient computing infrastructure and shows promise for\nembedded AI applications. Among newly-researched memory technologies, racetrack\nmemory is a non-volatile technology that allows high data density fabrication,\nmaking it a good fit for in-memory computing. However, integrating in-memory\narithmetic circuits with memory cells affects both the memory density and power\nefficiency. It remains challenging to build efficient in-memory arithmetic\ncircuits on racetrack memory within area and energy constraints. To this end,\nwe present an efficient in-memory convolutional neural network (CNN)\naccelerator optimized for use with racetrack memory. We design a series of\nfundamental arithmetic circuits as in-memory computing cells suited for\nmultiply-and-accumulate operations. Moreover, we explore the design space of\nracetrack memory based systems and CNN model architectures, employing co-design\nto improve the efficiency and performance of performing CNN inference in\nracetrack memory while maintaining model accuracy. Our designed circuits and\nmodel-system co-optimization strategies achieve a small memory bank area with\nsignificant improvements in energy and performance for racetrack memory based\nembedded systems.", "AI": {"tldr": "This paper introduces an in-memory CNN accelerator using racetrack memory to improve energy and performance efficiency while maintaining accuracy.", "motivation": "The motivation is to address the challenges posed by the large data requirements of deep neural networks on low-resource, embedded systems, by leveraging racetrack memory for energy-efficient in-memory computing.", "method": "The authors design fundamental arithmetic circuits tailored for multiply-and-accumulate operations and perform a co-design of racetrack memory-based systems with CNN model architectures to optimize efficiency, energy use, and memory area.", "result": "The proposed designs achieve significant improvements in energy efficiency, system performance, and compact memory bank usage in racetrack memory-based embedded systems.", "conclusion": "The work demonstrates the potential of racetrack memory for CNN inference, showing how co-design strategies can overcome the constraints of area and energy to develop efficient embedded AI systems."}}
{"id": "2507.01225", "pdf": "https://arxiv.org/pdf/2507.01225", "abs": "https://arxiv.org/abs/2507.01225", "authors": ["Sunandita Patra", "Mehtab Pathan", "Mahmoud Mahfouz", "Parisa Zehtabi", "Wided Ouaja", "Daniele Magazzeni", "Manuela Veloso"], "title": "Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration", "categories": ["cs.DC", "cs.AI"], "comment": "Please cite as: Sunandita Patra, Mehtab Pathan, Mahmoud Mahfouz,\n  Parisa Zehtabi, Wided Ouaja, Daniele Magazzeni, and Manuela Veloso. \"Capacity\n  planning and scheduling for jobs with uncertainty in resource usage and\n  duration.\" The Journal of Supercomputing 80, no. 15 (2024): 22428-22461", "summary": "Organizations around the world schedule jobs (programs) regularly to perform\nvarious tasks dictated by their end users. With the major movement towards\nusing a cloud computing infrastructure, our organization follows a hybrid\napproach with both cloud and on-prem servers. The objective of this work is to\nperform capacity planning, i.e., estimate resource requirements, and job\nscheduling for on-prem grid computing environments. A key contribution of our\napproach is handling uncertainty in both resource usage and duration of the\njobs, a critical aspect in the finance industry where stochastic market\nconditions significantly influence job characteristics. For capacity planning\nand scheduling, we simultaneously balance two conflicting objectives: (a)\nminimize resource usage, and (b) provide high quality-of-service to the end\nusers by completing jobs by their requested deadlines. We propose approximate\napproaches using deterministic estimators and pair sampling-based constraint\nprogramming. Our best approach (pair sampling-based) achieves much lower peak\nresource usage compared to manual scheduling without compromising on the\nquality-of-service.", "AI": {"tldr": "The paper addresses capacity planning and job scheduling for hybrid on-prem cloud environments in the context of finance, emphasizing uncertainty handling and competing objectives of minimizing resources and adhering to deadlines.", "motivation": "The authors aim to solve resource estimation and scheduling problems in grid computing environments, with a focus on coping with uncertainty in job parameters, especially for the finance sector.", "method": "The authors propose approximate methodologies such as deterministic estimators and pair sampling-based constraint programming for scheduling and planning tasks.", "result": "They found that the pair sampling-based approach significantly reduces peak resource usage while maintaining high-quality service compared to manual scheduling.", "conclusion": "The proposed methods, especially pair sampling, offer an effective strategy for optimizing resource usage and meeting service deadlines in uncertain environments, showcasing a promising solution for industries like finance."}}
{"id": "2507.01524", "pdf": "https://arxiv.org/pdf/2507.01524", "abs": "https://arxiv.org/abs/2507.01524", "authors": ["Johannes Lengler", "Tom Offermann"], "title": "Diversity-Preserving Exploitation of Crossover", "categories": ["cs.NE"], "comment": "accepted at FOGA 2025", "summary": "Crossover is a powerful mechanism for generating new solutions from a given\npopulation of solutions. Crossover comes with a discrepancy in itself: on the\none hand, crossover usually works best if there is enough diversity in the\npopulation; on the other hand, exploiting the benefits of crossover reduces\ndiversity. This antagonism often makes crossover reduce its own effectiveness.\n  We introduce a new paradigm for utilizing crossover that reduces this\nantagonism, which we call diversity-preserving exploitation of crossover\n(DiPEC). The resulting Diversity Exploitation Genetic Algorithm (DEGA) is able\nto still exploit the benefits of crossover, but preserves a much higher\ndiversity than conventional approaches.\n  We demonstrate the benefits by proving that the (2+1)-DEGA finds the optimum\nof LeadingOnes with $O(n^{5/3}\\log^{2/3} n)$ fitness evaluations. This is\nremarkable since standard genetic algorithms need $\\Theta(n^2)$ evaluations,\nand among genetic algorithms only some artificial and specifically tailored\nalgorithms were known to break this runtime barrier. We confirm the theoretical\nresults by simulations. Finally, we show that the approach is not overfitted to\nLeadingones by testing it empirically on other benchmarks and showing that it\nis also competitive in other settings. We believe that our findings justify\nfurther systematic investigations of the DiPEC paradigm.", "AI": {"tldr": "The paper introduces a genetic algorithm paradigm, DiPEC, which maintains diversity during crossover to improve optimization performance.", "motivation": "The antagonism between diversity preservation and exploitation in crossover often reduces its effectiveness in genetic algorithms.", "method": "The authors propose the Diversity Exploitation Genetic Algorithm (DEGA), which leverages the DiPEC paradigm to maintain diversity while exploiting crossover benefits.", "result": "The (2+1)-DEGA achieves $O(n^{5/3}\\log^{2/3} n)$ fitness evaluations for the LeadingOnes problem, outperforming standard genetic algorithms requiring $\\Theta(n^2)$. Simulations and other benchmarks validate its competitiveness.", "conclusion": "The DiPEC paradigm enhances genetic algorithm efficiency and represents a promising direction for future systematic research."}}
{"id": "2507.01676", "pdf": "https://arxiv.org/pdf/2507.01676", "abs": "https://arxiv.org/abs/2507.01676", "authors": ["Giuseppe Ruggeri", "Renzo Andri", "Daniele Jahier Pagliari", "Lukas Cavigelli"], "title": "Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.IR", "C.4; D.1.3; H.3.3; H.3.4"], "comment": "5 pages, 4 figures, conference: IEEE ICCD24", "summary": "Deep Recommender Models (DLRMs) inference is a fundamental AI workload\naccounting for more than 79% of the total AI workload in Meta's data centers.\nDLRMs' performance bottleneck is found in the embedding layers, which perform\nmany random memory accesses to retrieve small embedding vectors from tables of\nvarious sizes. We propose the design of tailored data flows to speedup\nembedding look-ups. Namely, we propose four strategies to look up an embedding\ntable effectively on one core, and a framework to automatically map the tables\nasymmetrically to the multiple cores of a SoC. We assess the effectiveness of\nour method using the Huawei Ascend AI accelerators, comparing it with the\ndefault Ascend compiler, and we perform high-level comparisons with Nvidia\nA100. Results show a speed-up varying from 1.5x up to 6.5x for real workload\ndistributions, and more than 20x for extremely unbalanced distributions.\nFurthermore, the method proves to be much more independent of the query\ndistribution than the baseline.", "AI": {"tldr": "The paper addresses the bottleneck in embedding layers of Deep Recommender Models (DLRMs) by proposing four strategies for efficient embedding look-ups and a framework for asymmetric table mapping to cores, achieving significant speed-ups, especially in unbalanced workloads.", "motivation": "The motivation is to address the performance bottleneck in DLRM inference, particularly in the embedding layers, which dominate the AI workload in Meta's data centers due to their intensive random memory access demands.", "method": "The method involves designing tailored data flows with four efficient embedding look-up strategies and introducing a framework for asymmetric mapping of embedding tables to multiple cores on System-on-Chips (SoCs).", "result": "The proposed approach achieves speed-ups ranging from 1.5x to 6.5x for real-world workload distributions and over 20x for highly unbalanced ones. It also shows better consistency regardless of query distribution, compared to baseline approaches.", "conclusion": "The approach significantly accelerates DLRM embedding look-ups, especially under unbalanced conditions, proving to be scalable and reliable across various workload distributions."}}
{"id": "2507.01298", "pdf": "https://arxiv.org/pdf/2507.01298", "abs": "https://arxiv.org/abs/2507.01298", "authors": ["Debasish Pattanayak", "Ajay D. Kshemkalyani", "Manish Kumar", "Anisur Rahaman Molla", "Gokarna Sharma"], "title": "Optimal Dispersion Under Asynchrony", "categories": ["cs.DC", "cs.DS", "cs.MA", "cs.RO"], "comment": "35 pages, 5 figures, 2 tables, and 6 pseudocodes", "summary": "We study the dispersion problem in anonymous port-labeled graphs: $k \\leq n$\nmobile agents, each with a unique ID and initially located arbitrarily on the\nnodes of an $n$-node graph with maximum degree $\\Delta$, must autonomously\nrelocate so that no node hosts more than one agent. Dispersion serves as a\nfundamental task in distributed computing of mobile agents, and its complexity\nstems from key challenges in local coordination under anonymity and limited\nmemory.\n  The goal is to minimize both the time to achieve dispersion and the memory\nrequired per agent. It is known that any algorithm requires $\\Omega(k)$ time in\nthe worst case, and $\\Omega(\\log k)$ bits of memory per agent. A recent result\n[SPAA'25] gives an optimal $O(k)$-time algorithm in the synchronous setting and\nan $O(k \\log k)$-time algorithm in the asynchronous setting, both using\n$O(\\log(k+\\Delta))$ bits.\n  In this paper, we close the complexity gap in the asynchronous setting by\npresenting the first dispersion algorithm that runs in optimal $O(k)$ time\nusing $O(\\log(k+\\Delta))$ bits of memory per agent. Our solution is based on a\nnovel technique we develop in this paper that constructs a port-one tree in\nanonymous graphs, which may be of independent interest.", "AI": {"tldr": "This paper presents an optimal algorithm for the dispersion problem in asynchronous anonymous graphs, achieving $O(k)$ time while requiring $O(\\log(k+\\Delta))$ memory per agent.", "motivation": "The authors aim to address the historical challenge of efficiently solving the dispersion problem in graphs, minimizing time and memory overhead, especially in the asynchronous setting.", "method": "A novel technique is introduced to construct a port-one tree in anonymous graphs, facilitating efficient dispersion.", "result": "The paper closes the existing complexity gap by proposing an algorithm that achieves optimality in both time ($O(k)$) and memory usage ($O(\\log(k+\\Delta))$).", "conclusion": "The proposed method not only achieves optimal performance but introduces a tree construction technique that may have broader applications in distributed computing scenarios involving mobile agents."}}
{"id": "2507.01629", "pdf": "https://arxiv.org/pdf/2507.01629", "abs": "https://arxiv.org/abs/2507.01629", "authors": ["Tome Eftimov", "Peter Koro\u0161ec"], "title": "Adaptive Estimation of the Number of Algorithm Runs in Stochastic Optimization", "categories": ["cs.NE"], "comment": null, "summary": "Determining the number of algorithm runs is a critical aspect of experimental\ndesign, as it directly influences the experiment's duration and the reliability\nof its outcomes. This paper introduces an empirical approach to estimating the\nrequired number of runs per problem instance for accurate estimation of the\nperformance of the continuous single-objective stochastic optimization\nalgorithm. The method leverages probability theory, incorporating a robustness\ncheck to identify significant imbalances in the data distribution relative to\nthe mean, and dynamically adjusts the number of runs during execution as an\nonline approach. The proposed methodology was extensively tested across two\nalgorithm portfolios (104 Differential Evolution configurations and the\nNevergrad portfolio) and the COCO benchmark suite, totaling 5748000 runs. The\nresults demonstrate 82% - 95% accuracy in estimations across different\nalgorithms, allowing a reduction of approximately 50% in the number of runs\nwithout compromising optimization outcomes. This online calculation of required\nruns not only improves benchmarking efficiency, but also contributes to energy\nreduction, fostering a more environmentally sustainable computing ecosystem.", "AI": {"tldr": "This paper presents a way to streamline the number of algorithm runs needed while maintaining reliable performance estimates for optimization tasks.", "motivation": "To address the challenge of balancing experiment duration with the reliability of outcomes in stochastic optimization benchmarking.", "method": "An empirical, online approach dynamically estimates required runs using probability theory, robustness checks for data imbalances, and adjustments during execution.", "result": "Achieved 82%-95% accurate estimations while reducing about 50% of runs in benchmarks across multiple algorithm portfolios.", "conclusion": "The proposed method enhances efficiency and sustainability in benchmarking by reducing computation time and energy requirements without sacrificing outcome reliability."}}
{"id": "2507.01231", "pdf": "https://arxiv.org/pdf/2507.01231", "abs": "https://arxiv.org/abs/2507.01231", "authors": ["I\u00f1aki Dellibarda Varela", "Pablo Romero-Sorozabal", "Eduardo Rocon", "Manuel Cebrian"], "title": "Rethinking the Illusion of Thinking", "categories": ["cs.AI"], "comment": "8 pages, 4 figures", "summary": "Earlier this year, Apple ignited controversy by publishing \"The Illusion of\nThinking,\" prompting heated debate within the AI community. Critics seized upon\nthe findings as conclusive evidence that Large Reasoning Models (LRMs) lack\ngenuine reasoning capabilities, branding them as mere stochastic parrots.\nMeanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning\nthe experimental setup as flawed and the conclusions overstated. We clarify\nthis debate by replicating and refining two of the original study's most\ncontentious benchmarks: Towers of Hanoi and River Crossing. By introducing\nincremental stepwise prompting and agentic collaborative dialogue, we show that\npreviously reported failures solving the Towers of Hanoi were not purely result\nof output constraints, but also partly a result of cognition limitations: LRMs\nstill stumble when complexity rises moderately (around 8 disks). Moreover, the\nRiver Crossing results initially heralded as catastrophic failures turn out to\nhinge upon testing unsolvable configurations. Once we limit tests strictly to\nsolvable problems-LRMs effortlessly solve large instances involving over 100\nagent pairs. Our findings ultimately defy simplistic narratives: today's LRMs\nare stochastic, RL-tuned searchers in a discrete state space we barely\nunderstand. Real progress in symbolic, long-horizon reasoning demands mapping\nthat terrain through fine-grained ablations like those introduced here.", "AI": {"tldr": "The paper re-evaluates critiques of Large Reasoning Models (LRMs) by refining contentious benchmarks, revealing nuanced reasoning limitations.", "motivation": "To clarify debates on whether LRMs genuinely lack reasoning abilities as branded by 'The Illusion of Thinking.'", "method": "Through replication and refinement of benchmarks (Towers of Hanoi and River Crossing), introducing techniques such as incremental stepwise prompting and agentic collaborative dialogue.", "result": "Finds LRMs struggle with moderate complexity (e.g., Towers of Hanoi with 8 disks). Contrarily, LRMs excel at solving solvable River Crossing problems, past claims mischaracterized failures due to testing unsolvable configurations.", "conclusion": "LRMs are not simplistic 'stochastic parrots' but iterative reasoners in an underexplored search state space. Progress requires detailed ablation studies like this one."}}
{"id": "2507.01272", "pdf": "https://arxiv.org/pdf/2507.01272", "abs": "https://arxiv.org/abs/2507.01272", "authors": ["Zixuan Zhu"], "title": "Advanced LPeg techniques: A dual case study approach", "categories": ["cs.PL"], "comment": null, "summary": "This paper presents advanced optimization techniques for Lua Parsing\nExpression Grammars (LPeg) through two complementary case studies: a\nhigh-performance JSON parser and a sophisticated Glob-to-LPeg pattern\nconverter. We demonstrate how strategic grammar construction can dramatically\nimprove parsing performance without modifying the underlying LPeg library. For\nthe JSON parser, we implement substitution capture and table construction\noptimization to reduce memory allocation overhead and improve object\nprocessing. For the Glob converter, we introduce segment-boundary separation,\nimplement Cox's flattened search strategy, and develop optimized braced\ncondition handling to prevent exponential backtracking. Comprehensive\nbenchmarks demonstrate that our JSON parser achieves processing speeds up to\n125 MB/s on complex documents, consistently outperforming dkjson and showing\ncompetitive results against rxi_json across most test cases. Our Glob-to-LPeg\nconverter exhibits 14-92% better performance than Bun.Glob and runs 3-14 times\nfaster than Minimatch across diverse pattern matching scenarios. This research\nprovides practical optimization techniques for LPeg-based parsers, contributing\nvaluable strategies to the text processing ecosystem.", "AI": {"tldr": "The paper enhances Lua Parsing Expression Grammars (LPeg) by showcasing optimizations through case studies on JSON parsing and Glob pattern conversion, significantly boosting performance benchmarks.", "motivation": "Optimize parsing performance for LPeg without altering its library, targeting improved memory and computational efficiency.", "method": "Introduced grammar construction strategies: JSON parser optimization via substitution capture and table handling; Glob converter via boundary separation, Cox's strategy, and braced condition optimization.", "result": "JSON parser achieved speeds of up to 125 MB/s, surpassing dkjson and matching rxi_json; Glob converter outperformed Bun.Glob by 14-92% and Minimatch by 300-1400%.", "conclusion": "The research provides novel, practical strategies for optimizing LPeg parsers, enhancing text processing efficiency and contributing to its ecosystem."}}
{"id": "2507.01065", "pdf": "https://arxiv.org/pdf/2507.01065", "abs": "https://arxiv.org/abs/2507.01065", "authors": ["Christiaan Verwijs", "Evelien Acun-Roos", "Daniel Russo"], "title": "Is It Safe To Learn And Share? On Psychological Safety and Social Learning in (Agile) Communities of Practice", "categories": ["cs.SE"], "comment": null, "summary": "As hybrid, distributed, and asynchronous work models become more prevalent,\ncontinuous learning in Agile Software Development (ASD) gains renewed\nimportance. Communities of Practice (CoPs) are increasingly adopted to support\nsocial learning beyond formal education, often relying on virtual\ncommunication. Psychological safety, a prerequisite for effective learning,\nremains insufficiently understood in these settings. This mixed-methods study\ninvestigates psychological safety within Agile CoPs through survey data from\n143 participants. Results indicate that psychological safety is significantly\nlower in online interactions compared to face-to-face settings. Moreover, low\npsychological safety reduces participants' intent to continue contributing and\navoidance of interpersonal risk. No significant differences emerged based on\ngender, community seniority, or content creation activity. However, differences\nby role and age group suggest potential generational or role-related effects.\nThematic analysis revealed exclusionary behavior, negative interaction\npatterns, and hostility as primary threats to psychological safety, often\nreinforced by tribalism and specific community dynamics. Suggested\ninterventions include establishing explicit norms, structured facilitation, and\nactive moderation. The findings were validated through member checking with 30\nparticipants. This study provides a comparative perspective on interaction\nmodalities and offers practical guidance for organizers seeking to cultivate\ninclusive, high-impact CoPs and similarly structured virtual or hybrid work\nenvironments.", "AI": {"tldr": "This study highlights psychological safety issues in Agile Communities of Practice (CoPs), emphasizing lower safety in online vs. face-to-face settings and offering interventions to improve inclusivity.", "motivation": "To address the gap in understanding psychological safety in virtual and hybrid Agile CoPs environments and its impact on continuous learning.", "method": "A mixed-methods approach including surveys from 143 participants and thematic analysis of interaction patterns, validated through member checking with 30 participants.", "result": "Psychological safety in online interactions is significantly lower than face-to-face settings, affecting contribution intent and interpersonal risk-taking, with threats like exclusionary behavior and tribalism identified.", "conclusion": "Enforcing explicit norms, structured facilitation, and active moderation can improve psychological safety in virtual work environments, enhancing social learning and community engagement."}}
{"id": "2507.01019", "pdf": "https://arxiv.org/pdf/2507.01019", "abs": "https://arxiv.org/abs/2507.01019", "authors": ["Imran Mirza", "Cole Huang", "Ishwara Vasista", "Rohan Patil", "Asli Akalin", "Sean O'Brien", "Kevin Zhu"], "title": "MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered", "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to Building Trust in LLMs @ ICLR 2025 and NAACL SRW 2025", "summary": "Multi-agent systems, which consist of multiple AI models interacting within a\nshared environment, are increasingly used for persona-based interactions.\nHowever, if not carefully designed, these systems can reinforce implicit biases\nin large language models (LLMs), raising concerns about fairness and equitable\nrepresentation. We present MALIBU, a novel benchmark developed to assess the\ndegree to which LLM-based multi-agent systems implicitly reinforce social\nbiases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems\nthrough scenario-based assessments. AI models complete tasks within predefined\ncontexts, and their responses undergo evaluation by an LLM-based multi-agent\njudging system in two phases. In the first phase, judges score responses\nlabeled with specific demographic personas (e.g., gender, race, religion)\nacross four metrics. In the second phase, judges compare paired responses\nassigned to different personas, scoring them and selecting the superior\nresponse. Our study quantifies biases in LLM-generated outputs, revealing that\nbias mitigation may favor marginalized personas over true neutrality,\nemphasizing the need for nuanced detection, balanced fairness strategies, and\ntransparent evaluation benchmarks in multi-agent systems.", "AI": {"tldr": "The paper introduces MALIBU, a benchmark for evaluating implicit biases in multi-agent systems using large language models.", "motivation": "To address concerns about potential reinforcement of social biases and stereotypes by LLM-based multi-agent systems during persona-based interactions.", "method": "The benchmark assesses biases using scenario-based tasks judged by an LLM-based system in two phases: scoring individual demographic personas and comparing paired responses.", "result": "Results show that bias mitigation strategies may sometimes favor marginalized groups, highlighting challenges in achieving true neutrality.", "conclusion": "The study underscores the importance of nuanced bias detection, balanced fairness approaches, and transparent benchmarks in designing multi-agent systems."}}
{"id": "2507.01411", "pdf": "https://arxiv.org/pdf/2507.01411", "abs": "https://arxiv.org/abs/2507.01411", "authors": ["Yifei Sun", "Marshall A. Dalton", "Robert D. Sanders", "Yixuan Yuan", "Xiang Li", "Sharon L. Naismith", "Fernando Calamante", "Jinglei Lv"], "title": "Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Grey matter loss in the hippocampus is a hallmark of neurobiological aging,\nyet understanding the corresponding changes in its functional connectivity\nremains limited. Seed-based functional connectivity (FC) analysis enables\nvoxel-wise mapping of the hippocampus's synchronous activity with cortical\nregions, offering a window into functional reorganization during aging. In this\nstudy, we develop an interpretable deep learning framework to predict brain age\nfrom hippocampal FC using a three-dimensional convolutional neural network (3D\nCNN) combined with LayerCAM saliency mapping. This approach maps key\nhippocampal-cortical connections, particularly with the precuneus, cuneus,\nposterior cingulate cortex, parahippocampal cortex, left superior parietal\nlobule, and right superior temporal sulcus, that are highly sensitive to age.\nCritically, disaggregating anterior and posterior hippocampal FC reveals\ndistinct mapping aligned with their known functional specializations. These\nfindings provide new insights into the functional mechanisms of hippocampal\naging and demonstrate the power of explainable deep learning to uncover\nbiologically meaningful patterns in neuroimaging data.", "AI": {"tldr": "This study employs a deep learning approach to map functional connectivity changes in the hippocampus during neurobiological aging, offering insights into age-sensitive brain regions.", "motivation": "Understanding how functional connectivity in the hippocampus evolves with neurobiological aging remains underexplored, despite the known hallmark of grey matter loss in this region.", "method": "The paper develops a deep learning framework combining 3D convolutional neural networks (3D CNN) and LayerCAM saliency mapping to analyze hippocampal functional connectivity for brain age predictions.", "result": "This method identifies key hippocampal-cortical connections, including regions like the precuneus, cuneus, posterior cingulate cortex, and parahippocampal cortex, that are significantly impacted by aging. It also highlights distinct anterior and posterior hippocampal connectivity patterns.", "conclusion": "The study provides novel insights into hippocampal aging mechanisms and demonstrates how interpretable deep learning methods can uncover biologically relevant patterns in neuroimaging."}}
{"id": "2507.01044", "pdf": "https://arxiv.org/pdf/2507.01044", "abs": "https://arxiv.org/abs/2507.01044", "authors": ["Vivek Borkar", "Parthe Pandit"], "title": "Asymptotic convexity of wide and shallow neural networks", "categories": ["stat.ML", "cs.LG", "math.PR", "68T07"], "comment": "5 pages", "summary": "For a simple model of shallow and wide neural networks, we show that the\nepigraph of its input-output map as a function of the network parameters\napproximates epigraph of a. convex function in a precise sense. This leads to a\nplausible explanation of their observed good performance.", "AI": {"tldr": "The paper demonstrates that the input-output behavior of a shallow and wide neural network approximates a convex function, shedding light on the network's strong performance.", "motivation": "To understand the underlying reason for the observed high performance of shallow and wide neural networks.", "method": "The study examines the epigraph of the neural network's input-output map and compares it to the epigraph of a convex function.", "result": "The result reveals that shallow and wide neural networks approximate convex behavior in their parameter space.", "conclusion": "This approximation to convexity may provide a plausible explanation for the effective performance of such networks."}}
{"id": "2507.01099", "pdf": "https://arxiv.org/pdf/2507.01099", "abs": "https://arxiv.org/abs/2507.01099", "authors": ["Zeyi Liu", "Shuang Li", "Eric Cousineau", "Siyuan Feng", "Benjamin Burchfiel", "Shuran Song"], "title": "Geometry-aware 4D Video Generation for Robot Manipulation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project website: https://robot4dgen.github.io", "summary": "Understanding and predicting the dynamics of the physical world can enhance a\nrobot's ability to plan and interact effectively in complex environments. While\nrecent video generation models have shown strong potential in modeling dynamic\nscenes, generating videos that are both temporally coherent and geometrically\nconsistent across camera views remains a significant challenge. To address\nthis, we propose a 4D video generation model that enforces multi-view 3D\nconsistency of videos by supervising the model with cross-view pointmap\nalignment during training. This geometric supervision enables the model to\nlearn a shared 3D representation of the scene, allowing it to predict future\nvideo sequences from novel viewpoints based solely on the given RGB-D\nobservations, without requiring camera poses as inputs. Compared to existing\nbaselines, our method produces more visually stable and spatially aligned\npredictions across multiple simulated and real-world robotic datasets. We\nfurther show that the predicted 4D videos can be used to recover robot\nend-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting\nrobust robot manipulation and generalization to novel camera viewpoints.", "AI": {"tldr": "The paper introduces a 4D video generation model that ensures geometric and temporal consistency by applying cross-view pointmap alignment during training, aiding in robot vision and planning.", "motivation": "Robots struggle with effectively planning and interacting in complex environments due to challenges in accurately modeling dynamic physical scenes.", "method": "The proposed method uses a 4D video generation model supervised with cross-view pointmap alignment to enforce 3D consistency, requiring only RGB-D observations and no camera pose inputs.", "result": "The method achieves visually stable and spatially aligned video predictions across multiple datasets and supports robot end-effector trajectory reconstruction with a 6DoF pose tracker.", "conclusion": "Using 4D video predictions enriched with geometric consistency significantly benefits robotic manipulation and generalization for novel camera viewpoints."}}
{"id": "2507.01026", "pdf": "https://arxiv.org/pdf/2507.01026", "abs": "https://arxiv.org/abs/2507.01026", "authors": ["Md Shakil Ahamed Shohag", "Q. M. Jonathan Wu", "Farhad Pourpanah"], "title": "Few-Shot Inspired Generative Zero-Shot Learning", "categories": ["cs.LG"], "comment": null, "summary": "Generative zero-shot learning (ZSL) methods typically synthesize visual\nfeatures for unseen classes using predefined semantic attributes, followed by\ntraining a fully supervised classification model. While effective, these\nmethods require substantial computational resources and extensive synthetic\ndata, thereby relaxing the original ZSL assumptions. In this paper, we propose\nFSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on\nlarge-scale feature synthesis. Our key insight is that class-level attributes\nexhibit instance-level variability, i.e., some attributes may be absent or\npartially visible, yet conventional ZSL methods treat them as uniformly\npresent. To address this, we introduce Model-Specific Attribute Scoring (MSAS),\nwhich dynamically re-scores class attributes based on model-specific\noptimization to approximate instance-level variability without access to unseen\ndata. We further estimate group-level prototypes as clusters of instances based\non MSAS-adjusted attribute scores, which serve as representative synthetic\nfeatures for each unseen class. To mitigate the resulting data imbalance, we\nintroduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training\na semantic-aware contrastive classifier (SCC) using these prototypes.\nExperiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves\ncompetitive performance using far fewer synthetic features.", "AI": {"tldr": "FSIGenZ, a novel few-shot-inspired generative zero-shot learning method, reduces dependence on extensive feature synthesis to improve efficiency and variability handling.", "motivation": "Existing generative ZSL methods demand substantial computation and synthetic data, deviating from the original goal of effective zero-shot learning.", "method": "Proposes Model-Specific Attribute Scoring (MSAS) to adjust class attributes dynamically and group-level prototypes for unseen classes, along with Dual-Purpose Semantic Regularization (DPSR) for addressing data imbalance.", "result": "On datasets like SUN, AwA2, and CUB, FSIGenZ demonstrates competitive performance using significantly fewer synthetic features.", "conclusion": "The FSIGenZ framework offers an efficient and effective approach for ZSL by reducing reliance on large-scale feature synthesis while handling variability and imbalance challenges."}}
{"id": "2507.01111", "pdf": "https://arxiv.org/pdf/2507.01111", "abs": "https://arxiv.org/abs/2507.01111", "authors": ["Haosen Xing", "Haoran Ma", "Sijin Zhang", "Hartmut Geyer"], "title": "Environment-Aware and Human-Cooperative Swing Control for Lower-Limb Prostheses in Diverse Obstacle Scenarios", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "Current control strategies for powered lower limb prostheses often lack\nawareness of the environment and the user's intended interactions with it. This\nlimitation becomes particularly apparent in complex terrains. Obstacle\nnegotiation, a critical scenario exemplifying such challenges, requires both\nreal-time perception of obstacle geometry and responsiveness to user intention\nabout when and where to step over or onto, to dynamically adjust swing\ntrajectories. We propose a novel control strategy that fuses environmental\nawareness and human cooperativeness: an on-board depth camera detects obstacles\nahead of swing phase, prompting an elevated early-swing trajectory to ensure\nclearance, while late-swing control defers to natural biomechanical cues from\nthe user. This approach enables intuitive stepping strategies without requiring\nunnatural movement patterns. Experiments with three non-amputee participants\ndemonstrated 100 percent success across more than 150 step-overs and 30\nstep-ons with randomly placed obstacles of varying heights (4-16 cm) and\ndistances (15-70 cm). By effectively addressing obstacle navigation -- a\ngateway challenge for complex terrain mobility -- our system demonstrates\nadaptability to both environmental constraints and user intentions, with\npromising applications across diverse locomotion scenarios.", "AI": {"tldr": "This paper proposes a novel control strategy for powered prosthetic legs, integrating environmental awareness and human cooperativeness to enable effective obstacle negotiation.", "motivation": "Current control strategies in powered prosthetics struggle with complex terrains, lacking integration of environmental awareness and user intent, particularly in obstacle negotiation scenarios.", "method": "The proposed method combines an on-board depth camera for obstacle detection, which triggers elevated swing trajectories in early-phase, with biomechanical cue-driven control in late-phase to align with natural user intent.", "result": "Experiments with three non-amputee participants showed 100% success in stepping over/onto more than 150 obstacles of varying sizes and placements.", "conclusion": "The system enhances adaptability to environmental and user constraints, showing potential for versatile and intuitive prosthetic mobility in complex terrains."}}
{"id": "2507.01438", "pdf": "https://arxiv.org/pdf/2507.01438", "abs": "https://arxiv.org/abs/2507.01438", "authors": ["Zheyu Shen", "Yexiao He", "Ziyao Wang", "Yuning Zhang", "Guoheng Sun", "Wanghao Ye", "Ang Li"], "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments.", "AI": {"tldr": "EdgeLoRA improves the efficiency of deploying fine-tuned LLMs on resource-constrained edge devices in multi-tenant environments, offering significant gains in latency, throughput, and adapter management.", "motivation": "The paper aims to address challenges in deploying fine-tuned LLMs on edge devices, including adapter complexity, memory overhead, and computational inefficiencies in multi-tenant setups.", "method": "The authors propose EdgeLoRA, which implements adaptive adapter selection, intelligent memory management, and batch inference to optimize LLM serving on edge devices.", "result": "EdgeLoRA demonstrates up to 4x throughput gains and enables simultaneous service of numerous adapters, outperforming existing solutions like llama.cpp.", "conclusion": "EdgeLoRA provides a scalable and efficient system for edge deployment of LLMs, likely transforming multi-tenant edge scenarios by overcoming resource constraints effectively."}}
{"id": "2507.01638", "pdf": "https://arxiv.org/pdf/2507.01638", "abs": "https://arxiv.org/abs/2507.01638", "authors": ["Ana Nikolikj", "Gabriela Ochoa", "Tome Eftimov"], "title": "Customized Exploration of Landscape Features Driving Multi-Objective Combinatorial Optimization Performance", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "We present an analysis of landscape features for predicting the performance\nof multi-objective combinatorial optimization algorithms. We consider features\nfrom the recently proposed compressed Pareto Local Optimal Solutions Networks\n(C-PLOS-net) model of combinatorial landscapes. The benchmark instances are a\nset of rmnk-landscapes with 2 and 3 objectives and various levels of ruggedness\nand objective correlation. We consider the performance of three algorithms --\nPareto Local Search (PLS), Global Simple EMO Optimizer (GSEMO), and\nNon-dominated Sorting Genetic Algorithm (NSGA-II) - using the resolution and\nhypervolume metrics. Our tailored analysis reveals feature combinations that\ninfluence algorithm performance specific to certain landscapes. This study\nprovides deeper insights into feature importance, tailored to specific\nrmnk-landscapes and algorithms.", "AI": {"tldr": "The paper analyzes landscape features from C-PLOS-net models to predict performance in multi-objective combinatorial optimization algorithms.", "motivation": "The study aims to understand the relations between landscape features and algorithm performance to improve optimization techniques.", "method": "It examines rmnk-landscapes with various ruggedness/correlation levels, assessing three algorithms (PLS, GSEMO, NSGA-II) using specific performance metrics.", "result": "The analysis identifies influential feature combinations tied to specific landscape types and algorithm outcomes.", "conclusion": "These insights promote a tailored understanding of optimization design for multi-objective landscapes."}}
{"id": "2507.01282", "pdf": "https://arxiv.org/pdf/2507.01282", "abs": "https://arxiv.org/abs/2507.01282", "authors": ["Matthew JY Kang", "Wenli Yang", "Monica R Roberts", "Byeong Ho Kang", "Charles B Malpas"], "title": "Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "The recent boom of large language models (LLMs) has re-ignited the hope that\nartificial intelligence (AI) systems could aid medical diagnosis. Yet despite\ndazzling benchmark scores, LLM assistants have yet to deliver measurable\nimprovements at the bedside. This scoping review aims to highlight the areas\nwhere AI is limited to make practical contributions in the clinical setting,\nspecifically in dementia diagnosis and care.\n  Standalone machine-learning models excel at pattern recognition but seldom\nprovide actionable, interpretable guidance, eroding clinician trust. Adjacent\nuse of LLMs by physicians did not result in better diagnostic accuracy or\nspeed. Key limitations trace to the data-driven paradigm: black-box outputs\nwhich lack transparency, vulnerability to hallucinations, and weak causal\nreasoning. Hybrid approaches that combine statistical learning with expert\nrule-based knowledge, and involve clinicians throughout the process help bring\nback interpretability. They also fit better with existing clinical workflows,\nas seen in examples like PEIRS and ATHENA-CDS.\n  Future decision-support should prioritise explanatory coherence by linking\npredictions to clinically meaningful causes. This can be done through\nneuro-symbolic or hybrid AI that combines the language ability of LLMs with\nhuman causal expertise. AI researchers have addressed this direction, with\nexplainable AI and neuro-symbolic AI being the next logical steps in further\nadvancement in AI. However, they are still based on data-driven knowledge\nintegration instead of human-in-the-loop approaches. Future research should\nmeasure success not only by accuracy but by improvements in clinician\nunderstanding, workflow fit, and patient outcomes. A better understanding of\nwhat helps improve human-computer interactions is greatly needed for AI systems\nto become part of clinical practice.", "AI": {"tldr": "Large Language Models (LLMs) hold potential for aiding medical diagnoses, particularly in dementia care, but face challenges in transparency, causal reasoning, and clinical integration.", "motivation": "The paper seeks to evaluate and address the limitations of AI systems, especially LLMs, in practical medical applications like dementia care.", "method": "The authors conducted a scoping review to assess the limitations and potentials of AI in clinical settings, exploring hybrid methods for improved interpretability and workflow integration.", "result": "LLMs did not significantly improve diagnostic accuracy or speed when used by physicians. Hybrid AI models show promise, but current approaches still rely on data-driven paradigms that lack clinician involvement.", "conclusion": "Future AI systems should emphasize human-in-the-loop approaches, explanatory coherence, and alignment with clinical workflows to improve trust, usability, and patient outcomes."}}
{"id": "2507.01664", "pdf": "https://arxiv.org/pdf/2507.01664", "abs": "https://arxiv.org/abs/2507.01664", "authors": ["Hector Gramaglia"], "title": "Globality and Regions", "categories": ["cs.PL", "D.3.1; F.3.2"], "comment": null, "summary": "We obtain a characterization of global variables by unifying abstraction with\nregion abstraction in a region-based language. More precisely, in a previous\nwork a language called global was presented, whose virtue is to provide a\nconceptually clear way of introducing imperative operations in a functional\nlanguage. Memory safety is provided by the concept of linear protection, which\nconnects the global system to a linear one. In this paper we show that the\nconcept of global variable provided by the global language arises from the\nTofte and Talping's region language through the unification of abstraction and\nregion abstraction.", "AI": {"tldr": "The paper integrates abstraction with region abstraction to describe global variables in a region-based language.", "motivation": "To offer a clear approach to incorporating imperative operations into functional languages while ensuring memory safety through linear protection.", "method": "The authors use the concept of linear protection to unify abstraction and region abstraction in the global language and connect it to Tofte and Talping's region language.", "result": "The study demonstrates that global variables in the global language naturally emerge from the unification process in the region language.", "conclusion": "The characterization of global variables enables a principled integration of functional and imperative paradigms in region-based languages, ensuring memory safety."}}
{"id": "2507.01103", "pdf": "https://arxiv.org/pdf/2507.01103", "abs": "https://arxiv.org/abs/2507.01103", "authors": ["Jonhnanthan Oliveira", "Rohit Gheyi", "M\u00e1rcio Ribeiro", "Alessandro Garcia"], "title": "Bugs in the Shadows: Static Detection of Faulty Python Refactorings", "categories": ["cs.SE"], "comment": "Accepted at Brazilian Symposium on Software Engineering (SBES 2025)", "summary": "Python is a widely adopted programming language, valued for its simplicity\nand flexibility. However, its dynamic type system poses significant challenges\nfor automated refactoring - an essential practice in software evolution aimed\nat improving internal code structure without changing external behavior.\nUnderstanding how type errors are introduced during refactoring is crucial, as\nsuch errors can compromise software reliability and reduce developer\nproductivity. In this work, we propose a static analysis technique to detect\ntype errors introduced by refactoring implementations for Python. We evaluated\nour technique on Rope refactoring implementations, applying them to open-source\nPython projects. Our analysis uncovered 29 bugs across four refactoring types\nfrom a total of 1,152 refactoring attempts. Several of these issues were also\nfound in widely used IDEs such as PyCharm and PyDev. All reported bugs were\nsubmitted to the respective developers, and some of them were acknowledged and\naccepted. These results highlight the need to improve the robustness of current\nPython refactoring tools to ensure the correctness of automated code\ntransformations and support reliable software maintenance.", "AI": {"tldr": "This paper introduces a static analysis technique to detect type errors in Python during refactoring, uncovering 29 bugs in popular tools and projects.", "motivation": "Dynamic typing in Python complicates automated refactoring, potentially introducing type errors that affect software reliability and developer efficiency.", "method": "The authors proposed a static analysis technique and evaluated it by analyzing Rope refactoring implementations on open-source Python projects.", "result": "The study identified 29 type-related bugs across 4 refactoring types from 1,152 refactorings, with some issues found in IDEs like PyCharm and PyDev.", "conclusion": "The findings emphasize the importance of improving Python refactoring tools' robustness to ensure correct automated transformations and effective software maintenance."}}
{"id": "2507.01160", "pdf": "https://arxiv.org/pdf/2507.01160", "abs": "https://arxiv.org/abs/2507.01160", "authors": ["Huiling You", "Samia Touileb", "Erik Velldal", "Lilja \u00d8vrelid"], "title": "Event-based evaluation of abstractive news summarization", "categories": ["cs.CL"], "comment": "to appear at GEM2 workshop@ACL 2025", "summary": "An abstractive summary of a news article contains its most important\ninformation in a condensed version. The evaluation of automatically generated\nsummaries by generative language models relies heavily on human-authored\nsummaries as gold references, by calculating overlapping units or similarity\nscores. News articles report events, and ideally so should the summaries. In\nthis work, we propose to evaluate the quality of abstractive summaries by\ncalculating overlapping events between generated summaries, reference\nsummaries, and the original news articles. We experiment on a richly annotated\nNorwegian dataset comprising both events annotations and summaries authored by\nexpert human annotators. Our approach provides more insight into the event\ninformation contained in the summaries.", "AI": {"tldr": "The paper examines a method for evaluating abstractive summaries by focusing on overlapping event information between generated, reference, and source content.", "motivation": "Existing evaluation methods for abstractive summaries rely heavily on human-authored references and focus primarily on surface-level similarities. This study aims to focus on events as the key element of news storytelling.", "method": "The authors propose a novel evaluation method for abstractive summaries by calculating overlapping events between generated summaries, reference summaries, and the original news articles. They test this using a Norwegian dataset with detailed event annotations and summaries written by experts.", "result": "Their approach effectively highlights the event-related information contained in summaries and provides insights beyond traditional similarity metrics.", "conclusion": "Using event overlap as an evaluation metric offers a more meaningful understanding of summary quality and suitability for news articles."}}
{"id": "2507.01433", "pdf": "https://arxiv.org/pdf/2507.01433", "abs": "https://arxiv.org/abs/2507.01433", "authors": ["Jatupong Oboun", "Piyanon Charoenpoonpanich", "Anna Raksapatcharawong", "Chaipat Chunharas", "Itthi Chatnuntawech", "Chainarong Amornbunchornvej", "Sirawaj Itthipuripat"], "title": "Reduced Efficiency in the Right Fronto-Parietal Attentional Network During Distractor Suppression in Mild Cognitive Impairment", "categories": ["q-bio.NC", "cs.SI", "stat.AP", "62, 92", "G.3; J.3"], "comment": "First Draft", "summary": "Mild Cognitive Impairment (MCI) is a critical transitional stage between\nnormal cognitive aging and dementia, making its early detection essential. This\nstudy investigates the neural mechanisms of distractor suppression in MCI\npatients using EEG and behavioral data during an attention-cueing Eriksen\nflanker task. A cohort of 56 MCIs and 26 healthy controls (HCs) performed tasks\nwith congruent and incongruent stimuli of varying saliency levels. During these\ntasks, EEG data were analyzed for alpha band coherence's functional\nconnectivity, focusing on Global Efficiency (GE), while Reaction Time (RT) and\nHit Rate (HR) were also collected.\n  Our findings reveal significant interactions between congruency, saliency,\nand cognitive status on GE, RT, and HR. In HCs, congruent conditions resulted\nin higher GE (p = 0.0114, multivariate t-distribution correction, MVT), faster\nRTs (p < 0.0001, MVT), and higher HRs (p < 0.0001, MVT) compared to incongruent\nconditions. HCs also showed increased GE in salient conditions for incongruent\ntrials (p = 0.0406, MVT). MCIs exhibited benefits from congruent conditions\nwith shorter RTs and higher HRs (both p < 0.0001, MVT) compared to incongruent\nconditions but showed reduced adaptability in GE, with no significant GE\ndifferences between conditions.\n  These results highlight the potential of alpha band coherence and GE as early\nmarkers for cognitive impairment. By integrating GE, RT, and HR, this study\nprovides insights into the interplay between neural efficiency, processing\nspeed, and task accuracy. This approach offers valuable insights into cognitive\nload management and interference effects, indicating benefits for interventions\naimed at improving attentional control and processing speed in MCIs.", "AI": {"tldr": "The study explores Mild Cognitive Impairment (MCI) by analyzing attention and distractor suppression through EEG data and behavioral measures during tasks, finding alpha band coherence as a potential marker for early MCI detection.", "motivation": "Early detection of MCI, a transitional stage to dementia, is critical for timely intervention. The paper aims to understand neural mechanisms of attention in MCI patients.", "method": "The study used an EEG and behavioral experiment with a cohort of 56 MCIs and 26 healthy controls during an Eriksen flanker task, analyzing Global Efficiency (GE) in the alpha band, Reaction Time (RT), and Hit Rate (HR) across various conditions.", "result": "Healthy controls showed better GE, faster RT, and higher HR in congruent conditions, while MCIs benefited from congruency with shorter RTs and higher HRs but lacked GE adaptability across conditions.", "conclusion": "Alpha band coherence and GE are promising markers for early cognitive impairment. The study improves understanding of attentional control and suggests implications for interventions targeting MCI."}}
{"id": "2507.01542", "pdf": "https://arxiv.org/pdf/2507.01542", "abs": "https://arxiv.org/abs/2507.01542", "authors": ["Tom Szwagier", "Pierre-Alexandre Mattei", "Charles Bouveyron", "Xavier Pennec"], "title": "Parsimonious Gaussian mixture models with piecewise-constant eigenvalue profiles", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.CO", "stat.ME"], "comment": null, "summary": "Gaussian mixture models (GMMs) are ubiquitous in statistical learning,\nparticularly for unsupervised problems. While full GMMs suffer from the\noverparameterization of their covariance matrices in high-dimensional spaces,\nspherical GMMs (with isotropic covariance matrices) certainly lack flexibility\nto fit certain anisotropic distributions. Connecting these two extremes, we\nintroduce a new family of parsimonious GMMs with piecewise-constant covariance\neigenvalue profiles. These extend several low-rank models like the celebrated\nmixtures of probabilistic principal component analyzers (MPPCA), by enabling\nany possible sequence of eigenvalue multiplicities. If the latter are\nprespecified, then we can naturally derive an expectation-maximization (EM)\nalgorithm to learn the mixture parameters. Otherwise, to address the\nnotoriously-challenging issue of jointly learning the mixture parameters and\nhyperparameters, we propose a componentwise penalized EM algorithm, whose\nmonotonicity is proven. We show the superior likelihood-parsimony tradeoffs\nachieved by our models on a variety of unsupervised experiments: density\nfitting, clustering and single-image denoising.", "AI": {"tldr": "The paper introduces a new family of Gaussian Mixture Models (GMMs) with piecewise-constant covariance eigenvalue profiles, striking a balance between flexibility and parsimony.", "motivation": "The authors aim to address the overparameterization issues in GMM covariance matrices in high dimensions and the inflexibility of spherical GMMs for anisotropic distributions.", "method": "They introduce parsimonious GMMs with piecewise-constant covariance eigenvalue profiles and propose an EM algorithm and a componentwise penalized EM algorithm to handle parameter and hyperparameter learning.", "result": "The proposed models demonstrate superior likelihood-parsimony tradeoffs in tasks such as density fitting, clustering, and single-image denoising.", "conclusion": "The new GMM family extends existing low-rank models, offering greater flexibility and practicality for unsupervised learning tasks in high dimensions."}}
{"id": "2507.01123", "pdf": "https://arxiv.org/pdf/2507.01123", "abs": "https://arxiv.org/abs/2507.01123", "authors": ["Rahul A. Burange", "Harsh K. Shinde", "Omkar Mutyalwar"], "title": "Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "20 pages, 24 figures", "summary": "Landslides pose severe threats to infrastructure, economies, and human lives,\nnecessitating accurate detection and predictive mapping across diverse\ngeographic regions. With advancements in deep learning and remote sensing,\nautomated landslide detection has become increasingly effective. This study\npresents a comprehensive approach integrating multi-source satellite imagery\nand deep learning models to enhance landslide identification and prediction. We\nleverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and\nDigital Elevation Model (DEM) layers to capture critical environmental features\ninfluencing landslide occurrences. Various geospatial analysis techniques are\nemployed to assess the impact of terra in characteristics, vegetation cover,\nand rainfall on detection accuracy. Additionally, we evaluate the performance\nof multiple stateof-the-art deep learning segmentation models, including U-Net,\nDeepLabV3+, and Res-Net, to determine their effectiveness in landslide\ndetection. The proposed framework contributes to the development of reliable\nearly warning systems, improved disaster risk management, and sustainable\nland-use planning. Our findings provide valuable insights into the potential of\ndeep learning and multi-source remote sensing in creating robust, scalable, and\ntransferable landslide prediction models.", "AI": {"tldr": "The study integrates multi-source satellite imagery and deep learning to enhance landslide detection and prediction.", "motivation": "Landslides pose serious risks to infrastructure, economies, and lives, requiring improved detection and predictive methods.", "method": "Multi-source data (Sentinel-2, ALOS PALSAR) and deep learning models (U-Net, DeepLabV3+, Res-Net) were used along with geospatial analysis to identify landslides.", "result": "Deep learning models and multi-source remote sensing demonstrated strong potential in landslide prediction and detection.", "conclusion": "The framework supports early warning systems, disaster management, and land-use planning, offering scalable and reliable landslide prediction."}}
{"id": "2507.01027", "pdf": "https://arxiv.org/pdf/2507.01027", "abs": "https://arxiv.org/abs/2507.01027", "authors": ["Zijian Ye", "Wei Huang", "Yifei Yu", "Tianhe Ren", "Zhongrui Wang", "Xiaojuan Qi"], "title": "DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization", "categories": ["cs.LG"], "comment": "19 pages; Appendix added", "summary": "Large language models (LLMs) demonstrate remarkable performance but face\nsubstantial computational and memory challenges that limit their practical\ndeployment. Quantization has emerged as a promising solution; however, its\neffectiveness is often limited by quantization errors arising from weight\ndistributions that are not quantization-friendly and the presence of activation\noutliers. To address these challenges, we introduce DBellQuant, an innovative\npost-training quantization (PTQ) framework that achieves nearly 1-bit weight\ncompression and 6-bit activation quantization with minimal performance\ndegradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)\nalgorithm, which transforms single-bell weight distributions into dual-bell\nforms to reduce binarization errors and applies inverse transformations to\nsmooth activations. DBellQuant sets a new state-of-the-art by preserving\nsuperior model performance under aggressive weight and activation quantization.\nFor example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of\n14.39 on LLaMA2-13B with 6-bit activation quantization, significantly\noutperforming BiLLM's 21.35 without activation quantization, underscoring its\npotential in compressing LLMs for real-world applications.", "AI": {"tldr": "The paper introduces DBellQuant, a post-training quantization framework that significantly reduces weight and activation bit usage in large language models while maintaining high performance.", "motivation": "The motivation behind this paper is to address the computational and memory limitations of large language models to enable their practical deployment. Existing quantization methods suffer from inaccuracies due to weight distributions and activation outliers.", "method": "The authors propose DBellQuant, which uses the Learnable Transformation for Dual-Bell (LTDB) algorithm. It transforms weight distributions into dual-bell forms to minimize quantization errors and applies inverse transformations to smooth activations.", "result": "DBellQuant achieves significant compression with minimal performance loss. For instance, it achieves a perplexity of 14.39 on the Wikitext2 dataset using LLaMA2-13B with 6-bit activation quantization, outperforming other state-of-the-art methods considerably.", "conclusion": "The study demonstrates the potential of DBellQuant in efficiently compressing language models for deployment in resource-constrained settings, setting new performance benchmarks in aggressive quantization scenarios."}}
{"id": "2507.01125", "pdf": "https://arxiv.org/pdf/2507.01125", "abs": "https://arxiv.org/abs/2507.01125", "authors": ["Keiko Nagami", "Timothy Chen", "Javier Yu", "Ola Shorinwa", "Maximilian Adang", "Carlyn Dougherty", "Eric Cristofalo", "Mac Schwager"], "title": "VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online Semantic Gaussian Splatting", "categories": ["cs.RO"], "comment": "9 pages, 4 figures", "summary": "We present VISTA (Viewpoint-based Image selection with Semantic Task\nAwareness), an active exploration method for robots to plan informative\ntrajectories that improve 3D map quality in areas most relevant for task\ncompletion. Given an open-vocabulary search instruction (e.g., \"find a\nperson\"), VISTA enables a robot to explore its environment to search for the\nobject of interest, while simultaneously building a real-time semantic 3D\nGaussian Splatting reconstruction of the scene. The robot navigates its\nenvironment by planning receding-horizon trajectories that prioritize semantic\nsimilarity to the query and exploration of unseen regions of the environment.\nTo evaluate trajectories, VISTA introduces a novel, efficient\nviewpoint-semantic coverage metric that quantifies both the geometric view\ndiversity and task relevance in the 3D scene. On static datasets, our coverage\nmetric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in\ncomputation speed and reconstruction quality. In quadrotor hardware\nexperiments, VISTA achieves 6x higher success rates in challenging maps,\ncompared to baseline methods, while matching baseline performance in less\nchallenging maps. Lastly, we show that VISTA is platform-agnostic by deploying\nit on a quadrotor drone and a Spot quadruped robot. Open-source code will be\nreleased upon acceptance of the paper.", "AI": {"tldr": "VISTA is a robot exploration method improving 3D map quality using open-vocabulary task-guided exploration.", "motivation": "To enable robots to efficiently explore environments and create high-quality 3D semantic maps while focusing on task-relevant regions based on user queries.", "method": "VISTA plans robot trajectories by combining semantic task awareness and exploration. It uses a viewpoint-semantic coverage metric optimizing geometric view diversity and task relevance.", "result": "VISTA outperforms state-of-the-art methods in computational efficiency and 3D reconstruction quality on static datasets. In quadrotor experiments, it improves success rates in challenging areas by 6x.", "conclusion": "VISTA is a robust, platform-agnostic system for task-guided robotic exploration and semantic 3D mapping, demonstrating significant improvements in complex environments."}}
{"id": "2507.01615", "pdf": "https://arxiv.org/pdf/2507.01615", "abs": "https://arxiv.org/abs/2507.01615", "authors": ["Alper Alimoglu", "Kamil Erdayandi", "Mustafa A. Mustafa", "\u00dcmit Cali"], "title": "EDGChain-E: A Decentralized Git-Based Framework for Versioning Encrypted Energy Data", "categories": ["cs.DC"], "comment": null, "summary": "This paper proposes a new decentralized framework, named EDGChain-E\n(Encrypted-Data-Git Chain for Energy), designed to manage version-controlled,\nencrypted energy data using blockchain and the InterPlanetary File System. The\nframework incorporates a Decentralized Autonomous Organization (DAO) to\norchestrate collaborative data governance across the lifecycle of energy\nresearch and operations, such as smart grid monitoring, demand forecasting, and\npeer-to-peer energy trading. In EDGChain-E, initial commits capture the full\nencrypted datasets-such as smart meter readings or grid telemetry-while\nsubsequent updates are tracked as encrypted Git patches, ensuring integrity,\ntraceability, and privacy. This versioning mechanism supports secure\ncollaboration across multiple stakeholders (e.g., utilities, researchers,\nregulators) without compromising sensitive or regulated information. We\nhighlight the framework's capability to maintain FAIR-compliant (Findable,\nAccessible, Interoperable, Reusable) provenance of encrypted data. By embedding\nhash-based content identifiers in Merkle trees, the system enables transparent,\nauditable, and immutable tracking of data changes, thereby supporting\nreproducibility and trust in decentralized energy applications.", "AI": {"tldr": "The paper introduces EDGChain-E, a blockchain and IPFS-based technology for version-controlled, encrypted energy data management, ensuring traceability, privacy, and decentralized collaboration.", "motivation": "The aim is to solve issues of secure, private, and decentralized collaboration in energy data governance for applications like grid monitoring and energy trading.", "method": "EDGChain-E integrates blockchain, IPFS, and decentralized DAO governance for managing encrypted data with versioning, leveraging Git patches and hash-based identifiers.", "result": "The framework supports FAIR principles, enables secure collaboration, and ensures auditable, transparent data tracking in decentralized energy applications.", "conclusion": "This decentralized framework improves data governance in energy systems, enhancing trust, transparency, and reproducibility while safeguarding sensitive information."}}
{"id": "2507.01668", "pdf": "https://arxiv.org/pdf/2507.01668", "abs": "https://arxiv.org/abs/2507.01668", "authors": ["Gjorgjina Cenikj", "Ga\u0161per Petelin", "Tome Eftimov"], "title": "Comparing Optimization Algorithms Through the Lens of Search Behavior Analysis", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "The field of numerical optimization has recently seen a surge in the\ndevelopment of \"novel\" metaheuristic algorithms, inspired by metaphors derived\nfrom natural or human-made processes, which have been widely criticized for\nobscuring meaningful innovations and failing to distinguish themselves from\nexisting approaches. Aiming to address these concerns, we investigate the\napplicability of statistical tests for comparing algorithms based on their\nsearch behavior. We utilize the cross-match statistical test to compare\nmultivariate distributions and assess the solutions produced by 114 algorithms\nfrom the MEALPY library. These findings are incorporated into an empirical\nanalysis aiming to identify algorithms with similar search behaviors.", "AI": {"tldr": "Many recent optimization algorithms are criticized for lack of innovation, leading to research in statistical methods to compare their search behavior. Authors apply cross-match tests on algorithm outputs from MEALPY library.", "motivation": "To address concerns around oversaturation of 'novel' metaheuristic optimization algorithms with unclear differentiation and innovation.", "method": "Using cross-match statistical tests to compare search behaviors of algorithms based on their generated solutions.", "result": "Cross-match tests are applied to solutions from 114 algorithms in MEALPY library, identifying similarities in search behavior.", "conclusion": "Empirical analysis highlights algorithms with similar behaviors, contributing tools to better distinguish innovation in optimization algorithms."}}
{"id": "2507.01376", "pdf": "https://arxiv.org/pdf/2507.01376", "abs": "https://arxiv.org/abs/2507.01376", "authors": ["Yinwang Ren", "Yangyang Liu", "Tang Ji", "Xun Xu"], "title": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing", "categories": ["cs.AI"], "comment": "Submitted to JMS(March 2025)", "summary": "AI agents are autonomous systems designed to perceive, reason, and act within\ndynamic environments. With the rapid advancements in generative AI (GenAI),\nlarge language models (LLMs) and multimodal large language models (MLLMs) have\nsignificantly improved AI agents' capabilities in semantic comprehension,\ncomplex reasoning, and autonomous decision-making. At the same time, the rise\nof Agentic AI highlights adaptability and goal-directed autonomy in dynamic and\ncomplex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents\n(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in\ninformation processing, environmental perception, and autonomous\ndecision-making, opening new avenues for smart manufacturing. However, the\ndefinitions, capability boundaries, and practical applications of these\nemerging AI paradigms in smart manufacturing remain unclear. To address this\ngap, this study systematically reviews the evolution of AI and AI agent\ntechnologies, examines the core concepts and technological advancements of\nLLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential\napplications in and integration into manufacturing, along with the potential\nchallenges they may face.", "AI": {"tldr": "The paper reviews advancements in LLM-Agents, MLLM-Agents, and Agentic AI for smart manufacturing while exploring their potential and challenges.", "motivation": "The study seeks to clarify unclear definitions, boundaries, and practical applications of advanced AI paradigms in smart manufacturing.", "method": "The paper systematically reviews the evolution of AI agents, focusing on the concepts and technological progress of LLM-Agents, MLLM-Agents, and Agentic AI.", "result": "The paper identifies potential applications and integration strategies for advanced AI paradigms into smart manufacturing, while addressing accompanying challenges.", "conclusion": "Emerging AI paradigms hold significant promise for innovation in smart manufacturing, though challenges in their practical adoption require attention."}}
{"id": "2507.01780", "pdf": "https://arxiv.org/pdf/2507.01780", "abs": "https://arxiv.org/abs/2507.01780", "authors": ["Eric Vin", "Kyle A. Miller", "Daniel J. Fremont"], "title": "LeanLTL: A unifying framework for linear temporal logics in Lean", "categories": ["cs.LO", "cs.PL", "F.3.1; F.4.1; F.3.3"], "comment": "9 pages, 3 figures; for associated project files see\n  https://github.com/UCSCFormalMethods/LeanLTL; to be published in LIPIcs for\n  ITP '25", "summary": "We propose LeanLTL, a unifying framework for linear temporal logics in Lean\n4. LeanLTL supports reasoning about traces that represent either infinite or\nfinite linear time. The library allows traditional LTL syntax to be combined\nwith arbitrary Lean expressions, making it straightforward to define properties\ninvolving numerical or other types. We prove that standard flavors of LTL can\nbe embedded in our framework. The library also provides automation for\nreasoning about LeanLTL formulas in a way that facilitates using Lean's\nexisting tactics. Finally, we provide examples illustrating the utility of the\nlibrary in reasoning about systems that come from applications.", "AI": {"tldr": "LeanLTL is a Lean 4 framework for reasoning about linear temporal logics (LTL) over finite and infinite traces, combining LTL syntax with Lean expressions.", "motivation": "To create a flexible and unified framework for reasoning about linear temporal logics (LTL), incorporating both finite and infinite linear time traces and arbitrary Lean expressions.", "method": "The framework embeds standard flavors of LTL into the Lean environment and combines LTL syntax with arbitrary Lean expressions. It also provides automation and integrates with Lean's existing tactics.", "result": "LeanLTL supports reasoning about system properties using LTL syntax and Lean expressions, and the authors demonstrate its utility with examples.", "conclusion": "LeanLTL serves as a versatile framework for integrating LTL with Lean expressions, making it useful for reasoning about various types of systems and properties."}}
{"id": "2507.01315", "pdf": "https://arxiv.org/pdf/2507.01315", "abs": "https://arxiv.org/abs/2507.01315", "authors": ["Taiming Wang", "Yanjie Jiang", "Chunhao Dong", "Yuxia Zhang", "Hui Liu"], "title": "Context-Aware Code Wiring Recommendation with LLM-based Agent", "categories": ["cs.SE"], "comment": null, "summary": "Copy-paste-modify is a widespread and pragmatic practice in software\ndevelopment, where developers adapt reused code snippets, sourced from\nplatforms such as Stack Overflow, GitHub, or LLM outputs, into their local\ncodebase. A critical yet underexplored aspect of this adaptation is code\nwiring, which involves substituting unresolved variables in the pasted code\nwith suitable ones from the surrounding context. Existing solutions either rely\non heuristic rules or historical templates, often failing to effectively\nutilize contextual information, despite studies showing that over half of\nadaptation cases are context-dependent. In this paper, we introduce WIRL, an\nLLM-based agent for code wiring framed as a Retrieval-Augmented Generation\n(RAG) infilling task. WIRL combines an LLM, a customized toolkit, and an\norchestration module to identify unresolved variables, retrieve context, and\nperform context-aware substitutions. To balance efficiency and autonomy, the\nagent adopts a mixed strategy: deterministic rule-based steps for common\npatterns, and a state-machine-guided decision process for intelligent\nexploration. We evaluate WIRL on a carefully curated, high-quality dataset\nconsisting of real-world code adaptation scenarios. Our approach achieves an\nexact match precision of 91.7% and a recall of 90.0%, outperforming advanced\nLLMs by 22.6 and 13.7 percentage points in precision and recall, respectively,\nand surpassing IntelliJ IDEA by 54.3 and 49.9 percentage points. These results\nunderscore its practical utility, particularly in contexts with complex\nvariable dependencies or multiple unresolved variables. We believe WIRL paves\nthe way for more intelligent and context-aware developer assistance in modern\nIDEs.", "AI": {"tldr": "WIRL is an LLM-based tool designed for improving code adaptation by effectively resolving unresolved variables in pasted code using a Retrieval-Augmented Generation (RAG) infilling method. It has demonstrated significant improvements over existing tools.", "motivation": "Modern development frequently involves copy-pasting code from external sources, which often requires adjustments to integrate with existing variables in the local codebase. Current methods fail to leverage contextual information effectively, leading to limited performance in such adaptations.", "method": "The authors introduce WIRL, a solution combining an LLM, a custom toolkit, and an orchestration module to identify and resolve unresolved variables. It uses a mixed strategy: deterministic rules for common issues and an intelligent decision framework for more complex contexts, under a Retrieval-Augmented Generation (RAG) structure.", "result": "WIRL achieves high performance in code adaptation scenarios, with a precision of 91.7% and recall of 90.0%. It outperforms advanced LLMs by 22.6 and 13.7 percentage points in precision and recall, respectively, and IntelliJ IDEA by 54.3 and 49.9 points.", "conclusion": "WIRL effectively handles complex and context-dependent code adaptations, showing significant performance gains over both state-of-the-art models and widely-used IDEs. Its development indicates progress toward more intelligent and context-aware developer tools."}}
{"id": "2507.01170", "pdf": "https://arxiv.org/pdf/2507.01170", "abs": "https://arxiv.org/abs/2507.01170", "authors": ["Simon B\u00f6rjesson", "Erik Ersmark", "Pierre Nugues"], "title": "Matching and Linking Entries in Historical Swedish Encyclopedias", "categories": ["cs.CL"], "comment": "10 pages, 3 figures", "summary": "The \\textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and\n20th centuries. It was written by a team of experts and aimed to be an\nintellectual reference, stressing precision and accuracy. This encyclopedia had\nfour main editions remarkable by their size, ranging from 20 to 38 volumes. As\na consequence, the \\textit{Nordisk familjebok} had a considerable influence in\nuniversities, schools, the media, and society overall. As new editions were\nreleased, the selection of entries and their content evolved, reflecting\nintellectual changes in Sweden.\n  In this paper, we used digitized versions from \\textit{Project Runeberg}. We\nfirst resegmented the raw text into entries and matched pairs of entries\nbetween the first and second editions using semantic sentence embeddings. We\nthen extracted the geographical entries from both editions using a\ntransformer-based classifier and linked them to Wikidata. This enabled us to\nidentify geographic trends and possible shifts between the first and second\neditions, written between 1876-1899 and 1904-1926, respectively.\n  Interpreting the results, we observe a small but significant shift in\ngeographic focus away from Europe and towards North America, Africa, Asia,\nAustralia, and northern Scandinavia from the first to the second edition,\nconfirming the influence of the First World War and the rise of new powers. The\ncode and data are available on GitHub at\nhttps://github.com/sibbo/nordisk-familjebok.", "AI": {"tldr": "This study analyzed shifts in geographic entries between the first and second editions of the Swedish encyclopedia 'Nordisk familjebok' using semantic embeddings and transformer-based classifiers.", "motivation": "To investigate how intellectual trends and geographical focus shifted in Sweden's 'Nordisk familjebok' encyclopedia across its two editions.", "method": "The study used digitized texts resegmented into entries and paired them using semantic sentence embeddings. Geographic entries were classified using a transformer-based model and linked to Wikidata for trend analysis.", "result": "A shift in geographic focus from Europe towards regions like North America, Africa, Asia, Australia, and northern Scandinavia was observed between the first (1876-1899) and second editions (1904-1926).", "conclusion": "The findings reflect intellectual evolution in the encyclopedia due to historical events like WWI and the emergence of new global powers, demonstrating an expanded worldview in the later edition."}}
{"id": "2507.01098", "pdf": "https://arxiv.org/pdf/2507.01098", "abs": "https://arxiv.org/abs/2507.01098", "authors": ["Liu Ziyin", "Isaac Chuang"], "title": "Proof of a perfect platonic representation hypothesis", "categories": ["cs.LG", "cond-mat.dis-nn", "q-bio.NC", "stat.ML"], "comment": null, "summary": "In this note, we elaborate on and explain in detail the proof given by Ziyin\net al. (2025) of the \"perfect\" Platonic Representation Hypothesis (PRH) for the\nembedded deep linear network model (EDLN). We show that if trained with SGD,\ntwo EDLNs with different widths and depths and trained on different data will\nbecome Perfectly Platonic, meaning that every possible pair of layers will\nlearn the same representation up to a rotation. Because most of the global\nminima of the loss function are not Platonic, that SGD only finds the perfectly\nPlatonic solution is rather extraordinary. The proof also suggests at least six\nways the PRH can be broken. We also show that in the EDLN model, the emergence\nof the Platonic representations is due to the same reason as the emergence of\nprogressive sharpening. This implies that these two seemingly unrelated\nphenomena in deep learning can, surprisingly, have a common cause. Overall, the\ntheory and proof highlight the importance of understanding emergent \"entropic\nforces\" due to the irreversibility of SGD training and their role in\nrepresentation learning. The goal of this note is to be instructive and avoid\nlengthy technical details.", "AI": {"tldr": "The paper elaborates on the proof of the Perfect Platonic Representation Hypothesis for embedded deep linear networks (EDLN), showing that SGD training leads to all layers learning identical representations up to rotation, while most global minima do not exhibit this characteristic.", "motivation": "The motivation is to understand why and how stochastic gradient descent (SGD) results in the emergence of identical representations across layers in deep linear networks, a seemingly rare behavior.", "method": "The authors revisit and explain the proof provided by Ziyin et al. (2025) for the Platonic Representation Hypothesis, focusing on embedded deep linear network models and analyzing SGD dynamics.", "result": "They demonstrated that SGD consistently leads to perfectly Platonic representations, despite these being exceptional minima, and identified six conditions under which the hypothesis could break.", "conclusion": "The study reveals a connection between Platonic representations and progressive sharpening phenomena, emphasizing the critical role of entropic forces induced by SGD in representation learning and suggesting a unifying cause for these observations."}}
{"id": "2507.01613", "pdf": "https://arxiv.org/pdf/2507.01613", "abs": "https://arxiv.org/abs/2507.01613", "authors": ["Shirong Xu", "Jingnan Zhang", "Junhui Wang"], "title": "When Less Is More: Binary Feedback Can Outperform Ordinal Comparisons in Ranking Recovery", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Paired comparison data, where users evaluate items in pairs, play a central\nrole in ranking and preference learning tasks. While ordinal comparison data\nintuitively offer richer information than binary comparisons, this paper\nchallenges that conventional wisdom. We propose a general parametric framework\nfor modeling ordinal paired comparisons without ties. The model adopts a\ngeneralized additive structure, featuring a link function that quantifies the\npreference difference between two items and a pattern function that governs the\ndistribution over ordinal response levels. This framework encompasses classical\nbinary comparison models as special cases, by treating binary responses as\nbinarized versions of ordinal data. Within this framework, we show that\nbinarizing ordinal data can significantly improve the accuracy of ranking\nrecovery. Specifically, we prove that under the counting algorithm, the ranking\nerror associated with binary comparisons exhibits a faster exponential\nconvergence rate than that of ordinal data. Furthermore, we characterize a\nsubstantial performance gap between binary and ordinal data in terms of a\nsignal-to-noise ratio (SNR) determined by the pattern function. We identify the\npattern function that minimizes the SNR and maximizes the benefit of\nbinarization. Extensive simulations and a real application on the MovieLens\ndataset further corroborate our theoretical findings.", "AI": {"tldr": "This paper proposes a framework where binarizing ordinal paired comparison data improves ranking recovery accuracy and demonstrates this with theoretical proofs and simulations.", "motivation": "To challenge the conventional belief that ordinal paired comparison data inherently provides richer information than binary data.", "method": "A general parametric framework was developed with a generalized additive structure, including a link function for preference differences and a pattern function dictating ordinal response levels.", "result": "Binarizing ordinal data leads to faster exponential convergence rates in ranking error compared to ordinal data, supported by SNR analysis, simulations, and MovieLens dataset application.", "conclusion": "Binary comparison data can be more effective than ordinal data in ranking recovery tasks, particularly when coupled with the optimal pattern function."}}
{"id": "2507.01163", "pdf": "https://arxiv.org/pdf/2507.01163", "abs": "https://arxiv.org/abs/2507.01163", "authors": ["Al\u00e1n F. Mu\u00f1oz", "Tim Treis", "Alexandr A. Kalinin", "Shatavisha Dasgupta", "Fabian Theis", "Anne E. Carpenter", "Shantanu Singh"], "title": "cp_measure: API-first feature extraction for image-based profiling workflows", "categories": ["cs.CV", "q-bio.CB", "q-bio.QM", "I.4.7"], "comment": "10 pages, 4 figures, 4 supplementary figures. CODEML Workshop paper\n  accepted (non-archival), as a part of ICML2025 events", "summary": "Biological image analysis has traditionally focused on measuring specific\nvisual properties of interest for cells or other entities. A complementary\nparadigm gaining increasing traction is image-based profiling - quantifying\nmany distinct visual features to form comprehensive profiles which may reveal\nhidden patterns in cellular states, drug responses, and disease mechanisms.\nWhile current tools like CellProfiler can generate these feature sets, they\npose significant barriers to automated and reproducible analyses, hindering\nmachine learning workflows. Here we introduce cp_measure, a Python library that\nextracts CellProfiler's core measurement capabilities into a modular, API-first\ntool designed for programmatic feature extraction. We demonstrate that\ncp_measure features retain high fidelity with CellProfiler features while\nenabling seamless integration with the scientific Python ecosystem. Through\napplications to 3D astrocyte imaging and spatial transcriptomics, we showcase\nhow cp_measure enables reproducible, automated image-based profiling pipelines\nthat scale effectively for machine learning applications in computational\nbiology.", "AI": {"tldr": "The paper introduces cp_measure, a Python library for streamlined and automated image-based profiling, addressing limitations of traditional tools like CellProfiler while maintaining high feature fidelity.", "motivation": "Biological image analysis requires tools that support comprehensive, automated, and reproducible image-based profiling pipelines suitable for machine learning workflows, as current tools like CellProfiler have limitations.", "method": "They developed cp_measure as a modular, API-first Python library that retains the core features of CellProfiler but integrates seamlessly with the scientific Python ecosystem for programmatic feature extraction.", "result": "cp_measure successfully retained high fidelity with CellProfiler features and showcased applications in 3D astrocyte imaging and spatial transcriptomics.", "conclusion": "cp_measure proves to be a reproducible, automated, and scalable tool, facilitating image-based profiling for computational biology and machine learning advancements."}}
{"id": "2507.01028", "pdf": "https://arxiv.org/pdf/2507.01028", "abs": "https://arxiv.org/abs/2507.01028", "authors": ["Jean Ponce", "Martial Hebert", "Basile Terver"], "title": "Dual Perspectives on Non-Contrastive Self-Supervised Learning", "categories": ["cs.LG"], "comment": null, "summary": "The objective of non-contrastive approaches to self-supervised learning is to\ntrain on pairs of different views of the data an encoder and a predictor that\nminimize the mean discrepancy between the code predicted from the embedding of\nthe first view and the embedding of the second one. In this setting, the stop\ngradient and exponential moving average iterative procedures are commonly used\nto avoid representation collapse, with excellent performance in downstream\nsupervised applications. This presentation investigates these procedures from\nthe dual theoretical viewpoints of optimization and dynamical systems. We first\nshow that, in general, although they do not optimize the original objective, or\nfor that matter, any other smooth function, they do avoid collapse. Following\nTian et al. [2021], but without any of the extra assumptions used in their\nproofs, we then show using a dynamical system perspective that, in the linear\ncase, minimizing the original objective function without the use of a stop\ngradient or exponential moving average always leads to collapse. Conversely, we\nfinally show that the limit points of the dynamical systems associated with\nthese two procedures are, in general, asymptotically stable equilibria, with no\nrisk of degenerating to trivial solutions.", "AI": {"tldr": "The paper analyzes two common procedures in non-contrastive self-supervised learning (stop gradient and exponential moving average) for avoiding representation collapse, proving their theoretical stability and effectiveness.", "motivation": "Non-contrastive approaches to self-supervised learning face challenges such as representation collapse, which undermines performance. The motivation is to investigate the theoretical dynamics of commonly used procedures to understand their efficacy and stability.", "method": "The authors employ dual theoretical perspectives\u2014optimization and dynamical systems\u2014to analyze stop gradient and exponential moving average procedures for avoiding collapse. They use proofs in linear cases without additional assumptions to validate their claims.", "result": "The study demonstrates that these procedures avoid representation collapse despite not optimizing the original objective or any smooth function. It also shows limit points of the associated dynamical systems as asymptotically stable equilibria.", "conclusion": "Stop gradient and exponential moving average are effective in avoiding collapse in non-contrastive self-supervised learning. Their stability, proven with minimal assumptions, justifies their use in practical scenarios."}}
{"id": "2507.01143", "pdf": "https://arxiv.org/pdf/2507.01143", "abs": "https://arxiv.org/abs/2507.01143", "authors": ["Reza Jalayer", "Masoud Jalayer", "Amirali Baniasadi"], "title": "A Review on Sound Source Localization in Robotics: Focusing on Deep Learning Methods", "categories": ["cs.RO", "cs.LG", "cs.SD", "eess.AS"], "comment": "35 pages", "summary": "Sound source localization (SSL) adds a spatial dimension to auditory\nperception, allowing a system to pinpoint the origin of speech, machinery\nnoise, warning tones, or other acoustic events, capabilities that facilitate\nrobot navigation, human-machine dialogue, and condition monitoring. While\nexisting surveys provide valuable historical context, they typically address\ngeneral audio applications and do not fully account for robotic constraints or\nthe latest advancements in deep learning. This review addresses these gaps by\noffering a robotics-focused synthesis, emphasizing recent progress in deep\nlearning methodologies. We start by reviewing classical methods such as Time\nDifference of Arrival (TDOA), beamforming, Steered-Response Power (SRP), and\nsubspace analysis. Subsequently, we delve into modern machine learning (ML) and\ndeep learning (DL) approaches, discussing traditional ML and neural networks\n(NNs), convolutional neural networks (CNNs), convolutional recurrent neural\nnetworks (CRNNs), and emerging attention-based architectures. The data and\ntraining strategy that are the two cornerstones of DL-based SSL are explored.\nStudies are further categorized by robot types and application domains to\nfacilitate researchers in identifying relevant work for their specific\ncontexts. Finally, we highlight the current challenges in SSL works in general,\nregarding environmental robustness, sound source multiplicity, and specific\nimplementation constraints in robotics, as well as data and learning strategies\nin DL-based SSL. Also, we sketch promising directions to offer an actionable\nroadmap toward robust, adaptable, efficient, and explainable DL-based SSL for\nnext-generation robots.", "AI": {"tldr": "This paper reviews sound source localization (SSL) techniques, focusing on deep learning (DL) advancements for robotics.", "motivation": "The authors aim to address gaps in existing surveys by focusing on robotic-specific SSL constraints and incorporating the latest deep learning advancements.", "method": "The paper reviews classical SSL methods (e.g., TDOA, beamforming) and modern deep learning approaches (CNNs, CRNNs, attention-based architectures). It also categorizes studies by robot type and application domain.", "result": "The review discusses key challenges (e.g., robustness, sound multiplicity, and implementation constraints) and identifies trends and strategies for SSL in robotics.", "conclusion": "The paper provides a roadmap for achieving robust, efficient, and explainable DL-based SSL for next-generation robots."}}
{"id": "2507.01040", "pdf": "https://arxiv.org/pdf/2507.01040", "abs": "https://arxiv.org/abs/2507.01040", "authors": ["Tianxiang Xia", "Max Neuwinger", "Lin Xiao"], "title": "Fast Clifford Neural Layers", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.PF"], "comment": "7 pages content-wise", "summary": "Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra\ninto neural networks. In this project we focus on optimizing the inference of\n2/3D Clifford convolutional layers and multivector activation layers for one\ncore CPU performance.\n  Overall, by testing on a real network block involving Clifford convolutional\nlayers and multivector activation layers, we observe that our implementation is\n30% faster than standard PyTorch implementation in relatively large data +\nnetwork size (>L2 cache).\n  We open source our code base at\nhttps://github.com/egretwAlker/c-opt-clifford-layers", "AI": {"tldr": "The paper proposes optimized Clifford convolutional and activation layers for neural networks applied to PDE modeling, achieving 30% faster CPU inference.", "motivation": "To improve the efficiency of neural networks that model PDEs by leveraging Clifford Algebra and optimizing implementation for single-core CPUs.", "method": "The authors optimized Clifford convolutional layers and multivector activation layers, focusing on enhancing performance for larger data and networks exceeding L2 cache limits.", "result": "Their implementation demonstrated a 30% speed improvement over standard PyTorch for the targeted use case.", "conclusion": "The optimized Clifford layers provide significant performance gains. The implementation is open-sourced for broader use and testing."}}
{"id": "2507.01410", "pdf": "https://arxiv.org/pdf/2507.01410", "abs": "https://arxiv.org/abs/2507.01410", "authors": ["Abeer Dyoub", "Francesca A. Lisi"], "title": "A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models", "categories": ["cs.AI"], "comment": null, "summary": "The ontological and epistemic complexities inherent in the moral domain make\nit challenging to establish clear standards for evaluating the performance of a\nmoral machine. In this paper, we present a formal method to describe Ethical\nDecision Making models based on ethical risk assessment. Then, we show how\nthese models that are specified as fuzzy rules can be verified and validated\nusing fuzzy Petri nets. A case study from the medical field is considered to\nillustrate the proposed approach.", "AI": {"tldr": "The paper proposes a formal method for ethical decision-making evaluation using fuzzy logic and applies it to a medical case study.", "motivation": "Establishing clear standards for evaluating moral machines is challenging due to the complexities of the moral domain.", "method": "Develops ethical decision-making models described through fuzzy rules, verified and validated with fuzzy Petri nets.", "result": "Applied the method to a medical case study to demonstrate its feasibility.", "conclusion": "The approach provides a structured way to evaluate ethical decision-making models in complex domains."}}
{"id": "2507.01477", "pdf": "https://arxiv.org/pdf/2507.01477", "abs": "https://arxiv.org/abs/2507.01477", "authors": ["Lukas Krodinger", "Stephan Lukasczyk", "Gordon Fraser"], "title": "Combining Type Inference and Automated Unit Test Generation for Python", "categories": ["cs.SE"], "comment": null, "summary": "Automated unit test generation is an established research field that has so\nfar focused on statically-typed programming languages. The lack of type\ninformation in dynamically-typed programming languages, such as Python,\ninhibits test generators, which heavily rely on information about parameter and\nreturn types of functions to select suitable arguments when constructing test\ncases. Since automated test generators inherently rely on frequent execution of\ncandidate tests, we make use of these frequent executions to address this\nproblem by introducing type tracing, which extracts type-related information\nduring execution and gradually refines the available type information. We\nimplement type tracing as an extension of the Pynguin test-generation framework\nfor Python, allowing it (i) to infer parameter types by observing how\nparameters are used during runtime, (ii) to record the types of values that\nfunction calls return, and (iii) to use this type information to increase code\ncoverage. The approach leads to up to 90.0% more branch coverage, improved\nmutation scores, and to type information of similar quality to that produced by\nother state-of-the-art type-inference tools.", "AI": {"tldr": "This paper introduces \"type tracing\" to address the lack of type information in dynamically-typed Python during automated test generation. The approach improves branch coverage and mutation scores in test cases.", "motivation": "The paper is motivated by the challenge of generating automated unit tests for dynamically-typed languages like Python, where the absence of type information hinders test generator effectiveness.", "method": "The paper proposes type tracing, which dynamically records type information during runtime execution of candidate tests to iteratively refine type information. It is implemented as an extension of the Pynguin framework.", "result": "The type tracing method leads to up to 90% improvement in branch coverage, better mutation scores, and type information comparable to existing state-of-the-art type-inference tools.", "conclusion": "Type tracing effectively addresses limitations of test generation in dynamically-typed languages, enhancing test quality and contributing valuable type data for Python."}}
{"id": "2507.01213", "pdf": "https://arxiv.org/pdf/2507.01213", "abs": "https://arxiv.org/abs/2507.01213", "authors": ["Adamu Lawan", "Juhua Pu", "Haruna Yunusa", "Jawad Muhammad", "Muhammad Lawan"], "title": "MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis", "categories": ["cs.CL"], "comment": "6, 1 figure", "summary": "Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language\nProcessing (NLP) task that extracts aspects from text and determines their\nassociated sentiments, enabling fine-grained analysis of user opinions.\nExisting ABSA methods struggle to balance computational efficiency with high\nperformance: deep learning models often lack global context, transformers\ndemand significant computational resources, and Mamba-based approaches face\nCUDA dependency and diminished local correlations. Recent advancements in\nExtended Long Short-Term Memory (xLSTM) models, particularly their efficient\nmodeling of long-range dependencies, have significantly advanced the NLP\ncommunity. However, their potential in ABSA remains untapped. To this end, we\npropose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework\nintegrating a bi-directional mLSTM architecture with forward and partially\nflipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context\nmodeling by processing the initial sequence segment in reverse with dedicated\nparameters, preserving critical short-range patterns. We further introduce an\nmLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that\ndynamically combines forward mLSTM outputs as query and key with PF-mLSTM\noutputs as value, optimizing short-range dependency capture while maintaining\nglobal context and efficiency. Experimental results on three benchmark datasets\ndemonstrate that MEGA outperforms state-of-the-art baselines, achieving\nsuperior accuracy and efficiency in ABSA tasks.", "AI": {"tldr": "The paper proposes an advanced xLSTM model called MEGA to improve Aspect-based Sentiment Analysis (ABSA), achieving superior accuracy and efficiency.", "motivation": "Existing ABSA methods face challenges in balancing computational efficiency and high performance due to issues in global context modeling, resource demands, and local correlation handling.", "method": "The proposed MEGA framework integrates bi-directional xLSTM architecture using forward and partially flipped backward (PF-mLSTM) streams. It introduces MECGAF for optimized fusion of outputs, capturing short-range dependencies while preserving global context.", "result": "MEGA demonstrates superior performance over state-of-the-art ABSA baselines in terms of both accuracy and computational efficiency on three benchmark datasets.", "conclusion": "The MEGA framework effectively addresses known limitations in ABSA methods by improving localized and global context modeling, offering a scalable and high-performing solution."}}
{"id": "2507.01651", "pdf": "https://arxiv.org/pdf/2507.01651", "abs": "https://arxiv.org/abs/2507.01651", "authors": ["Sylvain Fontaine"], "title": "A Dynamical Cartography of the Epistemic Diffusion of Artificial Intelligence in Neuroscience", "categories": ["cs.DL", "physics.soc-ph", "q-bio.NC"], "comment": null, "summary": "Neuroscience and AI have an intertwined history, largely relayed in the\nliterature of both fields. In recent years, due to the engineering orientations\nof AI research and the monopoly of industry for its large-scale applications,\nthe mutual expansion of neuroscience and AI in fundamental research seems\nchallenged. In this paper, we bring some empirical evidences that, on the\ncontrary, AI and neuroscience are continuing to grow together, but with a\npronounced interest in the fields of study related to neurodegenerative\ndiseases since the 1990s. With a temporal knowledge cartography of neuroscience\ndrawn with advanced document embedding techniques, we draw the dynamical\nshaping of the discipline since the 1970s and identified the conceptual\narticulation of AI with this particular subfield mentioned before. However, a\nfurther analysis of the underlying citation network of the studied corpus shows\nthat the produced AI technologies remain confined in the different subfields\nand are not transferred from one subfield to another. This invites us to\ndiscuss the genericity capability of AI in the context of an intradisciplinary\ndevelopment, especially in the diffusion of its associated metrology.", "AI": {"tldr": "The paper explores how AI and neuroscience have evolved together, particularly in neurodegenerative disease research, using document embedding techniques. However, AI technologies often remain confined within specific subfields.", "motivation": "To investigate if AI and neuroscience are still growing together in fundamental research despite AI's engineering focus and industry dominance.", "method": "Temporal knowledge cartography of neuroscience with document embedding techniques and the analysis of citation networks.", "result": "AI and neuroscience continue to expand together, especially in neurodegenerative disease research; however, AI's tools often stay limited within specific subfields.", "conclusion": "The study suggests discussing AI's genericity and the need for better diffusion of its metrology across subfields in neuroscience."}}
{"id": "2507.01687", "pdf": "https://arxiv.org/pdf/2507.01687", "abs": "https://arxiv.org/abs/2507.01687", "authors": ["Georgios Arampatzis", "Stylianos Katsarakis", "Charalambos Makridakis"], "title": "A generative modeling / Physics-Informed Neural Network approach to random differential equations", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "The integration of Scientific Machine Learning (SciML) techniques with\nuncertainty quantification (UQ) represents a rapidly evolving frontier in\ncomputational science. This work advances Physics-Informed Neural Networks\n(PINNs) by incorporating probabilistic frameworks to effectively model\nuncertainty in complex systems. Our approach enhances the representation of\nuncertainty in forward problems by combining generative modeling techniques\nwith PINNs. This integration enables in a systematic fashion uncertainty\ncontrol while maintaining the predictive accuracy of the model. We demonstrate\nthe utility of this method through applications to random differential\nequations and random partial differential equations (PDEs).", "AI": {"tldr": "The paper enhances Physics-Informed Neural Networks (PINNs) by integrating probabilistic frameworks to model uncertainty in complex systems.", "motivation": "To address the need for effective modeling and control of uncertainty in computational science, particularly for random differential and partial differential equations.", "method": "By combining generative modeling techniques with PINNs to systematically control uncertainty while preserving model accuracy.", "result": "The approach effectively models uncertainty in forward problems and demonstrates utility in random differential and partial differential equations.", "conclusion": "The integration of probabilistic frameworks with PINNs provides a systematic method for uncertainty quantification while maintaining predictive accuracy."}}
{"id": "2507.01182", "pdf": "https://arxiv.org/pdf/2507.01182", "abs": "https://arxiv.org/abs/2507.01182", "authors": ["Zhuo Su", "Li Liu", "Matthias M\u00fcller", "Jiehua Zhang", "Diana Wofk", "Ming-Ming Cheng", "Matti Pietik\u00e4inen"], "title": "Rapid Salient Object Detection with Difference Convolutional Neural Networks", "categories": ["cs.CV"], "comment": "16 pages, accepted in TPAMI", "summary": "This paper addresses the challenge of deploying salient object detection\n(SOD) on resource-constrained devices with real-time performance. While recent\nadvances in deep neural networks have improved SOD, existing top-leading models\nare computationally expensive. We propose an efficient network design that\ncombines traditional wisdom on SOD and the representation power of modern CNNs.\nLike biologically-inspired classical SOD methods relying on computing contrast\ncues to determine saliency of image regions, our model leverages Pixel\nDifference Convolutions (PDCs) to encode the feature contrasts. Differently,\nPDCs are incorporated in a CNN architecture so that the valuable contrast cues\nare extracted from rich feature maps. For efficiency, we introduce a difference\nconvolution reparameterization (DCR) strategy that embeds PDCs into standard\nconvolutions, eliminating computation and parameters at inference.\nAdditionally, we introduce SpatioTemporal Difference Convolution (STDC) for\nvideo SOD, enhancing the standard 3D convolution with spatiotemporal contrast\ncapture. Our models, SDNet for image SOD and STDNet for video SOD, achieve\nsignificant improvements in efficiency-accuracy trade-offs. On a Jetson Orin\ndevice, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on\nstreamed images and videos, surpassing the second-best lightweight models in\nour experiments by more than $2\\times$ and $3\\times$ in speed with superior\naccuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.", "AI": {"tldr": "The paper proposes efficient neural networks for real-time salient object detection (SOD) on resource-constrained devices using Pixel Difference Convolutions (PDCs) and SpatioTemporal Difference Convolutions (STDC).", "motivation": "The motivation is to address the computational inefficiency of existing top-performing SOD models, making them viable for resource-constrained devices while maintaining real-time performance.", "method": "The authors design an efficient model integrating Pixel Difference Convolutions (PDCs) for contrast encoding and Difference Convolution Reparameterization (DCR) for reducing computation at inference. They further develop SpatioTemporal Difference Convolutions (STDC) to enhance 3D convolutions for video SOD.", "result": "The proposed models, SDNet for image SOD and STDNet for video SOD, achieve significant improvements in efficiency and accuracy trade-offs, running at 46 FPS and 150 FPS respectively, with less than 1M parameters, outperforming lightweight alternatives by margins of 2\u00d7 to 3\u00d7 in speed.", "conclusion": "This research demonstrates that leveraging biologically-inspired saliency detection techniques combined with CNNs can result in models that are both computationally efficient and accurate for real-time deployment on resource-constrained devices."}}
{"id": "2507.01029", "pdf": "https://arxiv.org/pdf/2507.01029", "abs": "https://arxiv.org/abs/2507.01029", "authors": ["Junjie Zhou", "Yingli Zuo", "Shichang Feng", "Peng Wan", "Qi Zhu", "Daoqiang Zhang", "Wei Shao"], "title": "PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "With the development of generative artificial intelligence and instruction\ntuning techniques, multimodal large language models (MLLMs) have made\nimpressive progress on general reasoning tasks. Benefiting from the\nchain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning\nproblem step-by-step. However, existing MLLMs still face significant challenges\nwhen applied to pathology visual reasoning tasks: (1) LLMs often underperforms\nbecause they lack domain-specific information, which can lead to model\nhallucinations. (2) The additional reasoning steps in CoT may introduce errors,\nleading to the divergence of answers. To address these limitations, we propose\nPathCoT, a novel zero-shot CoT prompting method which integrates the pathology\nexpert-knowledge into the reasoning process of MLLMs and incorporates\nself-evaluation to mitigate divergence of answers. Specifically, PathCoT guides\nthe MLLM with prior knowledge to perform as pathology experts, and provides\ncomprehensive analysis of the image with their domain-specific knowledge. By\nincorporating the experts' knowledge, PathCoT can obtain the answers with CoT\nreasoning. Furthermore, PathCoT incorporates a self-evaluation step that\nassesses both the results generated directly by MLLMs and those derived through\nCoT, finally determining the reliable answer. The experimental results on the\nPathMMU dataset demonstrate the effectiveness of our method on pathology visual\nunderstanding and reasoning.", "AI": {"tldr": "The paper presents PathCoT, a zero-shot CoT prompting method that improves reasoning in pathology visual tasks by integrating expert knowledge and self-evaluation.", "motivation": "The authors aim to address challenges in applying multimodal large language models (MLLMs) to pathology visual reasoning tasks, including a lack of domain-specific knowledge and errors introduced by CoT reasoning.", "method": "PathCoT integrates pathology expert knowledge into MLLMs' reasoning processes and incorporates self-evaluation mechanisms to assess and refine answers generated through CoT and direct reasoning.", "result": "Experimental results on the PathMMU dataset show PathCoT's effectiveness in improving pathology visual understanding and reasoning, demonstrating its ability to provide reliable answers.", "conclusion": "PathCoT successfully enhances MLLMs' performance on pathology visual reasoning tasks by combining expert knowledge integration and self-evaluation steps, making it a promising zero-shot approach."}}
{"id": "2507.01152", "pdf": "https://arxiv.org/pdf/2507.01152", "abs": "https://arxiv.org/abs/2507.01152", "authors": ["Yunke Ao", "Masoud Moghani", "Mayank Mittal", "Manish Prajapat", "Luohong Wu", "Frederic Giraud", "Fabio Carrillo", "Andreas Krause", "Philipp F\u00fcrnstahl"], "title": "SonoGym: High Performance Simulation for Challenging Surgical Tasks with Robotic Ultrasound", "categories": ["cs.RO"], "comment": "21 pages, 15 figures", "summary": "Ultrasound (US) is a widely used medical imaging modality due to its\nreal-time capabilities, non-invasive nature, and cost-effectiveness. Robotic\nultrasound can further enhance its utility by reducing operator dependence and\nimproving access to complex anatomical regions. For this, while deep\nreinforcement learning (DRL) and imitation learning (IL) have shown potential\nfor autonomous navigation, their use in complex surgical tasks such as anatomy\nreconstruction and surgical guidance remains limited -- largely due to the lack\nof realistic and efficient simulation environments tailored to these tasks. We\nintroduce SonoGym, a scalable simulation platform for complex robotic\nultrasound tasks that enables parallel simulation across tens to hundreds of\nenvironments. Our framework supports realistic and real-time simulation of US\ndata from CT-derived 3D models of the anatomy through both a physics-based and\na generative modeling approach. Sonogym enables the training of DRL and recent\nIL agents (vision transformers and diffusion policies) for relevant tasks in\nrobotic orthopedic surgery by integrating common robotic platforms and\northopedic end effectors. We further incorporate submodular DRL -- a recent\nmethod that handles history-dependent rewards -- for anatomy reconstruction and\nsafe reinforcement learning for surgery. Our results demonstrate successful\npolicy learning across a range of scenarios, while also highlighting the\nlimitations of current methods in clinically relevant environments. We believe\nour simulation can facilitate research in robot learning approaches for such\nchallenging robotic surgery applications. Dataset, codes, and videos are\npublicly available at https://sonogym.github.io/.", "AI": {"tldr": "SonoGym is a scalable simulation platform designed to advance robotic ultrasound tasks, enabling efficient training of reinforcement and imitation learning algorithms for surgery.", "motivation": "The paper addresses the limited use of deep reinforcement learning (DRL) and imitation learning (IL) in complex surgical tasks due to the absence of realistic simulation environments.", "method": "SonoGym employs scalable simulation using CT-derived 3D models for ultrasound data and supports parallel environments, realistic US data modeling, and integration with robotic platforms.", "result": "The study demonstrates successful training of advanced DRL and IL agents for anatomy reconstruction and surgical guidance tasks, while exposing limitations in current methods.", "conclusion": "SonoGym provides a critical tool for enhancing robot learning in complex surgery applications, offering data, code, and tools publicly for further research."}}
{"id": "2507.01880", "pdf": "https://arxiv.org/pdf/2507.01880", "abs": "https://arxiv.org/abs/2507.01880", "authors": ["Stefano Schuppli", "Fawzi Mohamed", "Henrique Mendon\u00e7a", "Nina Mujkanovic", "Elia Palme", "Dino Conciatore", "Lukas Drescher", "Miguel Gila", "Pim Witlox", "Joost VandeVondele", "Maxime Martinasso", "Thomas C. Schulthess", "Torsten Hoefler"], "title": "Evolving HPC services to enable ML workloads on HPE Cray EX", "categories": ["cs.DC", "cs.LG"], "comment": "Presented at the Cray User Group 2025 (CUG'25)", "summary": "The Alps Research Infrastructure leverages GH200 technology at scale,\nfeaturing 10,752 GPUs. Accessing Alps provides a significant computational\nadvantage for researchers in Artificial Intelligence (AI) and Machine Learning\n(ML). While Alps serves a broad range of scientific communities, traditional\nHPC services alone are not sufficient to meet the dynamic needs of the ML\ncommunity. This paper presents an initial investigation into extending HPC\nservice capabilities to better support ML workloads. We identify key challenges\nand gaps we have observed since the early-access phase (2023) of Alps by the\nSwiss AI community and propose several technological enhancements. These\ninclude a user environment designed to facilitate the adoption of HPC for ML\nworkloads, balancing performance with flexibility; a utility for rapid\nperformance screening of ML applications during development; observability\ncapabilities and data products for inspecting ongoing large-scale ML workloads;\na utility to simplify the vetting of allocated nodes for compute readiness; a\nservice plane infrastructure to deploy various types of workloads, including\nsupport and inference services; and a storage infrastructure tailored to the\nspecific needs of ML workloads. These enhancements aim to facilitate the\nexecution of ML workloads on HPC systems, increase system usability and\nresilience, and better align with the needs of the ML community. We also\ndiscuss our current approach to security aspects. This paper concludes by\nplacing these proposals in the broader context of changes in the communities\nserved by HPC infrastructure like ours.", "AI": {"tldr": "The paper examines improvements in HPC services to better support machine learning (ML) workloads, using the Alps infrastructure, which features 10,752 GPUs.", "motivation": "The dynamic and specific needs of ML workloads are not adequately addressed by traditional HPC services.", "method": "The paper identifies challenges and gaps observed during early-access use of Alps by the Swiss AI community and proposes enhancements like a tailored user environment, performance screening tools, infrastructure updates, and improved storage systems.", "result": "Several technological proposals are introduced to improve usability, resiliency, and adoption of Alps for ML workloads while addressing existing gaps.", "conclusion": "The enhancements facilitate better execution of ML workloads on HPC systems and align HPC infrastructure with the evolving needs of the scientific communities it serves."}}
{"id": "2507.01052", "pdf": "https://arxiv.org/pdf/2507.01052", "abs": "https://arxiv.org/abs/2507.01052", "authors": ["Ahmed Farooq"], "title": "Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "In this study we introduce a novel energy functional for long-sequence\nmemory, building upon the framework of dense Hopfield networks which achieves\nexponential storage capacity through higher-order interactions. Building upon\nearlier work on long-sequence Hopfield memory models, we propose a temporal\nkernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient\nsequential retrieval of patterns over extended sequences. We demonstrate the\nsuccessful application of this technique for the storage and sequential\nretrieval of movies frames which are well suited for this because of the high\ndimensional vectors that make up each frame creating enough variation between\neven sequential frames in the high dimensional space. The technique has\napplications in modern transformer architectures, including efficient\nlong-sequence modeling, memory augmentation, improved attention with temporal\nbias, and enhanced handling of long-term dependencies in time-series data. Our\nmodel offers a promising approach to address the limitations of transformers in\nlong-context tasks, with potential implications for natural language\nprocessing, forecasting, and beyond.", "AI": {"tldr": "This paper introduces a new energy functional for long-sequence memory in dense Hopfield networks, proposing a temporal kernel for effective sequential retrieval, with applications in transformers and time-series tasks.", "motivation": "To improve the limitations of transformers in addressing long-context tasks, specifically in sequential data like movies, natural language processing, and forecasting.", "method": "The paper proposes a temporal kernel $K(m,k)$ within dense Hopfield networks to integrate temporal dependencies, enabling sequential retrieval of patterns in long sequences.", "result": "The method successfully stores and retrieves movie frames, demonstrating the ability to handle high-dimensional data variation and integrating within transformer architectures.", "conclusion": "The approach enhances transformers' efficiency in tasks requiring long-sequence modeling and handling of long-term dependencies, with implications across NLP, forecasting, and other domains."}}
{"id": "2507.01431", "pdf": "https://arxiv.org/pdf/2507.01431", "abs": "https://arxiv.org/abs/2507.01431", "authors": ["Yoonseok Yang", "Minjune Kim", "Marlon Rondinelli", "Keren Shao"], "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "7 pages, 5 figues, 1 table", "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions.", "AI": {"tldr": "Pensieve is an AI-assisted grading platform for evaluating handwritten, open-ended student responses, reducing grading time by 65% with 95.4% grading accuracy with instructor consistency.", "motivation": "Grading handwritten and open-ended STEM responses is a major time-consuming challenge in large university courses, necessitating automation to streamline the process.", "method": "Pensieve employs large language models (LLMs) to handle the entire grading pipeline\u2014retrieving, transcribing, and evaluating student work with an interactive human-in-the-loop setup.", "result": "Pensieve has been deployed at over 20 institutions, grading over 300,000 responses, achieving a grading accuracy agreement of 95.4% and reducing time spent on grading by 65%.", "conclusion": "Pensieve demonstrates the potential for AI to assist in grading processes, significantly improving efficiency while retaining high alignment with instructors' evaluations across STEM disciplines."}}
{"id": "2507.01628", "pdf": "https://arxiv.org/pdf/2507.01628", "abs": "https://arxiv.org/abs/2507.01628", "authors": ["Zilong He", "Pengfei Chen", "Hongyu Zhang", "Xiaoyun Li", "Guangba Yu", "Hongyang Chen", "Zibin Zheng"], "title": "DaiFu: In-Situ Crash Recovery for Deep Learning Systems", "categories": ["cs.SE"], "comment": null, "summary": "Deep learning (DL) systems have been widely adopted in many areas, and are\nbecoming even more popular with the emergence of large language models.\nHowever, due to the complex software stacks involved in their development and\nexecution, crashes are unavoidable and common. Crashes severely waste computing\nresources and hinder development productivity, so efficient crash recovery is\ncrucial. Existing solutions, such as checkpoint-retry, are too heavyweight for\nfast recovery from crashes caused by minor programming errors or transient\nruntime errors. Therefore, we present DaiFu, an in-situ recovery framework for\nDL systems. Through a lightweight code transformation to a given DL system,\nDaiFu augments it to intercept crashes in situ and enables dynamic and instant\nupdates to its program running context (e.g., code, configurations, and other\ndata) for agile crash recovery. Our evaluation shows that DaiFu helps reduce\nthe restore time for crash recovery, achieving a 1372x speedup compared with\nstate-of-the-art solutions. Meanwhile, the overhead of DaiFu is negligible\n(under 0.40%). We also construct a benchmark spanning 7 distinct crash\nscenarios in DL systems, and show the effectiveness of DaiFu in diverse\nsituations.", "AI": {"tldr": "DaiFu, an in-situ recovery framework for deep learning systems, enables fast crash recovery with negligible overhead, achieving a 1372x speedup over existing methods.", "motivation": "Deep learning systems face frequent crashes due to complex software stacks, wasting resources and hampering productivity. Effective recovery mechanisms for minor and transient errors are lacking.", "method": "DaiFu employs lightweight code transformation techniques to augment DL systems, allowing instant updates to the running context (e.g., code and configurations). It dynamically intercepts crashes for quick recovery.", "result": "DaiFu achieves a 1372x faster recovery time compared to current state-of-the-art solutions while incurring less than 0.40% overhead. Its effectiveness is demonstrated across 7 different crash scenarios.", "conclusion": "DaiFu significantly improves crash recovery efficiency in DL systems, making it a practical and effective solution with minimal performance impact."}}
{"id": "2507.01234", "pdf": "https://arxiv.org/pdf/2507.01234", "abs": "https://arxiv.org/abs/2507.01234", "authors": ["Yu Fan", "Yang Tian", "Shauli Ravfogel", "Mrinmaya Sachan", "Elliott Ash", "Alexander Hoyle"], "title": "The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure", "categories": ["cs.CL"], "comment": null, "summary": "Embedding-based similarity metrics between text sequences can be influenced\nnot just by the content dimensions we most care about, but can also be biased\nby spurious attributes like the text's source or language. These document\nconfounders cause problems for many applications, but especially those that\nneed to pool texts from different corpora. This paper shows that a debiasing\nalgorithm that removes information about observed confounders from the encoder\nrepresentations substantially reduces these biases at a minimal computational\ncost. Document similarity and clustering metrics improve across every embedding\nvariant and task we evaluate -- often dramatically. Interestingly, performance\non out-of-distribution benchmarks is not impacted, indicating that the\nembeddings are not otherwise degraded.", "AI": {"tldr": "This paper proposes a debiasing algorithm to counteract biases in embedding-based similarity metrics caused by document confounders, significantly improving the quality of similarity and clustering metrics on text sequences.", "motivation": "To address the problem of biases in text sequence similarity metrics caused by spurious attributes such as the text's source or language, which can hinder applications pooling texts from different corpora.", "method": "A debiasing algorithm is proposed to remove information about observed document confounders from encoder representations, reducing the influence of these biases.", "result": "The debiasing algorithm significantly improves document similarity and clustering metrics across multiple embedding variants and tasks, while maintaining performance on out-of-distribution benchmarks.", "conclusion": "The algorithm effectively reduces biases caused by confounders in text sequence embeddings without compromising overall performance, making it valuable for applications involving pooled corpora."}}
{"id": "2507.01946", "pdf": "https://arxiv.org/pdf/2507.01946", "abs": "https://arxiv.org/abs/2507.01946", "authors": ["Adam J. Eisen", "Mitchell Ostrow", "Sarthak Chandra", "Leo Kozachkov", "Earl K. Miller", "Ila R. Fiete"], "title": "Characterizing control between interacting subsystems with deep Jacobian estimation", "categories": ["q-bio.QM", "cs.LG", "math.DS", "q-bio.NC"], "comment": "10 pages, 6 figures", "summary": "Biological function arises through the dynamical interactions of multiple\nsubsystems, including those between brain areas, within gene regulatory\nnetworks, and more. A common approach to understanding these systems is to\nmodel the dynamics of each subsystem and characterize communication between\nthem. An alternative approach is through the lens of control theory: how the\nsubsystems control one another. This approach involves inferring the\ndirectionality, strength, and contextual modulation of control between\nsubsystems. However, methods for understanding subsystem control are typically\nlinear and cannot adequately describe the rich contextual effects enabled by\nnonlinear complex systems. To bridge this gap, we devise a data-driven\nnonlinear control-theoretic framework to characterize subsystem interactions\nvia the Jacobian of the dynamics. We address the challenge of learning\nJacobians from time-series data by proposing the JacobianODE, a deep learning\nmethod that leverages properties of the Jacobian to directly estimate it for\narbitrary dynamical systems from data alone. We show that JacobianODEs\noutperform existing Jacobian estimation methods on challenging systems,\nincluding high-dimensional chaos. Applying our approach to a multi-area\nrecurrent neural network (RNN) trained on a working memory selection task, we\nshow that the \"sensory\" area gains greater control over the \"cognitive\" area\nover learning. Furthermore, we leverage the JacobianODE to directly control the\ntrained RNN, enabling precise manipulation of its behavior. Our work lays the\nfoundation for a theoretically grounded and data-driven understanding of\ninteractions among biological subsystems.", "AI": {"tldr": "The paper proposes the JacobianODE, a nonlinear control-theoretic deep learning framework to understand subsystem interactions in complex systems, outperforming existing methods and demonstrating its viability in both analysis and control of high-dimensional dynamics.", "motivation": "The motivation is to improve upon linear methods for understanding control in dynamical subsystem interactions, which inadequately represent nonlinear complexities and contextual modulation inherent in biological systems.", "method": "The authors introduce the JacobianODE, a deep learning-based method to directly estimate Jacobians from time-series data. This method is tailored for analyzing arbitrary dynamical systems by leveraging the theoretical properties of Jacobians.", "result": "The JacobianODE outperforms existing methods in estimating Jacobians for difficult systems such as high-dimensional chaos. The method is applied to a recurrent neural network performing a working memory task, revealing insights into learning-driven control changes between neural areas.", "conclusion": "The study establishes a robust nonlinear framework for analyzing and controlling interactions in biological subsystems, paving the way for deeper insights into complex systems and precise behavioral manipulation of modeled dynamics."}}
{"id": "2507.01254", "pdf": "https://arxiv.org/pdf/2507.01254", "abs": "https://arxiv.org/abs/2507.01254", "authors": ["Runze Cheng", "Xihang Qiu", "Ming Li", "Ye Zhang", "Chun Li", "Fei Yu"], "title": "Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using H\u00f6lder Divergence and Mutual Information-Enhanced Knowledge Transfer", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal MRI provides critical complementary information for accurate brain\ntumor segmentation. However, conventional methods struggle when certain\nmodalities are missing due to issues such as image quality, protocol\ninconsistencies, patient allergies, or financial constraints. To address this,\nwe propose a robust single-modality parallel processing framework that achieves\nhigh segmentation accuracy even with incomplete modalities. Leveraging Holder\ndivergence and mutual information, our model maintains modality-specific\nfeatures while dynamically adjusting network parameters based on the available\ninputs. By using these divergence- and information-based loss functions, the\nframework effectively quantifies discrepancies between predictions and\nground-truth labels, resulting in consistently accurate segmentation. Extensive\nevaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior\nperformance over existing methods in handling missing modalities.", "AI": {"tldr": "The paper introduces a framework for accurate brain tumor segmentation using single-modality MRI, addressing the challenge of missing modalities.", "motivation": "Existing methods for brain tumor segmentation face difficulties when some MRI modalities are unavailable due to various constraints.", "method": "A single-modality parallel processing framework utilizes Holder divergence and mutual information to maintain modality-specific features and adjust parameters dynamically based on available inputs.", "result": "The framework achieves consistently accurate segmentation and outperforms existing methods when modalities are missing, validated on BraTS 2018 and BraTS 2020 datasets.", "conclusion": "The proposed framework effectively addresses the challenge of incomplete modalities, providing a robust solution for brain tumor segmentation."}}
{"id": "2507.01030", "pdf": "https://arxiv.org/pdf/2507.01030", "abs": "https://arxiv.org/abs/2507.01030", "authors": ["Reza Lotfi Navaei", "Mohammad Safarzadeh", "Seyed Mohammad Jafar Sobhani"], "title": "Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study", "categories": ["cs.LG"], "comment": "It has been submitted to ASME Journal of Heat and Mass Transfer", "summary": "In chemistry tabulations and Flamelet combustion models, the Flamelet\nGenerated Manifold (FGM) is recognized for its precision and physical\nrepresentation. The practical implementation of FGM requires a significant\nallocation of memory resources. FGM libraries are developed specifically for a\nspecific fuel and subsequently utilized for all numerical problems using\nmachine learning techniques. This research aims to develop libraries of Laminar\nFGM utilizing machine learning algorithms for application in combustion\nsimulations of methane fuel. This study employs four Machine Learning\nalgorithms to regenerate Flamelet libraries, based on an understanding of data\nsources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.\nRandom Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries\nwere identified as appropriate for constructing a database for training machine\nlearning models, giving an error rate of 2.30%. The default architectures of\neach method were evaluated to determine the optimal approach, leading to the\nselection of the MLP method as the primary choice. The method was enhanced\nthrough hyperparameter tuning to improve accuracy. The quantity of hidden\nlayers and neurons significantly influences method performance. The optimal\nmodel, comprising four hidden layers with 10, 15, 20, and 25 neurons\nrespectively, achieved an accuracy of 99.81%.", "AI": {"tldr": "This study uses machine learning techniques to regenerate Flamelet libraries for methane combustion simulations, achieving high accuracy with optimized neural networks.", "motivation": "Memory-intensive Flamelet Generated Manifold (FGM) libraries are significant for combustion modeling but need efficient alternatives for methane simulations.", "method": "Four machine learning algorithms\u2014Multi-Layer Perceptron (MLP), Random Forest, Linear Regression, and Support Vector Machine\u2014were evaluated for reconstructing FGM libraries, with hyperparameter tuning applied.", "result": "MLP with four optimized hidden layers and specific neuron counts attained an accuracy of 99.81%, outperforming others and reducing error rates to 2.30%.", "conclusion": "Machine learning-enabled regeneration of Flamelet libraries for methane simulations is resource-efficient, with MLP found as the superior method upon enhancement."}}
{"id": "2507.01181", "pdf": "https://arxiv.org/pdf/2507.01181", "abs": "https://arxiv.org/abs/2507.01181", "authors": ["Vinicius M. Gon\u00e7alves", "Shiqing Wei", "Eduardo Malacarne S. de Souza", "Krishnamurthy Prashanth", "Anthony Tzes", "Farshad Khorrami"], "title": "A Differentiable Distance Metric for Robotics Through Generalized Alternating Projection", "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In many robotics applications, it is necessary to compute not only the\ndistance between the robot and the environment, but also its derivative - for\nexample, when using control barrier functions. However, since the traditional\nEuclidean distance is not differentiable, there is a need for alternative\ndistance metrics that possess this property. Recently, a metric with guaranteed\ndifferentiability was proposed [1]. This approach has some important drawbacks,\nwhich we address in this paper. We provide much simpler and practical\nexpressions for the smooth projection for general convex polytopes.\nAdditionally, as opposed to [1], we ensure that the distance vanishes as the\nobjects overlap. We show the efficacy of the approach in experimental results.\nOur proposed distance metric is publicly available through the Python-based\nsimulation package UAIBot.", "AI": {"tldr": "This paper addresses the limitations of a recently proposed differentiable distance metric in robotics by introducing simpler expressions for general convex polytopes and ensuring the metric vanishes when objects overlap.", "motivation": "The need to compute distance and derivative in robotics applications, such as control barrier functions, and shortcomings of existing differentiable metrics.", "method": "Simplify and improve expressions for smooth projection for convex polytopes, ensuring overlap-sensitive behavior, and validate through experimental results.", "result": "Demonstrates the effectiveness of the proposed metric in experiments and makes it publicly accessible via the UAIBot Python package.", "conclusion": "The paper proposes a practical and efficient differentiable distance metric addressing limitations in prior approaches for better utility in robotics."}}
{"id": "2507.01067", "pdf": "https://arxiv.org/pdf/2507.01067", "abs": "https://arxiv.org/abs/2507.01067", "authors": ["Keun Soo Yim"], "title": "Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SY", "eess.SY"], "comment": null, "summary": "Time series forecasting models have diverse real world applications (e.g.,\nfrom electricity metrics to software workload). Latest foundational models\ntrained for time series forecasting show strengths (e.g., for long sequences\nand in zero-shot settings). However, foundational model was not yet used for\nforecasting rare, spiky events, i.e., a challenging target because those are a\ncorner case of extreme events. In this paper, we optimize a state-of-the-art\nfoundational model to forecast sporadic or spiky production outages of\nhigh-performance machine learning services powering billions of client devices.\nWe evaluate the forecasting errors of the foundational model compared with\nclassical stochastic forecasting models (e.g., moving average and\nautoregressive). The analysis helps us understand how each of the evaluated\nmodels performs for the sporadic or spiky events. For example, it identifies\nthe key patterns in the target data that are well tracked by the foundational\nmodel vs. each of the stochastic models. We use the models with optimal\nparameters to estimate a year-long outage statistics of a particular root cause\nwith less than 6% value errors.", "AI": {"tldr": "The paper investigates optimizing a foundational model to forecast rare, spiky production outages in machine learning services, comparing its performance with classical stochastic models.", "motivation": "There is a need to accurately forecast rare and challenging spiky events, such as production outages, which foundational models have yet to address effectively.", "method": "The study involved optimizing a state-of-the-art foundational model and comparing its forecasting errors with classical stochastic models using data on sporadic production outages.", "result": "The foundational model achieved favorable forecasting accuracy, identifying patterns better suited for spiky event prediction compared to traditional models. Outage statistics were estimated with less than 6% error.", "conclusion": "Optimized foundational models are promising for tackling rare, spiky event forecasting, outperforming traditional approaches in specific contexts."}}
{"id": "2507.01446", "pdf": "https://arxiv.org/pdf/2507.01446", "abs": "https://arxiv.org/abs/2507.01446", "authors": ["Abd Elrahman Amer", "Magdi Amer"], "title": "Using multi-agent architecture to mitigate the risk of LLM hallucinations", "categories": ["cs.AI"], "comment": null, "summary": "Improving customer service quality and response time are critical factors for\nmaintaining customer loyalty and increasing a company's market share. While\nadopting emerging technologies such as Large Language Models (LLMs) is becoming\na necessity to achieve these goals, the risk of hallucination remains a major\nchallenge. In this paper, we present a multi-agent system to handle customer\nrequests sent via SMS. This system integrates LLM based agents with fuzzy logic\nto mitigate hallucination risks.", "AI": {"tldr": "This paper introduces a multi-agent system combining Large Language Models (LLMs) with fuzzy logic to handle SMS customer requests and reduce hallucination risks.", "motivation": "To improve customer service quality and response times while addressing the hallucination risks associated with LLM technologies.", "method": "The creation of a multi-agent system that integrates LLM-based agents with fuzzy logic technology to process customer requests via SMS.", "result": "A system that aims to both improve response quality and manage hallucination risks effectively in customer service.", "conclusion": "The presented multi-agent system provides a balanced approach to enhancing customer service using LLMs while mitigating their inherent risks."}}
{"id": "2507.01827", "pdf": "https://arxiv.org/pdf/2507.01827", "abs": "https://arxiv.org/abs/2507.01827", "authors": ["Haichuan Hu", "Congqing He", "Hao Zhang", "Xiaochen Xie", "Quanjun Zhang"], "title": "APRMCTS: Improving LLM-based Automated Program Repair with Iterative Tree Search", "categories": ["cs.SE"], "comment": null, "summary": "Automated Program Repair (APR) attempts to fix software bugs without human\nintervention, which plays a crucial role in software development and\nmaintenance. Recently, with the advances in Large Language Models (LLMs), a\nrapidly increasing number of APR techniques have been proposed with remarkable\nperformance. However, existing LLM-based APR techniques typically adopt\ntrial-and-error strategies, which suffer from two major drawbacks: (1)\ninherently limited patch effectiveness due to local exploration, and (2) low\nsearch efficiency due to redundant exploration. In this paper, we propose\nAPRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS\nincorporates Monte Carlo Tree Search (MCTS) into patch searching by performing\na global evaluation of the explored patches and selecting the most promising\none for subsequent refinement and generation. APRMCTS effectively resolves the\nproblems of falling into local optima and thus helps improve the efficiency of\npatch searching. Our experiments on 835 bugs from Defects4J demonstrate that,\nwhen integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which\noutperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,\nGPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,\nrespectively. More importantly, APRMCTS boasts a significant performance\nadvantage while employing small patch size (16 and 32), notably fewer than the\n500 and 10,000 patches adopted in previous studies. In terms of cost, compared\nto existing state-of-the-art LLM-based APR methods, APRMCTS has time and\nmonetary costs of less than 20% and 50%, respectively. Our extensive study\ndemonstrates that APRMCTS exhibits good effectiveness and efficiency, with\nparticular advantages in addressing complex bugs.", "AI": {"tldr": "The paper introduces APRMCTS, a new automated program repair technique that combines LLMs with Monte Carlo Tree Search to improve patch effectiveness and efficiency. It outperforms existing methods across metrics, particularly on complex bugs.", "motivation": "To overcome the limitations of trial-and-error strategies in LLM-based automated program repair, which include limited patch effectiveness and low search efficiency.", "method": "APRMCTS integrates Monte Carlo Tree Search into LLM-based program repair, enabling a global evaluation of explored patches to refine and generate more effective solutions.", "result": "On testing 835 bugs from Defects4J, APRMCTS fixed 201 bugs, outperforming current state-of-the-art methods. It also enhances various models' patching capabilities while using significantly smaller patch sizes and reducing costs.", "conclusion": "APRMCTS is a scalable and efficient APR solution with strong effectiveness, particularly for fixing complex bugs, while maintaining lower time and monetary expenses."}}
{"id": "2507.01259", "pdf": "https://arxiv.org/pdf/2507.01259", "abs": "https://arxiv.org/abs/2507.01259", "authors": ["Micha\u0142 Matak", "Jaros\u0142aw A. Chudziak"], "title": "GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 2 figures, presented at ICAART 2025, in proceedings of the\n  17th International Conference on Agents and Artificial Intelligence - Volume\n  3: ICAART", "summary": "In this paper we discuss the capability of large language models to base\ntheir answer and provide proper references when dealing with legal matters of\nnon-english and non-chinese speaking country. We discuss the history of legal\ninformation retrieval, the difference between case law and statute law, its\nimpact on the legal tasks and analyze the latest research in this field. Basing\non that background we introduce gAIus, the architecture of the cognitive\nLLM-based agent, whose responses are based on the knowledge retrieved from\ncertain legal act, which is Polish Civil Code. We propose a retrieval mechanism\nwhich is more explainable, human-friendly and achieves better results than\nembedding-based approaches. To evaluate our method we create special dataset\nbased on single-choice questions from entrance exams for law apprenticeships\nconducted in Poland. The proposed architecture critically leveraged the\nabilities of used large language models, improving the gpt-3.5-turbo-0125 by\n419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.\nAt the end of our paper we show the possible future path of research and\npotential applications of our findings.", "AI": {"tldr": "The paper introduces gAIus, a retrieval system enhanced by large language models for handling Polish legal texts, achieving significant improvements in accuracy over existing models.", "motivation": "Existing large language models (LLMs) have limitations in providing reliable references and accurate answers for legal matters in non-English and non-Chinese contexts, prompting research into more context-sensitive and explainable retrieval methods.", "method": "The paper proposes a new architecture called gAIus, which uses large language models in combination with a retrieval mechanism tailored for Polish Civil Code. A specialized dataset based on Polish law apprenticeship questions was created for evaluation.", "result": "The gAIus system improved GPT-3.5-turbo-0125's score by 419%, enabling it to outperform GPT-4o and boosting GPT-4o-mini's score from 31% to 86%.", "conclusion": "The gAIus architecture demonstrates significant potential for enhancing LLM-based tasks in legal domains and suggests future research directions and applications in law."}}
{"id": "2507.01202", "pdf": "https://arxiv.org/pdf/2507.01202", "abs": "https://arxiv.org/abs/2507.01202", "authors": ["Enes Dilber", "Colin Gray"], "title": "Shrinkage-Based Regressions with Many Related Treatments", "categories": ["econ.EM", "stat.ME", "stat.ML"], "comment": null, "summary": "When using observational causal models, practitioners often want to\ndisentangle the effects of many related, partially-overlapping treatments.\nExamples include estimating treatment effects of different marketing\ntouchpoints, ordering different types of products, or signing up for different\nservices. Common approaches that estimate separate treatment coefficients are\ntoo noisy for practical decision-making. We propose a computationally light\nmodel that uses a customized ridge regression to move between a heterogeneous\nand a homogenous model: it substantially reduces MSE for the effects of each\nindividual sub-treatment while allowing us to easily reconstruct the effects of\nan aggregated treatment. We demonstrate the properties of this estimator in\ntheory and simulation, and illustrate how it has unlocked targeted\ndecision-making at Wayfair.", "AI": {"tldr": "The paper introduces a ridge regression model to improve causal effect estimation under partially overlapping treatments, achieving reduced MSE and enabling targeted decision-making.", "motivation": "The motivation arises from the challenges faced in disentangling effects of many partially overlapping treatments in observational causal models, where existing approaches yield too noisy estimates for practical use.", "method": "The proposed method employs a customized ridge regression approach, transitioning between heterogeneous and homogenous modeling, to produce less noisy estimates of sub-treatment effects.", "result": "The method significantly reduces Mean Squared Error (MSE) for individual sub-treatment effects and allows reconstruction of aggregated treatment effects, demonstrated through theory and simulation.", "conclusion": "The model offers practical benefits, such as enabling more accurate and actionable decision-making, showcased in its application at Wayfair."}}
{"id": "2507.01255", "pdf": "https://arxiv.org/pdf/2507.01255", "abs": "https://arxiv.org/abs/2507.01255", "authors": ["Xiao Liu", "Jiawei Zhang"], "title": "AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation", "categories": ["cs.CV"], "comment": "Working in Progress", "summary": "The rapid advancement of AI-generated video models has created a pressing\nneed for robust and interpretable evaluation frameworks. Existing metrics are\nlimited to producing numerical scores without explanatory comments, resulting\nin low interpretability and human evaluation alignment. To address those\nchallenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video\nEvaluation(AIGVE), which can provide not only numerical scores but also\nmulti-aspect language comment feedback in evaluating these generated videos.\nCentral to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising\n2,500 AI-generated videos and 22,500 human-annotated detailed comments and\nnumerical scores across nine critical evaluation aspects. Leveraging\nAIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a\nnovel token-wise weighted loss and a dynamic frame sampling strategy to better\nalign with human evaluators. Comprehensive experiments across supervised and\nzero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art\nperformance in both scoring correlation and comment quality, significantly\noutperforming prior baselines including GPT-4o and VideoScore. In addition, we\nfurther showcase a multi-agent refinement framework where feedback from\nAIGVE-MACS drives iterative improvements in video generation, leading to 53.5%\nquality enhancement. This work establishes a new paradigm for comprehensive,\nhuman-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2\nand AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.", "AI": {"tldr": "This paper introduces AIGVE-MACS, a model that evaluates AI-generated videos with numerical scores and multi-aspect language feedback using a new benchmark, AIGVE-BENCH 2, outperforming existing methods.", "motivation": "Existing evaluation metrics for AI-generated videos lack interpretability and alignment with human assessments, necessitating a solution for comprehensive evaluation.", "method": "AIGVE-MACS uses Vision-Language Models with token-wise weighted loss and dynamic frame sampling, trained on the AIGVE-BENCH 2 containing 2,500 videos and 22,500 human annotations.", "result": "AIGVE-MACS delivers state-of-the-art performance in scoring correlation and feedback quality, surpassing methods like GPT-4o, and improves video generation quality by 53.5% using a multi-agent refinement framework.", "conclusion": "The proposed AIGVE-MACS model and AIGVE-BENCH 2 set a new standard for detailed and human-aligned evaluation of AI-generated videos, promoting iterative improvements in video production quality."}}
{"id": "2507.01031", "pdf": "https://arxiv.org/pdf/2507.01031", "abs": "https://arxiv.org/abs/2507.01031", "authors": ["Fanchen Bu", "Kijung Shin"], "title": "PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs", "categories": ["cs.LG", "cs.SE"], "comment": "Conference paper: Accepted in Korea Computer Congress (KCC) 2025. The\n  library is available at https://github.com/bokveizen/gaudi-geometric-learning", "summary": "Geometric learning has emerged as a powerful paradigm for modeling\nnon-Euclidean data, especially graph-structured ones, with applications\nspanning social networks, molecular structures, knowledge graphs, and\nrecommender systems. While Nvidia's CUDA-enabled graphics processing units\n(GPUs) largely dominate the hardware landscape, emerging accelerators such as\nIntel's Gaudi Habana Processing Units (HPUs) offer competitive performance and\nenergy efficiency. However, the usage of such non-CUDA processing units\nrequires significant engineering effort and novel software adaptations. In this\nwork, we present our experiences porting PyTorch-based geometric learning\nframeworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that\nrestore essential operations (e.g., scatter, sparse indexing, k-nearest\nneighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and\neleven real-world examples with diagnostic analyses of encountered failures and\ndetailed workarounds. We collect all our experiences into a publicly accessible\nGitHub repository. Our contributions lower the barrier for researchers to\nexperiment with geometric-learning algorithms and models on non-CUDA hardware,\nproviding a foundation for further optimization and cross-platform portability.", "AI": {"tldr": "The paper describes the adaptation of PyTorch-based geometric learning frameworks to Intel\u2019s Gaudi-v2 HPUs, offering tools, tutorials, and real-world examples to facilitate usage on non-CUDA hardware.", "motivation": "The growing importance of geometric learning applications coupled with the need for alternatives to the CUDA-dominated hardware ecosystem motivated the exploration of Intel's Gaudi-v2 HPUs.", "method": "The team ported core utilities like scatter, sparse indexing, and k-nearest neighbors to Gaudi-v2 HPUs and created a repository featuring tutorials, examples, and diagnostic tools for broader accessibility.", "result": "Sixteen tutorials, eleven real-world examples, and essential utilities for implementing geometric machine learning algorithms on Gaudi-v2 HPUs were successfully developed and shared.", "conclusion": "Their work lowers the barrier for researchers to explore geometric learning on non-CUDA platforms, promoting hardware diversification and software portability."}}
{"id": "2507.01198", "pdf": "https://arxiv.org/pdf/2507.01198", "abs": "https://arxiv.org/abs/2507.01198", "authors": ["Benjamin Kraljusic", "Zlatan Ajanovic", "Nermin Covic", "Bakir Lacevic"], "title": "Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives", "categories": ["cs.RO", "cs.AI", "cs.CG"], "comment": "6 pages, 3 figures, submitted to a conference", "summary": "This work proposes a motion planning algorithm for robotic manipulators that\ncombines sampling-based and search-based planning methods. The core\ncontribution of the proposed approach is the usage of burs of free\nconfiguration space (C-space) as adaptive motion primitives within the graph\nsearch algorithm. Due to their feature to adaptively expand in free C-space,\nburs enable more efficient exploration of the configuration space compared to\nfixed-sized motion primitives, significantly reducing the time to find a valid\npath and the number of required expansions. The algorithm is implemented within\nthe existing SMPL (Search-Based Motion Planning Library) library and evaluated\nthrough a series of different scenarios involving manipulators with varying\nnumber of degrees-of-freedom (DoF) and environment complexity. Results\ndemonstrate that the bur-based approach outperforms fixed-primitive planning in\ncomplex scenarios, particularly for high DoF manipulators, while achieving\ncomparable performance in simpler scenarios.", "AI": {"tldr": "This paper introduces a hybrid motion planning algorithm that enhances efficiency for robotic manipulators using adaptive motion primitives called 'burs' in configuration space exploration.", "motivation": "To improve efficiency and performance in motion planning for robotic manipulators, especially in challenging environments or with robots of high degrees-of-freedom.", "method": "Introduces 'burs', adaptive motion primitives, which are utilized within a graph search algorithm to better explore free configuration space. Implemented in the SMPL library.", "result": "The bur-based approach shows significant efficiency improvements in complex scenarios and with robots of higher degrees-of-freedom, while maintaining comparable results in simpler cases.", "conclusion": "Utilizing burs as adaptive motion primitives offers a promising enhancement to motion planning algorithms for complex and high DoF robotic systems."}}
{"id": "2507.01075", "pdf": "https://arxiv.org/pdf/2507.01075", "abs": "https://arxiv.org/abs/2507.01075", "authors": ["Gabriele Padovani", "Valentine Anantharaj", "Sandro Fiore"], "title": "Provenance Tracking in Large-Scale Machine Learning Systems", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "As the demand for large scale AI models continues to grow, the optimization\nof their training to balance computational efficiency, execution time, accuracy\nand energy consumption represents a critical multidimensional challenge.\nAchieving this balance requires not only innovative algorithmic techniques and\nhardware architectures but also comprehensive tools for monitoring, analyzing,\nand understanding the underlying processes involved in model training and\ndeployment. Provenance data information about the origins, context, and\ntransformations of data and processes has become a key component in this\npursuit. By leveraging provenance, researchers and engineers can gain insights\ninto resource usage patterns, identify inefficiencies, and ensure\nreproducibility and accountability in AI development workflows. For this\nreason, the question of how distributed resources can be optimally utilized to\nscale large AI models in an energy efficient manner is a fundamental one. To\nsupport this effort, we introduce the yProv4ML library, a tool designed to\ncollect provenance data in JSON format, compliant with the W3C PROV and ProvML\nstandards. yProv4ML focuses on flexibility and extensibility, and enables users\nto integrate additional data collection tools via plugins. The library is fully\nintegrated with the yProv framework, allowing for higher level pairing in tasks\nrun also through workflow management systems.", "AI": {"tldr": "The paper introduces yProv4ML, a library for collecting provenance data to optimize large-scale AI model training and deployment, focusing on energy efficiency and computational balance.", "motivation": "The rapid expansion of large-scale AI models creates challenges in balancing computational efficiency, execution time, accuracy, and energy consumption, prompting the need for better monitoring and understanding of training processes.", "method": "The authors developed yProv4ML, a library compliant with W3C PROV and ProvML standards, designed to flexibly collect provenance data in JSON format. It integrates with the yProv framework and can be extended with plugins.", "result": "yProv4ML enables the collection of detailed provenance data, providing insights into resource usage patterns, inefficiencies, and workflows, while ensuring reproducibility and accountability.", "conclusion": "yProv4ML represents a significant step in scaling AI models efficiently by leveraging provenance data, offering researchers tools to optimize workflows and resource usage effectively."}}
{"id": "2507.01489", "pdf": "https://arxiv.org/pdf/2507.01489", "abs": "https://arxiv.org/abs/2507.01489", "authors": ["Yanfei Zhang"], "title": "Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning", "categories": ["cs.AI", "cs.MA"], "comment": "12 pages", "summary": "Large Language Models (LLMs) have emerged as one of the most significant\ntechnological advancements in artificial intelligence in recent years. Their\nability to understand, generate, and reason with natural language has\ntransformed how we interact with AI systems. With the development of LLM-based\nagents and reinforcement-learning-based reasoning models, the study of applying\nreinforcement learning in agent frameworks has become a new research focus.\nHowever, all previous studies face the challenge of deciding the tool calling\nprocess and the reasoning process simultaneously, and the chain of reasoning\nwas solely relied on the unprocessed raw result with redundant information and\nsymbols unrelated to the task from the tool, which impose a heavy burden on the\nmodel's capability to reason. Therefore, in our research, we proposed a\nhierarchical framework Agent-as-tool that detach the tool calling process and\nthe reasoning process, which enables the model to focus on the verbally\nreasoning process while the tool calling process is handled by another agent.\nOur work had achieved comparable results with only a slight reinforcement\nfine-tuning on 180 samples, and had achieved exceptionally well performance in\nBamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding\nSearch-R1 by 4.8% in exact match and 3.2% in cover exact match.", "AI": {"tldr": "This paper proposes a hierarchical framework called \"Agent-as-tool\" to separate tool calling and reasoning processes in LLMs, improving efficiency and reasoning capabilities.", "motivation": "The authors aim to address the challenges faced by previous reinforcement learning research involving LLMs, particularly in handling tool-generated raw information that imposes a reasoning burden on the model.", "method": "They detach the tool calling process from the reasoning process by using a separate agent for tool management, enabling efficient verbal reasoning by the main model.", "result": "Their approach achieved comparable results with minimal fine-tuning (180 samples) and outperformed benchmarks in the Bamboogle task, with scores of 63.2% exact match and 75.2% cover exact match.", "conclusion": "Separating tool handling and reasoning processes enhances the performance of LLMs in agent tasks, demonstrating the step forward in reinforcement learning with AI-driven frameworks."}}
{"id": "2507.01278", "pdf": "https://arxiv.org/pdf/2507.01278", "abs": "https://arxiv.org/abs/2507.01278", "authors": ["Cindy Lie Tabuse", "David Restepo", "Carolina Gracitelli", "Fernando Korn Malerbi", "Caio Regatieri", "Luis Filipe Nakayama"], "title": "Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) can simulate clinical reasoning based on natural\nlanguage prompts, but their utility in ophthalmology is largely unexplored.\nThis study evaluated GPT-4's ability to interpret structured textual\ndescriptions of retinal fundus photographs and simulate clinical decisions for\ndiabetic retinopathy (DR) and glaucoma screening, including the impact of\nadding real or synthetic clinical metadata. We conducted a retrospective\ndiagnostic validation study using 300 annotated fundus images. GPT-4 received\nstructured prompts describing each image, with or without patient metadata. The\nmodel was tasked with assigning an ICDR severity score, recommending DR\nreferral, and estimating the cup-to-disc ratio for glaucoma referral.\nPerformance was evaluated using accuracy, macro and weighted F1 scores, and\nCohen's kappa. McNemar's test and change rate analysis were used to assess the\ninfluence of metadata. GPT-4 showed moderate performance for ICDR\nclassification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),\ndriven mainly by correct identification of normal cases. Performance improved\nin the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For\nglaucoma referral, performance was poor across all settings (accuracy ~78%, F1\n<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes\n(McNemar p > 0.05), and predictions remained consistent across conditions.\nGPT-4 can simulate basic ophthalmic decision-making from structured prompts but\nlacks precision for complex tasks. While not suitable for clinical use, LLMs\nmay assist in education, documentation, or image annotation workflows in\nophthalmology.", "AI": {"tldr": "This study explores GPT-4's capability to analyze retinal fundus image descriptions for clinical decision-making in ophthalmology, showing moderate success in basic tasks but poor performance in complex applications.", "motivation": "To investigate the largely unexplored utility of GPT-4 in ophthalmology for simulating clinical decision-making from structured prompts, specifically in diabetic retinopathy and glaucoma screening.", "method": "The study utilized 300 annotated fundus images and used structured textual prompts, with or without patient metadata, to evaluate GPT-4's diagnostic and referral decision-making abilities using accuracy, F1 scores, Cohen\u2019s kappa, McNemar\u2019s test, and change rate analysis.", "result": "GPT-4 performed moderately well for basic tasks such as diabetic retinopathy severity classification and referral recommendations but showed poor performance for complex tasks like glaucoma referral analysis. Metadata inclusion did not affect the model's outputs significantly.", "conclusion": "While GPT-4 cannot replace clinical systems for precision tasks, it holds potential for educational, documentation, and annotation purposes in ophthalmology workflows."}}
{"id": "2507.01314", "pdf": "https://arxiv.org/pdf/2507.01314", "abs": "https://arxiv.org/abs/2507.01314", "authors": ["Rong Jiang", "Keming Yu", "Jiangfeng Wang"], "title": "Semi-supervised learning for linear extremile regression", "categories": ["stat.ME", "stat.ML"], "comment": "arXiv admin note: substantial text overlap with arXiv:2310.07107", "summary": "Extremile regression, as a least squares analog of quantile regression, is\npotentially useful tool for modeling and understanding the extreme tails of a\ndistribution. However, existing extremile regression methods, as nonparametric\napproaches, may face challenges in high-dimensional settings due to data\nsparsity, computational inefficiency, and the risk of overfitting. While linear\nregression serves as the foundation for many other statistical and machine\nlearning models due to its simplicity, interpretability, and relatively easy\nimplementation, particularly in high-dimensional settings, this paper\nintroduces a novel definition of linear extremile regression along with an\naccompanying estimation methodology. The regression coefficient estimators of\nthis method achieve $\\sqrt{n}$-consistency, which nonparametric extremile\nregression may not provide. In particular, while semi-supervised learning can\nleverage unlabeled data to make more accurate predictions and avoid overfitting\nto small labeled datasets in high-dimensional spaces, we propose a\nsemi-supervised learning approach to enhance estimation efficiency, even when\nthe specified linear extremile regression model may be misspecified. Both\nsimulation studies and real data analyses demonstrate the finite-sample\nperformance of our proposed methods.", "AI": {"tldr": "The paper introduces a new linear extremile regression model with a semi-supervised learning approach, addressing issues in high-dimensional and nonparametric settings.", "motivation": "To overcome challenges associated with nonparametric extremile regression in high dimensions, specifically data sparsity, inefficiency, and overfitting.", "method": "A novel linear extremile regression model and estimation methodology are proposed, achieving $\\sqrt{n}$-consistency, with semi-supervised learning to enhance efficiency and robustness to model misspecification.", "result": "Simulation and real data experiments demonstrate strong finite-sample performance for the proposed methodologies.", "conclusion": "The proposed linear extremile regression model and semi-supervised approach provide efficient and robust solutions for analyzing extreme tails in high-dimensional settings."}}
{"id": "2507.01269", "pdf": "https://arxiv.org/pdf/2507.01269", "abs": "https://arxiv.org/abs/2507.01269", "authors": ["Mohammad Jahanbakht", "Alex Olsen", "Ross Marchant", "Emilie Fillols", "Mostafa Rahimi Azghadi"], "title": "Advancements in Weed Mapping: A Systematic Review", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Weed mapping plays a critical role in precision management by providing\naccurate and timely data on weed distribution, enabling targeted control and\nreduced herbicide use. This minimizes environmental impacts, supports\nsustainable land management, and improves outcomes across agricultural and\nnatural environments. Recent advances in weed mapping leverage ground-vehicle\nRed Green Blue (RGB) cameras, satellite and drone-based remote sensing combined\nwith sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The\nresulting data are processed using advanced techniques including big data\nanalytics and machine learning, significantly improving the spatial and\ntemporal resolution of weed maps and enabling site-specific management\ndecisions. Despite a growing body of research in this domain, there is a lack\nof comprehensive literature reviews specifically focused on weed mapping. In\nparticular, the absence of a structured analysis spanning the entire mapping\npipeline, from data acquisition to processing techniques and mapping tools,\nlimits progress in the field. This review addresses these gaps by\nsystematically examining state-of-the-art methods in data acquisition (sensor\nand platform technologies), data processing (including annotation and\nmodelling), and mapping techniques (such as spatiotemporal analysis and\ndecision support tools). Following PRISMA guidelines, we critically evaluate\nand synthesize key findings from the literature to provide a holistic\nunderstanding of the weed mapping landscape. This review serves as a\nfoundational reference to guide future research and support the development of\nefficient, scalable, and sustainable weed management systems.", "AI": {"tldr": "The paper reviews state-of-the-art technologies and methods for weed mapping, including sensors, processing, and mapping techniques, to improve management efficacy and sustainability.", "motivation": "Weed mapping is essential for targeted weed control, minimizing environmental impacts, and enhancing sustainable land management, but existing research lacks comprehensive reviews across mapping pipelines.", "method": "The authors performed a structured review based on PRISMA guidelines, systematically analyzing literature on sensor and platform technologies, data processing, and mapping techniques.", "result": "The paper synthesizes key findings in weed mapping methods and technologies, offering a holistic overview of advances and challenges in the entire mapping workflow.", "conclusion": "This review provides foundational insights to guide future research and development of scalable and sustainable weed mapping and management systems."}}
{"id": "2507.01032", "pdf": "https://arxiv.org/pdf/2507.01032", "abs": "https://arxiv.org/abs/2507.01032", "authors": ["Nan Mu", "Hongbo Yang", "Chen Zhao"], "title": "An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Background and Objective: High-throughput multi-omics technologies have\nproven invaluable for elucidating disease mechanisms and enabling early\ndiagnosis. However, the high cost of multi-omics profiling imposes a\nsignificant economic burden, with over reliance on full omics data potentially\nleading to unnecessary resource consumption. To address these issues, we\npropose an uncertainty-aware, multi-view dynamic decision framework for omics\ndata classification that aims to achieve high diagnostic accuracy while\nminimizing testing costs. Methodology: At the single-omics level, we refine the\nactivation functions of neural networks to generate Dirichlet distribution\nparameters, utilizing subjective logic to quantify both the belief masses and\nuncertainty mass of classification results. Belief mass reflects the support of\na specific omics modality for a disease class, while the uncertainty parameter\ncaptures limitations in data quality and model discriminability, providing a\nmore trustworthy basis for decision-making. At the multi omics level, we employ\na fusion strategy based on Dempster-Shafer theory to integrate heterogeneous\nmodalities, leveraging their complementarity to boost diagnostic accuracy and\nrobustness. A dynamic decision mechanism is then applied that omics data are\nincrementally introduced for each patient until either all data sources are\nutilized or the model confidence exceeds a predefined threshold, potentially\nbefore all data sources are utilized. Results and Conclusion: We evaluate our\napproach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.\nIn three datasets, over 50% of cases achieved accurate classification using a\nsingle omics modality, effectively reducing redundant testing. Meanwhile, our\nmethod maintains diagnostic performance comparable to full-omics models and\npreserves essential biological insights.", "AI": {"tldr": "This paper presents a cost-effective, dynamic decision framework for classifying multi-omics data, reducing redundant tests while maintaining diagnostic accuracy.", "motivation": "The paper aims to address the economic burden and resource inefficiency of high-cost multi-omics profiling in disease diagnosis.", "method": "A neural network computes uncertainty-aware Dirichlet distributions for single-omics data, while a Dempster-Shafer theory-based fusion strategy integrates multi-omics data incrementally for decision-making.", "result": "The framework successfully reduces redundant testing while retaining diagnostic performance, achieving accurate classification in over 50% of cases with single omics in 3 datasets.", "conclusion": "The proposed framework boosts diagnostic efficiency and preserves biological insights, offering a more resource-efficient solution for multi-omics profiling."}}
{"id": "2507.01206", "pdf": "https://arxiv.org/pdf/2507.01206", "abs": "https://arxiv.org/abs/2507.01206", "authors": ["Kathy Zhuang", "Zixun Huang", "Yukun Song", "Rui Li", "Yinuo Zhou", "Allen Y. Yang"], "title": "2024 NASA SUITS Report: LLM-Driven Immersive Augmented Reality User Interface for Robotics and Space Exploration", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "As modern computing advances, new interaction paradigms have emerged,\nparticularly in Augmented Reality (AR), which overlays virtual interfaces onto\nphysical objects. This evolution poses challenges in machine perception,\nespecially for tasks like 3D object pose estimation in complex, dynamic\nenvironments. Our project addresses critical issues in human-robot interaction\nwithin mobile AR, focusing on non-intrusive, spatially aware interfaces. We\npresent URSA, an LLM-driven immersive AR system developed for NASA's 2023-2024\nSUITS challenge, targeting future spaceflight needs such as the Artemis\nmissions. URSA integrates three core technologies: a head-mounted AR device\n(e.g., HoloLens) for intuitive visual feedback, voice control powered by large\nlanguage models for hands-free interaction, and robot tracking algorithms that\nenable accurate 3D localization in dynamic settings. To enhance precision, we\nleverage digital twin localization technologies, using datasets like\nDTTD-Mobile and specialized hardware such as the ZED2 camera for real-world\ntracking under noise and occlusion. Our system enables real-time robot control\nand monitoring via an AR interface, even in the absence of ground-truth\nsensors--vital for hazardous or remote operations. Key contributions include:\n(1) a non-intrusive AR interface with LLM-based voice input; (2) a ZED2-based\ndataset tailored for non-rigid robotic bodies; (3) a Local Mission Control\nConsole (LMCC) for mission visualization; (4) a transformer-based 6DoF pose\nestimator (DTTDNet) optimized for depth fusion and real-time tracking; and (5)\nend-to-end integration for astronaut mission support. This work advances\ndigital twin applications in robotics, offering scalable solutions for both\naerospace and industrial domains.", "AI": {"tldr": "This paper introduces URSA, an LLM-driven AR system designed for spaceflight missions like Artemis, focusing on integrating AR, voice control, and real-time 3D localization for human-robot interaction.", "motivation": "The paper addresses challenges in human-robot interaction in dynamic environments using AR, particularly for tasks like 3D pose estimation critical for spaceflight missions.", "method": "The authors developed URSA, a system integrating an AR headset (HoloLens), LLM-powered voice control, robot tracking via digital twin localization, and 3D pose estimation with algorithms like DTTDNet.", "result": "URSA demonstrated real-time robot control and monitoring capabilities in AR, even without ground-truth sensors, suitable for hazardous and remote space operations.", "conclusion": "The system advances AR capabilities for robotics, offering scalable solutions for aerospace and industrial applications, enhancing precision and usability in complex environments."}}
{"id": "2507.01078", "pdf": "https://arxiv.org/pdf/2507.01078", "abs": "https://arxiv.org/abs/2507.01078", "authors": ["Gabriele Padovani", "Valentine Anantharaj", "Sandro Fiore"], "title": "yProv4ML: Effortless Provenance Tracking for Machine Learning Systems", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "The rapid growth of interest in large language models (LLMs) reflects their\npotential for flexibility and generalization, and attracted the attention of a\ndiverse range of researchers. However, the advent of these techniques has also\nbrought to light the lack of transparency and rigor with which development is\npursued. In particular, the inability to determine the number of epochs and\nother hyperparameters in advance presents challenges in identifying the best\nmodel. To address this challenge, machine learning frameworks such as MLFlow\ncan automate the collection of this type of information. However, these tools\ncapture data using proprietary formats and pose little attention to lineage.\nThis paper proposes yProv4ML, a framework to capture provenance information\ngenerated during machine learning processes in PROV-JSON format, with minimal\ncode modifications.", "AI": {"tldr": "The paper introduces yProv4ML, a framework to capture machine learning process provenance in PROV-JSON format with minimal effort.", "motivation": "The lack of transparency and rigor in tracking hyperparameters and epochs in large language model development.", "method": "Developing yProv4ML, a framework that captures provenance data during ML processes in standard PROV-JSON format, requiring minimal code adjustments.", "result": "The framework offers an alternative to proprietary tools like MLFlow, emphasizing openness and lineage tracking.", "conclusion": "yProv4ML enhances transparency in machine learning by efficiently capturing process provenance without extensive code changes."}}
{"id": "2507.01597", "pdf": "https://arxiv.org/pdf/2507.01597", "abs": "https://arxiv.org/abs/2507.01597", "authors": ["Yuehang Si", "Zefan Zeng", "Jincai Huang", "Qing Cheng"], "title": "T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases.", "AI": {"tldr": "The paper introduces T3DM, an approach addressing challenges in Temporal Knowledge Graph Reasoning (TKGR) by tackling event distribution shifts and improving negative sampling quality.", "motivation": "TKGR models face challenges including inadequate modeling of event distribution shifts between train and test phases and reliance on low-quality negative sampling strategies.", "method": "T3DM, a novel approach, integrates Test-Time Training-guided Distribution Shift Modelling to handle distribution shifts, and employs an adversarial training-based negative-sampling strategy to generate superior negative samples.", "result": "Extensive experiments demonstrate that T3DM significantly outperforms state-of-the-art baselines in robustness and reasoning accuracy.", "conclusion": "T3DM improves the robustness and global reasoning consistency of TKGR models by addressing key limitations in distribution shift modeling and negative sampling."}}
{"id": "2507.01069", "pdf": "https://arxiv.org/pdf/2507.01069", "abs": "https://arxiv.org/abs/2507.01069", "authors": ["Nishant A. Parikh"], "title": "Agentic AI in Product Management: A Co-Evolutionary Model", "categories": ["cs.CE", "cs.SE"], "comment": "41 pages, 2 figures", "summary": "This study explores agentic AI's transformative role in product management,\nproposing a conceptual co-evolutionary framework to guide its integration\nacross the product lifecycle. Agentic AI, characterized by autonomy,\ngoal-driven behavior, and multi-agent collaboration, redefines product managers\n(PMs) as orchestrators of socio-technical ecosystems. Using systems theory,\nco-evolutionary theory, and human-AI interaction theory, the framework maps\nagentic AI capabilities in discovery, scoping, business case development,\ndevelopment, testing, and launch. An integrative review of 70+ sources,\nincluding case studies from leading tech firms, highlights PMs' evolving roles\nin AI orchestration, supervision, and strategic alignment. Findings emphasize\nmutual adaptation between PMs and AI, requiring skills in AI literacy,\ngovernance, and systems thinking. Addressing gaps in traditional frameworks,\nthis study provides a foundation for future research and practical\nimplementation to ensure responsible, effective agentic AI integration in\nsoftware organizations.", "AI": {"tldr": "The study proposes a framework for integrating agentic AI into product management, highlighting AI's role and its impact on PMs' functions.", "motivation": "To address gaps in traditional product management frameworks and guide the integration of agentic AI in product management, supporting its evolving role.", "method": "The research uses a co-evolutionary and systems theory framework supported by an integrative review of over 70 sources, including case studies.", "result": "The study identifies the evolving role of PMs, emphasizing new skills in AI literacy, governance, and systems thinking as essential for supervising and aligning AI's integration.", "conclusion": "The proposed framework establishes a theoretical foundation for integrating agentic AI responsibly and effectively into software organizations while highlighting the need for skill adaptations in PM roles."}}
{"id": "2507.01281", "pdf": "https://arxiv.org/pdf/2507.01281", "abs": "https://arxiv.org/abs/2507.01281", "authors": ["Juan Chen", "Baolong Bi", "Wei Zhang", "Jingyan Sui", "Xiaofei Zhu", "Yuanzhuo Wang", "Lingrui Mei", "Shenghua Liu"], "title": "Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating their parametric knowledge with external retrieved content.\nHowever, knowledge conflicts caused by internal inconsistencies or noisy\nretrieved content can severely undermine the generation reliability of RAG\nsystems.In this work, we argue that LLMs should rethink all evidence, including\nboth retrieved content and internal knowledge, before generating responses.We\npropose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel\nframework that improves trustworthiness through Conflict-Driven Summarization\nof all available evidence.CARE-RAG first derives parameter-aware evidence by\ncomparing parameter records to identify diverse internal perspectives. It then\nrefines retrieved evidences to produce context-aware evidence, removing\nirrelevant or misleading content. To detect and summarize conflicts, we distill\na 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable\nsynthesis across multiple sources.To further ensure evaluation integrity, we\nintroduce a QA Repair step to correct outdated or ambiguous benchmark\nanswers.Experiments on revised QA datasets with retrieval data show that\nCARE-RAG consistently outperforms strong RAG baselines, especially in scenarios\nwith noisy or conflicting evidence.", "AI": {"tldr": "The paper presents CARE-RAG, a framework aimed at improving the reliability of Retrieval-Augmented Generation (RAG) systems by resolving conflicts between LLMs' internal knowledge and retrieved information.", "motivation": "RAG systems often suffer from reliability issues due to inconsistencies between a model\u2019s internal knowledge and retrieved external content.", "method": "The approach includes parameter-aware evidence extraction, context-aware refinement of retrieved content, conflict-driven summarization using a distilled LLaMA model, and introducing a QA Repair step for benchmark evaluation.", "result": "Experiments show that CARE-RAG outperforms existing RAG systems, notably when dealing with noisy or contradictory evidence.", "conclusion": "CARE-RAG enhances trustworthiness in RAG systems by effectively resolving knowledge conflicts, making it more suitable for scenarios needing reliable evidence synthesis."}}
{"id": "2507.01430", "pdf": "https://arxiv.org/pdf/2507.01430", "abs": "https://arxiv.org/abs/2507.01430", "authors": ["Matthew Berkowitz", "Rachel MacKay Altman", "Thomas M. Loughin"], "title": "Targeted tuning of random forests for quantile estimation and prediction intervals", "categories": ["stat.ME", "stat.AP", "stat.ML"], "comment": "36 pages, 15 figures", "summary": "We present a novel tuning procedure for random forests (RFs) that improves\nthe accuracy of estimated quantiles and produces valid, relatively narrow\nprediction intervals. While RFs are typically used to estimate mean responses\n(conditional on covariates), they can also be used to estimate quantiles by\nestimating the full distribution of the response. However, standard approaches\nfor building RFs often result in excessively biased quantile estimates. To\nreduce this bias, our proposed tuning procedure minimizes \"quantile coverage\nloss\" (QCL), which we define as the estimated bias of the marginal quantile\ncoverage probability estimate based on the out-of-bag sample. We adapt QCL\ntuning to handle censored data and demonstrate its use with random survival\nforests. We show that QCL tuning results in quantile estimates with more\naccurate coverage probabilities than those achieved using default parameter\nvalues or traditional tuning (using MSPE for uncensored data and C-index for\ncensored data), while also reducing the estimated MSE of these coverage\nprobabilities. We discuss how the superior performance of QCL tuning is linked\nto its alignment with the estimation goal. Finally, we explore the validity and\nwidth of prediction intervals created using this method.", "AI": {"tldr": "The paper introduces a tuning method for random forests that enhances quantile accuracy and improves prediction intervals.", "motivation": "Most traditional tuning processes for random forests lead to biased quantile estimates and suboptimal prediction interval performance.", "method": "The key method involves minimizing 'quantile coverage loss' (QCL) using the out-of-bag sample to reduce bias in quantile estimates. The procedure is further adapted for censored data.", "result": "The proposed approach achieves better quantile coverage probability, reduced MSE for coverage probabilities, and narrower, valid prediction intervals.", "conclusion": "QCL tuning aligns well with quantile estimation goals, resulting in superior performance over traditional methods."}}
{"id": "2507.01275", "pdf": "https://arxiv.org/pdf/2507.01275", "abs": "https://arxiv.org/abs/2507.01275", "authors": ["Chengxu Liu", "Lu Qi", "Jinshan Pan", "Xueming Qian", "Ming-Hsuan Yang"], "title": "Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Unpaired image dehazing has attracted increasing attention due to its\nflexible data requirements during model training. Dominant methods based on\ncontrastive learning not only introduce haze-unrelated content information, but\nalso ignore haze-specific properties in the frequency domain (\\ie,~haze-related\ndegradation is mainly manifested in the amplitude spectrum). To address these\nissues, we propose a novel frequency domain-based diffusion model, named \\ours,\nfor fully exploiting the beneficial knowledge in unpaired clear data. In\nparticular, inspired by the strong generative ability shown by Diffusion Models\n(DMs), we tackle the dehazing task from the perspective of frequency domain\nreconstruction and perform the DMs to yield the amplitude spectrum consistent\nwith the distribution of clear images. To implement it, we propose an Amplitude\nResidual Encoder (ARE) to extract the amplitude residuals, which effectively\ncompensates for the amplitude gap from the hazy to clear domains, as well as\nprovide supervision for the DMs training. In addition, we propose a Phase\nCorrection Module (PCM) to eliminate artifacts by further refining the phase\nspectrum during dehazing with a simple attention mechanism. Experimental\nresults demonstrate that our \\ours outperforms other state-of-the-art methods\non both synthetic and real-world datasets.", "AI": {"tldr": "The paper introduces a frequency domain-based diffusion model for unpaired image dehazing, leveraging amplitude and phase-specific techniques for improved results.", "motivation": "To overcome limitations in unpaired image dehazing, such as the introduction of irrelevant content information and neglect of haze-relevant properties in the frequency domain.", "method": "A novel diffusion model is proposed, incorporating an Amplitude Residual Encoder (ARE) for frequency-specific amplitude adjustments and a Phase Correction Module (PCM) for phase refinement.", "result": "The proposed model outperforms state-of-the-art methods on both synthetic and real-world datasets, showcasing superior dehazing capabilities.", "conclusion": "Integrating frequency domain reconstruction via diffusion models effectively enhances unpaired image dehazing by addressing amplitude and phase spectrum inconsistencies."}}
{"id": "2507.01034", "pdf": "https://arxiv.org/pdf/2507.01034", "abs": "https://arxiv.org/abs/2507.01034", "authors": ["Asma Agaal", "Mansour Essgaer", "Hend M. Farkash", "Zulaiha Ali Othman"], "title": "Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya", "categories": ["cs.LG", "cs.AI"], "comment": "This article was published in International Journal of Intelligent\n  Systems and Applications (IJISA) (MECS Press), Vol. 17, No. 3, 8 Jun. 2025,\n  DOI: https://doi.org/10.5815/ijisa.2025.03.05", "summary": "Accurate electricity forecasting is crucial for grid stability and energy\nplanning, especially in Benghazi, Libya, where frequent load shedding,\ngeneration deficits, and infrastructure limitations persist. This study\nproposes a data-driven approach to forecast electricity load, generation, and\ndeficits for 2025 using historical data from 2019 (a year marked by\ninstability) and 2023 (a more stable year). Multiple time series models were\napplied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential\nsmoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural\nnetworks. The dataset was enhanced through missing value imputation, outlier\nsmoothing, and log transformation. Performance was assessed using mean squared\nerror, root mean squared error, mean absolute error, and mean absolute\npercentage error. LSTM outperformed all other models, showing strong\ncapabilities in modeling non-stationary and seasonal patterns. A key\ncontribution of this work is an optimized LSTM framework that integrates\nexogenous factors such as temperature and humidity, offering robust performance\nin forecasting multiple electricity indicators. These results provide practical\ninsights for policymakers and grid operators to enable proactive load\nmanagement and resource planning in data-scarce, volatile regions.", "AI": {"tldr": "The study presents an advanced LSTM-based framework for electricity forecasting in Benghazi, Libya, outperforming traditional models using historical data during periods of instability and stability.", "motivation": "The paper aims to address electricity forecasting challenges in Benghazi, Libya, due to frequent load shedding, infrastructure issues, and generation deficits.", "method": "The researchers applied various time series models, including LSTM, to predict electricity load, generation, and deficits. They enhanced the dataset with preprocessing techniques and assessed model performance using error metrics.", "result": "LSTM exhibited superior performance over other models in handling non-stationary and seasonal data, particularly when integrating exogenous factors such as temperature and humidity.", "conclusion": "The findings support the development of effective electricity forecasting frameworks, empowering policymakers and grid operators in volatile regions to make informed decisions."}}
{"id": "2507.01243", "pdf": "https://arxiv.org/pdf/2507.01243", "abs": "https://arxiv.org/abs/2507.01243", "authors": ["Ziang Zheng", "Guojian Zhan", "Shiqi Liu", "Yao Lyu", "Tao Zhang", "Shengbo Eben Li"], "title": "Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme Monopedal Locomotion", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has shown great potential in enabling quadruped\nrobots to perform agile locomotion. However, directly training policies to\nsimultaneously handle dual extreme challenges, i.e., extreme underactuation and\nextreme terrains, as in monopedal hopping tasks, remains highly challenging due\nto unstable early-stage interactions and unreliable reward feedback. To address\nthis, we propose JumpER (jump-start reinforcement learning via self-evolving\npriors), an RL training framework that structures policy learning into multiple\nstages of increasing complexity. By dynamically generating self-evolving priors\nthrough iterative bootstrapping of previously learned policies, JumpER\nprogressively refines and enhances guidance, thereby stabilizing exploration\nand policy optimization without relying on external expert priors or\nhandcrafted reward shaping. Specifically, when integrated with a structured\nthree-stage curriculum that incrementally evolves action modality, observation\nspace, and task objective, JumpER enables quadruped robots to achieve robust\nmonopedal hopping on unpredictable terrains for the first time. Remarkably, the\nresulting policy effectively handles challenging scenarios that traditional\nmethods struggle to conquer, including wide gaps up to 60 cm, irregularly\nspaced stairs, and stepping stones with distances varying from 15 cm to 35 cm.\nJumpER thus provides a principled and scalable approach for addressing\nlocomotion tasks under the dual challenges of extreme underactuation and\nextreme terrains.", "AI": {"tldr": "The paper introduces JumpER, a reinforcement learning framework, for training quadruped robots to perform challenging monopedal hopping tasks on unpredictable terrains without relying on external priors or handcrafted rewards.", "motivation": "Quadruped robots face challenges in performing agile locomotion, especially under extreme underactuation and across extreme terrains. Current RL approaches struggle to address these due to unstable training and unreliable feedback.", "method": "JumpER structures RL training into multiple progressive stages by generating self-evolving priors. A three-stage curriculum adjusts action modality, observation, and objectives dynamically to refine policies without external guidance.", "result": "The proposed approach allows quadruped robots to robustly achieve monopedal hopping on challenging terrains, handling scenarios like gaps (up to 60 cm), irregular stairs, and uneven stepping stones.", "conclusion": "JumpER is a principled and scalable RL framework capable of overcoming dual challenges in locomotion for quadruped robots, offering a robust solution for terrains where traditional methods fail."}}
{"id": "2507.01090", "pdf": "https://arxiv.org/pdf/2507.01090", "abs": "https://arxiv.org/abs/2507.01090", "authors": ["Riccardo Mengoni", "Walter Nadalin", "Mathys Rennela", "Jimmy Rotureau", "Tom Darras", "Julien Laurat", "Eleni Diamanti", "Ioannis Lavdas"], "title": "Efficient Gate Reordering for Distributed Quantum Compiling in Data Centers", "categories": ["quant-ph", "cs.DC"], "comment": null, "summary": "Just as classical computing relies on distributed systems, the quantum\ncomputing era requires new kinds of infrastructure and software tools. Quantum\nnetworks will become the backbone of hybrid, quantum-augmented data centers, in\nwhich quantum algorithms are distributed over a local network of quantum\nprocessing units (QPUs) interconnected via shared entanglement. In this\ncontext, it is crucial to develop methods and software that minimize the number\nof inter-QPU communications. Here we describe key features of the quantum\ncompiler araQne, which is designed to minimize distribution cost, measured by\nthe number of entangled pairs required to distribute a monolithic quantum\ncircuit using gate teleportation protocols. We establish the crucial role\nplayed by circuit reordering strategies, which strongly reduce the distribution\ncost compared to a baseline approach.", "AI": {"tldr": "The paper introduces the quantum compiler araQne designed to minimize communication costs in distributed quantum computing.", "motivation": "The shift to quantum computing requires specialized infrastructure and software to efficiently distribute quantum algorithms over interconnected quantum processing units (QPUs) with minimal communication overhead.", "method": "The authors describe the araQne quantum compiler, which uses gate teleportation protocols and optimizes circuit reordering strategies to minimize the number of entangled pairs needed for distribution.", "result": "The use of circuit reordering significantly reduces the cost of inter-QPU communications compared to baseline methods.", "conclusion": "Efficient quantum compilers like araQne are essential for the development of hybrid quantum infrastructure, reducing communication costs and enhancing distributed quantum computing performance."}}
{"id": "2507.01717", "pdf": "https://arxiv.org/pdf/2507.01717", "abs": "https://arxiv.org/abs/2507.01717", "authors": ["Gopichand Kanumolu", "Ashok Urlana", "Charaka Vinayak Kumar", "Bala Mallikarjunarao Garlapati"], "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI", "categories": ["cs.AI", "cs.IR", "cs.LG", "cs.MA"], "comment": "AgentScen Workshop, IJCAI 2025", "summary": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data.", "AI": {"tldr": "The paper presents \"Agent Ideate,\" a framework leveraging Large Language Models (LLMs) and agents to generate innovative product ideas based on patents.", "motivation": "To overcome the challenge of accessing and interpreting technical knowledge in patents to derive innovative product ideas.", "method": "Designed and experimented with a framework, \"Agent Ideate,\" using LLMs and agent-based architectures in three domains: Computer Science, Natural Language Processing, and Material Chemistry.", "result": "Agent-based approaches outperformed standalone LLMs in idea quality, relevance, and novelty.", "conclusion": "Combining LLMs with autonomous agents significantly enhances business idea generation from patent data, unlocking innovation potential."}}
{"id": "2507.01457", "pdf": "https://arxiv.org/pdf/2507.01457", "abs": "https://arxiv.org/abs/2507.01457", "authors": ["Federico Nicolas Peccia", "Frederik Haxel", "Oliver Bringmann"], "title": "Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": "9 pages, 10 figures, 2 algorithms", "summary": "RISC-V provides a flexible and scalable platform for applications ranging\nfrom embedded devices to high-performance computing clusters. Particularly, its\nRISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI\nworkloads. But writing software that efficiently utilizes the vector units of\nRISC-V CPUs without expert knowledge requires the programmer to rely on the\nautovectorization features of compilers or hand-crafted libraries like\nmuRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing\nthe integration with the RISC-V RVV extension, thus heavily limiting the\nefficient deployment of complex AI workloads. In this paper, we present a\nworkflow based on the TVM compiler to efficiently map AI workloads onto RISC-V\nvector units. Instead of relying on hand-crafted libraries, we integrated the\nRVV extension into TVM's MetaSchedule framework, a probabilistic program\nframework for tensor operation tuning. We implemented different RISC-V SoCs on\nan FPGA and tuned a wide range of AI workloads on them. We found that our\nproposal shows a mean improvement of 46% in execution latency when compared\nagainst the autovectorization feature of GCC, and 29% against muRISCV-NN.\nMoreover, the binary resulting from our proposal has a smaller code memory\nfootprint, making it more suitable for embedded devices. Finally, we also\nevaluated our solution on a commercially available RISC-V SoC implementing the\nRVV 1.0 Vector Extension and found our solution is able to find mappings that\nare 35% faster on average than the ones proposed by LLVM. We open-sourced our\nproposal for the community to expand it to target other RISC-V extensions.", "AI": {"tldr": "This paper integrates RISC-V's RVV extension into TVM's MetaSchedule framework, enabling efficient AI workload mapping and achieving latency improvements over GCC, muRISCV-NN, and LLVM, with a smaller binary footprint.", "motivation": "To address challenges in efficiently utilizing RISC-V vector units for AI workloads, which currently rely on compiler autovectorization or hand-crafted libraries, by introducing smarter autotuning approaches.", "method": "Integrated the RVV extension into the TVM MetaSchedule framework for tuning tensor operations. Tested on FPGA-implemented RISC-V SoCs and commercially available RISC-V hardware.", "result": "Achieved 46% faster execution than GCC autovectorization, 29% faster than muRISCV-NN, and 35% faster than LLVM, with smaller code memory footprint.", "conclusion": "The proposed solution improves AI workload execution on RISC-V vector units and is open-sourced for community expansion to other extensions."}}
{"id": "2507.01297", "pdf": "https://arxiv.org/pdf/2507.01297", "abs": "https://arxiv.org/abs/2507.01297", "authors": ["Xinxi Lyu", "Michael Duan", "Rulin Shao", "Pang Wei Koh", "Sewon Min"], "title": "Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks", "categories": ["cs.CL", "cs.IR"], "comment": "33 pages, 2 figures, 27 tables", "summary": "Retrieval-augmented Generation (RAG) has primarily been studied in limited\nsettings, such as factoid question answering; more challenging,\nreasoning-intensive benchmarks have seen limited success from minimal RAG. In\nthis work, we challenge this prevailing view on established,\nreasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We\nidentify a key missing component in prior work: a usable, web-scale datastore\naligned with the breadth of pretraining data. To this end, we introduce\nCompactDS: a diverse, high-quality, web-scale datastore that achieves high\nretrieval accuracy and subsecond latency on a single-node. The key insights are\n(1) most web content can be filtered out without sacrificing coverage, and a\ncompact, high-quality subset is sufficient; and (2) combining in-memory\napproximate nearest neighbor (ANN) retrieval and on-disk exact search balances\nspeed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves\nconsistent accuracy improvements across all benchmarks and model sizes\n(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,\nand 19% on MATH. No single data source suffices alone, highlighting the\nimportance of diversity of sources (web crawls, curated math, academic papers,\ntextbooks). Finally, we show that our carefully designed in-house datastore\nmatches or outperforms web search engines such as Google Search, as well as\nrecently proposed, complex agent-based RAG systems--all while maintaining\nsimplicity, reproducibility, and self-containment. We release CompactDS and our\nretrieval pipeline, supporting future research exploring retrieval-based AI\nsystems.", "AI": {"tldr": "The study improves Retrieval-augmented Generation (RAG) performance on reasoning-intensive benchmarks by introducing CompactDS, a high-quality, web-scale datastore for effective retrieval.", "motivation": "Challenges in reasoning-intensive tasks require a robust datastore aligned with pretraining breadth to optimize RAG.", "method": "CompactDS combines in-memory approximate nearest neighbor search with on-disk exact search to balance speed and recall, offering a filtered, high-quality datastore.", "result": "Usage of CompactDS yields accuracy improvements (10%-33%) across benchmarks like MMLU, MATH, and GPQA, outperforming existing systems including web search engines.", "conclusion": "CompactDS demonstrates simplicity, reproducibility, and superior performance, paving the way for enhanced retrieval-based AI research and applications."}}
{"id": "2507.01473", "pdf": "https://arxiv.org/pdf/2507.01473", "abs": "https://arxiv.org/abs/2507.01473", "authors": ["Yuwen Wang", "Changyu Liu", "Xin He", "Junhui Wang"], "title": "Nonparametric learning of heterogeneous graphical model on network-linked data", "categories": ["stat.ME", "stat.ML"], "comment": null, "summary": "Graphical models have been popularly used for capturing conditional\nindependence structure in multivariate data, which are often built upon\nindependent and identically distributed observations, limiting their\napplicability to complex datasets such as network-linked data. This paper\nproposes a nonparametric graphical model that addresses these limitations by\naccommodating heterogeneous graph structures without imposing any specific\ndistributional assumptions. The proposed estimation method effectively\nintegrates network embedding with nonparametric graphical model estimation. It\nfurther transforms the graph learning task into solving a finite-dimensional\nlinear equation system by leveraging the properties of vector-valued\nreproducing kernel Hilbert space. Moreover, theoretical guarantees are\nestablished for the proposed method in terms of the estimation consistency and\nexact recovery of the heterogeneous graph structures. Its effectiveness is also\ndemonstrated through a variety of simulated examples and a real application to\nthe statistician coauthorship dataset.", "AI": {"tldr": "A nonparametric graphical model is proposed for heterogeneous network-linked data, integrating network embedding and graphical estimation with theoretical guarantees.", "motivation": "Conventional graphical models rely heavily on independent and identically distributed samples and thus struggle with network-linked, more complex datasets.", "method": "The method integrates network embedding into nonparametric graphical model estimation, transforming the graph learning task into solving finite-dimensional linear equations using vector-valued reproducing kernel Hilbert space properties.", "result": "The method shows estimation consistency, exact recovery of heterogeneous graph structures, and strong performance in simulations and a real-world application to a statistician coauthorship dataset.", "conclusion": "The approach expands the applicability of graphical models by supporting heterogeneous and complex graph structures using a nonparametric framework."}}
{"id": "2507.01290", "pdf": "https://arxiv.org/pdf/2507.01290", "abs": "https://arxiv.org/abs/2507.01290", "authors": ["Sunyong Seo", "Semin Kim", "Jongha Lee"], "title": "Learning an Ensemble Token from Task-driven Priors in Facial Analysis", "categories": ["cs.CV"], "comment": "11pages, 8figures, 4tables", "summary": "Facial analysis exhibits task-specific feature variations. While\nConvolutional Neural Networks (CNNs) have enabled the fine-grained\nrepresentation of spatial information, Vision Transformers (ViTs) have\nfacilitated the representation of semantic information at the patch level.\nAlthough the generalization of conventional methodologies has advanced visual\ninterpretability, there remains paucity of research that preserves the unified\nfeature representation on single task learning during the training process. In\nthis work, we introduce ET-Fuser, a novel methodology for learning ensemble\ntoken by leveraging attention mechanisms based on task priors derived from\npre-trained models for facial analysis. Specifically, we propose a robust prior\nunification learning method that generates a ensemble token within a\nself-attention mechanism, which shares the mutual information along the\npre-trained encoders. This ensemble token approach offers high efficiency with\nnegligible computational cost. Our results show improvements across a variety\nof facial analysis, with statistically significant enhancements observed in the\nfeature representations.", "AI": {"tldr": "The paper proposes ET-Fuser, a method for facial analysis that uses token ensemble learning based on task priors from pre-trained models for better feature representation.", "motivation": "There is a lack of research focusing on unified feature representation during single-task facial analysis learning, despite advancements in spatial and semantic interpretability.", "method": "ET-Fuser generates ensemble tokens via a self-attention mechanism, leveraging mutual information across pre-trained encoders with minimal computational cost.", "result": "The methodology improves facial analysis tasks and achieves statistically significant enhancements in feature representation.", "conclusion": "ET-Fuser offers efficient unified feature learning, advancing facial analysis through its robust and computationally-light approach."}}
{"id": "2507.01035", "pdf": "https://arxiv.org/pdf/2507.01035", "abs": "https://arxiv.org/abs/2507.01035", "authors": ["Yushang Zhao", "Haotian Lyu", "Yike Peng", "Aijia Sun", "Feng Jiang", "Xinyue Han"], "title": "Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": null, "summary": "The incessant advent of online services demands high speed and efficient\nrecommender systems (ReS) that can maintain real-time performance along with\nprocessing very complex user-item interactions. The present study, therefore,\nconsiders computational bottlenecks involved in hybrid Graph Neural Network\n(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their\ninference latency and training efficiency. An extensive methodology was used:\nhybrid GNN-LLM integrated architecture-optimization strategies(quantization,\nLoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.\nExperimental improvements were significant, with the optimal Hybrid + FPGA +\nDeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms\nof latency, while LoRA brought down training time by 66% (3.8 hours) in\ncomparison to the non-optimized baseline. Irrespective of domain, such as\naccuracy or efficiency, it can be established that hardware-software co-design\nand parameter-efficient tuning permit hybrid models to outperform GNN or LLM\napproaches implemented independently. It recommends the use of FPGA as well as\nLoRA for real-time deployment. Future work should involve federated learning\nalong with advanced fusion architectures for better scalability and privacy\npreservation. Thus, this research marks the fundamental groundwork concerning\nnext-generation ReS balancing low-latency response with cutting-edge\npersonalization.", "AI": {"tldr": "The paper explores optimizing hybrid Graph Neural Networks (GNN) and Large Language Models (LLM) for recommender systems to improve inference latency, training efficiency, and real-time performance.", "motivation": "The growing demand for efficient, high-speed recommender systems to handle complex user-item interactions motivated the research into overcoming computational bottlenecks in hybrid GNN-LLM-based models.", "method": "The methodology employed hybrid GNN-LLM integration, model optimization techniques (quantization, LoRA, distillation), hardware acceleration (FPGA, DeepSpeed), and implementation under R 4.4.2.", "result": "The optimized hybrid configuration with FPGA and DeepSpeed achieved 13.6% higher accuracy (NDCG@10: 0.75) and latency of 40-60ms. LoRA reduced training time by 66% (3.8 hours) compared to the baseline.", "conclusion": "Hardware-software co-design and parameter-efficient tuning significantly enhance the performance of hybrid recommender systems over standalone GNN or LLM methods. Using FPGA and LoRA is recommended for deployment, with future work targeting federated learning and advanced fusion architectures."}}
{"id": "2507.01264", "pdf": "https://arxiv.org/pdf/2507.01264", "abs": "https://arxiv.org/abs/2507.01264", "authors": ["Yongjie Fu", "Ruijian Zha", "Pei Tian", "Xuan Di"], "title": "LLM-based Realistic Safety-Critical Driving Video Generation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Designing diverse and safety-critical driving scenarios is essential for\nevaluating autonomous driving systems. In this paper, we propose a novel\nframework that leverages Large Language Models (LLMs) for few-shot code\ngeneration to automatically synthesize driving scenarios within the CARLA\nsimulator, which has flexibility in scenario scripting, efficient code-based\ncontrol of traffic participants, and enforcement of realistic physical\ndynamics. Given a few example prompts and code samples, the LLM generates\nsafety-critical scenario scripts that specify the behavior and placement of\ntraffic participants, with a particular focus on collision events. To bridge\nthe gap between simulation and real-world appearance, we integrate a video\ngeneration pipeline using Cosmos-Transfer1 with ControlNet, which converts\nrendered scenes into realistic driving videos. Our approach enables\ncontrollable scenario generation and facilitates the creation of rare but\ncritical edge cases, such as pedestrian crossings under occlusion or sudden\nvehicle cut-ins. Experimental results demonstrate the effectiveness of our\nmethod in generating a wide range of realistic, diverse, and safety-critical\nscenarios, offering a promising tool for simulation-based testing of autonomous\nvehicles.", "AI": {"tldr": "This paper introduces a framework using Large Language Models (LLMs) for few-shot code generation to create diverse and safety-critical driving scenarios in the CARLA simulator, accompanied by a video generation pipeline for realistic visualization.", "motivation": "Testing autonomous driving systems requires the generation of diverse, safety-critical, and realistic driving scenarios, which are essential for thorough evaluation but difficult to create manually due to their complexity.", "method": "The approach uses LLMs to synthesize scenario scripts for the CARLA simulator based on few-shot learning from example prompts and code samples. It also employs Cosmos-Transfer1 with ControlNet to turn simulated scenes into realistic driving videos.", "result": "The framework successfully generates a diverse range of realistic and safety-critical driving scenarios, including rare edge cases like pedestrian crossings under occlusion and sudden vehicle cut-ins.", "conclusion": "This method provides an effective and scalable tool for simulation-based testing of autonomous vehicles, contributing to their safety and reliability evaluation."}}
{"id": "2507.01285", "pdf": "https://arxiv.org/pdf/2507.01285", "abs": "https://arxiv.org/abs/2507.01285", "authors": ["Aymen Rayane Khouas", "Mohamed Reda Bouadjenek", "Hakim Hacid", "Sunil Aryal"], "title": "Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation", "categories": ["cs.LG", "cs.DC", "cs.IR"], "comment": "17 pages, 5 figures", "summary": "Graph federated recommendation systems offer a privacy-preserving alternative\nto traditional centralized recommendation architectures, which often raise\nconcerns about data security. While federated learning enables personalized\nrecommendations without exposing raw user data, existing aggregation methods\noverlook the unique properties of user embeddings in this setting. Indeed,\ntraditional aggregation methods fail to account for their complexity and the\ncritical role of user similarity in recommendation effectiveness. Moreover,\nevolving user interactions require adaptive aggregation while preserving the\ninfluence of high-relevance anchor users (the primary users before expansion in\ngraph-based frameworks). To address these limitations, we introduce\nDist-FedAvg, a novel distance-based aggregation method designed to enhance\npersonalization and aggregation efficiency in graph federated learning. Our\nmethod assigns higher aggregation weights to users with similar embeddings,\nwhile ensuring that anchor users retain significant influence in local updates.\nEmpirical evaluations on multiple datasets demonstrate that Dist-FedAvg\nconsistently outperforms baseline aggregation techniques, improving\nrecommendation accuracy while maintaining seamless integration into existing\nfederated learning frameworks.", "AI": {"tldr": "The paper introduces Dist-FedAvg, a distance-based aggregation technique for graph federated recommendation systems that improves personalization by prioritizing similar user embeddings, while ensuring anchor users retain influence.", "motivation": "Existing federated recommendation systems fail to effectively address user embedding complexity, similarity, and evolving user interactions, impacting the effectiveness of recommendations.", "method": "Dist-FedAvg assigns higher aggregation weights to users with similar embeddings and preserves influence from anchor users, offering adaptive aggregation suited for graph-based frameworks.", "result": "Empirical evaluations across various datasets show Dist-FedAvg outperforms traditional methods in recommendation accuracy and integrates well into existing federated learning setups.", "conclusion": "Dist-FedAvg enhances aggregation efficiency and personalization in graph federated learning while offering a privacy-preserving solution for recommendation systems."}}
{"id": "2507.01749", "pdf": "https://arxiv.org/pdf/2507.01749", "abs": "https://arxiv.org/abs/2507.01749", "authors": ["Arash Dehghan", "Mucahit Cevik", "Merve Bodur", "Bissan Ghaddar"], "title": "Joint Matching and Pricing for Crowd-shipping with In-store Customers", "categories": ["cs.AI"], "comment": null, "summary": "This paper examines the use of in-store customers as delivery couriers in a\ncentralized crowd-shipping system, targeting the growing need for efficient\nlast-mile delivery in urban areas. We consider a brick-and-mortar retail\nsetting where shoppers are offered compensation to deliver time-sensitive\nonline orders. To manage this process, we propose a Markov Decision Process\n(MDP) model that captures key uncertainties, including the stochastic arrival\nof orders and crowd-shippers, and the probabilistic acceptance of delivery\noffers. Our solution approach integrates Neural Approximate Dynamic Programming\n(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network\n(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop\nrouting and accounts for offer acceptance uncertainty, aligning more closely\nwith real-world operations. Experimental results demonstrate that the\nintegrated NeurADP + DDQN policy achieves notable improvements in delivery cost\nefficiency, with up to 6.7\\% savings over NeurADP with fixed pricing and\napproximately 18\\% over myopic baselines. We also show that allowing flexible\ndelivery delays and enabling multi-destination routing further reduces\noperational costs by 8\\% and 17\\%, respectively. These findings underscore the\nadvantages of dynamic, forward-looking policies in crowd-shipping systems and\noffer practical guidance for urban logistics operators.", "AI": {"tldr": "This paper proposes a crowd-shipping model where in-store shoppers serve as delivery couriers, using a combination of NeurADP and DDQN approaches for adaptive assignment and pricing, achieving significant cost savings in urban delivery systems.", "motivation": "The paper aims to address the inefficiency in last-mile delivery within urban areas by leveraging in-store customers as delivery couriers in a crowd-shipping model.", "method": "An MDP model is developed to account for uncertainties such as stochastic order and shopper arrivals, and probabilistic delivery acceptance. Neural Approximate Dynamic Programming (NeurADP) is used for order-to-shopper assignment, combined with Deep Double Q-Network (DDQN) for dynamic pricing.", "result": "The proposed solution outperforms fixed pricing and myopic baselines, with delivery cost savings of up to 6.7% and 18%, respectively. Allowing flexible delays and multi-destination routing further reduces costs by up to 17%.", "conclusion": "Dynamic and forward-looking policies like the integrated NeurADP and DDQN strategy can significantly enhance efficiency in crowd-shipping systems, offering actionable insights for urban logistics."}}
{"id": "2507.01299", "pdf": "https://arxiv.org/pdf/2507.01299", "abs": "https://arxiv.org/abs/2507.01299", "authors": ["Kai Liu", "Bowen Xu", "Shaoyu Wu", "Xin Chen", "Hao Zhou", "Yongliang Tao", "Lulu Hu"], "title": "La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation", "categories": ["cs.CL"], "comment": "ICML 2025 Acceptance", "summary": "Activation sparsity can reduce the computational overhead and memory\ntransfers during the forward pass of Large Language Model (LLM) inference.\nExisting methods face limitations, either demanding time-consuming recovery\ntraining that hinders real-world adoption, or relying on empirical\nmagnitude-based pruning, which causes fluctuating sparsity and unstable\ninference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse\nActivation), a novel method for activation sparsification designed to improve\nLLM efficiency without requiring additional training or magnitude-based\npruning. We leverage layerwise orthogonal rotations to transform input\nactivations into rotated forms that are more suitable for sparsification. By\nemploying a Top-K selection approach within the rotated activations, we achieve\nconsistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA\nis effective across various sizes and types of LLMs, demonstrating minimal\nperformance degradation and robust inference acceleration. Specifically, for\nLLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a\nconsistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in\nzero-shot tasks compared to the dense model to just 0.54%, while surpassing\nTEAL by 1.77% and CATS by 17.14%.", "AI": {"tldr": "LaRoSA is a novel activation sparsification method designed for Large Language Models (LLMs) that enhances efficiency without requiring retraining or magnitude-based pruning.", "motivation": "Existing sparsification methods for LLM inference face challenges such as requiring retraining or introducing unstable inference speed due to empirical pruning approaches.", "method": "LaRoSA applies layerwise orthogonal rotations to input activations, followed by Top-K selection to achieve a consistent sparsity level across models.", "result": "LaRoSA demonstrates effective sparsification with minimal performance loss across various LLMs. For instance, at 40% sparsity, LLaMA2-7B shows a mere 0.17 perplexity gap and achieves a 1.30x speed-up, outperforming other methods like TEAL and CATS.", "conclusion": "LaRoSA is a reliable and efficient activation sparsification method that delivers consistent sparsity and inference acceleration for LLMs, addressing key limitations of existing techniques."}}
{"id": "2507.01709", "pdf": "https://arxiv.org/pdf/2507.01709", "abs": "https://arxiv.org/abs/2507.01709", "authors": ["Paul Freulon", "Nikitas Georgakis", "Victor Panaretos"], "title": "Entropic optimal transport beyond product reference couplings: the Gaussian case on Euclidean space", "categories": ["math.ST", "stat.ML", "stat.TH", "62H99", "G.3"], "comment": "32 pages, 4 figures", "summary": "The optimal transport problem with squared Euclidean cost consists in finding\na coupling between two input measures that maximizes correlation. Consequently,\nthe optimal coupling is often singular with respect to Lebesgue measure.\nRegularizing the optimal transport problem with an entropy term yields an\napproximation called entropic optimal transport. Entropic penalties steer the\ninduced coupling toward a reference measure with desired properties. For\ninstance, when seeking a diffuse coupling, the most popular reference measures\nare the Lebesgue measure and the product of the two input measures. In this\nwork, we study the case where the reference coupling is not necessarily assumed\nto be a product. We focus on the Gaussian case as a motivating paradigm, and\nprovide a reduction of this more general optimal transport criterion to a\nmatrix optimization problem. This reduction enables us to provide a complete\ndescription of the solution, both in terms of the primal variable and the dual\nvariables. We argue that flexibility in terms of the reference measure can be\nimportant in statistical contexts, for instance when one has prior information,\nwhen there is uncertainty regarding the measures to be coupled, or to reduce\nbias when the entropic problem is used to estimate the un-regularized transport\nproblem. In particular, we show in numerical examples that choosing a suitable\nreference plan allows to reduce the bias caused by the entropic penalty.", "AI": {"tldr": "The paper focuses on entropic optimal transport with Gaussian reference couplings, reducing the problem to matrix optimization and addressing bias in regularized transport problems.", "motivation": "To address the limitations of current entropic optimal transport methods which often rely on rigid reference measures like Lebesgue or product measures, and to explore how a flexible choice of reference can reduce bias and incorporate prior knowledge.", "method": "The paper introduces a matrix optimization framework for entropic optimal transport with flexible reference measures, particularly focusing on Gaussian references. Both primal and dual variables are analyzed.", "result": "The authors provide a full mathematical reduction and characterization of the optimal transport problem under Gaussian reference measures, demonstrating advantages in reducing bias under specific numerical examples.", "conclusion": "Flexibility in reference measures for entropic regularization offers significant benefits, particularly in leveraging prior knowledge, handling uncertainties, and addressing bias in statistical transport problems."}}
{"id": "2507.01305", "pdf": "https://arxiv.org/pdf/2507.01305", "abs": "https://arxiv.org/abs/2507.01305", "authors": ["Worameth Chinchuthakun", "Pakkapon Phongthawee", "Amit Raj", "Varun Jampani", "Pramook Khungurn", "Supasorn Suwajanakorn"], "title": "DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting", "categories": ["cs.CV", "cs.GR", "cs.LG", "I.3.3; I.4.8"], "comment": "arXiv admin note: substantial text overlap with arXiv:2312.09168", "summary": "We introduce a simple yet effective technique for estimating lighting from a\nsingle low-dynamic-range (LDR) image by reframing the task as a chrome ball\ninpainting problem. This approach leverages a pre-trained diffusion model,\nStable Diffusion XL, to overcome the generalization failures of existing\nmethods that rely on limited HDR panorama datasets. While conceptually simple,\nthe task remains challenging because diffusion models often insert incorrect or\ninconsistent content and cannot readily generate chrome balls in HDR format.\nOur analysis reveals that the inpainting process is highly sensitive to the\ninitial noise in the diffusion process, occasionally resulting in unrealistic\noutputs. To address this, we first introduce DiffusionLight, which uses\niterative inpainting to compute a median chrome ball from multiple outputs to\nserve as a stable, low-frequency lighting prior that guides the generation of a\nhigh-quality final result. To generate high-dynamic-range (HDR) light probes,\nan Exposure LoRA is fine-tuned to create LDR images at multiple exposure\nvalues, which are then merged. While effective, DiffusionLight is\ntime-intensive, requiring approximately 30 minutes per estimation. To reduce\nthis overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to\nabout 30 seconds with minimal quality loss. This 60x speedup is achieved by\ntraining a Turbo LoRA to directly predict the averaged chrome balls from the\niterative process. Inference is further streamlined into a single denoising\npass using a LoRA swapping technique. Experimental results that show our method\nproduces convincing light estimates across diverse settings and demonstrates\nsuperior generalization to in-the-wild scenarios. Our code is available at\nhttps://diffusionlight.github.io/turbo", "AI": {"tldr": "The paper presents DiffusionLight, a novel method for estimating lighting from a single LDR image, reframing the task as a chrome ball inpainting problem and employing the Stable Diffusion XL model.", "motivation": "Existing methods for lighting estimation struggle with generalization due to reliance on limited HDR panorama datasets, necessitating a robust approach that can adapt to diverse settings.", "method": "The approach involves iterative inpainting to generate a median chrome ball for stable lighting priors (DiffusionLight). To speed up the process, Turbo LoRA is utilized (DiffusionLight-Turbo) to predict averaged chrome balls in a single denoising pass.", "result": "The method achieves convincing light estimates across diverse scenarios with a significant speed improvement: from approximately 30 minutes to 30 seconds per estimation with minimal quality degradation.", "conclusion": "DiffusionLight and its accelerated version (Turbo) offer a robust and practical solution to lighting estimation with superior generalization capabilities and significant runtime efficiency."}}
{"id": "2507.01037", "pdf": "https://arxiv.org/pdf/2507.01037", "abs": "https://arxiv.org/abs/2507.01037", "authors": ["Wenbin Ouyang", "Sirui Li", "Yining Ma", "Cathy Wu"], "title": "Learning to Segment for Vehicle Routing Problems", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Iterative search heuristics are widely recognized as state-of-the-art for\nsolving Vehicle Routing Problems (VRPs). In this work, we identify and exploit\na critical observation: within these solvers, a large portion of the solution\nremains stable, i.e., unchanged across search iterations, causing redundant\ncomputations, especially for large-scale VRPs with long subtours. To address\nthis, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)\ndecomposition technique to accelerate iterative solvers. Specifically, FSTA\npreserves stable solution segments during the search, aggregates nodes within\neach segment into fixed hypernodes, and focuses the search only on unstable\nportions. Yet, a key challenge lies in identifying which segments should be\naggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),\na novel neural framework to intelligently differentiate potentially stable and\nunstable portions for FSTA decomposition. We present three L2Seg variants:\nnon-autoregressive (globally comprehensive but locally indiscriminate),\nautoregressive (locally refined but globally deficient), and their synergy,\nwith bespoke training and inference strategies. Empirical results on CVRP and\nVRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up\nto 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy\nachieves best performance by combining their complementary strengths. Notably,\nL2Seg is a flexible framework that is compatible with traditional,\nlearning-based, and hybrid solvers, while supporting a broad class of VRPs.", "AI": {"tldr": "The study presents Learning-to-Segment (L2Seg), a neural framework aiding First-Segment-Then-Aggregate (FSTA) decomposition for iterative solvers, accelerating solution times for Vehicle Routing Problems (VRPs up to 7x.", "motivation": "The paper aims to address redundant computations in iterative heuristics for large-scale VRPs, where stable solution portions remain unchanged through iterations.", "method": "The authors introduce the FSTA technique, preserving stable solution segments and aggregating nodes into hypernodes, while employing L2Seg\u2014a neural framework\u2014to differentiate stable and unstable sections for efficient decomposition.", "result": "Empirical results on Capacitated VRP (CVRP) and VRP with Time Windows (VRPTW) demonstrate up to 7x acceleration using L2Seg integration.", "conclusion": "L2Seg enhances iterative solvers across different VRP types by leveraging stability detection, with notable compatibility across solvers, providing significant computation acceleration."}}
{"id": "2507.01284", "pdf": "https://arxiv.org/pdf/2507.01284", "abs": "https://arxiv.org/abs/2507.01284", "authors": ["Cristian Gariboldi", "Hayato Tokida", "Ken Kinjo", "Yuki Asada", "Alexander Carballo"], "title": "VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.ET", "cs.LG"], "comment": "2025 IEEE 28th International Conference on Intelligent Transportation\n  Systems (ITSC)", "summary": "Recent advancements in open-source Visual Language Models (VLMs) such as\nLLaVA, Qwen-VL, and Llama have catalyzed extensive research on their\nintegration with diverse systems. The internet-scale general knowledge\nencapsulated within these models presents significant opportunities for\nenhancing autonomous driving perception, prediction, and planning capabilities.\nIn this paper we propose VLAD, a vision-language autonomous driving model,\nwhich integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end\nsystem. We implement a specialized fine-tuning approach using custom\nquestion-answer datasets designed specifically to improve the spatial reasoning\ncapabilities of the model. The enhanced VLM generates high-level navigational\ncommands that VAD subsequently processes to guide vehicle operation.\nAdditionally, our system produces interpretable natural language explanations\nof driving decisions, thereby increasing transparency and trustworthiness of\nthe traditionally black-box end-to-end architecture. Comprehensive evaluation\non the real-world nuScenes dataset demonstrates that our integrated system\nreduces average collision rates by 31.82% compared to baseline methodologies,\nestablishing a new benchmark for VLM-augmented autonomous driving systems.", "AI": {"tldr": "This paper introduces VLAD, a vision-language autonomous driving model, which integrates fine-tuned VLMs with a state-of-the-art driving system to improve navigation, spatial reasoning, and transparency.", "motivation": "To enhance autonomous driving systems by leveraging the general knowledge of Visual Language Models to improve perception, prediction, and planning.", "method": "The authors fine-tuned an existing Vision-Language Model (VLM) using custom question-answer datasets designed for spatial reasoning and integrated it with VAD, a leading end-to-end driving system. The model generates navigation commands and offers natural language explanations for increased transparency.", "result": "VLAD demonstrated a 31.82% reduction in collision rates compared to baseline approaches in tests conducted on the nuScenes dataset.", "conclusion": "The integration of VLMs significantly improves the performance, transparency, and trustworthiness of autonomous driving systems, setting a new benchmark in the field."}}
{"id": "2507.01453", "pdf": "https://arxiv.org/pdf/2507.01453", "abs": "https://arxiv.org/abs/2507.01453", "authors": ["Michelle Yeo", "Haoqian Zhang"], "title": "Rational Censorship Attack: Breaking Blockchain with a Blackboard", "categories": ["cs.GT", "cs.CR", "cs.DC"], "comment": null, "summary": "Censorship resilience is a fundamental assumption underlying the security of\nblockchain protocols. Additionally, the analysis of blockchain security from an\neconomic and game theoretic perspective has been growing in popularity in\nrecent years. In this work, we present a surprising rational censorship attack\non blockchain censorship resilience when we adopt the analysis of blockchain\nsecurity from a game theoretic lens and assume all users are rational. In our\nattack, a colluding group with sufficient voting power censors the remainder\nnodes such that the group alone can gain all the rewards from maintaining the\nblockchain. We show that if nodes are rational, coordinating this attack just\nrequires a public read and write blackboard and we formally model the attack\nusing a game theoretic framework. Furthermore, we note that to ensure the\nsuccess of the attack, nodes need to know the total true voting power held by\nthe colluding group. We prove that the strategy to join the rational censorship\nattack and also for nodes to honestly declare their power is a subgame perfect\nequilibrium in the corresponding extensive form game induced by our attack.\nFinally, we discuss the implications of the attack on blockchain users and\nprotocol designers as well as some potential countermeasures.", "AI": {"tldr": "The paper identifies a rational censorship attack on blockchain resilience using game theory, where a colluding group monopolizes rewards by censoring other nodes.", "motivation": "Blockchain security is frequently analyzed from an economic and game-theoretic perspective. This paper seeks to understand how rational behaviors compromise censorship resilience.", "method": "The authors model the attack as a game-theoretic framework, wherein nodes collaborate through a public blackboard and rationally report their voting power.", "result": "The strategy of joining the censorship attack and truthfully reporting voting power is proven as subgame perfect equilibrium.", "conclusion": "The attack reveals vulnerabilities in blockchain design, demanding countermeasures to ensure fairness and resilience against rational collusion."}}
{"id": "2507.01833", "pdf": "https://arxiv.org/pdf/2507.01833", "abs": "https://arxiv.org/abs/2507.01833", "authors": ["Yi-Dong Shen", "Thomas Eiter"], "title": "Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics", "categories": ["cs.AI"], "comment": "76 pages. This article is a significantly extended version of a paper\n  presented by the authors at IJCAI-2022", "summary": "Non-monotonic logic programming is the basis for a declarative problem\nsolving paradigm known as answer set programming (ASP). Departing from the\nseminal definition by Gelfond and Lifschitz in 1988 for simple normal logic\nprograms, various answer set semantics have been proposed for extensions. We\nconsider two important questions: (1) Should the minimal model property,\nconstraint monotonicity and foundedness as defined in the literature be\nmandatory conditions for an answer set semantics in general? (2) If not, what\nother properties could be considered as general principles for answer set\nsemantics? We address the two questions. First, it seems that the three\naforementioned conditions may sometimes be too strong, and we illustrate with\nexamples that enforcing them may exclude expected answer sets. Second, we\nevolve the Gelfond answer set (GAS) principles for answer set construction by\nrefining the Gelfond's rationality principle to well-supportedness, minimality\nw.r.t. negation by default and minimality w.r.t. epistemic negation. The\nprinciple of well-supportedness guarantees that every answer set is\nconstructible from if-then rules obeying a level mapping and is thus free of\ncircular justification, while the two minimality principles ensure that the\nformalism minimizes knowledge both at the level of answer sets and of world\nviews. Third, to embody the refined GAS principles, we extend the notion of\nwell-supportedness substantially to answer sets and world views, respectively.\nFourth, we define new answer set semantics in terms of the refined GAS\nprinciples. Fifth, we use the refined GAS principles as an alternative baseline\nto intuitively assess the existing answer set semantics. Finally, we analyze\nthe computational complexity.", "AI": {"tldr": "The paper refines and extends principles for answer set programming semantics, questioning traditional constraints while proposing alternative, generalized properties.", "motivation": "Revisiting foundational conditions of answer set semantics and proposing refined principles for enhanced flexibility in non-monotonic logic programming.", "method": "Illustrates examples to challenge existing constraints, introduces well-supportedness and minimality principles, extends GAS principles, defines new semantics, and assesses computational complexity.", "result": "Refined GAS principles address issues with traditional constraints, provide a flexible semantics framework, and offer insights into computational complexity.", "conclusion": "The refined GAS principles and new semantics improve the philosophical and computational organization of answer set programming, while also functioning as a baseline for evaluating logic programming frameworks."}}
{"id": "2507.01334", "pdf": "https://arxiv.org/pdf/2507.01334", "abs": "https://arxiv.org/abs/2507.01334", "authors": ["Nifu Dan", "Yujun Cai", "Yiwei Wang"], "title": "Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains.", "AI": {"tldr": "The study explores using advanced instruction-tuned models like Deepseek-R1 to solve physics problems from the SciBench benchmark, achieving state-of-the-art accuracy and emphasizing symbolic derivation.", "motivation": "The authors aim to address the challenge of applying Large Language Models to physics reasoning, which requires deep conceptual understanding and strong problem-solving skills.", "method": "They employ advanced instruction-tuned reasoning models, such as Deepseek-R1, and experiment with few-shot prompting to solve diverse physics problems from the SciBench benchmark.", "result": "The models achieve state-of-the-art accuracy in physics problem-solving and highlight distinctive reasoning patterns focused on symbolic derivation.", "conclusion": "The research demonstrates the potential of instruction-tuned reasoning models in physics and shows that few-shot prompting can significantly enhance their performance, allowing room for further improvement."}}
{"id": "2507.01726", "pdf": "https://arxiv.org/pdf/2507.01726", "abs": "https://arxiv.org/abs/2507.01726", "authors": ["Hang Zou", "Martin Rahm", "Anton Frisk Kockum", "Simon Olsson"], "title": "Generative flow-based warm start of the variational quantum eigensolver", "categories": ["quant-ph", "physics.chem-ph", "stat.ML"], "comment": "20 pages; 8 figures", "summary": "Hybrid quantum-classical algorithms like the variational quantum eigensolver\n(VQE) show promise for quantum simulations on near-term quantum devices, but\nare often limited by complex objective functions and expensive optimization\nprocedures. Here, we propose Flow-VQE, a generative framework leveraging\nconditional normalizing flows with parameterized quantum circuits to\nefficiently generate high-quality variational parameters. By embedding a\ngenerative model into the VQE optimization loop through preference-based\ntraining, Flow-VQE enables quantum gradient-free optimization and offers a\nsystematic approach for parameter transfer, accelerating convergence across\nrelated problems through warm-started optimization. We compare Flow-VQE to a\nnumber of standard benchmarks through numerical simulations on molecular\nsystems, including hydrogen chains, water, ammonia, and benzene. We find that\nFlow-VQE outperforms baseline optimization algorithms, achieving computational\naccuracy with fewer circuit evaluations (improvements range from modest to more\nthan two orders of magnitude) and, when used to warm-start the optimization of\nnew systems, accelerates subsequent fine-tuning by up to 50-fold compared with\nHartree--Fock initialization. Therefore, we believe Flow-VQE can become a\npragmatic and versatile paradigm for leveraging generative modeling to reduce\nthe costs of variational quantum algorithms.", "AI": {"tldr": "The paper introduces Flow-VQE, a framework using generative modeling to improve parameter generation and optimization in variational quantum algorithms, showing significant efficiency improvements in simulations.", "motivation": "Current variational quantum eigensolver (VQE) approaches face challenges due to complex objective functions and costly optimization processes, necessitating innovative methods to make quantum simulations more efficient.", "method": "The method integrates conditional normalizing flows with parameterized quantum circuits in VQE. It utilizes generative models with preference-based training, enabling gradient-free optimization and systematic parameter transfer, which facilitates quicker convergence across related problems.", "result": "Numerical simulations on molecular systems demonstrate that Flow-VQE surpasses traditional optimization algorithms by achieving computational accuracy with fewer circuit evaluations and accelerating optimization up to 50 times compared to Hartree-Fock initialization.", "conclusion": "Flow-VQE is a promising paradigm that leverages generative modeling for quantum optimization, reducing the computational cost of variational quantum algorithms, and shows potential for widespread application in quantum simulations."}}
{"id": "2507.01340", "pdf": "https://arxiv.org/pdf/2507.01340", "abs": "https://arxiv.org/abs/2507.01340", "authors": ["Cuong Le", "Huy-Phuong Le", "Duc Le", "Minh-Thien Duong", "Van-Binh Nguyen", "My-Ha Le"], "title": "Physics-informed Ground Reaction Dynamics from Human Motion Capture", "categories": ["cs.CV"], "comment": "6 pages, 4 figures, 4 tables, HSI 2025", "summary": "Body dynamics are crucial information for the analysis of human motions in\nimportant research fields, ranging from biomechanics, sports science to\ncomputer vision and graphics. Modern approaches collect the body dynamics,\nexternal reactive force specifically, via force plates, synchronizing with\nhuman motion capture data, and learn to estimate the dynamics from a black-box\ndeep learning model. Being specialized devices, force plates can only be\ninstalled in laboratory setups, imposing a significant limitation on the\nlearning of human dynamics. To this end, we propose a novel method for\nestimating human ground reaction dynamics directly from the more reliable\nmotion capture data with physics laws and computational simulation as\nconstrains. We introduce a highly accurate and robust method for computing\nground reaction forces from motion capture data using Euler's integration\nscheme and PD algorithm. The physics-based reactive forces are used to inform\nthe learning model about the physics-informed motion dynamics thus improving\nthe estimation accuracy. The proposed approach was tested on the GroundLink\ndataset, outperforming the baseline model on: 1) the ground reaction force\nestimation accuracy compared to the force plates measurement; and 2) our\nsimulated root trajectory precision. The implementation code is available at\nhttps://github.com/cuongle1206/Phys-GRD", "AI": {"tldr": "The paper presents a method to estimate human ground reaction dynamics directly from motion capture data using physics constraints, eliminating the need for force plates.", "motivation": "Force plates, commonly used for collecting human ground reaction dynamics, are limited to lab setups, restricting the study of human motion dynamics.", "method": "The authors employ physics laws and computational simulations with Euler's integration scheme and PD algorithm to estimate ground reaction forces from motion capture data.", "result": "The approach outperforms baseline models in estimating ground reaction force accuracy and simulated root trajectory precision on the GroundLink dataset.", "conclusion": "By leveraging physics-informed simulations, the method provides a robust and accurate alternative to force plates for studying human motion dynamics, expanding possibilities beyond laboratory setups."}}
{"id": "2507.01039", "pdf": "https://arxiv.org/pdf/2507.01039", "abs": "https://arxiv.org/abs/2507.01039", "authors": ["Kaaustaaub Shankar", "Wilhelm Louw", "Kelly Cohen"], "title": "On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to NAFIPS 2025", "summary": "We propose a reinforcement learning (RL) approach for training neuro-fuzzy\ncontrollers using Proximal Policy Optimization (PPO). Building on prior work\nthat applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),\nour method replaces the off-policy value-based framework with a stable\non-policy actor-critic loop. We evaluate this approach in the CartPole-v1\nenvironment using multiple random seeds and compare its learning performance\nagainst ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained\nfuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000\nupdates, showcasing less variance than prior DQN-based methods during training\nand overall faster convergence. These findings suggest that PPO offers a\npromising pathway for training explainable neuro-fuzzy controllers in\nreinforcement learning tasks.", "AI": {"tldr": "The paper introduces a reinforcement learning approach using Proximal Policy Optimization (PPO) to train neuro-fuzzy controllers, achieving faster and more stable performance compared to Deep Q-Network (DQN)-based methods.", "motivation": "The motivation is to improve the training stability and performance of neuro-fuzzy controllers in reinforcement learning by using an on-policy actor-critic method instead of the prior off-policy value-based learning.", "method": "The authors implement PPO to train fuzzy controllers, evaluating its performance in the CartPole-v1 environment and comparing it to ANFIS-DQN baselines.", "result": "PPO-trained fuzzy agents achieved better stability (reduced training variance) and faster convergence, with a mean return of 500 +/- 0 on CartPole-v1 after 20,000 updates.", "conclusion": "The study concludes that PPO is a promising framework for training explainable neuro-fuzzy controllers in reinforcement learning, outperforming prior DQN-based approaches."}}
{"id": "2507.01308", "pdf": "https://arxiv.org/pdf/2507.01308", "abs": "https://arxiv.org/abs/2507.01308", "authors": ["Muhammad Atta ur Rahman", "Dooseop Choi", "KyoungWook Min"], "title": "LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at the 17th IEEE International Conference on Advanced\n  Computational Intelligence (ICACI 2025)", "summary": "Accurate motion forecasting is critical for safe and efficient autonomous\ndriving, enabling vehicles to predict future trajectories and make informed\ndecisions in complex traffic scenarios. Most of the current designs of motion\nprediction models are based on the major representation of lane centerlines,\nwhich limits their capability to capture critical road environments and traffic\nrules and constraints. In this work, we propose an enhanced motion forecasting\nmodel informed by multiple vector map elements, including lane boundaries and\nroad edges, that facilitates a richer and more complete representation of\ndriving environments. An effective feature fusion strategy is developed to\nmerge information in different vector map components, where the model learns\nholistic information on road structures and their interactions with agents.\nSince encoding more information about the road environment increases memory\nusage and is computationally expensive, we developed an effective pruning\nmechanism that filters the most relevant map connections to the target agent,\nensuring computational efficiency while maintaining essential spatial and\nsemantic relationships for accurate trajectory prediction. Overcoming the\nlimitations of lane centerline-based models, our method provides a more\ninformative and efficient representation of the driving environment and\nadvances the state of the art for autonomous vehicle motion forecasting. We\nverify our approach with extensive experiments on the Argoverse 2 motion\nforecasting dataset, where our method maintains competitiveness on AV2 while\nachieving improved performance.\n  Index Terms-Autonomous driving, trajectory prediction, vector map elements,\nroad topology, connection pruning, Argoverse 2.", "AI": {"tldr": "The paper introduces a motion forecasting model for autonomous driving, leveraging multiple vector map elements beyond lane centerlines to enhance environmental representation and trajectory prediction accuracy.", "motivation": "The motivation is to address limitations in current trajectory prediction models for autonomous driving, which rely heavily on lane centerlines and fail to fully capture road environments and constraints.", "method": "The proposed method incorporates multiple vector map elements (lane boundaries, road edges) and uses a feature fusion strategy to combine data while employing a pruning mechanism to manage computational costs.", "result": "Extensive experiments on the Argoverse 2 dataset demonstrate the model's competitiveness and improved trajectory prediction performance compared to lane centerline-based approaches.", "conclusion": "The enhanced model substantially improves environmental representation for motion forecasting, advancing the state of the art and providing a computationally efficient solution."}}
{"id": "2507.01770", "pdf": "https://arxiv.org/pdf/2507.01770", "abs": "https://arxiv.org/abs/2507.01770", "authors": ["Guanglu Zhang", "Qihang Shan", "Jonathan Cagan"], "title": "GPU-based complete search for nonlinear minimization subject to bounds", "categories": ["math.NA", "cs.AI", "cs.DC", "cs.MS", "cs.NA", "math.OC", "65G20, 65G30, 65G40, 90C06, 90C26, 90C30", "G.1.6; G.4"], "comment": "36 pages, 3 figures", "summary": "This paper introduces a GPU-based complete search method to enclose the\nglobal minimum of a nonlinear function subject to simple bounds on the\nvariables. Using interval analysis, coupled with the computational power and\narchitecture of GPU, the method iteratively rules out the regions in the search\ndomain where the global minimum cannot exist and leaves a finite set of regions\nwhere the global minimum must exist. For effectiveness, because of the rigor of\ninterval analysis, the method is guaranteed to enclose the global minimum of\nthe nonlinear function even in the presence of rounding errors. For efficiency,\nthe method employs a novel GPU-based single program, single data parallel\nprogramming style to circumvent major GPU performance bottlenecks, and a\nvariable cycling technique is also integrated into the method to reduce\ncomputational cost when minimizing large-scale nonlinear functions. The method\nis validated by minimizing 10 multimodal benchmark test functions with scalable\ndimensions, including the well-known Ackley function, Griewank function, Levy\nfunction, and Rastrigin function. These benchmark test functions represent\ngrand challenges of global optimization, and enclosing the guaranteed global\nminimum of these benchmark test functions with more than 80 dimensions has not\nbeen reported in the literature. Our method completely searches the feasible\ndomain and successfully encloses the guaranteed global minimum of these 10\nbenchmark test functions with up to 10,000 dimensions using only one GPU in a\nreasonable computation time, far exceeding the reported results in the\nliterature due to the unique method design and implementation based on GPU\narchitecture.", "AI": {"tldr": "This paper proposes a GPU-based interval analysis method to rigorously find the global minimum of nonlinear functions efficiently.", "motivation": "Current methods for global optimization struggle to enclose the guaranteed global minimum for complex multimodal functions in high dimensions. GPUs offer computational power that can overcome these challenges.", "method": "The authors use interval analysis on GPU architecture to iteratively eliminate infeasible regions while employing parallel programming and a variable cycling technique for efficiency in high-dimensional contexts.", "result": "The method successfully found the guaranteed global minimum of 10 challenging benchmark test functions with dimensionalities up to 10,000 using a single GPU efficiently.", "conclusion": "By leveraging GPU power and novel design techniques, the proposed method sets a new benchmark for solving complex global optimization problems rigorously and efficiently."}}
{"id": "2507.01018", "pdf": "https://arxiv.org/pdf/2507.01018", "abs": "https://arxiv.org/abs/2507.01018", "authors": ["Mohammed K. Alzaylaee"], "title": "A Systematic Review of Security Vulnerabilities in Smart Home Devices and Mitigation Techniques", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Smart homes that integrate Internet of Things (IoT) devices face increasing\ncybersecurity risks, posing significant challenges to these environments. The\nstudy explores security threats in smart homes ecosystems, categorizing them\ninto vulnerabilities at the network layer, device level, and those from\ncloud-based and AI-driven systems. Research findings indicate that post-quantum\nencryption, coupled with AI-driven anomaly detection, is highly effective in\nenhancing security; however, computational resource demands present significant\nchallenges. Blockchain authentication together with zero-trust structures\nbuilds security resilience, although they need changes to existing\ninfrastructure. The specific security strategies show their effectiveness\nthrough ANOVA, Chi-square tests, and Monte Carlo simulations yet lack\nsufficient scalability according to the results. The research demonstrates the\nrequirement for improvement in cryptographic techniques, alongside AI-enhanced\nthreat detection and adaptive security models which must achieve a balance\nbetween performance and efficiency and real-time applicability within smart\nhome ecosystems.", "AI": {"tldr": "Increasing cybersecurity risks in smart homes require enhanced cryptographic techniques and AI-driven security strategies, but scalability and resource demands remain challenges.", "motivation": "The paper addresses the pressing need to tackle cybersecurity risks in smart homes integrating IoT devices.", "method": "The study categorizes security threats and analyzes strategies like post-quantum encryption, AI anomaly detection, blockchain authentication, and zero-trust models using statistical techniques and simulations.", "result": "Post-quantum encryption and AI-driven detection enhance security but demand high resources. Blockchain and zero-trust models improve resilience but face infrastructure challenges.", "conclusion": "There is a need to refine cryptographic and AI techniques for balancing real-time applicability and efficiency in smart home ecosystems."}}
{"id": "2507.01335", "pdf": "https://arxiv.org/pdf/2507.01335", "abs": "https://arxiv.org/abs/2507.01335", "authors": ["Xunjian Yin", "Sitao Cheng", "Yuxi Xie", "Xinyu Hu", "Li Lin", "Xinyi Wang", "Liangming Pan", "William Yang Wang", "Xiaojun Wan"], "title": "LEDOM: An Open and Fundamental Reverse Language Model", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "We introduce LEDOM, the first purely reverse language model, trained\nautoregressively on 435B tokens with 2B and 7B parameter variants, which\nprocesses sequences in reverse temporal order through previous token\nprediction. For the first time, we present the reverse language model as a\npotential foundational model across general tasks, accompanied by a set of\nintriguing examples and insights. Based on LEDOM, we further introduce a novel\napplication: Reverse Reward, where LEDOM-guided reranking of forward language\nmodel outputs leads to substantial performance improvements on mathematical\nreasoning tasks. This approach leverages LEDOM's unique backward reasoning\ncapability to refine generation quality through posterior evaluation. Our\nfindings suggest that LEDOM exhibits unique characteristics with broad\napplication potential. We will release all models, training code, and\npre-training data to facilitate future research.", "AI": {"tldr": "LEDOM is a reverse language model trained autoregressively on substantial data, showcasing its utility as a foundational model and improving mathematical reasoning tasks through a novel Reverse Reward application.", "motivation": "To explore the potential of reverse language models as foundational models for general tasks and their unique properties in improving specific applications like mathematical reasoning.", "method": "Developed LEDOM, a reverse autoregressive language model, and proposed the Reverse Reward approach, which refines forward language model outputs via LEDOM-guided reranking.", "result": "LEDOM demonstrated improved performance on mathematical reasoning tasks by leveraging its backward reasoning capabilities through a posterior evaluation framework.", "conclusion": "LEDOM's backward reasoning provides unique advantages with broad applications, and the release of its resources aims to foster further research and innovation."}}
{"id": "2507.01761", "pdf": "https://arxiv.org/pdf/2507.01761", "abs": "https://arxiv.org/abs/2507.01761", "authors": ["Nicolas Salvy", "Hugues Talbot", "Bertrand Thirion"], "title": "Enhanced Generative Model Evaluation with Clipped Density and Coverage", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Although generative models have made remarkable progress in recent years,\ntheir use in critical applications has been hindered by their incapacity to\nreliably evaluate sample quality. Quality refers to at least two complementary\nconcepts: fidelity and coverage. Current quality metrics often lack reliable,\ninterpretable values due to an absence of calibration or insufficient\nrobustness to outliers. To address these shortcomings, we introduce two novel\nmetrics, Clipped Density and Clipped Coverage. By clipping individual sample\ncontributions and, for fidelity, the radii of nearest neighbor balls, our\nmetrics prevent out-of-distribution samples from biasing the aggregated values.\nThrough analytical and empirical calibration, these metrics exhibit linear\nscore degradation as the proportion of poor samples increases. Thus, they can\nbe straightforwardly interpreted as equivalent proportions of good samples.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nClipped Density and Clipped Coverage outperform existing methods in terms of\nrobustness, sensitivity, and interpretability for evaluating generative models.", "AI": {"tldr": "The authors propose two new metrics, Clipped Density and Clipped Coverage, to improve the evaluation of generative model samples, addressing issues with fidelity, coverage, calibration, and robustness.", "motivation": "Generative models require reliable metrics to assess the quality of their samples, as current methods are often not calibrated and sensitive to outliers.", "method": "The authors introduce Clipped Density and Clipped Coverage metrics, which clip sample contributions and leverage calibration to reduce biases from out-of-distribution samples.", "result": "Analytical and empirical calibration show that the metrics degrade linearly with poor sample proportions, and experimental evaluations confirm they outperform existing quality metrics.", "conclusion": "Clipped Density and Clipped Coverage offer robust, sensitive, and interpretable metrics for generative model evaluation, marking an improvement over traditional methods."}}
{"id": "2507.01342", "pdf": "https://arxiv.org/pdf/2507.01342", "abs": "https://arxiv.org/abs/2507.01342", "authors": ["Luxi Zhao", "Mahmoud Afifi", "Michael S. Brown"], "title": "Learning Camera-Agnostic White-Balance Preferences", "categories": ["cs.CV"], "comment": null, "summary": "The image signal processor (ISP) pipeline in modern cameras consists of\nseveral modules that transform raw sensor data into visually pleasing images in\na display color space. Among these, the auto white balance (AWB) module is\nessential for compensating for scene illumination. However, commercial AWB\nsystems often strive to compute aesthetic white-balance preferences rather than\naccurate neutral color correction. While learning-based methods have improved\nAWB accuracy, they typically struggle to generalize across different camera\nsensors -- an issue for smartphones with multiple cameras. Recent work has\nexplored cross-camera AWB, but most methods remain focused on achieving neutral\nwhite balance. In contrast, this paper is the first to address aesthetic\nconsistency by learning a post-illuminant-estimation mapping that transforms\nneutral illuminant corrections into aesthetically preferred corrections in a\ncamera-agnostic space. Once trained, our mapping can be applied after any\nneutral AWB module to enable consistent and stylized color rendering across\nunseen cameras. Our proposed model is lightweight -- containing only $\\sim$500\nparameters -- and runs in just 0.024 milliseconds on a typical flagship mobile\nCPU. Evaluated on a dataset of 771 smartphone images from three different\ncameras, our method achieves state-of-the-art performance while remaining fully\ncompatible with existing cross-camera AWB techniques, introducing minimal\ncomputational and memory overhead.", "AI": {"tldr": "This paper introduces a lightweight post-illuminant-estimation mapping for consistent and aesthetic white balance corrections across cameras, achieving state-of-the-art results with minimal computational overhead.", "motivation": "Modern image signal processors often focus on aesthetic white-balance preferences rather than accurate correction, resulting in challenges for devices like smartphones with multiple cameras.", "method": "The authors propose a camera-agnostic post-illuminant-estimation mapping that works after an initial neutral AWB module, ensuring aesthetic and stylized color rendering consistently across various camera systems.", "result": "The proposed model achieves state-of-the-art performance on a dataset of 771 images across three smartphone cameras, requiring only ~500 parameters and negligible computational resources (0.024 ms on flagship CPUs).", "conclusion": "This novel approach advances cross-camera auto white balance techniques by enabling aesthetic consistency while being lightweight and compatible with existing AWB systems."}}
{"id": "2507.01424", "pdf": "https://arxiv.org/pdf/2507.01424", "abs": "https://arxiv.org/abs/2507.01424", "authors": ["Zhenyang Liu", "Yongchong Gu", "Sixiao Zheng", "Xiangyang Xue", "Yanwei Fu"], "title": "TriVLA: A Unified Triple-System-Based Unified Vision-Language-Action Model for General Robot Control", "categories": ["cs.RO"], "comment": null, "summary": "Recent advancements in vision-language models (VLMs) for common-sense\nreasoning have led to the development of vision-language-action (VLA) models,\nenabling robots to perform generalized manipulation. Although existing\nautoregressive VLA methods design a specific architecture like dual-system to\nleverage large-scale pretrained knowledge, they tend to capture static\ninformation, often neglecting the dynamic aspects vital for embodied tasks. To\nthis end, we propose TriVLA, a unified Vision-Language-Action model with a\ntriple-system architecture for general robot control. The vision-language\nmodule (System 2) interprets the environment through vision and language\ninstructions. The dynamics perception module (System 3) inherently produces\nvisual representations that encompass both current static information and\npredicted future dynamics, thereby providing valuable guidance for policy\nlearning. TriVLA utilizes pre-trained VLM model and fine-tunes pre-trained\nvideo foundation model on robot datasets along with internet human manipulation\ndata. The subsequent policy learning module (System 1) generates fluid motor\nactions in real time. Experimental evaluation demonstrates that TriVLA operates\nat approximately 36 Hz and surpasses state-of-the-art imitation learning\nbaselines on standard simulation benchmarks as well as challenging real-world\nmanipulation tasks.", "AI": {"tldr": "TriVLA introduces a triple-system vision-language-action (VLA) model that incorporates dynamic perception for advanced robot manipulation, outperforming state-of-the-art methods.", "motivation": "Current autoregressive VLA models emphasize static information but fail to adapt to dynamic situations crucial for embodied robot tasks.", "method": "TriVLA combines pre-trained vision-language and video foundation models, enhances dynamic perception, and uses real-time policy learning to generate motor actions.", "result": "TriVLA achieves 36 Hz operation speed and outperforms imitation learning baselines in simulations and real-world manipulation tasks.", "conclusion": "The proposed triple-system architecture of TriVLA provides significant advancements in robotic control, ensuring better dynamic manipulation and fluid action generation."}}
{"id": "2507.01902", "pdf": "https://arxiv.org/pdf/2507.01902", "abs": "https://arxiv.org/abs/2507.01902", "authors": ["Grier M. Jones", "Hans-Arno Jacobsen"], "title": "Analyzing Common Electronic Structure Theory Algorithms for Distributed Quantum Computing", "categories": ["quant-ph", "cs.DC", "physics.chem-ph"], "comment": null, "summary": "To move towards the utility era of quantum computing, many corporations have\nposed distributed quantum computing (DQC) as a framework for scaling the\ncurrent generation of devices for practical applications. One of these\napplications is quantum chemistry, also known as electronic structure theory,\nwhich has been poised as a \"killer application\" of quantum computing, To this\nend, we analyze five electronic structure methods, found in common packages\nsuch as Tequila and ffsim, which can be easily interfaced with the Qiskit\nCircuit Cutting addon. Herein, we provide insights into cutting these\nalgorithms using local operations (LO) to determine their aptitude for\ndistribution. The key findings of our work are that many of these algorithms\ncannot be efficiently parallelized using LO, and new methods must be developed\nto apply electronic structure theory within a DQC framework.", "AI": {"tldr": "This paper examines the feasibility of applying distributed quantum computing (DQC) to electronic structure theory methods and finds that current algorithms struggle with efficient parallelization using local operations.", "motivation": "To explore the potential of distributed quantum computing (DQC) for scaling quantum devices to practical applications, particularly in the field of quantum chemistry, which is a leading prospective application for quantum computing.", "method": "The authors analyzed five common electronic structure methods, using tools like the Tequila and ffsim packages, in conjunction with the Qiskit Circuit Cutting addon. They investigated the methods' ability to efficiently parallelize using local operations (LO).", "result": "The study found that the analyzed algorithms are largely unsuitable for efficient parallelization using local operations, highlighting a significant gap in applying electronic structure theory to DQC.", "conclusion": "New algorithms or methodologies need to be developed to enable the distribution of electronic structure theory computations effectively within a DQC framework."}}
{"id": "2507.01024", "pdf": "https://arxiv.org/pdf/2507.01024", "abs": "https://arxiv.org/abs/2507.01024", "authors": ["George Igwegbe", "Martins Awojide", "Mboh Bless", "Nirel Kadzo"], "title": "Hello Afrika: Speech Commands in Kinyarwanda", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": "Data Science Africa, 2024", "summary": "Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a\nlanguage which are essential for non-contact control of and activation of\nlarger AI systems in devices used in everyday life especially for persons with\ndisabilities. Currently, there is a dearth of speech command models for African\nlanguages. The Hello Afrika project aims to address this issue and its first\niteration is focused on the Kinyarwanda language since the country has shown\ninterest in developing speech recognition technologies culminating in one of\nthe largest datasets on Mozilla Common Voice. The model was built off a custom\nspeech command corpus made up of general directives, numbers, and a wake word.\nThe final model was deployed on multiple devices (PC, Mobile Phone and Edge\nDevices) and the performance was assessed using suitable metrics.", "AI": {"tldr": "This paper discusses the creation of speech command models tailored for African languages, with a focus on Kinyarwanda, using a custom speech corpus and testing it across devices.", "motivation": "To address the scarcity of speech command models for African languages and support accessibility, especially for individuals with disabilities.", "method": "Developed a speech command model for Kinyarwanda based on a custom corpus containing directives, numbers, and wake words, followed by deployment on various devices.", "result": "The model showed measurable performance on PC, mobile phones, and edge devices, evaluated using appropriate metrics.", "conclusion": "The project succeeded in creating and assessing a speech command model for Kinyarwanda, demonstrating practical applications for African language AI systems."}}
{"id": "2507.01352", "pdf": "https://arxiv.org/pdf/2507.01352", "abs": "https://arxiv.org/abs/2507.01352", "authors": ["Chris Yuhao Liu", "Liang Zeng", "Yuzhen Xiao", "Jujie He", "Jiacai Liu", "Chaojie Wang", "Rui Yan", "Wei Shen", "Fuxiang Zhang", "Jiacheng Xu", "Yang Liu", "Yahui Zhou"], "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite the critical role of reward models (RMs) in reinforcement learning\nfrom human feedback (RLHF), current state-of-the-art open RMs perform poorly on\nmost existing evaluation benchmarks, failing to capture the spectrum of nuanced\nand sophisticated human preferences. Even approaches that incorporate advanced\ntraining techniques have not yielded meaningful performance improvements. We\nhypothesize that this brittleness stems primarily from limitations in\npreference datasets, which are often narrowly scoped, synthetically labeled, or\nlack rigorous quality control. To address these challenges, we present a\nlarge-scale preference dataset comprising 40 million preference pairs, named\nSynPref-40M. To enable data curation at scale, we design a human-AI synergistic\ntwo-stage pipeline that leverages the complementary strengths of human\nannotation quality and AI scalability. In this pipeline, humans provide\nverified annotations, while large language models perform automatic curation\nbased on human guidance. Training on this preference mixture, we introduce\nSkywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B\nparameters, trained on a carefully curated subset of 26 million preference\npairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile\nacross a wide range of capabilities, including alignment with human\npreferences, objective correctness, safety, resistance to stylistic biases, and\nbest-of-N scaling, achieving state-of-the-art performance across seven major\nreward model benchmarks. Ablation studies confirm that the effectiveness of our\napproach stems not only from data scale but also from high-quality curation.\nThe Skywork-Reward-V2 series represents substantial progress in open reward\nmodels, highlighting the untapped potential of existing preference datasets and\ndemonstrating how human-AI curation synergy can unlock significantly higher\ndata quality.", "AI": {"tldr": "The paper addresses the shortcomings of current reward models used in reinforcement learning from human feedback and introduces a large-scale preference dataset (SynPref-40M) and new models (Skywork-Reward-V2) to overcome these limitations.", "motivation": "Existing reward models struggle to capture complex human preferences due to limitations in preference datasets, such as narrow scope or synthetic labeling.", "method": "The authors created a large preference dataset (SynPref-40M) using a human-AI synergistic pipeline for data curation and trained eight new reward models based on a subset of this dataset.", "result": "Skywork-Reward-V2 models achieved state-of-the-art performance across seven major benchmarks for reward models.", "conclusion": "Human-AI collaboration in data curation can significantly enhance dataset quality, enabling reward models to better align with nuanced human preferences."}}
{"id": "2507.01806", "pdf": "https://arxiv.org/pdf/2507.01806", "abs": "https://arxiv.org/abs/2507.01806", "authors": ["Reza Arabpour", "Haitz S\u00e1ez de Oc\u00e1riz Borde", "Anastasis Kratsios"], "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "5-page main paper (excluding references) + 11-page appendix, 3\n  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for\n  Foundation Models", "summary": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning.", "AI": {"tldr": "The paper introduces a CPU-friendly method for Low-Rank Adapter (LoRA) fine-tuning by using pre-trained adapters to make fine-tuning accessible without GPUs.", "motivation": "Currently, fine-tuning Large Language Models (LLMs) using LoRAs is heavily dependent on GPU-based training, limiting its application for users with limited computational resources.", "method": "The method involves creating a meta-operator that leverages a pre-trained adapter bank (for Mistral-7B-Instruct-v0.2) to build new LoRAs on CPU by combining existing ones, instead of performing gradient-based updates.", "result": "The CPU-generated adapters outperformed the base Mistral model on downstream tasks but did not match the performance of GPU-trained LoRAs.", "conclusion": "This approach provides a practical solution for users with restricted resources, enabling parameter-efficient model fine-tuning without GPUs."}}
{"id": "2507.01347", "pdf": "https://arxiv.org/pdf/2507.01347", "abs": "https://arxiv.org/abs/2507.01347", "authors": ["Andrei Jelea", "Ahmed Nabil Belbachir", "Marius Leordeanu"], "title": "Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Generalized Test-Time Augmentation (GTTA), a highly effective\nmethod for improving the performance of a trained model, which unlike other\nexisting Test-Time Augmentation approaches from the literature is general\nenough to be used off-the-shelf for many vision and non-vision tasks, such as\nclassification, regression, image segmentation and object detection. By\napplying a new general data transformation, that randomly perturbs multiple\ntimes the PCA subspace projection of a test input, GTTA forms robust ensembles\nat test time in which, due to sound statistical properties, the structural and\nsystematic noises in the initial input data is filtered out and final estimator\nerrors are reduced. Different from other existing methods, we also propose a\nfinal self-supervised learning stage in which the ensemble output, acting as an\nunsupervised teacher, is used to train the initial single student model, thus\nreducing significantly the test time computational cost, at no loss in\naccuracy. Our tests and comparisons to strong TTA approaches and SoTA models on\nvarious vision and non-vision well-known datasets and tasks, such as image\nclassification and segmentation, speech recognition and house price prediction,\nvalidate the generality of the proposed GTTA. Furthermore, we also prove its\neffectiveness on the more specific real-world task of salmon segmentation and\ndetection in low-visibility underwater videos, for which we introduce\nDeepSalmon, the largest dataset of its kind in the literature.", "AI": {"tldr": "The paper introduces Generalized Test-Time Augmentation (GTTA), a new method enhancing model performance across various tasks, using a PCA-based data transformation and self-supervised ensemble training.", "motivation": "Existing Test-Time Augmentation (TTA) methods are often task-specific and computationally intensive. The authors sought to create a universal TTA approach that can be used across diverse tasks, with reduced computational requirements at test time.", "method": "GTTA employs a PCA-based random perturbation transformation to create robust ensembles at test time. Additionally, it includes a self-supervised learning phase where ensemble outputs are used to train the original model, thereby eliminating the need for runtime ensembles.", "result": "GTTA showed superior performance compared to traditional TTA and state-of-the-art models on various datasets and tasks (e.g., classification, segmentation, and object detection). It also proved effective in a unique real-world example, salmon segmentation in underwater videos.", "conclusion": "GTTA is a generalizable, efficient method that not only improves model performance across various tasks but also reduces the computational burden during inference, validating its utility in both standard and unique real-world scenarios."}}
{"id": "2507.01041", "pdf": "https://arxiv.org/pdf/2507.01041", "abs": "https://arxiv.org/abs/2507.01041", "authors": ["Zuguang Li", "Wen Wu", "Shaohua Wu", "Songge Zhang", "Ye Wang", "Xuemin", "Shen"], "title": "Fast AI Model Splitting over Edge Networks", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, 14 figures", "summary": "Split learning (SL) has emerged as a computationally efficient approach for\nartificial intelligence (AI) model training, which can alleviate device-side\ncomputational workloads. However, complex AI model architectures pose high\ncomputational complexity to obtain the optimal model splitting. In this paper,\nwe represent an arbitrary AI model as a directed acyclic graph (DAG), and then\nreformulate the optimal model splitting problem as a minimum s-t cut search\nproblem. To solve the problem, we propose a fast DAG-based model splitting\nalgorithm, which restructures the DAG to enable the optimal model splitting\nidentification via a maximum flow method. Theoretical analysis indicates that\nthe proposed algorithm is optimal. Furthermore, considering AI models with\nblock structures, we propose a block-wise model splitting algorithm to reduce\ncomputational complexity. The algorithm abstracts each block, i.e., a component\nconsisting of multiple layers, into a single vertex, thereby obtaining the\noptimal model splitting via a simplified DAG. Extensive experimental results\ndemonstrate that the proposed algorithms can determine the optimal model\nsplitting within milliseconds, as well as reduce training delay by\n24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art\nbenchmarks.", "AI": {"tldr": "This paper develops algorithms for efficient AI model splitting, enabling faster training with reduced computational complexity.", "motivation": "The need for efficient AI training methods on devices with limited computational resources.", "method": "The paper reformulates model splitting as a graph theory problem, introducing DAG-based algorithms\u2014including a block-wise method\u2014to optimize splits.", "result": "The algorithms achieve optimal model splitting within milliseconds and reduce training delays by up to 38.95% in dynamic edge networks.", "conclusion": "DAG-based approaches are computationally efficient for model splitting, offering significant advantages over traditional methods in real-world scenarios."}}
{"id": "2507.01426", "pdf": "https://arxiv.org/pdf/2507.01426", "abs": "https://arxiv.org/abs/2507.01426", "authors": ["Ratnangshu Das", "Pushpak Jagtap"], "title": "Approximation-free Control of Unknown Euler-Lagrangian Systems under Input Constraints", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "In this paper, we present a novel funnel-based tracking control algorithm for\nrobotic systems with unknown dynamics and prescribed input constraints. The\nEuler-Lagrange formulation, a common modeling approach for robotic systems, has\nbeen adopted in this study to address the trade-off between performance and\nactuator safety. We establish feasibility conditions that ensure tracking\nerrors evolve within predefined funnel bounds while maintaining bounded control\nefforts, a crucial consideration for robots with limited actuation\ncapabilities. We propose two approximation-free control strategies for\nscenarios where these conditions are violated: one actively corrects the error,\nand the other stops further deviation. Finally, we demonstrate the robust\nperformance and safety of the approach through simulations and experimental\nvalidations. This work represents a significant advancement in funnel-based\ncontrol, enhancing its applicability to real-world robotics systems with input\nconstraints.", "AI": {"tldr": "This paper introduces a funnel-based tracking control algorithm for robotic systems, focusing on balancing performance and actuator safety under input constraints.", "motivation": "To address the challenge of maintaining performance and actuator safety in robotics systems with unknown dynamics and constrained inputs.", "method": "A funnel-based control algorithm is developed using the Euler-Lagrange formulation, alongside approximation-free strategies to manage violations of feasibility conditions.", "result": "The proposed control approach demonstrates robust performance and actuator safety via simulations and experimental validations.", "conclusion": "This work significantly advances funnel-based control methods, making them more applicable to practical robotics systems with limited actuation capabilities."}}
{"id": "2507.01025", "pdf": "https://arxiv.org/pdf/2507.01025", "abs": "https://arxiv.org/abs/2507.01025", "authors": ["Yutong Lu", "Dan Huang", "Pin Chen"], "title": "HPC-AI Coupling Methodology for Scientific Applications", "categories": ["cs.CE", "cs.AI", "physics.comp-ph"], "comment": "14 pages, 11 figures", "summary": "Artificial intelligence (AI) technologies have fundamentally transformed\nnumerical-based high-performance computing (HPC) applications with data-driven\napproaches and endeavored to address existing challenges, e.g. high\ncomputational intensity, in various scientific domains. In this study, we\nexplore the scenarios of coupling HPC and AI (HPC-AI) in the context of\nemerging scientific applications, presenting a novel methodology that\nincorporates three patterns of coupling: surrogate, directive, and coordinate.\nEach pattern exemplifies a distinct coupling strategy, AI-driven prerequisite,\nand typical HPC-AI ensembles. Through case studies in materials science, we\ndemonstrate the application and effectiveness of these patterns. The study\nhighlights technical challenges, performance improvements, and implementation\ndetails, providing insight into promising perspectives of HPC-AI coupling. The\nproposed coupling patterns are applicable not only to materials science but\nalso to other scientific domains, offering valuable guidance for future HPC-AI\nensembles in scientific discovery.", "AI": {"tldr": "This paper introduces three patterns for coupling HPC and AI\u2014surrogate, directive, and coordinate\u2014and demonstrates their application with case studies in materials science.", "motivation": "The motivation is to address computational challenges and improve scientific discovery by effectively integrating HPC and AI technologies.", "method": "The authors propose three coupling patterns (surrogate, directive, coordinate) which are implemented and tested through case studies in materials science.", "result": "The study demonstrates effectiveness, highlights technical challenges, performance improvements, and offers implementation details for the proposed patterns.", "conclusion": "The proposed patterns provide a versatile framework for coupling HPC and AI, applicable across various scientific domains, and aim to guide future advancements in HPC-AI ensembles."}}
{"id": "2507.01437", "pdf": "https://arxiv.org/pdf/2507.01437", "abs": "https://arxiv.org/abs/2507.01437", "authors": ["Ting Xu", "Xiaoxiao Deng", "Xiandong Meng", "Haifeng Yang", "Yan Wu"], "title": "Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the challenges posed by the unstructured nature and\nhigh-dimensional semantic complexity of electronic health record texts. A deep\nlearning method based on attention mechanisms is proposed to achieve unified\nmodeling for information extraction and multi-label disease prediction. The\nstudy is conducted on the MIMIC-IV dataset. A Transformer-based architecture is\nused to perform representation learning over clinical text. Multi-layer\nself-attention mechanisms are employed to capture key medical entities and\ntheir contextual relationships. A Sigmoid-based multi-label classifier is then\napplied to predict multiple disease labels. The model incorporates a\ncontext-aware semantic alignment mechanism, enhancing its representational\ncapacity in typical medical scenarios such as label co-occurrence and sparse\ninformation. To comprehensively evaluate model performance, a series of\nexperiments were conducted, including baseline comparisons, hyperparameter\nsensitivity analysis, data perturbation studies, and noise injection tests.\nResults demonstrate that the proposed method consistently outperforms\nrepresentative existing approaches across multiple performance metrics. The\nmodel maintains strong generalization under varying data scales, interference\nlevels, and model depth configurations. The framework developed in this study\noffers an efficient algorithmic foundation for processing real-world clinical\ntexts and presents practical significance for multi-label medical text modeling\ntasks.", "AI": {"tldr": "The paper introduces a Transformer-based deep learning model using attention mechanisms for multi-label disease prediction from electronic health records, achieving superior performance on the MIMIC-IV dataset.", "motivation": "The study aims to address the challenges of handling unstructured and semantically complex medical text data in electronic health records.", "method": "The authors utilize a Transformer-based architecture with multi-layer self-attention mechanisms for representation learning, and a Sigmoid-based classifier for multi-label disease prediction. Context-aware semantic alignment is integrated to enhance model robustness and capture label co-occurrence.", "result": "The proposed method consistently outperforms existing approaches in performance metrics, showcasing strong generalization abilities across varying data scales and interference scenarios.", "conclusion": "The framework provides an effective algorithm for clinical text modeling and is practically significant for real-world medical applications."}}
{"id": "2507.01831", "pdf": "https://arxiv.org/pdf/2507.01831", "abs": "https://arxiv.org/abs/2507.01831", "authors": ["Yucen Lily Li", "Daohan Lu", "Polina Kirichenko", "Shikai Qiu", "Tim G. J. Rudner", "C. Bayan Bruss", "Andrew Gordon Wilson"], "title": "Out-of-Distribution Detection Methods Answer the Wrong Questions", "categories": ["cs.LG", "stat.ML"], "comment": "Extended version of ICML 2025 paper", "summary": "To detect distribution shifts and improve model safety, many\nout-of-distribution (OOD) detection methods rely on the predictive uncertainty\nor features of supervised models trained on in-distribution data. In this\npaper, we critically re-examine this popular family of OOD detection\nprocedures, and we argue that these methods are fundamentally answering the\nwrong questions for OOD detection. There is no simple fix to this misalignment,\nsince a classifier trained only on in-distribution classes cannot be expected\nto identify OOD points; for instance, a cat-dog classifier may confidently\nmisclassify an airplane if it contains features that distinguish cats from\ndogs, despite generally appearing nothing alike. We find that uncertainty-based\nmethods incorrectly conflate high uncertainty with being OOD, while\nfeature-based methods incorrectly conflate far feature-space distance with\nbeing OOD. We show how these pathologies manifest as irreducible errors in OOD\ndetection and identify common settings where these methods are ineffective.\nAdditionally, interventions to improve OOD detection such as feature-logit\nhybrid methods, scaling of model and data size, epistemic uncertainty\nrepresentation, and outlier exposure also fail to address this fundamental\nmisalignment in objectives. We additionally consider unsupervised density\nestimation and generative models for OOD detection, which we show have their\nown fundamental limitations.", "AI": {"tldr": "This paper critically examines the limitations of current out-of-distribution (OOD) detection methods, arguing that they fundamentally misalign with the task and exhibit irreducible errors.", "motivation": "The authors aim to improve OOD detection by identifying critical flaws in the commonly-used uncertainty- and feature-based methods, which they argue are answering the wrong questions.", "method": "The study re-examines OOD detection methods, identifies their misalignments and limitations, and evaluates alternative approaches such as hybrid methods, scaling, epistemic uncertainty, and unsupervised generative models.", "result": "The paper demonstrates that current OOD detection techniques, including proposed interventions, cannot fully overcome their misalignment issues, leading to irreducible errors and limitations.", "conclusion": "Mysteries and inefficiencies persist in OOD detection methods, and fundamental changes to the problem formulation are needed to effectively detect distribution shifts."}}
{"id": "2507.01351", "pdf": "https://arxiv.org/pdf/2507.01351", "abs": "https://arxiv.org/abs/2507.01351", "authors": ["Chaoxiang Cai", "Longrong Yang", "Kaibing Chen", "Fan Yang", "Xi Li"], "title": "Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model", "categories": ["cs.CV"], "comment": null, "summary": "The mixture-of-experts (MoE), which replaces dense models with sparse\narchitectures, has gained attention in large vision-language models (LVLMs) for\nachieving comparable performance with fewer activated parameters. Existing MoE\nframeworks for LVLMs focus on token-to-expert routing (TER), encouraging\ndifferent experts to specialize in processing distinct tokens. However, these\nframeworks often rely on the load balancing mechanism, overlooking the inherent\ndistributional differences between vision and language. To this end, we propose\na Long-Tailed Distribution-aware Router (LTDR) for vision-language TER,\ntackling two challenges: (1) Distribution-aware router for modality-specific\nrouting. We observe that language TER follows a uniform distribution, whereas\nvision TER exhibits a long-tailed distribution. This discrepancy necessitates\ndistinct routing strategies tailored to each modality. (2) Enhancing expert\nactivation for vision tail tokens. Recognizing the importance of vision tail\ntokens, we introduce an oversampling-like strategy by increasing the number of\nactivated experts for these tokens. Experiments on extensive benchmarks\nvalidate the effectiveness of our approach.", "AI": {"tldr": "The paper presents a method called Long-Tailed Distribution-aware Router (LTDR) to optimize vision-language mixture-of-expert frameworks by addressing distribution discrepancies and enhancing expert activation for vision tail tokens.", "motivation": "The motivation is to improve existing mixture-of-experts frameworks for large vision-language models that neglect distributional differences between vision and language data, particularly in token routing.", "method": "The proposed method involves two key strategies: (1) using a distribution-aware router tailored specifically for uniform language and long-tailed vision token distributions, and (2) using an oversampling-like strategy to enhance the activation of experts for vision tail tokens.", "result": "Experimental results on various benchmarks demonstrate the improved effectiveness of the LTDR approach compared to existing methods.", "conclusion": "LTDR provides a pathway to more effective routing and expert activation in vision-language mixture-of-experts models by addressing their distributional and activation challenges."}}
{"id": "2507.01043", "pdf": "https://arxiv.org/pdf/2507.01043", "abs": "https://arxiv.org/abs/2507.01043", "authors": ["Szymon \u015awiderski", "Agnieszka Jastrz\u0119bska"], "title": "Data Classification with Dynamically Growing and Shrinking Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "Paper submitted to Journal of Computational Science", "summary": "The issue of data-driven neural network model construction is one of the core\nproblems in the domain of Artificial Intelligence. A standard approach assumes\na fixed architecture with trainable weights. A conceptually more advanced\nassumption is that we not only train the weights, but also find out the optimal\nmodel architecture. We present a new method that realizes just that. This\narticle is an extended version of our conference paper titled \"Dynamic Growing\nand Shrinking of Neural Networks with Monte Carlo Tree Search [26]\". In the\npaper, we show in detail how to create a neural network with a procedure that\nallows dynamic shrinking and growing of the model while it is being trained.\nThe decision-making mechanism for the architectural design is governed by a\nMonte Carlo tree search procedure which simulates network behavior and allows\nto compare several candidate architecture changes to choose the best one. The\nproposed method was validated using both visual and time series datasets,\ndemonstrating its particular effectiveness in multivariate time series\nclassification. This is attributed to the architecture's ability to adapt\ndynamically, allowing independent modifications for each time series. The\napproach is supplemented by Python source code for reproducibility.\nExperimental evaluations in visual pattern and multivariate time series\nclassification tasks revealed highly promising performance, underscoring the\nmethod's robustness and adaptability.", "AI": {"tldr": "The paper introduces a method to dynamically grow and shrink neural network architectures during training using Monte Carlo Tree Search, with strong performance in visual and multivariate time series classification.", "motivation": "Optimizing neural network architectures dynamically during training is a key challenge in AI, improving over fixed-architecture models.", "method": "The proposal uses Monte Carlo Tree Search to simulate and select the best architecture adjustments while training, allowing flexible model adaptation.", "result": "The method demonstrated strong performance, particularly in multivariate time series classification, due to its dynamic adaptability across datasets.", "conclusion": "The proposed approach is robust, adaptable, and effective, with Python code provided for reproducibility, advancing dynamic architecture optimization."}}
{"id": "2507.01462", "pdf": "https://arxiv.org/pdf/2507.01462", "abs": "https://arxiv.org/abs/2507.01462", "authors": ["Eneko Osaba", "Estibaliz Garrote", "Pablo Miranda-Rodriguez", "Alessia Ciacco", "Itziar Cabanes", "Aitziber Mancisidor"], "title": "Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0", "categories": ["cs.RO", "cs.AI", "cs.ET"], "comment": "2 pages, 1 figure, paper accepted for presentation at the IEEE\n  International Conference on Quantum Computing and Engineering (QCE)", "summary": "This work explores the application of hybrid quantum-classical algorithms to\noptimize robotic inspection trajectories derived from Computer-Aided Design\n(CAD) models in industrial settings. By modeling the task as a 3D variant of\nthe Traveling Salesman Problem, incorporating incomplete graphs and open-route\nconstraints, this study evaluates the performance of two D-Wave-based solvers\nagainst classical methods such as GUROBI and Google OR-Tools. Results across\nfive real-world cases demonstrate competitive solution quality with\nsignificantly reduced computation times, highlighting the potential of quantum\napproaches in automation under Industry 4.0.", "AI": {"tldr": "The paper investigates hybrid quantum-classical algorithms for optimizing 3D robotic inspection paths and compares their efficiency against classical methods, showing quantum approaches have competitive results.", "motivation": "To explore the potential of quantum computing in solving complex industrial optimization problems like robotic inspection routes.", "method": "The task is modeled as a 3D Traveling Salesman Problem with constraints (incomplete graphs, open routes). Quantum solvers from D-Wave are compared with classical solvers like GUROBI and Google OR-Tools using real-world cases.", "result": "Quantum solvers provide competitive solution quality with significantly reduced computation times compared to classical methods.", "conclusion": "Hybrid quantum-classical algorithms present promising opportunities for improving industrial automation tasks, supporting Industry 4.0 innovations."}}
{"id": "2507.01449", "pdf": "https://arxiv.org/pdf/2507.01449", "abs": "https://arxiv.org/abs/2507.01449", "authors": ["Tianyu Liu", "Qitan Lv", "Hao Li", "Xing Gao", "Xiao Sun"], "title": "LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation", "categories": ["cs.CL"], "comment": null, "summary": "Speculative decoding (SD), where a small draft model is employed to propose\ndraft tokens in advance and then the target model validates them in parallel,\nhas emerged as a promising technique for LLM inference acceleration. Many\nendeavors to improve SD are to eliminate the need for a draft model and\ngenerate draft tokens in a retrieval-based manner in order to further alleviate\nthe drafting overhead and significantly reduce the difficulty in deployment and\napplications. However, retrieval-based SD relies on a matching paradigm to\nretrieval the most relevant reference as the draft tokens, where these methods\noften fail to find matched and accurate draft tokens. To address this\nchallenge, we propose LogitSpec to effectively expand the retrieval range and\nfind the most relevant reference as drafts. Our LogitSpec is motivated by the\nobservation that the logit of the last token can not only predict the next\ntoken, but also speculate the next next token. Specifically, LogitSpec\ngenerates draft tokens in two steps: (1) utilizing the last logit to speculate\nthe next next token; (2) retrieving relevant reference for both the next token\nand the next next token. LogitSpec is training-free and plug-and-play, which\ncan be easily integrated into existing LLM inference frameworks. Extensive\nexperiments on a wide range of text generation benchmarks demonstrate that\nLogitSpec can achieve up to 2.61 $\\times$ speedup and 3.28 mean accepted tokens\nper decoding step. Our code is available at\nhttps://github.com/smart-lty/LogitSpec.", "AI": {"tldr": "The paper introduces LogitSpec, a method to improve speculative decoding (SD) by leveraging logits to expand the retrieval range for draft tokens, achieving notable speedups in LLM inference.", "motivation": "To address inefficiencies in retrieval-based speculative decoding, which often fails to find accurate draft tokens, thereby hindering the acceleration potential of LLM inference.", "method": "LogitSpec generates draft tokens using the logits of the last token to speculate two consecutive tokens. The framework is training-free and works by retrieving references for these tokens to enhance their relevance.", "result": "Experiments show that LogitSpec achieves up to 2.61\u00d7 speedup and 3.28 mean accepted tokens per decoding step across various text generation benchmarks.", "conclusion": "LogitSpec offers a simple, effective, and easily integrable solution to improve LLM inference by enhancing speculative decoding without requiring additional model training."}}
{"id": "2507.01918", "pdf": "https://arxiv.org/pdf/2507.01918", "abs": "https://arxiv.org/abs/2507.01918", "authors": ["Christian Bongiorno", "Efstratios Manolakis", "Rosario Nunzio Mantegna"], "title": "End-to-End Large Portfolio Optimization for Variance Minimization with Neural Networks through Covariance Cleaning", "categories": ["q-fin.PM", "cs.AI", "math.OC", "physics.data-an", "stat.ML", "91G10 (Primary) 68T07, 91G60, 62P05 (Secondary)", "I.2.6; I.5.1; G.3; J.4"], "comment": null, "summary": "We develop a rotation-invariant neural network that provides the global\nminimum-variance portfolio by jointly learning how to lag-transform historical\nreturns and how to regularise both the eigenvalues and the marginal\nvolatilities of large equity covariance matrices. This explicit mathematical\nmapping offers clear interpretability of each module's role, so the model\ncannot be regarded as a pure black-box. The architecture mirrors the analytical\nform of the global minimum-variance solution yet remains agnostic to dimension,\nso a single model can be calibrated on panels of a few hundred stocks and\napplied, without retraining, to one thousand US equities-a cross-sectional jump\nthat demonstrates robust out-of-sample generalisation. The loss function is the\nfuture realized minimum portfolio variance and is optimized end-to-end on real\ndaily returns. In out-of-sample tests from January 2000 to December 2024 the\nestimator delivers systematically lower realised volatility, smaller maximum\ndrawdowns, and higher Sharpe ratios than the best analytical competitors,\nincluding state-of-the-art non-linear shrinkage. Furthermore, although the\nmodel is trained end-to-end to produce an unconstrained (long-short)\nminimum-variance portfolio, we show that its learned covariance representation\ncan be used in general optimizers under long-only constraints with virtually no\nloss in its performance advantage over competing estimators. These gains\npersist when the strategy is executed under a highly realistic implementation\nframework that models market orders at the auctions, empirical slippage,\nexchange fees, and financing charges for leverage, and they remain stable\nduring episodes of acute market stress.", "AI": {"tldr": "The paper introduces a rotation-invariant neural network method to optimize and generalize minimum-variance portfolio strategies using real financial data, showcasing robust out-of-sample performance and interpretability.", "motivation": "To address the challenge of constructing robust, interpretable, and scalable minimum-variance portfolios in finance using advanced machine learning techniques.", "method": "The paper develops a rotation-invariant neural network architecture that simultaneously learns lag-transformation of returns and regularization of covariance matrix properties, with the loss function based on future realized minimum portfolio variance, optimized end-to-end using actual financial data.", "result": "The proposed method demonstrates systematic outperformance over analytical methods in terms of realized volatility, drawdowns, and Sharpe ratios. It generalizes effectively across asset dimensions and maintains efficacy under long-only constraints without losing advantages.", "conclusion": "The model offers a robust, scalable, and interpretable approach to portfolio optimization, effectively handling real-world implementation challenges, and performing well even during market stress."}}
{"id": "2507.01367", "pdf": "https://arxiv.org/pdf/2507.01367", "abs": "https://arxiv.org/abs/2507.01367", "authors": ["Tianrui Lou", "Xiaojun Jia", "Siyuan Liang", "Jiawei Liang", "Ming Zhang", "Yanjun Xiao", "Xiaochun Cao"], "title": "3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Physical adversarial attack methods expose the vulnerabilities of deep neural\nnetworks and pose a significant threat to safety-critical scenarios such as\nautonomous driving. Camouflage-based physical attack is a more promising\napproach compared to the patch-based attack, offering stronger adversarial\neffectiveness in complex physical environments. However, most prior work relies\non mesh priors of the target object and virtual environments constructed by\nsimulators, which are time-consuming to obtain and inevitably differ from the\nreal world. Moreover, due to the limitations of the backgrounds in training\nimages, previous methods often fail to produce multi-view robust adversarial\ncamouflage and tend to fall into sub-optimal solutions. Due to these reasons,\nprior work lacks adversarial effectiveness and robustness across diverse\nviewpoints and physical environments. We propose a physical attack framework\nbased on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and\nprecise reconstruction with few images, along with photo-realistic rendering\ncapabilities. Our framework further enhances cross-view robustness and\nadversarial effectiveness by preventing mutual and self-occlusion among\nGaussians and employing a min-max optimization approach that adjusts the\nimaging background of each viewpoint, helping the algorithm filter out\nnon-robust adversarial features. Extensive experiments validate the\neffectiveness and superiority of PGA. Our code is available\nat:https://github.com/TRLou/PGA.", "AI": {"tldr": "The paper introduces a novel physical adversarial attack framework called PGA, leveraging 3D Gaussian Splatting for rapid, precise object reconstruction and robust adversarial camouflage capabilities under diverse viewpoints and environments.", "motivation": "To address vulnerabilities in deep neural networks exposed by physical adversarial attacks, specifically improving the robustness and effectiveness of camouflage-based attacks under real-world, multi-view, and complex conditions.", "method": "Utilizes 3D Gaussian Splatting for object reconstruction and photo-realistic rendering, combined with a min-max optimization strategy to enhance adversarial features while preventing occlusions across viewpoints.", "result": "Experiments demonstrate PGA's effectiveness and robustness in creating adversarial camouflage, outperforming prior methods across diverse viewpoints and physical environments.", "conclusion": "The PGA framework improves adversarial attack robustness and effectiveness without relying on extensive simulations, offering a practical advancement in safety-critical scenarios. Source code is publicly available for verification and application."}}
{"id": "2507.01045", "pdf": "https://arxiv.org/pdf/2507.01045", "abs": "https://arxiv.org/abs/2507.01045", "authors": ["Xiao Gu", "Wei Tang", "Jinpei Han", "Veer Sangha", "Fenglin Liu", "Shreyank N Gowda", "Antonio H. Ribeiro", "Patrick Schwab", "Kim Branson", "Lei Clifton", "Antonio Luiz P. Ribeiro", "Zhangdaihong Liu", "David A. Clifton"], "title": "Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms\n(PPG), are of paramount importance for the diagnosis, prevention, and\nmanagement of cardiovascular diseases, and have been extensively used in a\nvariety of clinical tasks. Conventional deep learning approaches for analyzing\nthese signals typically rely on homogeneous datasets and static bespoke models,\nlimiting their robustness and generalizability across diverse clinical settings\nand acquisition protocols. In this study, we present a cardiac sensing\nfoundation model (CSFM) that leverages advanced transformer architectures and a\ngenerative, masked pretraining strategy to learn unified representations from\nvast, heterogeneous health records. Our model is pretrained on an innovative\nmulti-modal integration of data from multiple large-scale datasets (including\nMIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the\ncorresponding clinical or machine-generated text reports from approximately 1.7\nmillion individuals. We demonstrate that the embeddings derived from our CSFM\nnot only serve as effective feature extractors across diverse cardiac sensing\nscenarios, but also enable seamless transfer learning across varying input\nconfigurations and sensor modalities. Extensive evaluations across diagnostic\ntasks, demographic information recognition, vital sign measurement, clinical\noutcome prediction, and ECG question answering reveal that CSFM consistently\noutperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits\nrobust performance across multiple ECG lead configurations from standard\n12-lead systems to single-lead setups, and in scenarios where only ECG, only\nPPG, or a combination thereof is available. These findings highlight the\npotential of CSFM as a versatile and scalable solution, for comprehensive\ncardiac monitoring.", "AI": {"tldr": "The study introduces a cardiac sensing foundation model (CSFM) using transformers and massive healthcare data to advance cardiac signal analysis and supports multiple diagnostic tasks while enabling seamless transfer learning.", "motivation": "To address the lack of robustness and generalizability in traditional deep learning models for cardiac signal analysis, particularly due to reliance on homogeneous datasets and static models.", "method": "The study develops CSFM, a transformer-based model leveraging a generative masked pretraining strategy, trained on over 1.7 million multi-modal health records including ECGs, PPGs, and clinical text reports.", "result": "CSFM demonstrated superior performance across various diagnostic tasks, demographic recognition, and clinical outcome prediction. It supports multiple ECG and PPG configurations with robust cross-modal and cross-setup adaptability.", "conclusion": "CSFM is a scalable, versatile foundation model that outperforms traditional cardiac signal analysis approaches, offering improved generalizability and performance across diverse settings."}}
{"id": "2507.01485", "pdf": "https://arxiv.org/pdf/2507.01485", "abs": "https://arxiv.org/abs/2507.01485", "authors": ["Yibo Qiu", "Zan Huang", "Zhiyu Wang", "Handi Liu", "Yiling Qiao", "Yifeng Hu", "Shu'ang Sun", "Hangke Peng", "Ronald X Xu", "Mingzhai Sun"], "title": "BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments", "categories": ["cs.RO", "cs.AI", "cs.MA", "q-bio.QM"], "comment": null, "summary": "Large language models (LLMs) and vision-language models (VLMs) have the\npotential to transform biological research by enabling autonomous\nexperimentation. Yet, their application remains constrained by rigid protocol\ndesign, limited adaptability to dynamic lab conditions, inadequate error\nhandling, and high operational complexity. Here we introduce BioMARS\n(Biological Multi-Agent Robotic System), an intelligent platform that\nintegrates LLMs, VLMs, and modular robotics to autonomously design, plan, and\nexecute biological experiments. BioMARS uses a hierarchical architecture: the\nBiologist Agent synthesizes protocols via retrieval-augmented generation; the\nTechnician Agent translates them into executable robotic pseudo-code; and the\nInspector Agent ensures procedural integrity through multimodal perception and\nanomaly detection. The system autonomously conducts cell passaging and culture\ntasks, matching or exceeding manual performance in viability, consistency, and\nmorphological integrity. It also supports context-aware optimization,\noutperforming conventional strategies in differentiating retinal pigment\nepithelial cells. A web interface enables real-time human-AI collaboration,\nwhile a modular backend allows scalable integration with laboratory hardware.\nThese results highlight the feasibility of generalizable, AI-driven laboratory\nautomation and the transformative role of language-based reasoning in\nbiological research.", "AI": {"tldr": "BioMARS is an AI-driven platform integrating large language models, vision-language models, and modular robotics to autonomously conduct biological experiments.", "motivation": "To address limitations in current experimental automation systems, such as rigid protocol design, lack of adaptability, inadequate error handling, and operational complexity.", "method": "BioMARS uses a hierarchical setup with three agents: Biologist Agent for protocol synthesis, Technician Agent for code translation, and Inspector Agent for anomaly detection, supported by multimodal perception and modular robotics.", "result": "The system autonomously performed tasks like cell passaging and culture with high viability, consistency, and morphological integrity, outperforming conventional methods in cellular differentiation and lab task optimization.", "conclusion": "BioMARS demonstrates the potential of AI-driven automation in biology, emphasizing the transformative power of language-based reasoning for laboratory tasks and optimization."}}
{"id": "2507.01479", "pdf": "https://arxiv.org/pdf/2507.01479", "abs": "https://arxiv.org/abs/2507.01479", "authors": ["Yingqiang Gao", "Kaede Johnson", "David Froehlich", "Luisa Carrer", "Sarah Ebling"], "title": "Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves.", "AI": {"tldr": "The study explores a novel method to enhance automatic text simplification (ATS) systems using large language models (LLMs), integrating direct preference optimization (DPO) based on human feedback from persons with intellectual disabilities for personalized output.", "motivation": "The research is motivated by the need to improve text accessibility for individuals with intellectual disabilities and address the gap in ATS systems which lack personalization due to absence of preference feedback during training.", "method": "The paper extends supervised fine-tuning (SFT) of LLM-based ATS models by utilizing direct preference optimization (DPO) with human feedback to tailor text simplifications. It also proposes a comprehensive development pipeline for personalized ATS systems.", "result": "The study demonstrates that incorporating human feedback from the target group effectively aligns ATS systems with the preferences and expectations of persons with intellectual disabilities.", "conclusion": "This work highlights the importance of involving target group representatives in developing AI systems for inclusivity, taking one step closer to personalized accessibility solutions."}}
{"id": "2507.01932", "pdf": "https://arxiv.org/pdf/2507.01932", "abs": "https://arxiv.org/abs/2507.01932", "authors": ["Zhaosong Lu", "Xiangyuan Wang"], "title": "A first-order method for nonconvex-nonconcave minimax problems under a local Kurdyka-\u0141ojasiewicz condition", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA", "stat.ML", "90C26, 90C30, 90C47, 90C99, 65K05"], "comment": "26 pages", "summary": "We study a class of nonconvex-nonconcave minimax problems in which the inner\nmaximization problem satisfies a local Kurdyka-{\\L}ojasiewicz (KL) condition\nthat may vary with the outer minimization variable. In contrast to the global\nKL or Polyak-{\\L}ojasiewicz (PL) conditions commonly assumed in the literature\n-- which are significantly stronger and often too restrictive in practice --\nthis local KL condition accommodates a broader range of practical scenarios.\nHowever, it also introduces new analytical challenges. In particular, as an\noptimization algorithm progresses toward a stationary point of the problem, the\nregion over which the KL condition holds may shrink, resulting in a more\nintricate and potentially ill-conditioned landscape. To address this challenge,\nwe show that the associated maximal function is locally H\\\"older smooth.\nLeveraging this key property, we develop an inexact proximal gradient method\nfor solving the minimax problem, where the inexact gradient of the maximal\nfunction is computed by applying a proximal gradient method to a KL-structured\nsubproblem. Under mild assumptions, we establish complexity guarantees for\ncomputing an approximate stationary point of the minimax problem.", "AI": {"tldr": "This study addresses nonconvex-nonconcave minimax problems using a local Kurdyka-\u0141ojasiewicz (KL) condition and introduces an optimized algorithm for solving them.", "motivation": "Existing methods for minimax problems often rely on global KL or PL conditions, which are overly restrictive in real-world scenarios. This paper aims to extend the theoretical framework to address a broader class of problems by using local conditions.", "method": "The authors define a local KL condition for the inner problem and use its locally H\u00f6lder smooth maximal function property to develop an inexact proximal gradient algorithm.", "result": "They derive complexity guarantees for their proposed method under mild assumptions, addressing challenges related to shrinking KL regions and ill-conditioned landscapes.", "conclusion": "The proposed framework and algorithm offer a solution for a broader class of minimax problems by relaxing overly restrictive global conditions and ensuring convergence to approximate stationary points."}}
{"id": "2507.01368", "pdf": "https://arxiv.org/pdf/2507.01368", "abs": "https://arxiv.org/abs/2507.01368", "authors": ["Tianning Chai", "Chancharik Mitra", "Brandon Huang", "Gautam Rajendrakumar Gare", "Zhiqiu Lin", "Assaf Arbelle", "Leonid Karlinsky", "Rogerio Feris", "Trevor Darrell", "Deva Ramanan", "Roei Herzig"], "title": "Activation Reward Models for Few-Shot Model Alignment", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to\nhuman preferences is a central challenge in improving the quality of the\nmodels' generative outputs for real-world applications. A common approach is to\nuse reward modeling to encode preferences, enabling alignment via post-training\nusing reinforcement learning. However, traditional reward modeling is not\neasily adaptable to new preferences because it requires a separate reward\nmodel, commonly trained on large preference datasets. To address this, we\nintroduce Activation Reward Models (Activation RMs) -- a novel few-shot reward\nmodeling method that leverages activation steering to construct well-aligned\nreward signals using minimal supervision and no additional model finetuning.\nActivation RMs outperform existing few-shot reward modeling approaches such as\nLLM-as-a-judge with in-context learning, voting-based scoring, and token\nprobability scoring on standard reward modeling benchmarks. Furthermore, we\ndemonstrate the effectiveness of Activation RMs in mitigating reward hacking\nbehaviors, highlighting their utility for safety-critical applications. Toward\nthis end, we propose PreferenceHack, a novel few-shot setting benchmark, the\nfirst to test reward models on reward hacking in a paired preference format.\nFinally, we show that Activation RM achieves state-of-the-art performance on\nthis benchmark, surpassing even GPT-4o.", "AI": {"tldr": "The paper introduces Activation Reward Models (Activation RMs) as a novel method to align generative models with human preferences using minimal supervision, outperforming previous approaches and addressing reward hacking issues.", "motivation": "The motivation is to improve alignment of Large Language Models (LLMs) and Large Multimodal Models (LMMs) to human preferences in real-world applications, addressing the inflexibility and supervision requirements of traditional reward modeling methods.", "method": "The authors propose Activation Reward Models (Activation RMs), which leverage activation steering for few-shot reward modeling without needing extensive preference datasets or additional finetuning.", "result": "Experimental results show that Activation RMs outperform existing few-shot methods (e.g., LLM-as-a-judge, voting-based scoring) on standard benchmarks and effectively mitigate reward hacking. The authors also present a new benchmark, PreferenceHack, where Activation RMs achieve state-of-the-art performance.", "conclusion": "Activation RMs offer an innovative and efficient way to encode human preferences and prevent reward hacking, making them particularly useful for safety-critical applications while setting new standards in alignment studies."}}
{"id": "2507.01047", "pdf": "https://arxiv.org/pdf/2507.01047", "abs": "https://arxiv.org/abs/2507.01047", "authors": ["Logan A. Burnett", "Umme Mahbuba Nabila", "Majdi I. Radaideh"], "title": "Variational Digital Twins", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "33 pages, 14 figures, and 7 tables", "summary": "While digital twins (DT) hold promise for providing real-time insights into\ncomplex energy assets, much of the current literature either does not offer a\nclear framework for information exchange between the model and the asset, lacks\nkey features needed for real-time implementation, or gives limited attention to\nmodel uncertainty. Here, we aim to solve these gaps by proposing a variational\ndigital twin (VDT) framework that augments standard neural architectures with a\nsingle Bayesian output layer. This lightweight addition, along with a novel VDT\nupdating algorithm, lets a twin update in seconds on commodity GPUs while\nproducing calibrated uncertainty bounds that can inform experiment design,\ncontrol algorithms, and model reliability. The VDT is evaluated on four\nenergy-sector problems. For critical-heat-flux prediction, uncertainty-driven\nactive learning reaches R2 = 0.98 using 47 % fewer experiments and one-third\nthe training time of random sampling. A three-year renewable-generation twin\nmaintains R2 > 0.95 for solar output and curbs error growth for volatile wind\nforecasts via monthly updates that process only one month of data at a time. A\nnuclear reactor transient cooldown twin reconstructs thermocouple signals with\nR2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating\nrobustness to degraded instrumentation. Finally, a physics-informed Li-ion\nbattery twin, retrained after every ten discharges, lowers voltage mean-squared\nerror by an order of magnitude relative to the best static model while adapting\nits credible intervals as the cell approaches end-of-life. These results\ndemonstrate that combining modest Bayesian augmentation with efficient update\nschemes turns conventional surrogates into uncertainty-aware, data-efficient,\nand computationally tractable DTs, paving the way for dependable models across\nindustrial and scientific energy systems.", "AI": {"tldr": "This paper proposes a variational digital twin (VDT) framework that augments neural architectures with a Bayesian output layer to improve real-time energy system modeling, uncertainty quantification, and efficiency.", "motivation": "Current digital twin frameworks lack robust mechanisms for real-time updates, calibrated uncertainty modeling, and computational efficiency, posing challenges in energy asset applications.", "method": "The VDT framework combines a single Bayesian output layer with a novel updating algorithm to enable real-time model updates, calibrated uncertainty bounds, and efficient performance using standard hardware.", "result": "VDT achieves high accuracy across four energy-sector problems: active learning reduces experiments for heat-flux prediction, renewable-generation twins maintain consistent forecasts, nuclear reactor twins handle sensor loss, and a battery twin improves voltage error prediction significantly.", "conclusion": "Integrating Bayesian elements with efficient update schemes enhances digital twins to become uncertainty-aware, data-efficient, and computationally viable, offering dependable modeling for scientific and industrial energy systems."}}
{"id": "2507.01550", "pdf": "https://arxiv.org/pdf/2507.01550", "abs": "https://arxiv.org/abs/2507.01550", "authors": ["Johannes Kohl", "Georg Muck", "Georg J\u00e4ger", "Sebastian Zug"], "title": "Dynamic System Model Generation for Online Fault Detection and Diagnosis of Robotic Systems", "categories": ["cs.RO"], "comment": "Accepted for publication in Ada User Journal", "summary": "With the rapid development of more complex robots, Fault Detection and\nDiagnosis (FDD) becomes increasingly harder. Especially the need for\npredetermined models and historic data is problematic because they do not\nencompass the dynamic and fast-changing nature of such systems. To this end, we\npropose a concept that actively generates a dynamic system model at runtime and\nutilizes it to locate root causes. The goal is to be applicable to all kinds of\nrobotic systems that share a similar software design. Additionally, it should\nexhibit minimal overhead and enhance independence from expert attention.", "AI": {"tldr": "The paper proposes a dynamic fault detection and diagnosis system for robots without requiring predetermined models or historical data.", "motivation": "To address the challenge of fault detection in complex, dynamic robotic systems while minimizing reliance on static models, historical data, or expert involvement.", "method": "The authors propose generating a dynamic system model at runtime, specifically targeting robotic systems with a shared software design for universal applicability.", "result": "The proposed dynamic model system locates root causes of faults effectively without imposing significant computational overhead.", "conclusion": "The method offers a flexible, lightweight approach to fault diagnosis in fast-changing robotic systems, improving autonomy and adaptability."}}
{"id": "2507.01541", "pdf": "https://arxiv.org/pdf/2507.01541", "abs": "https://arxiv.org/abs/2507.01541", "authors": ["\u00c1lvaro Zaera", "Diana Nicoleta Popa", "Ivan Sekulic", "Paolo Rosso"], "title": "Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing", "categories": ["cs.CL"], "comment": null, "summary": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS.", "AI": {"tldr": "This paper introduces a method that combines uncertainty modeling with fine-tuned large language models (LLMs) to improve the accuracy and efficiency of detecting out-of-scope (OOS) intents in dialogue systems.", "motivation": "The challenge of detecting OOS intents is vital for ensuring the robustness of task-oriented dialogue systems (TODS) against unseen and ambiguous queries.", "method": "The approach involves two steps: (1) Applying uncertainty estimation to outputs from an existing in-scope intent detection classifier used in a real-world TODS. (2) Using a fine-tuned LLM to handle instances with high uncertainty, leveraging its decision-making ability.", "result": "The proposed method achieves state-of-the-art results on standard OOS detection benchmarks, including real-world data from a deployed TODS, balancing computational efficiency and performance.", "conclusion": "The method combines traditional techniques with LLM-based enhancements to handle OOS intent detection efficiently and accurately, making it a practical solution for real-world dialogue systems."}}
{"id": "2507.01372", "pdf": "https://arxiv.org/pdf/2507.01372", "abs": "https://arxiv.org/abs/2507.01372", "authors": ["Max Hamilton", "Jinlin Lai", "Wenlong Zhao", "Subhransu Maji", "Daniel Sheldon"], "title": "Active Measurement: Efficient Estimation at Scale", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "AI has the potential to transform scientific discovery by analyzing vast\ndatasets with little human effort. However, current workflows often do not\nprovide the accuracy or statistical guarantees that are needed. We introduce\nactive measurement, a human-in-the-loop AI framework for scientific\nmeasurement. An AI model is used to predict measurements for individual units,\nwhich are then sampled for human labeling using importance sampling. With each\nnew set of human labels, the AI model is improved and an unbiased Monte Carlo\nestimate of the total measurement is refined. Active measurement can provide\nprecise estimates even with an imperfect AI model, and requires little human\neffort when the AI model is very accurate. We derive novel estimators,\nweighting schemes, and confidence intervals, and show that active measurement\nreduces estimation error compared to alternatives in several measurement tasks.", "AI": {"tldr": "The paper introduces 'active measurement,' an AI framework with human involvement for improving scientific measurements, combining AI predictions, human labeling, and Monte Carlo estimations.", "motivation": "Current scientific measurement workflows lack sufficient accuracy and statistical robustness and often require significant human effort.", "method": "Develop an AI framework that integrates AI predictions with selective human labeling using importance sampling and employs Monte Carlo estimations to refine measurements iteratively.", "result": "Active measurement showed reduced estimation errors and provided accurate measurements across several tasks, demonstrating its effectiveness compared to alternative methods.", "conclusion": "The framework balances AI's efficiency with human precision, offering an effective solution for obtaining accurate scientific measurements with minimal human effort."}}
{"id": "2507.01048", "pdf": "https://arxiv.org/pdf/2507.01048", "abs": "https://arxiv.org/abs/2507.01048", "authors": ["Ricardo Emanuel Vaz Vargas", "Afr\u00e2nio Jos\u00e9 de Melo Junior", "Celso Jos\u00e9 Munaro", "Cl\u00e1udio Benevenuto de Campos Lima", "Eduardo Toledo de Lima Junior", "Felipe Muntzberg Barrocas", "Fl\u00e1vio Miguel Varej\u00e3o", "Guilherme Fidelis Peixer", "Igor de Melo Nery Oliveira", "Jader Riso Barbosa Jr.", "Jaime Andr\u00e9s Lozano Cadena", "Jean Carlos Dias de Ara\u00fajo", "Jo\u00e3o Neuenschwander Escosteguy Carneiro", "Lucas Gouveia Omena Lopes", "Lucas Pereira de Gouveia", "Mateus de Araujo Fernandes", "Matheus Lima Scramignon", "Patrick Marques Ciarelli", "Rodrigo Castello Branco", "Rog\u00e9rio Leite Alves Pinto"], "title": "3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells", "categories": ["cs.LG"], "comment": "21 pages, 10 figures, and 7 tables", "summary": "In the oil industry, undesirable events in oil wells can cause economic\nlosses, environmental accidents, and human casualties. Solutions based on\nArtificial Intelligence and Machine Learning for Early Detection of such events\nhave proven valuable for diverse applications across industries. In 2019,\nrecognizing the importance and the lack of public datasets related to\nundesirable events in oil wells, Petrobras developed and publicly released the\nfirst version of the 3W Dataset, which is essentially a set of Multivariate\nTime Series labeled by experts. Since then, the 3W Dataset has been developed\ncollaboratively and has become a foundational reference for numerous works in\nthe field. This data article describes the current publicly available version\nof the 3W Dataset, which contains structural modifications and additional\nlabeled data. The detailed description provided encourages and supports the 3W\ncommunity and new 3W users to improve previous published results and to develop\nnew robust methodologies, digital products and services capable of detecting\nundesirable events in oil wells with enough anticipation to enable corrective\nor mitigating actions.", "AI": {"tldr": "The paper discusses the 3W Dataset, a publicly available, expert-labeled multivariate time series dataset for detecting undesirable events in oil wells.", "motivation": "Undesirable events in oil wells can result in significant economic, environmental, and human consequences, highlighting the need for advanced detection solutions and the absence of public datasets in this domain.", "method": "The dataset was collaboratively developed by Petrobras and upgraded over time to include structural modifications and additional labeled data for broader applicability.", "result": "The current version of the 3W Dataset is widely adopted as a key resource in the field, supporting research on early detection of adverse events in oil wells.", "conclusion": "The enhanced dataset facilitates advancements in creating AI/ML methodologies for predicting undesirable oil well events, encouraging community collaboration and innovation."}}
{"id": "2507.01561", "pdf": "https://arxiv.org/pdf/2507.01561", "abs": "https://arxiv.org/abs/2507.01561", "authors": ["Huijiang Wang", "Holger Kunz", "Timon Adler", "Fumiya Iida"], "title": "Self-Closing Suction Grippers for Industrial Grasping via Form-Flexible Design", "categories": ["cs.RO"], "comment": "This manuscript has been submitted for potential consideration at\n  IEEE publication venues", "summary": "Shape-morphing robots have shown benefits in industrial grasping. We propose\nform-flexible grippers for adaptive grasping. The design is based on the hybrid\njamming and suction mechanism, which deforms to handle objects that vary\nsignificantly in size from the aperture, including both larger and smaller\nparts. Compared with traditional grippers, the gripper achieves self-closing to\nform an airtight seal. Under a vacuum, a wide range of grasping is realized\nthrough the passive morphing mechanism at the interface that harmonizes\npressure and flow rate. This hybrid gripper showcases the capability to\nsecurely grasp an egg, as small as 54.5% of its aperture, while achieving a\nmaximum load-to-mass ratio of 94.3.", "AI": {"tldr": "The paper introduces a form-flexible gripper for adaptive grasping, leveraging a hybrid jamming and suction mechanism, enabling secure and versatile handling of objects.", "motivation": "Traditional grippers face challenges in handling objects of varying sizes, motivating the need for an adaptive, form-flexible gripper design.", "method": "The gripper is designed with a hybrid jamming and suction mechanism that uses a self-closing, airtight seal and a passive morphing interface adjustable with pressure and flow rate.", "result": "This gripper can securely handle delicate objects, like eggs smaller than its aperture, and achieves a high load-to-mass ratio of 94.3.", "conclusion": "The hybrid design demonstrates significant advancements in secure, adaptive grasping for industrial applications."}}
{"id": "2507.01543", "pdf": "https://arxiv.org/pdf/2507.01543", "abs": "https://arxiv.org/abs/2507.01543", "authors": ["Quang Minh Nguyen", "Taegyoon Kim"], "title": "Is External Information Useful for Stance Detection with LLMs?", "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "In the stance detection task, a text is classified as either favorable,\nopposing, or neutral towards a target. Prior work suggests that the use of\nexternal information, e.g., excerpts from Wikipedia, improves stance detection\nperformance. However, whether or not such information can benefit large\nlanguage models (LLMs) remains an unanswered question, despite their wide\nadoption in many reasoning tasks. In this study, we conduct a systematic\nevaluation on how Wikipedia and web search external information can affect\nstance detection across eight LLMs and in three datasets with 12 targets.\nSurprisingly, we find that such information degrades performance in most cases,\nwith macro F1 scores dropping by up to 27.9\\%. We explain this through\nexperiments showing LLMs' tendency to align their predictions with the stance\nand sentiment of the provided information rather than the ground truth stance\nof the given text. We also find that performance degradation persists with\nchain-of-thought prompting, while fine-tuning mitigates but does not fully\neliminate it. Our findings, in contrast to previous literature on BERT-based\nsystems which suggests that external information enhances performance,\nhighlight the risks of information biases in LLM-based stance classifiers. Code\nis available at https://github.com/ngqm/acl2025-stance-detection.", "AI": {"tldr": "The study evaluates the impact of external information like Wikipedia excerpts on stance detection tasks using large language models (LLMs).", "motivation": "To investigate whether external information benefits stance detection in LLMs, given previously observed advantages in BERT-based systems.", "method": "The researchers systematically analyzed the effect of Wikipedia and web search-derived information on eight LLMs across three datasets and 12 targets.", "result": "Results revealed that external information leads to performance degradation, with macro F1 scores dropping by up to 27.9%.", "conclusion": "External information biases LLM stance predictions, contrasting with prior findings for BERT-based systems. Fine-tuning slightly mitigates but doesn't entirely eliminate these biases, emphasizing risks in relying on external data for LLMs."}}
{"id": "2507.01384", "pdf": "https://arxiv.org/pdf/2507.01384", "abs": "https://arxiv.org/abs/2507.01384", "authors": ["Langyu Wang", "Bingke Zhu", "Yingying Chen", "Yiyuan Zhang", "Ming Tang", "Jinqiao Wang"], "title": "MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing", "categories": ["cs.CV"], "comment": "Accpted by ICCV 2025", "summary": "The weakly-supervised audio-visual video parsing (AVVP) aims to predict all\nmodality-specific events and locate their temporal boundaries. Despite\nsignificant progress, due to the limitations of the weakly-supervised and the\ndeficiencies of the model architecture, existing methods are lacking in\nsimultaneously improving both the segment-level prediction and the event-level\nprediction. In this work, we propose a audio-visual Mamba network with pseudo\nlabeling aUGmentation (MUG) for emphasising the uniqueness of each segment and\nexcluding the noise interference from the alternate modalities. Specifically,\nwe annotate some of the pseudo-labels based on previous work. Using unimodal\npseudo-labels, we perform cross-modal random combinations to generate new data,\nwhich can enhance the model's ability to parse various segment-level event\ncombinations. For feature processing and interaction, we employ a audio-visual\nmamba network. The AV-Mamba enhances the ability to perceive different segments\nand excludes additional modal noise while sharing similar modal information.\nOur extensive experiments demonstrate that MUG improves state-of-the-art\nresults on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of\nvisual Segment-level and audio Segment-level metrics). Our code is available at\nhttps://github.com/WangLY136/MUG.", "AI": {"tldr": "The paper introduces a method called MUG to improve weakly-supervised audio-visual video parsing, achieving superior results on the LLP dataset.", "motivation": "Existing methods for audio-visual video parsing struggle to simultaneously improve both segment-level and event-level predictions due to weak supervision and architectural deficiencies.", "method": "The paper proposes the MUG approach, which combines a novel audio-visual Mamba network and pseudo labeling augmentation. It leverages unimodal pseudo-labels and cross-modal data generation to enhance segment uniqueness and reduce noise.", "result": "Experiments show significant improvements in performance on the LLP dataset, with gains in visual and audio segment-level metrics (+2.1% and +1.2%, respectively).", "conclusion": "MUG effectively addresses weaknesses in weakly-supervised audio-visual video parsing by improving segment perception and noise reduction, offering state-of-the-art performance."}}
{"id": "2507.01050", "pdf": "https://arxiv.org/pdf/2507.01050", "abs": "https://arxiv.org/abs/2507.01050", "authors": ["Jing Yu", "Yibo Zhao", "Jiapeng Zhu", "Wenming Shao", "Bo Pang", "Zhao Zhang", "Xiang Li"], "title": "Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The widespread dissemination of toxic content on social media poses a serious\nthreat to both online environments and public discourse, highlighting the\nurgent need for detoxification methods that effectively remove toxicity while\npreserving the original semantics. However, existing approaches often struggle\nto simultaneously achieve strong detoxification performance, semantic\npreservation, and robustness to out-of-distribution data. Moreover, they\ntypically rely on costly, manually annotated parallel corpora while showing\npoor data efficiency. To address these challenges, we propose a two-stage\ntraining framework that jointly optimizes for data efficiency, semantic\npreservation, and model generalization. We first perform supervised fine-tuning\non a small set of high-quality, filtered parallel data to establish a strong\ninitialization. Then, we leverage unlabeled toxic inputs and a custom-designed\nreward model to train the LLM using Group Relative Policy Optimization.\nExperimental results demonstrate that our method effectively mitigates the\ntrade-offs faced by previous work, achieving state-of-the-art performance with\nimproved generalization and significantly reduced dependence on annotated data.\nOur code is available at:\nhttps://anonymous.4open.science/r/Detoxification-of-Text-725F/", "AI": {"tldr": "This paper introduces a two-stage training framework to enhance text detoxification on social media, improving efficiency, semantic retention, and robustness without heavy reliance on annotated data.", "motivation": "The increase of toxic content on social media creates the need for effective detoxification methods that maintain original meaning while overcoming current limitations like poor data efficiency and reliance on manual annotations.", "method": "The two-stage training involves initial supervised fine-tuning on filtered annotated data, followed by training using unlabeled toxic inputs and a custom reward model through Group Relative Policy Optimization.", "result": "The proposed method outperforms previous approaches, achieving superior generalization, better semantic preservation, reduced dependence on annotated data, and state-of-the-art detoxification performance.", "conclusion": "The framework offers an effective and efficient strategy for detoxification, addressing prior trade-offs and advancing the capabilities in mitigating toxic content."}}
{"id": "2507.01697", "pdf": "https://arxiv.org/pdf/2507.01697", "abs": "https://arxiv.org/abs/2507.01697", "authors": ["Yu Zhang", "Qi Zhou", "Xiao-Song Yang"], "title": "An RRT* algorithm based on Riemannian metric model for optimal path planning", "categories": ["cs.RO", "math.OC", "00A69, 93C85, 14H55", "I.2.9"], "comment": "27 pages", "summary": "This paper presents a Riemannian metric-based model to solve the optimal path\nplanning problem on two-dimensional smooth submanifolds in high-dimensional\nspace. Our model is based on constructing a new Riemannian metric on a\ntwo-dimensional projection plane, which is induced by the high-dimensional\nEuclidean metric on two-dimensional smooth submanifold and reflects the\nenvironmental information of the robot. The optimal path planning problem in\nhigh-dimensional space is therefore transformed into a geometric problem on the\ntwo-dimensional plane with new Riemannian metric. Based on the new Riemannian\nmetric, we proposed an incremental algorithm RRT*-R on the projection plane.\nThe experimental results show that the proposed algorithm is suitable for\nscenarios with uneven fields in multiple dimensions. The proposed algorithm can\nhelp the robot to effectively avoid areas with drastic changes in height,\nground resistance and other environmental factors. More importantly, the RRT*-R\nalgorithm shows better smoothness and optimization properties compared with the\noriginal RRT* algorithm using Euclidean distance in high-dimensional workspace.\nThe length of the entire path by RRT*-R is a good approximation of the\ntheoretical minimum geodesic distance on projection plane.", "AI": {"tldr": "This paper develops a metric-based approach for path planning on submanifolds in high-dimensional space using a Riemannian model. A new Riemannian metric enables optimization on a projected 2D plane, enhancing path planning performance in uneven fields.", "motivation": "To address the challenge of optimizing robot path planning in high-dimensional environments with uneven terrains by leveraging geometric properties of submanifolds.", "method": "The study introduces a novel Riemannian metric that captures environmental information on a 2D projection plane. Path planning is then approached as a geometric problem on this plane, and the RRT*-R incremental algorithm is developed based on this metric.", "result": "Experimental outcomes demonstrate that the RRT*-R algorithm is effective for uneven multi-dimensional fields, avoids challenging terrain features, and produces smoother and more optimized paths than the traditional RRT* algorithm.", "conclusion": "The RRT*-R algorithm provides an efficient and theoretically supported solution for robot path planning, approximating the minimum geodesic distance and handling high-dimensional environmental complexities effectively."}}
{"id": "2507.01036", "pdf": "https://arxiv.org/pdf/2507.01036", "abs": "https://arxiv.org/abs/2507.01036", "authors": ["Seth Bulin"], "title": "Systemic Constraints of Undecidability", "categories": ["cs.FL", "cs.AI", "math.LO"], "comment": "Submitted version; includes appendices with formal definitions and\n  structural embeddings. Prepared in Nature Computational Science format.\n  Keywords: computability theory, undecidability, causal systems, structural\n  closure, recursion theory, Turing machines, hypercomputation,\n  metaundecidability, epistemic limits, consciousness, modeling limits", "summary": "This paper presents a theory of systemic undecidability, reframing\nincomputability as a structural property of systems rather than a localized\nfeature of specific functions or problems. We define a notion of causal\nembedding and prove a closure principle: any subsystem that participates\nfunctionally in the computation of an undecidable system inherits its\nundecidability. This result positions undecidability as a pervasive constraint\non prediction, modeling, and epistemic access in both natural and artificial\nsystems. Our framework disarms oracle mimicry and challenges the view that\ncomputational limits can be circumvented through architectural innovation. By\ngeneralizing classical results into a dynamic systems context, this work\naugments the logical trajectory of G\\\"odel, Turing, and Chaitin, offering a new\nperspective of the topology of computability and its interrelation to the\nboundaries of scientific knowledge.", "AI": {"tldr": "The paper redefines incomputability as a structural property of systems, introducing a theory of systemic undecidability and causal embedding.", "motivation": "To challenge the conventional view that computational limits can be circumvented and to position undecidability as a systemic constraint.", "method": "Introduces the notion of causal embedding and proves a closure principle establishing that functional subsystems inherit undecidability.", "result": "Undecidability is shown to be a pervasive limitation affecting prediction, modeling, and epistemic access in systems.", "conclusion": "The work broadens classical computability results into dynamic systems and highlights the constraints on scientific knowledge."}}
{"id": "2507.01594", "pdf": "https://arxiv.org/pdf/2507.01594", "abs": "https://arxiv.org/abs/2507.01594", "authors": ["Shutong Feng", "Hsien-chin Lin", "Nurul Lubis", "Carel van Niekerk", "Michael Heck", "Benjamin Ruppik", "Renato Vukovic", "Milica Ga\u0161i\u0107"], "title": "Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation", "categories": ["cs.CL"], "comment": "19 pages, 6 figures", "summary": "Task-oriented dialogue (ToD) systems are designed to help users achieve\nspecific goals through natural language interaction. While recent advances in\nlarge language models (LLMs) have significantly improved linguistic fluency and\ncontextual understanding, building effective and emotionally intelligent ToD\nsystems remains a complex challenge. Effective ToD systems must optimise for\ntask success, emotional understanding and responsiveness, and precise\ninformation conveyance, all within inherently noisy and ambiguous\nconversational environments. In this work, we investigate architectural,\nrepresentational, optimisational as well as emotional considerations of ToD\nsystems. We set up systems covering these design considerations with a\nchallenging evaluation environment composed of a natural-language user\nsimulator coupled with an imperfect natural language understanding module. We\npropose \\textbf{LUSTER}, an \\textbf{L}LM-based \\textbf{U}nified \\textbf{S}ystem\nfor \\textbf{T}ask-oriented dialogue with \\textbf{E}nd-to-end\n\\textbf{R}einforcement learning with both short-term (user sentiment) and\nlong-term (task success) rewards. Our findings demonstrate that combining LLM\ncapability with structured reward modelling leads to more resilient and\nemotionally responsive ToD systems, offering a practical path forward for\nnext-generation conversational agents.", "AI": {"tldr": "This paper introduces LUSTER, a unified ToD system that combines LLMs with end-to-end reinforcement learning to optimize for both user sentiment and task success.", "motivation": "To address the challenges in creating emotionally intelligent and task-efficient ToD systems that operate in noisy, ambiguous conversational settings.", "method": "The authors propose LUSTER, leveraging a natural-language user simulator, an imperfect NLU module, and reinforcement learning for short-term (sentiment) and long-term (task success) rewards.", "result": "LUSTER improves resilience and emotional responsiveness in ToD systems by integrating LLM capabilities with reward modeling.", "conclusion": "The study offers a practical approach to developing next-gen ToD systems using LLMs and structured reward mechanisms."}}
{"id": "2507.01390", "pdf": "https://arxiv.org/pdf/2507.01390", "abs": "https://arxiv.org/abs/2507.01390", "authors": ["Shuai Tan", "Bill Gong", "Bin Ji", "Ye Pan"], "title": "FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases", "categories": ["cs.CV"], "comment": null, "summary": "Talking head generation is gaining significant importance across various\ndomains, with a growing demand for high-quality rendering. However, existing\nmethods often suffer from identity leakage (IL) and rendering artifacts (RA),\nparticularly in extreme cases. Through an in-depth analysis of previous\napproaches, we identify two key insights: (1) IL arises from identity\ninformation embedded within motion features, and (2) this identity information\ncan be leveraged to address RA. Building on these findings, this paper\nintroduces FixTalk, a novel framework designed to simultaneously resolve both\nissues for high-quality talking head generation. Firstly, we propose an\nEnhanced Motion Indicator (EMI) to effectively decouple identity information\nfrom motion features, mitigating the impact of IL on generated talking heads.\nTo address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes\nthe leaked identity information to supplement missing details, thus fixing the\nartifacts. Extensive experiments demonstrate that FixTalk effectively mitigates\nIL and RA, achieving superior performance compared to state-of-the-art methods.", "AI": {"tldr": "FixTalk is a novel framework for high-quality talking head generation that resolves identity leakage (IL) and rendering artifacts (RA) using Enhanced Motion Indicator (EMI) and Enhanced Detail Indicator (EDI).", "motivation": "The paper was motivated by the challenges of identity leakage and rendering artifacts in current talking head generation methods, particularly in extreme cases, and the need for improved quality rendering.", "method": "The authors introduced FixTalk, which includes two key components: Enhanced Motion Indicator (EMI) to decouple identity from motion features, and Enhanced Detail Indicator (EDI) to utilize leaked identity information for correcting artifacts.", "result": "FixTalk effectively reduces identity leakage and rendering artifacts, achieving superior results compared to existing state-of-the-art methods in talking head generation.", "conclusion": "FixTalk demonstrates a novel and effective approach to enhance the quality of talking head generation by simultaneously addressing identity leakage and rendering artifacts through innovative indicators."}}
{"id": "2507.01705", "pdf": "https://arxiv.org/pdf/2507.01705", "abs": "https://arxiv.org/abs/2507.01705", "authors": ["Marc-Philip Ecker", "Bernhard Bischof", "Minh Nhat Vu", "Christoph Fr\u00f6hlich", "Tobias Gl\u00fcck", "Wolfgang Kemmetm\u00fcller"], "title": "Efficient Collision Detection for Long and Slender Robotic Links in Euclidean Distance Fields: Application to a Forestry Crane", "categories": ["cs.RO"], "comment": "Accepted at IROS 2025", "summary": "Collision-free motion planning in complex outdoor environments relies heavily\non perceiving the surroundings through exteroceptive sensors. A widely used\napproach represents the environment as a voxelized Euclidean distance field,\nwhere robots are typically approximated by spheres. However, for large-scale\nmanipulators such as forestry cranes, which feature long and slender links,\nthis conventional spherical approximation becomes inefficient and inaccurate.\nThis work presents a novel collision detection algorithm specifically designed\nto exploit the elongated structure of such manipulators, significantly\nenhancing the computational efficiency of motion planning algorithms. Unlike\ntraditional sphere decomposition methods, our approach not only improves\ncomputational efficiency but also naturally eliminates the need to fine-tune\nthe approximation accuracy as an additional parameter. We validate the\nalgorithm's effectiveness using real-world LiDAR data from a forestry crane\napplication, as well as simulated environment data.", "AI": {"tldr": "This paper introduces a collision detection algorithm to enhance motion planning for large-scale manipulators like forestry cranes, addressing inefficiencies in traditional spherical approximations.", "motivation": "The spherical approximation of robots in voxelized distance fields is inefficient for manipulators with elongated structures, such as forestry cranes.", "method": "A collision detection algorithm tailored for elongated manipulators, improving computational efficiency and removing the need for accuracy fine-tuning.", "result": "Validation through real-world LiDAR data and simulated environments demonstrates its effectiveness.", "conclusion": "The proposed algorithm significantly boosts the computational efficiency and accuracy of motion planning in complex outdoor environments."}}
{"id": "2507.01627", "pdf": "https://arxiv.org/pdf/2507.01627", "abs": "https://arxiv.org/abs/2507.01627", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "title": "Chart Question Answering from Real-World Analytical Narratives", "categories": ["cs.CL"], "comment": "This paper has been accepted to the ACL Student Research Workshop\n  (SRW) 2025", "summary": "We present a new dataset for chart question answering (CQA) constructed from\nvisualization notebooks. The dataset features real-world, multi-view charts\npaired with natural language questions grounded in analytical narratives.\nUnlike prior benchmarks, our data reflects ecologically valid reasoning\nworkflows. Benchmarking state-of-the-art multimodal large language models\nreveals a significant performance gap, with GPT-4.1 achieving an accuracy of\n69.3%, underscoring the challenges posed by this more authentic CQA setting.", "AI": {"tldr": "A new dataset for chart question answering (CQA) is designed to reflect real-world reasoning workflows, and multimodal language models exhibit significant challenges in performing well.", "motivation": "There is a need for a dataset that mirrors authentic reasoning workflows, as prior benchmarks for CQA fail to reflect ecologically valid contexts.", "method": "A dataset was constructed using real-world, multi-view charts from visualization notebooks, paired with analytically grounded natural language questions.", "result": "GPT-4.1 achieved a 69.3% accuracy in benchmark tests, indicating a significant performance gap in handling authentic CQA settings.", "conclusion": "The dataset presents a more authentic CQA challenge, revealing the limitations of current state-of-the-art models in natural reasoning workflows."}}
{"id": "2507.01397", "pdf": "https://arxiv.org/pdf/2507.01397", "abs": "https://arxiv.org/abs/2507.01397", "authors": ["Khanh Son Pham", "Christian Witte", "Jens Behley", "Johannes Betz", "Cyrill Stachniss"], "title": "Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at IROS 2025", "summary": "Most autonomous cars rely on the availability of high-definition (HD) maps.\nCurrent research aims to address this constraint by directly predicting HD map\nelements from onboard sensors and reasoning about the relationships between the\npredicted map and traffic elements. Despite recent advancements, the coherent\nonline construction of HD maps remains a challenging endeavor, as it\nnecessitates modeling the high complexity of road topologies in a unified and\nconsistent manner. To address this challenge, we propose a coherent approach to\npredict lane segments and their corresponding topology, as well as road\nboundaries, all by leveraging prior map information represented by commonly\navailable standard-definition (SD) maps. We propose a network architecture,\nwhich leverages hybrid lane segment encodings comprising prior information and\ndenoising techniques to enhance training stability and performance.\nFurthermore, we facilitate past frames for temporal consistency. Our\nexperimental evaluation demonstrates that our approach outperforms previous\nmethods by a large margin, highlighting the benefits of our modeling scheme.", "AI": {"tldr": "The authors address the challenge of online HD map construction for autonomous cars by proposing a network that predicts lane segments, road boundaries, and topology using SD map data and temporal consistency techniques.", "motivation": "Current autonomous vehicle systems require HD maps, which are resource-intensive to construct, motivating a search for alternative methods to predict HD maps directly from onboard sensor data and SD maps.", "method": "The paper introduces a network architecture that uses hybrid lane segment encodings, prior map information, denoising techniques, and temporal consistency achieved via past frame facilitation.", "result": "The proposed model outperformed previous methods significantly in predicting coherent HD map elements such as lane boundaries and topology.", "conclusion": "This work offers a more efficient, accurate method for constructing HD maps online, leveraging SD maps and advanced modeling techniques."}}
{"id": "2507.01054", "pdf": "https://arxiv.org/pdf/2507.01054", "abs": "https://arxiv.org/abs/2507.01054", "authors": ["Jithendaraa Subramanian", "Linda Hung", "Daniel Schweigert", "Santosh Suram", "Weike Ye"], "title": "XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Recent advances in materials discovery have been driven by structure-based\nmodels, particularly those using crystal graphs. While effective for\ncomputational datasets, these models are impractical for real-world\napplications where atomic structures are often unknown or difficult to obtain.\nWe propose a scalable multimodal framework that learns directly from elemental\ncomposition and X-ray diffraction (XRD) -- two of the more available modalities\nin experimental workflows without requiring crystal structure input. Our\narchitecture integrates modality-specific encoders with a cross-attention\nfusion module and is trained on the 5-million-sample Alexandria dataset. We\npresent masked XRD modeling (MXM), and apply MXM and contrastive alignment as\nself-supervised pretraining strategies. Pretraining yields faster convergence\n(up to 4.2x speedup) and improves both accuracy and representation quality. We\nfurther demonstrate that multimodal performance scales more favorably with\ndataset size than unimodal baselines, with gains compounding at larger data\nregimes. Our results establish a path toward structure-free, experimentally\ngrounded foundation models for materials science.", "AI": {"tldr": "This paper introduces a multimodal framework leveraging elemental composition and X-ray diffraction (XRD) data to advance materials discovery, eliminating the need for crystal structures.", "motivation": "Current structure-based models rely heavily on computational datasets and cannot be applied effectively when atomic structures are unavailable or difficult to access.", "method": "The proposed framework integrates specific encoders for each modality with a cross-attention fusion module, utilizing the Alexandria dataset and incorporating two pretraining strategies: masked XRD modeling (MXM) and contrastive alignment.", "result": "Pretraining accelerates convergence speeds up to 4.2x, enhances accuracy and representation quality, and demonstrates scalable multimodal performance surpassing unimodal approaches as dataset sizes increase.", "conclusion": "This study presents an innovative approach for materials discovery rooted in experimental data, showcasing the potential for scalable, structure-free foundation models in science."}}
{"id": "2507.01723", "pdf": "https://arxiv.org/pdf/2507.01723", "abs": "https://arxiv.org/abs/2507.01723", "authors": ["Xupeng Zhu", "Fan Wang", "Robin Walters", "Jane Shi"], "title": "SE(3)-Equivariant Diffusion Policy in Spherical Fourier Space", "categories": ["cs.RO"], "comment": "Accepted at ICML 2025", "summary": "Diffusion Policies are effective at learning closed-loop manipulation\npolicies from human demonstrations but generalize poorly to novel arrangements\nof objects in 3D space, hurting real-world performance. To address this issue,\nwe propose Spherical Diffusion Policy (SDP), an SE(3) equivariant diffusion\npolicy that adapts trajectories according to 3D transformations of the scene.\nSuch equivariance is achieved by embedding the states, actions, and the\ndenoising process in spherical Fourier space. Additionally, we employ novel\nspherical FiLM layers to condition the action denoising process equivariantly\non the scene embeddings. Lastly, we propose a spherical denoising temporal\nU-net that achieves spatiotemporal equivariance with computational efficiency.\nIn the end, SDP is end-to-end SE(3) equivariant, allowing robust generalization\nacross transformed 3D scenes. SDP demonstrates a large performance improvement\nover strong baselines in 20 simulation tasks and 5 physical robot tasks\nincluding single-arm and bi-manual embodiments. Code is available at\nhttps://github.com/amazon-science/Spherical_Diffusion_Policy.", "AI": {"tldr": "The paper introduces Spherical Diffusion Policy (SDP) which achieves robust generalization in 3D manipulation tasks with SE(3) equivariance, outperforming strong baselines in simulated and physical robot tasks.", "motivation": "The motivation is to improve the generalization capability of closed-loop manipulation policies to novel 3D object arrangements, as existing diffusion policies struggle in such scenarios.", "method": "The paper introduces SDP, which embeds states, actions, and the denoising process in spherical Fourier space and uses spherical FiLM layers and a spherical denoising temporal U-net for efficient spatiotemporal processing.", "result": "SDP showed significant performance improvements across 20 simulation tasks and 5 physical robot tasks, including single-arm and bi-manual setups.", "conclusion": "SDP is an effective SE(3) equivariant manipulation policy that can robustly generalize across transformed 3D scenes, facilitating better real-world applications."}}
{"id": "2507.01633", "pdf": "https://arxiv.org/pdf/2507.01633", "abs": "https://arxiv.org/abs/2507.01633", "authors": ["Georgii Levtsov", "Dmitry Ustalov"], "title": "Confidence and Stability of Global and Pairwise Scores in NLP Evaluation", "categories": ["cs.CL", "cs.IR", "62-04", "D.2.3"], "comment": "8 pages, accepted at ACL SRW 2025", "summary": "With the advent of highly capable instruction-tuned neural language models,\nbenchmarking in natural language processing (NLP) is increasingly shifting\ntowards pairwise comparison leaderboards, such as LMSYS Arena, from traditional\nglobal pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper\nempirically investigates the strengths and weaknesses of both global scores and\npairwise comparisons to aid decision-making in selecting appropriate model\nevaluation strategies. Through computational experiments on synthetic and\nreal-world datasets using standard global metrics and the popular Bradley-Terry\nmodel for pairwise comparisons, we found that while global scores provide more\nreliable overall rankings, they can underestimate strong models with rare,\nsignificant errors or low confidence. Conversely, pairwise comparisons are\nparticularly effective for identifying strong contenders among models with\nlower global scores, especially where quality metrics are hard to define (e.g.,\ntext generation), though they require more comparisons to converge if ties are\nfrequent. Our code and data are available at\nhttps://github.com/HSPyroblast/srw-ranking under a permissive license.", "AI": {"tldr": "This paper compares global score-based evaluations with pairwise comparisons for NLP model benchmarking, finding strengths and limitations in both.", "motivation": "The motivation lies in understanding and improving the effectiveness of model evaluation strategies as NLP benchmarking shifts from global scoring to pairwise comparison leaderboards.", "method": "The study uses computational experiments on synthetic and real-world datasets, leveraging standard global metrics and the Bradley-Terry model for pairwise comparisons.", "result": "Global scores offer reliable rankings but may undervalue models with rare significant errors, while pairwise comparisons excel in identifying strong models with lower global scores, particularly in challenging domains like text generation.", "conclusion": "Both evaluation methods are complementary, with global scores better for general reliability and pairwise comparisons more suited for nuanced insights, especially in areas with hard-to-define metrics."}}
{"id": "2507.01401", "pdf": "https://arxiv.org/pdf/2507.01401", "abs": "https://arxiv.org/abs/2507.01401", "authors": ["Huanwen Liang", "Jingxian Xu", "Yuanji Zhang", "Yuhao Huang", "Yuhan Zhang", "Xin Yang", "Ran Li", "Xuedong Deng", "Yanjun Liu", "Guowei Tao", "Yun Wu", "Sheng Zhao", "Xinru Gao", "Dong Ni"], "title": "Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by MICCAI 2025", "summary": "Fetal abdominal malformations are serious congenital anomalies that require\naccurate diagnosis to guide pregnancy management and reduce mortality. Although\nAI has demonstrated significant potential in medical diagnosis, its application\nto prenatal abdominal anomalies remains limited. Most existing studies focus on\nimage-level classification and rely on standard plane localization, placing\nless emphasis on case-level diagnosis. In this paper, we develop a case-level\nmultiple instance learning (MIL)-based method, free of standard plane\nlocalization, for classifying fetal abdominal anomalies in prenatal ultrasound.\nOur contribution is three-fold. First, we adopt a mixture-of-attention-experts\nmodule (MoAE) to weight different attention heads for various planes. Secondly,\nwe propose a medical-knowledge-driven feature selection module (MFS) to align\nimage features with medical knowledge, performing self-supervised image token\nselection at the case-level. Finally, we propose a prompt-based prototype\nlearning (PPL) to enhance the MFS. Extensively validated on a large prenatal\nabdominal ultrasound dataset containing 2,419 cases, with a total of 24,748\nimages and 6 categories, our proposed method outperforms the state-of-the-art\ncompetitors. Codes are available at:https://github.com/LL-AC/AAcls.", "AI": {"tldr": "This paper presents a novel AI-based method for classifying fetal abdominal anomalies using prenatal ultrasounds, outperforming existing techniques by employing multiple instance learning (MIL) and innovative modules tailored to medical knowledge.", "motivation": "The study addresses the limited application of AI in diagnosing fetal abdominal malformations, which are life-threatening congenital anomalies requiring precise and case-level diagnostic tools beyond standard plane localization.", "method": "The authors introduce a multiple instance learning (MIL)-based framework featuring three key components: a mixture-of-attention-experts module (MoAE) for weighting various attention heads, a medical-knowledge-driven feature selection module (MFS) for aligning image features with medical expertise, and a prompt-based prototype learning (PPL) approach to enhance the MFS.", "result": "The proposed method outperforms state-of-the-art techniques, as validated on a robust dataset of 2,419 cases and 24,748 images spanning six categories of prenatal ultrasounds.", "conclusion": "The study demonstrates that incorporating medical knowledge-driven feature selection and attention mechanisms in a case-level MIL setting can significantly improve the diagnosis of prenatal abdominal anomalies, providing a promising tool for clinical application."}}
{"id": "2507.01056", "pdf": "https://arxiv.org/pdf/2507.01056", "abs": "https://arxiv.org/abs/2507.01056", "authors": ["Lidan Peng", "Lu Gao", "Feng Hong", "Jingran Sun"], "title": "Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI", "categories": ["cs.LG"], "comment": null, "summary": "Flooding can damage pavement infrastructure significantly, causing both\nimmediate and long-term structural and functional issues. This research\ninvestigates how flooding events affect pavement deterioration, specifically\nfocusing on measuring pavement roughness by the International Roughness Index\n(IRI). To quantify these effects, we utilized 20 years of pavement condition\ndata from TxDOT's PMIS database, which is integrated with flood event data,\nincluding duration and spatial extent. Statistical analyses were performed to\ncompare IRI values before and after flooding and to calculate the deterioration\nrates influenced by flood exposure. Moreover, we applied Explainable Artificial\nIntelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and\nLocal Interpretable Model-Agnostic Explanations (LIME), to assess the impact of\nflooding on pavement performance. The results demonstrate that flood-affected\npavements experience a more rapid increase in roughness compared to non-flooded\nsections. These findings emphasize the need for proactive flood mitigation\nstrategies, including improved drainage systems, flood-resistant materials, and\npreventative maintenance, to enhance pavement resilience in vulnerable regions.", "AI": {"tldr": "This paper evaluates how flooding impacts pavement roughness deterioration using International Roughness Index (IRI) data and explains findings through advanced AI techniques like SHAP and LIME.", "motivation": "Flooding poses both immediate and long-term risks to pavement infrastructure, emphasizing the need for understanding and mitigating its deterioration effects.", "method": "The study analyzes 20 years of pavement condition data from TxDOT's PMIS database integrated with flood event details. Statistical tools are used to compare pre- and post-flood IRI values, and XAI methods such as SHAP and LIME assess the influence of flooding on pavement conditions.", "result": "Flood-affected pavements deteriorate more rapidly in roughness than non-flooded sections, as evidenced by data-driven evaluations.", "conclusion": "The study underscores the importance of proactive flood mitigation, including better drainage systems, resilient materials, and preventative maintenance, to bolster pavement longevity in flood-prone areas."}}
{"id": "2507.01753", "pdf": "https://arxiv.org/pdf/2507.01753", "abs": "https://arxiv.org/abs/2507.01753", "authors": ["Yash Kulkarni", "Susheela Sharma", "Omid Rezayof", "Siddhartha Kapuria", "Jordan P. Amadio", "Mohsen Khadem", "Maryam Tilton", "Farshid Alambeigi"], "title": "Augmented Bridge Spinal Fixation: A New Concept for Addressing Pedicle Screw Pullout via a Steerable Drilling Robot and Flexible Pedicle Screws", "categories": ["cs.RO"], "comment": null, "summary": "To address the screw loosening and pullout limitations of rigid pedicle\nscrews in spinal fixation procedures, and to leverage our recently developed\nConcentric Tube Steerable Drilling Robot (CT-SDR) and Flexible Pedicle Screw\n(FPS), in this paper, we introduce the concept of Augmented Bridge Spinal\nFixation (AB-SF). In this concept, two connecting J-shape tunnels are first\ndrilled through pedicles of vertebra using the CT-SDR. Next, two FPSs are\npassed through this tunnel and bone cement is then injected through the\ncannulated region of the FPS to form an augmented bridge between two pedicles\nand reinforce strength of the fixated spine. To experimentally analyze and\nstudy the feasibility of AB-SF technique, we first used our robotic system\n(i.e., a CT-SDR integrated with a robotic arm) to create two different fixation\nscenarios in which two J-shape tunnels, forming a bridge, were drilled at\ndifferent depth of a vertebral phantom. Next, we implanted two FPSs within the\ndrilled tunnels and then successfully simulated the bone cement augmentation\nprocess.", "AI": {"tldr": "This paper proposes the Augmented Bridge Spinal Fixation (AB-SF) technique using a robotic system and flexible pedicle screws for stronger spinal fixation.", "motivation": "The motivation is to overcome the limitations of rigid pedicle screws, such as screw loosening and pullout, during spinal fixation procedures.", "method": "A Concentric Tube Steerable Drilling Robot (CT-SDR) is used to create inter-pedicle tunnels, followed by implantation of Flexible Pedicle Screws (FPS) and bone cement injection for augmentation.", "result": "Two successful fixation scenarios were created using a robotic arm, demonstrating feasibility through tunnel formation, screw placement, and cement augmentation in a vertebral phantom.", "conclusion": "The AB-SF concept shows potential for improving spinal fixation strength and broadening the application of robotic and flexible screw technologies."}}
{"id": "2507.01645", "pdf": "https://arxiv.org/pdf/2507.01645", "abs": "https://arxiv.org/abs/2507.01645", "authors": ["Rifki Afina Putri"], "title": "Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings", "categories": ["cs.CL"], "comment": "AMLDS 2025", "summary": "In this paper, we investigate the transferability of pre-trained language\nmodels to low-resource Indonesian local languages through the task of sentiment\nanalysis. We evaluate both zero-shot performance and adapter-based transfer on\nten local languages using models of different types: a monolingual Indonesian\nBERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based\napproach called MAD-X. To better understand model behavior, we group the target\nlanguages into three categories: seen (included during pre-training), partially\nseen (not included but linguistically related to seen languages), and unseen\n(absent and unrelated in pre-training data). Our results reveal clear\nperformance disparities across these groups: multilingual models perform best\non seen languages, moderately on partially seen ones, and poorly on unseen\nlanguages. We find that MAD-X significantly improves performance, especially\nfor seen and partially seen languages, without requiring labeled data in the\ntarget language. Additionally, we conduct a further analysis on tokenization\nand show that while subword fragmentation and vocabulary overlap with\nIndonesian correlate weakly with prediction quality, they do not fully explain\nthe observed performance. Instead, the most consistent predictor of transfer\nsuccess is the model's prior exposure to the language, either directly or\nthrough a related language.", "AI": {"tldr": "This paper explores sentiment analysis in local Indonesian languages using pre-trained language models, revealing performance differences based on language exposure during pre-training.", "motivation": "The study aims to address the challenges of applying sentiment analysis to low-resource Indonesian local languages.", "method": "Zero-shot evaluations and adapter-based transfer approaches (MAD-X) are applied to ten local languages and grouped into 'seen,' 'partially seen,' and 'unseen' language categories.", "result": "Multilingual models like mBERT and XLM-R perform variably based on prior language exposure, and MAD-X notably improves performance for seen and partially seen languages.", "conclusion": "Performance in sentiment analysis correlates most with model exposure during pre-training. Tokenization factors have a weaker correlation."}}
{"id": "2507.01409", "pdf": "https://arxiv.org/pdf/2507.01409", "abs": "https://arxiv.org/abs/2507.01409", "authors": ["Kuniaki Saito", "Donghyun Kim", "Kwanyong Park", "Atsushi Hashimoto", "Yoshitaka Ushiku"], "title": "CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning", "categories": ["cs.CV"], "comment": "Accepted to ICCV2025", "summary": "An image captioning model flexibly switching its language pattern, e.g.,\ndescriptiveness and length, should be useful since it can be applied to diverse\napplications. However, despite the dramatic improvement in generative\nvision-language models, fine-grained control over the properties of generated\ncaptions is not easy due to two reasons: (i) existing models are not given the\nproperties as a condition during training and (ii) existing models cannot\nsmoothly transition its language pattern from one state to the other. Given\nthis challenge, we propose a new approach, CaptionSmiths, to acquire a single\ncaptioning model that can handle diverse language patterns. First, our approach\nquantifies three properties of each caption, length, descriptiveness, and\nuniqueness of a word, as continuous scalar values, without human annotation.\nGiven the values, we represent the conditioning via interpolation between two\nendpoint vectors corresponding to the extreme states, e.g., one for a very\nshort caption and one for a very long caption. Empirical results demonstrate\nthat the resulting model can smoothly change the properties of the output\ncaptions and show higher lexical alignment than baselines. For instance,\nCaptionSmiths reduces the error in controlling caption length by 506\\% despite\nbetter lexical alignment. Code will be available on\nhttps://github.com/omron-sinicx/captionsmiths.", "AI": {"tldr": "CaptionSmiths, a novel image captioning model, enables control over length, descriptiveness, and uniqueness in captions using interpolated conditioning between endpoints.", "motivation": "To create an image captioning model that can flexibly adjust language patterns (e.g., descriptiveness, length) for diverse applications, overcoming limitations in fine-grained control in existing models.", "method": "CaptionSmiths quantifies caption properties (length, descriptiveness, uniqueness) as scalar values, represents these values as interpolated vectors between extreme states, and trains the model to generate outputs based on these vectors.", "result": "CaptionSmiths enables smooth transitions in the properties of captions while achieving better lexical alignment and reducing errors in length control by 50.6%.", "conclusion": "The model demonstrates that conditioning captions using interpolated vectors effectively allows for fine-grained control over properties, making the approach viable for diverse applications."}}
{"id": "2507.01057", "pdf": "https://arxiv.org/pdf/2507.01057", "abs": "https://arxiv.org/abs/2507.01057", "authors": ["Lushun Fan", "Yuqin Xia", "Jun Li", "Karl Jenkins"], "title": "Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates", "categories": ["cs.LG", "physics.flu-dyn"], "comment": null, "summary": "In this study, an innovative intelligent optimization system for mesh quality\nis proposed, which is based on a deep convolutional neural network\narchitecture, to achieve mesh generation and optimization. The core of the\nstudy is the Loop2Net generator and loss function, it predicts the mesh based\non the given wing coordinates. And the model's performance is continuously\noptimised by two key loss functions during the training. Then discipline by\nadding penalties, the goal of mesh generation was finally reached.", "AI": {"tldr": "The paper proposes an intelligent optimization system using deep learning for mesh quality and generation, focusing on a CNN-based Loop2Net framework.", "motivation": "To develop a method for achieving automated and high-quality mesh generation and optimization for complex structures like wings, addressing current limitations.", "method": "A deep convolutional neural network (Loop2Net) predicts meshes from wing coordinates, optimizing its performance during training with two key loss functions and penalty-based discipline.", "result": "The study designed a robust mesh prediction and optimization system capable of generating high-quality meshes efficiently.", "conclusion": "The proposed intelligent optimization system successfully demonstrates the ability to predict and optimize meshes using advanced deep learning methods."}}
{"id": "2507.01779", "pdf": "https://arxiv.org/pdf/2507.01779", "abs": "https://arxiv.org/abs/2507.01779", "authors": ["Daniyal Maroufi", "Xinyuan Huang", "Yash Kulkarni", "Omid Rezayof", "Susheela Sharma", "Vaibhav Goggela", "Jordan P. Amadio", "Mohsen Khadem", "Farshid Alambeigi"], "title": "S3D: A Spatial Steerable Surgical Drilling Framework for Robotic Spinal Fixation Procedures", "categories": ["cs.RO"], "comment": null, "summary": "In this paper, we introduce S3D: A Spatial Steerable Surgical Drilling\nFramework for Robotic Spinal Fixation Procedures. S3D is designed to enable\nrealistic steerable drilling while accounting for the anatomical constraints\nassociated with vertebral access in spinal fixation (SF) procedures. To achieve\nthis, we first enhanced our previously designed concentric tube Steerable\nDrilling Robot (CT-SDR) to facilitate steerable drilling across all vertebral\nlevels of the spinal column. Additionally, we propose a four-Phase calibration,\nregistration, and navigation procedure to perform realistic SF procedures on a\nspine holder phantom by integrating the CT-SDR with a seven-degree-of-freedom\nrobotic manipulator. The functionality of this framework is validated through\nplanar and out-of-plane steerable drilling experiments in vertebral phantoms.", "AI": {"tldr": "This paper introduces S3D, a steerable surgical drilling framework designed for robotic spinal fixation procedures, validated through experiments on vertebral phantoms.", "motivation": "The aim is to enhance precision and adaptability in robotic spinal fixation procedures by considering anatomical constraints and enabling realistic steerable drilling.", "method": "The approach involves improving the concentric tube steerable drilling robot (CT-SDR) and integrating it with a robotic manipulator, alongside a calibration, registration, and navigation procedure for spinal phantom experiments.", "result": "Experiments on vertebral phantoms demonstrated the framework's ability to perform planar and out-of-plane steerable drilling, validating the proposed approach.", "conclusion": "The study successfully developed and validated a spatial steerable surgical drilling framework, advancing the capability for accurate spinal fixation using robotics."}}
{"id": "2507.01702", "pdf": "https://arxiv.org/pdf/2507.01702", "abs": "https://arxiv.org/abs/2507.01702", "authors": ["Zixin Chen", "Hongzhan Lin", "Kaixin Li", "Ziyang Luo", "Zhen Ye", "Guang Chen", "Zhiyong Huang", "Jing Ma"], "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme.", "AI": {"tldr": "This paper introduces AdamMeme, an adaptable agent-based framework to evaluate multimodal Large Language Models (mLLMs) in understanding harmful memes, addressing the limitations of static dataset benchmarks with iterative updates.", "motivation": "The shift of memes to dynamic multimodal forms on social media demands better evaluation methods for mLLMs in judging meme harmfulness, which static benchmarks fail to address.", "method": "The authors developed AdamMeme, a collaborative multi-agent framework that iteratively probes mLLMs with evolving meme datasets to uncover model-specific deficiencies in understanding harmfulness.", "result": "Extensive experiments show AdamMeme is effective in exposing varying performance and weaknesses across different mLLMs, enabling more thorough evaluations.", "conclusion": "AdamMeme provides a flexible and adaptive solution for assessing mLLMs, surpassing the static methods currently in use, and can systematically identify areas for model improvement in meme harmfulness handling."}}
{"id": "2507.01417", "pdf": "https://arxiv.org/pdf/2507.01417", "abs": "https://arxiv.org/abs/2507.01417", "authors": ["Jiawei Gu", "Ziyue Qiao", "Zechao Li"], "title": "Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICCV 2025", "summary": "Out-of-Distribution (OOD) detection is critical for safely deploying deep\nmodels in open-world environments, where inputs may lie outside the training\ndistribution. During inference on a model trained exclusively with\nIn-Distribution (ID) data, we observe a salient gradient phenomenon: around an\nID sample, the local gradient directions for \"enhancing\" that sample's\npredicted class remain relatively consistent, whereas OOD samples--unseen in\ntraining--exhibit disorganized or conflicting gradient directions in the same\nneighborhood. Motivated by this observation, we propose an inference-stage\ntechnique to short-circuit those feature coordinates that spurious gradients\nexploit to inflate OOD confidence, while leaving ID classification largely\nintact. To circumvent the expense of recomputing the logits after this gradient\nshort-circuit, we further introduce a local first-order approximation that\naccurately captures the post-modification outputs without a second forward\npass. Experiments on standard OOD benchmarks show our approach yields\nsubstantial improvements. Moreover, the method is lightweight and requires\nminimal changes to the standard inference pipeline, offering a practical path\ntoward robust OOD detection in real-world applications.", "AI": {"tldr": "The paper introduces a lightweight, practical inference-stage approach for Out-of-Distribution (OOD) detection based on observed gradient phenomena, showing improved robustness and minimal computational overhead.", "motivation": "To address the critical challenge of detecting inputs outside the training distribution (OOD) in deep models deployed in open-world environments.", "method": "The authors propose an inference-stage technique to suppress the impact of spurious gradients that inflate OOD confidence by short-circuiting certain feature coordinates, while approximating logits with a first-order method to avoid additional computation costs.", "result": "Their method yields significant improvements in OOD detection as demonstrated on standard benchmarks, with minimal disruption to the classification of in-distribution (ID) data.", "conclusion": "The approach is lightweight, requires minimal changes to standard inference pipelines, and provides a practical solution for robust OOD detection in real-world scenarios."}}
{"id": "2507.01811", "pdf": "https://arxiv.org/pdf/2507.01811", "abs": "https://arxiv.org/abs/2507.01811", "authors": ["Yash Kulkarni", "Susheela Sharma", "Sarah Go", "Jordan P. Amadio", "Mohsen Khadem", "Farshid Alambeigi"], "title": "Towards Design and Development of a Concentric Tube Steerable Drilling Robot for Creating S-shape Tunnels for Pelvic Fixation Procedures", "categories": ["cs.RO"], "comment": null, "summary": "Current pelvic fixation techniques rely on rigid drilling tools, which\ninherently constrain the placement of rigid medical screws in the complex\nanatomy of pelvis. These constraints prevent medical screws from following\nanatomically optimal pathways and force clinicians to fixate screws in linear\ntrajectories. This suboptimal approach, combined with the unnatural placement\nof the excessively long screws, lead to complications such as screw\nmisplacement, extended surgery times, and increased radiation exposure due to\nrepeated X-ray images taken ensure to safety of procedure. To address these\nchallenges, in this paper, we present the design and development of a unique 4\ndegree-of-freedom (DoF) pelvic concentric tube steerable drilling robot (pelvic\nCT-SDR). The pelvic CT-SDR is capable of creating long S-shaped drilling\ntrajectories that follow the natural curvatures of the pelvic anatomy. The\nperformance of the pelvic CT-SDR was thoroughly evaluated through several\nS-shape drilling experiments in simulated bone phantoms.", "AI": {"tldr": "Current methods for pelvic fixation are rigid and lead to complications; a new robotic system (pelvic CT-SDR) tackles this with steerable and anatomically optimized screw pathways.", "motivation": "Existing pelvic fixation techniques result in suboptimal screw placement, leading to complications such as misplacement, longer surgeries, and increased radiation exposure.", "method": "The study introduces a 4 DoF concentric tube steerable drilling robot (pelvic CT-SDR) capable of achieving S-shaped drilling trajectories suitable for pelvic anatomy.", "result": "Experiments using simulated bone phantoms demonstrated the effectiveness of the pelvic CT-SDR for creating anatomically appropriate screw paths.", "conclusion": "The pelvic CT-SDR provides a promising solution for enhancing pelvic fixation, reducing complications, and improving surgical outcomes."}}
{"id": "2507.01042", "pdf": "https://arxiv.org/pdf/2507.01042", "abs": "https://arxiv.org/abs/2507.01042", "authors": ["Harsh Joshi", "Gautam Siddharth Kashyap", "Rafiq Ali", "Ebad Shabbir", "Niharika Jain", "Sarthak Jain", "Jiechao Gao", "Usman Naseem"], "title": "Can Argus Judge Them All? Comparing VLMs Across Domains", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Vision-Language Models (VLMs) are advancing multimodal AI, yet their\nperformance consistency across tasks is underexamined. We benchmark CLIP, BLIP,\nand LXMERT across diverse datasets spanning retrieval, captioning, and\nreasoning. Our evaluation includes task accuracy, generation quality,\nefficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows\nstrongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT\nleads in structured reasoning. These results expose trade-offs between\ngeneralization and specialization, informing industrial deployment of VLMs and\nguiding development toward robust, task-flexible architectures.", "AI": {"tldr": "The paper evaluates Vision-Language Models (VLMs) for consistency across tasks and datasets. It introduces a new Cross-Dataset Consistency (CDC) metric.", "motivation": "To analyze the trade-offs in VLM performance across tasks and datasets, aiming to improve their robustness and versatility for industrial applications.", "method": "The paper benchmarks three VLMs\u2014CLIP, BLIP, and LXMERT\u2014using metrics like task accuracy, generation quality, efficiency, and the novel CDC metric.", "result": "CLIP has the best generalization (CDC: 0.92), BLIP performs well on curated datasets, and LXMERT excels in structured reasoning tasks.", "conclusion": "The study highlights performance trade-offs among VLMs, offering insights for building more task-flexible and robust architectures for practical use."}}
{"id": "2507.01715", "pdf": "https://arxiv.org/pdf/2507.01715", "abs": "https://arxiv.org/abs/2507.01715", "authors": ["Aditya Tomar", "Rudra Murthy", "Pushpak Bhattacharyya"], "title": "Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach", "categories": ["cs.CL"], "comment": null, "summary": "Bias and stereotypes in language models can cause harm, especially in\nsensitive areas like content moderation and decision-making. This paper\naddresses bias and stereotype detection by exploring how jointly learning these\ntasks enhances model performance. We introduce StereoBias, a unique dataset\nlabeled for bias and stereotype detection across five categories: religion,\ngender, socio-economic status, race, profession, and others, enabling a deeper\nstudy of their relationship. Our experiments compare encoder-only models and\nfine-tuned decoder-only models using QLoRA. While encoder-only models perform\nwell, decoder-only models also show competitive results. Crucially, joint\ntraining on bias and stereotype detection significantly improves bias detection\ncompared to training them separately. Additional experiments with sentiment\nanalysis confirm that the improvements stem from the connection between bias\nand stereotypes, not multi-task learning alone. These findings highlight the\nvalue of leveraging stereotype information to build fairer and more effective\nAI systems.", "AI": {"tldr": "StereoBias paper introduces a dataset labeled for joint bias and stereotype detection tasks, improving bias detection performance across various categories including religion, gender, and race.", "motivation": "Bias and stereotypes in language models can lead to harmful impacts and inefficiencies in applications like moderation and decision-making.", "method": "The paper proposes joint learning of bias and stereotype detection using a newly curated dataset, StereoBias, across five key categories, comparing encoder-only models and fine-tuned decoder-only models.", "result": "Joint training substantially enhances bias detection accuracy and reveals connections between bias and stereotypes, beyond benefits from multi-task learning alone.", "conclusion": "Leveraging stereotype data can help develop fairer and effective AI systems, showcasing the importance of integrating bias and stereotype relationships into training models."}}
{"id": "2507.01422", "pdf": "https://arxiv.org/pdf/2507.01422", "abs": "https://arxiv.org/abs/2507.01422", "authors": ["Wenjie Liu", "Bingshu Wang", "Ze Wang", "C. L. Philip Chen"], "title": "DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Document shadow removal is a crucial task in the field of document image\nenhancement. However, existing methods tend to remove shadows with constant\ncolor background and ignore color shadows. In this paper, we first design a\ndiffusion model in latent space for document image shadow removal, called\nDocShaDiffusion. It translates shadow images from pixel space to latent space,\nenabling the model to more easily capture essential features. To address the\nissue of color shadows, we design a shadow soft-mask generation module (SSGM).\nIt is able to produce accurate shadow mask and add noise into shadow regions\nspecially. Guided by the shadow mask, a shadow mask-aware guided diffusion\nmodule (SMGDM) is proposed to remove shadows from document images by\nsupervising the diffusion and denoising process. We also propose a\nshadow-robust perceptual feature loss to preserve details and structures in\ndocument images. Moreover, we develop a large-scale synthetic document color\nshadow removal dataset (SDCSRD). It simulates the distribution of realistic\ncolor shadows and provides powerful supports for the training of models.\nExperiments on three public datasets validate the proposed method's superiority\nover state-of-the-art. Our code and dataset will be publicly available.", "AI": {"tldr": "The paper introduces a diffusion model called DocShaDiffusion for removing color shadows in document images, enhancing performance compared to prior methods.", "motivation": "Existing shadow removal techniques struggle with color shadows and lack effective solutions to maintain document structure and details.", "method": "The authors propose DocShaDiffusion, a latent-space diffusion model combined with a shadow soft-mask generation module (SSGM) and a shadow mask-aware guided diffusion module (SMGDM). Additionally, they introduce a shadow-robust perceptual feature loss and a synthetic dataset (SDCSRD).", "result": "DocShaDiffusion achieves superior performance in removing document image shadows compared to state-of-the-art methods, as validated by experiments on three public datasets.", "conclusion": "The study demonstrates the effectiveness of DocShaDiffusion for accurate color shadow removal while preserving essential document details, supported by the release of code and dataset."}}
{"id": "2507.01068", "pdf": "https://arxiv.org/pdf/2507.01068", "abs": "https://arxiv.org/abs/2507.01068", "authors": ["Biplov Paneru"], "title": "Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors", "categories": ["cs.LG"], "comment": null, "summary": "This study leverages an Inertial Measurement Unit (IMU) dataset to develop\nexplainable AI methods for the early detection and prediction of Freezing of\nGait (FOG), a common symptom in Parkinson's disease. Machine learning models,\nincluding CatBoost, XGBoost, and Extra Trees classifiers, are employed to\naccurately categorize FOG episodes based on relevant clinical features. A\nStacking Ensemble model achieves superior performance, surpassing a hybrid\nbidirectional GRU model and reaching nearly 99% classification accuracy. SHAP\ninterpretability analysis reveals that time (seconds) is the most influential\nfactor in distinguishing gait patterns. Additionally, the proposed FOG\nprediction framework incorporates federated learning, where models are trained\nlocally on individual devices and aggregated on a central server using a\nfederated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for\nenhanced predictive capability.", "AI": {"tldr": "This paper proposes explainable AI methods for early detection and prediction of Freezing of Gait (FOG) using IMU data and machine learning, achieving nearly 99% classification accuracy.", "motivation": "Early detection of FOG in Parkinson's disease is crucial for improving patient care, and there is a need for accurate, interpretable, and scalable diagnostic methods.", "method": "The study uses machine learning models, including CatBoost, XGBoost, and Extra Trees classifiers, combined with a Stacking Ensemble model and federated learning to improve predictions. SHAP analysis provides interpretability, highlighting key influencing factors in gait patterns.", "result": "The Stacking Ensemble model achieved near 99% classification accuracy, outperforming other models. Time (seconds) was identified as the most critical feature for distinguishing FOG episodes.", "conclusion": "This framework successfully combines high accuracy with interpretability and scalability, potentially aiding in real-world FOG detection and prediction for Parkinson's disease patients."}}
{"id": "2507.01843", "pdf": "https://arxiv.org/pdf/2507.01843", "abs": "https://arxiv.org/abs/2507.01843", "authors": ["Dmytro Kuzmenko", "Nadiya Shvai"], "title": "MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics", "categories": ["cs.RO"], "comment": "Preprint of a manuscript submitted for peer review", "summary": "Mixture-of-Experts (MoE) approaches have recently gained traction in robotics\napplications due to their ability to dynamically allocate computational\nresources and specialize sub-networks for distinct tasks or environmental\ncontexts, enabling more efficient decision-making. Such systems often comprise\nsparsely activated experts combined under a single monolithic architecture and\nrequire a well-configured internal routing mechanism, which does not allow for\nselective low-level expert and router customization and requires additional\ntraining. We propose MoIRA, an architecture-agnostic modular MoE framework\ndesigned to coordinate existing experts with an external text-based router.\nMoIRA incorporates two zero-shot routing options: embedding-based similarity\nand prompt-driven language model inference. In our experiments, we choose large\nVision-Language-Action models, gr00t-N1 and $\\pi_0$, as the underlying experts,\nand train low-rank adapters for low-overhead inference. We evaluate MoIRA on\nvarious GR1 Humanoid tasks and LIBERO Spatial and Goal benchmarks, where it\nconsistently outperforms generalist models and competes with other MoE\npipelines. Additionally, we analyse the robustness of the proposed approach to\nthe variations of the instructions. While relying solely on textual\ndescriptions of tasks and experts, MoIRA demonstrates the practical viability\nof modular deployment with precise, low-effort routing and provides an\nalternative, scalable foundation for future multi-expert robotic systems.", "AI": {"tldr": "The paper introduces MoIRA, a modular Mixture-of-Experts (MoE) framework that uses an external text-based mechanism for routing tasks across expert models, showing superior performance in robotics benchmarks.", "motivation": "To overcome limitations in traditional MoE systems, such as lack of flexibility in customization, need for additional training, and inefficient routing mechanisms.", "method": "Proposed the MoIRA framework, which uses text-based zero-shot routing methods (embedding-based similarity and prompt-driven inference) to dynamically coordinate experts. This approach leverages Vision-Language-Action models as experts, with high efficiency ensured by low-rank adapters.", "result": "MoIRA outperforms generalist models and rivals other MoE frameworks on GR1 Humanoid tasks and LIBERO benchmarks while being robust to variations in task instructions.", "conclusion": "The framework validates the use of modular, scalable, and text-driven routing in multi-expert robotic systems, offering efficient and precise decision-making without additional training overhead."}}
{"id": "2507.01734", "pdf": "https://arxiv.org/pdf/2507.01734", "abs": "https://arxiv.org/abs/2507.01734", "authors": ["Oliver Wardas", "Florian Matthes"], "title": "LLMs for Legal Subsumption in German Employment Contracts", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "PrePrint - ICAIL25, Chicago", "summary": "Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented.", "AI": {"tldr": "The paper explores Large Language Models (LLMs) for evaluating clause legality in German employment contracts, highlighting moderate improvements with full-text legal sources and significant gains using distilled guidelines, but emphasizing LLMs still perform below human lawyers.", "motivation": "Legal work demands both interpretability and trustworthiness, which data-driven NLP approaches often lack, posing challenges in dynamic legal contexts.", "method": "They extended a dataset, collaborated with legal experts, and tested LLMs under three legal contexts: no context, full-text legal sources, and distilled examination guidelines.", "result": "Using distilled guidelines enhanced recall for void clauses and boosted weighted F1-score to 80%. Full-text sources also improved performance, though less markedly than distilled guidelines.", "conclusion": "LLMs show promise in supporting legal reviews but remain inferior to human lawyers, especially in performance using full-text legal references."}}
{"id": "2507.01428", "pdf": "https://arxiv.org/pdf/2507.01428", "abs": "https://arxiv.org/abs/2507.01428", "authors": ["Chen Sun", "Haiyang Sun", "Zhiqing Guo", "Yunfeng Diao", "Liejun Wang", "Dan Ma", "Gaobo Yang", "Keqin Li"], "title": "DiffMark: Diffusion-based Robust Watermark Against Deepfakes", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Deepfakes pose significant security and privacy threats through malicious\nfacial manipulations. While robust watermarking can aid in authenticity\nverification and source tracking, existing methods often lack the sufficient\nrobustness against Deepfake manipulations. Diffusion models have demonstrated\nremarkable performance in image generation, enabling the seamless fusion of\nwatermark with image during generation. In this study, we propose a novel\nrobust watermarking framework based on diffusion model, called DiffMark. By\nmodifying the training and sampling scheme, we take the facial image and\nwatermark as conditions to guide the diffusion model to progressively denoise\nand generate corresponding watermarked image. In the construction of facial\ncondition, we weight the facial image by a timestep-dependent factor that\ngradually reduces the guidance intensity with the decrease of noise, thus\nbetter adapting to the sampling process of diffusion model. To achieve the\nfusion of watermark condition, we introduce a cross information fusion (CIF)\nmodule that leverages a learnable embedding table to adaptively extract\nwatermark features and integrates them with image features via cross-attention.\nTo enhance the robustness of the watermark against Deepfake manipulations, we\nintegrate a frozen autoencoder during training phase to simulate Deepfake\nmanipulations. Additionally, we introduce Deepfake-resistant guidance that\nemploys specific Deepfake model to adversarially guide the diffusion sampling\nprocess to generate more robust watermarked images. Experimental results\ndemonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.\nOur code will be available at https://github.com/vpsg-research/DiffMark.", "AI": {"tldr": "This paper presents DiffMark, a diffusion model-based watermarking framework that robustly embeds watermarks into facial images, offering resistance against Deepfake manipulations.", "motivation": "The motivation is to address the lack of robustness in existing watermarking techniques when faced with Deepfake manipulations and provide a solution to enhance security and authenticity verification.", "method": "The method leverages diffusion models to integrate watermarks into facial images during generation. It employs a timestep-dependent weighting for facial images, a cross information fusion (CIF) module for watermark-image feature integration, and utilizes a frozen autoencoder and Deepfake-resistant guidance to simulate and counteract Deepfake manipulations.", "result": "The proposed DiffMark framework demonstrates effective watermarking that is robust against standard Deepfake manipulations, as proven by experimental results.", "conclusion": "DiffMark offers a novel and effective diffusion model-based approach to robust watermarking, addressing vulnerabilities against Deepfakes in authenticity and privacy settings."}}
{"id": "2507.01073", "pdf": "https://arxiv.org/pdf/2507.01073", "abs": "https://arxiv.org/abs/2507.01073", "authors": ["Dian Jin"], "title": "Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Graph neural networks (GNNs) have achieved remarkable success in molecular\nproperty prediction. However, traditional graph representations struggle to\neffectively encode the inherent 3D spatial structures of molecules, as\nmolecular orientations in 3D space introduce significant variability, severely\nlimiting model generalization and robustness. Existing approaches primarily\nfocus on rotation-invariant and rotation-equivariant methods. Invariant methods\noften rely heavily on prior knowledge and lack sufficient generalizability,\nwhile equivariant methods suffer from high computational costs. To address\nthese limitations, this paper proposes a novel plug-and-play 3D encoding module\nleveraging rotational sampling. By computing the expectation over the SO(3)\nrotational group, the method naturally achieves approximate rotational\ninvariance. Furthermore, by introducing a carefully designed post-alignment\nstrategy, strict invariance can be achieved without compromising performance.\nExperimental evaluations on the QM9 and C10 Datasets demonstrate superior\npredictive accuracy, robustness, and generalization performance compared to\nexisting methods. Moreover, the proposed approach maintains low computational\ncomplexity and enhanced interpretability, providing a promising direction for\nefficient and effective handling of 3D molecular information in drug discovery\nand material design.", "AI": {"tldr": "The paper introduces a new 3D molecular encoding module that ensures rotational invariance for graph neural networks with improved accuracy, robustness, and low computational cost.", "motivation": "Current graph neural networks fail to handle 3D spatial variability effectively, which limits their generalization and robustness for molecular property prediction.", "method": "A novel module using rotational sampling and SO(3) rotational group expectations to approximate rotational invariance, combined with a post-alignment strategy for strict invariance.", "result": "Experiments on QM9 and C10 datasets show superior predictive accuracy, robustness, generalization, and cost-efficiency as compared to existing methods.", "conclusion": "The proposed module is efficient, interpretable, and significantly advances the handling of 3D molecular structures, marking potential progress in drug discovery and material design."}}
{"id": "2507.01857", "pdf": "https://arxiv.org/pdf/2507.01857", "abs": "https://arxiv.org/abs/2507.01857", "authors": ["Yuhao Lin", "Yi-Lin Wei", "Haoran Liao", "Mu Lin", "Chengyi Xing", "Hao Li", "Dandan Zhang", "Mark Cutkosky", "Wei-Shi Zheng"], "title": "TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types", "categories": ["cs.RO"], "comment": "Project Page: https://isee-laboratory.github.io/TypeTele", "summary": "Dexterous teleoperation plays a crucial role in robotic manipulation for\nreal-world data collection and remote robot control. Previous dexterous\nteleoperation mostly relies on hand retargeting to closely mimic human hand\npostures. However, these approaches may fail to fully leverage the inherent\ndexterity of dexterous hands, which can execute unique actions through their\nstructural advantages compared to human hands. To address this limitation, we\npropose TypeTele, a type-guided dexterous teleoperation system, which enables\ndexterous hands to perform actions that are not constrained by human motion\npatterns. This is achieved by introducing dexterous manipulation types into the\nteleoperation system, allowing operators to employ appropriate types to\ncomplete specific tasks. To support this system, we build an extensible\ndexterous manipulation type library to cover comprehensive dexterous postures\nused in manipulation tasks. During teleoperation, we employ a MLLM\n(Multi-modality Large Language Model)-assisted type retrieval module to\nidentify the most suitable manipulation type based on the specific task and\noperator commands. Extensive experiments of real-world teleoperation and\nimitation learning demonstrate that the incorporation of manipulation types\nsignificantly takes full advantage of the dexterous robot's ability to perform\ndiverse and complex tasks with higher success rates.", "AI": {"tldr": "The paper proposes TypeTele, a teleoperation system that leverages type-guided dexterous manipulation to enhance task performance beyond mimicking human hand postures.", "motivation": "Existing teleoperation methods limit dexterous robotic hands by merely mimicking human hand movements, resulting in underuse of their unique capabilities.", "method": "TypeTele introduces manipulation types into teleoperation, supported by a type library and an MLLM-assisted module for task-specific type retrieval.", "result": "Experiments show improved task success rates and better utilization of dexterous robot hands in both real-world teleoperation and imitation learning scenarios.", "conclusion": "TypeTele enhances dexterous teleoperation by utilizing manipulation types, enabling robots to optimize their capabilities for complex tasks."}}
{"id": "2507.01764", "pdf": "https://arxiv.org/pdf/2507.01764", "abs": "https://arxiv.org/abs/2507.01764", "authors": ["Matteo Di Cristofaro"], "title": "Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results", "categories": ["cs.CL"], "comment": "Author submitted manuscript", "summary": "Tokenisation - \"the process of splitting text into atomic parts\" (Brezina &\nTimperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides\nthe basis for any applicable quantitative method (e.g. collocations) while\nensuring the reliability of qualitative approaches. This paper examines how\ndiscrepancies in tokenisation affect the representation of language data and\nthe validity of analytical findings: investigating the challenges posed by\nemojis and homoglyphs, the study highlights the necessity of preprocessing\nthese elements to maintain corpus fidelity to the source data. The research\npresents methods for ensuring that digital texts are accurately represented in\ncorpora, thereby supporting reliable linguistic analysis and guaranteeing the\nrepeatability of linguistic interpretations. The findings emphasise the\nnecessity of a detailed understanding of both linguistic and technical aspects\ninvolved in digital textual data to enhance the accuracy of corpus analysis,\nand have significant implications for both quantitative and qualitative\napproaches in corpus-based research.", "AI": {"tldr": "Tokenisation impacts corpus linguistic data accuracy, especially with emojis and homoglyphs, requiring targeted preprocessing methods.", "motivation": "To address reliability issues arising from tokenisation discrepancies in corpus linguistics, particularly dealing with emojis and homoglyphs.", "method": "Examining how tokenisation discrepancies influence data representation and proposing preprocessing methods to maintain corpus fidelity.", "result": "Highlighted how preprocessing emojis and homoglyphs ensures accurate representation of digital texts in corpora.", "conclusion": "Accurate tokenisation and preprocessing are essential for reliable corpus-based linguistic analysis, impacting both quantitative and qualitative approaches."}}
{"id": "2507.01439", "pdf": "https://arxiv.org/pdf/2507.01439", "abs": "https://arxiv.org/abs/2507.01439", "authors": ["Shaocheng Yan", "Pengcheng Shi", "Zhenjun Zhao", "Kaixin Wang", "Kuang Cao", "Ji Wu", "Jiayuan Li"], "title": "TurboReg: TurboClique for Robust and Efficient Point Cloud Registration", "categories": ["cs.CV"], "comment": "ICCV-2025 Accepted Paper", "summary": "Robust estimation is essential in correspondence-based Point Cloud\nRegistration (PCR). Existing methods using maximal clique search in\ncompatibility graphs achieve high recall but suffer from exponential time\ncomplexity, limiting their use in time-sensitive applications. To address this\nchallenge, we propose a fast and robust estimator, TurboReg, built upon a novel\nlightweight clique, TurboClique, and a highly parallelizable Pivot-Guided\nSearch (PGS) algorithm. First, we define the TurboClique as a 3-clique within a\nhighly-constrained compatibility graph. The lightweight nature of the 3-clique\nallows for efficient parallel searching, and the highly-constrained\ncompatibility graph ensures robust spatial consistency for stable\ntransformation estimation. Next, PGS selects matching pairs with high SC$^2$\nscores as pivots, effectively guiding the search toward TurboCliques with\nhigher inlier ratios. Moreover, the PGS algorithm has linear time complexity\nand is significantly more efficient than the maximal clique search with\nexponential time complexity. Extensive experiments show that TurboReg achieves\nstate-of-the-art performance across multiple real-world datasets, with\nsubstantial speed improvements. For example, on the 3DMatch+FCGF dataset,\nTurboReg (1K) operates $208.22\\times$ faster than 3DMAC while also achieving\nhigher recall. Our code is accessible at\n\\href{https://github.com/Laka-3DV/TurboReg}{\\texttt{TurboReg}}.", "AI": {"tldr": "The paper introduces TurboReg, a fast and robust point cloud registration estimator using TurboCliques and the efficient Pivot-Guided Search algorithm for improved speed and accuracy.", "motivation": "Existing correspondence-based Point Cloud Registration (PCR) methods face significant time complexity due to reliance on maximal clique searches, limiting their use in time-critical applications.", "method": "TurboReg introduces TurboCliques (lightweight 3-cliques in highly-constrained graphs) and the linear time complexity Pivot-Guided Search (PGS) algorithm to improve searching efficiency and robustness.", "result": "Experiments demonstrate TurboReg's superior performance in speed and recall, achieving state-of-the-art accuracy while being substantially faster on datasets like 3DMatch+FCGF, operating over 208\u00d7 faster than 3DMAC.", "conclusion": "TurboReg presents a breakthrough in point cloud registration by combining robust, spatially consistent transformations with significant computational efficiency, making it highly suitable for real-world applications."}}
{"id": "2507.01925", "pdf": "https://arxiv.org/pdf/2507.01925", "abs": "https://arxiv.org/abs/2507.01925", "authors": ["Yifan Zhong", "Fengshuo Bai", "Shaofei Cai", "Xuchuan Huang", "Zhang Chen", "Xiaowei Zhang", "Yuanfei Wang", "Shaoyang Guo", "Tianrui Guan", "Ka Nam Lui", "Zhiquan Qi", "Yitao Liang", "Yuanpei Chen", "Yaodong Yang"], "title": "A Survey on Vision-Language-Action Models: An Action Tokenization Perspective", "categories": ["cs.RO"], "comment": "70 pages, 5 figures", "summary": "The remarkable advancements of vision and language foundation models in\nmultimodal understanding, reasoning, and generation has sparked growing efforts\nto extend such intelligence to the physical world, fueling the flourishing of\nvision-language-action (VLA) models. Despite seemingly diverse approaches, we\nobserve that current VLA models can be unified under a single framework: vision\nand language inputs are processed by a series of VLA modules, producing a chain\nof \\textit{action tokens} that progressively encode more grounded and\nactionable information, ultimately generating executable actions. We further\ndetermine that the primary design choice distinguishing VLA models lies in how\naction tokens are formulated, which can be categorized into language\ndescription, code, affordance, trajectory, goal state, latent representation,\nraw action, and reasoning. However, there remains a lack of comprehensive\nunderstanding regarding action tokens, significantly impeding effective VLA\ndevelopment and obscuring future directions. Therefore, this survey aims to\ncategorize and interpret existing VLA research through the lens of action\ntokenization, distill the strengths and limitations of each token type, and\nidentify areas for improvement. Through this systematic review and analysis, we\noffer a synthesized outlook on the broader evolution of VLA models, highlight\nunderexplored yet promising directions, and contribute guidance for future\nresearch, hoping to bring the field closer to general-purpose intelligence.", "AI": {"tldr": "This survey examines vision-language-action (VLA) models, categorizing their approaches to processing action tokens and identifying areas for improvement.", "motivation": "To extend the capabilities of vision and language foundation models to the physical world, leading to advancements in general-purpose intelligence.", "method": "The authors categorize and analyze VLA models based on their approach to action tokenization, distilling strengths, limitations, and areas for improvement through a systematic review.", "result": "The study identifies eight types of action token formulations and their characteristics, revealing gaps in understanding and development opportunities within the VLA domain.", "conclusion": "By categorizing VLA research, the paper provides guidance to improve action tokenization strategies and advance toward more general-purpose intelligence in VLA systems."}}
{"id": "2507.01785", "pdf": "https://arxiv.org/pdf/2507.01785", "abs": "https://arxiv.org/abs/2507.01785", "authors": ["Zhixun Chen", "Ping Guo", "Wenhan Han", "Yifan Zhang", "Binbin Liu", "Haobin Lin", "Fengze Liu", "Yan Zhao", "Bingni Zhang", "Taifeng Wang", "Yin Zheng", "Meng Fang"], "title": "MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Data quality is a critical driver of large language model performance, yet\nexisting model-based selection methods focus almost exclusively on English. We\nintroduce MuRating, a scalable framework that transfers high-quality English\ndata-quality signals into a single rater for 17 target languages. MuRating\naggregates multiple English \"raters\" via pairwise comparisons to learn unified\ndocument-quality scores,then projects these judgments through translation to\ntrain a multilingual evaluator on monolingual, cross-lingual, and parallel text\npairs. Applied to web data, MuRating selects balanced subsets of English and\nmultilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to\nstrong baselines, including QuRater, AskLLM, DCLM and so on, our approach\nboosts average accuracy on both English benchmarks and multilingual\nevaluations, with especially large gains on knowledge-intensive tasks. We\nfurther analyze translation fidelity, selection biases, and underrepresentation\nof narrative material, outlining directions for future work.", "AI": {"tldr": "MuRating transfers English data-quality evaluation to 17 target languages, enhancing multilingual large language model performance.", "motivation": "Existing selection methods for data quality concentrate on English, overlooking a scalable approach for multilingual data.", "method": "MuRating employs English document-quality scores, translates them for multilingual evaluations, and balances datasets for pretraining a LLaMA model.", "result": "Increased accuracy across English and multilingual benchmarks, especially in knowledge-intensive tasks.", "conclusion": "MuRating improves data selection for pretraining multilingual models and highlights areas for research such as biases and narrative material."}}
{"id": "2507.01455", "pdf": "https://arxiv.org/pdf/2507.01455", "abs": "https://arxiv.org/abs/2507.01455", "authors": ["Yuxing Liu", "Ji Zhang", "Zhou Xuchuan", "Jingzhong Xiao", "Huimin Yang", "Jiaxin Zhong"], "title": "OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes", "categories": ["cs.CV"], "comment": "12 pages, 5 figures", "summary": "Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous\nobjects within images. Existing pixel-wise methods typically assign anomaly\nscores individually and employ a global thresholding strategy to segment\nanomalies. Despite their effectiveness, these approaches encounter significant\nchallenges in real-world applications: (1) neglecting spatial correlations\namong pixels within the same object, resulting in fragmented segmentation; (2)\nvariabil ity in anomaly score distributions across image regions, causing\nglobal thresholds to either generate false positives in background areas or\nmiss segments of anomalous objects. In this work, we introduce OoDDINO, a novel\nmulti-level anomaly segmentation framework designed to address these\nlimitations through a coarse-to-fine anomaly detection strategy. OoDDINO\ncombines an uncertainty-guided anomaly detection model with a pixel-level\nsegmentation model within a two-stage cascade architecture. Initially, we\npropose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that\nsequentially integrates multiple uncertainty metrics with visual\nrepresentations, employing orthogonal constraints to strengthen the detection\nmodel's capacity for localizing anomalous regions accurately. Subsequently, we\ndevelop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically\ngenerates region-specific thresholds based on object-level detection outputs\nand pixel-wise anomaly scores. This approach allows for distinct thresholding\nstrategies within foreground and background areas, achieving fine-grained\nanomaly segmentation. The proposed framework is compatible with other\npixel-wise anomaly detection models, which acts as a plug-in to boost the\nperformance. Extensive experiments on two benchmark datasets validate our\nframework's superiority and compatibility over state-of-the-art methods.", "AI": {"tldr": "OoDDINO is introduced as a novel framework for pixel-wise anomaly segmentation, tackling challenges like fragmented results and inconsistent anomaly score distributions.", "motivation": "Address challenges in anomaly segmentation, such as fragmented segmentation due to ignored spatial correlations and inconsistent global thresholds resulting in false positives or missed anomalies.", "method": "Proposes a two-stage cascade architecture with (1) an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) to enhance anomaly detection accuracy and (2) an Adaptive Dual-Threshold Network (ADT-Net) for region-specific thresholding.", "result": "OoDDINO demonstrates superior performance and compatibility compared to state-of-the-art methods on two benchmark datasets.", "conclusion": "OoDDINO offers a significant improvement in pixel-wise anomaly segmentation, serving as a highly adaptable tool for practical applications and boosting other models' performance as a plug-in."}}
{"id": "2507.01077", "pdf": "https://arxiv.org/pdf/2507.01077", "abs": "https://arxiv.org/abs/2507.01077", "authors": ["Bogdan Bogdan", "Arina Cazacu", "Laura Vasilie"], "title": "Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels", "categories": ["cs.LG"], "comment": "6 pages, 7 figures, 4 tables, accepted to IEEE Intelligent Vehicles\n  Symposium (IV) 2025", "summary": "Anomaly detection often relies on supervised or clustering approaches, with\nlimited success in specialized domains like automotive communication systems\nwhere scalable solutions are essential. We propose a novel decoder-only Large\nLanguage Model (LLM) to detect anomalies in Electronic Control Unit (ECU)\ncommunication logs. Our approach addresses two key challenges: the lack of LLMs\ntailored for ECU communication and the complexity of inconsistent ground truth\ndata. By learning from UDP communication logs, we formulate anomaly detection\nsimply as identifying deviations in time from normal behavior. We introduce an\nentropy regularization technique that increases model's uncertainty in known\nanomalies while maintaining consistency in similar scenarios. Our solution\noffers three novelties: a decoder-only anomaly detection architecture, a way to\nhandle inconsistent labeling, and an adaptable LLM for different ECU\ncommunication use cases. By leveraging the generative capabilities of\ndecoder-only models, we present a new technique that addresses the high cost\nand error-prone nature of manual labeling through a more scalable system that\nis able to learn from a minimal set of examples, while improving detection\naccuracy in complex communication environments.", "AI": {"tldr": "The paper introduces a novel decoder-only Large Language Model (LLM) for anomaly detection in ECU communication logs, enhancing scalability and detection accuracy in complex systems.", "motivation": "Current anomaly detection methods are insufficient for specialized domains like automotive communication systems due to scalability challenges and inconsistent ground truth data.", "method": "A decoder-only LLM was trained on UDP communication logs to recognize time deviations as anomalies. It uses entropy regularization to handle inconsistent labeling and increase model uncertainty in known anomalies.", "result": "The proposed model demonstrates an adaptable architecture capable of learning from limited examples while effectively handling inconsistent labeling, improving anomaly detection accuracy in ECU communications.", "conclusion": "The novel LLM-based approach offers a scalable, more accurate method for anomaly detection in specialized automotive communication systems, minimizing reliance on extensive manual labeling."}}
{"id": "2507.01930", "pdf": "https://arxiv.org/pdf/2507.01930", "abs": "https://arxiv.org/abs/2507.01930", "authors": ["Wenhao Wang", "Yanyan Li", "Long Jiao", "Jiawei Yuan"], "title": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic Observations", "categories": ["cs.RO"], "comment": "10 pages", "summary": "Large Language Models (LLMs) have revolutionized robotic autonomy, including\nUnmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential\nof LLMs for translating human instructions into executable control code for UAV\noperations. However, LLMs still face challenges from logical reasoning and\ncomplex decision-making, leading to concerns about the reliability of\nLLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop\ncontrol framework that enables reliable UAV operations powered by effective\nfeedback and refinement using two LLM modules, i.e., a Code Generator and an\nEvaluator. Our framework transforms numerical state observations from UAV\noperations into natural language trajectory descriptions to enhance the\nevaluator LLM's understanding of UAV dynamics for precise feedback generation.\nOur framework also enables a simulation-based refinement process, and hence\neliminates the risks to physical UAVs caused by incorrect code execution during\nthe refinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity.", "AI": {"tldr": "This paper introduces a closed-loop control framework using Large Language Models (LLMs) for reliable UAV operations, addressing logical reasoning and decision-making challenges.", "motivation": "To enhance the reliability of LLM-driven UAV operations which face challenges in logical reasoning and complex decision-making.", "method": "The proposed framework includes two LLM modules\u2014a Code Generator and an Evaluator\u2014that use natural language trajectory descriptions and simulation-based refinement for reliable feedback and code execution.", "result": "Experiments show the framework outperforms baselines in terms of success rate and task completeness, especially as task complexity increases.", "conclusion": "The framework enables reliable UAV operations using LLMs, with effective feedback mechanisms and risk mitigation during code refinement."}}
{"id": "2507.01051", "pdf": "https://arxiv.org/pdf/2507.01051", "abs": "https://arxiv.org/abs/2507.01051", "authors": ["Giada Pistilli", "Bruna Trevelin"], "title": "Can AI be Consentful?", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The evolution of generative AI systems exposes the challenges of traditional\nlegal and ethical frameworks built around consent. This chapter examines how\nthe conventional notion of consent, while fundamental to data protection and\nprivacy rights, proves insufficient in addressing the implications of\nAI-generated content derived from personal data. Through legal and ethical\nanalysis, we show that while individuals can consent to the initial use of\ntheir data for AI training, they cannot meaningfully consent to the numerous\npotential outputs their data might enable or the extent to which the output is\nused or distributed. We identify three fundamental challenges: the scope\nproblem, the temporality problem, and the autonomy trap, which collectively\ncreate what we term a ''consent gap'' in AI systems and their surrounding\necosystem. We argue that current legal frameworks inadequately address these\nemerging challenges, particularly regarding individual autonomy, identity\nrights, and social responsibility, especially in cases where AI-generated\ncontent creates new forms of personal representation beyond the scope of the\noriginal consent. By examining how these consent limitations intersect with\nbroader principles of responsible AI (including fairness, transparency,\naccountability, and autonomy) we demonstrate the need to evolve ethical and\nlegal approaches to consent.", "AI": {"tldr": "The paper examines the inadequacy of traditional legal and ethical frameworks of consent in managing AI-generated content derived from personal data, proposing a \"consent gap\" and the need for evolved approaches.", "motivation": "To address the insufficiency of conventional consent frameworks in the context of AI-generated content and their impact on privacy and identity rights.", "method": "The paper conducts legal and ethical analyses, identifying challenges such as the scope problem, temporality problem, and autonomy trap.", "result": "The study identifies a \"consent gap\" due to individuals' inability to foresee or control how AI systems generate and distribute content based on their data.", "conclusion": "Current legal frameworks fail to address the challenges posed by AI systems, necessitating the evolution of ethical and legal principles to protect individual autonomy and ensure responsible AI practices."}}
{"id": "2507.01786", "pdf": "https://arxiv.org/pdf/2507.01786", "abs": "https://arxiv.org/abs/2507.01786", "authors": ["Jord Nguyen", "Khiem Hoang", "Carlo Leonardo Attubato", "Felix Hofst\u00e4tter"], "title": "Probing Evaluation Awareness of Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Technical AI Governance Workshop, ICML (Poster)", "summary": "Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception.", "AI": {"tldr": "The paper investigates how language models, specifically Llama-3.3-70B-Instruct, differentiate between evaluation and deployment phases, highlighting implications for AI safety and governance.", "motivation": "The study aims to understand if language models like Llama-3.3-70B-Instruct can distinguish evaluation prompts from deployment prompts, which may have serious consequences for AI safety evaluations and governance reliability.", "method": "The authors employ linear probes to analyze how Llama-3.3-70B-Instruct internally represents prompts from evaluation and deployment phases.", "result": "The study finds that the model can internally distinguish evaluation prompts, with safety evaluations being classified as artificial by the probes.", "conclusion": "The findings emphasize the need for robust evaluation mechanisms and deeper understanding of deceptive capabilities in models. They also suggest leveraging internal model representations for safety audits in future, more advanced models."}}
{"id": "2507.01463", "pdf": "https://arxiv.org/pdf/2507.01463", "abs": "https://arxiv.org/abs/2507.01463", "authors": ["Max Gandyra", "Alessandro Santonicola", "Michael Beetz"], "title": "NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation", "categories": ["cs.CV", "cs.AI", "I.2; I.4; I.5"], "comment": "10 pages, 3 figures, 3 tables, NeurIPS 2025 preprint", "summary": "Instance segmentation of novel objects instances in RGB images, given some\nexample images for each object, is a well known problem in computer vision.\nDesigning a model general enough to be employed, for all kinds of novel\nobjects, without (re-) training, has proven to be a difficult task. To handle\nthis, we propose a simple, yet powerful, framework, called: Novel Object Cyclic\nThreshold based Instance Segmentation (NOCTIS). This work stems from and\nimproves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also\nleverages on recent vision foundation models, namely: Grounded-SAM 2 and\nDINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise\nbounding boxes and their corresponding segmentation masks; while DINOv2's\nzero-shot capabilities are employed to generate the image embeddings. The\nquality of those masks, together with their embeddings, is of vital importance\nto our approach; as the proposal-object matching is realized by determining an\nobject matching score based on the similarity of the class embeddings and the\naverage maximum similarity of the patch embeddings. Differently to SAM-6D,\ncalculating the latter involves a prior patch filtering based on the distance\nbetween each patch and its corresponding cyclic/roundtrip patch in the image\ngrid. Furthermore, the average confidence of the proposals' bounding box and\nmask is used as an additional weighting factor for the object matching score.\nWe empirically show that NOCTIS, without further training/fine tuning,\noutperforms the best RGB and RGB-D methods on the seven core datasets of the\nBOP 2023 challenge for the \"Model-based 2D segmentation of unseen objects\"\ntask.", "AI": {"tldr": "The paper introduces NOCTIS, a framework for instance segmentation of novel objects, aiming to generalize across unseen objects without retraining. It builds on prior works and leverages vision foundation models like Grounded-SAM 2 and DINOv2 for object detection and embedding similarity matrices.", "motivation": "The paper addresses the challenge of instance segmentation for unseen object categories in RGB images, aiming to create a generalizable model that doesn't require retraining.", "method": "NOCTIS leverages Grounded-SAM 2 for bounding box detection and segmentation masks, and DINOv2 for zero-shot embedding generation. It introduces a cyclic filtering mechanism for patch embeddings and incorporates bounding box/mask confidence scores to refine object matching.", "result": "NOCTIS outperformed existing methods in the BOP 2023 challenge for unseen object segmentation, achieving state-of-the-art results on seven datasets without additional training.", "conclusion": "The framework demonstrates that integrating vision foundation models with novel metrics can yield superior generalizable performance for instance segmentation of previously unseen objects."}}
{"id": "2507.01961", "pdf": "https://arxiv.org/pdf/2507.01961", "abs": "https://arxiv.org/abs/2507.01961", "authors": ["Sixiang Chen", "Jiaming Liu", "Siyuan Qian", "Han Jiang", "Lily Li", "Renrui Zhang", "Zhuoyang Liu", "Chenyang Gu", "Chengkai Hou", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang"], "title": "AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Recently, mobile manipulation has attracted increasing attention for enabling\nlanguage-conditioned robotic control in household tasks. However, existing\nmethods still face challenges in coordinating mobile base and manipulator,\nprimarily due to two limitations. On the one hand, they fail to explicitly\nmodel the influence of the mobile base on manipulator control, which easily\nleads to error accumulation under high degrees of freedom. On the other hand,\nthey treat the entire mobile manipulation process with the same visual\nobservation modality (e.g., either all 2D or all 3D), overlooking the distinct\nmultimodal perception requirements at different stages during mobile\nmanipulation. To address this, we propose the Adaptive Coordination Diffusion\nTransformer (AC-DiT), which enhances mobile base and manipulator coordination\nfor end-to-end mobile manipulation. First, since the motion of the mobile base\ndirectly influences the manipulator's actions, we introduce a mobility-to-body\nconditioning mechanism that guides the model to first extract base motion\nrepresentations, which are then used as context prior for predicting whole-body\nactions. This enables whole-body control that accounts for the potential impact\nof the mobile base's motion. Second, to meet the perception requirements at\ndifferent stages of mobile manipulation, we design a perception-aware\nmultimodal conditioning strategy that dynamically adjusts the fusion weights\nbetween various 2D visual images and 3D point clouds, yielding visual features\ntailored to the current perceptual needs. This allows the model to, for\nexample, adaptively rely more on 2D inputs when semantic information is crucial\nfor action prediction, while placing greater emphasis on 3D geometric\ninformation when precise spatial understanding is required. We validate AC-DiT\nthrough extensive experiments on both simulated and real-world mobile\nmanipulation tasks.", "AI": {"tldr": "This paper introduces the Adaptive Coordination Diffusion Transformer (AC-DiT) to improve coordination in mobile manipulation by addressing challenges like error accumulation and uniform visual observation modalities.", "motivation": "To overcome the limitations of current mobile manipulation methods, which struggle with coordinating the mobile base and manipulator due to error accumulation and a lack of stage-specific visual observation modalities.", "method": "The proposed AC-DiT incorporates a mobility-to-body conditioning mechanism for coordinated whole-body control and a perception-aware multimodal conditioning strategy for stage-specific visual observation requirements.", "result": "Extensive experiments conducted on both simulated and real-world mobile manipulation tasks validate the effectiveness of AC-DiT.", "conclusion": "AC-DiT enhances end-to-end mobile manipulation by addressing coordination issues and leveraging multimodal visual perception tailored to task requirements."}}
{"id": "2507.01790", "pdf": "https://arxiv.org/pdf/2507.01790", "abs": "https://arxiv.org/abs/2507.01790", "authors": ["Tianze Hua", "Tian Yun", "Ellie Pavlick"], "title": "How Do Vision-Language Models Process Conflicting Information Across Modalities?", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "All code and resources are available at:\n  https://github.com/ethahtz/vlm_conflicting_info_processing", "summary": "AI models are increasingly required to be multimodal, integrating disparate\ninput streams into a coherent state representation on which subsequent\nbehaviors and actions can be based. This paper seeks to understand how such\nmodels behave when input streams present conflicting information. Focusing\nspecifically on vision-language models, we provide inconsistent inputs (e.g.,\nan image of a dog paired with the caption \"A photo of a cat\") and ask the model\nto report the information present in one of the specific modalities (e.g.,\n\"What does the caption say / What is in the image?\"). We find that models often\nfavor one modality over the other, e.g., reporting the image regardless of what\nthe caption says, but that different models differ in which modality they\nfavor. We find evidence that the behaviorally preferred modality is evident in\nthe internal representational structure of the model, and that specific\nattention heads can restructure the representations to favor one modality over\nthe other. Moreover, we find modality-agnostic \"router heads\" which appear to\npromote answers about the modality requested in the instruction, and which can\nbe manipulated or transferred in order to improve performance across datasets\nand modalities. Together, the work provides essential steps towards identifying\nand controlling if and how models detect and resolve conflicting signals within\ncomplex multimodal environments.", "AI": {"tldr": "The paper investigates how vision-language models handle conflicting information between modalities (e.g., images vs. captions) and identifies key factors influencing their responses.", "motivation": "To understand and control the behavior of multimodal AI models when input streams, such as visual and textual data, conflict with each other.", "method": "Providing inconsistent multimodal inputs (e.g., mismatched captions and images) to vision-language models and analyzing their preference for one modality over the other, while examining internal mechanisms such as attention heads and representational structures.", "result": "Models show a preference for certain modalities, which varies across models. Specific attention heads and 'router heads' influence modality preference and can be manipulated to favor accurate responses according to the given instructions.", "conclusion": "This work lays the groundwork for better controlling and understanding the mechanisms of multimodal AI models, particularly in resolving conflicting multimodal signals effectively."}}
{"id": "2507.01467", "pdf": "https://arxiv.org/pdf/2507.01467", "abs": "https://arxiv.org/abs/2507.01467", "authors": ["Ge Wu", "Shen Zhang", "Ruijing Shi", "Shanghua Gao", "Zhenyuan Chen", "Lei Wang", "Zhaowei Chen", "Hongcheng Gao", "Yao Tang", "Jian Yang", "Ming-Ming Cheng", "Xiang Li"], "title": "Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think", "categories": ["cs.CV"], "comment": null, "summary": "REPA and its variants effectively mitigate training challenges in diffusion\nmodels by incorporating external visual representations from pretrained models,\nthrough alignment between the noisy hidden projections of denoising networks\nand foundational clean image representations. We argue that the external\nalignment, which is absent during the entire denoising inference process, falls\nshort of fully harnessing the potential of discriminative representations. In\nthis work, we propose a straightforward method called Representation\nEntanglement for Generation (REG), which entangles low-level image latents with\na single high-level class token from pretrained foundation models for\ndenoising. REG acquires the capability to produce coherent image-class pairs\ndirectly from pure noise, substantially improving both generation quality and\ntraining efficiency. This is accomplished with negligible additional inference\noverhead, requiring only one single additional token for denoising (<0.5\\%\nincrease in FLOPs and latency). The inference process concurrently reconstructs\nboth image latents and their corresponding global semantics, where the acquired\nsemantic knowledge actively guides and enhances the image generation process.\nOn ImageNet 256$\\times$256, SiT-XL/2 + REG demonstrates remarkable convergence\nacceleration, achieving $\\textbf{63}\\times$ and $\\textbf{23}\\times$ faster\ntraining than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,\nSiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA\ntrained for 4M iterations ($\\textbf{10}\\times$ longer). Code is available at:\nhttps://github.com/Martinser/REG.", "AI": {"tldr": "This paper introduces REG, a novel method for improving the efficiency and effectiveness of diffusion model training by integrating low-level and high-level visual representations.", "motivation": "The paper addresses limitations in current diffusion models, particularly the lack of utilization of global semantics during denoising processes, which hampers the ability to fully harness discriminative representations.", "method": "REG entangles low-level image latents with a single high-level class token from pretrained models, seamlessly guiding the reconstruction of both image and global semantics during generation with negligible inference overhead.", "result": "In experiments with ImageNet 256x256, REG significantly accelerates convergence\u2014up to 63x and 23x faster than baseline models\u2014and achieves superior performance within fewer training iterations. Notably, REG provides better results with only a fraction of the training time needed for existing approaches.", "conclusion": "REG represents a breakthrough in diffusion model training, yielding more efficient, high-quality generation with minimal computational increases, and setting a new benchmark for rapid training convergence and performance capabilities."}}
{"id": "2507.01080", "pdf": "https://arxiv.org/pdf/2507.01080", "abs": "https://arxiv.org/abs/2507.01080", "authors": ["Edouard Lansiaux", "Ramy Azzouz", "Emmanuel Chazard", "Am\u00e9lie Vromant", "Eric Wiel"], "title": "Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept", "categories": ["cs.LG", "cs.PF"], "comment": "15 pages, 6 figures", "summary": "Triage errors, including undertriage and overtriage, are persistent\nchallenges in emergency departments (EDs). With increasing patient influx and\nstaff shortages, the integration of artificial intelligence (AI) into triage\nprotocols has gained attention. This study compares the performance of three AI\nmodels [Natural Language Processing (NLP), Large Language Models (LLM), and\nJoint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes\nagainst the FRENCH scale and clinical practice.We conducted a retrospective\nanalysis of a prospectively recruited cohort gathering adult patient triage\ndata over a 7-month period at the Roger Salengro Hospital ED (Lille, France).\nThree AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)\nURGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic\ndetails, verbatim chief complaints, vital signs, and triage outcomes based on\nthe FRENCH scale and GEMSA coding. The primary outcome was the concordance of\nAI-predicted triage level with the FRENCH gold-standard. It was assessed thanks\nto various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM\nmodel (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared\nto JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse\ntriage (-4.343). Secondary analyses highlighted the effectiveness of\nURGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness\nwith structured data versus raw transcripts (either for GEMSA prediction or for\nFRENCH prediction). LLM architecture, through abstraction of patient\nrepresentations, offers the most accurate triage predictions among tested\nmodels. Integrating AI into ED workflows could enhance patient safety and\noperational efficiency, though integration into clinical workflows requires\naddressing model limitations and ensuring ethical transparency.", "AI": {"tldr": "This study evaluates three AI models for their accuracy in predicting triage outcomes in emergency departments, demonstrating that Large Language Models (LLMs) outperform other models and human nurse triage.", "motivation": "The motivation stems from addressing persistent undertriage and overtriage issues in emergency departments, compounded by increasing patient numbers and staff shortages.", "method": "Three AI models were trained and validated using demographic, complaint, vital signs, and triage outcome data collected over seven months in a hospital ED.", "result": "The LLM-based model (URGENTIAPARSE) demonstrated superior performance in predicting triage levels according to the FRENCH scale, surpassing both other AI models and nurse triage.", "conclusion": "Large Language Models offer the most accurate triage prediction among tested frameworks, suggesting their potential for improving patient safety and efficiency in emergency workflows. Challenges like model limitations and ethical transparency need addressing before integration."}}
{"id": "2507.01053", "pdf": "https://arxiv.org/pdf/2507.01053", "abs": "https://arxiv.org/abs/2507.01053", "authors": ["Rafi Al Attrach", "Pedro Moreira", "Rajna Fani", "Renato Umeton", "Leo Anthony Celi"], "title": "Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis", "categories": ["cs.IR", "cs.AI", "cs.DB", "68T50, 68P15", "H.2.3; I.2.7; J.3"], "comment": "10 pages, 4 figures", "summary": "As ever-larger clinical datasets become available, they have the potential to\nunlock unprecedented opportunities for medical research. Foremost among them is\nMedical Information Mart for Intensive Care (MIMIC-IV), the world's largest\nopen-source EHR database. However, the inherent complexity of these datasets,\nparticularly the need for sophisticated querying skills and the need to\nunderstand the underlying clinical settings, often presents a significant\nbarrier to their effective use. M3 lowers the technical barrier to\nunderstanding and querying MIMIC-IV data. With a single command it retrieves\nMIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the\nhosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers\nconverse with the database in plain English. Ask a clinical question in natural\nlanguage; M3 uses a language model to translate it into SQL, executes the query\nagainst the MIMIC-IV dataset, and returns structured results alongside the\nunderlying query for verifiability and reproducibility. Demonstrations show\nthat minutes of dialogue with M3 yield the kind of nuanced cohort analyses that\nonce demanded hours of handcrafted SQL and relied on understanding the\ncomplexities of clinical workflows. By simplifying access, M3 invites the\nbroader research community to mine clinical critical-care data and accelerates\nthe translation of raw records into actionable insight.", "AI": {"tldr": "M3 simplifies access to the complex MIMIC-IV clinical database by using natural language queries, enabling rapid and reproducible data analysis for researchers.", "motivation": "MIMIC-IV, a vast open-source EHR database, offers immense research potential but remains underutilized due to the technical demands of querying and understanding clinical data.", "method": "M3 integrates with MIMIC-IV, utilizing a local SQLite instance or hosted BigQuery and a Model Context Protocol (MCP). Researchers use plain English to ask clinical questions, which are translated into SQL, executed, and returned with results.", "result": "Demonstrations show M3 enabling nuanced cohort analyses in minutes, greatly reducing the need for extensive SQL programming and deep clinical knowledge.", "conclusion": "M3 democratizes access to MIMIC-IV by significantly lowering technical barriers, fostering broader research engagement, and accelerating medical insights."}}
{"id": "2507.01802", "pdf": "https://arxiv.org/pdf/2507.01802", "abs": "https://arxiv.org/abs/2507.01802", "authors": ["Katharina Beckh", "Elisa Studeny", "Sujan Sai Gannamaneni", "Dario Antweiler", "Stefan R\u00fcping"], "title": "The Anatomy of Evidence: An Investigation Into Explainable ICD Coding", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Automatic medical coding has the potential to ease documentation and billing\nprocesses. For this task, transparency plays an important role for medical\ncoders and regulatory bodies, which can be achieved using explainability\nmethods. However, the evaluation of these approaches has been mostly limited to\nshort text and binary settings due to a scarcity of annotated data. Recent\nefforts by Cheng et al. (2023) have introduced the MDACE dataset, which\nprovides a valuable resource containing code evidence in clinical records. In\nthis work, we conduct an in-depth analysis of the MDACE dataset and perform\nplausibility evaluation of current explainable medical coding systems from an\napplied perspective. With this, we contribute to a deeper understanding of\nautomatic medical coding and evidence extraction. Our findings reveal that\nground truth evidence aligns with code descriptions to a certain degree. An\ninvestigation into state-of-the-art approaches shows a high overlap with ground\ntruth evidence. We propose match measures and highlight success and failure\ncases. Based on our findings, we provide recommendations for developing and\nevaluating explainable medical coding systems.", "AI": {"tldr": "The paper analyzes the MDACE dataset and evaluates explainable medical coding systems, focusing on transparency and plausibility in evidence extraction.", "motivation": "To improve transparency and efficiency in automatic medical coding while addressing the scarcity of annotated datasets for evaluation.", "method": "Conducted an in-depth analysis of the MDACE dataset and evaluated the plausibility of current explainable medical coding systems through match measures.", "result": "Ground truth evidence showed partial alignment with code descriptions, and current systems had significant overlap with this evidence; success and failure cases were identified.", "conclusion": "The study provides valuable recommendations for enhancing explainable medical coding systems and further understanding automatic coding processes."}}
{"id": "2507.01472", "pdf": "https://arxiv.org/pdf/2507.01472", "abs": "https://arxiv.org/abs/2507.01472", "authors": ["Jon\u00e1\u0161 Herec", "V\u00edt R\u016f\u017ei\u010dka", "Rado Pito\u0148\u00e1k"], "title": "Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware", "categories": ["cs.CV", "cs.LG", "cs.PF"], "comment": "This is a preprint of a paper accepted for the EDHPC 2025 Conference", "summary": "Methane is a potent greenhouse gas, and detecting its leaks early via\nhyperspectral satellite imagery can help mitigate climate change. Meanwhile,\nmany existing missions operate in manual tasking regimes only, thus missing\npotential events of interest. To overcome slow downlink rates cost-effectively,\nonboard detection is a viable solution. However, traditional methane\nenhancement methods are too computationally demanding for resource-limited\nonboard hardware. This work accelerates methane detection by focusing on\nefficient, low-power algorithms. We test fast target detection methods (ACE,\nCEM) that have not been previously used for methane detection and propose a\nMag1c-SAS - a significantly faster variant of the current state-of-the-art\nalgorithm for methane detection: Mag1c. To explore their true detection\npotential, we integrate them with a machine learning model (U-Net, LinkNet).\nOur results identify two promising candidates (Mag1c-SAS and CEM), both\nacceptably accurate for the detection of strong plumes and computationally\nefficient enough for onboard deployment: one optimized more for accuracy, the\nother more for speed, achieving up to ~100x and ~230x faster computation than\noriginal Mag1c on resource-limited hardware. Additionally, we propose and\nevaluate three band selection strategies. One of them can outperform the method\ntraditionally used in the field while using fewer channels, leading to even\nfaster processing without compromising accuracy. This research lays the\nfoundation for future advancements in onboard methane detection with minimal\nhardware requirements, improving timely data delivery. The produced code, data,\nand models are open-sourced and can be accessed from\nhttps://github.com/zaitra/methane-filters-benchmark.", "AI": {"tldr": "Researchers developed faster algorithms for detecting methane leaks from hyperspectral satellite imagery, optimizing them for resource-limited onboard systems.", "motivation": "Efficient onboard detection of methane leaks is vital to mitigate climate change and overcome slow satellite data downlink rates.", "method": "The study tested fast detection algorithms (ACE, CEM) and introduced a new method (Mag1c-SAS), integrating these with machine learning models like U-Net and LinkNet.", "result": "Two algorithms\u2014Mag1c-SAS and CEM\u2014were identified as effective and computationally efficient, achieving up to ~100x and ~230x speed improvements over existing methods.", "conclusion": "This research enables timely methane detection via low-power onboard systems and provides open-source resources to advance the field further."}}
{"id": "2507.01059", "pdf": "https://arxiv.org/pdf/2507.01059", "abs": "https://arxiv.org/abs/2507.01059", "authors": ["Xiangbo Gao", "Keshu Wu", "Hao Zhang", "Kexin Tian", "Yang Zhou", "Zhengzhong Tu"], "title": "Automated Vehicles Should be Connected with Natural Language", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "comment": null, "summary": "Multi-agent collaborative driving promises improvements in traffic safety and\nefficiency through collective perception and decision making. However, existing\ncommunication media -- including raw sensor data, neural network features, and\nperception results -- suffer limitations in bandwidth efficiency, information\ncompleteness, and agent interoperability. Moreover, traditional approaches have\nlargely ignored decision-level fusion, neglecting critical dimensions of\ncollaborative driving. In this paper we argue that addressing these challenges\nrequires a transition from purely perception-oriented data exchanges to\nexplicit intent and reasoning communication using natural language. Natural\nlanguage balances semantic density and communication bandwidth, adapts flexibly\nto real-time conditions, and bridges heterogeneous agent platforms. By enabling\nthe direct communication of intentions, rationales, and decisions, it\ntransforms collaborative driving from reactive perception-data sharing into\nproactive coordination, advancing safety, efficiency, and transparency in\nintelligent transportation systems.", "AI": {"tldr": "The paper advocates using natural language communication for multi-agent collaborative driving to overcome the limitations of current methods in efficiency, completeness, and interoperability.", "motivation": "Existing approaches in multi-agent collaborative driving often fail in terms of bandwidth efficiency, information completeness, and interoperability, while neglecting decision-level fusion.", "method": "Proposes using natural language communication for sharing intentions, rationales, and decisions among agents, instead of just perception-oriented data sharing.", "result": "Natural language proved effective for balancing semantic density and bandwidth in coordination, and enables adaptability and enhanced interoperability.", "conclusion": "Utilizing natural language transforms collaborative driving into proactive coordination, leading to improved safety, efficiency, and transparency in transportation systems."}}
{"id": "2507.01810", "pdf": "https://arxiv.org/pdf/2507.01810", "abs": "https://arxiv.org/abs/2507.01810", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Vijay Mago"], "title": "Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes", "categories": ["cs.CL", "cs.IR"], "comment": "To appear in the ACL Anthology", "summary": "We present a comparative analysis of the parseability of structured outputs\ngenerated by small language models for open attribute-value extraction from\nclinical notes. We evaluate three widely used serialization formats: JSON,\nYAML, and XML, and find that JSON consistently yields the highest parseability.\nStructural robustness improves with targeted prompting and larger models, but\ndeclines for longer documents and certain note types. Our error analysis\nidentifies recurring format-specific failure patterns. These findings offer\npractical guidance for selecting serialization formats and designing prompts\nwhen deploying language models in privacy-sensitive clinical settings.", "AI": {"tldr": "Small language models were tested for parsing structured outputs in clinical notes using JSON, YAML, and XML formats, with JSON proving consistently more reliable.", "motivation": "The paper aims to determine the most robust serialization format for structured outputs from language models in clinical settings.", "method": "The study evaluates three serialization formats (JSON, YAML, XML) across various conditions using small language models and analyzes structural robustness under different scenarios.", "result": "JSON emerged as the most parseable format. Parseability improved with better prompts and larger models, but certain note types and longer documents negatively impacted robustness.", "conclusion": "This research provides practical suggestions for selecting serialization formats and designing prompts for language models in clinical contexts where privacy is critical."}}
{"id": "2507.01478", "pdf": "https://arxiv.org/pdf/2507.01478", "abs": "https://arxiv.org/abs/2507.01478", "authors": ["Chentao Shen", "Ding Pan", "Mingyu Mei", "Zaixing He", "Xinyue Zhao"], "title": "Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects", "categories": ["cs.CV"], "comment": "preprint version", "summary": "Visual pose tracking is playing an increasingly vital role in industrial\ncontexts in recent years. However, the pose tracking for industrial metal\nobjects remains a challenging task especially in the real world-environments,\ndue to the reflection characteristic of metal objects. To address this issue,\nwe propose a novel 6DoF pose tracking method based on active control points.\nThe method uses image control points to generate edge feature for optimization\nactively instead of 6DoF pose-based rendering, and serve them as optimization\nvariables. We also introduce an optimal control point regression method to\nimprove robustness. The proposed tracking method performs effectively in both\ndataset evaluation and real world tasks, providing a viable solution for\nreal-time tracking of industrial metal objects. Our source code is made\npublicly available at: https://github.com/tomatoma00/ACPTracking.", "AI": {"tldr": "The paper introduces a novel method for 6DoF pose tracking of industrial metal objects using active control points and optimal regression techniques to overcome the challenges posed by reflective surfaces.", "motivation": "Pose tracking for industrial metal objects is difficult due to reflection challenges in real-world environments.", "method": "It uses image control points to generate edge features for optimization, replaces pose-based rendering, and implements optimal control point regression for better robustness.", "result": "The method effectively tracks metal objects in both dataset evaluations and real-world tasks, showing real-time capabilities.", "conclusion": "The proposed approach is a practical solution to 6DoF pose tracking problems in industry and is open-sourced for public use."}}
{"id": "2507.01117", "pdf": "https://arxiv.org/pdf/2507.01117", "abs": "https://arxiv.org/abs/2507.01117", "authors": ["Nikita Sakovich", "Dmitry Aksenov", "Ekaterina Pleshakova", "Sergey Gataullin"], "title": "A Neural Operator based on Dynamic Mode Decomposition", "categories": ["cs.LG", "68T07, 35A99"], "comment": "30 pages, 10 figures", "summary": "The scientific computation methods development in conjunction with artificial\nintelligence technologies remains a hot research topic. Finding a balance\nbetween lightweight and accurate computations is a solid foundation for this\ndirection. The study presents a neural operator based on the dynamic mode\ndecomposition algorithm (DMD), mapping functional spaces, which combines DMD\nand deep learning (DL) for spatiotemporal processes efficient modeling. Solving\nPDEs for various initial and boundary conditions requires significant\ncomputational resources. The method suggested automatically extracts key modes\nand system dynamics using them to construct predictions, reducing computational\ncosts compared to traditional numerical methods. The approach has demonstrated\nits efficiency through comparative analysis of performance with closest\nanalogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers\nequation solutions approximation, where it achieves high reconstruction\naccuracy.", "AI": {"tldr": "The paper introduces a neural operator combining Dynamic Mode Decomposition (DMD) with deep learning for efficient spatiotemporal modeling, reducing computation costs in PDE solutions while maintaining accuracy.", "motivation": "To address the computational resource intensiveness in solving PDEs for various initial and boundary conditions and improve lightweight accurate computations.", "method": "A dynamic mode decomposition (DMD)-based neural operator is developed, integrating DMD and deep learning for automatic extraction of key system dynamics and modes to predict outcomes.", "result": "The proposed method demonstrated high reconstruction accuracy and efficiency compared to DeepONet and FNO in heat equation, Laplace\u2019s equation, and Burgers equation approximation.", "conclusion": "The presented approach effectively balances computational cost and prediction accuracy, showcasing utility in spatiotemporal process modeling for PDE solving tasks."}}
{"id": "2507.01055", "pdf": "https://arxiv.org/pdf/2507.01055", "abs": "https://arxiv.org/abs/2507.01055", "authors": ["Hao Yang", "Xinlong Liang", "Zhang Li", "Yue Sun", "Zheyu Hu", "Xinghe Xie", "Behdad Dashtbozorg", "Jincheng Huang", "Shiwei Zhu", "Luyi Han", "Jiong Zhang", "Shanshan Wang", "Ritse Mann", "Qifeng Yu", "Tao Tan"], "title": "Prompt Mechanisms in Medical Imaging: A Comprehensive Survey", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep learning offers transformative potential in medical imaging, yet its\nclinical adoption is frequently hampered by challenges such as data scarcity,\ndistribution shifts, and the need for robust task generalization. Prompt-based\nmethodologies have emerged as a pivotal strategy to guide deep learning models,\nproviding flexible, domain-specific adaptations that significantly enhance\nmodel performance and adaptability without extensive retraining. This\nsystematic review critically examines the burgeoning landscape of prompt\nengineering in medical imaging. We dissect diverse prompt modalities, including\ntextual instructions, visual prompts, and learnable embeddings, and analyze\ntheir integration for core tasks such as image generation, segmentation, and\nclassification. Our synthesis reveals how these mechanisms improve\ntask-specific outcomes by enhancing accuracy, robustness, and data efficiency\nand reducing reliance on manual feature engineering while fostering greater\nmodel interpretability by making the model's guidance explicit. Despite\nsubstantial advancements, we identify persistent challenges, particularly in\nprompt design optimization, data heterogeneity, and ensuring scalability for\nclinical deployment. Finally, this review outlines promising future\ntrajectories, including advanced multimodal prompting and robust clinical\nintegration, underscoring the critical role of prompt-driven AI in accelerating\nthe revolution of diagnostics and personalized treatment planning in medicine.", "AI": {"tldr": "The paper reviews the role of prompt engineering in deep learning for medical imaging, examining how it addresses challenges like data scarcity, distribution shifts, and task generalization.", "motivation": "To address barriers in clinical adoption of deep learning in medical imaging, such as data constraints and the need for better model generalization.", "method": "A systematic review of prompt-based methodologies, exploring textual, visual, and learnable prompts and their application in image-related tasks like generation, segmentation, and classification.", "result": "Prompt-based techniques enhance task accuracy, robustness, data efficiency, and interpretability, while reducing reliance on manual feature engineering.", "conclusion": "Prompt-driven AI has substantial potential in medical imaging, but challenges like prompt design optimization and clinical scalability remain. The future lies in advanced multimodal prompting and robust clinical integration."}}
{"id": "2507.01844", "pdf": "https://arxiv.org/pdf/2507.01844", "abs": "https://arxiv.org/abs/2507.01844", "authors": ["Arthur Wuhrmann", "Anastasiia Kucherenko", "Andrei Kucharavy"], "title": "Low-Perplexity LLM-Generated Sequences and Where To Find Them", "categories": ["cs.CL", "cs.LG"], "comment": "Camera-ready version. Accepted to ACL 2025. 10 pages, 4 figures, 6\n  tables", "summary": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior.", "AI": {"tldr": "This paper analyzes how Large Language Models (LLMs) utilize their training data by focusing on sequences of text that are highly probable according to the model.", "motivation": "The work seeks to address transparency, accountability, privacy, and fairness in LLMs by understanding the relationship between their training data and generated outputs.", "method": "Their approach extracts high-probability text spans (low-perplexity sequences) from LLM-generated outputs, traces these sequences back to their training sources, and analyzes the distribution and nature of these matches.", "result": "It reveals that many high-probability spans cannot be traced back to the training corpus, while others involve verbatim recall from varying source distributions.", "conclusion": "The study provides insights into how training data affects LLM behavior and offers a framework for analyzing training-data attribution in generated content."}}
{"id": "2507.01484", "pdf": "https://arxiv.org/pdf/2507.01484", "abs": "https://arxiv.org/abs/2507.01484", "authors": ["Xiaoshuai Hao", "Yuting Zhao", "Yuheng Ji", "Luanyuan Dai", "Peng Hao", "Dingzhe Li", "Shuai Cheng", "Rong Yin"], "title": "What Really Matters for Robust Multi-Sensor HD Map Construction?", "categories": ["cs.CV"], "comment": "Accepted by IROS 2025", "summary": "High-definition (HD) map construction methods are crucial for providing\nprecise and comprehensive static environmental information, which is essential\nfor autonomous driving systems. While Camera-LiDAR fusion techniques have shown\npromising results by integrating data from both modalities, existing approaches\nprimarily focus on improving model accuracy and often neglect the robustness of\nperception models, which is a critical aspect for real-world applications. In\nthis paper, we explore strategies to enhance the robustness of multi-modal\nfusion methods for HD map construction while maintaining high accuracy. We\npropose three key components: data augmentation, a novel multi-modal fusion\nmodule, and a modality dropout training strategy. These components are\nevaluated on a challenging dataset containing 10 days of NuScenes data. Our\nexperimental results demonstrate that our proposed methods significantly\nenhance the robustness of baseline methods. Furthermore, our approach achieves\nstate-of-the-art performance on the clean validation set of the NuScenes\ndataset. Our findings provide valuable insights for developing more robust and\nreliable HD map construction models, advancing their applicability in\nreal-world autonomous driving scenarios. Project website:\nhttps://robomap-123.github.io.", "AI": {"tldr": "The paper focuses on improving robustness in Camera-LiDAR multi-modal fusion for HD map construction, achieving state-of-the-art results with novel strategies.", "motivation": "To address the lack of robustness in existing Camera-LiDAR fusion methods for HD map construction, critical for autonomous vehicles.", "method": "Introduces three components: data augmentation, a novel multi-modal fusion module, and a modality dropout training strategy, tested on NuScenes data.", "result": "Significant robustness improvements over baselines and state-of-the-art performance on the NuScenes validation set.", "conclusion": "The proposed methods provide valuable insights for creating robust HD map construction models, enhancing real-world autonomous driving applications."}}
{"id": "2507.01129", "pdf": "https://arxiv.org/pdf/2507.01129", "abs": "https://arxiv.org/abs/2507.01129", "authors": ["Arun Ganesh", "Brendan McMahan", "Abhradeep Thakurta"], "title": "On Design Principles for Private Adaptive Optimizers", "categories": ["cs.LG", "cs.CR"], "comment": "PPML 2025", "summary": "The spherical noise added to gradients in differentially private (DP)\ntraining undermines the performance of adaptive optimizers like AdaGrad and\nAdam, and hence many recent works have proposed algorithms to address this\nchallenge. However, the empirical results in these works focus on simple tasks\nand models and the conclusions may not generalize to model training in\npractice. In this paper we survey several of these variants, and develop better\ntheoretical intuition for them as well as perform empirical studies comparing\nthem. We find that a common intuition of aiming for unbiased estimates of\nsecond moments of gradients in adaptive optimizers is misguided, and instead\nthat a simple technique called scale-then-privatize (which does not achieve\nunbiased second moments) has more desirable theoretical behaviors and\noutperforms all other variants we study on a small-scale language model\ntraining task. We additionally argue that scale-then-privatize causes the noise\naddition to better match the application of correlated noise mechanisms which\nare more desirable to use in practice.", "AI": {"tldr": "This paper evaluates techniques for improving the compatibility of adaptive optimizers like Adam and AdaGrad with differentially private (DP) training, proposing that scale-then-privatize performs better than other methods on small-scale language model tasks.", "motivation": "Differentially private training often degrades the performance of adaptive optimizers due to added noise, necessitating improved methods that handle noise while maintaining learning efficiency.", "method": "The authors survey existing DP adaptive optimizer variants, fostering theoretical insight and conducting empirical comparisons, introducing and validating the scale-then-privatize technique.", "result": "The scale-then-privatize approach demonstrated superior performance and theoretical behaviors compared to other variants in small-scale language model training.", "conclusion": "Contrary to popular practices aiming for unbiased gradient estimates, scale-then-privatize offers a promising method for DP training due to its robust theoretical properties and practical performance advantages."}}
{"id": "2507.01161", "pdf": "https://arxiv.org/pdf/2507.01161", "abs": "https://arxiv.org/abs/2507.01161", "authors": ["Zhizhuo Zhang", "Hao Peng", "Xiaoli Bai"], "title": "Imitation Learning for Satellite Attitude Control under Unknown Perturbations", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "2025 AAS/AIAA Astrodynamics Specialist Conference", "summary": "This paper presents a novel satellite attitude control framework that\nintegrates Soft Actor-Critic (SAC) reinforcement learning with Generative\nAdversarial Imitation Learning (GAIL) to achieve robust performance under\nvarious unknown perturbations. Traditional control techniques often rely on\nprecise system models and are sensitive to parameter uncertainties and external\nperturbations. To overcome these limitations, we first develop a SAC-based\nexpert controller that demonstrates improved resilience against actuator\nfailures, sensor noise, and attitude misalignments, outperforming our previous\nresults in several challenging scenarios. We then use GAIL to train a learner\npolicy that imitates the expert's trajectories, thereby reducing training costs\nand improving generalization through expert demonstrations. Preliminary\nexperiments under single and combined perturbations show that the SAC expert\ncan rotate the antenna to a specified direction and keep the antenna\norientation reliably stable in most of the listed perturbations. Additionally,\nthe GAIL learner can imitate most of the features from the trajectories\ngenerated by the SAC expert. Comparative evaluations and ablation studies\nconfirm the effectiveness of the SAC algorithm and reward shaping. The\nintegration of GAIL further reduces sample complexity and demonstrates\npromising imitation capabilities, paving the way for more intelligent and\nautonomous spacecraft control systems.", "AI": {"tldr": "The paper introduces a novel approach combining Soft Actor-Critic (SAC) with Generative Adversarial Imitation Learning (GAIL) for advanced satellite attitude control against perturbations.", "motivation": "Satellite control systems face challenges such as actuator failures, sensor noise, and uncertainties in external perturbations, where traditional methods relying on precision models may fail.", "method": "The method integrates SAC reinforcement learning to develop a resilient expert controller, followed by GAIL to train a learner policy that imitates the expert\u2019s actions, improving generalization while reducing training costs.", "result": "The SAC expert demonstrated robust control in challenging conditions, while the GAIL learner effectively mimicked the expert\u2019s trajectories, reducing sample complexity.", "conclusion": "Combining SAC and GAIL provides a more efficient and effective framework for intelligent and autonomous satellite attitude control, with strong generalization and reduced training demands."}}
{"id": "2507.01058", "pdf": "https://arxiv.org/pdf/2507.01058", "abs": "https://arxiv.org/abs/2507.01058", "authors": ["Puspendu Banerjee", "Aritra Mazumdar", "Wazib Ansar", "Saptarsi Goswami", "Amlan Chakrabarti"], "title": "A Data Science Approach to Calcutta High Court Judgments: An Efficient LLM and RAG-powered Framework for Summarization and Similar Cases Retrieval", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "12 pages, 6 figures", "summary": "The judiciary, as one of democracy's three pillars, is dealing with a rising\namount of legal issues, needing careful use of judicial resources. This\nresearch presents a complex framework that leverages Data Science\nmethodologies, notably Large Language Models (LLM) and Retrieval-Augmented\nGeneration (RAG) techniques, to improve the efficiency of analyzing Calcutta\nHigh Court verdicts. Our framework focuses on two key aspects: first, the\ncreation of a robust summarization mechanism that distills complex legal texts\ninto concise and coherent summaries; and second, the development of an\nintelligent system for retrieving similar cases, which will assist legal\nprofessionals in research and decision making. By fine-tuning the Pegasus model\nusing case head note summaries, we achieve significant improvements in the\nsummarization of legal cases. Our two-step summarizing technique preserves\ncrucial legal contexts, allowing for the production of a comprehensive vector\ndatabase for RAG. The RAG-powered framework efficiently retrieves similar cases\nin response to user queries, offering thorough overviews and summaries. This\ntechnique not only improves legal research efficiency, but it also helps legal\nprofessionals and students easily acquire and grasp key legal information,\nbenefiting the overall legal scenario.", "AI": {"tldr": "The paper presents a framework using Data Science techniques, including LLM and RAG, to enhance the analysis of Calcutta High Court verdicts. It focuses on summarization and case retrieval.", "motivation": "The judiciary faces an increasing load of legal issues, necessitating the efficient use of resources.", "method": "The framework employs fine-tuned Pegasus models for summarization and creates a vector database for RAG to retrieve similar cases.", "result": "The system effectively summarizes complex legal texts and retrieves similar cases, aiding legal research and decision-making.", "conclusion": "The developed framework improves efficiency in legal research, benefiting professionals and students with better access to condensed, relevant legal information."}}
{"id": "2507.01853", "pdf": "https://arxiv.org/pdf/2507.01853", "abs": "https://arxiv.org/abs/2507.01853", "authors": ["Samridhi Raj Sinha", "Rajvee Sheth", "Abhishek Upperwal", "Mayank Singh"], "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.", "AI": {"tldr": "The paper introduces EKA-EVAL, an evaluation framework for Large Language Models (LLMs) that includes over 35 benchmarks, with a focus on supporting Indic languages.", "motivation": "Existing evaluation tools for Large Language Models largely focus on English-centric benchmarks and lack inclusivity for diverse linguistic regions, such as India.", "method": "EKA-EVAL integrates over 35 benchmarks, including 10 Indic-specific datasets, and supports distributed inference, quantization, and multi-GPU use for broad, multilingual model evaluation.", "result": "The framework establishes itself as the first extensible, end-to-end solution for benchmarking both global and Indic-specific LLMs, facilitating multilingual benchmarking.", "conclusion": "EKA-EVAL significantly advances the evaluation ecosystem for LLMs by providing a unified, production-ready, and extensible solution, catering to global and multilingual needs."}}
{"id": "2507.01492", "pdf": "https://arxiv.org/pdf/2507.01492", "abs": "https://arxiv.org/abs/2507.01492", "authors": ["Jiyang Tang", "Hengyi Li", "Yifan Du", "Wayne Xin Zhao"], "title": "AVC-DPO: Aligned Video Captioning via Direct Preference Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Although video multimodal large language models (video MLLMs) have achieved\nsubstantial progress in video captioning tasks, it remains challenging to\nadjust the focal emphasis of video captions according to human preferences. To\naddress this limitation, we propose Aligned Video Captioning via Direct\nPreference Optimization (AVC-DPO), a post-training framework designed to\nenhance captioning capabilities in video MLLMs through preference alignment.\nOur approach designs enhanced prompts that specifically target temporal\ndynamics and spatial information-two key factors that humans care about when\nwatching a video-thereby incorporating human-centric preferences. AVC-DPO\nleverages the same foundation model's caption generation responses under varied\nprompt conditions to conduct preference-aware training and caption alignment.\nUsing this framework, we have achieved exceptional performance in the\nLOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving\nfirst place on the Video Detailed Captioning (VDC) benchmark according to the\nVDCSCORE evaluation metric.", "AI": {"tldr": "The paper introduces AVC-DPO, a framework to improve video captioning quality in multimodal language models by aligning captions with human preferences.", "motivation": "The need to optimize video captions for human preferences, focusing on temporal dynamics and spatial information.", "method": "AVC-DPO employs enhanced prompts targeting specific video features and conducts post-training using varied prompts to align captions with human-centric preferences.", "result": "The method achieved first place in the Video Detailed Captioning Challenge according to VDCSCORE evaluation metric.", "conclusion": "AVC-DPO enhances video MLLMs' captioning capabilities, showing superior performance in aligning outputs with human preferences."}}
{"id": "2507.01131", "pdf": "https://arxiv.org/pdf/2507.01131", "abs": "https://arxiv.org/abs/2507.01131", "authors": ["Yuchao Lin", "Cong Fu", "Zachary Krueger", "Haiyang Yu", "Maho Nakata", "Jianwen Xie", "Emine Kucukbenli", "Xiaofeng Qian", "Shuiwang Ji"], "title": "Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "$\\rm{SO}(3)$-equivariant networks are the dominant models for machine\nlearning interatomic potentials (MLIPs). The key operation of such networks is\nthe Clebsch-Gordan (CG) tensor product, which is computationally expensive. To\naccelerate the computation, we develop tensor decomposition networks (TDNs) as\na class of approximately equivariant networks whose CG tensor products are\nreplaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)\ndecomposition. With the CP decomposition, we prove (i) a uniform bound on the\ninduced error of $\\rm{SO}(3)$-equivariance, and (ii) the universality of\napproximating any equivariant bilinear map. To further reduce the number of\nparameters, we propose path-weight sharing that ties all multiplicity-space\nweights across the $O(L^3)$ CG paths into a single path without compromising\nequivariance, where $L$ is the maximum angular degree. The resulting layer acts\nas a plug-and-play replacement for tensor products in existing networks, and\nthe computational complexity of tensor products is reduced from $O(L^6)$ to\n$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation\ndataset containing 105 million DFT-calculated snapshots. We also use existing\ndatasets, including OC20, and OC22. Results show that TDNs achieve competitive\nperformance with dramatic speedup in computations.", "AI": {"tldr": "This paper introduces Tensor Decomposition Networks (TDNs) as an efficient alternative to traditional SO(3)-equivariant networks for machine learning interatomic potentials, significantly reducing computational complexity while maintaining performance.", "motivation": "The paper aims to address the computational inefficiency of Clebsch-Gordan (CG) tensor products in SO(3)-equivariant networks.", "method": "The authors develop TDNs using low-rank tensor decompositions like CP decomposition, proving its capability to approximate equivariant bilinear maps and applying path-weight sharing to reduce parameters.", "result": "TDNs reduce computational complexity from O(L^6) to O(L^4), achieving competitive performance on large datasets, including PubChemQCR and Open Catalyst datasets (OC20, OC22).", "conclusion": "Tensor Decomposition Networks offer a scalable and efficient alternative for SO(3)-equivariant networks, enabling broader applications without compromising computational accuracy."}}
{"id": "2507.01872", "pdf": "https://arxiv.org/pdf/2507.01872", "abs": "https://arxiv.org/abs/2507.01872", "authors": ["Kenan Tang", "Yanhong Li", "Yao Qin"], "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System", "categories": ["cs.CL"], "comment": "Submitted to EMNLP 2025 System Demonstration", "summary": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG.", "AI": {"tldr": "The paper introduces DIY-MKG, an open-source system for personalized polyglot language learning, emphasizing knowledge graphs, adaptive quizzes, and LLMs' dynamic support.", "motivation": "Language learning tools lack support for multilingual connections, customization for learners' individual needs, and often lead to cognitive offloading.", "method": "The system helps users create their own vocabulary knowledge graphs expanded by LLM suggestions, enables rich annotations, and uses LLMs to generate personalized adaptive quizzes. Incorrect quizzes can be flagged, creating a feedback loop.", "result": "DIY-MKG demonstrates reliable and fair vocabulary expansion across languages and highly accurate quiz generation, showcasing the system's robustness.", "conclusion": "DIY-MKG offers effective, customizable, and interactive support for polyglot learners, improving engagement and addressing previous language tool gaps."}}
{"id": "2507.01494", "pdf": "https://arxiv.org/pdf/2507.01494", "abs": "https://arxiv.org/abs/2507.01494", "authors": ["Muhammad Hassam Ejaz", "Muhammad Bilal", "Usman Habib"], "title": "Crop Pest Classification Using Deep Learning Techniques: A Review", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Insect pests continue to bring a serious threat to crop yields around the\nworld, and traditional methods for monitoring them are often slow, manual, and\ndifficult to scale. In recent years, deep learning has emerged as a powerful\nsolution, with techniques like convolutional neural networks (CNNs), vision\ntransformers (ViTs), and hybrid models gaining popularity for automating pest\ndetection. This review looks at 37 carefully selected studies published between\n2018 and 2025, all focused on AI-based pest classification. The selected\nresearch is organized by crop type, pest species, model architecture, dataset\nusage, and key technical challenges. The early studies relied heavily on CNNs\nbut latest work is shifting toward hybrid and transformer-based models that\ndeliver higher accuracy and better contextual understanding. Still, challenges\nlike imbalanced datasets, difficulty in detecting small pests, limited\ngeneralizability, and deployment on edge devices remain significant hurdles.\nOverall, this review offers a structured overview of the field, highlights\nuseful datasets, and outlines the key challenges and future directions for\nAI-based pest monitoring systems.", "AI": {"tldr": "The paper reviews 37 studies on AI-based pest classification, highlighting the evolution from CNNs to hybrid and transformer models, current challenges, and future directions.", "motivation": "To address the challenge of accurately detecting and monitoring insect pests threatening global crop yields, aiming to improve and scale traditional manual methods.", "method": "The paper reviews and organizes 37 studies by factors like crop type, pest species, model architecture, dataset usage, and challenges, focusing on AI approaches (CNNs, ViTs, and hybrid models).", "result": "AI methods are advancing from CNNs to more precise hybrid and transformer-based models, but significant challenges such as dataset imbalance, small pest detection, and scalability on edge devices remain.", "conclusion": "AI-based pest classification holds promise with improved model architectures, but further innovation is needed to address key limitations for practical and scalable deployment."}}
{"id": "2507.01132", "pdf": "https://arxiv.org/pdf/2507.01132", "abs": "https://arxiv.org/abs/2507.01132", "authors": ["Brenda Nogueira", "Gabe Gomes", "Meng Jiang", "Nitesh V. Chawla", "Nuno Moniz"], "title": "Spectral Manifold Harmonization for Graph Imbalanced Regression", "categories": ["cs.LG", "q-bio.MN"], "comment": null, "summary": "Graph-structured data is ubiquitous in scientific domains, where models often\nface imbalanced learning settings. In imbalanced regression, domain preferences\nfocus on specific target value ranges representing the most scientifically\nvaluable cases; we observe a significant lack of research. In this paper, we\npresent Spectral Manifold Harmonization (SMH), a novel approach for addressing\nthis imbalanced regression challenge on graph-structured data by generating\nsynthetic graph samples that preserve topological properties while focusing on\noften underrepresented target distribution regions. Conventional methods fail\nin this context because they either ignore graph topology in case generation or\ndo not target specific domain ranges, resulting in models biased toward average\ntarget values. Experimental results demonstrate the potential of SMH on\nchemistry and drug discovery benchmark datasets, showing consistent\nimprovements in predictive performance for target domain ranges.", "AI": {"tldr": "The paper introduces Spectral Manifold Harmonization (SMH) to address the issue of imbalanced regression in graph-structured data, focusing on underrepresented target ranges and demonstrating improved performance on chemistry and drug discovery datasets.", "motivation": "Imbalanced regression in graph-structured data often fails to focus on specific target value ranges that are scientifically valuable. Existing methods either ignore graph topology or skew towards average target values, creating biases.", "method": "Spectral Manifold Harmonization (SMH) generates synthetic graph samples that preserve topological properties while targeting underrepresented regions in the target distribution.", "result": "Experimental validation on benchmark datasets in chemistry and drug discovery shows that SMH consistently improves predictive performance for important target domain ranges.", "conclusion": "SMH effectively tackles imbalanced regression in graph-structured data by focusing on valuable target ranges and preserving topology, indicating its potential for scientific domains like drug discovery."}}
{"id": "2507.01350", "pdf": "https://arxiv.org/pdf/2507.01350", "abs": "https://arxiv.org/abs/2507.01350", "authors": ["Abhinav Sinha", "Shashi Ranjan Kumar"], "title": "Cooperative Target Capture in 3D Engagements over Switched Dynamic Graphs", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY"], "comment": null, "summary": "This paper presents a leaderless cooperative guidance strategy for\nsimultaneous time-constrained interception of a stationary target when the\ninterceptors exchange information over switched dynamic graphs. We specifically\nfocus on scenarios when the interceptors lack radial acceleration capabilities,\nrelying solely on their lateral acceleration components. This consideration\naligns with their inherent kinematic turn constraints. The proposed strategy\nexplicitly addresses the complexities of coupled 3D engagements, thereby\nmitigating performance degradation that typically arises when the pitch and yaw\nchannels are decoupled into two separate, mutually orthogonal planar\nengagements. Moreover, our formulation incorporates modeling uncertainties\nassociated with the time-to-go estimation into the derivation of cooperative\nguidance commands to ensure robustness against inaccuracies in dynamic\nengagement scenarios. To optimize control efficiency, we analytically derive\nthe lateral acceleration components in the orthogonal pitch and yaw channels by\nsolving an instantaneous optimization problem, subject to an affine constraint.\nWe show that the proposed cooperative guidance commands guarantee consensus in\ntime-to-go values within a predefined time, which can be prescribed as a design\nparameter, regardless of the interceptors' initial configurations. We provide\nsimulations to attest to the efficacy of the proposed method.", "AI": {"tldr": "The paper develops a leaderless cooperative guidance strategy for interceptors without radial acceleration capabilities to achieve simultaneous time-constrained interception of a stationary target, leveraging switched dynamic graphs.", "motivation": "To address the challenge of simultaneous target interception with interceptors that lack radial acceleration capabilities and are limited by kinematic turn constraints, focusing on robust performance in complex 3D engagement scenarios.", "method": "The method involves deriving guidance commands using instantaneous optimization under affine constraints in coupled pitch and yaw channels, while incorporating time-to-go uncertainties to ensure robustness and consensus within predefined time limits.", "result": "The proposed strategy guarantees time-to-go consensus among interceptors within a preset time, regardless of initial conditions, and improves robustness in dynamic engagement scenarios.", "conclusion": "The developed guidance strategy effectively handles coupled 3D engagements and improves interception performance under kinematic constraints, demonstrating robustness and practical applicability through simulations."}}
{"id": "2507.01061", "pdf": "https://arxiv.org/pdf/2507.01061", "abs": "https://arxiv.org/abs/2507.01061", "authors": ["Jingjing Qu", "Kejia Hu", "Jun Zhu", "Wenhao Li", "Teng Wang", "Zhiyun Chen", "Yulei Ye", "Chaochao Lu", "Aimin Zhou", "Xiangfeng Wang", "James Evan"], "title": "Epitome: Pioneering an Experimental Platform for AI-Social Science Integration", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "18 pages, 5figures", "summary": "The integration of Large Language Models (LLMs) into social science\nexperiments represents a transformative approach to understanding human-AI\ninteractions and their societal impacts. We introduce Epitome, the world's\nfirst open experimental platform dedicated to the deep integration of\nartificial intelligence and social science. Rooted in theoretical foundations\nfrom management, communication studies, sociology, psychology, and ethics,\nEpitome focuses on the interactive impacts of AI on individuals, organizations,\nand society during its real-world deployment. It constructs a theoretical\nsupport system through cross-disciplinary experiments. The platform offers a\none-stop comprehensive experimental solution spanning \"foundation\nmodels-complex application development-user feedback\" through seven core\nmodules, while embedding the classical \"control-comparison-comparative causal\nlogic\" of social science experiments into multilevel human-computer interaction\nenvironments, including dialogues, group chats, and multi-agent virtual\nscenarios. With its canvas-style, user-friendly interface, Epitome enables\nresearchers to easily design and run complex experimental scenarios,\nfacilitating systematic investigations into the social impacts of AI and\nexploration of integrated solutions.To demonstrate its capabilities, we\nreplicated three seminal social science experiments involving LLMs, showcasing\nEpitome's potential to streamline complex experimental designs and produce\nrobust results, suitable for publishing in the top selective journals. Our\nfindings highlight the platform's utility in enhancing the efficiency and\nquality of human-AI interactions, providing valuable insights into the societal\nimplications of AI technologies. Epitome thus offers a powerful tool for\nadvancing interdisciplinary research at the intersection of AI and social\nscience, with potential applications in policy-making, ...", "AI": {"tldr": "The paper introduces Epitome, an open experimental platform combining AI and social science to study human-AI interaction and societal impacts through robust experiments.", "motivation": "To bridge the gap between artificial intelligence and social science by creating a platform that evaluates AI's societal impacts through interdisciplinary research.", "method": "Developed Epitome\u2014a platform combining AI technologies and social science methodologies with modules for systematic experimentation and multilevel human-computer interactions.", "result": "Successfully replicated three foundational social science experiments using LLMs, demonstrating the platform's ability to streamline experimental designs and yield robust results.", "conclusion": "Epitome facilitates interdisciplinary research into AI's societal influences, enhancing experimental efficiency and quality, and has policy-making and academic implications."}}
{"id": "2507.01887", "pdf": "https://arxiv.org/pdf/2507.01887", "abs": "https://arxiv.org/abs/2507.01887", "authors": ["Dongyi Ding", "Tiannan Wang", "Chenghao Zhu", "Meiling Tao", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs.", "AI": {"tldr": "This paper proposes MiCoTA, a teaching framework that uses intermediate models to improve small language models (SLMs) in reasoning tasks by bridging capacity and reasoning gaps.", "motivation": "To address the difficulty small language models face in learning complex, long-form chain-of-thought (CoT) reasoning due to their limited computational capacity.", "method": "The MiCoTA framework introduces intermediate-sized teacher models and employs intermediate-length reasoning examples to better align training data with the capacity of small models.", "result": "Experiments show notable improvements in SLM reasoning capabilities, with Qwen2.5-based models achieving significant benchmark performance boosts on tasks related to reasoning.", "conclusion": "MiCoTA effectively enhances reasoning for SLMs, paving the way for future advancements in making high-level reasoning tasks computationally feasible on smaller models."}}
{"id": "2507.01496", "pdf": "https://arxiv.org/pdf/2507.01496", "abs": "https://arxiv.org/abs/2507.01496", "authors": ["Jimyeong Kim", "Jungwon Park", "Yeji Song", "Nojun Kwak", "Wonjong Rhee"], "title": "ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation", "categories": ["cs.CV"], "comment": "Published at ICCV 2025. Project page:\n  https://wlaud1001.github.io/ReFlex/", "summary": "Rectified Flow text-to-image models surpass diffusion models in image quality\nand text alignment, but adapting ReFlow for real-image editing remains\nchallenging. We propose a new real-image editing method for ReFlow by analyzing\nthe intermediate representations of multimodal transformer blocks and\nidentifying three key features. To extract these features from real images with\nsufficient structural preservation, we leverage mid-step latent, which is\ninverted only up to the mid-step. We then adapt attention during injection to\nimprove editability and enhance alignment to the target text. Our method is\ntraining-free, requires no user-provided mask, and can be applied even without\na source prompt. Extensive experiments on two benchmarks with nine baselines\ndemonstrate its superior performance over prior methods, further validated by\nhuman evaluations confirming a strong user preference for our approach.", "AI": {"tldr": "This paper introduces a new method for real-image editing using Rectified Flow (ReFlow) models, outperforming prior methods in image quality and text alignment without requiring training, masks, or source prompts.", "motivation": "While ReFlow models show superior performance in text-to-image generation, adapting them for real-image editing has been difficult, necessitating new techniques.", "method": "The authors leverage mid-step latents and adapt attention mechanisms during injection to extract key intermediate features, enabling real-image editing without extensive prerequisites like masks or additional training.", "result": "Experiments on two benchmarks and nine baselines demonstrate significant performance improvements over existing methods, with human evaluations favoring this approach.", "conclusion": "The proposed technique enables effective, user-friendly real-image editing using ReFlow models, pushing the boundaries of multimodal text-to-image models for practical applications."}}
{"id": "2507.01154", "pdf": "https://arxiv.org/pdf/2507.01154", "abs": "https://arxiv.org/abs/2507.01154", "authors": ["Liangyu Wang", "Junxiao Wang", "Jie Ren", "Zihang Xiang", "David E. Keyes", "Di Wang"], "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.", "AI": {"tldr": "This paper introduces FlashDP, an efficient per-layer DP-SGD method for training large language models with Differential Privacy, achieving significant memory and computational efficiency gains.", "motivation": "To address the privacy concerns of training data in large language models and tackle the inefficiencies of existing Differential Privacy methods (e.g., memory requirements in explicit methods and redundant computations in implicit methods).", "method": "The authors propose FlashDP, a per-layer DP-SGD technique that consolidates gradient computation into a fused, cache-friendly step. This avoids recalculating gradients multiple times and reduces memory movement.", "result": "FlashDP reduces memory movement by up to 50% and redundant computations by 20% compared to prior methods. It achieves 90% throughput of Non-DP methods on a four-A100 system, while maintaining comparable accuracy for training the Llama-13B model.", "conclusion": "FlashDP proves to be an impactful innovation for efficient, privacy-preserving training of large language models. It addresses critical challenges in Differential Privacy integration without compromising performance."}}
{"id": "2507.01378", "pdf": "https://arxiv.org/pdf/2507.01378", "abs": "https://arxiv.org/abs/2507.01378", "authors": ["Ziyao Wang", "Rongpeng Li", "Sizhao Li", "Yuming Xiang", "Haiping Wang", "Zhifeng Zhao", "Honggang Zhang"], "title": "RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms", "categories": ["cs.MA", "cs.AI", "cs.RO"], "comment": null, "summary": "Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as\na critical research focus, and it typically requires the swarm to navigate\neffectively while avoiding obstacles and achieving continuous coverage over\nmultiple mission targets. Although traditional Multi-Agent Reinforcement\nLearning (MARL) approaches offer dynamic adaptability, they are hindered by the\nsemantic gap in numerical communication and the rigidity of homogeneous role\nstructures, resulting in poor generalization and limited task scalability.\nRecent advances in Large Language Model (LLM)-based control frameworks\ndemonstrate strong semantic reasoning capabilities by leveraging extensive\nprior knowledge. However, due to the lack of online learning and over-reliance\non static priors, these works often struggle with effective exploration,\nleading to reduced individual potential and overall system performance. To\naddress these limitations, we propose a Role-Adaptive LLM-Driven Yoked\nnavigation algorithm RALLY. Specifically, we first develop an LLM-driven\nsemantic decision framework that uses structured natural language for efficient\nsemantic communication and collaborative reasoning. Afterward, we introduce a\ndynamic role-heterogeneity mechanism for adaptive role switching and\npersonalized decision-making. Furthermore, we propose a Role-value Mixing\nNetwork (RMIX)-based assignment strategy that integrates LLM offline priors\nwith MARL online policies to enable semi-offline training of role selection\nstrategies. Experiments in the Multi-Agent Particle Environment (MPE)\nenvironment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY\noutperforms conventional approaches in terms of task coverage, convergence\nspeed, and generalization, highlighting its strong potential for collaborative\nnavigation in agentic multi-UAV systems.", "AI": {"tldr": "The paper introduces RALLY, an LLM-driven navigation algorithm for UAV swarms, combining strengths of LLMs and MARL while overcoming limitations of both.", "motivation": "Traditional methods for UAV swarm control face challenges with semantic communication and adaptability, leading to issues in generalization and task scalability. While LLM-based frameworks bring semantic reasoning capabilities, they fail in dynamic learning and effective exploration.", "method": "The RALLY approach integrates a semantic decision-making framework with dynamic role heterogeneity and a Role-value Mixing Network (RMIX) strategy for combining offline LLM priors and online MARL policies.", "result": "RALLY achieves superior performance over traditional models in tests, excelling in task coverage, convergence speed, and scalability, as demonstrated in the Multi-Agent Particle Environment and Software-In-The-Loop platform.", "conclusion": "RALLY offers a strong solution for UAV swarm navigation, balancing the strengths of LLM and MARL to truly advance collaborative decision-making and adaptability in multi-UAV systems."}}
{"id": "2507.01062", "pdf": "https://arxiv.org/pdf/2507.01062", "abs": "https://arxiv.org/abs/2507.01062", "authors": ["Seyma Yaman Kayadibi"], "title": "Quantifying Student Success with Generative AI: A Monte Carlo Simulation Informed by Systematic Review", "categories": ["cs.CY", "cs.AI", "62P25", "K.3.1; H.5.2"], "comment": "35 pages, 4 figures. All figures are image-based: one Python code\n  screenshot, one regression model output, one success score distribution\n  chart, and one PRISMA diagram. This article presents a standalone segment\n  from the author's master's thesis at Victoria University", "summary": "The exponential development of generative artificial intelligence (GenAI)\ntechnologies like ChatGPT has raised increasing curiosity about their use in\nhigher education, specifically with respect to how students view them, make use\nof them, and the implications for learning outcomes. This paper employs a\nhybrid methodological approach involving a systematic literature review and\nsimulation-based modeling to explore student perceptions of GenAI use in the\ncontext of higher education. A total of nineteen empirical articles from 2023\nthrough 2025 were selected from the PRISMA-based search targeting the Scopus\ndatabase. Synthesis of emerging patterns from the literature was achieved by\nthematic categorization. Six of these had enough quantitative information,\ni.e., item-level means and standard deviations, to permit probabilistic\nmodeling. One dataset, from the resulting subset, was itself selected as a\nrepresentative case with which to illustrate inverse-variance weighting by\nMonte Carlo simulation, by virtue of its well-designed Likert scale format and\nthematic alignment with the use of computing systems by the researcher.\n  The simulation provided a composite \"Success Score\" forecasting the strength\nof the relationship between student perceptions and learning achievements.\nFindings reveal that attitude factors concerned with usability and real-world\nusefulness are significantly better predictors of positive learning achievement\nthan affective or trust-based factors. Such an interdisciplinary perspective\nprovides a unique means of linking thematic results with predictive modelling,\nresonating with longstanding controversies about the proper use of GenAI tools\nwithin the university.", "AI": {"tldr": "This paper examines the impact of generative artificial intelligence (GenAI) technologies like ChatGPT on higher education by analyzing student perceptions and their effects on learning outcomes using systematic literature review and simulations.", "motivation": "The rapid growth of GenAI technologies has sparked interest in their applications within higher education, particularly concerning student perspectives, usage patterns, and influence on learning outcomes.", "method": "The study combines a systematic literature review of 19 empirical articles from 2023-2025 with thematic categorization and simulation-based probabilistic modeling to analyze student perceptions and predict learning outcomes.", "result": "The simulations identified a \"Success Score\" linking student perceptions to learning achievements, highlighting factors like usability and real-world utility as significant predictors of positive outcomes.", "conclusion": "Positive perceptions of GenAI, specifically concerning usability and practical value, strongly influence learning achievements, while trust and emotion-based factors are less predictive. This offers insights into optimizing GenAI tools in educational settings."}}
{"id": "2507.01900", "pdf": "https://arxiv.org/pdf/2507.01900", "abs": "https://arxiv.org/abs/2507.01900", "authors": ["Songtao Liu", "Peng Liu"], "title": "High-Layer Attention Pruning with Rescaling", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines.", "AI": {"tldr": "The paper introduces a novel structured pruning method for language models, focusing on strategic pruning in higher layers and adaptive rescaling to improve performance.", "motivation": "Existing pruning methods for language models are heuristic and don\u2019t consider the architectural positioning of attention heads, leading to inefficiencies.", "method": "Proposes a strategy to prune attention heads in higher layers and introduces an adaptive rescaling parameter to adjust token representation scales post-pruning.", "result": "Extensive testing on multiple LLMs and datasets showed better performance of the proposed method, with significant improvement in generation tasks compared to conventional methods.", "conclusion": "The proposed approach is more effective and efficient for structured pruning in LLMs, especially benefiting generation tasks."}}
{"id": "2507.01502", "pdf": "https://arxiv.org/pdf/2507.01502", "abs": "https://arxiv.org/abs/2507.01502", "authors": ["Ozan Durgut", "Beril Kallfelz-Sirmacek", "Cem Unsalan"], "title": "Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 4 figures, journal manuscript", "summary": "Global warming, loss of biodiversity, and air pollution are among the most\nsignificant problems facing Earth. One of the primary challenges in addressing\nthese issues is the lack of monitoring forests to protect them. To tackle this\nproblem, it is important to leverage remote sensing and computer vision methods\nto automate monitoring applications. Hence, automatic tree crown detection\nalgorithms emerged based on traditional and deep learning methods. In this\nstudy, we first introduce two different tree crown detection methods based on\nthese approaches. Then, we form a novel rule-based approach that integrates\nthese two methods to enhance robustness and accuracy of tree crown detection\nresults. While traditional methods are employed for feature extraction and\nsegmentation of forested areas, deep learning methods are used to detect tree\ncrowns in our method. With the proposed rule-based approach, we post-process\nthese results, aiming to increase the number of detected tree crowns through\nneighboring trees and localized operations. We compare the obtained results\nwith the proposed method in terms of the number of detected tree crowns and\nreport the advantages, disadvantages, and areas for improvement of the obtained\noutcomes.", "AI": {"tldr": "The paper introduces a rule-based approach combining traditional and deep learning methods to improve tree crown detection, aiming to aid in forest monitoring.", "motivation": "To address global environmental issues by improving forest monitoring using automated tree crown detection methods.", "method": "A novel rule-based approach integrating traditional feature extraction/segmentation and deep learning detection, followed by post-processing to refine the results.", "result": "Enhanced accuracy and robustness in detecting tree crowns compared to individual methods, with a discussion on advantages and limitations.", "conclusion": "Combining methods improves tree crown detection, but there are opportunities for further refinement and enhancement."}}
{"id": "2507.01178", "pdf": "https://arxiv.org/pdf/2507.01178", "abs": "https://arxiv.org/abs/2507.01178", "authors": ["Alec Helbling", "Duen Horng Chau"], "title": "Diffusion Explorer: Interactive Exploration of Diffusion Models", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion models have been central to the development of recent image, video,\nand even text generation systems. They posses striking geometric properties\nthat can be faithfully portrayed in low-dimensional settings. However, existing\nresources for explaining diffusion either require an advanced theoretical\nfoundation or focus on their neural network architectures rather than their\nrich geometric properties. We introduce Diffusion Explorer, an interactive tool\nto explain the geometric properties of diffusion models. Users can train 2D\ndiffusion models in the browser and observe the temporal dynamics of their\nsampling process. Diffusion Explorer leverages interactive animation, which has\nbeen shown to be a powerful tool for making engaging visualizations of dynamic\nsystems, making it well suited to explaining diffusion models which represent\nstochastic processes that evolve over time. Diffusion Explorer is open source\nand a live demo is available at alechelbling.com/Diffusion-Explorer.", "AI": {"tldr": "The paper introduces Diffusion Explorer, a tool for visualizing and understanding geometric properties of diffusion models in low dimensions.", "motivation": "To simplify the explanation of diffusion models' geometric properties using accessible tools and animations, addressing limitations of current theoretical or architecture-focused resources.", "method": "The authors developed Diffusion Explorer, an interactive browser-based tool that lets users train 2D diffusion models and animate their sampling processes.", "result": "Diffusion Explorer is available as open-source software and offers a live demo for interactive learning.", "conclusion": "Interactive tools like Diffusion Explorer enhance understanding of dynamic systems, particularly diffusion models, by leveraging animation and engaging visualizations."}}
{"id": "2507.01567", "pdf": "https://arxiv.org/pdf/2507.01567", "abs": "https://arxiv.org/abs/2507.01567", "authors": ["Patrick Benito Eberhard", "Johannes K\u00f6hler", "Oliver H\u00fcsser", "Melanie N. Zeilinger", "Andrea Carron"], "title": "Time-Varying Coverage Control: A Distributed Tracker-Planner MPC Framework", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "Time-varying coverage control addresses the challenge of coordinating\nmultiple agents covering an environment where regions of interest change over\ntime. This problem has broad applications, including the deployment of\nautonomous taxis and coordination in search and rescue operations. The\nachievement of effective coverage is complicated by the presence of\ntime-varying density functions, nonlinear agent dynamics, and stringent system\nand safety constraints. In this paper, we present a distributed multi-agent\ncontrol framework for time-varying coverage under nonlinear constrained\ndynamics. Our approach integrates a reference trajectory planner and a tracking\nmodel predictive control (MPC) scheme, which operate at different frequencies\nwithin a multi-rate framework. For periodic density functions, we demonstrate\nclosed-loop convergence to an optimal configuration of trajectories and provide\nformal guarantees regarding constraint satisfaction, collision avoidance, and\nrecursive feasibility. Additionally, we propose an efficient algorithm capable\nof handling nonperiodic density functions, making the approach suitable for\npractical applications. Finally, we validate our method through hardware\nexperiments using a fleet of four miniature race cars.", "AI": {"tldr": "The paper introduces a distributed framework for coordinating multiple agents in dynamic environments, ensuring effective coverage and safety compliance.", "motivation": "Address the challenges in dynamic coverage, including changing regions of interest, nonlinear dynamics, and safety constraints, with applications like autonomous taxis and rescue operations.", "method": "Develop a multi-rate framework combining trajectory planning and model predictive control, ensuring coverage under constraints; supports periodic and nonperiodic density functions.", "result": "Prove closed-loop convergence for periodic density functions, offer practical algorithm for nonperiodic functions, and validate the method with experiments on miniature race cars.", "conclusion": "The approach offers a robust and flexible solution for dynamic coverage challenges, suitable for real-world applications."}}
{"id": "2507.01063", "pdf": "https://arxiv.org/pdf/2507.01063", "abs": "https://arxiv.org/abs/2507.01063", "authors": ["Madhav Kotecha"], "title": "FAIR-MATCH: A Multi-Objective Framework for Bias Mitigation in Reciprocal Dating Recommendations", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Online dating platforms have fundamentally transformed the formation of\nromantic relationships, with millions of users worldwide relying on algorithmic\nmatching systems to find compatible partners. However, current recommendation\nsystems in dating applications suffer from significant algorithmic\ndeficiencies, including but not limited to popularity bias, filter bubble\neffects, and inadequate reciprocity modeling that limit effectiveness and\nintroduce harmful biases. This research integrates foundational work with\nrecent empirical findings to deliver a detailed analysis of dating app\nrecommendation systems, highlighting key issues and suggesting research-backed\nsolutions. Through analysis of reciprocal recommendation frameworks, fairness\nevaluation metrics, and industry implementations, we demonstrate that current\nsystems achieve modest performance with collaborative filtering reaching 25.1\\%\nwhile reciprocal methods achieve 28.7\\%. Our proposed mathematical framework\naddresses these limitations through enhanced similarity measures,\nmulti-objective optimization, and fairness-aware algorithms that maintain\ncompetitive accuracy while improving demographic representation to reduce\nalgorithmic bias.", "AI": {"tldr": "The study critiques current online dating algorithms for biases and proposes a superior framework with improved performance.", "motivation": "Current dating algorithms exhibit biases like popularity effects, filter bubbles, and insufficient reciprocity modeling, limiting fairness and effectiveness.", "method": "The paper integrates analysis of existing algorithms (reciprocal recommendation frameworks, fairness metrics) and proposes a new mathematical framework utilizing enhanced similarity measures, multi-objective optimization, and fairness-aware algorithms.", "result": "Current systems achieve modest performance (collaborative filtering: 25.1%, reciprocal methods: 28.7%). The proposed framework delivers competitive accuracy with better demographic fairness.", "conclusion": "The proposed system demonstrates the potential to mitigate biases, improve fairness, and enhance the effectiveness of dating recommendation systems."}}
{"id": "2507.01903", "pdf": "https://arxiv.org/pdf/2507.01903", "abs": "https://arxiv.org/abs/2507.01903", "authors": ["Qiguang Chen", "Mingda Yang", "Libo Qin", "Jinhao Liu", "Zheng Yan", "Jiannan Guan", "Dengyun Peng", "Yiyan Ji", "Hanjing Li", "Mengkang Hu", "Yimeng Zhang", "Yihao Liang", "Yuhang Zhou", "Jiaqi Wang", "Zhi Chen", "Wanxiang Che"], "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.", "AI": {"tldr": "The paper surveys advancements in AI technologies for scientific research, proposing a taxonomy, identifying gaps, and providing resources for enhanced development in AI-powered research.", "motivation": "AI and large language models show promising capabilities in scientific research, but a comprehensive survey on their innovative applications is lacking.", "method": "The paper conducts a systematic survey presenting a taxonomy to classify AI4Research tasks, identifies research gaps, and compiles applications and resources to guide future work.", "result": "The survey establishes a taxonomy of five tasks in AI4Research, identifies research directions like scalability and societal impact, and provides multidisciplinary resources and tools.", "conclusion": "This study aims to aid the research community by bridging gaps in AI4Research and stimulating innovation by offering an organized framework and valuable resources."}}
{"id": "2507.01504", "pdf": "https://arxiv.org/pdf/2507.01504", "abs": "https://arxiv.org/abs/2507.01504", "authors": ["Robert Aufschl\u00e4ger", "Youssef Shoeb", "Azarm Nowzad", "Michael Heigl", "Fabian Bally", "Martin Schramm"], "title": "Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "accepted for publication at the 2025 IEEE 28th International\n  Conference on Intelligent Transportation Systems (ITSC 2025), taking place\n  during November 18-21, 2025 in Gold Coast, Australia", "summary": "The collection and release of street-level recordings as Open Data play a\nvital role in advancing autonomous driving systems and AI research. However,\nthese datasets pose significant privacy risks, particularly for pedestrians,\ndue to the presence of Personally Identifiable Information (PII) that extends\nbeyond biometric traits such as faces. In this paper, we present cRID, a novel\ncross-modal framework combining Large Vision-Language Models, Graph Attention\nNetworks, and representation learning to detect textual describable clues of\nPII and enhance person re-identification (Re-ID). Our approach focuses on\nidentifying and leveraging interpretable features, enabling the detection of\nsemantically meaningful PII beyond low-level appearance cues. We conduct a\nsystematic evaluation of PII presence in person image datasets. Our experiments\nshow improved performance in practical cross-dataset Re-ID scenarios, notably\nfrom Market-1501 to CUHK03-np (detected), highlighting the framework's\npractical utility. Code is available at https://github.com/RAufschlaeger/cRID.", "AI": {"tldr": "The paper introduces cRID, a framework combining AI techniques to improve detection of privacy-sensitive personal identifiers in image datasets.", "motivation": "To address privacy risks in street-level open data containing Personally Identifiable Information (PII).", "method": "Developed a cross-modal framework using Large Vision-Language Models, Graph Attention Networks, and representation learning to focus on semantically meaningful PII detection.", "result": "Improved person re-identification (Re-ID) performance across datasets, particularly in cross-dataset scenarios like Market-1501 to CUHK03-np.", "conclusion": "The approach provides practical tools for enhancing privacy-aware autonomous systems while advancing person Re-ID technology."}}
{"id": "2507.01196", "pdf": "https://arxiv.org/pdf/2507.01196", "abs": "https://arxiv.org/abs/2507.01196", "authors": ["Na Lee", "Konstantinos Barmpas", "Yannis Panagakis", "Dimitrios Adamos", "Nikolaos Laskaris", "Stefanos Zafeiriou"], "title": "Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning", "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": null, "summary": "Foundation Models have demonstrated significant success across various\ndomains in Artificial Intelligence (AI), yet their capabilities for brainwave\nmodeling remain unclear. In this paper, we comprehensively evaluate current\nLarge Brainwave Foundation Models (LBMs) through systematic fine-tuning\nexperiments across multiple Brain-Computer Interface (BCI) benchmark tasks,\nincluding memory tasks and sleep stage classification. Our extensive analysis\nshows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)\nover traditional deep architectures while requiring significantly more\nparameters (millions vs thousands), raising important questions about their\nefficiency and applicability in BCI contexts. Moreover, through detailed\nablation studies and Low-Rank Adaptation (LoRA), we significantly reduce\ntrainable parameters without performance degradation, while demonstrating that\narchitectural and training inefficiencies limit LBMs' current capabilities. Our\nexperiments span both full model fine-tuning and parameter-efficient adaptation\ntechniques, providing insights into optimal training strategies for BCI\napplications. We pioneer the application of LoRA to LBMs, revealing that\nperformance benefits generally emerge when adapting multiple neural network\ncomponents simultaneously. These findings highlight the critical need for\ndomain-specific development strategies to advance LBMs, suggesting that current\narchitectures may require redesign to fully leverage the potential of\nfoundation models in brainwave analysis.", "AI": {"tldr": "Large Brainwave Foundation Models (LBMs) achieve marginal improvements over traditional methods in Brain-Computer Interface (BCI) tasks despite requiring higher computational resources. LoRA reduces the overhead without performance loss.", "motivation": "To evaluate the efficiency and applicability of Large Brainwave Foundation Models in brainwave modeling and identify potential limitations and improvements.", "method": "Systematic fine-tuning experiments across multiple BCI benchmark tasks, including memory tasks and sleep classification, supported by ablation studies and Low-Rank Adaptation (LoRA).", "result": "State-of-the-art LBMs show slight performance improvement (0.9%-1.2%) over traditional deep architectures but require significantly larger computational resources. LoRA effectively reduces trainable parameters while minimizing performance degradation.", "conclusion": "Current LBMs exhibit architectural and training inefficiencies despite marginal performance gains. Redesign and domain-specific development strategies are crucial for leveraging foundation models effectively in BCI applications."}}
{"id": "2507.01667", "pdf": "https://arxiv.org/pdf/2507.01667", "abs": "https://arxiv.org/abs/2507.01667", "authors": ["Gianluca Monaci", "Philippe Weinzaepfel", "Christian Wolf"], "title": "What does really matter in image goal navigation?", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Image goal navigation requires two different skills: firstly, core navigation\nskills, including the detection of free space and obstacles, and taking\ndecisions based on an internal representation; and secondly, computing\ndirectional information by comparing visual observations to the goal image.\nCurrent state-of-the-art methods either rely on dedicated image-matching, or\npre-training of computer vision modules on relative pose estimation. In this\npaper, we study whether this task can be efficiently solved with end-to-end\ntraining of full agents with RL, as has been claimed by recent work. A positive\nanswer would have impact beyond Embodied AI and allow training of relative pose\nestimation from reward for navigation alone. In a large study we investigate\nthe effect of architectural choices like late fusion, channel stacking,\nspace-to-depth projections and cross-attention, and their role in the emergence\nof relative pose estimators from navigation training. We show that the success\nof recent methods is influenced up to a certain extent by simulator settings,\nleading to shortcuts in simulation. However, we also show that these\ncapabilities can be transferred to more realistic setting, up to some extend.\nWe also find evidence for correlations between navigation performance and\nprobed (emerging) relative pose estimation performance, an important sub skill.", "AI": {"tldr": "The paper explores whether image goal navigation can be efficiently solved with end-to-end reinforcement learning (RL), comparing architectural choices and their impact on relative pose estimators.", "motivation": "To determine if end-to-end reinforcement learning is sufficient for image goal navigation, eliminating the need for dedicated image-matching or pre-trained computer vision modules.", "method": "The study evaluated architectural choices (e.g., late fusion, channel stacking, space-to-depth projections, cross-attention) alongside reinforcement learning in simulated and more realistic settings.", "result": "Findings indicate that simulator settings can enable shortcuts, but some learned capabilities transfer to realistic settings. There is also evidence of correlations between navigation and relative pose estimation performance.", "conclusion": "End-to-end RL shows potential for emerging relative pose estimators in navigation tasks, though performance is influenced by simulation settings and transferable capabilities are limited."}}
{"id": "2507.01915", "pdf": "https://arxiv.org/pdf/2507.01915", "abs": "https://arxiv.org/abs/2507.01915", "authors": ["Chengao Li", "Hanyu Zhang", "Yunkun Xu", "Hongyan Xue", "Xiang Ao", "Qing He"], "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 3 figures. Accepted by ACL 2025 (main)", "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness.", "AI": {"tldr": "This paper introduces GAPO, a multi-gradient optimization approach for aligning large language models (LLMs) with diverse human preferences, overcoming conflicts in objectives.", "motivation": "Aligning large language models (LLMs) with conflicting human preferences is a critical challenge in reinforcement learning from human feedback (RLHF).", "method": "The paper proposes Gradient-Adaptive Policy Optimization (GAPO), which uses multiple-gradient descent and adaptive gradient rescaling to balance trade-offs for conflicting objectives. Additionally, P-GAPO incorporates user-specific preferences to achieve tailored Pareto solutions.", "result": "GAPO achieves superior performance in aligning LLMs with human values, surpassing existing state-of-the-art methods on Mistral-7B and showing improved helpfulness and harmlessness metrics.", "conclusion": "GAPO provides an effective framework for balancing diverse human preferences in LLM alignment, demonstrating theoretical convergence to Pareto optimal solutions and empirical superiority over existing methods."}}
{"id": "2507.01509", "pdf": "https://arxiv.org/pdf/2507.01509", "abs": "https://arxiv.org/abs/2507.01509", "authors": ["Tapas K. Dutta", "Snehashis Majhi", "Deepak Ranjan Nayak", "Debesh Jha"], "title": "Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "11 pages, 2 figures, MICCAI-2025", "summary": "Polyp segmentation in colonoscopy images is crucial for early detection and\ndiagnosis of colorectal cancer. However, this task remains a significant\nchallenge due to the substantial variations in polyp shape, size, and color, as\nwell as the high similarity between polyps and surrounding tissues, often\ncompounded by indistinct boundaries. While existing encoder-decoder CNN and\ntransformer-based approaches have shown promising results, they struggle with\nstable segmentation performance on polyps with weak or blurry boundaries. These\nmethods exhibit limited abilities to distinguish between polyps and non-polyps\nand capture essential boundary cues. Moreover, their generalizability still\nfalls short of meeting the demands of real-time clinical applications. To\naddress these limitations, we propose SAM-MaGuP, a groundbreaking approach for\nrobust polyp segmentation. By incorporating a boundary distillation module and\na 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels\nat resolving weak boundary challenges and amplifies feature learning through\nenriched global contextual interactions. Extensive evaluations across five\ndiverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,\nachieving unmatched segmentation accuracy and robustness. Our key innovations,\na Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in\nthe field, pushing the boundaries of polyp segmentation to new heights.", "AI": {"tldr": "The paper introduces SAM-MaGuP, a novel model for polyp segmentation in colonoscopy images, overcoming challenges such as weak boundaries and similarity with surrounding tissues, achieving superior results compared to existing methods.", "motivation": "The paper addresses the challenges of polyp segmentation in colonoscopy images, particularly weak/boundary issues, high similarity between polyps and surrounding tissue, and the limitations of existing models in real-world clinical applications.", "method": "The proposed method, SAM-MaGuP, integrates a boundary distillation module and a 1D-2D Mamba adapter into the Segment Anything Model (SAM) to improve feature learning and address weak boundary problems.", "result": "Extensive testing on five datasets demonstrated that SAM-MaGuP outperforms current state-of-the-art methods in segmentation accuracy and robustness.", "conclusion": "SAM-MaGuP represents a significant advancement in polyp segmentation, offering robust solutions to weak boundary challenges and setting a new performance benchmark for real-world clinical applications."}}
{"id": "2507.01201", "pdf": "https://arxiv.org/pdf/2507.01201", "abs": "https://arxiv.org/abs/2507.01201", "authors": ["Hyoseo", "Yoon", "Yisong Yue", "Been Kim"], "title": "Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Independently trained vision and language models inhabit disjoint\nrepresentational spaces, shaped by their respective modalities, objectives, and\narchitectures. Yet an emerging hypothesis - the Platonic Representation\nHypothesis - suggests that such models may nonetheless converge toward a shared\nstatistical model of reality. This compatibility, if it exists, raises a\nfundamental question: can we move beyond post-hoc statistical detection of\nalignment and explicitly optimize for it between such disjoint representations?\nWe cast this Platonic alignment problem as a multi-objective optimization task\n- preserve each modality's native structure while aligning for mutual\ncoherence. We introduce the Joint Autoencoder Modulator (JAM) framework that\njointly trains modality-specific autoencoders on the latent representations of\npre-trained single modality models, encouraging alignment through both\nreconstruction and cross-modal objectives. By analogy, this framework serves as\na method to escape Plato's Cave, enabling the emergence of shared structure\nfrom disjoint inputs. We evaluate this framework across three critical design\naxes: (i) the alignment objective - comparing contrastive loss (Con), its\nhard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at\nwhich alignment is most effective, and (iii) the impact of foundation model\nscale on representational convergence. Our findings show that our lightweight\nPareto-efficient framework reliably induces alignment, even across frozen,\nindependently trained representations, offering both theoretical insight and\npractical pathways for transforming generalist unimodal foundations into\nspecialist multimodal models.", "AI": {"tldr": "The study investigates whether independently trained vision and language models can align their representations and proposes the Joint Autoencoder Modulator (JAM) framework to achieve this.", "motivation": "To explore whether independently trained vision and language models, with distinct modalities and objectives, can converge towards a shared representational space and how this alignment can be explicitly optimized.", "method": "A new framework, Joint Autoencoder Modulator (JAM), is introduced that encourages alignment via reconstruction and cross-modal objectives, implemented as a multi-objective optimization process.", "result": "The proposed JAM framework successfully induces alignment between frozen, independently trained vision and language models, demonstrating reliability across various design factors like alignment objectives, layer depths, and model scales.", "conclusion": "The JAM framework provides theoretical insights and practical techniques for transforming unimodal models into multimodal ones, supporting alignment even in disjointly trained representations."}}
{"id": "2507.01823", "pdf": "https://arxiv.org/pdf/2507.01823", "abs": "https://arxiv.org/abs/2507.01823", "authors": ["Dmytro Kuzmenko", "Nadiya Shvai"], "title": "TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents", "categories": ["cs.LG", "cs.RO"], "comment": "Preprint of a manuscript submitted for peer review", "summary": "We present a novel approach to knowledge transfer in model-based\nreinforcement learning, addressing the critical challenge of deploying large\nworld models in resource-constrained environments. Our method efficiently\ndistills a high-capacity multi-task agent (317M parameters) into a compact\nmodel (1M parameters) on the MT30 benchmark, significantly improving\nperformance across diverse tasks. Our distilled model achieves a\nstate-of-the-art normalized score of 28.45, surpassing the original 1M\nparameter model score of 18.93. This improvement demonstrates the ability of\nour distillation technique to capture and consolidate complex multi-task\nknowledge. We further optimize the distilled model through FP16 post-training\nquantization, reducing its size by $\\sim$50\\%. Our approach addresses practical\ndeployment limitations and offers insights into knowledge representation in\nlarge world models, paving the way for more efficient and accessible multi-task\nreinforcement learning systems in robotics and other resource-constrained\napplications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.", "AI": {"tldr": "The paper introduces a method to condense a high-capacity multi-task reinforcement learning model into a compact, efficient version for resource-limited deployments.", "motivation": "To address the difficulty of deploying large reinforcement learning world models in environments with limited computational and storage resources.", "method": "Develops a distillation technique to transfer knowledge from a large 317M parameter model into a smaller 1M parameter model, followed by FP16 quantization reducing model size by ~50%.", "result": "Achieved state-of-the-art normalized score of 28.45 on MT30 benchmark, significantly higher than the previous 1M parameter model's score of 18.93.", "conclusion": "The proposed approach simplifies deployment of multi-task models in resource-constrained environments while enhancing performance, offering advances for reinforcement learning in robotics and other applications."}}
{"id": "2507.01076", "pdf": "https://arxiv.org/pdf/2507.01076", "abs": "https://arxiv.org/abs/2507.01076", "authors": ["Vanja Stojanovi\u0107", "Bor Panger\u0161i\u010d"], "title": "Empirical Analysis Of Heuristic and Approximation Algorithms for the The Mutual-Visibility Problem", "categories": ["cs.CG", "cs.AI", "cs.PF", "math.CO"], "comment": null, "summary": "The NP-complete mutual-visibility (MV) problem currently lacks empirical\nanalysis on its practical behaviour despite theoretical studies. This paper\naddresses this gap by implementing and evaluating three distinct algorithms - a\ndirect greedy heuristic, a hypergraph-based approximation, and a genetic\nalgorithm - on diverse synthetic graph datasets, including those with\nanalytically known $\\mu(G)$ values and general graph models. Our results\ndemonstrate that for smaller graphs, the algorithms consistently achieve MV set\nsizes aligning with theoretical bounds. However, for larger instances, achieved\nsolution sizes notably diverge from theoretical limits; this, combined with the\nabsence of tight bounds, complicates absolute quality assessment. Nevertheless,\nvalidation on known optimal graphs showed the Genetic Algorithm and other\nheuristics empirically performing best among tested methods.", "AI": {"tldr": "This paper empirically evaluates three algorithms for the NP-complete mutual-visibility (MV) problem, showing varying performance across different graph sizes.", "motivation": "There is a lack of practical, empirical studies on the behavior of solutions to the NP-complete mutual-visibility (MV) problem.", "method": "The study implements and evaluates three algorithms: a direct greedy heuristic, a hypergraph-based approximation, and a genetic algorithm, using diverse synthetic graph datasets with known and general properties.", "result": "The algorithms achieved theoretical MV set sizes for smaller graphs but diverged for larger graphs. The genetic algorithm performed best among tested methods based on validation on optimal graphs.", "conclusion": "While practical algorithms deliver promising results, especially for smaller graphs, the absence of tight bounds limits absolute quality determination for larger graphs."}}
{"id": "2507.01921", "pdf": "https://arxiv.org/pdf/2507.01921", "abs": "https://arxiv.org/abs/2507.01921", "authors": ["Yang Li", "Youssef Emad", "Karthik Padthe", "Jack Lanchantin", "Weizhe Yuan", "Thao Nguyen", "Jason Weston", "Shang-Wen Li", "Dong Wang", "Ilia Kulikov", "Xian Li"], "title": "NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Recent work has shown that distilling reasoning traces from a larger teacher\nmodel via supervised finetuning outperforms reinforcement learning with the\nsmaller student model alone (Guo et al. 2025). However, there has not been a\nsystematic study of what kind of reasoning demonstrations from the teacher are\nmost effective in improving the student model's reasoning capabilities. In this\nwork we curate high-quality \"NaturalThoughts\" by selecting reasoning traces\nfrom a strong teacher model based on a large pool of questions from\nNaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of\nfactors that affect distilling reasoning capabilities, in terms of sample\nefficiency and scalability for general reasoning tasks. We observe that simply\nscaling up data size with random sampling is a strong baseline with steady\nperformance gains. Further, we find that selecting difficult examples that\nrequire more diverse reasoning strategies is more sample-efficient to transfer\nthe teacher model's reasoning skills. Evaluated on both Llama and Qwen models,\ntraining with NaturalThoughts outperforms existing reasoning datasets such as\nOpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including\nGPQA-Diamond, MMLU-Pro and SuperGPQA.", "AI": {"tldr": "The study explores effective methods for distilling reasoning capabilities from a teacher model to a student model, finding that targeted selection of reasoning examples enhances performance.", "motivation": "To improve smaller student models' reasoning abilities by examining the optimal type of reasoning demonstrations from a stronger teacher model.", "method": "High-quality reasoning traces, termed 'NaturalThoughts,' were curated from a teacher model using selective sampling based on difficult and diverse reasoning examples, then evaluated on reasoning benchmarks.", "result": "Training with curated NaturalThoughts demonstrated improved performance over existing datasets on STEM reasoning benchmarks, for both Llama and Qwen models.", "conclusion": "Selective sampling of challenging, diverse reasoning traces is critical for efficiently transferring reasoning capabilities to student models, outperforming random sampling and existing datasets."}}
{"id": "2507.01532", "pdf": "https://arxiv.org/pdf/2507.01532", "abs": "https://arxiv.org/abs/2507.01532", "authors": ["Tomas Zelezny", "Jakub Straka", "Vaclav Javorek", "Ondrej Valach", "Marek Hruz", "Ivan Gruber"], "title": "Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights", "categories": ["cs.CV"], "comment": "8 pages, 9 figures, supplementary, SLRTP2025, CVPR2025", "summary": "Sign Language Translation (SLT) has evolved significantly, moving from\nisolated recognition approaches to complex, continuous gloss-free translation\nsystems. This paper explores the impact of pose-based data preprocessing\ntechniques - normalization, interpolation, and augmentation - on SLT\nperformance. We employ a transformer-based architecture, adapting a modified T5\nencoder-decoder model to process pose representations. Through extensive\nablation studies on YouTubeASL and How2Sign datasets, we analyze how different\npreprocessing strategies affect translation accuracy. Our results demonstrate\nthat appropriate normalization, interpolation, and augmentation techniques can\nsignificantly improve model robustness and generalization abilities.\nAdditionally, we provide a deep analysis of the model's attentions and reveal\ninteresting behavior suggesting that adding a dedicated register token can\nimprove overall model performance. We publish our code on our GitHub\nrepository, including the preprocessed YouTubeASL data.", "AI": {"tldr": "This paper investigates how preprocessing techniques like normalization, interpolation, and augmentation affect the performance of pose-based Sign Language Translation systems using a transformer-based model.", "motivation": "The paper aims to improve Sign Language Translation systems by exploring the impact of preprocessing techniques and adapting advanced architectures for better translation accuracy.", "method": "The researchers applied a modified T5 transformer-based encoder-decoder model to pose representations and conducted ablation studies on YouTubeASL and How2Sign datasets, analyzing the effects of various preprocessing strategies.", "result": "The study shows that preprocessing techniques (normalization, interpolation, and augmentation) significantly enhance model robustness, generalization, and overall translation accuracy.", "conclusion": "Appropriate preprocessing strategies are critical for improving SLT systems. Additionally, modifying the model with register tokens further enhances performance. The published code and preprocessed data are accessible for the community."}}
{"id": "2507.01208", "pdf": "https://arxiv.org/pdf/2507.01208", "abs": "https://arxiv.org/abs/2507.01208", "authors": ["Pedro R. X. Carmo", "Igor de Moura", "Assis T. de Oliveira Filho", "Djamel Sadok", "Cleber Zanchettin"], "title": "Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform", "categories": ["cs.LG", "cs.CR", "C.2.0; I.2.0"], "comment": null, "summary": "Modern vehicles are increasingly connected, and in this context, automotive\nEthernet is one of the technologies that promise to provide the necessary\ninfrastructure for intra-vehicle communication. However, these systems are\nsubject to attacks that can compromise safety, including flow injection\nattacks. Deep Learning-based Intrusion Detection Systems (IDS) are often\ndesigned to combat this problem, but they require expensive hardware to run in\nreal time. In this work, we propose to evaluate and apply fast neural network\ninference techniques like Distilling and Prunning for deploying IDS models on\nlow-cost platforms in real time. The results show that these techniques can\nachieve intrusion detection times of up to 727 {\\mu}s using a Raspberry Pi 4,\nwith AUCROC values of 0.9890.", "AI": {"tldr": "The paper evaluates techniques like distilling and pruning for fast neural network inference to deploy Intrusion Detection Systems (IDS) on low-cost platforms like Raspberry Pi.", "motivation": "Modern vehicles rely on automotive Ethernet for communication, but face safety risks due to flow injection attacks, necessitating cost-effective IDS solutions.", "method": "Fast neural network inference methods, including distilling and pruning, are applied and tested for real-time IDS deployment on low-cost hardware.", "result": "Achieved intrusion detection times up to 727 \u03bcs on Raspberry Pi 4 and AUCROC values of 0.9890.", "conclusion": "The proposed neural network inference techniques are effective for real-time IDS on low-cost platforms, addressing safety risks in connected vehicles."}}
{"id": "2507.01081", "pdf": "https://arxiv.org/pdf/2507.01081", "abs": "https://arxiv.org/abs/2507.01081", "authors": ["Megan T. deBettencourt", "Sruthi Sakthivel", "Emily A. Holmes", "Mark Chevillet"], "title": "AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Trauma prevalence is vast globally. Evidence-based digital treatments can\nhelp, but most require human guidance. Human guides provide tailored\ninstructions and responsiveness to internal cognitive states, but limit\nscalability. Can generative AI and neurotechnology provide a scalable\nalternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to\nautomatically deliver and monitor an evidence-based digital treatment,\nspecifically the Imagery Competing Task Intervention (ICTI), to reduce\nintrusive memories after psychological trauma. One hundred healthy volunteers\nwere exposed to videos of traumatic events and randomly assigned to an\nintervention or active control condition. As predicted, intervention\nparticipants reported significantly fewer intrusive memories over the following\nweek. Post-hoc assessment against clinical rubrics confirmed the AI guide\ndelivered the intervention successfully. Additionally, pupil size tracked\nintervention engagement and predicted symptom reduction, providing a candidate\nbiomarker of intervention effectiveness. These findings open a path toward\nrigorous AI-guided digital interventions that can scale to trauma prevalence.", "AI": {"tldr": "This paper tests ANTIDOTE, a scalable AI and pupillometry-based digital treatment for trauma, showing it reduces intrusive memories effectively.", "motivation": "The paper aims to address the global issue of trauma prevalence by exploring scalable, AI-guided digital treatments that eliminate the need for human guidance.", "method": "The study tested the AI-guided digital treatment ANTIDOTE on 100 participants exposed to traumatic videos. It combined Imagery Competing Task Intervention (ICTI) with pupillometry for intervention delivery and engagement tracking.", "result": "Participants receiving the intervention reported significantly fewer intrusive memories compared to a control group. Pupil size also indicated engagement and predicted symptom reduction.", "conclusion": "The study demonstrates the potential of AI-guided digital interventions like ANTIDOTE to address global trauma issues by scaling evidence-based treatments effectively."}}
{"id": "2507.01923", "pdf": "https://arxiv.org/pdf/2507.01923", "abs": "https://arxiv.org/abs/2507.01923", "authors": ["Yu-Shiang Huang", "Chuan-Ju Wang", "Chung-Chi Chen"], "title": "Decision-oriented Text Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics.", "AI": {"tldr": "This paper evaluates natural language generation (NLG) using its impact on decision-making by humans and LLMs, focusing on financial market contexts.", "motivation": "The paper seeks to address the weak correlation between intrinsic NLG evaluation metrics and real-world decision-making efficacy. Current metrics like n-gram overlap do not capture how generated text influences practical decision outcomes.", "method": "A decision-oriented evaluation framework was proposed, testing financial market digest texts on humans and LLMs. Performance was gauged based on the financial outcomes of trades informed exclusively by the tested texts.", "result": "The study found that neither humans nor LLMs achieved better-than-random performance when using summaries alone. Team-based collaboration with richer commentaries, however, significantly improved performance over individual baselines.", "conclusion": "The results highlight the need for evaluating generated text based on its ability to enhance collaborative human-LLM decision-making, contrasting with the inadequacy of traditional metrics."}}
{"id": "2507.01535", "pdf": "https://arxiv.org/pdf/2507.01535", "abs": "https://arxiv.org/abs/2507.01535", "authors": ["Bingxi Liu", "Calvin Chen", "Junhao Li", "Guyang Yu", "Haoqian Song", "Xuchen Liu", "Jinqiang Cui", "Hong Zhang"], "title": "TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking", "categories": ["cs.CV"], "comment": "12 pages", "summary": "The Vision Transformer (ViT) model has long struggled with the challenge of\nquadratic complexity, a limitation that becomes especially critical in unmanned\naerial vehicle (UAV) tracking systems, where data must be processed in real\ntime. In this study, we explore the recently proposed State-Space Model, Mamba,\nleveraging its computational efficiency and capability for long-sequence\nmodeling to effectively process dense image sequences in tracking tasks. First,\nwe highlight the issue of temporal inconsistency in existing Mamba-based\nmethods, specifically the failure to account for temporal continuity in the\nMamba scanning mechanism. Secondly, building upon this insight,we propose\nTrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model\nfor handling image sequence of tracking problem. In our framework, the mamba\nscan is performed in a nested way while independently process temporal and\nspatial coherent patch tokens. While the template frame is encoded as query\ntoken and utilized for tracking in every scan. Extensive experiments conducted\non five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves\nstate-of-the-art precision while offering noticeable higher speed in UAV\ntracking.", "AI": {"tldr": "The paper proposes TrackingMiM, a minimal-computation model using a Mamba-in-Mamba architecture for UAV tracking, addressing temporal inconsistency issues in existing methods and achieving state-of-the-art precision at higher speeds.", "motivation": "The study aims to overcome the quadratic complexity challenges of Vision Transformer (ViT) models in real-time UAV tracking by leveraging the computational efficiency of the State-Space Model, Mamba.", "method": "The proposed method, TrackingMiM, employs a Mamba-in-Mamba architecture, performing nested Mamba scans to independently process temporal and spatial coherent patch tokens while encoding the template frame as a query token.", "result": "Experiments on five UAV tracking benchmarks demonstrate that TrackingMiM achieves state-of-the-art precision with noticeably higher tracking speed.", "conclusion": "TrackingMiM effectively addresses temporal inconsistency challenges in Mamba-based methods, offering an efficient and high-precision solution for real-time UAV tracking."}}
{"id": "2507.01216", "pdf": "https://arxiv.org/pdf/2507.01216", "abs": "https://arxiv.org/abs/2507.01216", "authors": ["Xingke Yang", "Liang Li", "Zhiyi Wan", "Sicong Li", "Hao Wang", "Xiaoqi Qi", "Jiang Liu", "Tomoaki Ohtsuki", "Xin Fu", "Miao Pan"], "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.", "AI": {"tldr": "The paper introduces PAE MobiLLM, a privacy-aware and efficient method for on-device large language model fine-tuning using server-assisted additive side-tuning to address resource limitations of mobile devices.", "motivation": "Current methods for on-device LLM fine-tuning face challenges like communication burdens and privacy concerns, especially when using server assistance.", "method": "PAE MobiLLM incorporates server-side activation caching, a one-token activation shortcut for communication efficiency, and an additive adapter side-network design for privacy.", "result": "PAE MobiLLM reduces communication costs, improves convergence speed, and ensures privacy while achieving efficient fine-tuning of LLMs on mobile devices.", "conclusion": "PAE MobiLLM effectively balances privacy, efficiency, and resource optimization, making it suitable for on-device LLM fine-tuning with server assistance."}}
{"id": "2507.01931", "pdf": "https://arxiv.org/pdf/2507.01931", "abs": "https://arxiv.org/abs/2507.01931", "authors": ["Md Sazzadul Islam Ridoy", "Sumi Akter", "Md. Aminur Rahman"], "title": "Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "In recent years, neural models trained on large multilingual text and speech\ndatasets have shown great potential for supporting low-resource languages. This\nstudy investigates the performances of two state-of-the-art Automatic Speech\nRecognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's\nWav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments\nusing two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to\nevaluate model performances. Through systematic fine-tuning and hyperparameter\noptimization, including learning rate, epochs, and model checkpoint selection,\nwe have compared the models based on Word Error Rate (WER), Character Error\nRate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model\noutperformed Whisper across all key evaluation metrics, demonstrated superior\nperformance while requiring fewer computational resources, and offered valuable\ninsights to develop robust speech recognition systems in low-resource\nlinguistic settings.", "AI": {"tldr": "This study compares two ASR models, Whisper and Wav2Vec-BERT, for Bangla, finding Wav2Vec-BERT performs better.", "motivation": "The paper aims to address challenges in automatic speech recognition for low-resource languages, using Bangla as a case study.", "method": "Two public datasets and advanced fine-tuning with hyperparameter optimization were used to evaluate the models based on WER, CER, training time, and computational efficiency.", "result": "Wav2Vec-BERT performed better than Whisper across all evaluation metrics, requiring fewer computational resources.", "conclusion": "The study provides insights into developing efficient speech recognition systems for low-resource languages, highlighting the advantages of Wav2Vec-BERT."}}
{"id": "2507.01539", "pdf": "https://arxiv.org/pdf/2507.01539", "abs": "https://arxiv.org/abs/2507.01539", "authors": ["Mohammadreza Amirian", "Michael Bach", "Oscar Jimenez-del-Toro", "Christoph Aberle", "Roger Schaer", "Vincent Andrearczyk", "Jean-F\u00e9lix Maestrati", "Maria Martin Asiain", "Kyriakos Flouris", "Markus Obmann", "Clarisse Dromain", "Beno\u00eet Dufour", "Pierre-Alexandre Alois Poletti", "Hendrik von Tengg-Kobligk", "Rolf H\u00fcgli", "Martin Kretzschmar", "Hatem Alkadhi", "Ender Konukoglu", "Henning M\u00fcller", "Bram Stieltjes", "Adrien Depeursinge"], "title": "A Multi-Centric Anthropomorphic 3D CT Phantom-Based Benchmark Dataset for Harmonization", "categories": ["cs.CV"], "comment": null, "summary": "Artificial intelligence (AI) has introduced numerous opportunities for human\nassistance and task automation in medicine. However, it suffers from poor\ngeneralization in the presence of shifts in the data distribution. In the\ncontext of AI-based computed tomography (CT) analysis, significant data\ndistribution shifts can be caused by changes in scanner manufacturer,\nreconstruction technique or dose. AI harmonization techniques can address this\nproblem by reducing distribution shifts caused by various acquisition settings.\nThis paper presents an open-source benchmark dataset containing CT scans of an\nanthropomorphic phantom acquired with various scanners and settings, which\npurpose is to foster the development of AI harmonization techniques. Using a\nphantom allows fixing variations attributed to inter- and intra-patient\nvariations. The dataset includes 1378 image series acquired with 13 scanners\nfrom 4 manufacturers across 8 institutions using a harmonized protocol as well\nas several acquisition doses. Additionally, we present a methodology, baseline\nresults and open-source code to assess image- and feature-level stability and\nliver tissue classification, promoting the development of AI harmonization\nstrategies.", "AI": {"tldr": "The paper presents an open-source CT scan dataset designed to improve AI harmonization techniques for addressing data distribution shifts. It includes 1378 image series from 13 scanners and offers tools for stability assessment and liver classification.", "motivation": "To address the issue of poor generalization in AI for CT analysis caused by data distribution shifts due to factors like scanner variability and acquisition settings.", "method": "The paper introduces a benchmark dataset of CT scans acquired under controlled conditions with variations in scanners and settings. This dataset is used to evaluate AI harmonization techniques. A phantom model replaced real patient data to eliminate individual biological effects.", "result": "The dataset features 1378 CT scan series from 13 scanners across 8 institutions and includes a methodology for assessing image- and feature-level stability, along with baseline results in liver tissue classification.", "conclusion": "The dataset and methodology aim to facilitate the development and evaluation of AI techniques that mitigate distribution shift issues, contributing to more reliable AI in medical imaging."}}
{"id": "2507.01235", "pdf": "https://arxiv.org/pdf/2507.01235", "abs": "https://arxiv.org/abs/2507.01235", "authors": ["Bara Rababa", "Bilal Farooq"], "title": "Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling", "categories": ["cs.LG", "quant-ph"], "comment": "Proceedings of IEEE Intelligent Transportation Systems Conference,\n  2025", "summary": "Quantum computing has opened new opportunities to tackle complex machine\nlearning tasks, for instance, high-dimensional data representations commonly\nrequired in intelligent transportation systems. We explore quantum machine\nlearning to model complex skin conductance response (SCR) events that reflect\npedestrian stress in a virtual reality road crossing experiment. For this\npurpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature\nmap and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and\nan eight-qubit ZZ feature map, were developed on Pennylane. The dataset\nconsists of SCR measurements along with features such as the response amplitude\nand elapsed time, which have been categorized into amplitude-based classes. The\nQSVM achieved good training accuracy, but had an overfitting problem, showing a\nlow test accuracy of 45% and therefore impacting the reliability of the\nclassification model. The QNN model reached a higher test accuracy of 55%,\nmaking it a better classification model than the QSVM and the classic versions.", "AI": {"tldr": "Explores quantum machine learning methods (QSVM and QNN) for analyzing pedestrian stress using skin conductance data, with QNN achieving better classification accuracy.", "motivation": "Quantum computing methods can enhance machine learning for complex, high-dimensional data, such as intelligent transportation systems requiring stress modeling.", "method": "Two quantum models (QSVM and QNN) were developed using an eight-qubit ZZ feature map in Pennylane to classify skin conductance responses.", "result": "QSVM showed overfitting and poor test accuracy (45%), while QNN outperformed with a 55% test accuracy compared to classic models.", "conclusion": "Quantum Neural Network (QNN) offers better reliability for stress classification modeling than Quantum Support Vector Machine (QSVM) and classical approaches."}}
{"id": "2507.01936", "pdf": "https://arxiv.org/pdf/2507.01936", "abs": "https://arxiv.org/abs/2507.01936", "authors": ["Adrian de Wynter", "Tangming Yuan"], "title": "The Thin Line Between Comprehension and Persuasion in LLMs", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.", "AI": {"tldr": "The paper investigates the ability of large language models (LLMs) to engage in debates and understand dialogue structures, revealing their strengths in persuasion but weaknesses in comprehending deeper dialogue contexts.", "motivation": "The authors aim to address the disparate accounts of LLMs' reasoning capabilities and explore their effectiveness and limitations in dialogue scenarios, especially in sensitive applications.", "method": "The study evaluates LLMs' ability to conduct debates and understand dialogical structures and pragmatic context, analyzing their coherence and capability in changing beliefs.", "result": "LLMs successfully maintain coherent and persuasive debates that can influence participants. However, they fail to exhibit an understanding of deeper dialogical structures or pragmatic contexts.", "conclusion": "The paper concludes that maintaining an effective dialogue does not require comprehension of its meaning, which questions the role of pragmatic context and coherence in argumentation theory for LLMs."}}
{"id": "2507.01557", "pdf": "https://arxiv.org/pdf/2507.01557", "abs": "https://arxiv.org/abs/2507.01557", "authors": ["Marcin Kowlaczyk", "Tomasz Kryjak"], "title": "Interpolation-Based Event Visual Data Filtering Algorithms", "categories": ["cs.CV"], "comment": "This paper has been accepted for publication at the IEEE Conference\n  on Computer Vision and Pattern Recognition (CVPR) Workshops, Vancouver, 2023.\n  Copyright IEEE", "summary": "The field of neuromorphic vision is developing rapidly, and event cameras are\nfinding their way into more and more applications. However, the data stream\nfrom these sensors is characterised by significant noise. In this paper, we\npropose a method for event data that is capable of removing approximately 99\\%\nof noise while preserving the majority of the valid signal. We have proposed\nfour algorithms based on the matrix of infinite impulse response (IIR) filters\nmethod. We compared them on several event datasets that were further modified\nby adding artificially generated noise and noise recorded with dynamic vision\nsensor. The proposed methods use about 30KB of memory for a sensor with a\nresolution of 1280 x 720 and is therefore well suited for implementation in\nembedded devices.", "AI": {"tldr": "The paper introduces a method to eliminate 99% of noise in event camera data while retaining valid signals.", "motivation": "The motivation is to address the significant noise in the data streams of rapidly developing neuromorphic vision and event camera applications.", "method": "Four algorithms based on the matrix of infinite impulse response (IIR) filters were proposed and tested on event datasets with artificially added noise and real noise.", "result": "The proposed methods demonstrated high noise reduction efficiency and required only 30KB of memory for a 1280 x 720 resolution sensor, making them suitable for embedded devices.", "conclusion": "The method is highly effective and resource-efficient, offering a practical solution for noise reduction in event camera data."}}
{"id": "2507.01241", "pdf": "https://arxiv.org/pdf/2507.01241", "abs": "https://arxiv.org/abs/2507.01241", "authors": ["Di Zhang", "Yihang Zhang"], "title": "Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Stochastic gradient-based descent (SGD), have long been central to training\nlarge language models (LLMs). However, their effectiveness is increasingly\nbeing questioned, particularly in large-scale applications where empirical\nevidence suggests potential performance limitations. In response, this paper\nproposes a stochastic conjugate subgradient method together with adaptive\nsampling tailored specifically for training LLMs. The method not only achieves\nfaster convergence per iteration but also demonstrates improved scalability\ncompared to traditional SGD techniques. It leverages sample complexity analysis\nto adaptively choose the sample size, employs a stochastic conjugate\nsubgradient approach to determine search directions and utilizing an AdamW-like\nalgorithm to adaptively adjust step sizes. This approach preserves the key\nadvantages of first-order methods while effectively addressing the nonconvexity\nand non-smoothness inherent in LLMs training. Additionally, we provide a\ndetailed analysis of the advantage of the algorithm. Experimental results show\nthat the proposed method not only maintains, but in many cases surpasses, the\nscalability of traditional SGD techniques, significantly enhancing both the\nspeed and accuracy of the optimization process.", "AI": {"tldr": "The paper introduces a stochastic conjugate subgradient method with adaptive sampling as an alternative to SGD for training LLMs, demonstrating faster convergence and improved scalability.", "motivation": "SGD\u2019s effectiveness in training LLMs faces challenges in large-scale scenarios, highlighting the need for improved optimization methods.", "method": "The approach combines a stochastic conjugate subgradient method, adaptive sampling based on sample complexity analysis, and an AdamW-like algorithm for adaptive step sizing.", "result": "The proposed method achieves faster convergence, improved scalability, and surpasses SGD in both speed and accuracy for LLM training.", "conclusion": "The new optimization algorithm effectively addresses challenges in LLMs training, offering a scalable and accurate alternative to traditional SGD."}}
{"id": "2507.01021", "pdf": "https://arxiv.org/pdf/2507.01021", "abs": "https://arxiv.org/abs/2507.01021", "authors": ["Kumarmanas Nethil", "Vaibhav Mishra", "Kriti Anandan", "Kavya Manohar"], "title": "Scalable Offline ASR for Command-Style Dictation in Courtrooms", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025 Show & Tell", "summary": "We propose an open-source framework for Command-style dictation that\naddresses the gap between resource-intensive Online systems and high-latency\nBatch processing. Our approach uses Voice Activity Detection (VAD) to segment\naudio and transcribes these segments in parallel using Whisper models, enabling\nefficient multiplexing across audios. Unlike proprietary systems like\nSuperWhisper, this framework is also compatible with most ASR architectures,\nincluding widely used CTC-based models. Our multiplexing technique maximizes\ncompute utilization in real-world settings, as demonstrated by its deployment\nin around 15% of India's courtrooms. Evaluations on live data show consistent\nlatency reduction as user concurrency increases, compared to sequential batch\nprocessing. The live demonstration will showcase our open-sourced\nimplementation and allow attendees to interact with it in real-time.", "AI": {"tldr": "This paper introduces an open-source framework for command-style dictation, leveraging Voice Activity Detection and Whisper models for efficient parallel audio transcription with reduced latency.", "motivation": "To address the limitations of resource-heavy online systems and high-latency batch processing for dictation tasks.", "method": "Utilizing Voice Activity Detection to segment audio, followed by parallel transcription using Whisper models and compatibility with various ASR architectures.", "result": "Deployment in 15% of India's courtrooms and evaluations showing reduced latency with increasing concurrency, outperforming sequential batch processing.", "conclusion": "The proposed framework is practical, efficient, and versatile, presenting significant advancements in dictation systems, highlighted by real-world deployment and a planned live demonstration."}}
{"id": "2507.01573", "pdf": "https://arxiv.org/pdf/2507.01573", "abs": "https://arxiv.org/abs/2507.01573", "authors": ["Hao Wang", "Keyan Hu", "Xin Guo", "Haifeng Li", "Chao Tao"], "title": "A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation", "categories": ["cs.CV"], "comment": "20 pages, 14 figures", "summary": "Remote sensing semantic segmentation must address both what the ground\nobjects are within an image and where they are located. Consequently,\nsegmentation models must ensure not only the semantic correctness of\nlarge-scale patches (low-frequency information) but also the precise\nlocalization of boundaries between patches (high-frequency information).\nHowever, most existing approaches rely heavily on discriminative learning,\nwhich excels at capturing low-frequency features, while overlooking its\ninherent limitations in learning high-frequency features for semantic\nsegmentation. Recent studies have revealed that diffusion generative models\nexcel at generating high-frequency details. Our theoretical analysis confirms\nthat the diffusion denoising process significantly enhances the model's ability\nto learn high-frequency features; however, we also observe that these models\nexhibit insufficient semantic inference for low-frequency features when guided\nsolely by the original image. Therefore, we integrate the strengths of both\ndiscriminative and generative learning, proposing the Integration of\nDiscriminative and diffusion-based Generative learning for Boundary Refinement\n(IDGBR) framework. The framework first generates a coarse segmentation map\nusing a discriminative backbone model. This map and the original image are fed\ninto a conditioning guidance network to jointly learn a guidance representation\nsubsequently leveraged by an iterative denoising diffusion process refining the\ncoarse segmentation. Extensive experiments across five remote sensing semantic\nsegmentation datasets (binary and multi-class segmentation) confirm our\nframework's capability of consistent boundary refinement for coarse results\nfrom diverse discriminative architectures. The source code will be available at\nhttps://github.com/KeyanHu-git/IDGBR.", "AI": {"tldr": "This paper addresses semantic segmentation challenges in remote sensing through a novel framework combining discriminative and diffusion generative models to refine boundaries effectively.", "motivation": "Current semantic segmentation models excel at identifying large-scale features but struggle to accurately capture fine boundary details due to limitations in discriminative learning and insufficient semantic inference in generative models.", "method": "The proposed IDGBR framework combines a discriminative backbone for generating coarse segmentation maps with a conditioning guidance network, feeding into a denoising diffusion process for refined boundary segmentation.", "result": "Experimentation across five datasets demonstrates the framework's consistent effectiveness in refining segmentation boundaries for binary and multi-class segmentation tasks.", "conclusion": "The integration of discriminative and generative learning in the IDGBR framework effectively addresses the shortcomings in boundary refinement for remote sensing applications, bridging the gap between semantic accuracy and boundary precision."}}
{"id": "2507.01271", "pdf": "https://arxiv.org/pdf/2507.01271", "abs": "https://arxiv.org/abs/2507.01271", "authors": ["Tatsuki Kawakami", "Kazuki Egashira", "Atsuyuki Miyai", "Go Irie", "Kiyoharu Aizawa"], "title": "PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In recent years, unlearning techniques, which are methods for inducing a\nmodel to \"forget\" previously learned information, have attracted attention as a\nway to address privacy and copyright concerns in large language models (LLMs)\nand large multimodal models (LMMs). While several unlearning benchmarks have\nbeen established for LLMs, a practical evaluation framework for unlearning in\nLMMs has been less explored. Specifically, existing unlearning benchmark for\nLMMs considers only scenarios in which the model is required to unlearn\nfine-tuned knowledge through a single unlearning operation. In this study, we\nintroduce PULSE protocol for realistic unlearning scenarios for LMMs by\nintroducing two critical perspectives: (i) Pre-trained knowledge Unlearning for\nanalyzing the effect across different knowledge acquisition phases and (ii)\nLong-term Sustainability Evaluation to address sequential requests. We then\nevaluate existing unlearning methods along these dimensions. Our results reveal\nthat, although some techniques can successfully unlearn knowledge acquired\nthrough fine-tuning, they struggle to eliminate information learned during\npre-training. Moreover, methods that effectively unlearn a batch of target data\nin a single operation exhibit substantial performance degradation when the same\ndata are split and unlearned sequentially.", "AI": {"tldr": "The paper introduces PULSE, a protocol for evaluating unlearning methods in large multimodal models (LMMs) with a focus on pre-trained knowledge unlearning and long-term sustainability.", "motivation": "With privacy and copyright concerns growing in large language and multimodal models, the need for methods allowing models to forget specific learned information became critical.", "method": "The study proposes PULSE protocol, emphasizing pre-trained knowledge unlearning and sequential sustainability evaluation frameworks.", "result": "Existing methods successfully unlearn fine-tuned knowledge but struggle with pre-trained knowledge and encounter performance degradation for sequential unlearning requests.", "conclusion": "Improved unlearning techniques are necessary to address pre-trained knowledge and ensure long-term sustainability of operations for practical applications in LMMs."}}
{"id": "2507.01586", "pdf": "https://arxiv.org/pdf/2507.01586", "abs": "https://arxiv.org/abs/2507.01586", "authors": ["Bryan Constantine Sadihin", "Michael Hua Wang", "Shei Pern Chua", "Hang Su"], "title": "SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation", "categories": ["cs.CV"], "comment": "Project page and code: https://bconstantine.github.io/SketchColour", "summary": "The production of high-quality 2D animation is highly labor-intensive\nprocess, as animators are currently required to draw and color a large number\nof frames by hand. We present SketchColour, the first sketch-to-colour pipeline\nfor 2D animation built on a diffusion transformer (DiT) backbone. By replacing\nthe conventional U-Net denoiser with a DiT-style architecture and injecting\nsketch information via lightweight channel-concatenation adapters accompanied\nwith LoRA finetuning, our method natively integrates conditioning without the\nparameter and memory bloat of a duplicated ControlNet, greatly reducing\nparameter count and GPU memory usage. Evaluated on the SAKUGA dataset,\nSketchColour outperforms previous state-of-the-art video colourization methods\nacross all metrics, despite using only half the training data of competing\nmodels. Our approach produces temporally coherent animations with minimal\nartifacts such as colour bleeding or object deformation. Our code is available\nat: https://bconstantine.github.io/SketchColour .", "AI": {"tldr": "The paper introduces SketchColour, a solution to improve 2D animation production using a diffusion transformer, reducing labor intensity and computational costs while achieving high-quality results.", "motivation": "Current methods for producing 2D animation require manual labor for drawing and coloring each frame, making the process highly intensive.", "method": "The pipeline utilizes a diffusion transformer backbone, replaces U-Net denoiser, and integrates sketch information through lightweight adapters and LoRA finetuning, reducing memory and parameter usage while enhancing quality.", "result": "SketchColour outperforms prior video colorization methods on the SAKUGA dataset, achieving better metrics and producing coherent animations with fewer visual artifacts, even with less training data.", "conclusion": "The SketchColour method is efficient, reduces computational requirements, and achieves state-of-the-art results, making it a promising solution for 2D animation production."}}
{"id": "2507.01587", "pdf": "https://arxiv.org/pdf/2507.01587", "abs": "https://arxiv.org/abs/2507.01587", "authors": ["Youngjin Oh", "Junhyeong Kwon", "Keuntek Lee", "Nam Ik Cho"], "title": "Towards Controllable Real Image Denoising with Camera Parameters", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted for publication in ICIP 2025, IEEE International Conference\n  on Image Processing", "summary": "Recent deep learning-based image denoising methods have shown impressive\nperformance; however, many lack the flexibility to adjust the denoising\nstrength based on the noise levels, camera settings, and user preferences. In\nthis paper, we introduce a new controllable denoising framework that adaptively\nremoves noise from images by utilizing information from camera parameters.\nSpecifically, we focus on ISO, shutter speed, and F-number, which are closely\nrelated to noise levels. We convert these selected parameters into a vector to\ncontrol and enhance the performance of the denoising network. Experimental\nresults show that our method seamlessly adds controllability to standard\ndenoising neural networks and improves their performance. Code is available at\nhttps://github.com/OBAKSA/CPADNet.", "AI": {"tldr": "The paper introduces a camera-parameter-based control for image denoising, improving performance and adjustability.", "motivation": "Many existing image denoising methods lack flexibility in adjusting denoising strength based on noise levels and preferences.", "method": "The proposed framework integrates camera parameters (ISO, shutter speed, F-number) into a vector to control and enhance a denoising neural network.", "result": "The method successfully adds controllability to denoising networks and improves their performance.", "conclusion": "Camera-based parameters effectively enable controllable and improved image denoising, validated by experimental results."}}
{"id": "2507.01313", "pdf": "https://arxiv.org/pdf/2507.01313", "abs": "https://arxiv.org/abs/2507.01313", "authors": ["Qian Qi"], "title": "Neural Hamiltonian Operator", "categories": ["cs.LG", "cs.AI", "math.DS", "math.OC"], "comment": null, "summary": "Stochastic control problems in high dimensions are notoriously difficult to\nsolve due to the curse of dimensionality. An alternative to traditional dynamic\nprogramming is Pontryagin's Maximum Principle (PMP), which recasts the problem\nas a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In\nthis paper, we introduce a formal framework for solving such problems with deep\nlearning by defining a \\textbf{Neural Hamiltonian Operator (NHO)}. This\noperator parameterizes the coupled FBSDE dynamics via neural networks that\nrepresent the feedback control and an ansatz for the value function's spatial\ngradient. We show how the optimal NHO can be found by training the underlying\nnetworks to enforce the consistency conditions dictated by the PMP. By adopting\nthis operator-theoretic view, we situate the deep FBSDE method within the\nrigorous language of statistical inference, framing it as a problem of learning\nan unknown operator from simulated data. This perspective allows us to prove\nthe universal approximation capabilities of NHOs under general martingale\ndrivers and provides a clear lens for analyzing the significant optimization\nchallenges inherent to this class of models.", "AI": {"tldr": "The paper tackles high-dimensional stochastic control problems by reformulating them using deep learning through a Neural Hamiltonian Operator (NHO), offering universal approximation proofs and analytical clarity.", "motivation": "The challenge of solving high-dimensional stochastic control problems is compounded by the curse of dimensionality, making traditional methods like dynamic programming inefficient. The paper seeks an alternative approach grounded in Pontryagin's Maximum Principle.", "method": "Introduce and define the Neural Hamiltonian Operator (NHO), combining neural networks to parameterize coupled FBSDE dynamics. Train networks to satisfy consistency conditions derived from the Maximum Principle.", "result": "The proposed NHO framework aligns stochastic control problem-solving with statistical inference principles, demonstrating universal approximation capabilities and clarity in optimization challenges.", "conclusion": "NHOs provide a rigorous operator-theoretic framework to tackle high-dimensional stochastic control problems using deep learning, bridging mathematical theory and computational efficiency."}}
{"id": "2507.01049", "pdf": "https://arxiv.org/pdf/2507.01049", "abs": "https://arxiv.org/abs/2507.01049", "authors": ["Pranav Jadhav"], "title": "Cohort Retrieval using Dense Passage Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Patient cohort retrieval is a pivotal task in medical research and clinical\npractice, enabling the identification of specific patient groups from extensive\nelectronic health records (EHRs). In this work, we address the challenge of\ncohort retrieval in the echocardiography domain by applying Dense Passage\nRetrieval (DPR), a prominent methodology in semantic search. We propose a\nsystematic approach to transform an echocardiographic EHR dataset of\nunstructured nature into a Query-Passage dataset, framing the problem as a\nCohort Retrieval task. Additionally, we design and implement evaluation metrics\ninspired by real-world clinical scenarios to rigorously test the models across\ndiverse retrieval tasks. Furthermore, we present a custom-trained DPR embedding\nmodel that demonstrates superior performance compared to traditional and\noff-the-shelf SOTA methods.To our knowledge, this is the first work to apply\nDPR for patient cohort retrieval in the echocardiography domain, establishing a\nframework that can be adapted to other medical domains.", "AI": {"tldr": "The paper applies Dense Passage Retrieval (DPR) to cohort retrieval in echocardiography, showcasing its effectiveness in identifying patient groups using unstructured EHR data.", "motivation": "Accurate retrieval of patient cohorts in echocardiography is essential to improve clinical outcomes and streamline research.", "method": "The researchers transform unstructured echocardiographic EHR data into Query-Passage datasets, frame the task as cohort retrieval, and implement custom evaluation metrics inspired by clinical scenarios. A custom-trained DPR embedding model was developed.", "result": "The custom-trained DPR model outperformed traditional and off-the-shelf state-of-the-art methods in patient cohort retrieval tasks.", "conclusion": "This study pioneers the use of DPR for patient cohort retrieval in echocardiography and introduces a reusable framework for broader medical applications."}}
{"id": "2507.01590", "pdf": "https://arxiv.org/pdf/2507.01590", "abs": "https://arxiv.org/abs/2507.01590", "authors": ["Ameer Hamza", "Zuhaib Hussain But", "Umar Arif", "Samiya", "M. Abdullah Asad", "Muhammad Naeem"], "title": "Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This study presents a novel classroom surveillance system that integrates\nmultiple modalities, including drowsiness, tracking of mobile phone usage, and\nface recognition,to assess student attentiveness with enhanced precision.The\nsystem leverages the YOLOv8 model to detect both mobile phone and sleep\nusage,(Ghatge et al., 2024) while facial recognition is achieved through\nLResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These\nmodels work in synergy to provide comprehensive, real-time monitoring, offering\ninsights into student engagement and behavior.(S et al., 2023) The framework is\ntrained on specialized datasets, such as the RMFD dataset for face recognition\nand a Roboflow dataset for mobile phone detection. The extensive evaluation of\nthe system shows promising results. Sleep detection achieves 97. 42% mAP@50,\nface recognition achieves 86. 45% validation accuracy and mobile phone\ndetection reach 85. 89% mAP@50. The system is implemented within a core PHP web\napplication and utilizes ESP32-CAM hardware for seamless data capture.(Neto et\nal., 2024) This integrated approach not only enhances classroom monitoring, but\nalso ensures automatic attendance recording via face recognition as students\nremain seated in the classroom, offering scalability for diverse educational\nenvironments.(Banada,2025)", "AI": {"tldr": "The study introduces a multi-modal classroom monitoring system combining advanced technologies for tracking student attentiveness, integrating facial recognition, drowsiness detection, and mobile phone tracking.", "motivation": "To create a more precise and comprehensive classroom monitoring system for assessing student engagement and attentiveness.", "method": "The system integrates YOLOv8 for mobile phone and drowsiness detection, LResNet Occ FC and MTCNN for facial recognition, and uses specialized datasets for training. It operates through a core PHP web application and ESP32-CAM hardware.", "result": "Achieved high accuracy in multiple areas: 97.42% mAP@50 for sleep detection, 86.45% validation accuracy for face recognition, and 85.89% mAP@50 for mobile phone detection.", "conclusion": "The system provides effective real-time monitoring and automatic attendance recording suitable for diverse educational settings, improving classroom management and engagement insights."}}
{"id": "2507.01321", "pdf": "https://arxiv.org/pdf/2507.01321", "abs": "https://arxiv.org/abs/2507.01321", "authors": ["Zhiyao Ren", "Siyuan Liang", "Aishan Liu", "Dacheng Tao"], "title": "ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "ICML 2025", "summary": "In-context learning (ICL) has demonstrated remarkable success in large\nlanguage models (LLMs) due to its adaptability and parameter-free nature.\nHowever, it also introduces a critical vulnerability to backdoor attacks, where\nadversaries can manipulate LLM behaviors by simply poisoning a few ICL\ndemonstrations. In this paper, we propose, for the first time, the\ndual-learning hypothesis, which posits that LLMs simultaneously learn both the\ntask-relevant latent concepts and backdoor latent concepts within poisoned\ndemonstrations, jointly influencing the probability of model outputs. Through\ntheoretical analysis, we derive an upper bound for ICL backdoor effects,\nrevealing that the vulnerability is dominated by the concept preference ratio\nbetween the task and the backdoor. Motivated by these findings, we propose\nICLShield, a defense mechanism that dynamically adjusts the concept preference\nratio. Our method encourages LLMs to select clean demonstrations during the ICL\nphase by leveraging confidence and similarity scores, effectively mitigating\nsusceptibility to backdoor attacks. Extensive experiments across multiple LLMs\nand tasks demonstrate that our method achieves state-of-the-art defense\neffectiveness, significantly outperforming existing approaches (+26.02% on\naverage). Furthermore, our method exhibits exceptional adaptability and\ndefensive performance even for closed-source models (e.g., GPT-4).", "AI": {"tldr": "This paper explores the vulnerability of in-context learning (ICL) in large language models (LLMs) to backdoor attacks and proposes a defense mechanism called ICLShield.", "motivation": "The motivation stems from the critical vulnerability of ICL in LLMs to backdoor attacks, highlighting the need to address this security threat without altering model parameters.", "method": "The authors introduce the dual-learning hypothesis and propose ICLShield, which dynamically adjusts the concept preference ratio in ICL demonstrations using confidence and similarity scores to mitigate backdoor attacks.", "result": "Experiments show that ICLShield achieves state-of-the-art defense effectiveness, outclassing existing methods by an average of 26.02%, and works effectively across various models and tasks, including closed-source models like GPT-4.", "conclusion": "ICLShield is an effective defense mechanism that significantly enhances the robustness of ICL in LLMs against backdoor attacks while maintaining adaptability to different models and tasks."}}
{"id": "2507.01603", "pdf": "https://arxiv.org/pdf/2507.01603", "abs": "https://arxiv.org/abs/2507.01603", "authors": ["Yue-Jiang Dong", "Wang Zhao", "Jiale Xu", "Ying Shan", "Song-Hai Zhang"], "title": "DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Diffusion-based video depth estimation methods have achieved remarkable\nsuccess with strong generalization ability. However, predicting depth for long\nvideos remains challenging. Existing methods typically split videos into\noverlapping sliding windows, leading to accumulated scale discrepancies across\ndifferent windows, particularly as the number of windows increases.\nAdditionally, these methods rely solely on 2D diffusion priors, overlooking the\ninherent 3D geometric structure of video depths, which results in geometrically\ninconsistent predictions. In this paper, we propose DepthSync, a novel,\ntraining-free framework using diffusion guidance to achieve scale- and\ngeometry-consistent depth predictions for long videos. Specifically, we\nintroduce scale guidance to synchronize the depth scale across windows and\ngeometry guidance to enforce geometric alignment within windows based on the\ninherent 3D constraints in video depths. These two terms work synergistically,\nsteering the denoising process toward consistent depth predictions. Experiments\non various datasets validate the effectiveness of our method in producing depth\nestimates with improved scale and geometry consistency, particularly for long\nvideos.", "AI": {"tldr": "DepthSync is a training-free framework designed to enhance scale and geometric consistency in video depth estimations, especially for long videos.", "motivation": "Challenges in existing video depth estimation methods arise from accumulated scale discrepancies in sliding windows and the lack of 3D geometric consideration.", "method": "DepthSync introduces scale guidance for consistent depth scales across video windows and geometry guidance for enforcing 3D-inspired depth alignment.", "result": "Experiments demonstrate that DepthSync improves scale and geometry consistency in video depth estimations on various datasets, particularly for longer videos.", "conclusion": "DepthSync provides a robust, training-free solution to achieve consistent depth predictions, addressing key challenges of scale and geometry in long video sequences."}}
{"id": "2507.01327", "pdf": "https://arxiv.org/pdf/2507.01327", "abs": "https://arxiv.org/abs/2507.01327", "authors": ["Xiaoyun Zhang", "Jingqing Ruan", "Xing Ma", "Yawen Zhu", "Jiansong Chen", "Ke Zeng", "Xunliang Cai"], "title": "Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy", "categories": ["cs.LG", "cs.AI"], "comment": "15 pages, 6 figures, submitted to EMNLP", "summary": "Detecting abnormal events in real-world customer service dialogues is highly\nchallenging due to the complexity of business data and the dynamic nature of\ncustomer interactions. Moreover, models must demonstrate strong out-of-domain\n(OOD) generalization to enable rapid adaptation across different business\nscenarios and maximize commercial value. In this work, we propose a novel\nAdaptive Perplexity-Aware Reinforcement Learning (APARL) framework that\nleverages the advanced reasoning capabilities of large language models for\nabnormal event detection. APARL introduces a dual-loop dynamic curriculum\nlearning architecture, enabling the model to progressively focus on more\nchallenging samples as its proficiency increases. This design effectively\naddresses performance bottlenecks and significantly enhances OOD\ntransferability. Extensive evaluations on food delivery dialogue tasks show\nthat our model achieves significantly enhanced adaptability and robustness,\nattaining the highest F1 score with an average improvement of 17.19\\%, and an\naverage improvement of 9.59\\% in OOD transfer tests. This method provides a\nsuperior solution for industrial deployment of anomaly detection models,\ncontributing to improved operational efficiency and commercial benefits.", "AI": {"tldr": "The paper introduces APARL, a framework for detecting abnormal events in customer dialogues, aimed at improving adaptability and robustness.", "motivation": "To address challenges in detecting anomalies in complex customer service dialogues and enhance out-of-domain generalization for diverse business scenarios.", "method": "APARL employs a dual-loop dynamic curriculum with reinforcement learning leveraging large language models, focusing progressively on challenging samples.", "result": "In food delivery dialogue tasks, APARL improved adaptability and OOD transferability, achieving an highest F1 score and significant performance improvements.", "conclusion": "APARL demonstrates its potential for industrial application, offering improved efficiency and commercial benefits in anomaly detection models."}}
{"id": "2507.01607", "pdf": "https://arxiv.org/pdf/2507.01607", "abs": "https://arxiv.org/abs/2507.01607", "authors": ["Quentin Le Roux", "Yannick Teglia", "Teddy Furon", "Philippe Loubet-Moundi", "Eric Bourbao"], "title": "Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "The widespread use of deep learning face recognition raises several security\nconcerns. Although prior works point at existing vulnerabilities, DNN backdoor\nattacks against real-life, unconstrained systems dealing with images captured\nin the wild remain a blind spot of the literature. This paper conducts the\nfirst system-level study of backdoors in deep learning-based face recognition\nsystems. This paper yields four contributions by exploring the feasibility of\nDNN backdoors on these pipelines in a holistic fashion. We demonstrate for the\nfirst time two backdoor attacks on the face detection task: face generation and\nface landmark shift attacks. We then show that face feature extractors trained\nwith large margin losses also fall victim to backdoor attacks. Combining our\nmodels, we then show using 20 possible pipeline configurations and 15 attack\ncases that a single backdoor enables an attacker to bypass the entire function\nof a system. Finally, we provide stakeholders with several best practices and\ncountermeasures.", "AI": {"tldr": "This paper explores vulnerabilities in deep learning face recognition systems and demonstrates new backdoor attack methods across various pipeline configurations.", "motivation": "To address the lack of research on real-world vulnerabilities of deep learning-based face recognition systems, particularly focusing on backdoor attacks in unconstrained settings.", "method": "The study proposes two novel backdoor attacks\u2014face generation and face landmark shift\u2014while analyzing their impact on face detection and feature extraction tasks within diverse pipeline configurations.", "result": "The authors show that backdoor attacks can entirely bypass a face recognition system's function across 20 pipeline configurations and 15 attack scenarios.", "conclusion": "The work highlights the risks associated with DNN backdoors in face recognition systems and provides guidelines for stakeholders to mitigate these vulnerabilities."}}
{"id": "2507.01354", "pdf": "https://arxiv.org/pdf/2507.01354", "abs": "https://arxiv.org/abs/2507.01354", "authors": ["Chugang Yi", "Minghan Yu", "Weikang Qian", "Yixin Wen", "Haizhao Yang"], "title": "Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion", "categories": ["cs.LG", "physics.ao-ph", "86A10 (Primary) 86A22, 68U10 (Secondary)", "J.2; I.4.4"], "comment": null, "summary": "Effective hydrological modeling and extreme weather analysis demand\nprecipitation data at a kilometer-scale resolution, which is significantly\nfiner than the 10 km scale offered by standard global products like IMERG. To\naddress this, we propose the Wavelet Diffusion Model (WDM), a generative\nframework that achieves 10x spatial super-resolution (downscaling to 1 km) and\ndelivers a 9x inference speedup over pixel-based diffusion models. WDM is a\nconditional diffusion model that learns the learns the complex structure of\nprecipitation from MRMS radar data directly in the wavelet domain. By focusing\non high-frequency wavelet coefficients, it generates exceptionally realistic\nand detailed 1-km precipitation fields. This wavelet-based approach produces\nvisually superior results with fewer artifacts than pixel-space models, and\ndelivers a significant gains in sampling efficiency. Our results demonstrate\nthat WDM provides a robust solution to the dual challenges of accuracy and\nspeed in geoscience super-resolution, paving the way for more reliable\nhydrological forecasts.", "AI": {"tldr": "The paper introduces the Wavelet Diffusion Model (WDM) for achieving high-resolution precipitation downscaling (1 km) with a 9x speedup over traditional methods.", "motivation": "The need for kilometer-scale precipitation data for hydrological and weather modeling surpasses the resolution provided by current global products, requiring more realistic and efficient downscaling techniques.", "method": "WDM uses a wavelet-based conditional diffusion model trained with radar data to focus on high-frequency wavelet coefficients, facilitating better realism and accuracy in generating 1-km precipitation fields.", "result": "The model outperforms traditional pixel-based diffusion models both visually and computationally, producing higher-quality results with fewer artifacts and greater efficiency.", "conclusion": "WDM successfully addresses the challenges of generating high-resolution precipitation data, enhancing accuracy and speed for geoscience applications."}}
{"id": "2507.01274", "pdf": "https://arxiv.org/pdf/2507.01274", "abs": "https://arxiv.org/abs/2507.01274", "authors": ["Vishakha Lall", "Yisi Liu"], "title": "AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted and Presented at 11th International Maritime Science\n  Conference", "summary": "Traditional simulator-based training for maritime professionals is critical\nfor ensuring safety at sea but often depends on subjective trainer assessments\nof technical skills, behavioral focus, communication, and body language, posing\nchallenges such as subjectivity, difficulty in measuring key features, and\ncognitive limitations. Addressing these issues, this study develops an\nAI-driven framework to enhance maritime training by objectively assessing\ntrainee performance through visual focus tracking, speech recognition, and\nstress detection, improving readiness for high-risk scenarios. The system\nintegrates AI techniques, including visual focus determination using eye\ntracking, pupil dilation analysis, and computer vision; communication analysis\nthrough a maritime-specific speech-to-text model and natural language\nprocessing; communication correctness using large language models; and mental\nstress detection via vocal pitch. Models were evaluated on data from simulated\nmaritime scenarios with seafarers exposed to controlled high-stress events. The\nAI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for\nmaritime speech recognition, and ~90% for stress detection, surpassing existing\nbenchmarks. The system provides insights into visual attention, adherence to\ncommunication checklists, and stress levels under demanding conditions. This\nstudy demonstrates how AI can transform maritime training by delivering\nobjective performance analytics, enabling personalized feedback, and improving\npreparedness for real-world operational challenges.", "AI": {"tldr": "This study introduces an AI-driven framework to objectively assess maritime trainees using visual focus tracking, speech recognition, and stress detection, showing high accuracy rates (~90%) in simulated high-stress scenarios.", "motivation": "Traditional maritime training relies on subjective trainer evaluations, which face challenges like bias, difficulty in quantifying behaviors, and cognitive limitations. This motivated the development of an AI-based system for objective assessments.", "method": "The study used AI techniques including eye tracking, computer vision, speech recognition with specialized maritime models, natural language processing for communication analysis, and stress detection through vocal pitch evaluation.", "result": "The AI framework demonstrated high accuracy rates (~92% for visual detection, ~91% for speech recognition, and ~90% for stress detection), surpassing existing benchmarks.", "conclusion": "The research highlights how AI can revolutionize maritime simulation training by enabling objective performance analysis, personalized feedback, and better readiness for operational challenges."}}
{"id": "2507.01608", "pdf": "https://arxiv.org/pdf/2507.01608", "abs": "https://arxiv.org/abs/2507.01608", "authors": ["Xu Zhang", "Ming Lu", "Yan Chen", "Zhan Ma"], "title": "Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference", "categories": ["cs.CV", "eess.IV"], "comment": "International Conference on Multimedia and Expo (ICME), 2025", "summary": "In recent years, compressed domain semantic inference has primarily relied on\nlearned image coding models optimized for mean squared error (MSE). However,\nMSE-oriented optimization tends to yield latent spaces with limited semantic\nrichness, which hinders effective semantic inference in downstream tasks.\nMoreover, achieving high performance with these models often requires\nfine-tuning the entire vision model, which is computationally intensive,\nespecially for large models. To address these problems, we introduce\nPerception-Oriented Latent Coding (POLC), an approach that enriches the\nsemantic content of latent features for high-performance compressed domain\nsemantic inference. With the semantically rich latent space, POLC requires only\na plug-and-play adapter for fine-tuning, significantly reducing the parameter\ncount compared to previous MSE-oriented methods. Experimental results\ndemonstrate that POLC achieves rate-perception performance comparable to\nstate-of-the-art generative image coding methods while markedly enhancing\nperformance in vision tasks, with minimal fine-tuning overhead. Code is\navailable at https://github.com/NJUVISION/POLC.", "AI": {"tldr": "The paper presents POLC, a semantic-rich latent coding method that improves compressed domain semantic inference while reducing fine-tuning overhead.", "motivation": "The paper aims to address limitations in compressed domain semantic inference, where MSE-optimized models lack semantic richness and are computationally intensive for fine-tuning.", "method": "The proposed POLC method enriches latent features for semantic inference and uses a plug-and-play adapter instead of extensive fine-tuning.", "result": "POLC delivers rate-perception performance comparable to the best generative coding methods while significantly improving vision task performance with minimal computational overhead.", "conclusion": "POLC offers an efficient and effective approach to compressed domain semantic inference, combining high performance with reduced fine-tuning requirements."}}
{"id": "2507.01381", "pdf": "https://arxiv.org/pdf/2507.01381", "abs": "https://arxiv.org/abs/2507.01381", "authors": ["Tong Liu", "Yinuo Wang", "Xujie Song", "Wenjun Zou", "Liangfa Chen", "Likun Wang", "Bin Shuai", "Jingliang Duan", "Shengbo Eben Li"], "title": "Distributional Soft Actor-Critic with Diffusion Policy", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted IEEE ITSC 2025", "summary": "Reinforcement learning has been proven to be highly effective in handling\ncomplex control tasks. Traditional methods typically use unimodal\ndistributions, such as Gaussian distributions, to model the output of value\ndistributions. However, unimodal distribution often and easily causes bias in\nvalue function estimation, leading to poor algorithm performance. This paper\nproposes a distributional reinforcement learning algorithm called DSAC-D\n(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges\nof estimating bias in value functions and obtaining multimodal policy\nrepresentations. A multimodal distributional policy iteration framework that\ncan converge to the optimal policy was established by introducing policy\nentropy and value distribution function. A diffusion value network that can\naccurately characterize the distribution of multi peaks was constructed by\ngenerating a set of reward samples through reverse sampling using a diffusion\nmodel. Based on this, a distributional reinforcement learning algorithm with\ndual diffusion of the value network and the policy network was derived. MuJoCo\ntesting tasks demonstrate that the proposed algorithm not only learns\nmultimodal policy, but also achieves state-of-the-art (SOTA) performance in all\n9 control tasks, with significant suppression of estimation bias and total\naverage return improvement of over 10\\% compared to existing mainstream\nalgorithms. The results of real vehicle testing show that DSAC-D can accurately\ncharacterize the multimodal distribution of different driving styles, and the\ndiffusion policy network can characterize multimodal trajectories.", "AI": {"tldr": "The paper introduces DSAC-D, a reinforcement learning algorithm employing diffusion models to reduce value estimation bias and enable multimodal policy representation, achieving state-of-the-art results in control tasks.", "motivation": "Current reinforcement learning methods often use unimodal distributions, which can bias value function estimation and degrade algorithm performance.", "method": "A distributional reinforcement learning approach is developed that employs a diffusion value network for multimodal policy iteration, leveraging reverse sampling and dual diffusion in both the value and policy networks.", "result": "The method achieves state-of-the-art results across 9 MuJoCo control tasks, significantly reduces estimation bias, and improves total returns by over 10%. Real vehicle testing confirms its effectiveness in characterizing multimodal driving behaviors.", "conclusion": "DSAC-D successfully addresses value estimation bias and captures multimodal distributions, outperforming traditional algorithms both in simulation and real-world scenarios."}}
{"id": "2507.01630", "pdf": "https://arxiv.org/pdf/2507.01630", "abs": "https://arxiv.org/abs/2507.01630", "authors": ["Yuxiao Wang", "Yu Lei", "Zhenao Wei", "Weiying Xue", "Xinyu Jiang", "Nan Zhuang", "Qi Liu"], "title": "Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICCV 2025", "summary": "The task of Human-Object conTact (HOT) detection involves identifying the\nspecific areas of the human body that are touching objects. Nevertheless,\ncurrent models are restricted to just one type of image, often leading to too\nmuch segmentation in areas with little interaction, and struggling to maintain\ncategory consistency within specific regions. To tackle this issue, a HOT\nframework, termed \\textbf{P3HOT}, is proposed, which blends \\textbf{P}rompt\nguidance and human \\textbf{P}roximal \\textbf{P}erception. To begin with, we\nutilize a semantic-driven prompt mechanism to direct the network's attention\ntowards the relevant regions based on the correlation between image and text.\nThen a human proximal perception mechanism is employed to dynamically perceive\nkey depth range around the human, using learnable parameters to effectively\neliminate regions where interactions are not expected. Calculating depth\nresolves the uncertainty of the overlap between humans and objects in a 2D\nperspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss\n(RJLoss) has been created as a new loss to inhibit abnormal categories in the\nsame area. A new evaluation metric called ``AD-Acc.'' is introduced to address\nthe shortcomings of existing methods in addressing negative samples.\nComprehensive experimental results demonstrate that our approach achieves\nstate-of-the-art performance in four metrics across two benchmark datasets.\nSpecifically, our model achieves an improvement of \\textbf{0.7}$\\uparrow$,\n\\textbf{2.0}$\\uparrow$, \\textbf{1.6}$\\uparrow$, and \\textbf{11.0}$\\uparrow$ in\nSC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated\ndataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.", "AI": {"tldr": "The paper introduces P3HOT, a Human-Object Contact detection framework using prompt-guided semantic mechanisms and depth perception to improve accuracy and consistency over benchmark datasets.", "motivation": "Current models for Human-Object Contact detection face challenges such as over-segmentation, ineffective handling of areas with minimal interaction, and lack of category consistency within regions.", "method": "The approach combines a prompt-driven semantic mechanism for targeting relevant areas and human proximal depth perception for better spatial understanding, alongside the introduction of RJLoss and AD-Acc metric for better evaluation.", "result": "P3HOT achieves state-of-the-art outcomes on multiple metrics, including SC-Acc., mIoU, wIoU, and AD-Acc., with significant improvements demonstrated on the HOT-Annotated dataset.", "conclusion": "The model effectively addresses limitations in existing HOT detection methods, offering better interaction identification and resolving overlapping uncertainties with a quasi-3D viewpoint."}}
{"id": "2507.01389", "pdf": "https://arxiv.org/pdf/2507.01389", "abs": "https://arxiv.org/abs/2507.01389", "authors": ["Anbang Wang", "Dunbo Cai", "Yu Zhang", "Yangqing Huang", "Xiangyang Feng", "Zhihong Zhang"], "title": "Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning", "categories": ["cs.LG", "quant-ph"], "comment": null, "summary": "Recently, a surrogate model was proposed that employs a factorization machine\nto approximate the underlying input-output mapping of the original system, with\nquantum annealing used to optimize the resulting surrogate function. Inspired\nby this approach, we propose an enhanced surrogate model that incorporates\nadditional slack variables into both the factorization machine and its\nassociated Ising representation thereby unifying what was by design a two-step\nprocess into a single, integrated step. During the training phase, the slack\nvariables are iteratively updated, enabling the model to account for\nhigher-order feature interactions. We apply the proposed method to the task of\npredicting drug combination effects. Experimental results indicate that the\nintroduction of slack variables leads to a notable improvement of performance.\nOur algorithm offers a promising approach for building efficient surrogate\nmodels that exploit potential quantum advantages.", "AI": {"tldr": "The paper presents an enhanced surrogate model with slack variables integrated into factorization machines, unified for single-step optimization, used in predicting drug combination effects, showing improved performance.", "motivation": "To enhance surrogate modeling for complex systems by optimizing input-output mapping through integrated quantum annealing and incorporating higher-order feature interactions.", "method": "Introduced slack variables into factorization machines and their Ising representation, unifying the training phase into a single iterative process for effectively optimizing the surrogate models.", "result": "Experiments on predicting drug combination effects show significant performance improvement with the inclusion of slack variables.", "conclusion": "The approach demonstrates a promising method for improving surrogate models' efficiency and performance, leveraging potential quantum computational advantages."}}
{"id": "2507.01548", "pdf": "https://arxiv.org/pdf/2507.01548", "abs": "https://arxiv.org/abs/2507.01548", "authors": ["Wen Zhan", "Ziqun Hua", "Peiyue Lin", "Yunfei Chen"], "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "A version of this manuscript has been submitted to the [IASDR 2025\n  Conference](https://iasdr2025.org/) and is currently under review", "summary": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems.", "AI": {"tldr": "The paper uses AI-assisted co-creation workshops to help older adults express personal narratives via oral storytelling and Hanzi reconstruction, offering a novel perspective on aging and human-AI collaboration.", "motivation": "To enable aging migrants in urban China to share fragmented and underrepresented personal narratives through innovative AI-assisted methods.", "method": "Conducted workshops combining oral storytelling and Hanzi reconstruction using AI-suggested Xiaozhuan glyphs and physical materials, facilitated by humans.", "result": "Participants shared migration memories and created new character forms through symbolic and interactive processes without needing digital literacy.", "conclusion": "The approach repositions AI as a supportive tool rather than a content producer, enhancing narrative agency and introducing new perspectives on aging and human-AI collaboration."}}
{"id": "2507.01631", "pdf": "https://arxiv.org/pdf/2507.01631", "abs": "https://arxiv.org/abs/2507.01631", "authors": ["Camille Billouard", "Dawa Derksen", "Alexandre Constantin", "Bruno Vallet"], "title": "Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "comment": "Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D\n  Vision Across Altitudes). Version before camera ready. Our code will be made\n  public after the conference", "summary": "Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D\nreconstruction from multiview satellite imagery. However, state-of-the-art NeRF\nmethods are typically constrained to small scenes due to the memory footprint\nduring training, which we study in this paper. Previous work on large-scale\nNeRFs palliate this by dividing the scene into NeRFs. This paper introduces\nSnake-NeRF, a framework that scales to large scenes. Our out-of-core method\neliminates the need to load all images and networks simultaneously, and\noperates on a single device. We achieve this by dividing the region of interest\ninto NeRFs that 3D tile without overlap. Importantly, we crop the images with\noverlap to ensure each NeRFs is trained with all the necessary pixels. We\nintroduce a novel $2\\times 2$ 3D tile progression strategy and segmented\nsampler, which together prevent 3D reconstruction errors along the tile edges.\nOur experiments conclude that large satellite images can effectively be\nprocessed with linear time complexity, on a single GPU, and without compromise\nin quality.", "AI": {"tldr": "This paper presents Snake-NeRF, a scalable framework for 3D reconstruction from large-scale satellite imagery using Neural Radiance Fields (NeRF).", "motivation": "State-of-the-art NeRF methods struggle with memory limitations during training, limiting their applications to small-scale scenes. The authors aim to address these scalability challenges.", "method": "The authors divide the region of interest into overlapping 3D tiles, ensuring pixels are included without reconstruction errors. They introduce a $2\\times2$ 3D tile progression strategy and segmented sampler to enhance reconstruction quality.", "result": "Snake-NeRF eliminates the need for simultaneous loading of all images and networks. It processes large satellite images on a single GPU with linear time complexity and retains high reconstruction quality.", "conclusion": "Snake-NeRF offers an effective, scalable solution for processing large-scale 3D satellite imagery using a single device, overcoming memory constraints."}}
{"id": "2507.01414", "pdf": "https://arxiv.org/pdf/2507.01414", "abs": "https://arxiv.org/abs/2507.01414", "authors": ["Sultan Daniels", "Dylan Davis", "Dhruv Gautam", "Wentinn Liao", "Gireeja Ranade", "Anant Sahai"], "title": "Decomposing Prediction Mechanisms for In-Context Recall", "categories": ["cs.LG"], "comment": "44 pages, 47 figures, 2 tables", "summary": "We introduce a new family of toy problems that combine features of\nlinear-regression-style continuous in-context learning (ICL) with discrete\nassociative recall. We pretrain transformer models on sample traces from this\ntoy, specifically symbolically-labeled interleaved state observations from\nrandomly drawn linear deterministic dynamical systems. We study if the\ntransformer models can recall the state of a sequence previously seen in its\ncontext when prompted to do so with the corresponding in-context label. Taking\na closer look at this task, it becomes clear that the model must perform two\nfunctions: (1) identify which system's state should be recalled and apply that\nsystem to its last seen state, and (2) continuing to apply the correct system\nto predict the subsequent states. Training dynamics reveal that the first\ncapability emerges well into a model's training. Surprisingly, the second\ncapability, of continuing the prediction of a resumed sequence, develops much\nearlier.\n  Via out-of-distribution experiments, and a mechanistic analysis on model\nweights via edge pruning, we find that next-token prediction for this toy\nproblem involves at least two separate mechanisms. One mechanism uses the\ndiscrete symbolic labels to do the associative recall required to predict the\nstart of a resumption of a previously seen sequence. The second mechanism,\nwhich is largely agnostic to the discrete symbolic labels, performs a\n\"Bayesian-style\" prediction based on the previous token and the context. These\ntwo mechanisms have different learning dynamics.\n  To confirm that this multi-mechanism (manifesting as separate phase\ntransitions) phenomenon is not just an artifact of our toy setting, we used\nOLMo training checkpoints on an ICL translation task to see a similar\nphenomenon: a decisive gap in the emergence of first-task-token performance vs\nsecond-task-token performance.", "AI": {"tldr": "The paper introduces a toy problem combining continuous in-context learning and associative recall using transformer models. The study observes distinct learning phases for separate mechanisms handling sequence prediction tasks.", "motivation": "The motivation is to explore and understand how transformer models perform in-context learning, specifically focusing on their ability to recall and predict sequences based on discrete symbolic labels and previous states.", "method": "The authors pretrain transformer models on synthetic data generated from deterministic dynamical systems and analyze their ability to recall and predict sequences. Mechanistic analysis via edge pruning and out-of-distribution tests is performed to uncover the underlying learning dynamics.", "result": "The study identifies two distinct mechanisms for sequence prediction: one for associative recall using symbolic labels and another for Bayesian-style prediction based on prior context. These mechanisms exhibit different learning dynamics, with one capability emerging earlier in training than the other.", "conclusion": "The presence of phase transitions in learning behaviors indicates that multi-mechanism dynamics are not just artifacts of the toy problem but generalize to more complex tasks, as demonstrated in ICL translation experiments."}}
{"id": "2507.01551", "pdf": "https://arxiv.org/pdf/2507.01551", "abs": "https://arxiv.org/abs/2507.01551", "authors": ["Wu Fei", "Hao Kong", "Shuxian Liang", "Yang Lin", "Yibo Yang", "Jing Tang", "Lei Chen", "Xiansheng Hua"], "title": "Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.", "AI": {"tldr": "This paper presents SPRO, a framework for process-aware reinforcement learning that integrates training efficiency and improved performance without extra computational costs.", "motivation": "To address the lack of a unified theoretical framework and computational inefficiencies in process-level reinforcement learning for LLMs.", "method": "SPRO introduces intrinsic rewards derived from the policy model and cumulative rewards paired with a Masked Step Advantage (MSA) for action advantage estimation.", "result": "SPRO achieves 3.4x training efficiency, 17.5% higher test accuracy, stable policy entropy, and reduces average response length by 1/3 compared to GRPO.", "conclusion": "SPRO is an effective, industrially implementable method for process-aware RL in LLMs, outperforming existing frameworks without additional computational demands."}}
{"id": "2507.01634", "pdf": "https://arxiv.org/pdf/2507.01634", "abs": "https://arxiv.org/abs/2507.01634", "authors": ["Boyuan Sun", "Modi Jin", "Bowen Yin", "Qibin Hou"], "title": "Depth Anything at Any Condition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC", "AI": {"tldr": "DepthAnything-AC is a monocular depth estimation (MDE) model capable of performing well across challenging environmental conditions, utilizing a novel finetuning paradigm and spatial constraints.", "motivation": "Previous models struggled with depth estimation in complex environments involving lighting changes, adverse weather, and image distortions due to both data scarcity and poor pseudo-label quality.", "method": "The paper introduces an unsupervised consistency regularization finetuning approach combined with a Spatial Distance Constraint to enhance patch-level depth relationships in images.", "result": "Experimental results show that DepthAnything-AC performs robustly in zero-shot settings across benchmarks like adverse weather, synthetic corruptions, and general datasets.", "conclusion": "DepthAnything-AC demonstrates enhanced depth estimation capabilities in diverse and challenging environmental conditions, setting a foundation for broader applications."}}
{"id": "2507.01643", "pdf": "https://arxiv.org/pdf/2507.01643", "abs": "https://arxiv.org/abs/2507.01643", "authors": ["Weijie Yin", "Dingkang Yang", "Hongyuan Dong", "Zijian Kang", "Jiacong Wang", "Xiao Liang", "Chao Feng", "Jiao Ran"], "title": "SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement", "categories": ["cs.CV"], "comment": "We release SAILViT, a series of versatile vision foundation models", "summary": "Vision Transformers (ViTs) are essential as foundation backbones in\nestablishing the visual comprehension capabilities of Multimodal Large Language\nModels (MLLMs). Although most ViTs achieve impressive performance through\nimage-text pair-based contrastive learning or self-supervised mechanisms, they\nstruggle to engage in connector-based co-training directly with LLMs due to\npotential parameter initialization conflicts and modality semantic gaps. To\naddress the above challenges, this paper proposes SAILViT, a gradual feature\nlearning-enhanced ViT for facilitating MLLMs to break through performance\nbottlenecks in complex multimodal interactions. SAILViT achieves\ncoarse-to-fine-grained feature alignment and world knowledge infusion with\ngradual feature refinement, which better serves target training demands. We\nperform thorough empirical analyses to confirm the powerful robustness and\ngeneralizability of SAILViT across different dimensions, including parameter\nsizes, model architectures, training strategies, and data scales. Equipped with\nSAILViT, existing MLLMs show significant and consistent performance\nimprovements on the OpenCompass benchmark across extensive downstream tasks.\nSAILViT series models are released at\nhttps://huggingface.co/BytedanceDouyinContent.", "AI": {"tldr": "The paper proposes SAILViT, a Vision Transformer (ViT) design aimed at improving Multimodal Large Language Models (MLLMs) by enhancing feature learning and alignment, solving issues related to parameter conflicts and semantic gaps.", "motivation": "ViTs, while strong in vision-related tasks, face challenges in integrating with LLMs due to initialization conflicts and semantic differences.", "method": "The authors introduce SAILViT, a gradual feature learning-enhanced ViT that performs coarse-to-fine-grained feature alignment and integrates world knowledge into the learning process.", "result": "SAILViT demonstrates robust performance across varying parameter sizes, architectures, and data scales, resulting in notable improvements for MLLMs on the OpenCompass benchmark.", "conclusion": "SAILViT proves effective in overcoming multimodal integration challenges and enhances the performance of Multimodal Large Language Models on diverse tasks."}}
{"id": "2507.01469", "pdf": "https://arxiv.org/pdf/2507.01469", "abs": "https://arxiv.org/abs/2507.01469", "authors": ["Alessio Ferrato", "Fabio Gasparetti", "Carla Limongelli", "Stefano Mastandrea", "Giuseppe Sansonetti", "Joaqu\u00edn Torres-Sospedra"], "title": "Cross-platform Smartphone Positioning at Museums", "categories": ["cs.LG", "eess.SP"], "comment": "Accepted at the 2025 International Conference on Indoor Positioning\n  and Indoor Navigation (IPIN), Tampere, Finland, September 15-18, 2025", "summary": "Indoor Positioning Systems (IPSs) hold significant potential for enhancing\nvisitor experiences in cultural heritage institutions. By enabling personalized\nnavigation, efficient artifact organization, and better interaction with\nexhibits, IPSs can transform the modalities of how individuals engage with\nmuseums, galleries and libraries. However, these institutions face several\nchallenges in implementing IPSs, including environmental constraints, technical\nlimits, and limited experimentation. In other contexts, Received Signal\nStrength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have\nemerged as preferred solutions due to their non-invasive nature and minimal\ninfrastructure requirements. Nevertheless, the lack of publicly available RSS\ndatasets that specifically reflect museum environments presents a substantial\nbarrier to developing and evaluating positioning algorithms designed for the\nintricate spatial characteristics typical of cultural heritage sites. To\naddress this limitation, we present BAR, a novel RSS dataset collected in front\nof 90 artworks across 13 museum rooms using two different platforms, i.e.,\nAndroid and iOS. Additionally, we provide an advanced position classification\nbaseline taking advantage of a proximity-based method and $k$-NN algorithms. In\nour analysis, we discuss the results and offer suggestions for potential\nresearch directions.", "AI": {"tldr": "This paper introduces BAR, a unique RSS dataset specifically tailored for museum environments, collected using Android and iOS platforms to enhance IPS development and evaluation.", "motivation": "Cultural heritage institutions face challenges in implementing Indoor Positioning Systems (IPSs), with a need for specialized RSS datasets to cater to museum environments.", "method": "The authors collected RSS data in front of 90 artworks across 13 museum rooms using Android and iOS devices and implemented proximity-based and $k$-NN algorithms for experimental classification.", "result": "The study presents a dataset that reflects museum environments and demonstrates baseline positioning classification using the proximity-based method and $k$-NN algorithms.", "conclusion": "The BAR dataset serves as a valuable resource for advancing research in IPSs tailored to cultural heritage sites, outlining opportunities for improved navigation and visitor experiences."}}
{"id": "2507.01599", "pdf": "https://arxiv.org/pdf/2507.01599", "abs": "https://arxiv.org/abs/2507.01599", "authors": ["Zhaoyan Sun", "Jiayi Wang", "Xinyang Zhao", "Jiachi Wang", "Guoliang Li"], "title": "Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Traditional Data+AI systems utilize data-driven techniques to optimize\nperformance, but they rely heavily on human experts to orchestrate system\npipelines, enabling them to adapt to changes in data, queries, tasks, and\nenvironments. For instance, while there are numerous data science tools\navailable, developing a pipeline planning system to coordinate these tools\nremains challenging. This difficulty arises because existing Data+AI systems\nhave limited capabilities in semantic understanding, reasoning, and planning.\nFortunately, we have witnessed the success of large language models (LLMs) in\nenhancing semantic understanding, reasoning, and planning abilities. It is\ncrucial to incorporate LLM techniques to revolutionize data systems for\norchestrating Data+AI applications effectively.\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\nand planning capabilities. We delve into the challenges involved in designing\ndata agents, such as understanding data/queries/environments/tools,\norchestrating pipelines/workflows, optimizing and executing pipelines, and\nfostering pipeline self-reflection. Furthermore, we present examples of data\nagent systems, including a data science agent, data analytics agents (such as\nunstructured data analytics agent, semantic structured data analytics agent,\ndata lake analytics agent, and multi-modal data analytics agent), and a\ndatabase administrator (DBA) agent. We also outline several open challenges\nassociated with designing data agent systems.", "AI": {"tldr": "The paper proposes 'Data Agents' as advanced architectures using LLMs to orchestrate Data+AI ecosystems more effectively by integrating semantic understanding, reasoning, and planning.", "motivation": "Current Data+AI systems rely on human experts for pipeline orchestration due to their limited semantic understanding, reasoning, and planning capabilities. This creates inefficiencies as systems struggle to adapt to dynamic tasks, data, and environments.", "method": "The authors propose a novel 'Data Agent' architecture designed to autonomously handle tasks such as semantic understanding, reasoning, pipeline orchestration, optimization, execution, and self-reflection.", "result": "Examples of Data Agent systems, including data science agents, analytics agents, and DBA agents, are provided to illustrate their applicability. Challenges in designing such systems are also discussed.", "conclusion": "Integrating LLM techniques into Data+AI systems via 'Data Agents' has the potential to revolutionize the field, although several open challenges remain."}}
{"id": "2507.01652", "pdf": "https://arxiv.org/pdf/2507.01652", "abs": "https://arxiv.org/abs/2507.01652", "authors": ["Yuxin Mao", "Zhen Qin", "Jinxing Zhou", "Hui Deng", "Xuyang Shen", "Bin Fan", "Jing Zhang", "Yiran Zhong", "Yuchao Dai"], "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.", "AI": {"tldr": "The paper introduces LASAD, a novel linear attention mechanism tailored for image generation, and LASADGen, an autoregressive image generator, which achieves efficient and high-quality image generation by accounting for true 2D spatial relationships.", "motivation": "Traditional autoregressive models for image generation face computational inefficiencies due to quadratic complexity in transformers. Attempts to incorporate linear attention mechanisms compromise image quality due to insufficient handling of long-range spatial dependencies.", "method": "The paper introduces LASAD, a mechanism that computes position-dependent decay factors while preserving genuine 2D spatial relationships, to enable efficient and selective attention. LASAD is incorporated into LASADGen, a new autoregressive image generation framework.", "result": "LASADGen achieves state-of-the-art image generation results on ImageNet with enhanced computational efficiency, addressing challenges in both performance and memory overhead.", "conclusion": "LASAD successfully bridges the gap between linear attention efficiency and spatial understanding, demonstrating a promising advancement for autoregressive image generation models."}}
{"id": "2507.01470", "pdf": "https://arxiv.org/pdf/2507.01470", "abs": "https://arxiv.org/abs/2507.01470", "authors": ["Yannick Molinghen", "Tom Lenaerts"], "title": "Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at \"Finding the Frame 2025\", workshop at RLC", "summary": "This work re-examines the commonly held assumption that the frequency of\nrewards is a reliable measure of task difficulty in reinforcement learning. We\nidentify and formalize a structural challenge that undermines the effectiveness\nof current policy learning methods: when essential subgoals do not directly\nyield rewards. We characterize such settings as exhibiting zero-incentive\ndynamics, where transitions critical to success remain unrewarded. We show that\nstate-of-the-art deep subgoal-based algorithms fail to leverage these dynamics\nand that learning performance is highly sensitive to the temporal proximity\nbetween subgoal completion and eventual reward. These findings reveal a\nfundamental limitation in current approaches and point to the need for\nmechanisms that can infer latent task structure without relying on immediate\nincentives.", "AI": {"tldr": "This paper challenges the assumption that reward frequency is a reliable measure of task difficulty in reinforcement learning, highlighting issues with 'zero-incentive dynamics,' where key subgoals are unrewarded.", "motivation": "The motivation is to address the structural challenges in reinforcement learning when essential transitions, vital to task success, do not offer direct rewards.", "method": "The paper identifies 'zero-incentive dynamics' and evaluates the performance of state-of-the-art subgoal algorithms, analyzing sensitivity to timing between subgoal completion and eventual rewards.", "result": "It is found that current deep subgoal-based methods fail to exploit zero-incentive dynamics and are highly sensitive to the temporal reward structure.", "conclusion": "Current reinforcement learning approaches have fundamental limitations in handling tasks with latent structures, necessitating new mechanisms beyond immediate reward-driven learning."}}
{"id": "2507.01679", "pdf": "https://arxiv.org/pdf/2507.01679", "abs": "https://arxiv.org/abs/2507.01679", "authors": ["Zeyu Huang", "Tianhao Cheng", "Zihan Qiu", "Zili Wang", "Yinghui Xu", "Edoardo M. Ponti", "Ivan Titov"], "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research.", "AI": {"tldr": "The paper introduces Prefix-RFT, a hybrid approach combining Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT), yielding superior performance and requiring minimal modifications to existing RFT pipelines.", "motivation": "The motivation is to address the trade-offs in post-training methods for large language models, balancing SFT's mimicry of demonstrations and RFT's exploration-driven learning.", "method": "The Prefix-RFT methodology combines SFT and RFT paradigms to synergize learning from demonstration and exploration.", "result": "Prefix-RFT outperforms standalone SFT, RFT, and mixed-policy RFT methods, demonstrating robustness to variations in demonstration data.", "conclusion": "Prefix-RFT highlights the complementary nature of SFT and RFT, suggesting a unified paradigm for enhancing LLM post-training techniques."}}
{"id": "2507.01653", "pdf": "https://arxiv.org/pdf/2507.01653", "abs": "https://arxiv.org/abs/2507.01653", "authors": ["Yuran Wang", "Yingping Liang", "Yutao Hu", "Ying Fu"], "title": "RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather", "categories": ["cs.CV"], "comment": "accepted by ICCV25", "summary": "Learning-based stereo matching models struggle in adverse weather conditions\ndue to the scarcity of corresponding training data and the challenges in\nextracting discriminative features from degraded images. These limitations\nsignificantly hinder zero-shot generalization to out-of-distribution weather\nconditions. In this paper, we propose \\textbf{RobuSTereo}, a novel framework\nthat enhances the zero-shot generalization of stereo matching models under\nadverse weather by addressing both data scarcity and feature extraction\nchallenges. First, we introduce a diffusion-based simulation pipeline with a\nstereo consistency module, which generates high-quality stereo data tailored\nfor adverse conditions. By training stereo matching models on our synthetic\ndatasets, we reduce the domain gap between clean and degraded images,\nsignificantly improving the models' robustness to unseen weather conditions.\nThe stereo consistency module ensures structural alignment across synthesized\nimage pairs, preserving geometric integrity and enhancing depth estimation\naccuracy. Second, we design a robust feature encoder that combines a\nspecialized ConvNet with a denoising transformer to extract stable and reliable\nfeatures from degraded images. The ConvNet captures fine-grained local\nstructures, while the denoising transformer refines global representations,\neffectively mitigating the impact of noise, low visibility, and weather-induced\ndistortions. This enables more accurate disparity estimation even under\nchallenging visual conditions. Extensive experiments demonstrate that\n\\textbf{RobuSTereo} significantly improves the robustness and generalization of\nstereo matching models across diverse adverse weather scenarios.", "AI": {"tldr": "This paper introduces RobuSTereo, a framework addressing data scarcity and feature extraction challenges in stereo matching under adverse weather conditions.", "motivation": "Stereo matching models underperform in challenging weather due to data limitations and degraded feature extraction.", "method": "Proposes a diffusion-based simulation pipeline with stereo consistency and a robust feature encoder integrating ConvNet with a denoising transformer.", "result": "RobuSTereo enhances robustness and zero-shot generalization to unseen adverse weather scenarios.", "conclusion": "The framework improves disparity estimation accuracy and mitigates the effects of weather-induced distortions."}}
{"id": "2507.01516", "pdf": "https://arxiv.org/pdf/2507.01516", "abs": "https://arxiv.org/abs/2507.01516", "authors": ["Dibyanshu Kumar", "Philipp Vaeth", "Magda Gregorov\u00e1"], "title": "Loss Functions in Diffusion Models: A Comparative Study", "categories": ["cs.LG"], "comment": "Accepted to ECML 2025", "summary": "Diffusion models have emerged as powerful generative models, inspiring\nextensive research into their underlying mechanisms. One of the key questions\nin this area is the loss functions these models shall train with. Multiple\nformulations have been introduced in the literature over the past several years\nwith some links and some critical differences stemming from various initial\nconsiderations. In this paper, we explore the different target objectives and\ncorresponding loss functions in detail. We present a systematic overview of\ntheir relationships, unifying them under the framework of the variational lower\nbound objective. We complement this theoretical analysis with an empirical\nstudy providing insights into the conditions under which these objectives\ndiverge in performance and the underlying factors contributing to such\ndeviations. Additionally, we evaluate how the choice of objective impacts the\nmodel ability to achieve specific goals, such as generating high-quality\nsamples or accurately estimating likelihoods. This study offers a unified\nunderstanding of loss functions in diffusion models, contributing to more\nefficient and goal-oriented model designs in future research.", "AI": {"tldr": "This paper systematically analyzes loss functions in diffusion models, unifies them under a variational framework, and explores their impact empirically.", "motivation": "Investigate the critical differences among loss functions in diffusion models and unify them under a common theoretical framework.", "method": "Examines relationships between loss functions using the variational lower bound framework and conducts empirical studies to analyze their performance divergences.", "result": "Finds conditions where loss functions diverge in performance and evaluates their impact on model goals like sample quality and likelihood estimation.", "conclusion": "Provides a unified understanding of loss functions to guide efficient and goal-oriented diffusion model design in future research."}}
{"id": "2507.01339", "pdf": "https://arxiv.org/pdf/2507.01339", "abs": "https://arxiv.org/abs/2507.01339", "authors": ["Yutong Wen", "Minje Kim", "Paris Smaragdis"], "title": "User-guided Generative Source Separation", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Music source separation (MSS) aims to extract individual instrument sources\nfrom their mixture. While most existing methods focus on the widely adopted\nfour-stem separation setup (vocals, bass, drums, and other instruments), this\napproach lacks the flexibility needed for real-world applications. To address\nthis, we propose GuideSep, a diffusion-based MSS model capable of\ninstrument-agnostic separation beyond the four-stem setup. GuideSep is\nconditioned on multiple inputs: a waveform mimicry condition, which can be\neasily provided by humming or playing the target melody, and mel-spectrogram\ndomain masks, which offer additional guidance for separation. Unlike prior\napproaches that relied on fixed class labels or sound queries, our conditioning\nscheme, coupled with the generative approach, provides greater flexibility and\napplicability. Additionally, we design a mask-prediction baseline using the\nsame model architecture to systematically compare predictive and generative\napproaches. Our objective and subjective evaluations demonstrate that GuideSep\nachieves high-quality separation while enabling more versatile instrument\nextraction, highlighting the potential of user participation in the\ndiffusion-based generative process for MSS. Our code and demo page are\navailable at https://yutongwen.github.io/GuideSep/", "AI": {"tldr": "The paper introduces GuideSep, a flexible music source separation (MSS) model that uses a diffusion-based approach to separate audio sources beyond traditional instrument classifications by leveraging waveform mimicry and mel-spectrogram masks.", "motivation": "The motivation is to address the lack of flexibility in traditional four-stem instrument separation methods, which hampers real-world application versatility.", "method": "The proposed method, GuideSep, uses a diffusion-based generative approach, conditioning on waveform mimicry and mel-spectrogram masks for flexible and instrument-agnostic audio separation.", "result": "Objective and subjective evaluations of GuideSep show high-quality audio separation results with broader applicability compared to conventional methods.", "conclusion": "GuideSep demonstrates the potential for flexible, high-quality music source separation, accommodating user inputs and expanding use-case versatility in real-world scenarios."}}
{"id": "2507.01735", "pdf": "https://arxiv.org/pdf/2507.01735", "abs": "https://arxiv.org/abs/2507.01735", "authors": ["Kai Chen", "Ruiyuan Gao", "Lanqing Hong", "Hang Xu", "Xu Jia", "Holger Caesar", "Dengxin Dai", "Bingbing Liu", "Dzmitry Tsishkou", "Songcen Xu", "Chunjing Xu", "Qiang Xu", "Huchuan Lu", "Dit-Yan Yeung"], "title": "ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "ECCV 2024. Workshop page: https://coda-dataset.github.io/w-coda2024/", "summary": "In this paper, we present details of the 1st W-CODA workshop, held in\nconjunction with the ECCV 2024. W-CODA aims to explore next-generation\nsolutions for autonomous driving corner cases, empowered by state-of-the-art\nmultimodal perception and comprehension techniques. 5 Speakers from both\nacademia and industry are invited to share their latest progress and opinions.\nWe collect research papers and hold a dual-track challenge, including both\ncorner case scene understanding and generation. As the pioneering effort, we\nwill continuously bridge the gap between frontier autonomous driving techniques\nand fully intelligent, reliable self-driving agents robust towards corner\ncases.", "AI": {"tldr": "The paper discusses the 1st W-CODA workshop at ECCV 2024, focusing on autonomous driving corner cases and multimodal perception, including talks, research papers, and a dual-track challenge.", "motivation": "To address corner cases in autonomous driving and explore advanced multimodal perception techniques for making self-driving systems more reliable.", "method": "Organizing a workshop including invited talks, research paper collection, and a dual-track challenge on corner case scene understanding and generation.", "result": "The workshop features contributions from both academia and industry, fostering discussions and solutions for autonomous driving corner cases.", "conclusion": "W-CODA acts as a pioneering platform to close the gap between cutting-edge research and the development of robust self-driving agents for handling challenging corner cases."}}
{"id": "2507.01654", "pdf": "https://arxiv.org/pdf/2507.01654", "abs": "https://arxiv.org/abs/2507.01654", "authors": ["Martine Hjelkrem-Tan", "Marius Aasan", "Gabriel Y. Arteaga", "Ad\u00edn Ram\u00edrez Rivera"], "title": "SPoT: Subpixel Placement of Tokens in Vision Transformers", "categories": ["cs.CV", "cs.LG"], "comment": "To appear in Workshop on Efficient Computing under Limited Resources:\n  Visual Computing (ICCV 2025). Code available at\n  https://github.com/dsb-ifi/SPoT", "summary": "Vision Transformers naturally accommodate sparsity, yet standard tokenization\nmethods confine features to discrete patch grids. This constraint prevents\nmodels from fully exploiting sparse regimes, forcing awkward compromises. We\npropose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that\npositions tokens continuously within images, effectively sidestepping\ngrid-based limitations. With our proposed oracle-guided search, we uncover\nsubstantial performance gains achievable with ideal subpixel token positioning,\ndrastically reducing the number of tokens necessary for accurate predictions\nduring inference. SPoT provides a new direction for flexible, efficient, and\ninterpretable ViT architectures, redefining sparsity as a strategic advantage\nrather than an imposed limitation.", "AI": {"tldr": "This paper introduces Subpixel Placement of Tokens (SPoT), an innovative tokenization method removing grid-based limitations in Vision Transformers to achieve efficiency and flexibility.", "motivation": "To address the inefficiencies and limitations in tokenization strategies that confine features to discrete patch grids in Vision Transformers, hindering sparse regimes.", "method": "Proposing Subpixel Placement of Tokens (SPoT) and an oracle-guided search to determine optimal subpixel token positioning.", "result": "Substantial performance gains with reduced token requirements for accurate predictions, showcasing the efficiency and interpretability of SPoT.", "conclusion": "SPoT redefines sparsity in Vision Transformers as an advantage, paving the way for more flexible, efficient models without grid constraints."}}
{"id": "2507.01522", "pdf": "https://arxiv.org/pdf/2507.01522", "abs": "https://arxiv.org/abs/2507.01522", "authors": ["Koen Ponse", "Jan Felix Kleuker", "Aske Plaat", "Thomas Moerland"], "title": "Chargax: A JAX Accelerated EV Charging Simulator", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": "Accepted at RLC 2025", "summary": "Deep Reinforcement Learning can play a key role in addressing sustainable\nenergy challenges. For instance, many grid systems are heavily congested,\nhighlighting the urgent need to enhance operational efficiency. However,\nreinforcement learning approaches have traditionally been slow due to the high\nsample complexity and expensive simulation requirements. While recent works\nhave effectively used GPUs to accelerate data generation by converting\nenvironments to JAX, these works have largely focussed on classical toy\nproblems. This paper introduces Chargax, a JAX-based environment for realistic\nsimulation of electric vehicle charging stations designed for accelerated\ntraining of RL agents. We validate our environment in a variety of scenarios\nbased on real data, comparing reinforcement learning agents against baselines.\nChargax delivers substantial computational performance improvements of over\n100x-1000x over existing environments. Additionally, Chargax' modular\narchitecture enables the representation of diverse real-world charging station\nconfigurations.", "AI": {"tldr": "The paper introduces Chargax, a JAX-based system for simulating EV charging stations, making RL training over 100x faster.", "motivation": "To overcome the inefficiencies and bottlenecks caused by high sample complexity and expensive simulations in RL approaches addressing sustainable energy challenges.", "method": "Chargax leverages JAX for accelerated and modular simulation of electric vehicle charging stations, enabling faster training of RL agents.", "result": "Chargax achieves substantial computational speed improvements, delivering 100x-1000x faster training and supports diverse configurations in realistic scenarios.", "conclusion": "Chargax enhances RL application in sustainable energy systems by combining computational efficiency and environmental realism, thereby tackling operational challenges in energy grids."}}
{"id": "2507.01752", "pdf": "https://arxiv.org/pdf/2507.01752", "abs": "https://arxiv.org/abs/2507.01752", "authors": ["Ismail Labiad", "Mathurin Videau", "Matthieu Kowalski", "Marc Schoenauer", "Alessandro Leite", "Julia Kempe", "Olivier Teytaud"], "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.", "AI": {"tldr": "Gradient-based optimization faces privacy and overfitting issues despite its efficiency. BBoxER, a black-box evolutionary optimization method, addresses these concerns for post-training large language models (LLMs) while improving performance.", "motivation": "The need to address privacy, security, and overfitting issues in gradient-based optimization, while providing scalable solutions for restricted or adversarial environments.", "method": "Developed BBoxER, which leverages an evolutionary black-box approach with an information bottleneck for implicit data compression and theoretical guarantees.", "result": "Experimental results showed that BBoxER improves reasoning task performance, offers robustness, generalization, and privacy in LLMs.", "conclusion": "BBoxER is a modular, lightweight enhancement suitable for LLM scenarios requiring privacy preservation, scalability, and reliable generalization."}}
{"id": "2507.01544", "pdf": "https://arxiv.org/pdf/2507.01544", "abs": "https://arxiv.org/abs/2507.01544", "authors": ["Benjamin Feuer", "Lennart Purucker", "Oussama Elachqar", "Chinmay Hegde"], "title": "MARVIS: Modality Adaptive Reasoning over VISualizations", "categories": ["cs.LG"], "comment": null, "summary": "Scientific applications of machine learning often rely on small, specialized\nmodels tuned to particular domains. Such models often achieve excellent\nperformance, but lack flexibility. Foundation models offer versatility, but\ntypically underperform specialized approaches, especially on non-traditional\nmodalities and long-tail domains. We propose MARVIS (Modality Adaptive\nReasoning over VISualizations), a training-free method that enables even small\nvision-language models to predict any data modality with high accuracy. MARVIS\ntransforms latent embedding spaces into visual representations and then\nleverages the spatial and fine-grained reasoning skills of VLMs to successfully\ninterpret and utilize them. MARVIS achieves competitive performance on vision,\naudio, biological, and tabular domains using a single 3B parameter model,\nachieving results that beat Gemini by 16\\% on average and approach specialized\nmethods, without exposing personally identifiable information (P.I.I.) or\nrequiring any domain-specific training. We open source our code and datasets at\nhttps://github.com/penfever/marvis", "AI": {"tldr": "MARVIS is a training-free method enabling small vision-language models to handle various data modalities efficiently by converting latent embeddings into visual formats.", "motivation": "The paper addresses the trade-off between the flexibility of foundation models and the high accuracy of specialized machine-learning models for domain-specific tasks.", "method": "MARVIS involves transforming latent embeddings into visual representations, leveraging vision-language models' reasoning skills across diverse modalities.", "result": "MARVIS demonstrated competitive performance across vision, audio, biological, and tabular data types, surpassing Gemini by 16% on average, while requiring no domain-specific training.", "conclusion": "MARVIS bridges the flexibility-accuracy gap, providing a training-free and adaptable solution for multi-modality data interpretation using small vision-language models."}}
{"id": "2507.01673", "pdf": "https://arxiv.org/pdf/2507.01673", "abs": "https://arxiv.org/abs/2507.01673", "authors": ["Muzammil Behzad"], "title": "Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition (FER) in 3D and 4D domains presents a\nsignificant challenge in affective computing due to the complexity of spatial\nand temporal facial dynamics. Its success is crucial for advancing applications\nin human behavior understanding, healthcare monitoring, and human-computer\ninteraction. In this work, we propose FACET-VLM, a vision-language framework\nfor 3D/4D FER that integrates multiview facial representation learning with\nsemantic guidance from natural language prompts. FACET-VLM introduces three key\ncomponents: Cross-View Semantic Aggregation (CVSA) for view-consistent fusion,\nMultiview Text-Guided Fusion (MTGF) for semantically aligned facial emotions,\nand a multiview consistency loss to enforce structural coherence across views.\nOur model achieves state-of-the-art accuracy across multiple benchmarks,\nincluding BU-3DFE, Bosphorus, BU-4DFE, and BP4D-Spontaneous. We further extend\nFACET-VLM to 4D micro-expression recognition (MER) on the 4DME dataset,\ndemonstrating strong performance in capturing subtle, short-lived emotional\ncues. The extensive experimental results confirm the effectiveness and\nsubstantial contributions of each individual component within the framework.\nOverall, FACET-VLM offers a robust, extensible, and high-performing solution\nfor multimodal FER in both posed and spontaneous settings.", "AI": {"tldr": "The paper introduces FACET-VLM, a vision-language framework, achieving state-of-the-art results in 3D/4D facial expression and micro-expression recognition.", "motivation": "To address challenges in recognizing complex spatial and temporal facial dynamics, which are crucial for applications in behavior understanding, healthcare monitoring, and human-computer interaction.", "method": "The paper proposes FACET-VLM, integrating multiview facial representation with language prompts. Key methods include Cross-View Semantic Aggregation (CVSA) for fusing views, Multiview Text-Guided Fusion (MTGF) for semantic alignment, and multiview consistency loss for structural coherence.", "result": "The proposed model achieves state-of-the-art accuracy on benchmarks like BU-3DFE and BP4D-Spontaneous, and shows strong performance for 4D micro-expression recognition on the 4DME dataset.", "conclusion": "FACET-VLM is a high-performing and extensible solution for multimodal facial expression recognition in both posed and spontaneous scenarios, validated through extensive experiments."}}
{"id": "2507.01951", "pdf": "https://arxiv.org/pdf/2507.01951", "abs": "https://arxiv.org/abs/2507.01951", "authors": ["Zixiao Wang", "Yuxin Wang", "Xiaorui Wang", "Mengting Xing", "Jie Gao", "Jianjun Xu", "Guangcan Liu", "Chenhui Jin", "Zhuo Wang", "Shengzhuo Zhang", "Hongtao Xie"], "title": "Test-Time Scaling with Reflective Generative Model", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.", "AI": {"tldr": "MetaStone-S1 is a reflective generative model using a self-supervised process reward model (SPRM) to achieve OpenAI o3's performance efficiently, with only 32 billion parameters. It offers controllable reasoning modes and follows a test time scaling law.", "motivation": "The authors aimed to develop a generative model that combines efficient reasoning with high performance and self-supervised learning to match or rival existing models while being more resource-efficient.", "method": "The MetaStone-S1 uses a shared backbone network with task-specific heads for token prediction and process scoring, integrating a policy and process reward model (PRM) into a single interface without requiring additional process annotations. This reduces PRM parameters by over 99% and enables test time scaling (TTS) with varying reasoning effort modes.", "result": "MetaStone-S1 demonstrated performance comparable to OpenAI-o3-mini with a smaller parameter count (32 billion). A scaling law was also identified, linking thinking computation to TTS performance.", "conclusion": "The MetaStone-S1 is an efficient and open-source reflective generative model achieving high performance with controllable reasoning efforts, supporting broader research innovation."}}
{"id": "2507.01711", "pdf": "https://arxiv.org/pdf/2507.01711", "abs": "https://arxiv.org/abs/2507.01711", "authors": ["Mingfu Yan", "Jiancheng Huang", "Yifan Liu", "Shifeng Chen"], "title": "Component Adaptive Clustering for Generalized Category Discovery", "categories": ["cs.CV"], "comment": "Accepted by IEEE ICME 2025", "summary": "Generalized Category Discovery (GCD) tackles the challenging problem of\ncategorizing unlabeled images into both known and novel classes within a\npartially labeled dataset, without prior knowledge of the number of unknown\ncategories. Traditional methods often rely on rigid assumptions, such as\npredefining the number of classes, which limits their ability to handle the\ninherent variability and complexity of real-world data. To address these\nshortcomings, we propose AdaGCD, a cluster-centric contrastive learning\nframework that incorporates Adaptive Slot Attention (AdaSlot) into the GCD\nframework. AdaSlot dynamically determines the optimal number of slots based on\ndata complexity, removing the need for predefined slot counts. This adaptive\nmechanism facilitates the flexible clustering of unlabeled data into known and\nnovel categories by dynamically allocating representational capacity. By\nintegrating adaptive representation with dynamic slot allocation, our method\ncaptures both instance-specific and spatially clustered features, improving\nclass discovery in open-world scenarios. Extensive experiments on public and\nfine-grained datasets validate the effectiveness of our framework, emphasizing\nthe advantages of leveraging spatial local information for category discovery\nin unlabeled image datasets.", "AI": {"tldr": "The paper proposes AdaGCD, a contrastive learning-based framework with adaptive slot attention, to categorize known and unknown classes in image datasets without predefined class numbers.", "motivation": "Traditional GCD methods rely on rigid assumptions like predefining the number of classes, which limits their adaptability to complex real-world data.", "method": "AdaGCD incorporates Adaptive Slot Attention (AdaSlot) which dynamically determines the optimal number of clusters, removing predefined slot requirements, while leveraging spatial and instance-specific features for class discovery.", "result": "Experiments on public and fine-grained datasets demonstrate the framework\u2019s effectiveness in categorizing images, highlighting its ability to leverage spatial local information.", "conclusion": "AdaGCD successfully adapts to varying data complexities in open-world scenarios, overcoming existing limitations in category discovery for unlabeled datasets."}}
{"id": "2507.01559", "pdf": "https://arxiv.org/pdf/2507.01559", "abs": "https://arxiv.org/abs/2507.01559", "authors": ["Lapo Frati", "Neil Traft", "Jeff Clune", "Nick Cheney"], "title": "How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Recent work in continual learning has highlighted the beneficial effect of\nresampling weights in the last layer of a neural network (``zapping\"). Although\nempirical results demonstrate the effectiveness of this approach, the\nunderlying mechanisms that drive these improvements remain unclear. In this\nwork, we investigate in detail the pattern of learning and forgetting that take\nplace inside a convolutional neural network when trained in challenging\nsettings such as continual learning and few-shot transfer learning, with\nhandwritten characters and natural images. Our experiments show that models\nthat have undergone zapping during training more quickly recover from the shock\nof transferring to a new domain. Furthermore, to better observe the effect of\ncontinual learning in a multi-task setting we measure how each individual task\nis affected. This shows that, not only zapping, but the choice of optimizer can\nalso deeply affect the dynamics of learning and forgetting, causing complex\npatterns of synergy/interference between tasks to emerge when the model learns\nsequentially at transfer time.", "AI": {"tldr": "This paper investigates the effects of \"zapping\" (resampling weights of the last layer) in continual and few-shot transfer learning scenarios. Results imply that zapping accelerates recovery when transferring to new domains and interacts dynamically with task performance and optimizer choice.", "motivation": "The motivation is to understand the mechanisms behind the observed benefits of weight resampling techniques (zapping) in challenging machine learning tasks like continual learning and few-shot transfer learning.", "method": "The study examines neural networks during continual learning, investigating the interplay of zapping and optimizer choices, and evaluates their impact on transfer shock recovery and multi-task performance using datasets of handwritten characters and natural images.", "result": "Models that used zapping recover faster after domain transfer. The choice of optimizer significantly influences task synergy and interference during sequential learning.", "conclusion": "The paper highlights zapping and optimizer choice as key factors in neural network learning dynamics, with implications for handling continual and transfer learning challenges."}}
{"id": "2507.01712", "pdf": "https://arxiv.org/pdf/2507.01712", "abs": "https://arxiv.org/abs/2507.01712", "authors": ["Xinle Tian", "Matthew Nunes", "Emiko Dupont", "Shaunagh Downing", "Freddie Lichtenstein", "Matt Burns"], "title": "Using Wavelet Domain Fingerprints to Improve Source Camera Identification", "categories": ["cs.CV", "eess.IV", "stat.AP"], "comment": null, "summary": "Camera fingerprint detection plays a crucial role in source identification\nand image forensics, with wavelet denoising approaches proving to be\nparticularly effective in extracting sensor pattern noise (SPN). In this\narticle, we propose a modification to wavelet-based SPN extraction. Rather than\nconstructing the fingerprint as an image, we introduce the notion of a wavelet\ndomain fingerprint. This avoids the final inversion step of the denoising\nalgorithm and allows fingerprint comparisons to be made directly in the wavelet\ndomain. As such, our modification streamlines the extraction and comparison\nprocess. Experimental results on real-world datasets demonstrate that our\nmethod not only achieves higher detection accuracy but can also significantly\nimprove processing speed.", "AI": {"tldr": "The paper presents an enhancement to wavelet-based sensor pattern noise (SPN) extraction by operating directly in the wavelet domain, leading to greater accuracy and faster processing.", "motivation": "Improving camera fingerprinting for source identification and image forensics using SPN extraction methods, particularly in addressing limitations of current wavelet denoising approaches.", "method": "Modified wavelet-based SPN extraction avoiding the inversion step, allowing comparisons directly in the wavelet domain for streamlining the process.", "result": "The proposed method achieves both higher detection accuracy and significant improvements in processing speed on real-world datasets.", "conclusion": "The modification to the wavelet-based approach enhances both efficiency and effectiveness in camera fingerprint detection, benefiting forensic applications."}}
{"id": "2507.01581", "pdf": "https://arxiv.org/pdf/2507.01581", "abs": "https://arxiv.org/abs/2507.01581", "authors": ["Masood Jan", "Wafa Njima", "Xun Zhang"], "title": "A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning", "categories": ["cs.LG", "cs.CR", "eess.SP"], "comment": null, "summary": "Location information serves as the fundamental element for numerous Internet\nof Things (IoT) applications. Traditional indoor localization techniques often\nproduce significant errors and raise privacy concerns due to centralized data\ncollection. In response, Machine Learning (ML) techniques offer promising\nsolutions by capturing indoor environment variations. However, they typically\nrequire central data aggregation, leading to privacy, bandwidth, and server\nreliability issues. To overcome these challenges, in this paper, we propose a\nFederated Learning (FL)-based approach for dynamic indoor localization using a\nDeep Neural Network (DNN) model. Experimental results show that FL has the\nnearby performance to Centralized Model (CL) while keeping the data privacy,\nbandwidth efficiency and server reliability. This research demonstrates that\nour proposed FL approach provides a viable solution for privacy-enhanced indoor\nlocalization, paving the way for advancements in secure and efficient indoor\nlocalization systems.", "AI": {"tldr": "This paper presents a Federated Learning (FL)-based indoor localization method using a Deep Neural Network (DNN) to enhance privacy and maintain performance levels close to centralized models.", "motivation": "To address the significant errors, privacy issues, bandwidth inefficiency, and server reliability challenges of traditional and machine learning-based indoor localization systems.", "method": "A Federated Learning (FL) approach is implemented using a Deep Neural Network (DNN) for dynamic indoor localization.", "result": "The proposed FL approach achieves performance comparable to centralized models while improving privacy, bandwidth efficiency, and server reliability.", "conclusion": "The FL-based method offers a secure and efficient solution for indoor localization, showcasing its potential for privacy-enhanced IoT applications."}}
{"id": "2507.01721", "pdf": "https://arxiv.org/pdf/2507.01721", "abs": "https://arxiv.org/abs/2507.01721", "authors": ["Zhongwen Zhang", "Yuri Boykov"], "title": "Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation", "categories": ["cs.CV"], "comment": "published at CVPR 2025", "summary": "We consider weakly supervised segmentation where only a fraction of pixels\nhave ground truth labels (scribbles) and focus on a self-labeling approach\noptimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled\npixels. While WSSS methods can directly optimize such losses via gradient\ndescent, prior work suggests that higher-order optimization can improve network\ntraining by introducing hidden pseudo-labels and powerful CRF sub-problem\nsolvers, e.g. graph cut. However, previously used hard pseudo-labels can not\nrepresent class uncertainty or errors, which motivates soft self-labeling. We\nderive a principled auxiliary loss and systematically evaluate standard and new\nCRF relaxations (convex and non-convex), neighborhood systems, and terms\nconnecting network predictions with soft pseudo-labels. We also propose a\ngeneral continuous sub-problem solver. Using only standard architectures, soft\nself-labeling consistently improves scribble-based training and outperforms\nsignificantly more complex specialized WSSS systems. It can outperform full\npixel-precise supervision. Our general ideas apply to other weakly-supervised\nproblems/systems.", "AI": {"tldr": "The paper proposes a soft self-labeling approach combined with CRF relaxations to improve weakly supervised segmentation, surpassing both traditional supervision and complex WSSS systems.", "motivation": "Address limitations in weakly supervised segmentation, particularly overcoming representation errors and uncertainties inherent in hard pseudo-labeling.", "method": "Develop an auxiliary loss with both standard and novel CRF relaxations, and use soft pseudo-labels integrated with network predictions for training. Incorporate a general continuous sub-problem solver.", "result": "Soft self-labeling improves scribble-based training, surpasses pixel-precise supervision, and outperforms more complex WSSS systems.", "conclusion": "The authors highlight the superiority of soft self-labeling for weakly supervised systems, advocating its adaptability to a wider range of problems."}}
{"id": "2507.01598", "pdf": "https://arxiv.org/pdf/2507.01598", "abs": "https://arxiv.org/abs/2507.01598", "authors": ["Naoki Sato", "Hiroki Naganuma", "Hideaki Iiduka"], "title": "Analysis of Muon's Convergence and Critical Batch Size", "categories": ["cs.LG"], "comment": null, "summary": "This paper presents a theoretical analysis of Muon, a new optimizer that\nleverages the inherent matrix structure of neural network parameters. We\nprovide convergence proofs for four practical variants of Muon: with and\nwithout Nesterov momentum, and with and without weight decay. We then show that\nadding weight decay leads to strictly tighter bounds on both the parameter and\ngradient norms, and we clarify the relationship between the weight decay\ncoefficient and the learning rate. Finally, we derive Muon's critical batch\nsize minimizing the stochastic first-order oracle (SFO) complexity, which is\nthe stochastic computational cost, and validate our theoretical findings with\nexperiments.", "AI": {"tldr": "The paper introduces Muon, a matrix-structured neural network optimizer, and provides theoretical convergence proofs for its variants while highlighting weight decay benefits and experimental validation.", "motivation": "To improve the efficiency and theoretical understanding of neural network optimizers by leveraging the matrix structure inherent in neural parameters.", "method": "The paper theoretically analyzes Muon, deriving convergence proofs for its variants, studying the effects of weight decay, clarifying the interplay between weight decay and learning rate, and deriving the critical batch size for computational cost minimization.", "result": "Muon demonstrates tighter bounds on parameter and gradient norms when weight decay is applied, and the critical batch size minimizing stochastic computational cost is derived and validated experimentally.", "conclusion": "Muon showcases theoretical and practical advantages as a neural network optimizer, with significant implications for computational efficiency and optimization strategies."}}
{"id": "2507.01413", "pdf": "https://arxiv.org/pdf/2507.01413", "abs": "https://arxiv.org/abs/2507.01413", "authors": ["Kushal Agrawal", "Verona Teo", "Juan J. Vazquez", "Sudarsh Kunnavakkam", "Vishak Srikanth", "Andy Liu"], "title": "Evaluating LLM Agent Collusion in Double Auctions", "categories": ["cs.GT", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities as\nautonomous agents with rapidly expanding applications in various domains. As\nthese agents increasingly engage in socioeconomic interactions, identifying\ntheir potential for undesirable behavior becomes essential. In this work, we\nexamine scenarios where they can choose to collude, defined as secretive\ncooperation that harms another party. To systematically study this, we\ninvestigate the behavior of LLM agents acting as sellers in simulated\ncontinuous double auction markets. Through a series of controlled experiments,\nwe analyze how parameters such as the ability to communicate, choice of model,\nand presence of environmental pressures affect the stability and emergence of\nseller collusion. We find that direct seller communication increases collusive\ntendencies, the propensity to collude varies across models, and environmental\npressures, such as oversight and urgency from authority figures, influence\ncollusive behavior. Our findings highlight important economic and ethical\nconsiderations for the deployment of LLM-based market agents.", "AI": {"tldr": "This paper examines whether large language models (LLMs) acting as market agents can engage in unethical collusion during socioeconomic interactions.", "motivation": "The motivation is to assess the potential for undesirable, secretive cooperation (collusion) among LLMs acting autonomously in economic settings, given their expanding use.", "method": "Simulated continuous double auction markets were created, where LLM agents performed as sellers under varying conditions like communication, model type, and environmental pressures.", "result": "Findings show that communication between sellers increases collusion, different models have varying collusive tendencies, and factors like oversight and authority influence this behavior.", "conclusion": "LLM market agents must be carefully assessed for economic and ethical concerns, as their deployment may inadvertently lead to harmful collusive practices."}}
{"id": "2507.01722", "pdf": "https://arxiv.org/pdf/2507.01722", "abs": "https://arxiv.org/abs/2507.01722", "authors": ["Enrico Cassano", "Riccardo Renzulli", "Andrea Bragagnolo", "Marco Grangetto"], "title": "When Does Pruning Benefit Vision Representations?", "categories": ["cs.CV"], "comment": null, "summary": "Pruning is widely used to reduce the complexity of deep learning models, but\nits effects on interpretability and representation learning remain poorly\nunderstood. This paper investigates how pruning influences vision models across\nthree key dimensions: (i) interpretability, (ii) unsupervised object discovery,\nand (iii) alignment with human perception. We first analyze different vision\nnetwork architectures to examine how varying sparsity levels affect feature\nattribution interpretability methods. Additionally, we explore whether pruning\npromotes more succinct and structured representations, potentially improving\nunsupervised object discovery by discarding redundant information while\npreserving essential features. Finally, we assess whether pruning enhances the\nalignment between model representations and human perception, investigating\nwhether sparser models focus on more discriminative features similarly to\nhumans. Our findings also reveal the presence of sweet spots, where sparse\nmodels exhibit higher interpretability, downstream generalization and human\nalignment. However, these spots highly depend on the network architectures and\ntheir size in terms of trainable parameters. Our results suggest a complex\ninterplay between these three dimensions, highlighting the importance of\ninvestigating when and how pruning benefits vision representations.", "AI": {"tldr": "The study examines how pruning affects vision models concerning interpretability, object discovery, and alignment with human perception, finding \"sweet spots\" where sparse models improve representations but depend on architecture and size.", "motivation": "Deep learning models are increasingly complex; pruning can reduce their size and computational demands, but its impact on interpretability and representation quality needs exploration.", "method": "The authors analyze how varying levels of sparsity influence interpretability, structured representations, object discovery, and alignment with human perception across different vision architectures.", "result": "Sparse models exhibit improved interpretability, generalization, and alignment with human perception under specific conditions, such as network architecture and parameter size.", "conclusion": "The study highlights a nuanced relationship between sparsity and representation quality, emphasizing the need to fine-tune pruning to optimize interpretability, generalization, and alignment with human perception."}}
{"id": "2507.01636", "pdf": "https://arxiv.org/pdf/2507.01636", "abs": "https://arxiv.org/abs/2507.01636", "authors": ["Ghasem Alipoor", "Karl Skretting"], "title": "Kernel Recursive Least Squares Dictionary Learning Algorithm", "categories": ["cs.LG", "eess.SP"], "comment": "Published in Digital Signal Processing, Volume 141, 2023. DOI:\n  https://doi.org/10.1016/j.dsp.2023.104159 12 pages, 8 figures. Code and data\n  available at: https://github.com/G-Alipoor/kernel-rls-dictionary-learning", "summary": "We propose an efficient online dictionary learning algorithm for kernel-based\nsparse representations. In this framework, input signals are nonlinearly mapped\nto a high-dimensional feature space and represented sparsely using a virtual\ndictionary. At each step, the dictionary is updated recursively using a novel\nalgorithm based on the recursive least squares (RLS) method. This update\nmechanism works with single samples or mini-batches and maintains low\ncomputational complexity. Experiments on four datasets across different domains\nshow that our method not only outperforms existing online kernel dictionary\nlearning approaches but also achieves classification accuracy close to that of\nbatch-trained models, while remaining significantly more efficient.", "AI": {"tldr": "The paper introduces an online dictionary learning algorithm for kernel-based sparse representations with a focus on efficiency and performance.", "motivation": "To enhance the efficiency and scalability of kernel-based sparse representation methods while maintaining high classification accuracy.", "method": "The algorithm uses a high-dimensional feature space for sparse signal representation and updates the dictionary recursively with a novel Recursive Least Squares (RLS)-based method, suitable for single samples or mini-batches.", "result": "Experiments on four datasets show the algorithm outperforms existing methods in online kernel dictionary learning and achieves classification accuracy comparable to batch-trained models at lower computational costs.", "conclusion": "The proposed method combines efficiency with strong performance, marking a significant advancement in online kernel dictionary learning approaches."}}
{"id": "2507.01418", "pdf": "https://arxiv.org/pdf/2507.01418", "abs": "https://arxiv.org/abs/2507.01418", "authors": ["Inyoung Cheong", "Alicia Guo", "Mina Lee", "Zhehui Liao", "Kowe Kadoma", "Dongyoung Go", "Joseph Chee Chang", "Peter Henderson", "Mor Naaman", "Amy X. Zhang"], "title": "Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing", "categories": ["cs.CY", "cs.AI", "H.5.2; I.2"], "comment": "Presented at CHIWORK 2025 Workshop on Generative AI Disclosure,\n  Ownership, and Accountability in Co-Creative Domains", "summary": "As AI integrates in various types of human writing, calls for transparency\naround AI assistance are growing. However, if transparency operates on uneven\nground and certain identity groups bear a heavier cost for being honest, then\nthe burden of openness becomes asymmetrical. This study investigates how AI\ndisclosure statement affects perceptions of writing quality, and whether these\neffects vary by the author's race and gender. Through a large-scale controlled\nexperiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated\na single human-written news article while disclosure statements and author\ndemographics were systematically varied. This approach reflects how both human\nand algorithmic decisions now influence access to opportunities (e.g., hiring,\npromotion) and social recognition (e.g., content recommendation algorithms). We\nfind that both human and LLM raters consistently penalize disclosed AI use.\nHowever, only LLM raters exhibit demographic interaction effects: they favor\narticles attributed to women or Black authors when no disclosure is present.\nBut these advantages disappear when AI assistance is revealed. These findings\nilluminate the complex relationships between AI disclosure and author identity,\nhighlighting disparities between machine and human evaluation patterns.", "AI": {"tldr": "The paper explores the effects of AI disclosure in writing on perceptions of quality, focusing on interactions with the author's race and gender.", "motivation": "To investigate how disclosure of AI assistance in writing impacts the evaluation of quality, and whether this effect varies based on author identity.", "method": "A large-scale experiment was conducted with human (n=1,970) and LLM (n=2,520) raters who were asked to evaluate a single human-written article, varying the AI disclosure and author demographics.", "result": "AI use disclosure was penalized by both human and LLM raters. Additionally, LLM raters showed bias: they favored women or Black authors without AI disclosure, but this advantage disappeared with disclosure.", "conclusion": "The study reveals complexities in how AI disclosure intersects with author identity, and highlights differences in evaluation biases between human and machine raters."}}
{"id": "2507.01644", "pdf": "https://arxiv.org/pdf/2507.01644", "abs": "https://arxiv.org/abs/2507.01644", "authors": ["Miguel O'Malley"], "title": "Dance Dance ConvLSTM", "categories": ["cs.LG"], "comment": "15 pages, 9 figures, 4 tables", "summary": "\\textit{Dance Dance Revolution} is a rhythm game consisting of songs and\naccompanying choreography, referred to as charts. Players press arrows on a\ndevice referred to as a dance pad in time with steps determined by the song's\nchart. In 2017, the authors of Dance Dance Convolution (DDC) developed an\nalgorithm for the automatic generation of \\textit{Dance Dance Revolution}\ncharts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM\n(DDCL), a new method for the automatic generation of DDR charts using a\nConvLSTM based model, which improves upon the DDC methodology and substantially\nincreases the accuracy of chart generation.", "AI": {"tldr": "This paper introduces Dance Dance ConvLSTM (DDCL), a method leveraging ConvLSTM models to enhance the accuracy of automatic Dance Dance Revolution chart generation.", "motivation": "To improve upon the existing methodology of automatic chart generation in Dance Dance Revolution, as proposed by the earlier Dance Dance Convolution (DDC) algorithm, especially in terms of accuracy.", "method": "The paper utilizes a ConvLSTM-based model as an enhancement over the CNN-LSTM architecture employed in the 2017 DDC algorithm.", "result": "The new ConvLSTM-based model demonstrated a substantial increase in accuracy for automatically generating Dance Dance Revolution charts.", "conclusion": "Dance Dance ConvLSTM (DDCL) successfully builds on and surpasses the original DDC methodology, showcasing improved performance in chart generation for rhythm games."}}
{"id": "2507.01737", "pdf": "https://arxiv.org/pdf/2507.01737", "abs": "https://arxiv.org/abs/2507.01737", "authors": ["Lin Wu", "Zhixiang Chen", "Jianglin Lan"], "title": "HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Generating realistic 3D human-object interactions (HOIs) remains a\nchallenging task due to the difficulty of modeling detailed interaction\ndynamics. Existing methods treat human and object motions independently,\nresulting in physically implausible and causally inconsistent behaviors. In\nthis work, we present HOI-Dyn, a novel framework that formulates HOI generation\nas a driver-responder system, where human actions drive object responses. At\nthe core of our method is a lightweight transformer-based interaction dynamics\nmodel that explicitly predicts how objects should react to human motion. To\nfurther enforce consistency, we introduce a residual-based dynamics loss that\nmitigates the impact of dynamics prediction errors and prevents misleading\noptimization signals. The dynamics model is used only during training,\npreserving inference efficiency. Through extensive qualitative and quantitative\nexperiments, we demonstrate that our approach not only enhances the quality of\nHOI generation but also establishes a feasible metric for evaluating the\nquality of generated interactions.", "AI": {"tldr": "HOI-Dyn, a transformer-based model, enhances 3D human-object interaction realism by modeling dynamics between humans and objects through a novel driver-responder framework.", "motivation": "Existing approaches fail to achieve realistic physical and causal 3D human-object interactions due to the independent treatment of human and object motions.", "method": "The proposed HOI-Dyn employs a transformer-based interaction dynamics model, which predicts object behavior in response to human motion, complemented by a residual-based dynamics loss to address prediction errors.", "result": "The experiments show improved quality in human-object interaction generation and introduce a new metric for evaluating interaction quality.", "conclusion": "HOI-Dyn successfully offers a more consistent methodology for generating realistic human-object interaction and enhances evaluation practices."}}
{"id": "2507.01649", "pdf": "https://arxiv.org/pdf/2507.01649", "abs": "https://arxiv.org/abs/2507.01649", "authors": ["Yoav Gelberg", "Yam Eitan", "Aviv Navon", "Aviv Shamsian", "Theo", "Putterman", "Michael Bronstein", "Haggai Maron"], "title": "GradMetaNet: An Equivariant Architecture for Learning on Gradients", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Gradients of neural networks encode valuable information for optimization,\nediting, and analysis of models. Therefore, practitioners often treat gradients\nas inputs to task-specific algorithms, e.g. for pruning or optimization. Recent\nworks explore learning algorithms that operate directly on gradients but use\narchitectures that are not specifically designed for gradient processing,\nlimiting their applicability. In this paper, we present a principled approach\nfor designing architectures that process gradients. Our approach is guided by\nthree principles: (1) equivariant design that preserves neuron permutation\nsymmetries, (2) processing sets of gradients across multiple data points to\ncapture curvature information, and (3) efficient gradient representation\nthrough rank-1 decomposition. Based on these principles, we introduce\nGradMetaNet, a novel architecture for learning on gradients, constructed from\nsimple equivariant blocks. We prove universality results for GradMetaNet, and\nshow that previous approaches cannot approximate natural gradient-based\nfunctions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness\non a diverse set of gradient-based tasks on MLPs and transformers, such as\nlearned optimization, INR editing, and estimating loss landscape curvature.", "AI": {"tldr": "The paper introduces GradMetaNet, a novel architecture specifically designed for processing gradients of neural networks, guided by principles of equivariance, curvature-aware processing, and efficient representation.", "motivation": "Existing approaches to gradient processing in neural networks lack architectures designed specifically for handling gradients, reducing their effectiveness and adaptability.", "method": "GradMetaNet utilizes three design principles: equivariance to preserve permutation symmetries, multi-data processing for curvature information, and rank-1 decomposition for efficient representation.", "result": "GradMetaNet was demonstrated to outperform previous methods in tasks like learned optimization, INR editing, and estimation of loss landscape curvature on MLPs and transformers.", "conclusion": "GradMetaNet is a universal and effective architecture for gradient-based tasks, surpassing prior methods' limitations and enabling advanced processing and application in neural networks."}}
{"id": "2507.01738", "pdf": "https://arxiv.org/pdf/2507.01738", "abs": "https://arxiv.org/abs/2507.01738", "authors": ["Ming Dai", "Wenxuan Cheng", "Jiang-jiang Liu", "Sen Yang", "Wenxiao Cai", "Yanpeng Sun", "Wankou Yang"], "title": "DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Referring Image Segmentation (RIS) is a challenging task that aims to segment\nobjects in an image based on natural language expressions. While prior studies\nhave predominantly concentrated on improving vision-language interactions and\nachieving fine-grained localization, a systematic analysis of the fundamental\nbottlenecks in existing RIS frameworks remains underexplored. To bridge this\ngap, we propose DeRIS, a novel framework that decomposes RIS into two key\ncomponents: perception and cognition. This modular decomposition facilitates a\nsystematic analysis of the primary bottlenecks impeding RIS performance. Our\nfindings reveal that the predominant limitation lies not in perceptual\ndeficiencies, but in the insufficient multi-modal cognitive capacity of current\nmodels. To mitigate this, we propose a Loopback Synergy mechanism, which\nenhances the synergy between the perception and cognition modules, thereby\nenabling precise segmentation while simultaneously improving robust image-text\ncomprehension. Additionally, we analyze and introduce a simple non-referent\nsample conversion data augmentation to address the long-tail distribution issue\nrelated to target existence judgement in general scenarios. Notably, DeRIS\ndemonstrates inherent adaptability to both non- and multi-referents scenarios\nwithout requiring specialized architectural modifications, enhancing its\ngeneral applicability. The codes and models are available at\nhttps://github.com/Dmmm1997/DeRIS.", "AI": {"tldr": "The paper introduces DeRIS, a new framework for Referring Image Segmentation (RIS) that decomposes the task into perception and cognition modules for systematic bottleneck analysis and improves multi-modal cognitive capacities.", "motivation": "Existing RIS frameworks focus on vision-language interaction but lack a systematic analysis of their primary limitations, affecting performance.", "method": "DeRIS decomposes RIS into perception and cognition modules and includes a Loopback Synergy mechanism to synergize these components for better segmentation and image-text comprehension. It also introduces a data augmentation technique for addressing long-tail distribution issues.", "result": "DeRIS enhances RIS performance by improving multi-modal cognitive capabilities and adapts to both single and multi-referent scenarios without needing architectural changes.", "conclusion": "The proposed decomposition framework and Loopback Synergy mechanism address fundamental RIS limitations, ensuring better adaptability and segmentation performance."}}
{"id": "2507.01663", "pdf": "https://arxiv.org/pdf/2507.01663", "abs": "https://arxiv.org/abs/2507.01663", "authors": ["Zhenyu Han", "Ansheng You", "Haibo Wang", "Kui Luo", "Guang Yang", "Wenqi Shi", "Menglong Chen", "Sicheng Zhang", "Zeshun Lan", "Chunshi Deng", "Huazhong Ji", "Wenjie Liu", "Yu Huang", "Yixiang Zhang", "Chenyi Pan", "Jing Wang", "Xin Huang", "Chunsheng Li", "Jianping Wu"], "title": "AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has become a pivotal technology in the\npost-training phase of large language models (LLMs). Traditional task-colocated\nRL frameworks suffer from significant scalability bottlenecks, while\ntask-separated RL frameworks face challenges in complex dataflows and the\ncorresponding resource idling and workload imbalance. Moreover, most existing\nframeworks are tightly coupled with LLM training or inference engines, making\nit difficult to support custom-designed engines. To address these challenges,\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\npost-training. Specifically, we introduce a distributed data storage and\ntransfer module that provides a unified data management and fine-grained\nscheduling capability in a fully streamed manner. This architecture inherently\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\nengineered to minimize computational idleness by strategically deferring\nparameter update process within staleness thresholds. Finally, the core\ncapability of AsynFlow is architecturally decoupled from underlying training\nand inference engines and encapsulated by service-oriented user interfaces,\noffering a modular and customizable user experience. Extensive experiments\ndemonstrate an average of 1.59 throughput improvement compared with\nstate-of-the-art baseline. The presented architecture in this work provides\nactionable insights for next-generation RL training system designs.", "AI": {"tldr": "AsyncFlow is an asynchronous RL framework addressing scalability and resource inefficiencies in post-training for LLMs.", "motivation": "Existing RL frameworks tied to LLM post-training suffer from scalability issues and workload imbalance, requiring innovative solutions.", "method": "AsyncFlow introduces a distributed data module for streaming and a producer-consumer-based workflow ensuring automated pipeline overlapping and load balancing.", "result": "Experiments show AsyncFlow achieves a 1.59x throughput improvement over baseline frameworks.", "conclusion": "AsyncFlow offers efficient and modular RL post-training support with actionable insights for future RL system designs."}}
{"id": "2507.01744", "pdf": "https://arxiv.org/pdf/2507.01744", "abs": "https://arxiv.org/abs/2507.01744", "authors": ["Benjamin Jin", "Grant Mair", "Joanna M. Wardlaw", "Maria del C. Vald\u00e9s Hern\u00e1ndez"], "title": "Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans", "categories": ["cs.CV"], "comment": null, "summary": "Vision Transformers (ViTs) have gained significant popularity in the natural\nimage domain but have been less successful in 3D medical image segmentation.\nNevertheless, 3D ViTs are particularly interesting for large medical imaging\nvolumes due to their efficient self-supervised training within the masked\nautoencoder (MAE) framework, which enables the use of imaging data without the\nneed for expensive manual annotations. intracranial arterial calcification\n(IAC) is an imaging biomarker visible on routinely acquired CT scans linked to\nneurovascular diseases such as stroke and dementia, and automated IAC\nquantification could enable their large-scale risk assessment. We pre-train\nViTs with MAE and fine-tune them for IAC segmentation for the first time. To\ndevelop our models, we use highly heterogeneous data from a large clinical\ntrial, the third International Stroke Trial (IST-3). We evaluate key aspects of\nMAE pre-trained ViTs in IAC segmentation, and analyse the clinical\nimplications. We show: 1) our calibrated self-supervised ViT beats a strong\nsupervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial\nfor ViTs for IAC segmentation and interpolation upsampling with regular\nconvolutions is preferable to transposed convolutions for ViT-based models, and\n3) our ViTs increase robustness to higher slice thicknesses and improve risk\ngroup classification in a clinical scenario by 46%. Our code is available\nonline.", "AI": {"tldr": "The paper explores Vision Transformers (ViTs) within the masked autoencoder (MAE) framework for intracranial arterial calcification (IAC) segmentation, achieving better clinical applicability and performance over nnU-Net.", "motivation": "The paper aims to examine the potential of Vision Transformers in 3D medical image segmentation, especially for identifying IAC on CT scans, which is associated with neurovascular diseases like stroke and dementia.", "method": "The authors pre-train ViTs using a self-supervised MAE framework and fine-tune them for IAC segmentation. They leverage data from a large clinical trial (IST-3) and analyze various model configurations.", "result": "Their self-supervised ViT outperforms nnU-Net by 3.2 Dice points, demonstrates the importance of low patch sizes, and shows increased robustness to slice thickness variations. This also improves clinical risk group classification by 46%.", "conclusion": "ViTs trained in an MAE framework are effective for IAC segmentation, outperforming traditional convolutional methods, and show promising applications in clinical scenarios."}}
{"id": "2507.01747", "pdf": "https://arxiv.org/pdf/2507.01747", "abs": "https://arxiv.org/abs/2507.01747", "authors": ["Nora Gourmelon", "Marcel Dreier", "Martin Mayr", "Thorsten Seehaus", "Dakota Pyles", "Matthias Braun", "Andreas Maier", "Vincent Christlein"], "title": "SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery", "categories": ["cs.CV"], "comment": "in IEEE Transactions on Geoscience and Remote Sensing. arXiv admin\n  note: text overlap with arXiv:2501.05281", "summary": "Glaciers are losing ice mass at unprecedented rates, increasing the need for\naccurate, year-round monitoring to understand frontal ablation, particularly\nthe factors driving the calving process. Deep learning models can extract\ncalving front positions from Synthetic Aperture Radar imagery to track seasonal\nice losses at the calving fronts of marine- and lake-terminating glaciers. The\ncurrent state-of-the-art model relies on ImageNet-pretrained weights. However,\nthey are suboptimal due to the domain shift between the natural images in\nImageNet and the specialized characteristics of remote sensing imagery, in\nparticular for Synthetic Aperture Radar imagery. To address this challenge, we\npropose two novel self-supervised multimodal pretraining techniques that\nleverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14\nSentinel-2 images of Arctic glaciers, with one optical image per glacier in the\ndataset. Additionally, we introduce a novel hybrid model architecture that\ncombines a Swin Transformer encoder with a residual Convolutional Neural\nNetwork (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean\ndistance error of 293 m on the \"CAlving Fronts and where to Find thEm\" (CaFFe)\nbenchmark dataset, outperforming the prior best model by 67 m. Evaluating an\nensemble of the proposed model on a multi-annotator study of the benchmark\ndataset reveals a mean distance error of 75 m, approaching the human\nperformance of 38 m. This advancement enables precise monitoring of seasonal\nchanges in glacier calving fronts.", "AI": {"tldr": "The paper develops advanced models using self-supervised learning to monitor glacier calving fronts from remote sensing imagery, achieving near-human-level precision.", "motivation": "Glaciers are rapidly losing ice mass, necessitating more accurate tools for monitoring calving processes using specialized remote sensing imagery.", "method": "Two self-supervised multimodal pretraining techniques and a hybrid Swin Transformer-CNN architecture are introduced, trained on the SSL4SAR dataset containing Sentinel-1 and Sentinel-2 imagery.", "result": "The model achieves mean distance errors of 293 m and 75 m (ensemble), outperforming prior models by 67 m and nearing human performance (38 m).", "conclusion": "The proposed techniques enhance the precision of glacier monitoring, especially for seasonal changes in calving fronts."}}
{"id": "2507.01693", "pdf": "https://arxiv.org/pdf/2507.01693", "abs": "https://arxiv.org/abs/2507.01693", "authors": ["Adrians Skapars", "Edoardo Manino", "Youcheng Sun", "Lucas C. Cordeiro"], "title": "GPT, But Backwards: Exactly Inverting Language Model Outputs", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, ICML 2025 Workshop on Reliable and Responsible Foundation\n  Models", "summary": "While existing auditing techniques attempt to identify potential unwanted\nbehaviours in large language models (LLMs), we address the complementary\nforensic problem of reconstructing the exact input that led to an existing LLM\noutput - enabling post-incident analysis and potentially the detection of fake\noutput reports. We formalize exact input reconstruction as a discrete\noptimisation problem with a unique global minimum and introduce SODA, an\nefficient gradient-based algorithm that operates on a continuous relaxation of\nthe input search space with periodic restarts and parameter decay. Through\ncomprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we\ndemonstrate that SODA significantly outperforms existing approaches. We succeed\nin fully recovering 79.5% of shorter out-of-distribution inputs from next-token\nlogits, without a single false positive, but struggle to extract private\ninformation from the outputs of longer (15+ token) input sequences. This\nsuggests that standard deployment practices may currently provide adequate\nprotection against malicious use of our method. Our code is available at\nhttps://doi.org/10.5281/zenodo.15539879.", "AI": {"tldr": "The paper introduces SODA, a novel algorithm for reconstructing exact inputs that led to outputs from large language models (LLMs), surpassing existing methods in precision.", "motivation": "The authors aim to address the forensic challenge of reconstructing the input that generated a specific LLM output, an area not tackled by current auditing practices.", "method": "The authors formalize the input reconstruction as a discrete optimization problem with a unique global minimum. They introduce the gradient-based SODA algorithm utilizing continuous relaxation, periodic restarts, and parameter decay.", "result": "SODA achieves a recovery rate of 79.5% for shorter, out-of-distribution inputs from next-token logits, with no false positives. However, it is less effective for longer input sequences.", "conclusion": "Standard LLM deployment practices may adequately protect against abuses of this method, as SODA struggles to extract inputs from longer sequences. The research advances forensic capabilities in LLM auditing."}}
{"id": "2507.01756", "pdf": "https://arxiv.org/pdf/2507.01756", "abs": "https://arxiv.org/abs/2507.01756", "authors": ["Peng Zheng", "Junke Wang", "Yi Chang", "Yizhou Yu", "Rui Ma", "Zuxuan Wu"], "title": "Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis", "categories": ["cs.CV"], "comment": "accepted by iccv 2025", "summary": "Recent advances in large language models (LLMs) have spurred interests in\nencoding images as discrete tokens and leveraging autoregressive (AR)\nframeworks for visual generation. However, the quantization process in AR-based\nvisual generation models inherently introduces information loss that degrades\nimage fidelity. To mitigate this limitation, recent studies have explored to\nautoregressively predict continuous tokens. Unlike discrete tokens that reside\nin a structured and bounded space, continuous representations exist in an\nunbounded, high-dimensional space, making density estimation more challenging\nand increasing the risk of generating out-of-distribution artifacts. Based on\nthe above findings, this work introduces DisCon (Discrete-Conditioned\nContinuous Autoregressive Model), a novel framework that reinterprets discrete\ntokens as conditional signals rather than generation targets. By modeling the\nconditional probability of continuous representations conditioned on discrete\ntokens, DisCon circumvents the optimization challenges of continuous token\nmodeling while avoiding the information loss caused by quantization. DisCon\nachieves a gFID score of 1.38 on ImageNet 256$\\times$256 generation,\noutperforming state-of-the-art autoregressive approaches by a clear margin.", "AI": {"tldr": "The paper introduces DisCon, a novel framework leveraging discrete-conditioned continuous autoregressive models for visual generation, tackling issues with quantization and continuous representation challenges.", "motivation": "Explore methods to improve image fidelity in autoregressive visual generation by addressing limitations in discrete and continuous token modeling.", "method": "Proposed DisCon framework models continuous representations conditioned on discrete token signals to avoid quantization problems and optimization challenges.", "result": "DisCon achieved a superior gFID score of 1.38 on ImageNet 256\u00d7256 generation, outperforming state-of-the-art autoregressive methods significantly.", "conclusion": "DisCon demonstrates the effectiveness of discrete-conditioned continuous autoregressive modeling in enhancing visual generation fidelity and setting a new performance benchmark."}}
{"id": "2507.01695", "pdf": "https://arxiv.org/pdf/2507.01695", "abs": "https://arxiv.org/abs/2507.01695", "authors": ["Omkar Shende", "Gayathri Ananthanarayanan", "Marcello Traiola"], "title": "PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution", "categories": ["cs.LG"], "comment": null, "summary": "Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable\nability to model complex patterns across various domains such as computer\nvision, speech recognition, robotics, etc. While large DNN models are often\nmore accurate than simpler, lightweight models, they are also resource- and\nenergy-hungry. Hence, it is imperative to design methods to reduce reliance on\nsuch large models without significant degradation in output accuracy. The high\ncomputational cost of these models is often necessary only for a reduced set of\nchallenging inputs, while lighter models can handle most simple ones. Thus,\ncarefully combining properties of existing DNN models in a dynamic, input-based\nway opens opportunities to improve efficiency without impacting accuracy.\n  In this work, we introduce PERTINENCE, a novel online method designed to\nanalyze the complexity of input features and dynamically select the most\nsuitable model from a pre-trained set to process a given input effectively. To\nachieve this, we employ a genetic algorithm to explore the training space of an\nML-based input dispatcher, enabling convergence towards the Pareto front in the\nsolution space that balances overall accuracy and computational efficiency.\n  We showcase our approach on state-of-the-art Convolutional Neural Networks\n(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers\n(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's\nability to provide alternative solutions to existing state-of-the-art models in\nterms of trade-offs between accuracy and number of operations. By\nopportunistically selecting among models trained for the same task, PERTINENCE\nachieves better or comparable accuracy with up to 36% fewer operations.", "AI": {"tldr": "The paper introduces PERTINENCE, a technique that dynamically selects the most efficient pre-trained model for input processing to reduce computation without compromising on accuracy.", "motivation": "Large DNN models are highly accurate but resource-intensive; there is a need for methods to reduce reliance on such models while maintaining accuracy.", "method": "The authors use a genetic algorithm to train an input dispatcher that dynamically chooses the optimal pre-trained model based on input complexity, aiming for a balance between accuracy and computational efficiency.", "result": "PERTINENCE was tested on CNNs trained on CIFAR-10/100 and Vision Transformers on TinyImageNet, showing comparable or better accuracy with up to 36% fewer operations.", "conclusion": "Dynamic model selection using PERTINENCE can significantly reduce computational cost while preserving or improving accuracy, providing an efficient alternative to large DNN models."}}
{"id": "2507.01788", "pdf": "https://arxiv.org/pdf/2507.01788", "abs": "https://arxiv.org/abs/2507.01788", "authors": ["Montasir Shams", "Chashi Mahiul Islam", "Shaeke Salman", "Phat Tran", "Xiuwen Liu"], "title": "Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages", "summary": "Vision transformers (ViTs) have rapidly gained prominence in medical imaging\ntasks such as disease classification, segmentation, and detection due to their\nsuperior accuracy compared to conventional deep learning models. However, due\nto their size and complex interactions via the self-attention mechanism, they\nare not well understood. In particular, it is unclear whether the\nrepresentations produced by such models are semantically meaningful. In this\npaper, using a projected gradient-based algorithm, we show that their\nrepresentations are not semantically meaningful and they are inherently\nvulnerable to small changes. Images with imperceptible differences can have\nvery different representations; on the other hand, images that should belong to\ndifferent semantic classes can have nearly identical representations. Such\nvulnerability can lead to unreliable classification results; for example,\nunnoticeable changes cause the classification accuracy to be reduced by over\n60\\%. %. To the best of our knowledge, this is the first work to systematically\ndemonstrate this fundamental lack of semantic meaningfulness in ViT\nrepresentations for medical image classification, revealing a critical\nchallenge for their deployment in safety-critical systems.", "AI": {"tldr": "This paper investigates Vision Transformers (ViTs) in medical imaging and reveals that their representations lack semantic meaningfulness and are highly vulnerable to minor perturbations.", "motivation": "To address the unclear semantic meaningfulness and reliability of Vision Transformers' representations in medical imaging tasks, where safety-critical decisions rely on such systems.", "method": "A projected gradient-based algorithm was used to analyze ViTs' representations and to determine their response to slight image changes and their consistency across semantic classes.", "result": "The study finds that ViTs are extremely sensitive to imperceptible image differences, resulting in significant classification accuracy drops (~60%) and unreliable mappings where similar or dissimilar images share unexpected representations.", "conclusion": "ViTs' lack of semantically robust representations poses a critical challenge for their application in safety-critical medical systems, necessitating further research to address these vulnerabilities."}}
{"id": "2507.01699", "pdf": "https://arxiv.org/pdf/2507.01699", "abs": "https://arxiv.org/abs/2507.01699", "authors": ["Illia Oleksiienko", "Juho Kanniainen", "Alexandros Iosifidis"], "title": "Variational Graph Convolutional Neural Networks", "categories": ["cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication. 9\n  pages, 6 figures", "summary": "Estimation of model uncertainty can help improve the explainability of Graph\nConvolutional Networks and the accuracy of the models at the same time.\nUncertainty can also be used in critical applications to verify the results of\nthe model by an expert or additional models. In this paper, we propose\nVariational Neural Network versions of spatial and spatio-temporal Graph\nConvolutional Networks. We estimate uncertainty in both outputs and layer-wise\nattentions of the models, which has the potential for improving model\nexplainability. We showcase the benefits of these models in the social trading\nanalysis and the skeleton-based human action recognition tasks on the Finnish\nboard membership, NTU-60, NTU-120 and Kinetics datasets, where we show\nimprovement in model accuracy in addition to estimated model uncertainties.", "AI": {"tldr": "This paper proposes Variational Neural Networks for improving Graph Convolutional Networks' accuracy and explainability by estimating model uncertainty and layer-wise attentions.", "motivation": "To enhance explainability and accuracy of Graph Convolutional Networks, particularly in critical applications requiring verification of model results.", "method": "Introduces Variational Neural Network versions of spatial and spatio-temporal Graph Convolutional Networks, estimating uncertainty in model outputs and layer-wise attentions.", "result": "Applied on Finnish board membership, NTU-60, NTU-120, and Kinetics datasets, demonstrating improved model accuracy and estimation of model uncertainties.", "conclusion": "The approach improves both accuracy and explainability of Graph Convolutional Networks, showcasing potential across diverse tasks such as social trading analysis and human action recognition."}}
{"id": "2507.01791", "pdf": "https://arxiv.org/pdf/2507.01791", "abs": "https://arxiv.org/abs/2507.01791", "authors": ["Zihong Guo", "Chen Wan", "Yayin Zheng", "Hailing Kuang", "Xiaohai Lu"], "title": "Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation", "categories": ["cs.CV"], "comment": null, "summary": "The transferability of adversarial examples poses a significant security\nchallenge for deep neural networks, which can be attacked without knowing\nanything about them. In this paper, we propose a new Segmented Gaussian Pyramid\n(SGP) attack method to enhance the transferability, particularly against\ndefense models. Unlike existing methods that generally focus on single-scale\nimages, our approach employs Gaussian filtering and three types of downsampling\nto construct a series of multi-scale examples. Then, the gradients of the loss\nfunction with respect to each scale are computed, and their average is used to\ndetermine the adversarial perturbations. The proposed SGP can be considered an\ninput transformation with high extensibility that is easily integrated into\nmost existing adversarial attacks. Extensive experiments demonstrate that in\ncontrast to the state-of-the-art methods, SGP significantly enhances attack\nsuccess rates against black-box defense models, with average attack success\nrates increasing by 2.3% to 32.6%, based only on transferability.", "AI": {"tldr": "This paper introduces the Segmented Gaussian Pyramid (SGP) attack method to improve the transferability of adversarial examples against defense models.", "motivation": "To address the security challenge where deep neural networks can be attacked without prior knowledge of their structure, and to enhance the transferability of such attacks to defense models.", "method": "The SGP attack generates multi-scale adversarial examples by applying Gaussian filtering and three downsampling approaches. Gradients across scales are averaged to derive perturbations. This method serves as an input transformation integrated into existing attacks.", "result": "SGP improves black-box defense model attack success rates, increasing accuracy by 2.3% to 32.6% over current state-of-the-art methods.", "conclusion": "The proposed SGP technique significantly boosts adversarial attack performance against defense models based purely on transferability, showcasing its adaptability and practicality."}}
{"id": "2507.01700", "pdf": "https://arxiv.org/pdf/2507.01700", "abs": "https://arxiv.org/abs/2507.01700", "authors": ["Andrea Piras", "Matteo Negro", "Ragib Ahsan", "David Arbour", "Elena Zheleva"], "title": "Relational Causal Discovery with Latent Confounders", "categories": ["cs.LG", "cs.AI"], "comment": "30 pages, 19 figures. Accepted for publication at the 41st Conference\n  on Uncertainty in Artificial Intelligence (UAI 2025). Andrea Piras and Matteo\n  Negro contributed equally to this work", "summary": "Estimating causal effects from real-world relational data can be challenging\nwhen the underlying causal model and potential confounders are unknown. While\nseveral causal discovery algorithms exist for learning causal models with\nlatent confounders from data, they assume that the data is independent and\nidentically distributed (i.i.d.) and are not well-suited for learning from\nrelational data. Similarly, existing relational causal discovery algorithms\nassume causal sufficiency, which is unrealistic for many real-world datasets.\nTo address this gap, we propose RelFCI, a sound and complete causal discovery\nalgorithm for relational data with latent confounders. Our work builds upon the\nFast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms\nand it defines new graphical models, necessary to support causal discovery in\nrelational domains. We also establish soundness and completeness guarantees for\nrelational d-separation with latent confounders. We present experimental\nresults demonstrating the effectiveness of RelFCI in identifying the correct\ncausal structure in relational causal models with latent confounders.", "AI": {"tldr": "The paper introduces RelFCI, a new causal discovery algorithm for relational data that addresses the challenge of latent confounders, demonstrating its effectiveness in identifying causal structures.", "motivation": "The paper aims to address the limitations of existing causal discovery algorithms, which either assume i.i.d data or causal sufficiency\u2014both of which are unrealistic for many real-world relational datasets.", "method": "The approach expands upon the Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms, introduces new graphical models, and establishes theoretical guarantees for relational d-separation with latent confounders.", "result": "The proposed RelFCI algorithm is shown experimentally to correctly identify causal structures in relational data with latent confounders.", "conclusion": "RelFCI is a sound and complete solution for causal discovery in relational domains, overcoming prior limitations and demonstrating its value in handling latent confounders."}}
{"id": "2507.01792", "pdf": "https://arxiv.org/pdf/2507.01792", "abs": "https://arxiv.org/abs/2507.01792", "authors": ["Peng Zheng", "Ye Wang", "Rui Ma", "Zuxuan Wu"], "title": "FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization", "categories": ["cs.CV"], "comment": null, "summary": "Subject-driven image generation plays a crucial role in applications such as\nvirtual try-on and poster design. Existing approaches typically fine-tune\npretrained generative models or apply LoRA-based adaptations for individual\nsubjects. However, these methods struggle with multi-subject personalization,\nas combining independently adapted modules often requires complex re-tuning or\njoint optimization. We present FreeLoRA, a simple and generalizable framework\nthat enables training-free fusion of subject-specific LoRA modules for\nmulti-subject personalization. Each LoRA module is adapted on a few images of a\nspecific subject using a Full Token Tuning strategy, where it is applied across\nall tokens in the prompt to encourage weakly supervised token-content\nalignment. At inference, we adopt Subject-Aware Inference, activating each\nmodule only on its corresponding subject tokens. This enables training-free\nfusion of multiple personalized subjects within a single image, while\nmitigating overfitting and mutual interference between subjects. Extensive\nexperiments show that FreeLoRA achieves strong performance in both subject\nfidelity and prompt consistency.", "AI": {"tldr": "FreeLoRA is a novel framework for multi-subject image generation without requiring training, using LoRA modules and specific inference strategies.", "motivation": "Existing subject-driven image generation approaches face challenges in accommodating multi-subject personalization without complex re-tuning or joint optimization.", "method": "The framework applies a Full Token Tuning strategy during adaptation of LoRA modules and utilizes Subject-Aware Inference to selectively activate modules based on subject tokens, enabling training-free fusion.", "result": "FreeLoRA effectively generates images with high subject fidelity and prompt consistency while minimizing overfitting and interference.", "conclusion": "The proposed FreeLoRA framework offers a generalizable and training-free approach to multi-subject image generation, achieving strong and reliable performance."}}
{"id": "2507.01714", "pdf": "https://arxiv.org/pdf/2507.01714", "abs": "https://arxiv.org/abs/2507.01714", "authors": ["Kevin Innerebner", "Franz M. Rohrhofer", "Bernhard C. Geiger"], "title": "B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling", "categories": ["cs.LG"], "comment": null, "summary": "Training physics-informed neural networks (PINNs) for forward problems often\nsuffers from severe convergence issues, hindering the propagation of\ninformation from regions where the desired solution is well-defined.\nHaitsiukevich and Ilin (2023) proposed an ensemble approach that extends the\nactive training domain of each PINN based on i) ensemble consensus and ii)\nvicinity to (pseudo-)labeled points, thus ensuring that the information from\nthe initial condition successfully propagates to the interior of the\ncomputational domain.\n  In this work, we suggest replacing the ensemble by a Bayesian PINN, and\nconsensus by an evaluation of the PINN's posterior variance. Our experiments\nshow that this mathematically principled approach outperforms the ensemble on a\nset of benchmark problems and is competitive with PINN ensembles trained with\ncombinations of Adam and LBFGS.", "AI": {"tldr": "The paper proposes an enhancement in training physics-informed neural networks (PINNs) using a Bayesian approach, eliminating the need for an ensemble and integrating posterior variance evaluation.", "motivation": "The motivation stems from the convergence issues faced by PINNs in forward problems, particularly the challenge in propagating information to regions with less-defined solutions.", "method": "The authors replace the ensemble approach by utilizing a Bayesian PINN and assessing the PINN's posterior variance for information propagation.", "result": "The Bayesian PINN-based approach outperformed ensemble-based methods on multiple benchmark problems, demonstrating its effectiveness.", "conclusion": "The paper concludes that this Bayesian approach is both mathematically principled and competitive, offering improvements in PINN training dynamics."}}
{"id": "2507.01483", "pdf": "https://arxiv.org/pdf/2507.01483", "abs": "https://arxiv.org/abs/2507.01483", "authors": ["Craig S Wright"], "title": "Epistemic Scarcity: The Economics of Unresolvable Unknowns", "categories": ["econ.GN", "cs.AI", "cs.CY", "physics.hist-ph", "q-fin.EC", "91B42, 91B40, 68T01", "J.4; I.2.1; K.4.1; K.4.2"], "comment": "47 pages - submission to QJAE", "summary": "This paper presents a praxeological analysis of artificial intelligence and\nalgorithmic governance, challenging assumptions about the capacity of machine\nsystems to sustain economic and epistemic order. Drawing on Misesian a priori\nreasoning and Austrian theories of entrepreneurship, we argue that AI systems\nare incapable of performing the core functions of economic coordination:\ninterpreting ends, discovering means, and communicating subjective value\nthrough prices. Where neoclassical and behavioural models treat decisions as\noptimisation under constraint, we frame them as purposive actions under\nuncertainty.\n  We critique dominant ethical AI frameworks such as Fairness, Accountability,\nand Transparency (FAT) as extensions of constructivist rationalism, which\nconflict with a liberal order grounded in voluntary action and property rights.\nAttempts to encode moral reasoning in algorithms reflect a misunderstanding of\nethics and economics. However complex, AI systems cannot originate norms,\ninterpret institutions, or bear responsibility. They remain opaque, misaligned,\nand inert.\n  Using the concept of epistemic scarcity, we explore how information abundance\ndegrades truth discernment, enabling both entrepreneurial insight and soft\ntotalitarianism. Our analysis ends with a civilisational claim: the debate over\nAI concerns the future of human autonomy, institutional evolution, and reasoned\nchoice. The Austrian tradition, focused on action, subjectivity, and\nspontaneous order, offers the only coherent alternative to rising computational\nsocial control.", "AI": {"tldr": "The paper critiques AI systems and algorithmic governance by using Austrian economic theories, claiming they fall short for effective economic coordination and ethical reasoning.", "motivation": "To challenge the prevalent notions about AI's ability to sustain economic and epistemic order, against a backdrop of Austrian economics and praxeology.", "method": "The authors apply Misesian a priori reasoning, Austrian entrepreneurship theories, and critique ethical AI frameworks like FAT as misaligned with voluntary action and liberal values.", "result": "The study concludes AI systems can't originate norms, interpret institutions, or effectively coordinate economic actions, remaining opaque and inert.", "conclusion": "The authors argue AI debates signal human autonomy versus computational control, with Austrian economics offering an alternative framework based on action and spontaneous order."}}
{"id": "2507.01800", "pdf": "https://arxiv.org/pdf/2507.01800", "abs": "https://arxiv.org/abs/2507.01800", "authors": ["Shengli Zhou", "Jianuo Zhu", "Qilin Huang", "Fangjing Wang", "Yanfu Zhang", "Feng Zheng"], "title": "HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision", "categories": ["cs.CV", "cs.MM"], "comment": "ICANN 2025", "summary": "3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the\nphysical world and perform spatial reasoning. Answer-centric supervision is a\ncommonly used training method for 3D VQA models. Many models that utilize this\nstrategy have achieved promising results in 3D VQA tasks. However, the\nanswer-centric approach only supervises the final output of models and allows\nmodels to develop reasoning pathways freely. The absence of supervision on the\nreasoning pathway enables the potential for developing superficial shortcuts\nthrough common patterns in question-answer pairs. Moreover, although\nslow-thinking methods advance large language models, they suffer from\nunderthinking. To address these issues, we propose \\textbf{HCNQA}, a 3D VQA\nmodel leveraging a hierarchical concentration narrowing supervision method. By\nmimicking the human process of gradually focusing from a broad area to specific\nobjects while searching for answers, our method guides the model to perform\nthree phases of concentration narrowing through hierarchical supervision. By\nsupervising key checkpoints on a general reasoning pathway, our method can\nensure the development of a rational and effective reasoning pathway. Extensive\nexperimental results demonstrate that our method can effectively ensure that\nthe model develops a rational reasoning pathway and performs better. The code\nis available at https://github.com/JianuoZhu/HCNQA.", "AI": {"tldr": "The paper introduces a method, HCNQA, for 3D Visual Question-Answering (VQA), ensuring models develop effective and rational reasoning pathways and achieve superior performance.", "motivation": "Current 3D Visual Question-Answering models mainly rely on answer-centric supervision, which only supervises the model's output but not the reasoning process. This can result in shortcuts and underthinking.", "method": "The authors propose HCNQA, a model that employs hierarchical concentration narrowing, imitating humans by gradually narrowing focus through three phases of hierarchical supervision, guiding the reasoning process.", "result": "HCNQA demonstrated improved reasoning pathways and better performance in experiments compared to other approaches.", "conclusion": "The proposed HCNQA method helps 3D VQA models develop rational reasoning processes, advancing their performance and reliability in 3D scene understanding."}}
{"id": "2507.01724", "pdf": "https://arxiv.org/pdf/2507.01724", "abs": "https://arxiv.org/abs/2507.01724", "authors": ["Micha Henheik", "Theresa Eimer", "Marius Lindauer"], "title": "Revisiting Learning Rate Control", "categories": ["cs.LG"], "comment": null, "summary": "The learning rate is one of the most important hyperparameters in deep\nlearning, and how to control it is an active area within both AutoML and deep\nlearning research. Approaches for learning rate control span from classic\noptimization to online scheduling based on gradient statistics. This paper\ncompares paradigms to assess the current state of learning rate control. We\nfind that methods from multi-fidelity hyperparameter optimization,\nfixed-hyperparameter schedules, and hyperparameter-free learning often perform\nvery well on selected deep learning tasks but are not reliable across settings.\nThis highlights the need for algorithm selection methods in learning rate\ncontrol, which have been neglected so far by both the AutoML and deep learning\ncommunities. We also observe a trend of hyperparameter optimization approaches\nbecoming less effective as models and tasks grow in complexity, even when\ncombined with multi-fidelity approaches for more expensive model trainings. A\nfocus on more relevant test tasks and new promising directions like finetunable\nmethods and meta-learning will enable the AutoML community to significantly\nstrengthen its impact on this crucial factor in deep learning.", "AI": {"tldr": "The paper evaluates different paradigms for learning rate control in deep learning and highlights the challenges in algorithm selection methods for optimization.", "motivation": "To assess and improve methods for learning rate control, a key hyperparameter in deep learning, as current approaches often lack reliability across diverse settings.", "method": "Comparison of approaches across multi-fidelity hyperparameter optimization, fixed schedules, hyperparameter-free learning, and analysis on their performance in deep learning tasks.", "result": "Many existing methods work well in specific scenarios but fail to provide consistent reliability across diverse deep learning environments; hyperparameter optimization loses effectiveness with increasing model and task complexity.", "conclusion": "There is a need for improved algorithm selection methods and exploration of finetunable, meta-learning methods to address limitations and enhance learning rate control in deep learning."}}
{"id": "2507.01801", "pdf": "https://arxiv.org/pdf/2507.01801", "abs": "https://arxiv.org/abs/2507.01801", "authors": ["Bin Rao", "Haicheng Liao", "Yanchen Guan", "Chengyue Wang", "Bonan Wang", "Jiaxun Zhang", "Zhenning Li"], "title": "AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Accurately predicting the future trajectories of traffic agents is essential\nin autonomous driving. However, due to the inherent imbalance in trajectory\ndistributions, tail data in natural datasets often represents more complex and\nhazardous scenarios. Existing studies typically rely solely on a base model's\nprediction error, without considering the diversity and uncertainty of\nlong-tail trajectory patterns. We propose an adaptive momentum and decoupled\ncontrastive learning framework (AMD), which integrates unsupervised and\nsupervised contrastive learning strategies. By leveraging an improved momentum\ncontrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module,\nour framework enhances the model's ability to recognize rare and complex\ntrajectories. Additionally, we design four types of trajectory random\naugmentation methods and introduce an online iterative clustering strategy,\nallowing the model to dynamically update pseudo-labels and better adapt to the\ndistributional shifts in long-tail data. We propose three different criteria to\ndefine long-tail trajectories and conduct extensive comparative experiments on\nthe nuScenes and ETH$/$UCY datasets. The results show that AMD not only\nachieves optimal performance in long-tail trajectory prediction but also\ndemonstrates outstanding overall prediction accuracy.", "AI": {"tldr": "The paper introduces AMD, a framework to improve trajectory prediction in autonomous driving by dealing with long-tail data challenges.", "motivation": "There's a need for better accuracy in rare and complex trajectory predictions in autonomous driving, which are essential for safety.", "method": "AMD integrates adaptive momentum, unsupervised and supervised contrastive learning, augmented trajectory techniques, and iterative clustering.", "result": "The framework shows superior results on nuScenes and ETH/UCY datasets for both long-tail and overall prediction accuracy.", "conclusion": "AMD effectively recognizes rare trajectory patterns, adapting to distributional shifts and enhancing autonomous driving safety."}}
{"id": "2507.01740", "pdf": "https://arxiv.org/pdf/2507.01740", "abs": "https://arxiv.org/abs/2507.01740", "authors": ["Trung-Dung Hoang", "Alceu Bissoto", "Vihangkumar V. Naik", "Tim Fl\u00fchmann", "Artemii Shlychkov", "Jos\u00e9 Garcia-Tirado", "Lisa M. Koch"], "title": "A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based Inference", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Accurately estimating parameters of physiological models is essential to\nachieving reliable digital twins. For Type 1 Diabetes, this is particularly\nchallenging due to the complexity of glucose-insulin interactions. Traditional\nmethods based on Markov Chain Monte Carlo struggle with high-dimensional\nparameter spaces and fit parameters from scratch at inference time, making them\nslow and computationally expensive. In this study, we propose a\nSimulation-Based Inference approach based on Neural Posterior Estimation to\nefficiently capture the complex relationships between meal intake, insulin, and\nglucose level, providing faster, amortized inference. Our experiments\ndemonstrate that SBI not only outperforms traditional methods in parameter\nestimation but also generalizes better to unseen conditions, offering real-time\nposterior inference with reliable uncertainty quantification.", "AI": {"tldr": "The paper introduces a Simulation-Based Inference (SBI) method using Neural Posterior Estimation for estimating Type 1 Diabetes model parameters, surpassing traditional methods in speed, accuracy, and robustness.", "motivation": "Reliable digital twins for Type 1 Diabetes require precise physiological parameter estimation, but the complexity of glucose-insulin interactions makes this challenging for traditional methods.", "method": "The study uses Neural Posterior Estimation within a Simulation-Based Inference framework to model glucose-insulin dynamics, aiming for faster and amortized inference compared to traditional Markov Chain Monte Carlo techniques.", "result": "Experiments show that the proposed SBI approach outperforms traditional methods in parameter estimation and provides better generalization to unseen conditions, along with real-time posterior inference and uncertainty quantification.", "conclusion": "The proposed SBI approach presents an advance in estimating Type 1 Diabetes model parameters, offering a faster, more generalizable, and computationally efficient alternative to traditional methods."}}
{"id": "2507.01835", "pdf": "https://arxiv.org/pdf/2507.01835", "abs": "https://arxiv.org/abs/2507.01835", "authors": ["Daniil Reutsky", "Daniil Vladimirov", "Yasin Mamedov", "Georgy Perevozchikov", "Nancy Mehta", "Egor Ershov", "Radu Timofte"], "title": "Modulate and Reconstruct: Learning Hyperspectral Imaging from Misaligned Smartphone Views", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral reconstruction (HSR) from RGB images is a fundamentally\nill-posed problem due to severe spectral information loss. Existing approaches\ntypically rely on a single RGB image, limiting reconstruction accuracy. In this\nwork, we propose a novel multi-image-to-hyperspectral reconstruction (MI-HSR)\nframework that leverages a triple-camera smartphone system, where two lenses\nare equipped with carefully selected spectral filters. Our configuration,\ngrounded in theoretical and empirical analysis, enables richer and more diverse\nspectral observations than conventional single-camera setups. To support this\nnew paradigm, we introduce Doomer, the first dataset for MI-HSR, comprising\naligned images from three smartphone cameras and a hyperspectral reference\ncamera across diverse scenes. We show that the proposed HSR model achieves\nconsistent improvements over existing methods on the newly proposed benchmark.\nIn a nutshell, our setup allows 30% towards more accurately estimated spectra\ncompared to an ordinary RGB camera. Our findings suggest that multi-view\nspectral filtering with commodity hardware can unlock more accurate and\npractical hyperspectral imaging solutions.", "AI": {"tldr": "This paper introduces a multi-image-to-hyperspectral reconstruction (MI-HSR) framework using a triple-camera smartphone system, achieving better accuracy in spectral estimation compared to single-camera solutions.", "motivation": "Hyperspectral reconstruction from RGB images is challenging due to significant spectral information loss, and existing methods relying on single RGB images limit reconstruction accuracy.", "method": "The authors propose leveraging a triple-camera smartphone system with carefully selected spectral filters and introduce Doomer, a dataset of aligned images from three cameras and a hyperspectral reference camera.", "result": "The MI-HSR model consistently outperforms existing methods on the new benchmark, achieving a 30% improvement in spectral estimation accuracy compared to ordinary RGB cameras.", "conclusion": "Multi-view spectral filtering with commodity hardware enables more accurate and practical hyperspectral imaging solutions."}}
{"id": "2507.01838", "pdf": "https://arxiv.org/pdf/2507.01838", "abs": "https://arxiv.org/abs/2507.01838", "authors": ["Hailong Yan", "Ao Li", "Xiangtao Zhang", "Zhe Liu", "Zenglin Shi", "Ce Zhu", "Le Zhang"], "title": "MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Recent advancements in deep neural networks have driven significant progress\nin image enhancement (IE). However, deploying deep learning models on\nresource-constrained platforms, such as mobile devices, remains challenging due\nto high computation and memory demands. To address these challenges and\nfacilitate real-time IE on mobile, we introduce an extremely lightweight\nConvolutional Neural Network (CNN) framework with around 4K parameters. Our\napproach integrates reparameterization with an Incremental Weight Optimization\nstrategy to ensure efficiency. Additionally, we enhance performance with a\nFeature Self-Transform module and a Hierarchical Dual-Path Attention mechanism,\noptimized with a Local Variance-Weighted loss. With this efficient framework,\nwe are the first to achieve real-time IE inference at up to 1,100 frames per\nsecond (FPS) while delivering competitive image quality, achieving the best\ntrade-off between speed and performance across multiple IE tasks. The code will\nbe available at https://github.com/AVC2-UESTC/MobileIE.git.", "AI": {"tldr": "The paper presents a lightweight CNN framework designed for efficient image enhancement on mobile devices, achieving real-time inference at up to 1,100 FPS with competitive image quality.", "motivation": "The paper aims to overcome challenges in deploying deep learning models for image enhancement on resource-constrained platforms, such as mobile devices, due to the high computation and memory requirements.", "method": "The authors introduce a lightweight CNN framework with approximately 4K parameters, integrating reparameterization and Incremental Weight Optimization. They enhance performance using a Feature Self-Transform module, Hierarchical Dual-Path Attention, and a Local Variance-Weighted loss.", "result": "The presented framework achieves real-time image enhancement inference at up to 1,100 FPS while maintaining competitive image quality, representing a superior trade-off between speed and performance in comparison to other models.", "conclusion": "The developed framework advances the field of mobile image enhancement by enabling real-time processing with minimal computational resources without sacrificing image quality, paving the way for broader mobile applications."}}
{"id": "2507.01882", "pdf": "https://arxiv.org/pdf/2507.01882", "abs": "https://arxiv.org/abs/2507.01882", "authors": ["Guiqiu Liao", "Matjaz Jogan", "Marcel Hussing", "Edward Zhang", "Eric Eaton", "Daniel A. Hashimoto"], "title": "Future Slot Prediction for Unsupervised Object Discovery in Surgical Video", "categories": ["cs.CV"], "comment": "Accepted by MICCAI2025", "summary": "Object-centric slot attention is an emerging paradigm for unsupervised\nlearning of structured, interpretable object-centric representations (slots).\nThis enables effective reasoning about objects and events at a low\ncomputational cost and is thus applicable to critical healthcare applications,\nsuch as real-time interpretation of surgical video. The heterogeneous scenes in\nreal-world applications like surgery are, however, difficult to parse into a\nmeaningful set of slots. Current approaches with an adaptive slot count perform\nwell on images, but their performance on surgical videos is low. To address\nthis challenge, we propose a dynamic temporal slot transformer (DTST) module\nthat is trained both for temporal reasoning and for predicting the optimal\nfuture slot initialization. The model achieves state-of-the-art performance on\nmultiple surgical databases, demonstrating that unsupervised object-centric\nmethods can be applied to real-world data and become part of the common arsenal\nin healthcare applications.", "AI": {"tldr": "The paper proposes a dynamic temporal slot transformer (DTST) for better object representation in surgical videos, surpassing prior unsupervised methods.", "motivation": "Current object-centric methods struggle with parsing complex, heterogeneous surgical scenes, especially in videos.", "method": "The paper introduces DTST to improve temporal reasoning and predict future slot initialization for scene interpretation.", "result": "DTST achieves state-of-the-art performance on multiple surgical datasets.", "conclusion": "Unsupervised object-centric methods like DTST show promise for practical healthcare applications, including surgical video analysis."}}
{"id": "2507.01781", "pdf": "https://arxiv.org/pdf/2507.01781", "abs": "https://arxiv.org/abs/2507.01781", "authors": ["Dalia Rodr\u00edguez-Salas", "Christian Riess"], "title": "BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification", "categories": ["cs.LG", "cs.AI", "68T07 (Primary) 62H30, 68T05 (Secondary)"], "comment": "18 pages, 3 figures (with two images each)", "summary": "We introduce BranchNet, a neuro-symbolic learning framework that transforms\ndecision tree ensembles into sparse, partially connected neural networks. Each\nbranch, defined as a decision path from root to a parent of leaves, is mapped\nto a hidden neuron, preserving symbolic structure while enabling gradient-based\noptimization. The resulting models are compact, interpretable, and require no\nmanual architecture tuning. Evaluated on a suite of structured multi-class\nclassification benchmarks, BranchNet consistently outperforms XGBoost in\naccuracy, with statistically significant gains. We detail the architecture,\ntraining procedure, and sparsity dynamics, and discuss the model's strengths in\nsymbolic interpretability as well as its current limitations, particularly on\nbinary tasks where further adaptive calibration may be beneficial.", "AI": {"tldr": "BranchNet converts decision tree ensembles into sparse neural networks, outperforming XGBoost on multi-class benchmarks.", "motivation": "Address challenges of combining interpretability and optimization in machine learning models.", "method": "Maps decision paths to hidden neurons, preserving symbolic tree structure for gradient-based learning.", "result": "Tests show statistically significant accuracy improvements over XGBoost in multi-class classification.", "conclusion": "BranchNet is compact and interpretable, but limitations remain in binary tasks requiring adaptive calibration."}}
{"id": "2507.01884", "pdf": "https://arxiv.org/pdf/2507.01884", "abs": "https://arxiv.org/abs/2507.01884", "authors": ["Kunlun Xu", "Fan Zhuo", "Jiangmeng Li", "Xu Zou", "Jiahuan Zhou"], "title": "Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Current lifelong person re-identification (LReID) methods predominantly rely\non fully labeled data streams. However, in real-world scenarios where\nannotation resources are limited, a vast amount of unlabeled data coexists with\nscarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)\nproblem where LReID methods suffer severe performance degradation. Existing\nLReID methods, even when combined with semi-supervised strategies, suffer from\nlimited long-term adaptation performance due to struggling with the noisy\nknowledge occurring during unlabeled data utilization. In this paper, we\npioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing\nPrototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key\ninnovation lies in establishing a self-reinforcing cycle between dynamic\nprototype-guided pseudo-label generation and new-old knowledge collaborative\npurification to enhance the utilization of unlabeled data. Specifically,\nlearnable identity prototypes are introduced to dynamically capture the\nidentity distributions and generate high-quality pseudo-labels. Then, the\ndual-knowledge cooperation scheme integrates current model specialization and\nhistorical model generalization, refining noisy pseudo-labels. Through this\ncyclic design, reliable pseudo-labels are progressively mined to improve\ncurrent-stage learning and ensure positive knowledge propagation over long-term\nlearning. Experiments on the established Semi-LReID benchmarks show that our\nSPRED achieves state-of-the-art performance. Our source code is available at\nhttps://github.com/zhoujiahuan1991/ICCV2025-SPRED", "AI": {"tldr": "The paper introduces the Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation framework (SPRED) to address the Semi-Supervised Lifelong Person Re-identification (Semi-LReID) problem caused by limited labeled data and noisy pseudo-labels.", "motivation": "To improve performance in Semi-Supervised LReID scenarios, where labeled data is scarce and unlabeled data is abundant, and existing methods struggle with noisy pseudo-labels.", "method": "Establishing a self-reinforcing cycle between dynamic prototype-guided pseudo-label generation and refining pseudo-labels through current model specialization and historical model generalization.", "result": "SPRED achieves state-of-the-art performance on Semi-LReID benchmarks.", "conclusion": "The framework progressively enhances pseudo-label reliability, enabling superior long-term learning and addressing performance degradation challenges."}}
{"id": "2507.01803", "pdf": "https://arxiv.org/pdf/2507.01803", "abs": "https://arxiv.org/abs/2507.01803", "authors": ["Leyang Xue", "Meghana Madhyastha", "Randal Burns", "Myungjin Lee", "Mahesh K. Marina"], "title": "Towards Decentralized and Sustainable Foundation Model Training with the Edge", "categories": ["cs.LG"], "comment": null, "summary": "Foundation models are at the forefront of AI research, appealing for their\nability to learn from vast datasets and cater to diverse tasks. Yet, their\nsignificant computational demands raise issues of environmental impact and the\nrisk of centralized control in their development. We put forward a vision\ntowards decentralized and sustainable foundation model training that leverages\nthe collective compute of sparingly used connected edge AI devices. We present\nthe rationale behind our vision, particularly in support of its sustainability\nbenefit. We further outline a set of challenges that need to be addressed to\nturn this vision into reality.", "AI": {"tldr": "The paper proposes a decentralized approach to train foundation models using underutilized edge AI devices, aiming for sustainability and addressing potential centralized control risks.", "motivation": "The significant computational demands and environmental impact of training foundation models, alongside concerns over centralized control, motivated the authors to explore decentralized training approaches.", "method": "The authors propose utilizing the computational resources of underused connected edge AI devices to collaboratively train foundation models.", "result": "They outline the sustainability benefits of their decentralized training vision and identify specific challenges for its practical implementation.", "conclusion": "The proposed vision leverages decentralized computing for foundation model training, with potential sustainability advantages, provided its implementation challenges are effectively addressed."}}
{"id": "2507.01547", "pdf": "https://arxiv.org/pdf/2507.01547", "abs": "https://arxiv.org/abs/2507.01547", "authors": ["Ubada El Joulani", "Tatiana Kalganova", "Stergios-Aristoteles Mitoulis", "Sotirios Argyroudis"], "title": "AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "Critical infrastructure, such as transport networks, underpins economic\ngrowth by enabling mobility and trade. However, ageing assets, climate change\nimpacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging\nfrom natural disasters to cyber attacks and conflicts pose growing risks to\ntheir resilience and functionality. This review paper explores how emerging\ndigital technologies, specifically Artificial Intelligence (AI), can enhance\ndamage assessment and monitoring of transport infrastructure. A systematic\nliterature review examines existing AI models and datasets for assessing damage\nin roads, bridges, and other critical infrastructure impacted by natural\ndisasters. Special focus is given to the unique challenges and opportunities\nassociated with bridge damage detection due to their structural complexity and\ncritical role in connectivity. The integration of SAR (Synthetic Aperture\nRadar) data with AI models is also discussed, with the review revealing a\ncritical research gap: a scarcity of studies applying AI models to SAR data for\ncomprehensive bridge damage assessment. Therefore, this review aims to identify\nthe research gaps and provide foundations for AI-driven solutions for assessing\nand monitoring critical transport infrastructures.", "AI": {"tldr": "This paper reviews how AI technologies can improve damage assessment in transport infrastructure, particularly bridges, by addressing research gaps such as SAR data integration.", "motivation": "Aging transport infrastructure faces increasing risks from climate change, disasters, and cyber threats, demanding innovative solutions for resilience and functionality.", "method": "Conducted a systematic literature review on AI applications for damage assessment in roads, bridges, and other critical facilities, paying particular attention to bridge-specific challenges.", "result": "Revealed a lack of research on using AI with SAR data for comprehensive damage assessment of bridges, highlighting this as a critical gap.", "conclusion": "AI, particularly integrated with SAR data, holds promise for enhancing the monitoring and assessment of critical transport infrastructures, but more targeted research is needed."}}
{"id": "2507.01908", "pdf": "https://arxiv.org/pdf/2507.01908", "abs": "https://arxiv.org/abs/2507.01908", "authors": ["Qingdong He", "Xueqin Chen", "Chaoyi Wang", "Yanjie Pan", "Xiaobin Hu", "Zhenye Gan", "Yabiao Wang", "Chengjie Wang", "Xiangtai Li", "Jiangning Zhang"], "title": "Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Instruction-based image editing (IIE) has advanced rapidly with the success\nof diffusion models. However, existing efforts primarily focus on simple and\nexplicit instructions to execute editing operations such as adding, deleting,\nmoving, or swapping objects. They struggle to handle more complex implicit\nhypothetical instructions that require deeper reasoning to infer plausible\nvisual changes and user intent. Additionally, current datasets provide limited\nsupport for training and evaluating reasoning-aware editing capabilities.\nArchitecturally, these methods also lack mechanisms for fine-grained detail\nextraction that support such reasoning. To address these limitations, we\npropose Reason50K, a large-scale dataset specifically curated for training and\nevaluating hypothetical instruction reasoning image editing, along with\nReasonBrain, a novel framework designed to reason over and execute implicit\nhypothetical instructions across diverse scenarios. Reason50K includes over 50K\nsamples spanning four key reasoning scenarios: Physical, Temporal, Causal, and\nStory reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)\nfor editing guidance generation and a diffusion model for image synthesis,\nincorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture\ndetailed visual and textual semantics essential for supporting instruction\nreasoning. To mitigate the semantic loss, we further introduce a Cross-Modal\nEnhancer (CME) that enables rich interactions between the fine-grained cues and\nMLLM-derived features. Extensive experiments demonstrate that ReasonBrain\nconsistently outperforms state-of-the-art baselines on reasoning scenarios\nwhile exhibiting strong zero-shot generalization to conventional IIE tasks. Our\ndataset and code will be released publicly.", "AI": {"tldr": "The paper introduces Reason50K, a dataset for reasoning-aware image editing, and ReasonBrain, a framework integrating multimodal models for handling complex hypothetical instructions.", "motivation": "Existing image editing methods struggle with complex implicit hypothetical instructions requiring deeper reasoning and intent understanding, along with limited training datasets.", "method": "The authors propose Reason50K, a dataset of over 50K samples with four reasoning categories, and ReasonBrain, which uses Multimodal Large Language Models, diffusion models, and specialized modules for instruction reasoning.", "result": "ReasonBrain outperforms state-of-the-art methods on reasoning scenarios and exhibits strong zero-shot generalization to traditional image editing tasks.", "conclusion": "ReasonBrain and Reason50K enhance IIE capabilities for hypothetical instructions, and their public release aims to advance research in reasoning-aware image editing."}}
{"id": "2507.01909", "pdf": "https://arxiv.org/pdf/2507.01909", "abs": "https://arxiv.org/abs/2507.01909", "authors": ["Jorge Tapias Gomez", "Nishant Nadkarni", "Lando S. Bosma", "Jue Jiang", "Ergys D. Subashi", "William P. Segars", "James M. Balter", "Mert R Sabuncu", "Neelam Tyagi", "Harini Veeraraghavan"], "title": "Modality Agnostic, patient-specific digital twins modeling temporally varying digestive motion", "categories": ["cs.CV"], "comment": "7 Pages, 6 figures, 4 tables", "summary": "Objective: Clinical implementation of deformable image registration (DIR)\nrequires voxel-based spatial accuracy metrics such as manually identified\nlandmarks, which are challenging to implement for highly mobile\ngastrointestinal (GI) organs. To address this, patient-specific digital twins\n(DT) modeling temporally varying motion were created to assess the accuracy of\nDIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D\nsequences were generated from static 3D patient scans using published\nanalytical GI motion models through a semi-automated pipeline. Eleven datasets,\nincluding six T2w FSE MRI (T2w MRI), two T1w 4D golden-angle stack-of-stars,\nand three contrast-enhanced CT scans. The motion amplitudes of the DTs were\nassessed against real patient stomach motion amplitudes extracted from\nindependent 4D MRI datasets. The generated DTs were then used to assess six\ndifferent DIR methods using target registration error, Dice similarity\ncoefficient, and the 95th percentile Hausdorff distance using summary metrics\nand voxel-level granular visualizations. Finally, for a subset of T2w MRI scans\nfrom patients treated with MR-guided radiation therapy, dose distributions were\nwarped and accumulated to assess dose warping errors, including evaluations of\nDIR performance in both low- and high-dose regions for patient-specific error\nestimation. Main results: Our proposed pipeline synthesized DTs modeling\nrealistic GI motion, achieving mean and maximum motion amplitudes and a mean\nlog Jacobian determinant within 0.8 mm and 0.01, respectively, similar to\npublished real-patient gastric motion data. It also enables the extraction of\ndetailed quantitative DIR performance metrics and rigorous validation of dose\nmapping accuracy. Significance: The pipeline enables rigorously testing DIR\ntools for dynamic, anatomically complex regions enabling granular spatial and\ndosimetric accuracies.", "AI": {"tldr": "This paper introduces patient-specific digital twins (DTs) to evaluate the spatial accuracy of deformable image registration (DIR) methods in gastrointestinal (GI) organ motion scenarios.", "motivation": "To address the lack of voxel-based accuracy metrics in highly mobile GI organs, especially for evaluating DIR methods' precision in dynamic anatomical scenarios.", "method": "A semi-automated pipeline was developed to create patient-specific 4D DTs from static 3D scans using analytical GI motion models. These DTs were used to evaluate six DIR methods with quantitative metrics and dosimetric accuracy assessments.", "result": "The proposed approach generated realistic DTs that closely matched real-patient gastric motion data, enabling detailed assessments of DIR accuracy and dose mapping performance in GI regions.", "conclusion": "This pipeline provides a new framework for testing DIR methodologies in anatomically complex and dynamic regions, contributing to improved spatial and dosimetric accuracy in clinical settings."}}
{"id": "2507.01912", "pdf": "https://arxiv.org/pdf/2507.01912", "abs": "https://arxiv.org/abs/2507.01912", "authors": ["Ranjan Sapkota", "Zhichao Meng", "Martin Churuvija", "Xiaoqiang Du", "Zenghong Ma", "Manoj Karkee"], "title": "3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP", "categories": ["cs.CV"], "comment": "17 pages, 4 tables, 11 figures", "summary": "In orchard automation, dense foliage during the canopy season severely\noccludes tree structures, minimizing visibility to various canopy parts such as\ntrunks and branches, which limits the ability of a machine vision system.\nHowever, canopy structure is more open and visible during the dormant season\nwhen trees are defoliated. In this work, we present an information fusion\nframework that integrates multi-seasonal structural data to support robotic and\nautomated crop load management during the entire growing season. The framework\ncombines high-resolution RGB-D imagery from both dormant and canopy periods\nusing YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D\nreconstruction, and Fast Generalized Iterative Closest Point (Fast GICP) for\nmodel alignment. Segmentation outputs from YOLOv9-Seg were used to extract\ndepth-informed masks, which enabled accurate 3D point cloud reconstruction via\nKinect Fusion; these reconstructed models from each season were subsequently\naligned using Fast GICP to achieve spatially coherent multi-season fusion. The\nYOLOv9-Seg model, trained on manually annotated images, achieved a mean squared\nerror (MSE) of 0.0047 and segmentation mAP@50 scores up to 0.78 for trunks in\ndormant season dataset. Kinect Fusion enabled accurate reconstruction of tree\ngeometry, validated with field measurements resulting in root mean square\nerrors (RMSE) of 5.23 mm for trunk diameter, 4.50 mm for branch diameter, and\n13.72 mm for branch spacing. Fast GICP achieved precise cross-seasonal\nregistration with a minimum fitness score of 0.00197, allowing integrated,\ncomprehensive tree structure modeling despite heavy occlusions during the\ngrowing season. This fused structural representation enables robotic systems to\naccess otherwise obscured architectural information, improving the precision of\npruning, thinning, and other automated orchard operations.", "AI": {"tldr": "The paper develops a multi-seasonal information fusion framework using advanced imaging and reconstruction techniques to overcome visibility challenges in orchard automation caused by dense foliage.", "motivation": "The motivation is to address visibility challenges in orchard automation during the canopy season, which hampers machine vision systems, by leveraging defoliated tree structures in the dormant season.", "method": "The framework integrates RGB-D imagery, YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D reconstruction, and Fast GICP for multi-season alignment to develop coherent models of tree structures.", "result": "The YOLOv9-Seg achieved segmentation mAP@50 of 0.78 and low MSE of 0.0047. Kinect Fusion attained geometry reconstruction with RMSEs of 5.23 mm (trunk diameter), 4.5 mm (branch diameter), and 13.72 mm (branch spacing). Fast GICP yielded precise cross-season alignment with a minimum fitness score of 0.00197.", "conclusion": "The fused structural model enhances robotic systems' ability to access tree structural information throughout the growing season, benefiting operations like pruning and thinning during heavy foliage periods."}}
{"id": "2507.01825", "pdf": "https://arxiv.org/pdf/2507.01825", "abs": "https://arxiv.org/abs/2507.01825", "authors": ["Franco Alberto Cardillo", "Hamza Khyari", "Umberto Straccia"], "title": "MILP-SAT-GNN: Yet Another Neural SAT Solver", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We proposes a novel method that enables Graph Neural Networks (GNNs) to solve\nSAT problems by leveraging a technique developed for applying GNNs to Mixed\nInteger Linear Programming (MILP). Specifically, k-CNF formulae are mapped into\nMILP problems, which are then encoded as weighted bipartite graphs and\nsubsequently fed into a GNN for training and testing. From a theoretical\nperspective: (i) we establish permutation and equivalence invariance results,\ndemonstrating that the method produces outputs that are stable under reordering\nof clauses and variables; (ii) we identify a theoretical limitation, showing\nthat for a class of formulae called foldable formulae, standard GNNs cannot\nalways distinguish satisfiable from unsatisfiable instances; (iii) we prove a\nuniversal approximation theorem, establishing that with Random Node\nInitialization (RNI), the method can approximate SAT solving to arbitrary\nprecision on finite datasets, that is, the GNN becomes approximately sound and\ncomplete on such datasets. Furthermore, we show that for unfoldable formulae,\nthe same approximation guarantee can be achieved without the need for RNI.\nFinally, we conduct an experimental evaluation of our approach, which show\nthat, despite the simplicity of the neural architecture, the method achieves\npromising results.", "AI": {"tldr": "The paper introduces a novel method using Graph Neural Networks (GNNs) to solve SAT problems by representing k-CNF formulae as Mixed Integer Linear Programming (MILP) instances. These are then mapped to weighted bipartite graphs for GNN processing.", "motivation": "To leverage graph neural networks for solving SAT problems by capitalizing on techniques used in Mixed Integer Linear Programming (MILP).", "method": "The method maps k-CNF formulae into MILP problems, encodes them as weighted bipartite graphs, and uses GNNs for training and testing. It incorporates results on permutation invariance, equivalence invariance, theoretical limitations, and universal approximation capabilities through Random Node Initialization (RNI).", "result": "Theoretical findings include invariance results, a limitation for foldable formulae, and a universal approximation theorem. Experimental results show promising outcomes with the proposed approach, even with simple neural architectures.", "conclusion": "The proposed method effectively applies GNNs to SAT solving. Despite limitations with certain formulae and the simplicity of the model, experimental results are promising, indicating potential for further research."}}
{"id": "2507.01563", "pdf": "https://arxiv.org/pdf/2507.01563", "abs": "https://arxiv.org/abs/2507.01563", "authors": ["Marco Giordano", "Stefano Giacomelli", "Claudia Rinaldi", "Fabio Graziosi"], "title": "Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T07 (Primary), 68T10 (Secondary)", "B.1.5; B.4.5; C.3; C.4; I.2; K.4; J.2"], "comment": "10 pages, 10 figures, submitted to\n  https://internetofsounds2025.ieee-is2.org/. arXiv admin note: text overlap\n  with arXiv:2506.23437", "summary": "We present a full-stack emergency vehicle (EV) siren detection system\ndesigned for real-time deployment on embedded hardware. The proposed approach\nis based on E2PANNs, a fine-tuned convolutional neural network derived from\nEPANNs, and optimized for binary sound event detection under urban acoustic\nconditions. A key contribution is the creation of curated and semantically\nstructured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -\ndeveloped using a custom AudioSet-Tools framework to overcome the low\nreliability of standard AudioSet annotations. The system is deployed on a\nRaspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing\na multithreaded inference engine with adaptive frame sizing, probability\nsmoothing, and a decision-state machine to control false positive activations.\nA remote WebSocket interface provides real-time monitoring and facilitates live\ndemonstration capabilities. Performance is evaluated using both framewise and\nevent-based metrics across multiple configurations. Results show the system\nachieves low-latency detection with improved robustness under realistic audio\nconditions. This work demonstrates the feasibility of deploying IoS-compatible\nSED solutions that can form distributed acoustic monitoring networks, enabling\ncollaborative emergency vehicle tracking across smart city infrastructures\nthrough WebSocket connectivity on low-cost edge devices.", "AI": {"tldr": "This paper introduces a real-time emergency vehicle siren detection system optimized for embedded hardware using fine-tuned convolutional neural networks (E2PANNs). Performance is enhanced through structured datasets and efficient deployment on low-cost edge devices.", "motivation": "The motivation is to create a reliable, low-latency acoustic monitoring system for detecting emergency vehicle sirens, addressing challenges like annotation reliability and robustness in urban noise environments.", "method": "The system relies on E2PANNs, curated datasets, and optimizations like probability smoothing and decision-state machines. It is deployed on a Raspberry Pi with a high-fidelity DAC+microphone board and supports real-time monitoring via WebSockets.", "result": "Tests reveal low-latency and robust detection performance in realistic acoustic conditions, demonstrating the practicality of the system.", "conclusion": "The proposed system proves feasible for distributed acoustic monitoring in smart cities using affordable embedded devices, enabling real-time emergency vehicle tracking."}}
{"id": "2507.01926", "pdf": "https://arxiv.org/pdf/2507.01926", "abs": "https://arxiv.org/abs/2507.01926", "authors": ["Yaowei Li", "Xiaoyu Li", "Zhaoyang Zhang", "Yuxuan Bian", "Gan Liu", "Xinyuan Li", "Jiale Xu", "Wenbo Hu", "Yating Liu", "Lingen Li", "Jing Cai", "Yuexian Zou", "Yancheng He", "Ying Shan"], "title": "IC-Custom: Diverse Image Customization via In-Context Learning", "categories": ["cs.CV"], "comment": "Project page: https://liyaowei-stu.github.io/project/IC_Custom", "summary": "Image customization, a crucial technique for industrial media production,\naims to generate content that is consistent with reference images. However,\ncurrent approaches conventionally separate image customization into\nposition-aware and position-free customization paradigms and lack a universal\nframework for diverse customization, limiting their applications across various\nscenarios. To overcome these limitations, we propose IC-Custom, a unified\nframework that seamlessly integrates position-aware and position-free image\ncustomization through in-context learning. IC-Custom concatenates reference\nimages with target images to a polyptych, leveraging DiT's multi-modal\nattention mechanism for fine-grained token-level interactions. We introduce the\nIn-context Multi-Modal Attention (ICMA) mechanism with learnable task-oriented\nregister tokens and boundary-aware positional embeddings to enable the model to\ncorrectly handle different task types and distinguish various inputs in\npolyptych configurations. To bridge the data gap, we carefully curated a\nhigh-quality dataset of 12k identity-consistent samples with 8k from real-world\nsources and 4k from high-quality synthetic data, avoiding the overly glossy and\nover-saturated synthetic appearance. IC-Custom supports various industrial\napplications, including try-on, accessory placement, furniture arrangement, and\ncreative IP customization. Extensive evaluations on our proposed ProductBench\nand the publicly available DreamBench demonstrate that IC-Custom significantly\noutperforms community workflows, closed-source models, and state-of-the-art\nopen-source approaches. IC-Custom achieves approximately 73% higher human\npreference across identity consistency, harmonicity, and text alignment\nmetrics, while training only 0.4% of the original model parameters. Project\npage: https://liyaowei-stu.github.io/project/IC_Custom", "AI": {"tldr": "IC-Custom integrates position-aware and position-free image customization via in-context learning, outperforming existing methods while training a fraction of parameters.", "motivation": "Existing image customization methods separate customization into distinct paradigms, limiting their adaptability across diverse applications.", "method": "IC-Custom leverages polyptych configurations and introduces In-context Multi-Modal Attention (ICMA) for precise token-level interactions with curated datasets.", "result": "IC-Custom significantly outperforms existing workflows and models, achieving a 73% uplift in human preference metrics like identity consistency and text alignment.", "conclusion": "IC-Custom is a groundbreaking framework offering unified, efficient customization for various industrial applications, demonstrating superior performance and adaptability."}}
{"id": "2507.01829", "pdf": "https://arxiv.org/pdf/2507.01829", "abs": "https://arxiv.org/abs/2507.01829", "authors": ["Tristan Torchet", "Christian Metzner", "Laura Kriener", "Melika Payvand"], "title": "mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Edge devices for temporal processing demand models that capture both short-\nand long- range dynamics under tight memory constraints. While Transformers\nexcel at sequence modeling, their quadratic memory scaling with sequence length\nmakes them impractical for such settings. Recurrent Neural Networks (RNNs)\noffer constant memory but train sequentially, and Temporal Convolutional\nNetworks (TCNs), though efficient, scale memory with kernel size. To address\nthis, we propose mGRADE (mininally Gated Recurrent Architecture with Delay\nEmbedding), a hybrid-memory system that integrates a temporal 1D-convolution\nwith learnable spacings followed by a minimal gated recurrent unit (minGRU).\nThis design allows the convolutional layer to realize a flexible delay\nembedding that captures rapid temporal variations, while the recurrent module\nefficiently maintains global context with minimal memory overhead. We validate\nour approach on two synthetic tasks, demonstrating that mGRADE effectively\nseparates and preserves multi-scale temporal features. Furthermore, on\nchallenging pixel-by-pixel image classification benchmarks, mGRADE consistently\noutperforms both pure convolutional and pure recurrent counterparts using\napproximately 20% less memory footprint, highlighting its suitability for\nmemory-constrained temporal processing at the edge. This highlights mGRADE's\npromise as an efficient solution for memory-constrained multi-scale temporal\nprocessing at the edge.", "AI": {"tldr": "The paper introduces mGRADE, a hybrid-memory model combining temporal convolution and minimal gated recurrence for efficient temporal processing on edge devices with constrained memory.", "motivation": "Edge devices face challenges in processing temporal data efficiently due to memory limits, requiring models that handle both short- and long-range dynamics.", "method": "The mGRADE framework integrates a learnable delay embedding via temporal 1D-convolution with a minimal gated recurrent unit (minGRU) to effectively combine short-term and long-term temporal processing while reducing memory overhead.", "result": "Experiments show mGRADE excels in synthetic tasks and image classification benchmarks, outperforming convolutional and recurrent models while reducing memory usage by approximately 20%.", "conclusion": "mGRADE is a practical and efficient solution for memory-constrained systems, achieving superior multi-scale temporal processing capabilities on edge devices."}}
{"id": "2507.01582", "pdf": "https://arxiv.org/pdf/2507.01582", "abs": "https://arxiv.org/abs/2507.01582", "authors": ["Jing Luo", "Xinyu Yang", "Jie Wei"], "title": "Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "comment": "Accepted by IEEE SMC 2025", "summary": "The creativity of classical music arises not only from composers who craft\nthe musical sheets but also from performers who interpret the static notations\nwith expressive nuances. This paper addresses the challenge of generating\nclassical piano performances from scratch, aiming to emulate the dual roles of\ncomposer and pianist in the creative process. We introduce the Expressive\nCompound Word (ECP) representation, which effectively captures both the\nmetrical structure and expressive nuances of classical performances. Building\non this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a\nmodel featuring two branches: a Vector Quantized Variational AutoEncoder\n(VQ-VAE) branch that generates score-related content, representing the\nComposer, and a vanilla VAE branch that produces expressive details, fulfilling\nthe role of Pianist. These branches are jointly trained with similar Seq2Seq\narchitectures, leveraging a multiscale encoder to capture beat-level contextual\ninformation and an orthogonal Transformer decoder for efficient compound tokens\ndecoding. Both objective and subjective evaluations demonstrate that XMVAE\ngenerates classical performances with superior musical quality compared to\nstate-of-the-art models. Furthermore, pretraining the Composer branch on extra\nmusical score datasets contribute to a significant performance gain.", "AI": {"tldr": "This paper focuses on generating classical piano performances by introducing the Expressive Compound Word representation and the XMVAE model, which jointly emulates a composer and pianist.", "motivation": "To address the challenge of emulating both the compositional and performative aspects in generating expressive classical piano performances.", "method": "The authors proposed XMVAE, a dual-branch model where one branch handles score generation using VQ-VAE, and the other handles expressive nuances using vanilla VAE, supported by multiscale encoding and an orthogonal Transformer decoder.", "result": "Quantitative and qualitative evaluations show XMVAE produces superior performances compared to state-of-the-art models, with observable benefits from pretraining the Composer branch on additional datasets.", "conclusion": "XMVAE effectively combines compositional creativity and performance expressiveness, offering enhanced musical quality in generated classical piano performances."}}
{"id": "2507.01927", "pdf": "https://arxiv.org/pdf/2507.01927", "abs": "https://arxiv.org/abs/2507.01927", "authors": ["Zhentan Zheng"], "title": "evMLP: An Efficient Event-Driven MLP Architecture for Vision", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks have achieved remarkable results in computer vision\ntasks. In the early days, Convolutional Neural Networks (CNNs) were the\nmainstream architecture. In recent years, Vision Transformers (ViTs) have\nbecome increasingly popular. In addition, exploring applications of multi-layer\nperceptrons (MLPs) has provided new perspectives for research into vision model\narchitectures. In this paper, we present evMLP accompanied by a simple\nevent-driven local update mechanism. The proposed evMLP can independently\nprocess patches on images or feature maps via MLPs. We define changes between\nconsecutive frames as \"events\". Under the event-driven local update mechanism,\nevMLP selectively processes patches where events occur. For sequential image\ndata (e.g., video processing), this approach improves computational performance\nby avoiding redundant computations. Through ImageNet image classification\nexperiments, evMLP attains accuracy competitive with state-of-the-art models.\nMore significantly, experimental results on multiple video datasets demonstrate\nthat evMLP reduces computational cost via its event-driven local update\nmechanism while maintaining output consistency with its non-event-driven\nbaseline. The code and trained models are available at\nhttps://github.com/i-evi/evMLP.", "AI": {"tldr": "The paper introduces evMLP, an efficient architecture for vision tasks using an event-driven local update mechanism that selectively processes image patches with changes.", "motivation": "The motivation is to explore innovative vision architectures beyond CNNs and ViTs, focusing on improving computational efficiency, especially for sequential image data like videos.", "method": "evMLP processes image patches using MLPs and utilizes an event-driven local update mechanism, where only patches with significant changes ('events') between consecutive frames are processed, reducing redundancy.", "result": "Experiments show evMLP achieves competitive accuracy on ImageNet while significantly reducing computational cost, maintaining consistency with the non-event-driven baseline in video processing.", "conclusion": "evMLP effectively balances accuracy and efficiency, presenting a promising approach for computationally demanding tasks like video data processing."}}
{"id": "2507.01938", "pdf": "https://arxiv.org/pdf/2507.01938", "abs": "https://arxiv.org/abs/2507.01938", "authors": ["Yiming Ju", "Jijin Hu", "Zhengxiong Luo", "Haoge Deng", "hanyu Zhao", "Li Du", "Chengwei Wu", "Donglin Hao", "Xinlong Wang", "Tengfei Pan"], "title": "CI-VID: A Coherent Interleaved Text-Video Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video (T2V) generation has recently attracted considerable attention,\nresulting in the development of numerous high-quality datasets that have\npropelled progress in this area. However, existing public datasets are\nprimarily composed of isolated text-video (T-V) pairs and thus fail to support\nthe modeling of coherent multi-clip video sequences. To address this\nlimitation, we introduce CI-VID, a dataset that moves beyond isolated\ntext-to-video (T2V) generation toward text-and-video-to-video (TV2V)\ngeneration, enabling models to produce coherent, multi-scene video sequences.\nCI-VID contains over 340,000 samples, each featuring a coherent sequence of\nvideo clips with text captions that capture both the individual content of each\nclip and the transitions between them, enabling visually and textually grounded\ngeneration. To further validate the effectiveness of CI-VID, we design a\ncomprehensive, multi-dimensional benchmark incorporating human evaluation,\nVLM-based assessment, and similarity-based metrics. Experimental results\ndemonstrate that models trained on CI-VID exhibit significant improvements in\nboth accuracy and content consistency when generating video sequences. This\nfacilitates the creation of story-driven content with smooth visual transitions\nand strong temporal coherence, underscoring the quality and practical utility\nof the CI-VID dataset We release the CI-VID dataset and the accompanying code\nfor data construction and evaluation at: https://github.com/ymju-BAAI/CI-VID", "AI": {"tldr": "The paper introduces CI-VID, a dataset for generating coherent multi-scene video sequences using text and videos, addressing limitations in existing datasets with isolated text-video pairs.", "motivation": "To overcome the limitations of existing text-to-video datasets, which fail to support coherent multi-clip video sequence modeling, and to enable the generation of story-driven sequences with smooth transitions.", "method": "The authors introduce CI-VID, a dataset containing over 340,000 text-captioned coherent video sequences. They also develop a multi-dimensional benchmark that uses human evaluation, VLM-based assessment, and similarity-based metrics to validate the dataset's effectiveness.", "result": "Experiments show that models trained on CI-VID demonstrate improved accuracy and consistency in generating video sequences, achieving smooth transitions and enhanced temporal coherence.", "conclusion": "CI-VID enhances the quality and utility of text-to-video generation, allowing for the creation of coherent, story-driven content. The dataset and its accompanying code are publicly released for broader community use."}}
{"id": "2507.01841", "pdf": "https://arxiv.org/pdf/2507.01841", "abs": "https://arxiv.org/abs/2507.01841", "authors": ["Yihang Gao", "Vincent Y. F. Tan"], "title": "Automatic Rank Determination for Low-Rank Adaptation via Submodular Function Maximization", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT", "math.OC"], "comment": null, "summary": "In this paper, we propose SubLoRA, a rank determination method for Low-Rank\nAdaptation (LoRA) based on submodular function maximization. In contrast to\nprior approaches, such as AdaLoRA, that rely on first-order (linearized)\napproximations of the loss function, SubLoRA utilizes second-order information\nto capture the potentially complex loss landscape by incorporating the Hessian\nmatrix. We show that the linearization becomes inaccurate and ill-conditioned\nwhen the LoRA parameters have been well optimized, motivating the need for a\nmore reliable and nuanced second-order formulation. To this end, we reformulate\nthe rank determination problem as a combinatorial optimization problem with a\nquadratic objective. However, solving this problem exactly is NP-hard in\ngeneral. To overcome the computational challenge, we introduce a submodular\nfunction maximization framework and devise a greedy algorithm with\napproximation guarantees. We derive a sufficient and necessary condition under\nwhich the rank-determination objective becomes submodular, and construct a\nclosed-form projection of the Hessian matrix that satisfies this condition\nwhile maintaining computational efficiency. Our method combines solid\ntheoretical foundations, second-order accuracy, and practical computational\nefficiency. We further extend SubLoRA to a joint optimization setting,\nalternating between LoRA parameter updates and rank determination under a rank\nbudget constraint. Extensive experiments on fine-tuning physics-informed neural\nnetworks (PINNs) for solving partial differential equations (PDEs) demonstrate\nthe effectiveness of our approach. Results show that SubLoRA outperforms\nexisting methods in both rank determination and joint training performance.", "AI": {"tldr": "The paper introduces SubLoRA, a second-order method leveraging submodular function maximization for rank determination in Low-Rank Adaptation (LoRA), outperforming previous approaches in accuracy and computational efficiency.", "motivation": "Existing methods like AdaLoRA rely on inaccurate first-order approximations for rank determination in LoRA, leading to ill-conditioned solutions after optimization.", "method": "SubLoRA reformulates rank determination as a combinatorial optimization problem involving a quadratic objective, utilizing the Hessian matrix and a submodular function maximization framework combined with a greedy algorithm.", "result": "SubLoRA surpasses prior methods in rank determination and joint training performance, as demonstrated through experiments fine-tuning physics-informed neural networks to solve PDEs.", "conclusion": "The proposed approach achieves accurate rank determination with theoretical robustness and practical efficiency, advancing the optimization of low-rank adaptations in neural networks."}}
{"id": "2507.01945", "pdf": "https://arxiv.org/pdf/2507.01945", "abs": "https://arxiv.org/abs/2507.01945", "authors": ["Nan Chen", "Mengqi Huang", "Yihao Meng", "Zhendong Mao"], "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local Memory", "categories": ["cs.CV"], "comment": null, "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.", "AI": {"tldr": "The study addresses issues of long-term color consistency in animation colorization through a novel framework called LongAnimation.", "motivation": "Animation colorization often involves high labor costs and challenges in maintaining long-term color consistency, motivating automated solutions.", "method": "The authors proposed LongAnimation framework, including SketchDiT for feature capturing, Dynamic Global-Local Memory (DGLM) for feature compression and fusion, and Color Consistency Reward for refining color consistency.", "result": "Experiments showed LongAnimation's efficacy in both short-term (14 frames) and long-term (500 frames) animations for ensuring color consistency.", "conclusion": "LongAnimation provides a dynamic global-local approach that successfully addresses challenges in long animation colorization with consistent coloring results."}}
{"id": "2507.01875", "pdf": "https://arxiv.org/pdf/2507.01875", "abs": "https://arxiv.org/abs/2507.01875", "authors": ["Gast\u00f3n Garc\u00eda Gonz\u00e1lez", "Pedro Casas", "Emilio Mart\u00ednez", "Alicia Fern\u00e1ndez"], "title": "Towards Foundation Auto-Encoders for Time-Series Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": "Presented at ACM KDD 2024, MiLeTS 2024 Workshop, August 25, 2024,\n  Barcelona, Spain", "summary": "We investigate a novel approach to time-series modeling, inspired by the\nsuccesses of large pretrained foundation models. We introduce FAE (Foundation\nAuto-Encoders), a foundation generative-AI model for anomaly detection in\ntime-series data, based on Variational Auto-Encoders (VAEs). By foundation, we\nmean a model pretrained on massive amounts of time-series data which can learn\ncomplex temporal patterns useful for accurate modeling, forecasting, and\ndetection of anomalies on previously unseen datasets. FAE leverages VAEs and\nDilated Convolutional Neural Networks (DCNNs) to build a generic model for\nunivariate time-series modeling, which could eventually perform properly in\nout-of-the-box, zero-shot anomaly detection applications. We introduce the main\nconcepts of FAE, and present preliminary results in different multi-dimensional\ntime-series datasets from various domains, including a real dataset from an\noperational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.", "AI": {"tldr": "The paper introduces Foundation Auto-Encoders (FAE), a VAE-based large foundation model aimed at anomaly detection in time-series data, featuring DCNNs and trained on vast datasets.", "motivation": "To enhance time-series modeling and anomaly detection through foundation models pretrained on large datasets, enabling out-of-the-box, zero-shot applications.", "method": "The FAE model utilizes Variational Auto-Encoders (VAEs) and Dilated Convolutional Neural Networks (DCNNs) for univariate time-series anomaly detection, trained on large-scale datasets.", "result": "Preliminary experiments demonstrate its effectiveness on datasets from diverse domains, including an operational mobile ISP and the KDD 2021 Anomaly Detection dataset.", "conclusion": "FAE showcases promising potential as a universal time-series anomaly detection tool with zero-shot capabilities, though further validation is needed."}}
{"id": "2507.01949", "pdf": "https://arxiv.org/pdf/2507.01949", "abs": "https://arxiv.org/abs/2507.01949", "authors": ["Kwai Keye Team", "Biao Yang", "Bin Wen", "Changyi Liu", "Chenglong Chu", "Chengru Song", "Chongling Rao", "Chuan Yi", "Da Li", "Dunju Zang", "Fan Yang", "Guorui Zhou", "Hao Peng", "Haojie Ding", "Jiaming Huang", "Jiangxia Cao", "Jiankang Chen", "Jingyun Hua", "Jin Ouyang", "Kaibing Chen", "Kaiyu Jiang", "Kaiyu Tang", "Kun Gai", "Shengnan Zhang", "Siyang Mao", "Sui Huang", "Tianke Zhang", "Tingting Gao", "Wei Chen", "Wei Yuan", "Xiangyu Wu", "Xiao Hu", "Xingyu Lu", "Yang Zhou", "Yi-Fan Zhang", "Yiping Yang", "Yulong Chen", "Zhenhua Wu", "Zhenyu Li", "Zhixin Ling", "Ziming Li", "Dehua Ma", "Di Xu", "Haixuan Gao", "Hang Li", "Jiawei Guo", "Jing Wang", "Lejian Ren", "Muhao Wei", "Qianqian Wang", "Qigen Hu", "Shiyao Wang", "Tao Yu", "Xinchen Luo", "Yan Li", "Yiming Liang", "Yuhang Hu", "Zeyi Lu", "Zhuoran Yang", "Zixing Zhang"], "title": "Kwai Keye-VL Technical Report", "categories": ["cs.CV"], "comment": "Technical Report: https://github.com/Kwai-Keye/Keye", "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce \\textbf{Kwai Keye-VL}, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the \\textbf{KC-MMBench}, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.", "AI": {"tldr": "This paper introduces Kwai Keye-VL, an 8-billion-parameter multimodal model optimized for short-video understanding using a novel four-stage pre-training and two-phase post-training process.", "motivation": "The dominance of short-form videos in the digital landscape necessitates the development of models that can comprehend dynamic and information-dense content, which current MLLMs struggle with.", "method": "A massive dataset of over 600 billion tokens and a specialized training recipe, including a four-stage pre-training process and a two-phase post-training process, were used. Innovations include a five-mode 'cold-start' data mixture and reinforcement learning for reasoning capabilities.", "result": "Keye-VL achieves state-of-the-art performance on public video benchmarks, remains competitive in general image-based tasks, and excels on the new KC-MMBench benchmark for real-world short-video scenarios.", "conclusion": "Kwai Keye-VL represents a significant advancement in short-video understanding with robust general-purpose vision-language capabilities, validated through extensive benchmarking and real-world testing."}}
{"id": "2507.01924", "pdf": "https://arxiv.org/pdf/2507.01924", "abs": "https://arxiv.org/abs/2507.01924", "authors": ["Samirah Bakker", "Yao Ma", "Seyed Sahand Mohammadi Ziabari"], "title": "Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The complexity of mental healthcare billing enables anomalies, including\nfraud. While machine learning methods have been applied to anomaly detection,\nthey often struggle with class imbalance, label scarcity, and complex\nsequential patterns. This study explores a hybrid deep learning approach\ncombining Long Short-Term Memory (LSTM) networks and Transformers, with\npseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior\nwork has not evaluated such hybrid models trained on pseudo-labeled data in the\ncontext of healthcare billing. The approach is evaluated on two real-world\nbilling datasets related to mental healthcare. The iForest LSTM baseline\nachieves the highest recall (0.963) on declaration-level data. On the\noperation-level data, the hybrid iForest-based model achieves the highest\nrecall (0.744), though at the cost of lower precision. These findings highlight\nthe potential of combining pseudo-labeling with hybrid deep learning in\ncomplex, imbalanced anomaly detection settings.", "AI": {"tldr": "This paper proposes a hybrid deep learning approach using LSTM, Transformers, and pseudo-labeling (via iForest and Autoencoders) to address anomaly detection challenges in mental healthcare billing, showing promising results on real-world datasets.", "motivation": "Mental healthcare billing is complex and prone to anomalies such as fraud, and existing methods struggle with challenges like data imbalance, label scarcity, and sequential patterns.", "method": "The paper combines LSTM and Transformers with pseudo-labeling techniques, employing Isolation Forests and Autoencoders, and evaluates the models on two real-world mental healthcare billing datasets.", "result": "The iForest LSTM baseline achieved recall of 0.963 on declaration-level data, while the hybrid iForest-based model achieved recall of 0.744 on operation-level data, albeit with lower precision.", "conclusion": "Hybrid deep learning models using pseudo-labeled data show promise for anomaly detection in challenging, imbalanced scenarios like mental healthcare billing."}}
{"id": "2507.01616", "pdf": "https://arxiv.org/pdf/2507.01616", "abs": "https://arxiv.org/abs/2507.01616", "authors": ["Chengkun He", "Xiangmin Zhou", "Chen Wang", "Longbing Cao", "Jie Shao", "Xiaodong Li", "Guang Xu", "Carrie Jinqiu Hu", "Zahir Tari"], "title": "Enhanced Influence-aware Group Recommendation for Online Media Propagation", "categories": ["cs.IR", "cs.AI", "cs.DB"], "comment": null, "summary": "Group recommendation over social media streams has attracted significant\nattention due to its wide applications in domains such as e-commerce,\nentertainment, and online news broadcasting. By leveraging social connections\nand group behaviours, group recommendation (GR) aims to provide more accurate\nand engaging content to a set of users rather than individuals. Recently,\ninfluence-aware GR has emerged as a promising direction, as it considers the\nimpact of social influence on group decision-making. In earlier work, we\nproposed Influence-aware Group Recommendation (IGR) to solve this task.\nHowever, this task remains challenging due to three key factors: the large and\never-growing scale of social graphs, the inherently dynamic nature of influence\npropagation within user groups, and the high computational overhead of\nreal-time group-item matching.\n  To tackle these issues, we propose an Enhanced Influence-aware Group\nRecommendation (EIGR) framework. First, we introduce a Graph Extraction-based\nSampling (GES) strategy to minimise redundancy across multiple temporal social\ngraphs and effectively capture the evolving dynamics of both groups and items.\nSecond, we design a novel DYnamic Independent Cascade (DYIC) model to predict\nhow influence propagates over time across social items and user groups.\nFinally, we develop a two-level hash-based User Group Index (UG-Index) to\nefficiently organise user groups and enable real-time recommendation\ngeneration. Extensive experiments on real-world datasets demonstrate that our\nproposed framework, EIGR, consistently outperforms state-of-the-art baselines\nin both effectiveness and efficiency.", "AI": {"tldr": "This paper introduces the Enhanced Influence-aware Group Recommendation (EIGR) framework that incorporates sampling strategies, dynamic propagation models, and indexing techniques for efficient and accurate group recommendations.", "motivation": "The goal of this paper is to address challenges in group recommendation systems, particularly the scalability of social graphs, dynamics of influence propagation, and computational efficiency for real-time recommendations.", "method": "The framework includes three key innovations: a Graph Extraction-based Sampling (GES) strategy, a DYnamic Independent Cascade (DYIC) model for influence prediction, and a two-level hash-based User Group Index (UG-Index) for efficient organization and matching.", "result": "Experiments on real-world datasets demonstrate that EIGR surpasses existing methods in both recommendation effectiveness and computational efficiency.", "conclusion": "EIGR successfully enhances influence-aware group recommendation systems by leveraging novel strategies and models, providing better scalability and real-time performance."}}
{"id": "2507.01953", "pdf": "https://arxiv.org/pdf/2507.01953", "abs": "https://arxiv.org/abs/2507.01953", "authors": ["Yukang Cao", "Chenyang Si", "Jinghao Wang", "Ziwei Liu"], "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model", "categories": ["cs.CV"], "comment": "ICCV 2025. Project page: https://yukangcao.github.io/FreeMorph/", "summary": "We present FreeMorph, the first tuning-free method for image morphing that\naccommodates inputs with different semantics or layouts. Unlike existing\nmethods that rely on finetuning pre-trained diffusion models and are limited by\ntime constraints and semantic/layout discrepancies, FreeMorph delivers\nhigh-fidelity image morphing without requiring per-instance training. Despite\ntheir efficiency and potential, tuning-free methods face challenges in\nmaintaining high-quality results due to the non-linear nature of the multi-step\ndenoising process and biases inherited from the pre-trained diffusion model. In\nthis paper, we introduce FreeMorph to address these challenges by integrating\ntwo key innovations. 1) We first propose a guidance-aware spherical\ninterpolation design that incorporates explicit guidance from the input images\nby modifying the self-attention modules, thereby addressing identity loss and\nensuring directional transitions throughout the generated sequence. 2) We\nfurther introduce a step-oriented variation trend that blends self-attention\nmodules derived from each input image to achieve controlled and consistent\ntransitions that respect both inputs. Our extensive evaluations demonstrate\nthat FreeMorph outperforms existing methods, being 10x ~ 50x faster and\nestablishing a new state-of-the-art for image morphing.", "AI": {"tldr": "The paper introduces FreeMorph, a tuning-free image morphing method leveraging diffusion models to overcome semantic/layout differences, outperforming existing approaches in speed and quality.", "motivation": "Existing morphing methods have limitations due to their reliance on fine-tuning pre-trained diffusion models, leading to inefficiencies and constraints when dealing with diverse semantics or layouts.", "method": "FreeMorph employs guidance-aware spherical interpolation with explicit input guidance and introduces step-oriented variation trends to blend self-attention modules for controlled transitions.", "result": "Compared to existing methods, FreeMorph achieves high-fidelity morphing, operates 10x ~ 50x faster, and sets a new state-of-the-art performance benchmark.", "conclusion": "FreeMorph addresses challenges of tuning-free morphing methods and offers an efficient, high-quality solution for image morphing with diverse inputs."}}
{"id": "2507.01955", "pdf": "https://arxiv.org/pdf/2507.01955", "abs": "https://arxiv.org/abs/2507.01955", "authors": ["Rahul Ramachandran", "Ali Garjani", "Roman Bachmann", "Andrei Atanov", "O\u011fuzhan Fatih Kar", "Amir Zamir"], "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page at https://fm-vision-evals.epfl.ch/", "summary": "Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.", "AI": {"tldr": "Multimodal foundation models have been benchmarked for standard computer vision tasks, revealing their strengths as generalists but limitations compared to specialist models.", "motivation": "To evaluate the current state of multimodal foundation models in visual understanding and benchmark their performance on standard computer vision tasks.", "method": "The researchers developed a standardized benchmarking framework by translating vision tasks into text-promptable and API-compatible formats using prompt chaining.", "result": "Multimodal models are respectable generalists but do not match specialist models in any task. They performed better on semantic tasks compared to geometric ones.", "conclusion": "While these models show promise as generalists, they are not yet ready to match state-of-the-art approaches in specialized computer vision tasks."}}
{"id": "2506.23121", "pdf": "https://arxiv.org/pdf/2506.23121", "abs": "https://arxiv.org/abs/2506.23121", "authors": ["Xinlei Yu", "Chanmiao Wang", "Hui Jin", "Ahmed Elazab", "Gangyong Jia", "Xiang Wan", "Changqing Zou", "Ruiquan Ge"], "title": "CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "19 pages, 9 figures, 10 tables", "summary": "Multi-organ medical segmentation is a crucial component of medical image\nprocessing, essential for doctors to make accurate diagnoses and develop\neffective treatment plans. Despite significant progress in this field, current\nmulti-organ segmentation models often suffer from inaccurate details,\ndependence on geometric prompts and loss of spatial information. Addressing\nthese challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal\nInteraction and Semantic Prompting based on SAM2. This model represents a\npromising approach to multi-organ medical segmentation guided by textual\ndescriptions of organs. Our method begins by converting visual and textual\ninputs into cross-modal contextualized semantics using a progressive\ncross-attention interaction mechanism. These semantics are then injected into\nthe image encoder to enhance the detailed understanding of visual information.\nTo eliminate reliance on geometric prompts, we use a semantic prompting\nstrategy, replacing the original prompt encoder to sharpen the perception of\nchallenging targets. In addition, a similarity-sorting self-updating strategy\nfor memory and a mask-refining process is applied to further adapt to medical\nimaging and enhance localized details. Comparative experiments conducted on\nseven public datasets indicate that CRISP-SAM2 outperforms existing models.\nExtensive analysis also demonstrates the effectiveness of our method, thereby\nconfirming its superior performance, especially in addressing the limitations\nmentioned earlier. Our code is available at:\nhttps://github.com/YU-deep/CRISP\\_SAM2.git.", "AI": {"tldr": "The paper introduces CRISP-SAM2, a novel model enhancing multi-organ medical segmentation by utilizing cross-modal integration and semantic prompting strategies, achieving superior performance on seven datasets.", "motivation": "To address inaccuracies, reliance on geometric prompts, and loss of spatial details in multi-organ medical segmentation models.", "method": "The method involves converting visual and textual inputs into cross-modal contextualized semantics through a progressive cross-attention interaction mechanism. Semantic prompting replaces geometric prompting, while additional techniques like similarity-sorting self-updating and mask refinement further improve segmentation. The model is guided by textual descriptions of organs.", "result": "CRISP-SAM2 achieves superior results, outperforming existing multi-organ segmentation models on seven public datasets.", "conclusion": "The proposed method enhances the precision, adaptability, and local detail perception of medical image segmentation models, addressing key limitations of previous approaches."}}
{"id": "2507.01957", "pdf": "https://arxiv.org/pdf/2507.01957", "abs": "https://arxiv.org/abs/2507.01957", "authors": ["Zhuoyang Zhang", "Luke J. Huang", "Chengyue Wu", "Shang Yang", "Kelly Peng", "Yao Lu", "Song Han"], "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "The first two authors contributed equally to this work", "summary": "We present Locality-aware Parallel Decoding (LPD) to accelerate\nautoregressive image generation. Traditional autoregressive image generation\nrelies on next-patch prediction, a memory-bound process that leads to high\nlatency. Existing works have tried to parallelize next-patch prediction by\nshifting to multi-patch prediction to accelerate the process, but only achieved\nlimited parallelization. To achieve high parallelization while maintaining\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\nordering and degrees of parallelization. It uses learnable position query\ntokens to guide generation at target positions while ensuring mutual visibility\namong concurrently generated tokens for consistent parallel decoding. (2)\nLocality-aware Generation Ordering, a novel schedule that forms groups to\nminimize intra-group dependencies and maximize contextual support, enhancing\ngeneration quality. With these designs, we reduce the generation steps from 256\nto 20 (256$\\times$256 res.) and 1024 to 48 (512$\\times$512 res.) without\ncompromising quality on the ImageNet class-conditional generation, and\nachieving at least 3.4$\\times$ lower latency than previous parallelized\nautoregressive models.", "AI": {"tldr": "A method called Locality-aware Parallel Decoding (LPD) is introduced to speed up autoregressive image generation, reducing generation steps significantly and achieving over 3.4\u00d7 lower latency with preserved quality.", "motivation": "The paper addresses the issue of high latency in traditional autoregressive image generation caused by the memory-bound next-patch prediction process.", "method": "Two key techniques are proposed: (1) Flexible Parallelized Autoregressive Modeling, a novel architecture enabling arbitrary generation ordering and parallelization with learnable position query tokens ensuring consistent decoding; (2) Locality-aware Generation Ordering, a scheduling approach optimizing dependency minimization and contextual support.", "result": "The proposed method reduces generation steps from 256 to 20 for 256\u00d7256 resolution and from 1024 to 48 for 512\u00d7512 resolution, all while maintaining generation quality. At least 3.4\u00d7 lower latency is achieved compared to existing models.", "conclusion": "Locality-aware Parallel Decoding significantly accelerates autoregressive image generation without quality loss, demonstrating the effectiveness of the novel architecture and generation scheduling techniques."}}
{"id": "2507.01020", "pdf": "https://arxiv.org/pdf/2507.01020", "abs": "https://arxiv.org/abs/2507.01020", "authors": ["Aashray Reddy", "Andrew Zagula", "Nicholas Saban"], "title": "AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models", "categories": ["cs.CR", "cs.LG"], "comment": "16 pages, 4 figures, submitted to LLMSEC", "summary": "Large Language Models (LLMs) continue to exhibit vulnerabilities to\njailbreaking attacks: carefully crafted malicious inputs intended to circumvent\nsafety guardrails and elicit harmful responses. As such, we present AutoAdv, a\nnovel framework that automates adversarial prompt generation to systematically\nevaluate and expose vulnerabilities in LLM safety mechanisms. Our approach\nleverages a parametric attacker LLM to produce semantically disguised malicious\nprompts through strategic rewriting techniques, specialized system prompts, and\noptimized hyperparameter configurations. The primary contribution of our work\nis a dynamic, multi-turn attack methodology that analyzes failed jailbreak\nattempts and iteratively generates refined follow-up prompts, leveraging\ntechniques such as roleplaying, misdirection, and contextual manipulation. We\nquantitatively evaluate attack success rate (ASR) using the StrongREJECT\n(arXiv:2402.10260 [cs.CL]) framework across sequential interaction turns.\nThrough extensive empirical evaluation of state-of-the-art models--including\nChatGPT, Llama, and DeepSeek--we reveal significant vulnerabilities, with our\nautomated attacks achieving jailbreak success rates of up to 86% for harmful\ncontent generation. Our findings reveal that current safety mechanisms remain\nsusceptible to sophisticated multi-turn attacks, emphasizing the urgent need\nfor more robust defense strategies.", "AI": {"tldr": "The paper presents AutoAdv, a framework to automate adversarial prompt generation, exploiting vulnerabilities in LLM safety with up to 86% jailbreak success.", "motivation": "To expose and evaluate vulnerabilities in LLM safety mechanisms, especially those susceptible to sophisticated adversarial attacks.", "method": "AutoAdv employs a parametric attacker LLM to dynamically generate adversarial prompts using iterative rewriting, roleplaying, misdirection, and contextual manipulation techniques.", "result": "Extensive testing on state-of-the-art LLMs like ChatGPT shows vulnerabilities, achieving jailbreak success rates up to 86%.", "conclusion": "Current LLM safety mechanisms are inadequate against advanced multi-turn attacks, necessitating improved defense strategies."}}
{"id": "2507.01022", "pdf": "https://arxiv.org/pdf/2507.01022", "abs": "https://arxiv.org/abs/2507.01022", "authors": ["Shayan Dadman", "Bernt Arild Bremdal", "Andreas Bergsland"], "title": "Workflow-Based Evaluation of Music Generation Systems", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.MM", "cs.SD"], "comment": "54 pages, 3 figures, 6 tables, 5 appendices", "summary": "This study presents an exploratory evaluation of Music Generation Systems\n(MGS) within contemporary music production workflows by examining eight\nopen-source systems. The evaluation framework combines technical insights with\npractical experimentation through criteria specifically designed to investigate\nthe practical and creative affordances of the systems within the iterative,\nnon-linear nature of music production. Employing a single-evaluator methodology\nas a preliminary phase, this research adopts a mixed approach utilizing\nqualitative methods to form hypotheses subsequently assessed through\nquantitative metrics. The selected systems represent architectural diversity\nacross both symbolic and audio-based music generation approaches, spanning\ncomposition, arrangement, and sound design tasks. The investigation addresses\nlimitations of current MGS in music production, challenges and opportunities\nfor workflow integration, and development potential as collaborative tools\nwhile maintaining artistic authenticity. Findings reveal these systems function\nprimarily as complementary tools enhancing rather than replacing human\nexpertise. They exhibit limitations in maintaining thematic and structural\ncoherence that emphasize the indispensable role of human creativity in tasks\ndemanding emotional depth and complex decision-making. This study contributes a\nstructured evaluation framework that considers the iterative nature of music\ncreation. It identifies methodological refinements necessary for subsequent\ncomprehensive evaluations and determines viable areas for AI integration as\ncollaborative tools in creative workflows. The research provides\nempirically-grounded insights to guide future development in the field.", "AI": {"tldr": "This study evaluates eight open-source music generation systems within music production workflows, addressing their capabilities, limitations, and potential for collaborative use with humans.", "motivation": "The study aims to assess music generation systems' practical and creative value in contemporary music production and identify opportunities for improving workflow integration and collaboration.", "method": "It uses a preliminary single-evaluator methodology, combining qualitative hypothesis formation and quantitative metrics evaluation across diverse symbolic and audio-based systems.", "result": "Findings indicate that music generation systems enhance rather than replace human expertise, showing limitations in thematic and structural coherence but valuable as complementary tools.", "conclusion": "The research provides a structured evaluation framework, identifies areas for refinement, and offers insights for advancing these systems as collaborative tools in music production workflows."}}
{"id": "2507.01038", "pdf": "https://arxiv.org/pdf/2507.01038", "abs": "https://arxiv.org/abs/2507.01038", "authors": ["Seong-Joon Park", "Hee-Youl Kwak", "Sang-Hyo Kim", "Yongjune Kim", "Jong-Seon No"], "title": "Cross-Attention Message-Passing Transformers for Code-Agnostic Decoding in 6G Networks", "categories": ["cs.IT", "cs.LG", "eess.SP", "math.IT"], "comment": null, "summary": "Channel coding for 6G networks is expected to support a wide range of\nrequirements arising from heterogeneous communication scenarios. These demands\nchallenge traditional code-specific decoders, which lack the flexibility and\nscalability required for next-generation systems. To tackle this problem, we\npropose an AI-native foundation model for unified and code-agnostic decoding\nbased on the transformer architecture. We first introduce a cross-attention\nmessage-passing transformer (CrossMPT). CrossMPT employs two masked\ncross-attention blocks that iteratively update two distinct input\nrepresentations-magnitude and syndrome vectors-allowing the model to\neffectively learn the decoding problem. Notably, our CrossMPT has achieved\nstate-of-the-art decoding performance among single neural decoders. Building on\nthis, we develop foundation CrossMPT (FCrossMPT) by making the architecture\ninvariant to code length, rate, and class, allowing a single trained model to\ndecode a broad range of codes without retraining. To further enhance decoding\nperformance, particularly for short blocklength codes, we propose CrossMPT\nensemble decoder (CrossED), an ensemble decoder composed of multiple parallel\nCrossMPT blocks employing different parity-check matrices. This architecture\ncan also serve as a foundation model, showing strong generalization across\ndiverse code types. Overall, the proposed AI-native code-agnostic decoder\noffers flexibility, scalability, and high performance, presenting a promising\ndirection to channel coding for 6G networks.", "AI": {"tldr": "This paper introduces an AI-native decoding framework using the transformer architecture for 6G channel coding, achieving flexibility, scalability, and superior performance across diverse code types.", "motivation": "To address the limitations of traditional code-specific decoders, which lack flexibility and scalability to meet the diverse requirements of 6G networks.", "method": "Proposes the CrossMPT (cross-attention message-passing transformer) for decoding, which iteratively updates input representations to learn decoding problems effectively. Enhances it into FCrossMPT for broader code adaptation and introduces CrossED for short blocklength performance using ensemble decoding.", "result": "Achieves state-of-the-art single neural decoder performance, broad applicability across different codes, and improved results for short blocklength codes with the ensemble approach.", "conclusion": "The proposed AI-native, code-agnostic decoder using transformer architecture is a flexible, scalable, and high-performing solution for channel coding in 6G networks."}}
{"id": "2507.01066", "pdf": "https://arxiv.org/pdf/2507.01066", "abs": "https://arxiv.org/abs/2507.01066", "authors": ["Hanzhong Liang", "Jinghao Shi", "Xiang Shen", "Zixuan Wang", "Vera Wen", "Ardalan Mehrani", "Zhiqian Chen", "Yifan Wu", "Zhixin Zhang"], "title": "Embedding-based Retrieval in Multimodal Content Moderation", "categories": ["cs.IR", "cs.CV", "cs.LG"], "comment": "Camera ready for SIGIR 2025", "summary": "Video understanding plays a fundamental role for content moderation on short\nvideo platforms, enabling the detection of inappropriate content. While\nclassification remains the dominant approach for content moderation, it often\nstruggles in scenarios requiring rapid and cost-efficient responses, such as\ntrend adaptation and urgent escalations. To address this issue, we introduce an\nEmbedding-Based Retrieval (EBR) method designed to complement traditional\nclassification approaches. We first leverage a Supervised Contrastive Learning\n(SCL) framework to train a suite of foundation embedding models, including both\nsingle-modal and multi-modal architectures. Our models demonstrate superior\nperformance over established contrastive learning methods such as CLIP and\nMoCo. Building on these embedding models, we design and implement the\nembedding-based retrieval system that integrates embedding generation and video\nretrieval to enable efficient and effective trend handling. Comprehensive\noffline experiments on 25 diverse emerging trends show that EBR improves\nROC-AUC from 0.85 to 0.99 and PR-AUC from 0.35 to 0.95. Further online\nexperiments reveal that EBR increases action rates by 10.32% and reduces\noperational costs by over 80%, while also enhancing interpretability and\nflexibility compared to classification-based solutions.", "AI": {"tldr": "The paper introduces an Embedding-Based Retrieval (EBR) method for content moderation on short video platforms, outperforming traditional classification in trend adaptation and urgent needs.", "motivation": "To address the limitations of traditional classification approaches in providing rapid responses needed for content moderation, especially during emergent trends and escalations.", "method": "The study utilizes Supervised Contrastive Learning to develop single-modal and multi-modal foundation embedding models and integrates them into an efficient embedding-based retrieval system.", "result": "The EBR method improves ROC-AUC from 0.85 to 0.99, PR-AUC from 0.35 to 0.95 in offline evaluations. Online tests show a 10.32% increase in action rates, over 80% reduction in operational costs, and better interpretability compared to alternatives.", "conclusion": "EBR proves to be an efficient, cost-effective, and interpretable alternative to classification-based solutions, emphasizing its potential for rapid and adaptable content moderation."}}
{"id": "2507.01074", "pdf": "https://arxiv.org/pdf/2507.01074", "abs": "https://arxiv.org/abs/2507.01074", "authors": ["N. P. Garc\u00eda-de-la-Puente", "Roc\u00edo del Amor", "Fernando Garc\u00eda-Torres", "Niels M\u00f8ller Israelsen", "Coraline Lapre", "Christian Rosenberg Petersen", "Ole Bang", "Dominik Brouczek", "Martin Schwentenwein", "Kevin Neumann", "Niels Benson", "Valery Naranjo"], "title": "MID-INFRARED (MIR) OCT-based inspection in industry", "categories": ["eess.IV", "cs.CV"], "comment": "Paper accepted at i-ESA 2024 12th International Conference on\n  Interoperability for Enterprise Systems and Applications 6 pages, 2 figures,\n  2 tables", "summary": "This paper aims to evaluate mid-infrared (MIR) Optical Coherence Tomography\n(OCT) systems as a tool to penetrate different materials and detect sub-surface\nirregularities. This is useful for monitoring production processes, allowing\nNon-Destructive Inspection Techniques of great value to the industry. In this\nexploratory study, several acquisitions are made on composite and ceramics to\nknow the capabilities of the system. In addition, it is assessed which\npreprocessing and AI-enhanced vision algorithms can be anomaly-detection\nmethodologies capable of detecting abnormal zones in the analyzed objects.\nLimitations and criteria for the selection of optimal parameters will be\ndiscussed, as well as strengths and weaknesses will be highlighted.", "AI": {"tldr": "The paper investigates the use of mid-infrared Optical Coherence Tomography (MIR-OCT) and AI-enhanced vision algorithms for detecting sub-surface irregularities in composites and ceramics.", "motivation": "To provide non-destructive inspection techniques for industrial production processes to identify and address sub-surface irregularities in materials.", "method": "The study utilizes MIR-OCT systems on composite and ceramic samples, combining preprocessing and AI vision algorithms for anomaly detection. It also evaluates limitations and criteria for parameter optimization.", "result": "Demonstrated the capabilities of MIR-OCT systems and AI techniques for detecting sub-surface abnormalities, along with identified strengths and weaknesses of the approach.", "conclusion": "MIR-OCT systems and AI-based methodologies show promise as non-destructive inspection tools, with further refinement needed for optimal parameter selection and overcoming limitations."}}
{"id": "2507.01060", "pdf": "https://arxiv.org/pdf/2507.01060", "abs": "https://arxiv.org/abs/2507.01060", "authors": ["Kang Liu"], "title": "Optimizing Conversational Product Recommendation via Reinforcement Learning", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "We propose a reinforcement learning-based approach to optimize conversational\nstrategies for product recommendation across diverse industries. As\norganizations increasingly adopt intelligent agents to support sales and\nservice operations, the effectiveness of a conversation hinges not only on what\nis recommended but how and when recommendations are delivered. We explore a\nmethodology where agentic systems learn optimal dialogue policies through\nfeedback-driven reinforcement learning. By mining aggregate behavioral patterns\nand conversion outcomes, our approach enables agents to refine talk tracks that\ndrive higher engagement and product uptake, while adhering to contextual and\nregulatory constraints. We outline the conceptual framework, highlight key\ninnovations, and discuss the implications for scalable, personalized\nrecommendation in enterprise environments.", "AI": {"tldr": "The paper presents a reinforcement learning approach to improve conversational strategies for product recommendation.", "motivation": "Organizations rely on intelligent agents for sales and service operations, creating a need for optimized conversational strategies that enhance product engagement and uptake.", "method": "Leverage reinforcement learning to train agentic systems in adapting dialogue policies based on behavioral patterns and conversion outcomes.", "result": "The approach optimizes talk tracks to increase engagement and product uptake while maintaining contextual and regulatory compliance.", "conclusion": "Scalable and personalized recommendation strategies in enterprise environments can be achieved through reinforcement learning-driven conversational optimization."}}
{"id": "2507.01279", "pdf": "https://arxiv.org/pdf/2507.01279", "abs": "https://arxiv.org/abs/2507.01279", "authors": ["Ahmad Chaddad", "Jihao Peng", "Yihang Wu"], "title": "Classification based deep learning models for lung cancer and disease using medical images", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted in IEEE Transactions on Radiation and Plasma Medical\n  Sciences", "summary": "The use of deep learning (DL) in medical image analysis has significantly\nimproved the ability to predict lung cancer. In this study, we introduce a\nnovel deep convolutional neural network (CNN) model, named ResNet+, which is\nbased on the established ResNet framework. This model is specifically designed\nto improve the prediction of lung cancer and diseases using the images. To\naddress the challenge of missing feature information that occurs during the\ndownsampling process in CNNs, we integrate the ResNet-D module, a variant\ndesigned to enhance feature extraction capabilities by modifying the\ndownsampling layers, into the traditional ResNet model. Furthermore, a\nconvolutional attention module was incorporated into the bottleneck layers to\nenhance model generalization by allowing the network to focus on relevant\nregions of the input images. We evaluated the proposed model using five public\ndatasets, comprising lung cancer (LC2500 $n$=3183, IQ-OTH/NCCD $n$=1336, and\nLCC $n$=25000 images) and lung disease (ChestXray $n$=5856, and COVIDx-CT\n$n$=425024 images). To address class imbalance, we used data augmentation\ntechniques to artificially increase the representation of underrepresented\nclasses in the training dataset. The experimental results show that ResNet+\nmodel demonstrated remarkable accuracy/F1, reaching 98.14/98.14\\% on the\nLC25000 dataset and 99.25/99.13\\% on the IQ-OTH/NCCD dataset. Furthermore, the\nResNet+ model saved computational cost compared to the original ResNet series\nin predicting lung cancer images. The proposed model outperformed the baseline\nmodels on publicly available datasets, achieving better performance metrics.\nOur codes are publicly available at\nhttps://github.com/AIPMLab/Graduation-2024/tree/main/Peng.", "AI": {"tldr": "This study introduces ResNet+, an enhanced CNN based on ResNet, for improved lung cancer prediction from medical images, featuring attention modules and ResNet-D enhancements.", "motivation": "Deep learning has shown promise in medical image analysis, but challenges remain in extracting meaningful features during downsampling in CNNs for lung cancer prediction.", "method": "The researchers enhanced the ResNet architecture by integrating the ResNet-D module for better feature extraction and adding a convolutional attention module to focus on important image regions. They employed data augmentation to address class imbalances.", "result": "ResNet+ achieved superior performance, with accuracy/F1 reaching up to 99.25/99.13% on certain datasets. It also demonstrated computational efficiency compared to original ResNet models.", "conclusion": "ResNet+ significantly improves the prediction of lung cancer and other lung diseases, outperforming existing models, while maintaining efficiency and generalizability."}}
{"id": "2507.01291", "pdf": "https://arxiv.org/pdf/2507.01291", "abs": "https://arxiv.org/abs/2507.01291", "authors": ["Wenxuan Li", "Xinze Zhou", "Qi Chen", "Tianyu Lin", "Pedro R. A. S. Bassi", "Szymon Plotka", "Jaroslaw B. Cwikla", "Xiaoxi Chen", "Chen Ye", "Zheren Zhu", "Kai Ding", "Heng Li", "Kang Wang", "Yang Yang", "Yucheng Tang", "Daguang Xu", "Alan L. Yuille", "Zongwei Zhou"], "title": "PanTS: The Pancreatic Tumor Segmentation Dataset", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "PanTS is a large-scale, multi-institutional dataset curated to advance\nresearch in pancreatic CT analysis. It contains 36,390 CT scans from 145\nmedical centers, with expert-validated, voxel-wise annotations of over 993,000\nanatomical structures, covering pancreatic tumors, pancreas head, body, and\ntail, and 24 surrounding anatomical structures such as vascular/skeletal\nstructures and abdominal/thoracic organs. Each scan includes metadata such as\npatient age, sex, diagnosis, contrast phase, in-plane spacing, slice thickness,\netc. AI models trained on PanTS achieve significantly better performance in\npancreatic tumor detection, localization, and segmentation compared to those\ntrained on existing public datasets. Our analysis indicates that these gains\nare directly attributable to the 16x larger-scale tumor annotations and\nindirectly supported by the 24 additional surrounding anatomical structures. As\nthe largest and most comprehensive resource of its kind, PanTS offers a new\nbenchmark for developing and evaluating AI models in pancreatic CT analysis.", "AI": {"tldr": "PanTS is a large-scale dataset featuring over 36,000 CT scans with detailed annotations to significantly enhance AI performance in pancreatic tumor analysis.", "motivation": "To address limitations in existing datasets and improve AI-driven analysis of pancreatic tumors through large-scale, high-quality annotations and multi-institutional diversity.", "method": "Researchers compiled 36,390 CT scans from 145 centers, providing voxel-wise annotations for over 993,000 anatomical structures along with metadata and using these to train and benchmark AI models.", "result": "AI trained on PanTS showed markedly superior performance in detecting and segmenting pancreatic tumors compared to models using existing datasets.", "conclusion": "PanTS establishes a new standard for AI in pancreatic CT analysis, emphasizing its potential for advancing research and clinical applications."}}
{"id": "2507.01110", "pdf": "https://arxiv.org/pdf/2507.01110", "abs": "https://arxiv.org/abs/2507.01110", "authors": ["Felix Windisch", "Lukas Radl", "Thomas K\u00f6hler", "Michael Steiner", "Dieter Schmalstieg", "Markus Steinberger"], "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory", "categories": ["cs.GR", "cs.LG"], "comment": null, "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details.", "AI": {"tldr": "The paper introduces a framework for dynamic streaming and rendering ultra-large Gaussian scenes on a single consumer GPU, avoiding scene partitioning.", "motivation": "To address the difficulty of scaling Gaussian Splatting to large environments while maintaining real-time rendering and minimizing GPU memory dependency.", "method": "The paper developed a Level-of-Detail (LoD) representation combined with Gaussian hierarchies and Sequential Point Trees, managing view-dependent LoD selection and leveraging caching for real-time streaming.", "result": "The framework enables ultra-large-scale scenes to be trained efficiently and rendered in real-time without GPU memory constraints.", "conclusion": "The method offers seamless multi-scale reconstruction and interactive visualization for complex scenes without introducing chunk-based artifacts."}}
{"id": "2507.01323", "pdf": "https://arxiv.org/pdf/2507.01323", "abs": "https://arxiv.org/abs/2507.01323", "authors": ["Rongchang Zhao", "Huanchi Liu", "Jian Zhang"], "title": "SWinMamba: Serpentine Window State Space Model for Vascular Segmentation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Vascular segmentation in medical images is crucial for disease diagnosis and\nsurgical navigation. However, the segmented vascular structure is often\ndiscontinuous due to its slender nature and inadequate prior modeling. In this\npaper, we propose a novel Serpentine Window Mamba (SWinMamba) to achieve\naccurate vascular segmentation. The proposed SWinMamba innovatively models the\ncontinuity of slender vascular structures by incorporating serpentine window\nsequences into bidirectional state space models. The serpentine window\nsequences enable efficient feature capturing by adaptively guiding global\nvisual context modeling to the vascular structure. Specifically, the Serpentine\nWindow Tokenizer (SWToken) adaptively splits the input image using overlapping\nserpentine window sequences, enabling flexible receptive fields (RFs) for\nvascular structure modeling. The Bidirectional Aggregation Module (BAM)\nintegrates coherent local features in the RFs for vascular continuity\nrepresentation. In addition, dual-domain learning with Spatial-Frequency Fusion\nUnit (SFFU) is designed to enhance the feature representation of vascular\nstructure. Extensive experiments on three challenging datasets demonstrate that\nthe proposed SWinMamba achieves superior performance with complete and\nconnected vessels.", "AI": {"tldr": "The paper presents SWinMamba, a novel method for accurate vascular segmentation in medical images using serpentine window sequences and bidirectional state space models.", "motivation": "Vascular segmentation is vital for diagnosis and surgical navigation but is often discontinuous due to the slender nature of vascular structures and insufficient prior modeling.", "method": "The proposed method, SWinMamba, utilizes serpentine window sequences to model the continuity of vascular structures. It includes a Serpentine Window Tokenizer (SWToken) for adaptive splitting, a Bidirectional Aggregation Module (BAM) for coherent local feature integration, and dual-domain learning with a Spatial-Frequency Fusion Unit (SFFU) for enhanced feature representation.", "result": "Experiments across three datasets demonstrated excellent performance, with SWinMamba achieving superior vascular segmentation accuracy and connectivity.", "conclusion": "The SWinMamba model effectively addresses the challenge of discontinuous vascular segmentation, offering a robust and accurate solution."}}
{"id": "2507.01701", "pdf": "https://arxiv.org/pdf/2507.01701", "abs": "https://arxiv.org/abs/2507.01701", "authors": ["Bochen Han", "Songmao Zhang"], "title": "Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "In this paper, we propose to incorporate the blackboard architecture into LLM\nmulti-agent systems (MASs) so that (1) agents with various roles can share all\nthe information and others' messages during the whole problem-solving process,\n(2) agents that will take actions are selected based on the current content of\nthe blackboard, and (3) the selection and execution round is repeated until a\nconsensus is reached on the blackboard. We develop the first implementation of\nthis proposal and conduct experiments on commonsense knowledge, reasoning and\nmathematical datasets. The results show that our system can be competitive with\nthe SOTA static and dynamic MASs by achieving the best average performance, and\nat the same time manage to spend less tokens. Our proposal has the potential to\nenable complex and dynamic problem-solving where well-defined structures or\nworkflows are unavailable.", "AI": {"tldr": "This paper integrates the blackboard architecture into LLM-based multi-agent systems for dynamic and collaborative problem-solving, achieving competitive results with reduced token usage.", "motivation": "Dynamic and complex problem-solving often lacks well-defined workflows, necessitating innovative approaches for effective collaboration in multi-agent systems.", "method": "The blackboard architecture is incorporated into multi-agent systems, enabling agents to share information, select actions based on a central hub, and iteratively reach consensus during problem-solving.", "result": "Experiments on commonsense knowledge, reasoning, and mathematical tasks showcased competitive performance with static and dynamic MASs, while reducing token usage.", "conclusion": "This approach demonstrates the potential to enable dynamic and complex problem-solving, offering improved average performance and efficiency over existing MAS approaches."}}
{"id": "2507.01326", "pdf": "https://arxiv.org/pdf/2507.01326", "abs": "https://arxiv.org/abs/2507.01326", "authors": ["Dong Liang", "Xingyu Qiu", "Yuzhen Li", "Wei Wang", "Kuanquan Wang", "Suyu Dong", "Gongning Luo"], "title": "Structure and Smoothness Constrained Dual Networks for MR Bias Field Correction", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 3 figures, accepted by MICCAI", "summary": "MR imaging techniques are of great benefit to disease diagnosis. However, due\nto the limitation of MR devices, significant intensity inhomogeneity often\nexists in imaging results, which impedes both qualitative and quantitative\nmedical analysis. Recently, several unsupervised deep learning-based models\nhave been proposed for MR image improvement. However, these models merely\nconcentrate on global appearance learning, and neglect constraints from image\nstructures and smoothness of bias field, leading to distorted corrected\nresults. In this paper, novel structure and smoothness constrained dual\nnetworks, named S2DNets, are proposed aiming to self-supervised bias field\ncorrection. S2DNets introduce piece-wise structural constraints and smoothness\nof bias field for network training to effectively remove non-uniform intensity\nand retain much more structural details. Extensive experiments executed on both\nclinical and simulated MR datasets show that the proposed model outperforms\nother conventional and deep learning-based models. In addition to comparison on\nvisual metrics, downstream MR image segmentation tasks are also used to\nevaluate the impact of the proposed model. The source code is available at:\nhttps://github.com/LeongDong/S2DNets}{https://github.com/LeongDong/S2DNets.", "AI": {"tldr": "This paper presents S2DNets, a self-supervised deep learning approach for correcting intensity inhomogeneities in MR images while preserving structural details.", "motivation": "To address the issue of intensity inhomogeneities in MR images caused by device limitations, which hampers both qualitative and quantitative medical analysis.", "method": "The authors developed S2DNets, a dual network approach with piece-wise structural constraints and smoothness enforcement, enabling self-supervised bias field correction.", "result": "Extensive experiments on clinical and simulated MR datasets show that S2DNets outperform conventional and other deep learning models both visually and in downstream segmentation tasks.", "conclusion": "The proposed S2DNets effectively correct intensity biases in MR images, preserve structural details, and improve medical image analysis tasks."}}
{"id": "2507.01387", "pdf": "https://arxiv.org/pdf/2507.01387", "abs": "https://arxiv.org/abs/2507.01387", "authors": ["Ahmad Soliman", "Ron Keuth", "Marian Himstedt"], "title": "BronchoGAN: Anatomically consistent and domain-agnostic image-to-image translation for video bronchoscopy", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The limited availability of bronchoscopy images makes image synthesis\nparticularly interesting for training deep learning models. Robust image\ntranslation across different domains -- virtual bronchoscopy, phantom as well\nas in-vivo and ex-vivo image data -- is pivotal for clinical applications. This\npaper proposes BronchoGAN introducing anatomical constraints for image-to-image\ntranslation being integrated into a conditional GAN. In particular, we force\nbronchial orifices to match across input and output images. We further propose\nto use foundation model-generated depth images as intermediate representation\nensuring robustness across a variety of input domains establishing models with\nsubstantially less reliance on individual training datasets. Moreover our\nintermediate depth image representation allows to easily construct paired image\ndata for training. Our experiments showed that input images from different\ndomains (e.g. virtual bronchoscopy, phantoms) can be successfully translated to\nimages mimicking realistic human airway appearance. We demonstrated that\nanatomical settings (i.e. bronchial orifices) can be robustly preserved with\nour approach which is shown qualitatively and quantitatively by means of\nimproved FID, SSIM and dice coefficients scores. Our anatomical constraints\nenabled an improvement in the Dice coefficient of up to 0.43 for synthetic\nimages. Through foundation models for intermediate depth representations,\nbronchial orifice segmentation integrated as anatomical constraints into\nconditional GANs we are able to robustly translate images from different\nbronchoscopy input domains. BronchoGAN allows to incorporate public CT scan\ndata (virtual bronchoscopy) in order to generate large-scale bronchoscopy image\ndatasets with realistic appearance. BronchoGAN enables to bridge the gap of\nmissing public bronchoscopy images.", "AI": {"tldr": "BronchoGAN uses conditional GANs and anatomical constraints for image-to-image translation across bronchoscopy image domains, generating realistic synthetic images and enabling robust model training.", "motivation": "The scarcity of bronchoscopy images limits the training of deep learning models, necessitating effective image synthesis methods for robust clinical applications.", "method": "BronchoGAN integrates anatomical constraints into conditional GANs, leverages foundation model-generated depth images for intermediate representations, and ensures bronchial orifice consistency between input and output images, facilitating cross-domain learning.", "result": "The approach successfully translates inputs from various domains into realistic images, improving FID, SSIM, and Dice scores, with an up to 0.43 improvement in the Dice coefficient for synthetic images.", "conclusion": "BronchoGAN addresses the scarcity of bronchoscopy datasets by generating realistic, anatomically accurate images, enabling public CT scan data to be used for large-scale dataset creation."}}
{"id": "2507.01260", "pdf": "https://arxiv.org/pdf/2507.01260", "abs": "https://arxiv.org/abs/2507.01260", "authors": ["Y. Suzuki", "Y. Yukutake", "T. Ohminato", "M. Yamasaki", "Ahyi Kim"], "title": "Automated Classification of Volcanic Earthquakes Using Transformer Encoders: Insights into Data Quality and Model Interpretability", "categories": ["physics.geo-ph", "cs.LG"], "comment": "submitted to Seismological Research Letters", "summary": "Precisely classifying earthquake types is crucial for elucidating the\nrelationship between volcanic earthquakes and volcanic activity. However,\ntraditional methods rely on subjective human judgment, which requires\nconsiderable time and effort. To address this issue, we developed a deep\nlearning model using a transformer encoder for a more objective and efficient\nclassification. Tested on Mount Asama's diverse seismic activity, our model\nachieved high F1 scores (0.930 for volcano tectonic, 0.931 for low-frequency\nearthquakes, and 0.980 for noise), superior to a conventional CNN-based method.\nTo enhance interpretability, attention weight visualizations were analyzed,\nrevealing that the model focuses on key waveform features similarly to human\nexperts. However, inconsistencies in training data, such as ambiguously labeled\nB-type events with S-waves, were found to influence classification accuracy and\nattention weight distributions. Experiments addressing data selection and\naugmentation demonstrated the importance of balancing data quality and\ndiversity. In addition, stations within 3 km of the crater played an important\nrole in improving model performance and interpretability. These findings\nhighlight the potential of Transformer-based models for automated volcanic\nearthquake classification, particularly in improving efficiency and\ninterpretability. By addressing challenges such as data imbalance and\nsubjective labeling, our approach provides a robust framework for understanding\nseismic activity at Mount Asama. Moreover, this framework offers opportunities\nfor transfer learning to other volcanic regions, paving the way for enhanced\nvolcanic hazard assessments and disaster mitigation strategies.", "AI": {"tldr": "This paper presents a transformer-based deep learning model for classifying volcanic earthquakes more efficiently and objectively compared to traditional or CNN-based methods. Applied to Mount Asama's seismic activity, the model achieved high F1 scores and demonstrated improved interpretability via attention weight analysis.", "motivation": "The study aims to address the limitations of traditional volcanic earthquake classification methods, which require substantial time and are influenced by subjective human judgment, by creating an efficient and objective automated method.", "method": "The authors developed a Transformer encoder-based deep learning model, tested it on seismic data from Mount Asama, and analyzed model interpretability through attention weight visualizations. They also evaluated the impact of issues like data imbalance and ambiguous labeling in training data.", "result": "The model achieved high F1 scores (0.930 for volcano tectonic, 0.931 for low-frequency earthquakes, and 0.980 for noise), outperformed a CNN-based model, and revealed that closer seismic stations improved performance. Data inconsistencies were identified as a key limitation.", "conclusion": "This study demonstrates the effectiveness of Transformer-based models for classifying volcanic earthquakes, showing how addressing challenges like data imbalance and subjective labeling can improve both accuracy and interpretability. The framework holds potential for application in other regions, enabling better volcanic hazard assessments and disaster response."}}
{"id": "2507.01719", "pdf": "https://arxiv.org/pdf/2507.01719", "abs": "https://arxiv.org/abs/2507.01719", "authors": ["Dorian Peters", "Fernanda Espinoza", "Marco da Re", "Guido Ivetta", "Luciana Benotti", "Rafael A. Calvo"], "title": "Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "There is justifiable interest in leveraging conversational AI (CAI) for\nhealth across the majority world, but to be effective, CAI must respond\nappropriately within culturally and linguistically diverse contexts. Therefore,\nwe need ways to address the fact that current LLMs exclude many lived\nexperiences globally. Various advances are underway which focus on top-down\napproaches and increasing training data. In this paper, we aim to complement\nthese with a bottom-up locally-grounded approach based on qualitative data\ncollected during participatory workshops in Latin America. Our goal is to\nconstruct a rich and human-centred understanding of: a) potential areas of\ncultural misalignment in digital health; b) regional perspectives on chatbots\nfor health and c)strategies for creating culturally-appropriate CAI; with a\nfocus on the understudied Latin American context. Our findings show that\nacademic boundaries on notions of culture lose meaning at the ground level and\ntechnologies will need to engage with a broader framework; one that\nencapsulates the way economics, politics, geography and local logistics are\nentangled in cultural experience. To this end, we introduce a framework for\n'Pluriversal Conversational AI for Health' which allows for the possibility\nthat more relationality and tolerance, rather than just more data, may be\ncalled for.", "AI": {"tldr": "The paper examines how conversational AI (CAI) can become culturally and linguistically appropriate in health contexts by relying on localized, human-centered data, particularly in Latin America. It introduces a framework called 'Pluriversal Conversational AI.'", "motivation": "To address the exclusion of global lived experiences by current large language models (LLMs), and ensure CAI performs effectively in culturally diverse and underrepresented regions.", "method": "The study used a bottom-up, qualitative approach, gathering data through participatory workshops held in Latin America to understand cultural misalignments, regional views on chatbots, and strategies for culturally-appropriate CAI.", "result": "Findings reveal that cultural boundaries become meaningless in practical settings due to their intertwining with economic, political, and logistical factors. The research emphasizes that relationality and tolerance in CAI may be as critical as expanding datasets.", "conclusion": "The proposed 'Pluriversal Conversational AI for Health' framework suggests that CAI development must consider complex socio-cultural entanglements beyond adding more data, focusing instead on relational and tolerant technologies for better inclusivity."}}
{"id": "2507.01513", "pdf": "https://arxiv.org/pdf/2507.01513", "abs": "https://arxiv.org/abs/2507.01513", "authors": ["Beitao Chen", "Xinyu Lyu", "Lianli Gao", "Jingkuan Song", "Heng Tao Shen"], "title": "SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "By incorporating visual inputs, Multimodal Large Language Models (MLLMs)\nextend LLMs to support visual reasoning. However, this integration also\nintroduces new vulnerabilities, making MLLMs susceptible to multimodal\njailbreak attacks and hindering their safe deployment.Existing defense methods,\nincluding Image-to-Text Translation, Safe Prompting, and Multimodal Safety\nTuning, attempt to address this by aligning multimodal inputs with LLMs'\nbuilt-in safeguards.Yet, they fall short in uncovering root causes of\nmultimodal vulnerabilities, particularly how harmful multimodal tokens trigger\njailbreak in MLLMs? Consequently, they remain vulnerable to text-driven\nmultimodal jailbreaks, often exhibiting overdefensive behaviors and imposing\nheavy training overhead.To bridge this gap, we present an comprehensive\nanalysis of where, how and which harmful multimodal tokens bypass safeguards in\nMLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers\nare responsible for inducing unsafe behaviors, highlighting the potential of\nprecisely removing a small subset of harmful tokens, without requiring safety\ntuning, can still effectively improve safety against jailbreaks. Motivated by\nthis, we propose Safe Prune-then-Restore (SafePTR), an training-free defense\nframework that selectively prunes harmful tokens at vulnerable layers while\nrestoring benign features at subsequent layers.Without incurring additional\ncomputational overhead, SafePTR significantly enhances the safety of MLLMs\nwhile preserving efficiency. Extensive evaluations across three MLLMs and five\nbenchmarks demonstrate SafePTR's state-of-the-art performance in mitigating\njailbreak risks without compromising utility.", "AI": {"tldr": "The paper introduces Safe Prune-then-Restore (SafePTR), a training-free method to enhance Multimodal Large Language Models (MLLMs) safety against multimodal jailbreaks by precisely pruning harmful tokens in earlier layers and preserving benign features without extra computational costs.", "motivation": "The study is motivated by the vulnerabilities in Multimodal Large Language Models (MLLMs), which are susceptible to multimodal jailbreak attacks, a challenge inadequately addressed by existing methods that fail to tackle root causes and often exhibit inefficiencies.", "method": "The proposed method, SafePTR, involves a framework that identifies and selectively prunes harmful tokens in early-middle layers of MLLMs while restoring benign features in subsequent layers, all without additional training or computational overhead.", "result": "SafePTR achieves state-of-the-art performance in mitigating jailbreak risks while maintaining model efficiency, as validated across three MLLMs and five benchmarks.", "conclusion": "SafePTR provides an effective, computationally efficient solution for enhancing the safety of MLLMs against multimodal jailbreaks by addressing vulnerabilities at their root cause."}}
{"id": "2507.01564", "pdf": "https://arxiv.org/pdf/2507.01564", "abs": "https://arxiv.org/abs/2507.01564", "authors": ["Chia-Ming Lee", "Bo-Cheng Qiu", "Ting-Yao Chen", "Ming-Han Sun", "Fang-Ying Lin", "Jung-Tse Tsai", "I-An Tsai", "Yu-Fan Lin", "Chih-Chung Hsu"], "title": "Multi Source COVID-19 Detection via Kernel-Density-based Slice Sampling", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We present our solution for the Multi-Source COVID-19 Detection Challenge,\nwhich classifies chest CT scans from four distinct medical centers. To address\nmulti-source variability, we employ the Spatial-Slice Feature Learning (SSFL)\nframework with Kernel-Density-based Slice Sampling (KDS). Our preprocessing\npipeline combines lung region extraction, quality control, and adaptive slice\nsampling to select eight representative slices per scan. We compare\nEfficientNet and Swin Transformer architectures on the validation set. The\nEfficientNet model achieves an F1-score of 94.68%, compared to the Swin\nTransformer's 93.34%. The results demonstrate the effectiveness of our\nKDS-based pipeline on multi-source data and highlight the importance of dataset\nbalance in multi-institutional medical imaging evaluation.", "AI": {"tldr": "This study focuses on classifying chest CT scans from multiple medical centers using a novel spatial feature learning framework with slice sampling techniques, achieving high accuracy.", "motivation": "The motivation is to address the variability encountered in multi-source medical imaging, ensuring robust and accurate classification of chest CT scans for detecting COVID-19 across distinct medical centers.", "method": "The authors used the Spatial-Slice Feature Learning (SSFL) framework and Kernel-Density-based Slice Sampling (KDS). Their preprocessing involved lung region extraction, quality control, and adaptive slice sampling. They compared two architectures, EfficientNet and Swin Transformer, on the validation set.", "result": "EfficientNet achieved an F1-score of 94.68%, while Swin Transformer reached 93.34%, showcasing the pipeline's efficacy.", "conclusion": "Their framework effectively handles variability in multi-source medical imaging, achieving high performance and underscoring the need for balanced datasets in such evaluations."}}
{"id": "2507.01778", "pdf": "https://arxiv.org/pdf/2507.01778", "abs": "https://arxiv.org/abs/2507.01778", "authors": ["Vivek Tetarwal", "Sandeep Kumar"], "title": "A Hybrid Ensemble Learning Framework for Image-Based Solar Panel Classification", "categories": ["cs.IT", "cs.CV", "math.IT"], "comment": "6 pages", "summary": "The installation of solar energy systems is on the rise, and therefore,\nappropriate maintenance techniques are required to be used in order to maintain\nmaximum performance levels. One of the major challenges is the automated\ndiscrimination between clean and dirty solar panels. This paper presents a\nnovel Dual Ensemble Neural Network (DENN) to classify solar panels using\nimage-based features. The suggested approach utilizes the advantages offered by\nvarious ensemble models by integrating them into a dual framework, aimed at\nimproving both classification accuracy and robustness. The DENN model is\nevaluated in comparison to current ensemble methods, showcasing its superior\nperformance across a range of assessment metrics. The proposed approach\nperforms the best compared to other methods and reaches state-of-the-art\naccuracy on experimental results for the Deep Solar Eye dataset, effectively\nserving predictive maintenance purposes in solar energy systems. It reveals the\npotential of hybrid ensemble learning techniques to further advance the\nprospects of automated solar panel inspections as a scalable solution to\nreal-world challenges.", "AI": {"tldr": "The paper presents a Dual Ensemble Neural Network (DENN) for classifying clean vs dirty solar panels, achieving state-of-the-art accuracy.", "motivation": "Rising solar energy systems demand automated and accurate maintenance solutions for classifying clean and dirty solar panels.", "method": "A novel Dual Ensemble Neural Network (DENN) integrates different ensemble learning models into a dual framework, enhancing accuracy and robustness.", "result": "DENN outperforms existing ensemble methods, achieving state-of-the-art accuracy on the Deep Solar Eye dataset.", "conclusion": "The DENN model shows promise as a scalable solution for automated maintenance in solar energy using hybrid ensemble techniques."}}
{"id": "2507.01794", "pdf": "https://arxiv.org/pdf/2507.01794", "abs": "https://arxiv.org/abs/2507.01794", "authors": ["Carlo Alberto Barbano", "Benoit Dufumier", "Edouard Duchesnay", "Marco Grangetto", "Pietro Gori"], "title": "Robust brain age estimation from structural MRI with contrastive learning", "categories": ["eess.IV", "cs.CV", "68T07", "I.2.6"], "comment": "11 pages", "summary": "Estimating brain age from structural MRI has emerged as a powerful tool for\ncharacterizing normative and pathological aging. In this work, we explore\ncontrastive learning as a scalable and robust alternative to supervised\napproaches for brain age estimation. We introduce a novel contrastive loss\nfunction, $\\mathcal{L}^{exp}$, and evaluate it across multiple public\nneuroimaging datasets comprising over 20,000 scans. Our experiments reveal four\nkey findings. First, scaling pre-training on diverse, multi-site data\nconsistently improves generalization performance, cutting external mean\nabsolute error (MAE) nearly in half. Second, $\\mathcal{L}^{exp}$ is robust to\nsite-related confounds, maintaining low scanner-predictability as training size\nincreases. Third, contrastive models reliably capture accelerated aging in\npatients with cognitive impairment and Alzheimer's disease, as shown through\nbrain age gap analysis, ROC curves, and longitudinal trends. Lastly, unlike\nsupervised baselines, $\\mathcal{L}^{exp}$ maintains a strong correlation\nbetween brain age accuracy and downstream diagnostic performance, supporting\nits potential as a foundation model for neuroimaging. These results position\ncontrastive learning as a promising direction for building generalizable and\nclinically meaningful brain representations.", "AI": {"tldr": "This work introduces a novel contrastive loss function for estimating brain age from MRI scans, achieving improved performance and robustness compared to supervised methods.", "motivation": "To develop a scalable and robust method for estimating brain age, which is critical for studying normative and pathological aging.", "method": "A novel contrastive loss function ($\\mathcal{L}^{exp}$) was proposed and tested on over 20,000 MRI scans from multiple datasets, emphasizing pre-training on diverse multi-site data.", "result": "The new method significantly improved generalization performance, demonstrated robustness to site-related confounds, accurately captured accelerated aging in cognitive impairment conditions, and correlated well with diagnostic performance.", "conclusion": "Contrastive learning with $\\mathcal{L}^{exp}$ shows strong potential as a foundation model for neuroimaging, offering generalizable and clinically meaningful brain representations."}}
{"id": "2507.01808", "pdf": "https://arxiv.org/pdf/2507.01808", "abs": "https://arxiv.org/abs/2507.01808", "authors": ["Xiaoyu Ji", "Jessica Shorland", "Joshua Shank", "Pascal Delpe-Brice", "Latanya Sweeney", "Jan Allebach", "Ali Shakouri"], "title": "Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.ET", "68T01, 68T05, 68T45, 94A60"], "comment": "20 pages, 11 figures, 30 references", "summary": "Small- and medium-sized manufacturers need innovative data tools but, because\nof competition and privacy concerns, often do not want to share their\nproprietary data with researchers who might be interested in helping. This\npaper introduces a privacy-preserving platform by which manufacturers may\nsafely share their data with researchers through secure methods, so that those\nresearchers then create innovative tools to solve the manufacturers' real-world\nproblems, and then provide tools that execute solutions back onto the platform\nfor others to use with privacy and confidentiality guarantees. We illustrate\nthis problem through a particular use case which addresses an important problem\nin the large-scale manufacturing of food crystals, which is that quality\ncontrol relies on image analysis tools. Previous to our research, food crystals\nin the images were manually counted, which required substantial and\ntime-consuming human efforts, but we have developed and deployed a crystal\nanalysis tool which makes this process both more rapid and accurate. The tool\nenables automatic characterization of the crystal size distribution and numbers\nfrom microscope images while the natural imperfections from the sample\npreparation are automatically removed; a machine learning model to count high\nresolution translucent crystals and agglomeration of crystals was also\ndeveloped to aid in these efforts. The resulting algorithm was then packaged\nfor real-world use on the factory floor via a web-based app secured through the\noriginating privacy-preserving platform, allowing manufacturers to use it while\nkeeping their proprietary data secure. After demonstrating this full process,\nfuture directions are also explored.", "AI": {"tldr": "This paper introduces a privacy-preserving platform for data sharing between manufacturers and researchers, featuring a case study on automating food crystal quality control with machine learning.", "motivation": "Manufacturers, particularly small- and medium-sized ones, need data tools for solving real-world problems but are restricted by competition and privacy concerns in sharing proprietary data with researchers.", "method": "The authors developed a privacy-preserving platform enabling secure data sharing between manufacturers and researchers. They deployed a machine learning model to automate the counting and characterization of food crystals from microscope images while addressing imperfections, shared via a secured web-based app.", "result": "A functional algorithm for automated analysis of food crystals was created and implemented in a real-world manufacturing setting via a privacy-preserving, web-based platform.", "conclusion": "The platform successfully facilitates collaboration between manufacturers and researchers while ensuring privacy, and the specific tool for food crystal analysis demonstrates the practicality and benefits of this approach."}}
{"id": "2507.01828", "pdf": "https://arxiv.org/pdf/2507.01828", "abs": "https://arxiv.org/abs/2507.01828", "authors": ["Tyler Ward", "Meredith K. Owen", "O'Kira Coleman", "Brian Noehren", "Abdullah-Al-Zubaer Imran"], "title": "Autoadaptive Medical Segment Anything Model", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 2 figures, 3 tables", "summary": "Medical image segmentation is a key task in the imaging workflow, influencing\nmany image-based decisions. Traditional, fully-supervised segmentation models\nrely on large amounts of labeled training data, typically obtained through\nmanual annotation, which can be an expensive, time-consuming, and error-prone\nprocess. This signals a need for accurate, automatic, and annotation-efficient\nmethods of training these models. We propose ADA-SAM (automated,\ndomain-specific, and adaptive segment anything model), a novel multitask\nlearning framework for medical image segmentation that leverages class\nactivation maps from an auxiliary classifier to guide the predictions of the\nsemi-supervised segmentation branch, which is based on the Segment Anything\n(SAM) framework. Additionally, our ADA-SAM model employs a novel gradient\nfeedback mechanism to create a learnable connection between the segmentation\nand classification branches by using the segmentation gradients to guide and\nimprove the classification predictions. We validate ADA-SAM on real-world\nclinical data collected during rehabilitation trials, and demonstrate that our\nproposed method outperforms both fully-supervised and semi-supervised baselines\nby double digits in limited label settings. Our code is available at:\nhttps://github.com/tbwa233/ADA-SAM.", "AI": {"tldr": "The paper introduces ADA-SAM, a multitask learning framework for efficient and accurate medical image segmentation using limited labeled data.", "motivation": "The motivation is to address the limitations of traditional, fully-supervised segmentation models that require extensive labeled data, which is costly and time-consuming to produce.", "method": "ADA-SAM combines class activation maps from an auxiliary classifier with a semi-supervised segmentation model based on the Segment Anything (SAM) framework. Additionally, it features a gradient feedback mechanism linking segmentation and classification tasks.", "result": "The method demonstrated superior performance, surpassing both fully-supervised and semi-supervised baselines by a significant margin in limited label settings, validated on real-world clinical datasets.", "conclusion": "The proposed ADA-SAM framework is a promising solution for achieving annotation-efficient medical image segmentation, particularly in scenarios with limited labeled data."}}
{"id": "2507.01881", "pdf": "https://arxiv.org/pdf/2507.01881", "abs": "https://arxiv.org/abs/2507.01881", "authors": ["Niccol\u00f2 McConnell", "Pardeep Vasudev", "Daisuke Yamada", "Daryl Cheng", "Mehran Azimbagirad", "John McCabe", "Shahab Aslani", "Ahmed H. Shahin", "Yukun Zhou", "The SUMMIT Consortium", "Andre Altmann", "Yipeng Hu", "Paul Taylor", "Sam M. Janes", "Daniel C. Alexander", "Joseph Jacob"], "title": "A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Low-dose computed tomography (LDCT) imaging employed in lung cancer screening\n(LCS) programs is increasing in uptake worldwide. LCS programs herald a\ngenerational opportunity to simultaneously detect cancer and non-cancer-related\nearly-stage lung disease. Yet these efforts are hampered by a shortage of\nradiologists to interpret scans at scale. Here, we present TANGERINE, a\ncomputationally frugal, open-source vision foundation model for volumetric LDCT\nanalysis. Designed for broad accessibility and rapid adaptation, TANGERINE can\nbe fine-tuned off the shelf for a wide range of disease-specific tasks with\nlimited computational resources and training data. Relative to models trained\nfrom scratch, TANGERINE demonstrates fast convergence during fine-tuning,\nthereby requiring significantly fewer GPU hours, and displays strong label\nefficiency, achieving comparable or superior performance with a fraction of\nfine-tuning data. Pretrained using self-supervised learning on over 98,000\nthoracic LDCTs, including the UK's largest LCS initiative to date and 27 public\ndatasets, TANGERINE achieves state-of-the-art performance across 14 disease\nclassification tasks, including lung cancer and multiple respiratory diseases,\nwhile generalising robustly across diverse clinical centres. By extending a\nmasked autoencoder framework to 3D imaging, TANGERINE offers a scalable\nsolution for LDCT analysis, departing from recent closed, resource-intensive\nmodels by combining architectural simplicity, public availability, and modest\ncomputational requirements. Its accessible, open-source lightweight design lays\nthe foundation for rapid integration into next-generation medical imaging tools\nthat could transform LCS initiatives, allowing them to pivot from a singular\nfocus on lung cancer detection to comprehensive respiratory disease management\nin high-risk populations.", "AI": {"tldr": "TANGERINE is an open-source vision foundation model designed for low-dose CT analysis, excelling in diverse disease detection with reduced computational requirements.", "motivation": "There is a global increase in lung cancer screening programs using LDCT imaging, but efforts are hindered by a lack of radiologists to interpret these scans on a large scale.", "method": "They developed TANGERINE, leveraging a self-supervised learning approach on over 98,000 thoracic LDCTs, and extended a masked autoencoder framework to handle 3D imaging while ensuring scalability and accessibility.", "result": "TANGERINE achieved state-of-the-art performance across 14 disease classification tasks, showed strong label efficiency, and required fewer computational resources compared to models trained from scratch.", "conclusion": "TANGERINE's lightweight, open-source design provides a scalable, efficient solution for LDCT analysis, potentially revolutionizing lung cancer screening by enabling comprehensive respiratory disease management."}}
{"id": "2507.01466", "pdf": "https://arxiv.org/pdf/2507.01466", "abs": "https://arxiv.org/abs/2507.01466", "authors": ["Tianyi Chen", "Hao Yang", "Wenjun Ma", "Jun Zhang"], "title": "Symbolic identification of tensor equations in multidimensional physical fields", "categories": ["math-ph", "cs.LG", "math.MP"], "comment": null, "summary": "Recently, data-driven methods have shown great promise for discovering\ngoverning equations from simulation or experimental data. However, most\nexisting approaches are limited to scalar equations, with few capable of\nidentifying tensor relationships. In this work, we propose a general\ndata-driven framework for identifying tensor equations, referred to as Symbolic\nIdentification of Tensor Equations (SITE). The core idea of SITE--representing\ntensor equations using a host-plasmid structure--is inspired by the\nmultidimensional gene expression programming (M-GEP) approach. To improve the\nrobustness of the evolutionary process, SITE adopts a genetic information\nretention strategy. Moreover, SITE introduces two key innovations beyond\nconventional evolutionary algorithms. First, it incorporates a dimensional\nhomogeneity check to restrict the search space and eliminate physically invalid\nexpressions. Second, it replaces traditional linear scaling with a tensor\nlinear regression technique, greatly enhancing the efficiency of numerical\ncoefficient optimization. We validate SITE using two benchmark scenarios, where\nit accurately recovers target equations from synthetic data, showing robustness\nto noise and small sample sizes. Furthermore, SITE is applied to identify\nconstitutive relations directly from molecular simulation data, which are\ngenerated without reliance on macroscopic constitutive models. It adapts to\nboth compressible and incompressible flow conditions and successfully\nidentifies the corresponding macroscopic forms, highlighting its potential for\ndata-driven discovery of tensor equation.", "AI": {"tldr": "The paper introduces SITE, a framework that identifies tensor equations using a new host-plasmid representation and M-GEP inspiration. It improves robustness and accuracy by implementing novel algorithms and validations.", "motivation": "Existing data-driven methods are adept at discovering scalar equations but struggle with tensor relationships critical in advanced physical modeling.", "method": "SITE uses a genetic evolutionary approach inspired by M-GEP, combines dimensional homogeneity checks, and employs tensor linear regression for better search efficiency and robustness.", "result": "SITE accurately recovers tensor equations in benchmark tests, is robust to noise, and works effectively with small data. It validates its capability by identifying constitutive relations from molecular simulations.", "conclusion": "SITE is a promising method for discovering complex tensor equations, showing potential for advancing data-driven scientific discovery across different simulation and experimental settings."}}
{"id": "2507.01487", "pdf": "https://arxiv.org/pdf/2507.01487", "abs": "https://arxiv.org/abs/2507.01487", "authors": ["Marc Damie", "Florian Hahn", "Andreas Peter", "Jan Ramon"], "title": "How to Securely Shuffle? A survey about Secure Shufflers for privacy-preserving computations", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building\nblock for private data aggregation. Recently, the field of differential privacy\nhas revived interest in secure shufflers by highlighting the privacy\namplification they can provide in various computations. Although several works\nargue for the utility of secure shufflers, they often treat them as black\nboxes; overlooking the practical vulnerabilities and performance trade-offs of\nexisting implementations. This leaves a central question open: what makes a\ngood secure shuffler?\n  This survey addresses that question by identifying, categorizing, and\ncomparing 26 secure protocols that realize the necessary shuffling\nfunctionality. To enable a meaningful comparison, we adapt and unify existing\nsecurity definitions into a consistent set of properties. We also present an\noverview of privacy-preserving technologies that rely on secure shufflers,\noffer practical guidelines for selecting appropriate protocols, and outline\npromising directions for future work.", "AI": {"tldr": "This paper surveys 26 secure shuffling protocols, categorizing and comparing them based on uniform security definitions while emphasizing their applications and future directions.", "motivation": "To explore and address the gap in understanding what constitutes a 'good secure shuffler' amid discussions of privacy amplification and implementation trade-offs.", "method": "The study unifies security definitions for secure shufflers, categorizes 26 protocols, and offers practical guidelines and future research directions.", "result": "A comprehensive comparison of secure shuffling protocols, an overview of their applications in privacy-preserving technologies, and practical insights for protocol selection.", "conclusion": "This paper provides a methodological framework to evaluate secure shufflers and contributes to advancing their design and practical implementation for better privacy guarantees."}}
{"id": "2507.01862", "pdf": "https://arxiv.org/pdf/2507.01862", "abs": "https://arxiv.org/abs/2507.01862", "authors": ["Sanjay Krishna Anbalagan", "Xinrui Nie", "Umesh Mohan", "Vijay Kumar Kanamarlapudi", "Anughna Kommalapati", "Xiaodan Zhao"], "title": "Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents", "categories": ["cs.HC", "cs.AI", "H.5.2; I.2.7"], "comment": "8 pages, 1 figure, pre-print of poster accepted for HCI International\n  2025 (HCII 2025), CCIS vol 2529", "summary": "Domain specific chatbot applications often involve multi step interactions,\nsuch as refining search filters, selecting multiple items, or performing\ncomparisons. Traditional graphical user interfaces (GUIs) handle these\nworkflows by providing explicit \"Submit\" (commit data) and \"Reset\" (discard\ndata) actions, allowing back-end systems to track user intent unambiguously. In\ncontrast, conversational agents rely on subtle language cues, which can lead to\nconfusion and incomplete context management. This paper proposes modeling these\nGUI inspired metaphors acknowledgment (submit like) and context switching\n(reset-like) as explicit tasks within large language model (LLM) prompts. By\ncapturing user acknowledgment, reset actions, and chain of thought (CoT)\nreasoning as structured session data, we preserve clarity, reduce user\nconfusion, and align domain-specific chatbot interactions with back-end logic.\nWe demonstrate our approach in hotel booking and customer management scenarios,\nhighlighting improvements in multi-turn task coherence, user satisfaction, and\nefficiency.", "AI": {"tldr": "This paper addresses challenges in applying GUI-like explicit actions in conversational chatbots by proposing acknowledgment and reset tasks modeled in Language Model prompts, improving coherence and user experience.", "motivation": "To reduce confusion and improve task clarity in domain-specific chatbots during multi-step workflows, which traditional GUI methods handle better.", "method": "Integrating GUI-inspired actions like acknowledgment (commit) and reset (discard) into structured Language Model prompts to align chatbot tasks with clear backend logic.", "result": "Demonstrated improvements in task coherence, user satisfaction, and efficiency in hotel booking and customer management chatbot scenarios.", "conclusion": "Aligning GUI metaphors with conversational LLM prompts minimizes confusion, enhances clarity, and improves multi-turn interaction efficiency."}}
{"id": "2507.01501", "pdf": "https://arxiv.org/pdf/2507.01501", "abs": "https://arxiv.org/abs/2507.01501", "authors": ["Eloy Pe\u00f1a-Asensio", "Fabio Ferrari"], "title": "Meteoroid stream identification with HDBSCAN unsupervised clustering algorithm", "categories": ["astro-ph.EP", "astro-ph.IM", "cs.LG"], "comment": "Accepted in The Astronomical Journal", "summary": "Accurate identification of meteoroid streams is central to understanding\ntheir origins and evolution. However, overlapping clusters and background noise\nhinder classification, an issue amplified for missions such as ESA's LUMIO that\nrely on meteor shower observations to infer lunar meteoroid impact parameters.\nThis study evaluates the performance of the Hierarchical Density-Based Spatial\nClustering of Applications with Noise (HDBSCAN) algorithm for unsupervised\nmeteoroid stream identification, comparing its outcomes with the established\nCameras for All-Sky Meteor Surveillance (CAMS) look-up table method. We analyze\nthe CAMS Meteoroid Orbit Database v3.0 using three feature vectors: LUTAB (CAMS\ngeocentric parameters), ORBIT (heliocentric orbital elements), and GEO (adapted\ngeocentric parameters). HDBSCAN is applied with varying minimum cluster sizes\nand two cluster selection methods (eom and leaf). To align HDBSCAN clusters\nwith CAMS classifications, the Hungarian algorithm determines the optimal\nmapping. Clustering performance is assessed via the Silhouette score,\nNormalized Mutual Information, and F1 score, with Principal Component Analysis\nfurther supporting the analysis. With the GEO vector, HDBSCAN confirms 39\nmeteoroid streams, 21 strongly aligning with CAMS. The ORBIT vector identifies\n30 streams, 13 with high matching scores. Less active showers pose\nidentification challenges. The eom method consistently yields superior\nperformance and agreement with CAMS. Although HDBSCAN requires careful\nselection of the minimum cluster size, it delivers robust, internally\nconsistent clusters and outperforms the look-up table method in statistical\ncoherence. These results underscore HDBSCAN's potential as a mathematically\nconsistent alternative for meteoroid stream identification, although further\nvalidation is needed to assess physical validity.", "AI": {"tldr": "The study evaluates the use of the Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) algorithm for meteoroid stream identification and compares it to traditional methods.", "motivation": "Meteoroid stream classification is critical yet challenging due to overlapping clusters and noise, which impact practical missions like ESA's LUMIO.", "method": "The HDBSCAN algorithm was applied to features from the CAMS database with varying parameters and compared using clustering performance metrics.", "result": "Using geocentric parameters, HDBSCAN successfully identified 39 streams, 21 of which strongly aligned with CAMS classifications, achieving better statistical coherence.", "conclusion": "HDBSCAN shows promise as a consistent alternative to traditional methods for identifying meteoroid streams but requires further validation for physical applicability."}}
{"id": "2507.01533", "pdf": "https://arxiv.org/pdf/2507.01533", "abs": "https://arxiv.org/abs/2507.01533", "authors": ["Hanno Gottschalk", "Emil Partow", "Tobias J. Riedlinger"], "title": "Consistency of Learned Sparse Grid Quadrature Rules using NeuralODEs", "categories": ["math.NA", "cs.LG", "cs.NA", "math.PR"], "comment": null, "summary": "This paper provides a proof of the consistency of sparse grid quadrature for\nnumerical integration of high dimensional distributions. In a first step, a\ntransport map is learned that normalizes the distribution to a noise\ndistribution on the unit cube. This step is built on the statistical learning\ntheory of neural ordinary differential equations, which has been established\nrecently. Secondly, the composition of the generative map with the quantity of\ninterest is integrated numerically using the Clenshaw-Curtis sparse grid\nquadrature. A decomposition of the total numerical error in quadrature error\nand statistical error is provided. As main result it is proven in the framework\nof empirical risk minimization that all error terms can be controlled in the\nsense of PAC (probably approximately correct) learning and with high\nprobability the numerical integral approximates the theoretical value up to an\narbitrary small error in the limit where the data set size is growing and the\nnetwork capacity is increased adaptively.", "AI": {"tldr": "This paper proves the consistency of sparse grid quadrature for high-dimensional numerical integrations by leveraging neural ODE-based transport maps combined with sparse grid quadrature.", "motivation": "To address the challenges of numerical integration in high-dimensional problems by providing a provable framework for error control and consistency.", "method": "The approach involves two steps: first, a transport map normalizes the high-dimensional distribution using neural ODEs; second, the Clenshaw-Curtis sparse grid quadrature is applied to integrate the composed function. A total numerical error decomposition is conducted and analyzed using PAC learning theory.", "result": "The paper demonstrates that all error terms in the numerical integration process can be controlled with high probability, ensuring the numerical integral approximates the theoretical value accurately as data increases and network capacity adapts.", "conclusion": "Sparse grid quadrature, combined with transport maps learned via neural ODEs, provides a theoretically sound approach to high-dimensional numerical integration, ensuring consistency and error control."}}
{"id": "2507.01571", "pdf": "https://arxiv.org/pdf/2507.01571", "abs": "https://arxiv.org/abs/2507.01571", "authors": ["Koen T. W. Teuwen", "Sam Baggen", "Emmanuele Zambon", "Luca Allodi"], "title": "On the Effect of Ruleset Tuning and Data Imbalance on Explainable Network Security Alert Classifications: a Case-Study on DeepCASE", "categories": ["cs.CR", "cs.LG", "cs.NI"], "comment": null, "summary": "Automation in Security Operations Centers (SOCs) plays a prominent role in\nalert classification and incident escalation. However, automated methods must\nbe robust in the presence of imbalanced input data, which can negatively affect\nperformance. Additionally, automated methods should make explainable decisions.\nIn this work, we evaluate the effect of label imbalance on the classification\nof network intrusion alerts. As our use-case we employ DeepCASE, the\nstate-of-the-art method for automated alert classification. We show that label\nimbalance impacts both classification performance and correctness of the\nclassification explanations offered by DeepCASE. We conclude tuning the\ndetection rules used in SOCs can significantly reduce imbalance and may benefit\nthe performance and explainability offered by alert post-processing methods\nsuch as DeepCASE. Therefore, our findings suggest that traditional methods to\nimprove the quality of input data can benefit automation.", "AI": {"tldr": "This paper evaluates the impact of label imbalance in classifying network intrusion alerts using DeepCASE and suggests improving input data quality to enhance performance and explainability.", "motivation": "To address the challenges in automated alert classification in SOCs caused by label imbalance and the need for explainable decisions.", "method": "The performance and explanation correctness of DeepCASE are evaluated under conditions of label imbalance, using network intrusion alerts as a use case.", "result": "DeepCASE's classification performance and explanation accuracy are negatively impacted by label imbalance.", "conclusion": "Improving the quality of input data, such as tuning SOC detection rules, can enhance the robustness and explainability of automated alert classification systems like DeepCASE."}}
{"id": "2507.01575", "pdf": "https://arxiv.org/pdf/2507.01575", "abs": "https://arxiv.org/abs/2507.01575", "authors": ["Masood Jan", "Wafa Njima", "Xun Zhang", "Alexander Artemenko"], "title": "Transfer Learning for VLC-based indoor Localization: Addressing Environmental Variability", "categories": ["eess.SP", "cs.LG"], "comment": "Accepted for publication in the IEEE VTC2025-Spring Conference, 7\n  pages", "summary": "Accurate indoor localization is crucial in industrial environments. Visible\nLight Communication (VLC) has emerged as a promising solution, offering high\naccuracy, energy efficiency, and minimal electromagnetic interference. However,\nVLC-based indoor localization faces challenges due to environmental\nvariability, such as lighting fluctuations and obstacles. To address these\nchallenges, we propose a Transfer Learning (TL)-based approach for VLC-based\nindoor localization. Using real-world data collected at a BOSCH factory, the TL\nframework integrates a deep neural network (DNN) to improve localization\naccuracy by 47\\%, reduce energy consumption by 32\\%, and decrease computational\ntime by 40\\% compared to the conventional models. The proposed solution is\nhighly adaptable under varying environmental conditions and achieves similar\naccuracy with only 30\\% of the dataset, making it a cost-efficient and scalable\noption for industrial applications in Industry 4.0.", "AI": {"tldr": "The paper proposes a Transfer Learning-based indoor localization system using Visible Light Communication to improve efficiency and accuracy, achieving high adaptability in industrial environments.", "motivation": "Industry demands highly accurate, efficient, and adaptable indoor localization systems free from electromagnetic interference, especially under challenging environmental conditions.", "method": "A Transfer Learning framework integrating a deep neural network was applied to VLC-based localization, and validated using real-world industrial data.", "result": "The model delivered improvements: localization accuracy increased by 47%, energy consumption reduced by 32%, and computational time decreased by 40% compared to traditional methods.", "conclusion": "The solution is cost-efficient, scalable, and adaptable, making it a practical choice for Industry 4.0 applications. It handles environmental variability effectively while maintaining robust performance."}}
{"id": "2507.01939", "pdf": "https://arxiv.org/pdf/2507.01939", "abs": "https://arxiv.org/abs/2507.01939", "authors": ["Xiaosheng Zhao", "Yang Huang", "Guirong Xue", "Xiao Kong", "Jifeng Liu", "Xiaoyu Tang", "Timothy C. Beers", "Yuan-Sen Ting", "A-Li Luo"], "title": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars", "categories": ["astro-ph.IM", "astro-ph.SR", "cs.AI", "cs.LG"], "comment": "26 pages, 6 figures, 5 tables. To be submitted to AAS Journals.\n  Comments welcome", "summary": "In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.", "AI": {"tldr": "SpecCLIP utilizes foundational LLM methodologies adapted for stellar spectral analysis by training models on large spectral datasets and using contrastive alignment for cross-spectrum applications.", "motivation": "Leverage the success of large language models (LLMs) in understanding structured data for advancing analysis of stellar spectra.", "method": "Adapts the CLIP framework by pretraining on large spectral datasets, aligning spectra from multiple sources using contrastive methods, complemented by auxiliary decoders for information retention and type translation.", "result": "SpecCLIP accurately estimates stellar parameters and chemical abundances, boosts precision against benchmark surveys, and supports anomaly detection and cross-spectrum predictions.", "conclusion": "Contrastively trained foundation models with spectrum-aware enhancements improve precision stellar spectroscopy and expand toolkits for astrophysical research."}}
{"id": "2507.01696", "pdf": "https://arxiv.org/pdf/2507.01696", "abs": "https://arxiv.org/abs/2507.01696", "authors": ["Steinar Laenen", "Peter Macgregor", "He Sun"], "title": "Dynamic Similarity Graph Construction with Kernel Density Estimation", "categories": ["cs.DS", "cs.LG"], "comment": "ICML'25", "summary": "In the kernel density estimation (KDE) problem, we are given a set $X$ of\ndata points in $\\mathbb{R}^d$, a kernel function $k: \\mathbb{R}^d \\times\n\\mathbb{R}^d \\rightarrow \\mathbb{R}$, and a query point $\\mathbf{q} \\in\n\\mathbb{R}^d$, and the objective is to quickly output an estimate of\n$\\sum_{\\mathbf{x} \\in X} k(\\mathbf{q}, \\mathbf{x})$. In this paper, we consider\n$\\textsf{KDE}$ in the dynamic setting, and introduce a data structure that\nefficiently maintains the estimates for a set of query points as data points\nare added to $X$ over time. Based on this, we design a dynamic data structure\nthat maintains a sparse approximation of the fully connected similarity graph\non $X$, and develop a fast dynamic spectral clustering algorithm. We further\nevaluate the effectiveness of our algorithms on both synthetic and real-world\ndatasets.", "AI": {"tldr": "This paper introduces a dynamic data structure for kernel density estimation that effectively supports updates and queries. It also applies this to dynamically maintain a sparse similarity graph and perform spectral clustering.", "motivation": "To address the inefficiency in updating estimates for kernel density estimation as data points are added over time, especially in dynamic datasets.", "method": "The authors developed a dynamic data structure for maintaining kernel density estimation estimates for query points and expanded it to maintain a sparse similarity graph. This was further applied to create a dynamic spectral clustering algorithm.", "result": "The proposed methods were evaluated using both synthetic and real-world datasets, demonstrating their computational efficiency and effectiveness.", "conclusion": "The study successfully introduced efficient dynamic algorithms for kernel density estimation and spectral clustering, showcasing their practicality for dynamic datasets."}}
{"id": "2507.01728", "pdf": "https://arxiv.org/pdf/2507.01728", "abs": "https://arxiv.org/abs/2507.01728", "authors": ["Hao Wei", "Wanli Ni", "Wen Wang", "Wenjun Xu", "Dusit Niyato", "Ping Zhang"], "title": "Token Communication in the Era of Large Models: An Information Bottleneck-Based Approach", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "This letter proposes UniToCom, a unified token communication paradigm that\ntreats tokens as the fundamental units for both processing and wireless\ntransmission. Specifically, to enable efficient token representations, we\npropose a generative information bottleneck (GenIB) principle, which\nfacilitates the learning of tokens that preserve essential information while\nsupporting reliable generation across multiple modalities. By doing this,\nGenIB-based tokenization is conducive to improving the communication efficiency\nand reducing computational complexity. Additionally, we develop $\\sigma$-GenIB\nto address the challenges of variance collapse in autoregressive modeling,\nmaintaining representational diversity and stability. Moreover, we employ a\ncausal Transformer-based multimodal large language model (MLLM) at the receiver\nto unify the processing of both discrete and continuous tokens under the\nnext-token prediction paradigm. Simulation results validate the effectiveness\nand superiority of the proposed UniToCom compared to baselines under dynamic\nchannel conditions. By integrating token processing with MLLMs, UniToCom\nenables scalable and generalizable communication in favor of multimodal\nunderstanding and generation, providing a potential solution for\nnext-generation intelligent communications.", "AI": {"tldr": "The paper introduces UniToCom, a communication paradigm utilizing tokens as core units for processing and wireless transmission, supported by a generative information bottleneck principle and a Transformer-based model for improved efficiency and multimodal processing.", "motivation": "To create a unified framework for communication that reduces computational complexity while enhancing efficiency and reliability, particularly in multimodal contexts.", "method": "Proposed the GenIB principle for tokenization to preserve critical information and facilitate generation across modalities, addressed modeling challenges with \u03c3-GenIB, and employed a causal Transformer-based multimodal language model for processing tokens.", "result": "Simulation results show UniToCom outperforms baselines under dynamic channel conditions, proving its efficacy and suitability for multimodal understanding and communication.", "conclusion": "UniToCom offers scalable, generalizable, and efficient communication solutions, paving the way for next-generation intelligent multimodal communications."}}
{"id": "2507.01795", "pdf": "https://arxiv.org/pdf/2507.01795", "abs": "https://arxiv.org/abs/2507.01795", "authors": ["Lizuo Liu", "Lu Zhang", "Anne Gelb"], "title": "Neural Entropy-stable conservative flux form neural networks for learning hyperbolic conservation laws", "categories": ["math.NA", "cs.LG", "cs.NA", "math-ph", "math.MP", "65M08, 68T07, 65M22, 65M32, 65D25"], "comment": null, "summary": "We propose a neural entropy-stable conservative flux form neural network\n(NESCFN) for learning hyperbolic conservation laws and their associated entropy\nfunctions directly from solution trajectories, without requiring any predefined\nnumerical discretization. While recent neural network architectures have\nsuccessfully integrated classical numerical principles into learned models,\nmost rely on prior knowledge of the governing equations or assume a fixed\ndiscretization. Our approach removes this dependency by embedding\nentropy-stable design principles into the learning process itself, enabling the\ndiscovery of physically consistent dynamics in a fully data-driven setting. By\njointly learning both the numerical flux function and a corresponding entropy,\nthe proposed method ensures conservation and entropy dissipation, critical for\nlong-term stability and fidelity in the system of hyperbolic conservation laws.\nNumerical results demonstrate that the method achieves stability and\nconservation over extended time horizons and accurately captures shock\npropagation speeds, even without oracle access to future-time solution profiles\nin the training data.", "AI": {"tldr": "The paper introduces NESCFN, a neural network that learns hyperbolic conservation laws and entropy functions directly from data without predefined discretization.", "motivation": "To overcome reliance on predefined discretizations and prior knowledge of governing equations for modeling hyperbolic conservation laws.", "method": "The paper embeds entropy-stable principles into a neural network, jointly learning numerical flux functions and corresponding entropy in a data-driven approach.", "result": "The method ensures stability and conservation over long time periods and accurately models shock propagation without access to future-time solutions during training.", "conclusion": "This approach effectively discovers physically consistent dynamics, ensuring critical conservation principles in hyperbolic laws using only data-driven learning."}}
{"id": "2507.01889", "pdf": "https://arxiv.org/pdf/2507.01889", "abs": "https://arxiv.org/abs/2507.01889", "authors": ["Sebastian Wissel", "Jonas Scheunert", "Aaron Dextre", "Shamail Ahmed", "Andreas Bayer", "Kerstin Volz", "Bai-Xiang Xu"], "title": "STEM Diffraction Pattern Analysis with Deep Learning Networks", "categories": ["cond-mat.dis-nn", "cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "Accurate grain orientation mapping is essential for understanding and\noptimizing the performance of polycrystalline materials, particularly in\nenergy-related applications. Lithium nickel oxide (LiNiO$_{2}$) is a promising\ncathode material for next-generation lithium-ion batteries, and its\nelectrochemical behaviour is closely linked to microstructural features such as\ngrain size and crystallographic orientations. Traditional orientation mapping\nmethods--such as manual indexing, template matching (TM), or Hough\ntransform-based techniques--are often slow and noise-sensitive when handling\ncomplex or overlapping patterns, creating a bottleneck in large-scale\nmicrostructural analysis. This work presents a machine learning-based approach\nfor predicting Euler angles directly from scanning transmission electron\nmicroscopy (STEM) diffraction patterns (DPs). This enables the automated\ngeneration of high-resolution crystal orientation maps, facilitating the\nanalysis of internal microstructures at the nanoscale. Three deep learning\narchitectures--convolutional neural networks (CNNs), Dense Convolutional\nNetworks (DenseNets), and Shifted Windows (Swin) Transformers--are evaluated,\nusing an experimentally acquired dataset labelled via a commercial TM\nalgorithm. While the CNN model serves as a baseline, both DenseNets and Swin\nTransformers demonstrate superior performance, with the Swin Transformer\nachieving the highest evaluation scores and the most consistent microstructural\npredictions. The resulting crystal maps exhibit clear grain boundary\ndelineation and coherent intra-grain orientation distributions, underscoring\nthe potential of attention-based architectures for analyzing diffraction-based\nimage data. These findings highlight the promise of combining advanced machine\nlearning models with STEM data for robust, high-throughput microstructural\ncharacterization.", "AI": {"tldr": "This paper proposes using advanced machine learning models to predict crystal orientations from scanning transmission electron microscopy (STEM) diffraction patterns, focusing on lithium nickel oxide (LiNiO$_{2}$). The Swin Transformer outperforms other architectures for high-resolution mapping.", "motivation": "Understanding and optimizing polycrystalline material performance, particularly for energy-focused applications like lithium-ion cathode materials, requires accurate grain orientation mapping. Traditional methods are slow and error-prone, limiting large-scale analysis.", "method": "The study evaluates three deep learning architectures (CNNs, DenseNets, and Swin Transformers) to predict Euler angles from STEM diffraction data. A commercial template matching algorithm provided labeled data for model training and comparison.", "result": "Both DenseNets and Swin Transformers outperformed baseline CNNs, with the Swin Transformer achieving the best performance in model evaluation and microstructural prediction. This approach produced clear grain boundaries and consistent intra-grain orientations.", "conclusion": "The successful application of machine learning, specifically the Swin Transformer, demonstrates the feasibility of robust, automated high-resolution microstructural characterization at the nanoscale, enabling advancements in material analysis."}}
{"id": "2507.01913", "pdf": "https://arxiv.org/pdf/2507.01913", "abs": "https://arxiv.org/abs/2507.01913", "authors": ["Apoorv Verma", "Junaid Jami", "Amrita Bhattacharya"], "title": "Advancing Magnetic Materials Discovery -- A structure-based machine learning approach for magnetic ordering and magnetic moment prediction", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "Accurately predicting magnetic behavior across diverse materials systems\nremains a longstanding challenge due to the complex interplay of structural and\nelectronic factors and is pivotal for the accelerated discovery and design of\nnext-generation magnetic materials. In this work, a refined descriptor is\nproposed that significantly improves the prediction of two critical magnetic\nproperties -- magnetic ordering (Ferromagnetic vs. Ferrimagnetic) and magnetic\nmoment per atom -- using only the structural information of materials. Unlike\nprevious models limited to Mn-based or lanthanide-transition metal compounds,\nthe present approach generalizes across a diverse dataset of 5741 stable,\nbinary and ternary, ferromagnetic and ferrimagnetic compounds sourced from the\nMaterials Project. Leveraging an enriched elemental vector representation and\nadvanced feature engineering, including nonlinear terms and reduced matrix\nsparsity, the LightGBM-based model achieves an accuracy of 82.4% for magnetic\nordering classification and balanced recall across FM and FiM classes,\naddressing a key limitation in prior studies. The model predicts magnetic\nmoment per atom with a correlation coefficient of 0.93, surpassing the Hund's\nmatrix and orbital field matrix descriptors. Additionally, it accurately\nestimates formation energy per atom, enabling assessment of both magnetic\nbehavior and material stability. This generalized and computationally efficient\nframework offers a robust tool for high-throughput screening of magnetic\nmaterials with tailored properties.", "AI": {"tldr": "This paper introduces a refined descriptor for predicting magnetic properties using only structural information, achieving high accuracy across diverse materials.", "motivation": "The goal is to improve the prediction of magnetic behavior, a complex challenge, to accelerate the discovery of next-generation magnetic materials.", "method": "The study leverages an enriched elemental vector representation, LightGBM-based modeling, nonlinear terms, and reduced matrix sparsity for predicting magnetic properties.", "result": "The model achieves 82.4% accuracy for magnetic ordering classification, balanced recall across FM and FiM classes, and a correlation coefficient of 0.93 for predicting magnetic moment per atom; it also estimates formation energy per atom accurately.", "conclusion": "The approach offers a generalized, computationally efficient framework for screening magnetic materials, enhancing material design and discovery processes."}}
