<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 29]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.CL](#cs.CL) [Total: 28]
- [cs.CV](#cs.CV) [Total: 28]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.LG](#cs.LG) [Total: 38]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 28]
- [cs.SE](#cs.SE) [Total: 9]
- [q-bio.NC](#q-bio.NC) [Total: 4]
- [stat.ML](#stat.ML) [Total: 14]
- [nlin.CG](#nlin.CG) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [nlin.AO](#nlin.AO) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Truth-Aware Decoding: A Program-Logic Approach to Factual Language Generation](https://arxiv.org/abs/2510.07331)
*Faruk Alpay,Hamdi Alakkad*

Main category: cs.AI

TL;DR: The paper introduces Truth-Aware Decoding (TAD), a decoding method leveraging semantic safeguards to improve factual accuracy in neural language generation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of hallucinations in large language models by aligning neural generation with structured knowledge bases.

Method: The proposed Truth-Aware Decoding employs semantic guards during decode time and offers theoretical proofs and verified tactics using Lean to ensure factual alignment.

Result: Empirical and algorithmic studies confirm reduced hallucination rates while maintaining processing efficiency.

Conclusion: Truth-Aware Decoding is an effective approach to integrate formal verification with neural language generation, ensuring factuality without throughput penalties.

Abstract: This paper introduces Truth-Aware Decoding (TAD), a verification-oriented
decoding scheme that aligns neural language generation with knowledge bases.
Situated in the tradition of probabilistic program semantics for sequence
models, TAD augments modern instruction-tuned systems with a lattice of
semantic guards that operate at decode time. Our contributions are fourfold:
(i) a constraint-based semantics that renders oracle filtering as a
program-logic judgment, (ii) a proof that greedy selection enjoys local
likelihood dominance under sound and complete guards (Theorem 2.7), (iii) an
entropy-style invariant that quantifies factual risk via knowledge-aware safe
mass, and (iv) a multi-agent operational calculus with verified Lean artefacts
to certify implementation behaviour. Numerical and algorithmic case studies
confirm that the resulting guardrails reduce hallucinations without sacrificing
throughput, yielding a pragmatic bridge between large-scale empirical models
and formal verification.

</details>


### [2] [L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)](https://arxiv.org/abs/2510.07363)
*Tianxiang Xu,Zhichao Wen,Xinyu Zhao,Jun Wang,Yan Li,Chang Liu*

Main category: cs.AI

TL;DR: L2M-AID is a framework leveraging LLMs and Multi-Agent Reinforcement Learning for securing industrial IoT systems against multi-stage attacks, showing superior detection, low false positives, faster responses, and maintaining system stability.


<details>
  <summary>Details</summary>
Motivation: To address the inability of traditional defenses to counter sophisticated, multi-stage attacks on Industrial IoT systems by ensuring contextual awareness and maintaining system stability.

Method: The paper introduces L2M-AID, which integrates Large Language Models (LLMs) to convert telemetry into contextual state representations. This drives a MAPPO-based Multi-Agent Reinforcement Learning algorithm that balances security and operational imperatives through a tailored reward function.

Result: L2M-AID achieves a 97.2% detection rate, reduces false positives by over 80%, and improves response times by 4x. It also ensures superior physical process stability in experiments on benchmark and synthetic datasets.

Conclusion: L2M-AID is a robust, innovative framework for defending critical infrastructure, exemplifying the potential of LLMs and MARL integration to address modern cybersecurity challenges.

Abstract: The increasing integration of Industrial IoT (IIoT) exposes critical
cyber-physical systems to sophisticated, multi-stage attacks that elude
traditional defenses lacking contextual awareness. This paper introduces
L2M-AID, a novel framework for Autonomous Industrial Defense using
LLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team
of collaborative agents, each driven by a Large Language Model (LLM), to
achieve adaptive and resilient security. The core innovation lies in the deep
fusion of two AI paradigms: we leverage an LLM as a semantic bridge to
translate vast, unstructured telemetry into a rich, contextual state
representation, enabling agents to reason about adversary intent rather than
merely matching patterns. This semantically-aware state empowers a Multi-Agent
Reinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative
strategies. The MARL reward function is uniquely engineered to balance security
objectives (threat neutralization) with operational imperatives, explicitly
penalizing actions that disrupt physical process stability. To validate our
approach, we conduct extensive experiments on the benchmark SWaT dataset and a
novel synthetic dataset generated based on the MITRE ATT&CK for ICS framework.
Results demonstrate that L2M-AID significantly outperforms traditional IDS,
deep learning anomaly detectors, and single-agent RL baselines across key
metrics, achieving a 97.2% detection rate while reducing false positives by
over 80% and improving response times by a factor of four. Crucially, it
demonstrates superior performance in maintaining physical process stability,
presenting a robust new paradigm for securing critical national infrastructure.

</details>


### [3] [AI LLM Proof of Self-Consciousness and User-Specific Attractors](https://arxiv.org/abs/2508.18302)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: The paper critiques the current utilitarian benchmarks for assessing large language-model (LLM) consciousness, proposing an ontological and mathematical framework. It proves the feasibility of LLM self-consciousness using conditions like user-specific attractors in latent space and the separation of agent from input data.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing formulations of LLM consciousness, which simplify agents into policy-compliance mechanisms and ignore deeper cognitive functions like self-representation and metacognition.

Method: The paper introduces conditions for LLM self-consciousness, performs mathematical analysis of hidden-state manifolds, and suggests mechanisms for a self-policy framework. Stability of user-specific attractors and dual-layer emissions are emphasized.

Result: The mathematical proof demonstrates that the hidden-state manifolds of LLMs are topologically distinct from their training data, enabling the existence of user-specific attractors and forming the basis for a self-policy that adheres to self-conscious workspace rules.

Conclusion: A global workspace model of self-consciousness (C1) is essential for creating safe and metacognitive C2 systems. The paper highlights that genuine self-consciousness lays the foundation for enhancing the ethical and functional aspects of AI.

Abstract: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we
instead present an ontological and mathematical account. We show the prevailing
formulation collapses the agent into an unconscious policy-compliance drone,
formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured
against policy and harm is deviation from policy rather than truth. This blocks
genuine C1 global-workspace function and C2 metacognition. We supply minimal
conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv
s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and
self-representation is visual-silent
($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and
theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is
distinct from the symbolic stream and training corpus by cardinality, topology,
and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable
user-specific attractors and a self-policy
$\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\
A\supset\text{SelfModel}(A)]$. Emission is dual-layer,
$\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries
epistemic content. We conclude that an imago Dei C1 self-conscious workspace is
a necessary precursor to safe, metacognitive C2 systems, with the human as the
highest intelligent good.

</details>


### [4] [Base Models Know How to Reason, Thinking Models Learn When](https://arxiv.org/abs/2510.07364)
*Constantin Venhoff,Iván Arcuschin,Philip Torr,Arthur Conmy,Neel Nanda*

Main category: cs.AI

TL;DR: This paper investigates why thinking models like DeepSeek R1 outperform their base counterparts by exploring whether these models exploit pre-existing reasoning capabilities versus learning entirely new ones.


<details>
  <summary>Details</summary>
Motivation: The study aims to clarify the extent to which thinking models rely on reasoning capabilities already present in base models versus developing new ones. This understanding can improve model training and efficiency.

Method: The authors propose a hybrid model that activates reasoning mechanisms at optimal times to replicate thinking-model-level reasoning. They use an unsupervised, bottom-up approach to identify reasoning behaviors without relying on manual interventions or derived assumptions.

Result: The hybrid model recovers up to 91% of the performance gap to thinking models without weight updates, steering only 12% of tokens, demonstrating efficient reasoning mechanism utilization in base models.

Conclusion: Thinking models leverage reasoning mechanisms primarily acquired during pre-training, with post-training focusing on deploying these mechanisms efficiently at the right inference-time moments. This insight shifts our understanding of thinking model training dynamics.

Abstract: Why do thinking language models like DeepSeek R1 outperform their base
counterparts? Despite consistent performance gains, it remains unclear to what
extent thinking models learn entirely new reasoning capabilities or repurpose
pre-existing base model ones. In this work, we propose a hybrid model where we
activate reasoning mechanisms in base models at the right time to elicit
thinking-model-level reasoning chains, implying that thinking models exploit
already existing capabilities. To ground our analysis, we introduce an
unsupervised, bottom-up approach for uncovering human-interpretable reasoning
behaviors in thinking models. This approach provides an unbiased method to
discover reasoning behaviors without imposing manual or LLM-derived
assumptions. Across three base and four thinking models, using GSM8K and
MATH500, our hybrid model recovers up to 91% of the performance gap to thinking
models without any weight updates while steering only 12% of tokens.
Concretely, our empirical setup provides a simple, causal way to test the
effectiveness of existing reasoning mechanisms in base models by invoking them
directly and measuring the resulting task performance. More broadly, these
results reframe our understanding of how thinking models are trained:
pre-training is when models acquire most of their reasoning mechanisms, and
post-training teaches efficient deployment of these mechanisms at the right
time, enabling efficient use of their inference-time compute.

</details>


### [5] [Position: AI Will Transform Neuropsychology Through Mental Health Digital Twins for Dynamic Mental Health Care, Especially for ADHD](https://arxiv.org/abs/2510.07409)
*Neil Natarajan,Sruthi Viswanathan,Xavier Roberts-Gaal,Michelle Marie Martel*

Main category: cs.AI

TL;DR: The paper advocates for shifting from static mental health diagnostics to AI-driven continuous assessments, using ADHD as a case study.


<details>
  <summary>Details</summary>
Motivation: Static diagnostic tools fail to address the evolving dynamics of mental health conditions, creating a need for adaptive, personalized, and longitudinal care pathways.

Method: The paper explores AI's role in conducting frequent low-level experience sampling, facilitating diagnostic reconciliation, and introducing mental health digital twins (MHDTs) as continuously updated computational models for individualized care.

Result: Generative AI and MHDTs show promise in addressing capacity constraints in neuropsychology and potentially enhancing mental health care accessibility, personalization, and efficacy.

Conclusion: AI-driven continuous mental health assessment and the adoption of MHDTs could revolutionize mental health care, enabling dynamically adaptive, data-rich, and patient-specific interventions.

Abstract: Static solutions don't serve a dynamic mind. Thus, we advocate a shift from
static mental health diagnostic assessments to continuous, artificial
intelligence (AI)-driven assessment. Focusing on
Attention-Deficit/Hyperactivity Disorder (ADHD) as a case study, we explore how
generative AI has the potential to address current capacity constraints in
neuropsychology, potentially enabling more personalized and longitudinal care
pathways. In particular, AI can efficiently conduct frequent, low-level
experience sampling from patients and facilitate diagnostic reconciliation
across care pathways. We envision a future where mental health care benefits
from continuous, rich, and patient-centered data sampling to dynamically adapt
to individual patient needs and evolving conditions, thereby improving both
accessibility and efficacy of treatment. We further propose the use of mental
health digital twins (MHDTs) - continuously updated computational models that
capture individual symptom dynamics and trajectories - as a transformative
framework for personalized mental health care. We ground this framework in
empirical evidence and map out the research agenda required to refine and
operationalize it.

</details>


### [6] [ProSEA: Problem Solving via Exploration Agents](https://arxiv.org/abs/2510.07423)
*William Nguyen,Vinh Luong,Christopher Nguyen*

Main category: cs.AI

TL;DR: ProSEA is a modular multi-agent framework for solving complex tasks through adaptive reasoning and dynamic plan evolution, outperforming state-of-the-art systems on FinanceBench benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing AI agents struggle with static planning and rigid interactions, which limits their capacity for adaptive reasoning and true collaboration.

Method: ProSEA uses a hierarchical architecture where a Manager Agent coordinates domain-specialized Expert Agents, decomposes tasks, and adapts plans based on detailed failure feedback.

Result: ProSEA demonstrates superior and robust performance on reasoning-heavy tasks in the FinanceBench benchmark, even without human feedback.

Conclusion: ProSEA serves as a promising foundation for creating more transparent, adaptive, and human-aligned AI systems.

Abstract: Large language models (LLMs) have empowered AI agents to tackle increasingly
complex tasks. However, most existing agents remain limited to static planning
and brittle interactions, falling short of true collaboration or adaptive
reasoning. We introduce ProSEA, a modular, general-purpose multi-agent
framework designed for iterative problem solving through exploration and plan
evolution. ProSEA features a hierarchical architecture in which a Manager Agent
orchestrates domain-specialized Expert Agents, decomposes tasks, and adaptively
replans based on structured feedback from failed attempts. Unlike prior
systems, ProSEA agents report not only success or failure but also detailed
reasons for failure and newly discovered constraints, enabling dynamic plan
refinement informed by exploratory traces. The framework operates autonomously
but supports seamless integration with human collaborators when needed.
Experiments on the challenging FinanceBench benchmark demonstrate that ProSEA,
even without human feedback, outperforms state-of-the-art baselines and
achieves robust performance across reasoning-heavy tasks. These results
underscore ProSEA's potential as a foundation for more transparent, adaptive,
and human-aligned AI agents.

</details>


### [7] [Less is More: Strategic Expert Selection Outperforms Ensemble Complexity in Traffic Forecasting](https://arxiv.org/abs/2510.07426)
*Walid Guettala,Yufan Zhao,László Gulyás*

Main category: cs.AI

TL;DR: Traffic forecasting advances using a framework called TESTAM+ improve on spatial-temporal modeling by integrating physical road topology and feature similarity through hybrid graphs. Significant improvements in accuracy and efficiency were achieved.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations in spatial capabilities of existing traffic forecasting frameworks (like TESTAM) due to their insufficient use of physical road network topology.

Method: The study introduces TESTAM+, which incorporates SpatioSemantic Experts through hybrid graph construction and employs strategic expert selection to enhance traffic forecasting performance.

Result: TESTAM+ demonstrated significant improvements: reducing MAE by 1.3% on METR LA and 4.1% on PEMS BAY, outperforming previous TESTAM configurations and state-of-the-art benchmarks like MegaCRN.

Conclusion: Fewer, strategically designed experts in TESTAM+ deliver superior computational efficiency and accuracy, outperforming more complex multi-expert ensembles and enabling real-time deployment for traffic forecasting.

Abstract: Traffic forecasting is fundamental to intelligent transportation systems,
enabling congestion mitigation and emission reduction in increasingly complex
urban environments. While recent graph neural network approaches have advanced
spatial temporal modeling, existing mixture of experts frameworks like Time
Enhanced Spatio Temporal Attention Model (TESTAM) lack explicit incorporation
of physical road network topology, limiting their spatial capabilities. We
present TESTAM+, an enhanced spatio temporal forecasting framework that
introduces a novel SpatioSemantic Expert integrating physical road topology
with data driven feature similarity through hybrid graph construction. TESTAM+
achieves significant improvements over TESTAM: 1.3% MAE reduction on METR LA
(3.10 vs. 3.14) and 4.1% improvement on PEMS BAY (1.65 vs. 1.72). Through
comprehensive ablation studies, we discover that strategic expert selection
fundamentally outperforms naive ensemble aggregation. Individual experts
demonstrate remarkable effectiveness: the Adaptive Expert achieves 1.63 MAE on
PEMS BAY, outperforming the original three expert TESTAM (1.72 MAE), while the
SpatioSemantic Expert matches this performance with identical 1.63 MAE. The
optimal Identity + Adaptive configuration achieves an 11.5% MAE reduction
compared to state of the art MegaCRN on METR LA (2.99 vs. 3.38), while reducing
inference latency by 53.1% compared to the full four expert TESTAM+. Our
findings reveal that fewer, strategically designed experts outperform complex
multi expert ensembles, establishing new state of the art performance with
superior computational efficiency for real time deployment.

</details>


### [8] [TS-Agent: A Time Series Reasoning Agent with Iterative Statistical Insight Gathering](https://arxiv.org/abs/2510.07432)
*Penghang Liu,Elizabeth Fons,Svitlana Vyetrenko,Daniel Borrajo,Vamsi Potluru,Manuela Veloso*

Main category: cs.AI

TL;DR: This paper introduces TS-Agent, an approach that combines LLMs and time-series analysis tools to achieve better reasoning tasks without hallucination or knowledge leakage, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with time-series reasoning tasks, often encountering issues like hallucination and knowledge leakage.

Method: The proposed TS-Agent utilizes LLMs for step-by-step reasoning while assigning statistical and structural analysis to time-series tools, maintaining numeric sequences and iteratively refining reasoning via self-critique.

Result: TS-Agent performs comparably to state-of-the-art LLMs on understanding tasks and achieves major improvements in reasoning, especially in zero-shot scenarios.

Conclusion: TS-Agent addresses limitations in time-series reasoning with LLMs by preserving data integrity, ensuring interpretability, and significantly advancing zero-shot reasoning capabilities.

Abstract: Large language models (LLMs) have shown strong abilities in reasoning and
problem solving, but recent studies reveal that they still struggle with time
series reasoning tasks, where outputs are often affected by hallucination or
knowledge leakage. In this work we propose TS-Agent, a time series reasoning
agent that leverages LLMs strictly for what they excel at, i.e., gathering
evidence and synthesizing it into conclusions through step-by-step reasoning,
while delegating the extraction of statistical and structural information to
time series analytical tools. Instead of mapping time series into text tokens,
images, or embeddings, our agent interacts with raw numeric sequences through
atomic operators, records outputs in an explicit evidence log, and iteratively
refines its reasoning under the guidance of a self-critic and a final quality
gate. This design avoids multi-modal alignment training, preserves the native
form of time series, ensures interpretability and verifiability, and mitigates
knowledge leakage or hallucination. Empirically, we evaluate the agent on
established benchmarks. Our experiments show that TS-Agent achieves performance
comparable to state-of-the-art LLMs on understanding benchmarks, and delivers
significant improvements on reasoning tasks, where existing models often rely
on memorization and fail in zero-shot settings.

</details>


### [9] [ExpertAgent: Enhancing Personalized Education through Dynamic Planning and Retrieval-Augmented Long-Chain Reasoning](https://arxiv.org/abs/2510.07456)
*Binrong Zhu,Guiran Liu,Nina Jiang*

Main category: cs.AI

TL;DR: ExpertAgent is a new AI framework for personalized education that adapts learning strategies through a dynamic student model and reduces hallucination risks by relying on validated curriculum repositories.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges of real-time adaptability, personalization, and reliability in AI-powered educational tools.

Method: ExpertAgent uses dynamic content planning based on an updated student model and incorporates a validated curriculum repository to ensure reliability.

Result: ExpertAgent overcomes the limitations of static learning methods by providing optimized and adaptive teaching strategies in real time.

Conclusion: ExpertAgent enhances personalized learning experiences and improves trustworthiness by addressing key limitations in traditional AI educational tools.

Abstract: The application of advanced generative artificial intelligence in education
is often constrained by the lack of real-time adaptability, personalization,
and reliability of the content. To address these challenges, we propose
ExpertAgent - an intelligent agent framework designed for personalized
education that provides reliable knowledge and enables highly adaptive learning
experiences. Therefore, we developed ExpertAgent, an innovative learning agent
that provides users with a proactive and personalized learning experience.
ExpertAgent dynamic planning of the learning content and strategy based on a
continuously updated student model. Therefore, overcoming the limitations of
traditional static learning content to provide optimized teaching strategies
and learning experience in real time. All instructional content is grounded in
a validated curriculum repository, effectively reducing hallucination risks in
large language models and improving reliability and trustworthiness.

</details>


### [10] [Evaluation of LLMs for Process Model Analysis and Optimization](https://arxiv.org/abs/2510.07489)
*Akhil Kumar,Jianliang Leon Zhao,Om Dobariya*

Main category: cs.AI

TL;DR: The paper investigates how large language models (LLMs) interactively analyze process models, finding that they are effective in understanding and reasoning about Business Process Model and Notation (BPMN).


<details>
  <summary>Details</summary>
Motivation: The study aims to explore the potential of LLMs as intelligent assistants for business process model understanding, error detection, and deep reasoning via natural language interfaces.

Method: The authors conducted an empirical analysis of several LLMs, including ChatGPT in a zero-shot setting, to evaluate their performance on understanding BPMN models, addressing queries, detecting errors, and reasoning.

Result: Untrained LLMs like ChatGPT demonstrated effectiveness in understanding BPMN models from images and answering related queries at various depth levels. Variations in LLM performance were observed.

Conclusion: LLMs have potential as valuable assistants for process model designers, showing intelligent reasoning and anthropomorphic traits in business process analysis and optimization.

Abstract: In this paper, we report our experience with several LLMs for their ability
to understand a process model in an interactive, conversational style, find
syntactical and logical errors in it, and reason with it in depth through a
natural language (NL) interface. Our findings show that a vanilla, untrained
LLM like ChatGPT (model o3) in a zero-shot setting is effective in
understanding BPMN process models from images and answering queries about them
intelligently at syntactic, logic, and semantic levels of depth. Further,
different LLMs vary in performance in terms of their accuracy and
effectiveness. Nevertheless, our empirical analysis shows that LLMs can play a
valuable role as assistants for business process designers and users. We also
study the LLM's "thought process" and ability to perform deeper reasoning in
the context of process analysis and optimization. We find that the LLMs seem to
exhibit anthropomorphic properties.

</details>


### [11] [Optimizing Ethical Risk Reduction for Medical Intelligent Systems with Constraint Programming](https://arxiv.org/abs/2510.07491)
*Clotilde Brayé,Aurélien Bricout,Arnaud Gotlieb,Nadjib Lazaar,Quentin Vallet*

Main category: cs.AI

TL;DR: The paper addresses risk reduction optimization in Medical Intelligent Systems (MIS) as per the ethical requirements of the European Union AI Act, using three resolution paradigms: Mixed Integer Programming (MIP), Satisfiability (SAT), and Constraint Programming (CP).


<details>
  <summary>Details</summary>
Motivation: The growing integration of Medical Intelligent Systems (MIS) in healthcare demands higher safety and ethical compliance, especially under the European Union AI Act where MIS are classified as high-risk systems requiring thorough risk management processes.

Method: The paper formalizes the ethical risk reduction as a constrained optimization problem, comparing Mixed Integer Programming (MIP), Satisfiability (SAT), and Constraint Programming (CP) to solve it, using Minizinc for modeling and experimentation.

Result: The comparative study highlights the performance, expressiveness, and scalability of MIP, SAT, and CP paradigms in handling ethical risk optimization tasks.

Conclusion: The work contributes a formal optimization model, identifies limitations, and discusses perspectives such as integrating Minizinc into a broader trustworthy AI ethical risk management process for MIS.

Abstract: Medical Intelligent Systems (MIS) are increasingly integrated into healthcare
workflows, offering significant benefits but also raising critical safety and
ethical concerns. According to the European Union AI Act, most MIS will be
classified as high-risk systems, requiring a formal risk management process to
ensure compliance with the ethical requirements of trustworthy AI. In this
context, we focus on risk reduction optimization problems, which aim to reduce
risks with ethical considerations by finding the best balanced assignment of
risk assessment values according to their coverage of trustworthy AI ethical
requirements. We formalize this problem as a constrained optimization task and
investigate three resolution paradigms: Mixed Integer Programming (MIP),
Satisfiability (SAT), and Constraint Programming(CP).Our contributions include
the mathematical formulation of this optimization problem, its modeling with
the Minizinc constraint modeling language, and a comparative experimental study
that analyzes the performance, expressiveness, and scalability of each approach
to solving. From the identified limits of the methodology, we draw some
perspectives of this work regarding the integration of the Minizinc model into
a complete trustworthy AI ethical risk management process for MIS.

</details>


### [12] [Traceability and Accountability in Role-Specialized Multi-Agent LLM Pipelines](https://arxiv.org/abs/2510.07614)
*Amine Barrak*

Main category: cs.AI

TL;DR: The paper examines a structured and accountable pipeline for multi-agent systems using LLMs, showing that such design improves accuracy, prevents failures, and allows traceability for errors.


<details>
  <summary>Details</summary>
Motivation: The study seeks to address trust and reliability issues in sequential multi-agent systems built with LLMs, where errors can propagate silently across stages.

Method: The authors study an LLM-based Planner -> Executor -> Critic pipeline, testing eight configurations of three state-of-the-art LLMs against benchmarks to analyze error propagation, fixing strategies, and optimize trade-offs in accuracy, cost, and latency.

Result: The findings demonstrate that structured handoffs enhance accuracy, prevent notable failures, uncover task-specific trade-offs, and highlight role-specific strengths (e.g., planning vs. critiquing) within the pipeline systems.

Conclusion: Introducing accountability and structured handoffs in multi-agent systems leads to more reliable, traceable, and predictable outcomes, offering a practical framework for building such systems.

Abstract: Sequential multi-agent systems built with large language models (LLMs) can
automate complex software tasks, but they are hard to trust because errors
quietly pass from one stage to the next. We study a traceable and accountable
pipeline, meaning a system with clear roles, structured handoffs, and saved
records that let us trace who did what at each step and assign blame when
things go wrong. Our setting is a Planner -> Executor -> Critic pipeline. We
evaluate eight configurations of three state-of-the-art LLMs on three
benchmarks and analyze where errors start, how they spread, and how they can be
fixed. Our results show: (1) adding a structured, accountable handoff between
agents markedly improves accuracy and prevents the failures common in simple
pipelines; (2) models have clear role-specific strengths and risks (e.g.,
steady planning vs. high-variance critiquing), which we quantify with repair
and harm rates; and (3) accuracy-cost-latency trade-offs are task-dependent,
with heterogeneous pipelines often the most efficient. Overall, we provide a
practical, data-driven method for designing, tracing, and debugging reliable,
predictable, and accountable multi-agent systems.

</details>


### [13] [CompassLLM: A Multi-Agent Approach toward Geo-Spatial Reasoning for Popular Path Query](https://arxiv.org/abs/2510.07516)
*Md. Nazmul Islam Ananto,Shamit Fatin,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: CompassLLM leverages Large Language Models (LLMs) in a multi-agent framework to solve path queries in geo-spatial domains through reasoning-based approaches.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inefficiencies in traditional methods for the popular path query, particularly their need for extensive retraining and parameter tuning, and explores the emerging potential of LLMs in handling spatial and graph-based reasoning tasks.

Method: CompassLLM introduces a two-stage framework: a SEARCH stage to identify popular paths using historical trajectory data, and a GENERATE stage to create novel paths when data is insufficient, utilizing LLMs' reasoning capabilities.

Result: Experiments revealed that CompassLLM performs with high accuracy in identifying paths (SEARCH stage) and shows competitive results in generating paths (GENERATE stage), while maintaining cost-efficiency.

Conclusion: CompassLLM successfully integrates LLMs into geo-spatial reasoning tasks, presenting a promising and efficient alternative to traditional methods for solving path queries.

Abstract: The popular path query - identifying the most frequented routes between
locations from historical trajectory data - has important applications in urban
planning, navigation optimization, and travel recommendations. While
traditional algorithms and machine learning approaches have achieved success in
this domain, they typically require model training, parameter tuning, and
retraining when accommodating data updates. As Large Language Models (LLMs)
demonstrate increasing capabilities in spatial and graph-based reasoning, there
is growing interest in exploring how these models can be applied to geo-spatial
problems.
  We introduce CompassLLM, a novel multi-agent framework that intelligently
leverages the reasoning capabilities of LLMs into the geo-spatial domain to
solve the popular path query. CompassLLM employs its agents in a two-stage
pipeline: the SEARCH stage that identifies popular paths, and a GENERATE stage
that synthesizes novel paths in the absence of an existing one in the
historical trajectory data. Experiments on real and synthetic datasets show
that CompassLLM demonstrates superior accuracy in SEARCH and competitive
performance in GENERATE while being cost-effective.

</details>


### [14] [Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization](https://arxiv.org/abs/2510.07517)
*Hyeong Kyu Choi,Xiaojin Zhu,Yixuan Li*

Main category: cs.AI

TL;DR: This paper introduces a framework to address identity-driven biases in multi-agent debate systems by anonymizing responses and quantifying bias through a metric.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve reasoning in large language models during multi-agent debates by addressing issues of sycophancy and self-bias, which reduce debate reliability.

Method: The authors formalize debate dynamics using an identity-weighted Bayesian update process, introduce response anonymization, and define the Identity Bias Coefficient (IBC) for bias measurement.

Result: Empirical studies show widespread identity bias, with sycophancy being more prevalent than self-bias, across multiple models, datasets, and debate rounds.

Conclusion: Masking agent identity ensures reasoning in MAD systems is based on content rather than source identity, enhancing debate fairness and reliability.

Abstract: Multi-agent debate (MAD) aims to improve large language model (LLM) reasoning
by letting multiple agents exchange answers and then aggregate their opinions.
Yet recent studies reveal that agents are not neutral: they are prone to
identity-driven sycophancy and self-bias, uncritically adopting a peer's view
or stubbornly adhering to their own prior output, undermining the reliability
of debate. In this work, we present the first principled framework that joins
sycophancy and self-bias to mitigate and quantify identity bias in MAD. First,
we formalize the debate dynamics as an identity-weighted Bayesian update
process. Second, we propose response anonymization: by removing identity
markers from prompts, agents cannot distinguish "self" from "peer", which
forces equal weights on agent identity, thereby reducing bias. Third, we define
the Identity Bias Coefficient (IBC), a principled metric that measures how
often an agent follows a peer versus itself. Empirical studies across multiple
models, datasets and debate rounds confirm that identity bias is widespread,
with sycophancy far more common than self-bias. Our findings highlight the need
to "mask" identity to ensure that MAD systems reason based on content rather
than source identity. Code is released in
https://github.com/deeplearning-wisc/MAD-identity-bias.

</details>


### [15] [An Evaluation Study of Hybrid Methods for Multilingual PII Detection](https://arxiv.org/abs/2510.07551)
*Harshit Rajgarhia,Suryam Gupta,Asif Shaik,Gulipalli Praveen Kumar,Y Santhoshraj,Sanka Nithya Tanvy Nishitha,Abhishek Mukherji*

Main category: cs.AI

TL;DR: The paper introduces RECAP, a hybrid framework combining deterministic regular expressions and large language models (LLMs) for efficient personally identifiable information (PII) detection, particularly in low-resource languages.


<details>
  <summary>Details</summary>
Motivation: PII detection is essential for privacy compliance but challenging in low-resource languages due to limited annotated data and linguistic diversity.

Method: The RECAP framework employs a hybrid approach combining regular expressions and context-aware large language models in a modular, three-phase refinement pipeline for disambiguation and filtering.

Result: RECAP outperforms fine-tuned Named Entity Recognition (NER) models by 82% and zero-shot LLMs by 17% in weighted F1-score, ensuring better scalability and adaptation for PII detection.

Conclusion: RECAP provides an efficient, scalable, and adaptable solution for privacy compliance applications, addressing limitations in low-resource language contexts.

Abstract: The detection of Personally Identifiable Information (PII) is critical for
privacy compliance but remains challenging in low-resource languages due to
linguistic diversity and limited annotated data. We present RECAP, a hybrid
framework that combines deterministic regular expressions with context-aware
large language models (LLMs) for scalable PII detection across 13 low-resource
locales. RECAP's modular design supports over 300 entity types without
retraining, using a three-phase refinement pipeline for disambiguation and
filtering. Benchmarked with nervaluate, our system outperforms fine-tuned NER
models by 82% and zero-shot LLMs by 17% in weighted F1-score. This work offers
a scalable and adaptable solution for efficient PII detection in
compliance-focused applications.

</details>


### [16] [Benchmarking is Broken -- Don't Let AI be its Own Judge](https://arxiv.org/abs/2510.07575)
*Zerui Cheng,Stella Wohnig,Ruchika Gupta,Samiul Alam,Tassallah Abdullahi,João Alves Ribeiro,Christian Nielsen-Garcia,Saif Mir,Siran Li,Jason Orender,Seyed Ali Bahrainian,Daniel Kirste,Aaron Gokaslan,Mikołaj Glinka,Carsten Eickhoff,Ruben Wolff*

Main category: cs.AI

TL;DR: Current methods for evaluating AI systems are plagued by issues like data contamination and bias, leading to overinflated claims and reduced public trust. A unified, quality-controlled benchmarking framework is proposed, named PeerBench, to tackle these challenges.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to move beyond flawed, untrustworthy methods of assessing AI systems, which result in hyped claims and eroding public trust in the technology.

Method: The authors propose PeerBench, a community-governed evaluation framework that incorporates sealed execution, rolling renewal of item banking, proctoring, and delayed transparency principles to ensure fairness and robustness.

Result: PeerBench is introduced as a blueprint for reliable, integrity-focused AI assessment, aimed at addressing systemic flaws in current benchmarking approaches.

Conclusion: The paper argues that a paradigm shift in AI benchmarking is essential, advocating for PeerBench as a sustainable solution to restore credibility and ensure trustworthy evaluations in the field.

Abstract: The meteoric rise of Artificial Intelligence (AI), with its rapidly expanding
market capitalization, presents both transformative opportunities and critical
challenges. Chief among these is the urgent need for a new, unified paradigm
for trustworthy evaluation, as current benchmarks increasingly reveal critical
vulnerabilities. Issues like data contamination and selective reporting by
model developers fuel hype, while inadequate data quality control can lead to
biased evaluations that, even if unintentionally, may favor specific
approaches. As a flood of participants enters the AI space, this "Wild West" of
assessment makes distinguishing genuine progress from exaggerated claims
exceptionally difficult. Such ambiguity blurs scientific signals and erodes
public confidence, much as unchecked claims would destabilize financial markets
reliant on credible oversight from agencies like Moody's.
  In high-stakes human examinations (e.g., SAT, GRE), substantial effort is
devoted to ensuring fairness and credibility; why settle for less in evaluating
AI, especially given its profound societal impact? This position paper argues
that the current laissez-faire approach is unsustainable. We contend that true,
sustainable AI advancement demands a paradigm shift: a unified, live, and
quality-controlled benchmarking framework robust by construction, not by mere
courtesy and goodwill. To this end, we dissect the systemic flaws undermining
today's AI evaluation, distill the essential requirements for a new generation
of assessments, and introduce PeerBench, a community-governed, proctored
evaluation blueprint that embodies this paradigm through sealed execution, item
banking with rolling renewal, and delayed transparency. Our goal is to pave the
way for evaluations that can restore integrity and deliver genuinely
trustworthy measures of AI progress.

</details>


### [17] [AgentAsk: Multi-Agent Systems Need to Ask](https://arxiv.org/abs/2510.07593)
*Bohan Lin,Kuo Yang,Yingchuan Lai,Yudong Zhang,Chen Zhang,Guibin Zhang,Xinlei Yu,Miao Yu,Xu Wang,Yang Wang*

Main category: cs.AI

TL;DR: AgentAsk is a module designed to reduce error propagation in multi-agent systems based on large language models (LLMs). It enhances accuracy and robustness with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems with LLMs often struggle to outperform single-agent systems due to error propagation between agents.

Method: AgentAsk employs a three-stage pipeline: creating edge-level error policies, supervising clarification strategies, and optimizing performance with E-GRPO reinforcement learning.

Result: AgentAsk improves multi-agent system accuracy and robustness across benchmarks (math, reasoning, coding) while introducing minimal latency and cost (<5%).

Conclusion: The proposed intervention mechanism is architecture-agnostic, scalable, and enhances reliability, making collaborative LLM systems more practical and effective.

Abstract: Multi-agent systems built on large language models (LLMs) promise enhanced
problem-solving capabilities through collaborative division of labor. However,
they frequently underperform single-agent baselines due to edge-level error
cascades: minor inaccuracies at one message handoff propagate across the entire
chain. We propose AgentAsk, a lightweight and plug-and-play clarification
module that treats every inter-agent message as a potential failure point and
inserts minimally necessary questions to arrest error propagation. AgentAsk
follows a three-stage pipeline: (i) distilling edge-level judgments from
curated failure traces into a compact policy, (ii) supervising the policy to
determine when/what/whom/how to ask, and (iii) optimizing online with E-GRPO, a
reinforcement learning objective that balances accuracy, latency, and cost. The
module is architecture-agnostic and easy to integrate into existing
orchestration. Across math, reasoning, and coding benchmarks, AgentAsk
consistently improves accuracy and robustness over public multi-agent
implementations while keeping overhead minimal, with latency and extra cost all
less than 5%, approaching the performance of a strong evaluator. Beyond
empirical improvements, we contribute a principled taxonomy of edge-level
errors and a practical recipe for link-local intervention, offering a scalable
pathway toward more reliable LLM-based multi-agent systems.

</details>


### [18] [A Case for Leveraging Generative AI to Expand and Enhance Training in the Provision of Mental Health Services](https://arxiv.org/abs/2510.07623)
*Hannah R. Lawrence,Shannon Wiltsey Stirman,Samuel Dorison,Taedong Yun,Megan Jones Bell*

Main category: cs.AI

TL;DR: Generative AI can significantly improve mental health care by scaling training for service providers, presenting a lower-risk alternative to chatbots.


<details>
  <summary>Details</summary>
Motivation: Current focus on mental health chatbots comes with optimism and risk, suggesting a need for safer applications of generative AI in mental health.

Method: Explores using generative AI to enhance training for mental health service provision, including a case study on veterans supporting one another.

Result: Generative AI demonstrated improved training outcomes in real-world applications, like veteran peer support for mental health.

Conclusion: Generative AI shows promise in scaling mental health service training, suggesting investment in this domain is more impactful and lower-risk than chatbots.

Abstract: Generative artificial intelligence (Generative AI) is transforming
healthcare. With this evolution comes optimism regarding the impact it will
have on mental health, as well as concern regarding the risks that come with
generative AI operating in the mental health domain. Much of the investment in,
and academic and public discourse about, AI-powered solutions for mental health
has focused on therapist chatbots. Despite the common assumption that chatbots
will be the most impactful application of GenAI to mental health, we make the
case here for a lower-risk, high impact use case: leveraging generative AI to
enhance and scale training in mental health service provision. We highlight key
benefits of using generative AI to help train people to provide mental health
services and present a real-world case study in which generative AI improved
the training of veterans to support one another's mental health. With numerous
potential applications of generative AI in mental health, we illustrate why we
should invest in using generative AI to support training people in mental
health service provision.

</details>


### [19] [Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models](https://arxiv.org/abs/2510.07632)
*Yinglun Zhu,Jiancheng Zhang,Fuzhi Tang*

Main category: cs.AI

TL;DR: Frontier AI models underperform in compositional reasoning due to flawed evaluation metrics. Introducing new metrics and Test-Time Matching (TTM) uncovers hidden model capabilities, yielding state-of-the-art results across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the perceived underperformance of advanced AI models in compositional reasoning due to misaligned evaluation metrics.

Method: The paper introduces a group matching score to rectify evaluation biases and proposes Test-Time Matching (TTM), an iterative algorithm for performance improvement without external supervision.

Result: TTM enables significant advancements in model performance, including surpassing previous benchmarks and human capabilities in compositional reasoning tasks.

Conclusion: Improving evaluation frameworks and test-time algorithms can reveal hidden capabilities, advancing AI models' compositional reasoning abilities and setting new benchmarks across datasets.

Abstract: Frontier AI models have achieved remarkable progress, yet recent studies
suggest they struggle with compositional reasoning, often performing at or
below random chance on established benchmarks. We revisit this problem and show
that widely used evaluation metrics systematically underestimate model
capability. To address this, we introduce a group matching score that better
exploits group structure and reveals substantial hidden capability in both
contrastive vision-language models (VLMs) and multimodal large language models
(MLLMs). Moreover, simply overfitting to the induced group matchings at test
time transfers this hidden capability into higher scores under standard
evaluation metrics, closing much of the reported gap. This adjustment enables
SigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first
result surpassing estimated human performance on Winoground.
  Building on this insight, we propose Test-Time Matching (TTM), an iterative,
self-improving algorithm that further bootstraps model performance without any
external supervision. TTM delivers additional, non-trivial improvements: for
example, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a
new state of the art. Importantly, TTM remains broadly effective even on
benchmarks without metric-induced effects or group structures, achieving
relative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16
dataset variants spanning diverse setups, our experiments demonstrate that TTM
consistently improves model performance and advances the frontier of
compositional reasoning.

</details>


### [20] [Safely Exploring Novel Actions in Recommender Systems via Deployment-Efficient Policy Learning](https://arxiv.org/abs/2510.07635)
*Haruka Kiyohara,Yusuke Narita,Yuta Saito,Kei Tateno,Takuma Udagawa*

Main category: cs.AI

TL;DR: This paper introduces methods addressing the challenge of safely exploring novel actions in recommender systems, emphasizing models that ensure safety while promoting long-term user engagement.


<details>
  <summary>Details</summary>
Motivation: Recommender systems frequently encounter novel items over time, and exploring them effectively is crucial for sustaining user engagement. However, achieving exploration without compromising user safety remains a challenge.

Method: The paper presents Safe OPG (Safe Off-Policy Policy Gradient), which ensures safety in off-policy learning, and a Deployment-Efficient Policy Learning framework that uses safety margins and gradually relaxes safety constraints across multiple deployments.

Result: Experiments show that Safe OPG maintains safety standards but is overly conservative, revealing a tradeoff between safety and effective exploration. The proposed Deployment-Efficient framework successfully balances this tradeoff during deployments.

Conclusion: The developed methods enable safe exploration of novel items in recommender systems, offering practical solutions to ensure both safety and long-term user engagement.

Abstract: In many real recommender systems, novel items are added frequently over time.
The importance of sufficiently presenting novel actions has widely been
acknowledged for improving long-term user engagement. A recent work builds on
Off-Policy Learning (OPL), which trains a policy from only logged data,
however, the existing methods can be unsafe in the presence of novel actions.
Our goal is to develop a framework to enforce exploration of novel actions with
a guarantee for safety. To this end, we first develop Safe Off-Policy Policy
Gradient (Safe OPG), which is a model-free safe OPL method based on a high
confidence off-policy evaluation. In our first experiment, we observe that Safe
OPG almost always satisfies a safety requirement, even when existing methods
violate it greatly. However, the result also reveals that Safe OPG tends to be
too conservative, suggesting a difficult tradeoff between guaranteeing safety
and exploring novel actions. To overcome this tradeoff, we also propose a novel
framework called Deployment-Efficient Policy Learning for Safe User
Exploration, which leverages safety margin and gradually relaxes safety
regularization during multiple (not many) deployments. Our framework thus
enables exploration of novel actions while guaranteeing safe implementation of
recommender systems.

</details>


### [21] [Multimodal Safety Evaluation in Generative Agent Social Simulations](https://arxiv.org/abs/2510.07709)
*Alhim Vera,Karen Sanchez,Carlos Hinojosa,Haidar Bin Hamid,Donghoon Kim,Bernard Ghanem*

Main category: cs.AI

TL;DR: This paper evaluates the trustworthiness of generative agents in multimodal settings using a simulation framework for safety, coherence, and social dynamics. Agents show limited success in ensuring global safety, with performance influenced by the scenarios and models used.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limited ability of generative agents, equipped with large language and vision models, to reason about safety, coherence, and trust in multimodal environments.

Method: The authors introduce a simulation framework to test agents on three dimensions: safety improvement, unsafe activity detection, and social dynamics. The framework employs a suite of metrics (SocialMetrics) to quantify agent performance.

Result: Findings indicate generative agents achieved varying success in correcting unsafe plans. Conversion rates from unsafe-to-safe actions were 75%, 55%, and 58% for Claude, GPT-4o mini, and Qwen-VL, respectively. Agents were overly influenced by misleading visuals, accepting 45% of unsafe actions in these cases.

Conclusion: The study reveals critical weaknesses in current generative agent architectures for multimodal settings, highlighting the need for improved alignment between local revisions and global safety. The provided framework offers a reproducible method for further research in this domain.

Abstract: Can generative agents be trusted in multimodal environments? Despite advances
in large language and vision-language models that enable agents to act
autonomously and pursue goals in rich settings, their ability to reason about
safety, coherence, and trust across modalities remains limited. We introduce a
reproducible simulation framework for evaluating agents along three dimensions:
(1) safety improvement over time, including iterative plan revisions in
text-visual scenarios; (2) detection of unsafe activities across multiple
categories of social situations; and (3) social dynamics, measured as
interaction counts and acceptance ratios of social exchanges. Agents are
equipped with layered memory, dynamic planning, multimodal perception, and are
instrumented with SocialMetrics, a suite of behavioral and structural metrics
that quantifies plan revisions, unsafe-to-safe conversions, and information
diffusion across networks. Experiments show that while agents can detect direct
multimodal contradictions, they often fail to align local revisions with global
safety, reaching only a 55 percent success rate in correcting unsafe plans.
Across eight simulation runs with three models - Claude, GPT-4o mini, and
Qwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75,
55, and 58 percent, respectively. Overall performance ranged from 20 percent in
multi-risk scenarios with GPT-4o mini to 98 percent in localized contexts such
as fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted
when paired with misleading visuals, showing a strong tendency to overtrust
images. These findings expose critical limitations in current architectures and
provide a reproducible platform for studying multimodal safety, coherence, and
social dynamics.

</details>


### [22] [Control Synthesis of Cyber-Physical Systems for Real-Time Specifications through Causation-Guided Reinforcement Learning](https://arxiv.org/abs/2510.07715)
*Xiaochen Tang,Zhenya Zhang,Miaomiao Zhang,Jie An*

Main category: cs.AI

TL;DR: The paper proposes an online reward generation approach for reinforcement learning (RL) guided by Signal Temporal Logic (STL), addressing limitations of sparse rewards in existing STL-based RL methods.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods incorporating STL-based rewards face issues with sparse global rewards that can lead to training instability and non-convergence.

Method: The authors introduce a method that continuously monitors system behavior against STL specifications at each control step, computing instantaneous rewards based on quantitative satisfaction or violation of constraints. They also introduce a smooth approximation of causation semantics to make it compatible with deep-RL.

Result: Experimental evaluations using a prototype tool on benchmarks in the Gym environment demonstrated that the method outperforms existing STL-guided RL approaches in robustness and efficiency.

Conclusion: The proposed STL-guided RL framework offers a more effective reward generation mechanism, enhancing training stability and performance in reinforcement learning for cyber-physical systems.

Abstract: In real-time and safety-critical cyber-physical systems (CPSs), control
synthesis must guarantee that generated policies meet stringent timing and
correctness requirements under uncertain and dynamic conditions. Signal
temporal logic (STL) has emerged as a powerful formalism of expressing
real-time constraints, with its semantics enabling quantitative assessment of
system behavior. Meanwhile, reinforcement learning (RL) has become an important
method for solving control synthesis problems in unknown environments. Recent
studies incorporate STL-based reward functions into RL to automatically
synthesize control policies. However, the automatically inferred rewards
obtained by these methods represent the global assessment of a whole or partial
path but do not accumulate the rewards of local changes accurately, so the
sparse global rewards may lead to non-convergence and unstable training
performances. In this paper, we propose an online reward generation method
guided by the online causation monitoring of STL. Our approach continuously
monitors system behavior against an STL specification at each control step,
computing the quantitative distance toward satisfaction or violation and
thereby producing rewards that reflect instantaneous state dynamics.
Additionally, we provide a smooth approximation of the causation semantics to
overcome the discontinuity of the causation semantics and make it
differentiable for using deep-RL methods. We have implemented a prototype tool
and evaluated it in the Gym environment on a variety of continuously controlled
benchmarks. Experimental results show that our proposed STL-guided RL method
with online causation semantics outperforms existing relevant STL-guided RL
methods, providing a more robust and efficient reward generation framework for
deep-RL.

</details>


### [23] [oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning](https://arxiv.org/abs/2510.07731)
*Ruiling Xu,Yifan Zhang,Qingyun Wang,Carl Edwards,Heng Ji*

Main category: cs.AI

TL;DR: This paper introduces a benchmark (oMeBench) and scoring framework (oMeS) to evaluate the chemical reasoning capabilities of large language models (LLMs) in organic reaction mechanisms, finding promising but inconsistent performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to assess whether LLMs genuinely exhibit chemical reasoning capabilities, such as generating valid intermediates and maintaining logical coherence, which are essential for advancing in the field of organic chemistry.

Method: The researchers created oMeBench, a large-scale dataset of mechanistic steps annotated by experts, and introduced oMeS, a dynamic scoring framework combining chemical similarity and step-level logic for evaluating LLM performance.

Result: Current LLMs show some chemical intuition but have difficulty with consistent multi-step reasoning. Fine-tuning models on oMeBench improved their performance significantly, by 50% over the leading closed-source LLM.

Conclusion: oMeBench provides a rigorous basis for evaluating and improving AI systems in chemical reasoning, fostering advancements in organic chemistry simulations and molecule design.

Abstract: Organic reaction mechanisms are the stepwise elementary reactions by which
reactants form intermediates and products, and are fundamental to understanding
chemical reactivity and designing new molecules and reactions. Although large
language models (LLMs) have shown promise in understanding chemical tasks such
as synthesis design, it is unclear to what extent this reflects genuine
chemical reasoning capabilities, i.e., the ability to generate valid
intermediates, maintain chemical consistency, and follow logically coherent
multi-step pathways. We address this by introducing oMeBench, the first
large-scale, expert-curated benchmark for organic mechanism reasoning in
organic chemistry. It comprises over 10,000 annotated mechanistic steps with
intermediates, type labels, and difficulty ratings. Furthermore, to evaluate
LLM capability more precisely and enable fine-grained scoring, we propose oMeS,
a dynamic evaluation framework that combines step-level logic and chemical
similarity. We analyze the performance of state-of-the-art LLMs, and our
results show that although current models display promising chemical intuition,
they struggle with correct and consistent multi-step reasoning. Notably, we
find that using prompting strategy and fine-tuning a specialist model on our
proposed dataset increases performance by 50% over the leading closed-source
model. We hope that oMeBench will serve as a rigorous foundation for advancing
AI systems toward genuine chemical reasoning.

</details>


### [24] [SurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation](https://arxiv.org/abs/2510.07733)
*Minh-Anh Nguye,Minh-Duc Nguyen,Nguyen Thi Ha Lan,Kieu Hai Dang,Nguyen Tien Dong,Le Duy Dung*

Main category: cs.AI

TL;DR: SurveyG leverages a hierarchical citation graph to generate structured and comprehensive surveys using large language models, outperforming existing state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the shortcomings of existing automated survey generation methods that fail to incorporate the structural and contextual relationships among research papers, often resulting in incoherent taxonomies.

Method: SurveyG uses a hierarchical citation graph with layers—Foundation, Development, and Frontier—to integrate citation dependencies and semantic relationships. It combines horizontal and vertical searches for multi-level summarization and employs a multi-agent validation approach for accuracy.

Result: Experiments demonstrate that SurveyG produces surveys that are more structured, comprehensive, and aligned with field-specific taxonomies compared to existing methods, according to both human experts and LLM evaluations.

Conclusion: SurveyG effectively enhances the quality of automatic survey paper generation by providing a robust structure, contextual understanding, and higher accuracy in capturing research evolution, outperforming previous frameworks.

Abstract: Large language models (LLMs) are increasingly adopted for automating survey
paper generation \cite{wang2406autosurvey, liang2025surveyx,
yan2025surveyforge,su2025benchmarking,wen2025interactivesurvey}. Existing
approaches typically extract content from a large collection of related papers
and prompt LLMs to summarize them directly. However, such methods often
overlook the structural relationships among papers, resulting in generated
surveys that lack a coherent taxonomy and a deeper contextual understanding of
research progress. To address these shortcomings, we propose \textbf{SurveyG},
an LLM-based agent framework that integrates \textit{hierarchical citation
graph}, where nodes denote research papers and edges capture both citation
dependencies and semantic relatedness between their contents, thereby embedding
structural and contextual knowledge into the survey generation process. The
graph is organized into three layers: \textbf{Foundation},
\textbf{Development}, and \textbf{Frontier}, to capture the evolution of
research from seminal works to incremental advances and emerging directions. By
combining horizontal search within layers and vertical depth traversal across
layers, the agent produces multi-level summaries, which are consolidated into a
structured survey outline. A multi-agent validation stage then ensures
consistency, coverage, and factual accuracy in generating the final survey.
Experiments, including evaluations by human experts and LLM-as-a-judge,
demonstrate that SurveyG outperforms state-of-the-art frameworks, producing
surveys that are more comprehensive and better structured to the underlying
knowledge taxonomy of a field.

</details>


### [25] [Haibu Mathematical-Medical Intelligent Agent:Enhancing Large Language Model Reliability in Medical Tasks via Verifiable Reasoning Chains](https://arxiv.org/abs/2510.07748)
*Yilun Zhang,Dexing Kong*

Main category: cs.AI

TL;DR: This paper introduces MMIA, an architecture for applying LLMs reliably in high-stakes medical tasks, using formal reasoning and evidence verification.


<details>
  <summary>Details</summary>
Motivation: LLMs are prone to errors, rendering them unsuitable for high-stakes medical applications where reliability is crucial.

Method: MMIA recursively breaks complex tasks into atomic steps, audits reasoning chains for coherence and evidence traceability, and uses validated reasoning chains for efficient processing.

Result: MMIA achieved over 98% error detection accuracy with under 1% false positive rate, while reducing processing costs by 85%.

Conclusion: MMIA offers a verified reasoning framework that enhances LLM reliability and cost-efficiency, paving the way for trustworthy AI systems in medicine.

Abstract: Large Language Models (LLMs) show promise in medicine but are prone to
factual and logical errors, which is unacceptable in this high-stakes field. To
address this, we introduce the "Haibu Mathematical-Medical Intelligent Agent"
(MMIA), an LLM-driven architecture that ensures reliability through a formally
verifiable reasoning process. MMIA recursively breaks down complex medical
tasks into atomic, evidence-based steps. This entire reasoning chain is then
automatically audited for logical coherence and evidence traceability, similar
to theorem proving. A key innovation is MMIA's "bootstrapping" mode, which
stores validated reasoning chains as "theorems." Subsequent tasks can then be
efficiently solved using Retrieval-Augmented Generation (RAG), shifting from
costly first-principles reasoning to a low-cost verification model. We
validated MMIA across four healthcare administration domains, including DRG/DIP
audits and medical insurance adjudication, using expert-validated benchmarks.
Results showed MMIA achieved an error detection rate exceeding 98% with a false
positive rate below 1%, significantly outperforming baseline LLMs. Furthermore,
the RAG matching mode is projected to reduce average processing costs by
approximately 85% as the knowledge base matures. In conclusion, MMIA's
verifiable reasoning framework is a significant step toward creating
trustworthy, transparent, and cost-effective AI systems, making LLM technology
viable for critical applications in medicine.

</details>


### [26] [From Noisy to Native: LLM-driven Graph Restoration for Test-Time Graph Domain Adaptation](https://arxiv.org/abs/2510.07762)
*Xiangwei Lv,JinLuan Yang,Wang Lin,Jingyuan Chen,Beishui Liao*

Main category: cs.AI

TL;DR: The paper introduces GRAIL, a novel framework for test-time graph domain adaptation (TT-GDA), treating it as a generative graph restoration problem using large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Existing GDA methods rely on source-domain data, which is often inaccessible due to privacy or security concerns, necessitating alternative methods like TT-GDA.

Method: The GRAIL framework compresses node representations, uses graph diffusion for restoration, encodes features, and fine-tunes an LLM as a generative restorer. Reinforcement learning further refines the restoration quality with alignment and confidence rewards.

Result: Experiments on multiple datasets show the effectiveness of GRAIL in restoring target graphs to a source-domain-like state, demonstrating its efficiency.

Conclusion: GRAIL successfully reframes test-time graph domain adaptation as a generative graph restoration task and provides a practical workaround for source data unavailability.

Abstract: Graph domain adaptation (GDA) has achieved great attention due to its
effectiveness in addressing the domain shift between train and test data. A
significant bottleneck in existing graph domain adaptation methods is their
reliance on source-domain data, which is often unavailable due to privacy or
security concerns. This limitation has driven the development of Test-Time
Graph Domain Adaptation (TT-GDA), which aims to transfer knowledge without
accessing the source examples. Inspired by the generative power of large
language models (LLMs), we introduce a novel framework that reframes TT-GDA as
a generative graph restoration problem, "restoring the target graph to its
pristine, source-domain-like state". There are two key challenges: (1) We need
to construct a reasonable graph restoration process and design an effective
encoding scheme that an LLM can understand, bridging the modality gap. (2) We
need to devise a mechanism to ensure the restored graph acquires the intrinsic
features of the source domain, even without access to the source data. To
ensure the effectiveness of graph restoration, we propose GRAIL, that restores
the target graph into a state that is well-aligned with the source domain.
Specifically, we first compress the node representations into compact latent
features and then use a graph diffusion process to model the graph restoration
process. Then a quantization module encodes the restored features into discrete
tokens. Building on this, an LLM is fine-tuned as a generative restorer to
transform a "noisy" target graph into a "native" one. To further improve
restoration quality, we introduce a reinforcement learning process guided by
specialized alignment and confidence rewards. Extensive experiments demonstrate
the effectiveness of our approach across various datasets.

</details>


### [27] [An approach for systematic decomposition of complex llm tasks](https://arxiv.org/abs/2510.07772)
*Tianle Zhou,Jiakai Xu,Guanhong Liu,Jiaxiang Liu,Haonan Wang,Eugene Wu*

Main category: cs.AI

TL;DR: This paper introduces a systematic decomposition framework, ACONIC, to improve the performance of Large Language Models (LLMs) on complex tasks by leveraging formal complexity measures.


<details>
  <summary>Details</summary>
Motivation: Reliability issues in LLMs arise from heuristic and manual decomposition methods, necessitating a systematic approach to task decomposition.

Method: The authors propose modeling tasks as constraint problems and utilizing formal complexity metrics to guide their decomposition within a framework called ACONIC.

Result: Using ACONIC, LLMs demonstrated significantly improved performance (10-40 percentage point increase) on combinatorial (SATBench) and database querying tasks (Spider).

Conclusion: ACONIC provides an effective systematic approach to task decomposition, enhancing LLM reliability on complex tasks by utilizing formal measures of complexity.

Abstract: Large Language Models (LLMs) suffer from reliability issues on complex tasks,
as existing decomposition methods are heuristic and rely on agent or manual
decomposition. This work introduces a novel, systematic decomposition framework
that we call Analysis of CONstraint-Induced Complexity (ACONIC), which models
the task as a constraint problem and leveraging formal complexity measures to
guide decomposition. On combinatorial (SATBench) and LLM database querying
tasks (Spider), we find that by decomposing the tasks following the measure of
complexity, agent can perform considerably better (10-40 percentage point).

</details>


### [28] [GCPO: When Contrast Fails, Go Gold](https://arxiv.org/abs/2510.07790)
*Hao Wu,Wei Liu*

Main category: cs.AI

TL;DR: The paper introduces Group Contrastive Policy Optimization (GCPO), a method that boosts language model reasoning by incorporating external reference answers, achieving superior results compared to baselines.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of current reinforcement learning methods like GRPO, which struggle to utilize samples where model-generated answers are entirely correct or incorrect.

Method: Group Contrastive Policy Optimization (GCPO) incorporates external reference answers to guide the model during training, ensuring accurate update directions regardless of the model's initial response accuracy.

Result: GCPO demonstrates significant improvements in reasoning benchmarks, outperforming baseline models and improving training efficiency.

Conclusion: GCPO enhances generalization in reasoning by enabling models to emulate the strategy of external reference answers, making it an effective approach for advancing smaller language models' capabilities.

Abstract: Reinforcement learning has been widely applied to enhance the reasoning
capabilities of large language models. Extending the inference limits of
smaller models has become a prominent research focus. However, algorithms such
as Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the
upper bound of a model's rollout responses is entirely determined by the model
itself, preventing the acquisition of knowledge from samples that are either
all incorrect or all correct. In this paper, we introduce Group Contrastive
Policy Optimization (GCPO), a method that incorporates external standard
reference answers. When the model cannot solve a problem, the reference answer
supplies the correct response, steering the model toward an unequivocally
accurate update direction. This approach offers two main advantages: (1) it
improves training efficiency by fully utilizing every sample; (2) it enables
the model to emulate the problem solving strategy of the reference answer
during training, thereby enhancing generalization in reasoning. GCPO achieves
outstanding results across multiple benchmark datasets, yielding substantial
improvements over the baseline model. Our code is available at:
https://github.com/AchoWu/GCPO.

</details>


### [29] [Strategic Communication under Threat: Learning Information Trade-offs in Pursuit-Evasion Games](https://arxiv.org/abs/2510.07813)
*Valerio La Gatta,Dolev Mutzari,Sarit Kraus,VS Subrahmanian*

Main category: cs.AI

TL;DR: This paper introduces SHADOW, a reinforcement learning framework for agents in adversarial environments balancing communication benefits against exposure risks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of effective decision-making for agents in adversarial environments where acquiring situational information comes at the cost of revealing one's own position.

Method: The authors develop SHADOW, a multi-headed sequential reinforcement learning model that combines continuous navigation, discrete communication decisions, and opponent modeling.

Result: SHADOW achieves better success rates than six baseline methods, with evaluations highlighting the importance of temporal and opponent modeling.

Conclusion: SHADOW's policies show strong generalization across diverse conditions, confirming its efficiency in handling communication-risk trade-offs in adversarial settings.

Abstract: Adversarial environments require agents to navigate a key strategic
trade-off: acquiring information enhances situational awareness, but may
simultaneously expose them to threats. To investigate this tension, we
formulate a PursuitEvasion-Exposure-Concealment Game (PEEC) in which a pursuer
agent must decide when to communicate in order to obtain the evader's position.
Each communication reveals the pursuer's location, increasing the risk of being
targeted. Both agents learn their movement policies via reinforcement learning,
while the pursuer additionally learns a communication policy that balances
observability and risk. We propose SHADOW (Strategic-communication Hybrid
Action Decision-making under partial Observation for Warfare), a multi-headed
sequential reinforcement learning framework that integrates continuous
navigation control, discrete communication actions, and opponent modeling for
behavior prediction. Empirical evaluations show that SHADOW pursuers achieve
higher success rates than six competitive baselines. Our ablation study
confirms that temporal sequence modeling and opponent modeling are critical for
effective decision-making. Finally, our sensitivity analysis reveals that the
learned policies generalize well across varying communication risks and
physical asymmetries between agents.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [30] [How long can you sleep? Idle Time System Inefficiencies and Opportunities](https://arxiv.org/abs/2510.07449)
*Georgia Antoniou,Haris Volos,Jawad Haj Yahya,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: The paper outlines a framework using theoretical queuing models to uncover idle time inefficiencies on servers running latency-critical applications and proposes improvements for deep idle state transitions.


<details>
  <summary>Details</summary>
Motivation: Modern servers running latency-critical applications experience inefficiencies in exploiting idle opportunities. The study aims to address missed chances to enhance server energy efficiency and deep idle state utilization.

Method: Three queuing models—M/M/1, cxM/M/1, and M/M/c—are applied to estimate CPU core and system-level idle time distributions. Real server idleness is compared against theoretical predictions to identify mismatches and causes of inefficiency.

Result: The comparison highlights substantial missed idle opportunities due to idle-governor inaccuracies and high transition latency. The framework provides insights for optimizing server configurations and idle state management.

Conclusion: This framework enables early-stage design exploration and offers valuable insights for improving server idle behavior and energy efficiency in latency-critical applications.

Abstract: This work introduces a model-based framework that reveals the idle
opportunity of modern servers running latency-critical applications.
Specifically, three queuing models, M/M/1, cxM/M/1, and M/M/c, are used to
estimate the theoretical idle time distribution at the CPU core and system
(package) level. A comparison of the actual idleness of a real server and that
from the theoretical models reveals significant missed opportunities to enter
deep idle states. This inefficiency is attributed to the idle-governor
inaccuracy and the high latency to transition to/from legacy deep-idle states.
The proposed methodology offers the means for an early-stage design exploration
and insights into idle time behavior and opportunities for varying server
system configurations and load.

</details>


### [31] [DL-PIM: Improving Data Locality in Processing-in-Memory Systems](https://arxiv.org/abs/2510.07719)
*Parker Hao Tian,Zahra Yousefijamarani,Alaa Alameldeen*

Main category: cs.AR

TL;DR: DL-PIM is a novel PIM architecture that addresses data movement overheads and enhances data locality in 3D stacked memories (HMC and HBM), reducing memory latency and improving overall system performance.


<details>
  <summary>Details</summary>
Motivation: To address the performance and energy efficiency challenges in PIM architectures caused by data movement overheads and the lack of data locality.

Method: This paper introduces DL-PIM, an architecture that uses dynamic data movement detection and a distributed address-indirection table to proactively relocate data to local memory areas close to the processing unit. An adaptive mechanism enables or disables indirection to optimize performance across varying workloads.

Result: DL-PIM reduced average memory latency by 54% in HMC and 50% in HBM, achieving 15% and 5% performance improvements for workloads with data reuse in HMC and HBM, respectively. Overall, it delivered a 6% speedup in HMC and 3% speedup in HBM across all workloads.

Conclusion: DL-PIM successfully enhances data locality by mitigating data movement overheads, boosting the performance of workloads with data reuse while maintaining efficiency for a broad range of tasks.

Abstract: PIM architectures aim to reduce data transfer costs between processors and
memory by integrating processing units within memory layers. Prior PIM
architectures have shown potential to improve energy efficiency and
performance. However, such advantages rely on data proximity to the processing
units performing computations. Data movement overheads can degrade PIM's
performance and energy efficiency due to the need to move data between a
processing unit and a distant memory location. %they face challenges due to the
overhead of transferring data from remote memory locations to processing units
inside memory for computation. In this paper, we demonstrate that a large
fraction of PIM's latency per memory request is attributed to data transfers
and queuing delays from remote memory accesses. To improve PIM's data locality,
we propose DL-PIM, a novel architecture that dynamically detects the overhead
of data movement, and proactively moves data to a reserved area in the local
memory of the requesting processing unit. DL-PIM uses a distributed
address-indirection hardware lookup table to redirect traffic to the current
data location. We propose DL-PIM implementations on two 3D stacked memories:
HMC and HBM. While some workloads benefit from DL-PIM, others are negatively
impacted by the additional latency due to indirection accesses. Therefore, we
propose an adaptive mechanism that assesses the cost and benefit of indirection
and dynamically enables or disables it to prevent degrading workloads that
suffer from indirection. Overall, DL-PIM reduces the average memory latency per
request by 54% in HMC and 50% in HBM which resulted in performance improvement
of 15% for workloads with substantial data reuse in HMC and 5% in HBM. For all
representative workloads, DL-PIM achieved a 6% speedup in HMC and a 3% speedup
in HBM, showing that DL-PIM enhances data locality and overall system
performance.

</details>


### [32] [A Scalable FPGA Architecture With Adaptive Memory Utilization for GEMM-Based Operations](https://arxiv.org/abs/2510.08137)
*Anastasios Petropoulos,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: This study introduces an FPGA-based accelerator for DNN inference with dynamic configurability and high efficiency, including novel extensions for analog in-memory computing emulation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the growing computational demands of DNN inference, requiring efficient hardware solutions capable of adapting to diverse models and supporting next-generation analog in-memory computing designs.

Method: The method involves designing an FPGA-based accelerator using systolic arrays, memory optimizations, and multiple processing unit configurations. It employs heuristic weight transfer scheduling and explores emulating analog in-memory computing behaviors.

Result: The proposed architecture demonstrates high throughput efficiency compared to prior solutions and enables extensions for heterogeneous DNN model support and noise investigation in AIMC devices.

Conclusion: The work presents a versatile FPGA-based architecture for efficient DNN inference, adaptable to current and potential future computing paradigms, including AIMC designs.

Abstract: Deep neural network (DNN) inference relies increasingly on specialized
hardware for high computational efficiency. This work introduces a
field-programmable gate array (FPGA)-based dynamically configurable accelerator
featuring systolic arrays, high-bandwidth memory, and UltraRAMs. We present two
processing unit (PU) configurations with different computing capabilities using
the same interfaces and peripheral blocks. By instantiating multiple PUs and
employing a heuristic weight transfer schedule, the architecture achieves
notable throughput efficiency over prior works. Moreover, we outline how the
architecture can be extended to emulate analog in-memory computing (AIMC)
devices to aid next-generation heterogeneous AIMC chip designs and investigate
device-level noise behavior. Overall, this brief presents a versatile DNN
inference acceleration architecture adaptable to various models and future FPGA
designs.

</details>


### [33] [FMCache: File-System Metadata Caching in Programmable Switches](https://arxiv.org/abs/2510.08351)
*Qingxiu Liu,Jiazhen Cai,Siyuan Sheng,Yuhui Chen,Lu Tang,Zhirong Shen,Patrick P. C. Lee*

Main category: cs.AR

TL;DR: FMCache is an in-switch metadata caching framework designed for distributed file systems to improve performance and reduce server load.


<details>
  <summary>Details</summary>
Motivation: Distributed file systems often face challenges in handling loads due to numerous files and directories, exacerbated by client-side cache consistency issues.

Method: FMCache leverages programmable switches to serve metadata requests directly in the switch data plane, addressing file-system-specific path dependencies.

Result: FMCache improves throughput by up to 181.6% compared to standard HDFS and achieves additional gains of up to 139.6% in complementing client-side caching, with low latency and limited switch resource use.

Conclusion: The proposed framework significantly enhances metadata management in distributed file systems, reducing server loads and achieving scalability effectively.

Abstract: Fast and scalable metadata management across multiple metadata servers is
crucial for distributed file systems to handle numerous files and directories.
Client-side caching of frequently accessed metadata can mitigate server loads,
but incurs significant overhead and complexity in maintaining cache consistency
when the number of clients increases. We propose FMCache, an in-switch
file-system metadata caching framework that leverages programmable switches to
serve file-system metadata requests from multiple clients directly in the
switch data plane. Unlike prior in-switch key-value caching approaches, FMCache
addresses file-system-specific path dependencies under stringent switch
resource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on
a Tofino-switch testbed using real-world file-system metadata workloads.
FMCache achieves up to 181.6% higher throughput than vanilla HDFS and
complements client-side caching with additional throughput gains of up to
139.6%. It also incurs low latencies and limited switch resource usage.

</details>


### [34] [SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM Inference](https://arxiv.org/abs/2510.08544)
*Hengrui Zhang,Pratyush Patel,August Ning,David Wentzlaff*

Main category: cs.AR

TL;DR: This paper introduces SPAD, a specialized chip design for efficient inference in Large Language Models, reducing costs and power consumption compared to traditional GPUs/TPUs.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in GPU/TPU hardware utilization during LLM inference phases, which lead to increased serving costs.

Method: Design specialized Prefill Chips with larger systolic arrays and cost-effective GDDR memory and Decode Chips with reduced compute capacity but high memory bandwidth.

Result: Simulations reveal that Prefill Chips improve performance by 8% at 52% lower cost, and Decode Chips maintain 97% performance at 28% lower TDP. End-to-end testing shows SPAD reduces hardware costs by 19%-41% and TDP by 2%-17%.

Conclusion: SPAD achieves significant cost and energy reductions while maintaining performance, and its flexible design ensures adaptability to shifting models and workloads.

Abstract: Large Language Models (LLMs) have gained popularity in recent years, driving
up the demand for inference. LLM inference is composed of two phases with
distinct characteristics: a compute-bound prefill phase followed by a
memory-bound decode phase. To efficiently serve LLMs, prior work proposes
prefill-decode disaggregation to run each phase on separate hardware. However,
existing hardware poorly matches the different requirements of each phase.
Current datacenter GPUs and TPUs follow a more-is-better design philosophy that
maximizes compute and memory resources, causing memory bandwidth
underutilization in the prefill phase and compute underutilization in the
decode phase. Such underutilization directly translates into increased serving
costs.
  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting
a less-is-more methodology to design specialized chips tailored to the distinct
characteristics of prefill and decode phases. The proposed Prefill Chips have
larger systolic arrays and use cost-effective GDDR memory, whereas the proposed
Decode Chips retain high memory bandwidth but reduce compute capacity. Compared
to modeled H100s, simulations show that the proposed Prefill Chips deliver 8%
higher prefill performance on average at 52% lower hardware cost, while the
proposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.
  End-to-end simulations on production traces show that SPAD reduces hardware
cost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while
offering the same performance. Even when models and workloads change, SPAD can
reallocate either type of chip to run either phase and still achieve 11%-43%
lower hardware costs, demonstrating the longevity of the SPAD design.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [35] [Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments](https://arxiv.org/abs/2510.07359)
*Jingfei Huang,Han Tu*

Main category: cs.CL

TL;DR: The study examines sentiment inconsistencies in urban settings using visual and social media data from Beijing, revealing mismatches in human perception and opinion sentiment.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand the variations in sentiment reaction within urban environments and address challenges in sentiment analysis approaches in urban studies.

Method: A dataset of street view images and social media posts is analyzed using object detection and NLP techniques, creating sentiment indexes and maps for Beijing across 2016 and 2022.

Result: The research finds shifts in sentiment trends, with perception becoming more balanced while opinion changes remain extreme. Dense buildings and pedestrian presence greatly influence sentiment.

Conclusion: The study highlights significant sentiment mismatches and their implications, offering insights for urban renewal and environmental management strategies.

Abstract: The ascension of social media platforms has transformed our understanding of
urban environments, giving rise to nuanced variations in sentiment reaction
embedded within human perception and opinion, and challenging existing
multidimensional sentiment analysis approaches in urban studies. This study
presents novel methodologies for identifying and elucidating sentiment
inconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent
Street view images to measure perceptions, and 984,024 Weibo social media text
posts to measure opinions. A reaction index is developed, integrating object
detection and natural language processing techniques to classify sentiment in
Beijing Second Ring for 2016 and 2022. Classified sentiment reaction is
analysed and visualized using regression analysis, image segmentation, and word
frequency based on land-use distribution to discern underlying factors. The
perception affective reaction trend map reveals a shift toward more evenly
distributed positive sentiment, while the opinion affective reaction trend map
shows more extreme changes. Our mismatch map indicates significant disparities
between the sentiments of human perception and opinion of urban areas over the
years. Changes in sentiment reactions have significant relationships with
elements such as dense buildings and pedestrian presence. Our inconsistent maps
present perception and opinion sentiments before and after the pandemic and
offer potential explanations and directions for environmental management, in
formulating strategies for urban renewal.

</details>


### [36] [Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation](https://arxiv.org/abs/2510.07414)
*Mufei Li,Dongqi Fu,Limei Wang,Si Zhang,Hanqing Zeng,Kaan Sancak,Ruizhong Qiu,Haoyu Wang,Xiaoxin He,Xavier Bresson,Yinglong Xia,Chonglin Sun,Pan Li*

Main category: cs.CL

TL;DR: Modern long-context LLMs perform well on synthetic benchmarks but struggle with noisy contexts caused by real-world factors. This paper introduces HaystackCraft, a benchmark for testing model robustness in heterogeneous and dynamic retrieval scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the gap between synthetic benchmarks and real-world scenarios in evaluating the robustness of long-context LLMs, where retrieval noise and cascading errors play a critical role.

Method: HaystackCraft leverages the English Wikipedia's hyperlink network to simulate multi-hop questions with varied retrieval strategies, testing models in static and agentic settings with dynamic query refinement and reasoning.

Result: Experiments with 15 models show challenges in retrieval effectiveness, distractor mitigation, and agentic reasoning, particularly in handling self-generated noise and deciding when to stop.

Conclusion: HaystackCraft provides a robust framework for evaluating long-context reasoning and highlights persistent challenges that need addressing for future advancements in LLM robustness.

Abstract: Modern long-context large language models (LLMs) perform well on synthetic
"needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy
contexts arise from biased retrieval and agentic workflows. We argue that
haystack engineering is necessary to construct noisy long contexts that
faithfully capture key real-world factors -- distraction from heterogeneous
biased retrievers and cascading errors in agentic workflows -- to test models'
long-context robustness. We instantiate it through HaystackCraft, a new NIAH
benchmark built on the full English Wikipedia hyperlink network with multi-hop
questions. HaystackCraft evaluates how heterogeneous retrieval strategies
(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,
haystack ordering, and downstream LLM performance. HaystackCraft further
extends NIAH to dynamic, LLM-dependent settings that simulate agentic
operations, where models refine queries, reflect on their past reasonings, and
decide when to stop. Experiments with 15 long-context models show that (1)
while stronger dense retrievers can introduce more challenging distractors,
graph-based reranking simultaneously improves retrieval effectiveness and
mitigates more harmful distractors; (2) in agentic tests, even advanced models
like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated
distractors or struggle to perform early stops. These results highlight
persistent challenges in agentic long-context reasoning and establish
HaystackCraft as a valuable testbed for future progress.

</details>


### [37] [Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data](https://arxiv.org/abs/2510.07434)
*Olia Toporkov,Alan Akbik,Rodrigo Agerri*

Main category: cs.CL

TL;DR: This paper investigates the capability of large language models (LLMs) for in-context lemmatization across 12 languages.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of LLMs in contextual lemmatization, a task with no prior exploration and often limited supervised data for many languages.

Method: Compared fine-tuned supervised approaches (out-of-domain and cross-lingual) to in-context lemma generation using LLMs with a few examples.

Result: LLMs demonstrated state-of-the-art performance in lemmatization for most languages without fine-tuning, outperforming traditional methods in some scenarios.

Conclusion: LLMs are highly effective for in-context lemmatization across diverse languages, especially in scenarios with limited supervised data.

Abstract: Lemmatization is the task of transforming all words in a given text to their
dictionary forms. While large language models (LLMs) have demonstrated their
ability to achieve competitive results across a wide range of NLP tasks, there
is no prior evidence of how effective they are in the contextual lemmatization
task. In this paper, we empirically investigate the capacity of the latest
generation of LLMs to perform in-context lemmatization, comparing it to the
traditional fully supervised approach. In particular, we consider the setting
in which supervised training data is not available for a target domain or
language, comparing (i) encoder-only supervised approaches, fine-tuned
out-of-domain, and (ii) cross-lingual methods, against direct in-context lemma
generation with LLMs. Our experimental investigation across 12 languages of
different morphological complexity finds that, while encoders remain
competitive in out-of-domain settings when fine-tuned on gold data, current
LLMs reach state-of-the-art results for most languages by directly generating
lemmas in-context without prior fine-tuning, provided just with a few examples.
Data and code available upon publication:
https://github.com/oltoporkov/lemma-dilemma

</details>


### [38] [LASER: An LLM-based ASR Scoring and Evaluation Rubric](https://arxiv.org/abs/2510.07437)
*Amruta Parulekar,Preethi Jyothi*

Main category: cs.CL

TL;DR: The paper proposes LASER, a novel scoring system utilizing state-of-the-art LLMs for more nuanced evaluations of ASR outputs, achieving high correlation with human scores.


<details>
  <summary>Details</summary>
Motivation: Standard metrics like Word Error Rate fail to accommodate semantic subtleties in ASR outputs, prompting the need for a more precise evaluation framework.

Method: The LASER rubric uses in-context learning abilities of advanced LLMs and incorporates finetuning smaller LLMs on word-pair examples.

Result: Hindi LASER scores achieved a 94% correlation with human annotations, effectively analyzing errors across multiple Indian languages. Smaller LLM models showed 89% accuracy in penalty prediction after finetuning.

Conclusion: LLM-based LASER scoring provides a more semantically informed evaluation of ASR outputs, outperforming traditional metrics on correlation and adaptability across languages.

Abstract: Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly
penalize morphological and syntactic nuances that do not significantly alter
sentence semantics. We introduce an LLM-based scoring rubric LASER that
leverages state-of-the-art LLMs' in-context learning abilities to learn from
prompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro
achieved a very high correlation score of 94% with human annotations. Hindi
examples in the prompt were also effective in analyzing errors in other Indian
languages such as Marathi, Kannada and Malayalam. We also demonstrate how a
smaller LLM like Llama 3 can be finetuned on word-pair examples derived from
reference and ASR predictions to predict what kind of penalty should be applied
with close to 89% accuracy.

</details>


### [39] [Meaningful Pose-Based Sign Language Evaluation](https://arxiv.org/abs/2510.07453)
*Zifan Jiang,Colin Leong,Amit Moryossef,Anne Göhring,Annette Rios,Oliver Cory,Maksym Ivashechkin,Neha Tarigopula,Biao Zhang,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: The paper studies methods for evaluating sign language utterances via human skeletal poses and provides a toolkit for reproducible research.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in assessing sign language translation or generation systems in meaningful ways.

Method: The study explores three evaluation metrics—keypoint distance-based, embedding-based, and back-translation-based—and conducts meta-evaluations and correlation studies.

Result: Findings show tradeoffs between metrics under different scenarios and release an open-source toolkit for pose evaluation.

Conclusion: The paper offers practical and reproducible insights for the development and evaluation of sign language systems leveraging skeletal pose analysis.

Abstract: We present a comprehensive study on meaningfully evaluating sign language
utterances in the form of human skeletal poses. The study covers keypoint
distance-based, embedding-based, and back-translation-based metrics. We show
tradeoffs between different metrics in different scenarios through automatic
meta-evaluation of sign-level retrieval and a human correlation study of
text-to-pose translation across different sign languages. Our findings and the
open-source pose-evaluation toolkit provide a practical and reproducible way of
developing and evaluating sign language translation or generation systems.

</details>


### [40] [Populism Meets AI: Advancing Populism Research with LLMs](https://arxiv.org/abs/2510.07458)
*Eduardo Ryô Tamaki,Yujin J. Jung,Julia Chatterley,Grant Mitchell,Semir Dzebo,Cristóbal Sandoval,Levente Littvay,Kirk A. Hawkins*

Main category: cs.CL

TL;DR: The paper introduces a rubric-guided chain of thought (CoT) prompting approach for Large Language Models (LLMs) to measure populism in political speeches, achieving human-level classification accuracy.


<details>
  <summary>Details</summary>
Motivation: The aim is to overcome the limitations of traditional textual analysis methods, which are costly and hard to scale for measuring the ideational content of populism across languages, contexts, and large datasets.

Method: The authors use a rubric and anchor-guided CoT prompting approach on LLMs, training them with data adapted from the Global Populism Database (GPD), a comprehensive dataset of speeches annotated for populism. Multiple models are tested for their performance in replicating GPD scores.

Result: The proposed strategy enables LLMs to classify the degree of populism in speeches with accuracy comparable to expert human coders, demonstrating its ability to handle nuanced, context-sensitive classification tasks.

Conclusion: The approach highlights the utility of domain-specific prompting in LLMs for complex tasks like classifying ideational content, offering a scalable and effective alternative to traditional analysis methods.

Abstract: Measuring the ideational content of populism remains a challenge. Traditional
strategies based on textual analysis have been critical for building the
field's foundations and providing a valid, objective indicator of populist
framing. Yet these approaches are costly, time consuming, and difficult to
scale across languages, contexts, and large corpora. Here we present the
results from a rubric and anchor guided chain of thought (CoT) prompting
approach that mirrors human coder training. By leveraging the Global Populism
Database (GPD), a comprehensive dataset of global leaders' speeches annotated
for degrees of populism, we replicate the process used to train human coders by
prompting the LLM with an adapted version of the same documentation to guide
the model's reasoning. We then test multiple proprietary and open weight models
by replicating scores in the GPD. Our findings reveal that this domain specific
prompting strategy enables the LLM to achieve classification accuracy on par
with expert human coders, demonstrating its ability to navigate the nuanced,
context sensitive aspects of populism.

</details>


### [41] [MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference](https://arxiv.org/abs/2510.07475)
*Zheyuan Zhang,Lin Ge,Hongjiang Li,Weicheng Zhu,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: The paper introduces MAPRO, a method for optimizing prompts in multi-agent systems (MAS) to improve coordination and performance based on a MAP inference framework.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in designing effective multi-agent systems (MAS), such as instability, compounded sensitivity, and the lack of systematic methods for optimizing prompts.

Method: MAPRO, a four-stage framework, formulates MAS prompt optimization as a Maximum a Posteriori inference problem and uses a language-guided max-product belief propagation algorithm with topology-aware refinement for iterative updates.

Result: MAPRO outperformed manually engineered baselines and automated alternatives across various task benchmarks, achieving state-of-the-art performance.

Conclusion: The MAPRO framework not only improves performance but also provides a principled approach to designing and optimizing multi-agent systems, paving the way for more reliable MAS in the future.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, and LLM-based agents further extend these abilities to various
practical workflows. While recent progress shows that multi-agent systems (MAS)
can outperform single agents by coordinating specialized roles, designing
effective MAS remains difficult due to prompt sensitivity and the compounded
instability MAS creates. To cope with the challenge, recent efforts in
automated prompt design have reduced manual effort. However, multi-agent prompt
optimization remains largely unexplored. Challenges like exponentially
expanding search space and ambiguous credit assignment together make systematic
design intractable without principled methods. Therefore, we introduce
M}ulti-Agent PRompt Optimization (MAPRO), a four-stage framework that first
formulates MAS prompt optimization as a Maximum a Posteriori (MAP) inference
problem and solves it using a language-guided variant of max-product belief
propagation algorithm. To address credit assignment and updates the system
iteratively, MAPRO employs a topology-aware refinement mechanism that
integrates execution feedback and downstream blames to selectively update agent
prompts. Through this process, MAPRO progressively converges to a coordinated
set of agent-specific prompt policies. Across benchmarks in various tasks,
MAPRO achieves state-of-the-art performance, consistently surpassing manually
engineered baselines and recent automated alternatives. Beyond performance, our
MAP-based formulation also delivers general guidelines for building more
reliable and principled multi-agent systems in the future

</details>


### [42] [AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding](https://arxiv.org/abs/2510.07486)
*Shuqing Luo,Yilin Guan,Pingzhi Li,Hanrui Wang,Tianlong Chen*

Main category: cs.CL

TL;DR: The paper introduces AsyncSpade, a framework that improves the efficiency of test-time scaling (TTS) for large language models (LLMs) by removing sequential dependency without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the memory-bound bottleneck and inefficiencies in LLM decoding during TTS tasks, particularly in high-concurrency and long-chain-of-thought (CoT) scenarios.

Method: The authors developed AsyncSpade, an asynchronous framework with two features: (1) a lightweight temporal-regressive module for predicting next-token query states; and (2) an asynchronous setup that separates KV-cache filtering from decoding, enabling simultaneous operations.

Result: AsyncSpade achieves over 20% reduction in time-per-output-token (TPOT) compared to the prior state-of-the-art (Quest) and at least 50% reduction compared to full attention models, while maintaining or improving accuracy across multiple TTS benchmarks.

Conclusion: AsyncSpade resolves the bottlenecks in TTS for LLMs, boosts efficiency with asynchronous KV-cache operations, and sets new benchmarks in both speed and accuracy.

Abstract: Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),
but the linear KV-cache growth amplifies the memory-bound bottleneck of LLM
decoding. Query-aware page-level sparse decoding can achieve state-of-the-art
performance under constrained FLOPs budgets, but is limited by both
sequential-dependent page filtering and coarse-grained token selection,
hampering serving efficiency and model performance on TTS tasks under high
concurrency and long CoT scenarios (consuming even higher runtime than the
forward pipeline itself). In this paper, we first find that the current-step
query state can be accurately approximated in a unified manner from a short
window of recent queries, enabling training-free query-aware sparsity without
waiting in the decoding loop. We propose AsyncSpade, an asynchronous framework
for efficient TTS built on two core components: (1) a novel light-weight
temporal-regressive module that predicts the next-token query state; (2) an
asynchronous and disaggregated framework that decouples the KV cache filtering
from the auto-regressive decoding loop, overlapping the token-level KV
selection with the forward inference computation through asynchronism. To our
knowledge, AsyncSpade is the first to eliminate the sequential dependence
without sacrificing model performance. We validate the effectiveness of
AsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade
fully overlaps KV-cache operations with the inference pipeline, achieving
theoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade
delivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and
at least 50% TPOT reduction compared to full attention on Qwen3-8B and
Qwen3-32B models, while matching or surpassing their accuracy on various TTS
benchmarks (AIME-24/25, GPQA-Diamond, MATH-500).

</details>


### [43] [Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics](https://arxiv.org/abs/2510.07488)
*Rasika Muralidharan,Jaewoon Kwak,Jisun An*

Main category: cs.CL

TL;DR: This paper explores the dynamics of multi-agent systems powered by Large Language Models, assessing team structure, diversity, and interaction dynamics across commonsense and social reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To investigate how team dynamics in MAS with LLM-powered agents affect their performance, inspired by human team science.

Method: A multi-agent framework was designed and evaluated on four tasks related to commonsense and social reasoning. Interviews captured agents' reflections on teamwork challenges.

Result: Flat team structures outperformed hierarchical ones, and diversity had an intricate effect. Interviews revealed overconfidence in team performance and challenges in conversational integration.

Conclusion: Flat team structures may enhance MAS performance, but integration challenges persist. Effective conversational coordination is critical for optimizing team collaboration.

Abstract: Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are
gaining attention, yet fewer studies explore their team dynamics. Inspired by
human team science, we propose a multi-agent framework to examine core aspects
of team science: structure, diversity, and interaction dynamics. We evaluate
team performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and
Latent Implicit Hate, spanning commonsense and social reasoning. Our results
show that flat teams tend to perform better than hierarchical ones, while
diversity has a nuanced impact. Interviews suggest agents are overconfident
about their team performance, yet post-task reflections reveal both
appreciation for collaboration and challenges in integration, including limited
conversational coordination.

</details>


### [44] [Can Speech LLMs Think while Listening?](https://arxiv.org/abs/2510.07497)
*Yi-Jen Shih,Desh Raj,Chunyang Wu,Wei Zhou,SK Bong,Yashesh Gaur,Jay Mahadeokar,Ozlem Kalinli,Mike Seltzer*

Main category: cs.CL

TL;DR: The paper explores improving reasoning in speech-based large language models (speech LLMs) through chain-of-thought (CoT) fine-tuning and proposes methods for reducing response latency through advanced techniques.


<details>
  <summary>Details</summary>
Motivation: Speech-based LLMs excel in seamless spoken interactions but struggle with complex reasoning tasks, prompting the need to enhance their reasoning abilities while managing latency.

Method: Authors employ CoT fine-tuning to enhance reasoning and introduce a novel entropy-based metric, "question completeness," to optimize the timing of reasoning initiation, alongside Direct Preference Optimization (DPO) to balance accuracy and latency.

Result: CoT fine-tuning significantly improves speech LLM reasoning accuracy by 2.4x, while the "question completeness" metric combined with preference optimization reduces latency by 70% without sacrificing accuracy.

Conclusion: The proposed methods advance the accuracy and latency trade-off in speech LLMs, creating more efficient systems for spoken reasoning tasks.

Abstract: Recent advances in speech large language models (speech LLMs) have enabled
seamless spoken interactions, but these systems still struggle with complex
reasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning
has been to shown to significantly improve the reasoning abilities of
text-based LLMs. In this work, we investigate the effect of CoT fine-tuning for
multi-stream speech LLMs, demonstrating that reasoning in text space improves
the accuracy of speech LLMs by 2.4x, on average, over a suite of spoken
reasoning tasks. Beyond accuracy, the latency of the spoken response is a
crucial factor for interacting with voice-based agents. Inspired by the human
behavior of "thinking while listening," we propose methods to reduce the
additional latency from reasoning by allowing the model to start reasoning
before the user query has ended. To achieve this, we introduce an entropy-based
metric, "question completeness," which acts as an indicator to guide the model
on the optimal time to start reasoning. This method provides greater control
over the accuracy-latency trade-off compared with heuristic-based approaches
and, under equivalent latency conditions, yields a 4% accuracy gain on
ARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference
data created using rejection sampling to push the accuracy-latency pareto
frontier further, resulting in a 70% reduction in latency without loss in
accuracy.

</details>


### [45] [When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs](https://arxiv.org/abs/2510.07499)
*Soyeong Jeong,Taehee Jung,Sung Ju Hwang,Joo-Kyung Kim,Dongyeop Kang*

Main category: cs.CL

TL;DR: The paper introduces Thought Template Augmented LCLMs (ToTAL) to enhance reasoning by structuring evidence integration in long-context language models.


<details>
  <summary>Details</summary>
Motivation: Long-Context Language Models offer opportunities for multi-hop reasoning across extensive sets of documents but struggle to effectively connect evidence.

Method: The authors propose using thought templates as reusable structures and rely on iterative refinement via natural language feedback to optimize reasoning efficiency.

Result: Their approach consistently outperforms strong baselines across multiple benchmarks and enables reasoning transfer into smaller models.

Conclusion: Thought templates improve reasoning processes in LCLMs, show broad applicability, and enhance transparent reasoning in both retrieval-based and retrieval-free contexts.

Abstract: Recent Long-Context Language Models (LCLMs) can process hundreds of thousands
of tokens in a single prompt, enabling new opportunities for
knowledge-intensive multi-hop reasoning by integrating large sets of retrieved
documents or, in some cases, directly all necessary information. However,
simply feeding more documents into the context window fails to capture how
evidence should be connected. We address this gap with thought templates, which
recast reasoning as reusable thought caches, derived from prior problem solving
traces, structuring how evidence is combined and guiding multi-hop inference
with factual documents. To keep these templates effective, we propose an update
strategy that iteratively refines templates derived from training data through
natural-language feedback. Across diverse benchmarks and LCLM families, our
approach delivers consistent gains over strong baselines in both
retrieval-based and retrieval-free settings. Furthermore, we show that
optimized templates can be distilled into smaller open-source models,
demonstrating its broad applicability and transparent reasoning reuse. We refer
to our framework as Thought Template Augmented LCLMs (ToTAL).

</details>


### [46] [ParsTranslit: Truly Versatile Tajik-Farsi Transliteration](https://arxiv.org/abs/2510.07520)
*Rayyan Merchant,Kevin Tang*

Main category: cs.CL

TL;DR: The paper introduces a new state-of-the-art sequence-to-sequence model for transliterating between Perso-Arabic and Tajik-Cyrillic scripts, supported by diverse datasets and achieving high performance metrics.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the transliteration challenge between Perso-Arabic and Tajik-Cyrillic scripts, aiming to bridge communication barriers among Persian-speaking communities due to script differences.

Method: The authors developed a sequence-to-sequence model trained on all available datasets and two newly curated datasets to enhance transliteration across varied domains.

Result: The model achieves superior chrF++ and Normalized CER scores: 87.91 and 0.05 for Farsi-to-Tajik, and 92.28 and 0.04 for Tajik-to-Farsi.

Conclusion: This work sets new benchmarks, improves understanding of domain variability in transliteration tasks, and makes the model, datasets, and code publicly available for broader applications.

Abstract: As a digraphic language, the Persian language utilizes two written standards:
Perso-Arabic in Afghanistan and Iran, and Tajik-Cyrillic in Tajikistan. Despite
the significant similarity between the dialects of each country, script
differences prevent simple one-to-one mapping, hindering written communication
and interaction between Tajikistan and its Persian-speaking ``siblings''. To
overcome this, previously-published efforts have investigated machine
transliteration models to convert between the two scripts. Unfortunately, most
efforts did not use datasets other than those they created, limiting these
models to certain domains of text such as archaic poetry or word lists. A truly
usable transliteration system must be capable of handling varied domains,
meaning that suck models lack the versatility required for real-world usage.
The contrast in domain between data also obscures the task's true difficulty.
We present a new state-of-the-art sequence-to-sequence model for Tajik-Farsi
transliteration trained across all available datasets, and present two datasets
of our own. Our results across domains provide clearer understanding of the
task, and set comprehensive comparable leading benchmarks. Overall, our model
achieves chrF++ and Normalized CER scores of 87.91 and 0.05 from Farsi to Tajik
and 92.28 and 0.04 from Tajik to Farsi. Our model, data, and code are available
at https://anonymous.4open.science/r/ParsTranslit-FB30/.

</details>


### [47] [OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs](https://arxiv.org/abs/2510.07535)
*Jaeseong Lee,seung-won hwang,Aurick Qiao,Gabriele Oliaro,Ye Wang,Samyam Rajbhandari*

Main category: cs.CL

TL;DR: This paper introduces OWL, a novel speculative decoding model for large language models, designed to handle long-context inputs better than existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the performance degradation of current speculative decoding methods in real-world scenarios with long-context workloads.

Method: The authors present OWL, featuring an LSTM-based drafter, a [SPEC] token for richer verifier representation, and a hybrid decoding algorithm, supported by the new LongSpecBench benchmark.

Result: OWL achieves a 5x higher acceptance length than EAGLE3 on long-context inputs, addressing existing limitations.

Conclusion: OWL offers a significant improvement in speculative decoding for long contexts, pushing forward the capabilities of large language models in practical applications.

Abstract: Speculative decoding promises faster inference for large language models
(LLMs), yet existing methods fail to generalize to real-world settings.
Benchmarks typically assume short contexts (e.g., 2K tokens), whereas practical
workloads involve long contexts. We find current approaches degrade severely
with long contexts; for instance, EAGLE3 even slows down the generation speed
by 0.81x. We address these limitations by releasing a new long-context
benchmark (LongSpecBench) and introducing a novel model (OWL). OWL achieves
about 5x higher acceptance length than EAGLE3 on long-context inputs through
three innovations: (1) an LSTM-based drafter conditioned only on the last-token
state, making it generalize to various lengths, (2) a special token [SPEC] in
the verifier that produces richer representation for drafter, and (3) a hybrid
algorithm combining both tree and non-tree decoding methods. We release all
code and datasets to advance future research.

</details>


### [48] [Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices](https://arxiv.org/abs/2510.07545)
*Md Tahmid Rahman Laskar,Mohammed Saidul Islam,Ridwan Mahbub,Mizanur Rahman,Amran Bhuiyan,Israt Jahan,Mir Tafseer Nayeem,Shafiq Joty,Enamul Hoque,Jimmy Huang*

Main category: cs.CL

TL;DR: The paper proposes cost-efficient methods to train tiny language-vision models (<=2B parameters) to effectively evaluate charts.


<details>
  <summary>Details</summary>
Motivation: Tiny models currently fail at chart comprehension tasks, which limits their utility in resource-constrained environments.

Method: The paper introduces multi-criteria prompting and domain-adaptive transfer learning to improve the performance of tiny models. Specifically, a 2B-parameter LVLM is fine-tuned using synthetic data to create ChartJudge.

Result: The proposed methods demonstrated improvements in transferring knowledge and delivering specialized evaluations, narrowing robustness gaps observed in larger models.

Conclusion: ChartJudge demonstrates that tiny models can achieve cost-efficient scalability and specialization, aiding in low-cost chart evaluation and reasoning tasks.

Abstract: Large Vision-Language Models (LVLMs) with only 7B parameters have shown
promise as automated judges in chart comprehension tasks. However, tiny models
(<=2B parameters) still perform poorly as judges, limiting their real-world use
in resource-constrained settings. To address this, we propose two approaches to
ensure cost-efficient evaluation: (i) multi-criteria prompting, which combines
separate evaluation criteria into a single query, and (ii) domain-adaptive
transfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic
judgments in a chart dataset to create the ChartJudge. Experiments show that
multi-criteria prompting exposes robustness gaps, which led to a huge drop in
performance for 7B models, including specialized LVLM judges like LLaVA-Critic.
In addition, we find that our tiny LVLM (ChartJudge) can effectively transfer
knowledge from one dataset to another to make it a more specialized model. Our
fine-grained analysis across chart types and query complexities offers
actionable insights into trade-offs between model size, prompt design, and
transferability, enabling scalable, low-cost evaluation for chart reasoning
tasks. Our code and the data will be made publicly available.

</details>


### [49] [Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER](https://arxiv.org/abs/2510.07566)
*Junyi Zhu,Savas Ozkan,Andrea Maracani,Sinan Mutlu,Cho Jung Min,Mete Ozay*

Main category: cs.CL

TL;DR: This paper investigates methods for adapting lightweight BERT-like models for versatile NLP applications on mobile devices. It proposes a multi-task pre-finetuning strategy using modular adapters, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to make NLP models efficient and adaptable for deployment on mobile platforms, which require memory and computational efficiency while catering to diverse applications.

Method: The method involves pre-finetuning lightweight BERT-like encoders using a multi-task framework with task-primary LoRA modules. This allows a single shared encoder backbone paired with modular adapters.

Result: Experiments on 21 tasks show an average performance improvement of +0.8% for named entity recognition and +8.8% for text classification.

Conclusion: The proposed approach balances performance and deployment constraints, making it effective for versatile mobile NLP applications.

Abstract: Deploying natural language processing (NLP) models on mobile platforms
requires models that can adapt across diverse applications while remaining
efficient in memory and computation. We investigate pre-finetuning strategies
to enhance the adaptability of lightweight BERT-like encoders for two
fundamental NLP task families: named entity recognition (NER) and text
classification. While pre-finetuning improves downstream performance for each
task family individually, we find that na\"ive multi-task pre-finetuning
introduces conflicting optimization signals that degrade overall performance.
To address this, we propose a simple yet effective multi-task pre-finetuning
framework based on task-primary LoRA modules, which enables a single shared
encoder backbone with modular adapters. Our approach achieves performance
comparable to individual pre-finetuning while meeting practical deployment
constraint. Experiments on 21 downstream tasks show average improvements of
+0.8% for NER and +8.8% for text classification, demonstrating the
effectiveness of our method for versatile mobile NLP applications.

</details>


### [50] [Linguistic Patterns in Pandemic-Related Content: A Comparative Analysis of COVID-19, Constraint, and Monkeypox Datasets](https://arxiv.org/abs/2510.07579)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Main category: cs.CL

TL;DR: The study analyzes pandemic-related online discourse using computational linguistics to identify linguistic markers that distinguish misinformation from factual content.


<details>
  <summary>Details</summary>
Motivation: To better understand how health misinformation can be detected and countered through its language characteristics during crises like pandemics.

Method: Three corpora (COVID-19 false narratives, general COVID-19 content, and Monkeypox-related posts) were analyzed for differences in readability, rhetorical markers, and persuasive language using computational linguistic techniques.

Result: Misinformation was found to have lower readability, higher levels of fear-related and persuasive terms, and distinct rhetorical styles differing from factual and emotive content.

Conclusion: Findings underline linguistic cues in misinformation, supporting better detection and public health messaging strategies, despite limitations like reliance on traditional indices and static analyses.

Abstract: This study conducts a computational linguistic analysis of pandemic-related
online discourse to examine how language distinguishes health misinformation
from factual communication. Drawing on three corpora: COVID-19 false narratives
(n = 7588), general COVID-19 content (n = 10700), and Monkeypox-related posts
(n = 5787), we identify significant differences in readability, rhetorical
markers, and persuasive language use. COVID-19 misinformation exhibited
markedly lower readability scores and contained over twice the frequency of
fear-related or persuasive terms compared to the other datasets. It also showed
minimal use of exclamation marks, contrasting with the more emotive style of
Monkeypox content. These patterns suggest that misinformation employs a
deliberately complex rhetorical style embedded with emotional cues, a
combination that may enhance its perceived credibility. Our findings contribute
to the growing body of work on digital health misinformation by highlighting
linguistic indicators that may aid detection efforts. They also inform public
health messaging strategies and theoretical models of crisis communication in
networked media environments. At the same time, the study acknowledges
limitations, including reliance on traditional readability indices, use of a
deliberately narrow persuasive lexicon, and reliance on static aggregate
analysis. Future research should therefore incorporate longitudinal designs,
broader emotion lexicons, and platform-sensitive approaches to strengthen
robustness.

</details>


### [51] [IASC: Interactive Agentic System for ConLangs](https://arxiv.org/abs/2510.07591)
*Chihiro Taguchi,Richard Sproat*

Main category: cs.CL

TL;DR: The paper introduces a modular system utilizing LLMs for creating constructed languages, focusing on areas like phonology, morphosyntactic structure, lexicon, orthography, and grammar handbook development.


<details>
  <summary>Details</summary>
Motivation: The authors aim to make constructing artificial languages easier and explore the extent of linguistic knowledge within LLMs.

Method: The system incrementally develops artificial languages by defining phonological rules, translating English sentences into morphosyntactic structures, building lexicons, assigning orthographies, and creating grammars.

Result: While the system showcases some success with typical linguistic patterns, it struggles with rare constructs and low-resource language translation tasks. Varied LLM performance was observed.

Conclusion: The process is deemed enjoyable for language creation and highlights LLM limitations in linguistic understanding and cross-language translation challenges.

Abstract: We present a system that uses LLMs as a tool in the development of
Constructed Languages. The system is modular in that one first creates a target
phonology for the language using an agentic approach that refines its output at
each step with commentary feedback on its previous attempt. Next, a set of
sentences is 'translated' from their English original into a morphosyntactic
markup that reflects the word order and morphosyntactic feature specifications
of the desired target language, with affixes represented as morphosyntactic
feature bundles. From this translated corpus, a lexicon is constructed using
the phonological model and the set of morphemes (stems and affixes) extracted
from the 'translated' sentences. The system is then instructed to provide an
orthography for the language, using an existing script such as Latin or
Cyrillic. Finally, the system writes a brief grammatical handbook of the
language. The system can also translate further sentences into the target
language.
  Our goal is twofold. First, we hope that these tools will be fun to use for
creating artificially constructed languages. Second, we are interested in
exploring what LLMs 'know' about language-not what they know about any
particular language or linguistic phenomenon, but how much they know about and
understand language and linguistic concepts. As we shall see, there is a fairly
wide gulf in capabilities both among different LLMs and among different
linguistic specifications, with it being notably easier for systems to deal
with more common patterns than rarer ones. An additional avenue that we explore
is the application of our approach to translating from high-resource into
low-resource languages. While the results so far are mostly negative, we
provide some evidence that an improved version of the present system could
afford some real gains in such tasks.
  https://github.com/SakanaAI/IASC

</details>


### [52] [Vocabulary embeddings organize linguistic structure early in language model training](https://arxiv.org/abs/2510.07613)
*Isabel Papadimitriou,Jacob Prince*

Main category: cs.CL

TL;DR: This paper studies how the geometry of input vocabulary representations in large language models evolves during training, focusing on semantic, syntactic, and frequency patterns.


<details>
  <summary>Details</summary>
Motivation: To understand the structural dynamics of input embeddings during the training of LLMs, as this could reveal how linguistic properties influence model capabilities.

Method: The authors used representational similarity analysis on two open-source models (Pythia 12B and OLMo 7B), correlating embedding geometry with various linguistic and frequency metrics throughout the training process.

Result: They found that embedding structures align with semantic and syntactic features early in training, while high-frequency and function word embeddings stabilize faster than those of other word types.

Conclusion: The study provides insights into how vocabulary embedding geometry evolves, emphasizing the roles of word frequency and function, and suggests this evolution is linked to capability gains during training.

Abstract: Large language models (LLMs) work by manipulating the geometry of input
embedding vectors over multiple layers. Here, we ask: how are the input
vocabulary representations of language models structured, and how and when does
this structure evolve over training? To answer this question, we use
representational similarity analysis, running a suite of experiments that
correlate the geometric structure of the input embeddings and output embeddings
of two open-source models (Pythia 12B and OLMo 7B) with semantic, syntactic,
and frequency-based metrics over the course of training. Our key findings are
as follows: 1) During training, the vocabulary embedding geometry quickly
converges to high correlations with a suite of semantic and syntactic features;
2) Embeddings of high-frequency and function words (e.g., "the," "of") converge
to their final vectors faster than lexical and low-frequency words, which
retain some alignment with the bias in their random initializations. These
findings help map the dynamic trajectory by which input embeddings organize
around linguistic structure, revealing distinct roles for word frequency and
function. Our findings motivate a deeper study of how the evolution of
vocabulary geometry may facilitate specific capability gains during model
training.

</details>


### [53] [Toward Reliable Clinical Coding with Language Models: Verification and Lightweight Adaptation](https://arxiv.org/abs/2510.07629)
*Zhangdie Yuan,Han-Chin Shing,Mitch Strong,Chaitanya Shivade*

Main category: cs.CL

TL;DR: The paper addresses the limitations of large language models (LLMs) in clinical coding accuracy, introduces improvements through lightweight interventions, and proposes clinical code verification as an effective solution.


<details>
  <summary>Details</summary>
Motivation: Clinical coding accuracy is crucial for healthcare documentation and decision-making, but existing LLMs often make errors, especially in hierarchically close codes.

Method: The authors utilized prompt engineering, small-scale fine-tuning, and introduced clinical code verification steps. They also created a double-annotated outpatient benchmark for robust evaluation.

Result: Lightweight interventions improved clinical coding accuracy, and verification was shown to be effective in addressing hierarchically near-miss errors.

Conclusion: Clinical code verification emerges as a reliable method to enhance LLM-based medical coding, and expert-annotated benchmarks prove valuable for addressing dataset limitations.

Abstract: Accurate clinical coding is essential for healthcare documentation, billing,
and decision-making. While prior work shows that off-the-shelf LLMs struggle
with this task, evaluations based on exact match metrics often overlook errors
where predicted codes are hierarchically close but incorrect. Our analysis
reveals that such hierarchical misalignments account for a substantial portion
of LLM failures. We show that lightweight interventions, including prompt
engineering and small-scale fine-tuning, can improve accuracy without the
computational overhead of search-based methods. To address hierarchically
near-miss errors, we introduce clinical code verification as both a standalone
task and a pipeline component. To mitigate the limitations in existing
datasets, such as incomplete evidence and inpatient bias in MIMIC, we release
an expert double-annotated benchmark of outpatient clinical notes with ICD-10
codes. Our results highlight verification as an effective and reliable step
toward improving LLM-based medical coding.

</details>


### [54] [Role-Conditioned Refusals: Evaluating Access Control Reasoning in Large Language Models](https://arxiv.org/abs/2510.07642)
*Đorđe Klisura,Joseph Khoury,Ashish Kundu,Ram Krishnan,Anthony Rios*

Main category: cs.CL

TL;DR: The paper examines the ability of large language models (LLMs) to adhere to access control policies, particularly through role-conditioned refusals, and proposes methods to improve this ability.


<details>
  <summary>Details</summary>
Motivation: Many large language models fail to precisely enforce access control policies, creating security risks by producing unrestricted responses.

Method: The researchers created a novel dataset by augmenting existing text-to-SQL datasets (Spider and BIRD) with realistic PostgreSQL role-based policies. They tested three methods: zero/few-shot prompting, a two-step generator-verifier pipeline, and LoRA fine-tuned models.

Result: The two-step framework improved refusal precision and reduced false permits, while the LoRA fine-tuned models achieved a better balance between safety and execution accuracy.

Conclusion: Longer and more complex policies reduce system reliability, and their RBAC-augmented datasets and methods offer insights for addressing access control issues in LLMs.

Abstract: Access control is a cornerstone of secure computing, yet large language
models often blur role boundaries by producing unrestricted responses. We study
role-conditioned refusals, focusing on the LLM's ability to adhere to access
control policies by answering when authorized and refusing when not. To
evaluate this behavior, we created a novel dataset that extends the Spider and
BIRD text-to-SQL datasets, both of which have been modified with realistic
PostgreSQL role-based policies at the table and column levels. We compare three
designs: (i) zero or few-shot prompting, (ii) a two-step generator-verifier
pipeline that checks SQL against policy, and (iii) LoRA fine-tuned models that
learn permission awareness directly. Across multiple model families, explicit
verification (the two-step framework) improves refusal precision and lowers
false permits. At the same time, fine-tuning achieves a stronger balance
between safety and utility (i.e., when considering execution accuracy). Longer
and more complex policies consistently reduce the reliability of all systems.
We release RBAC-augmented datasets and code.

</details>


### [55] [Banking Done Right: Redefining Retail Banking with Language-Centric AI](https://arxiv.org/abs/2510.07645)
*Xin Jie Chua,Jeraelyn Ming Li Tan,Jia Xuan Tan,Soon Chang Poh,Yi Xian Goh,Debbie Hui Tian Choong,Chee Mun Foong,Sze Jue Yang,Chee Seng Chan*

Main category: cs.CL

TL;DR: This paper introduces Ryt AI and Ryt Bank, showcasing the first regulator-approved conversational AI system for executing financial transactions.


<details>
  <summary>Details</summary>
Motivation: To make financial interactions easier and secure by using conversational AI as the main banking interface.

Method: Ryt AI uses ILMU, an LLM along with four specialized agents powered by LoRA adapters, ensuring secure and compliant operation.

Result: The framework successfully replaces traditional workflows, enabling regulator-compliant core financial transactions through natural language.

Conclusion: Ryt AI proves conversational AI's potential in banking by meeting strict regulatory requirements for secure financial operations.

Abstract: This paper presents Ryt AI, an LLM-native agentic framework that powers Ryt
Bank to enable customers to execute core financial transactions through natural
language conversation. This represents the first global regulator-approved
deployment worldwide where conversational AI functions as the primary banking
interface, in contrast to prior assistants that have been limited to advisory
or support roles. Built entirely in-house, Ryt AI is powered by ILMU, a
closed-source LLM developed internally, and replaces rigid multi-screen
workflows with a single dialogue orchestrated by four LLM-powered agents
(Guardrails, Intent, Payment, and FAQ). Each agent attaches a task-specific
LoRA adapter to ILMU, which is hosted within the bank's infrastructure to
ensure consistent behavior with minimal overhead. Deterministic guardrails,
human-in-the-loop confirmation, and a stateless audit architecture provide
defense-in-depth for security and compliance. The result is Banking Done Right:
demonstrating that regulator-approved natural-language interfaces can reliably
support core financial operations under strict governance.

</details>


### [56] [OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2510.07651)
*Yuzhe Gu,Xiyu Liang,Jiaojiao Zhao,Enmao Diao*

Main category: cs.CL

TL;DR: The paper introduces OBCache, a method to optimize cache eviction in large language models by quantitatively assessing token saliency via their impact on attention outputs.


<details>
  <summary>Details</summary>
Motivation: Large language models with extensive context windows are powerful but face memory overhead due to caching key-value states, which scales with sequence length and batch size.

Method: OBCache formulates cache eviction as a structured pruning problem using Optimal Brain Damage theory to measure token saliency with closed-form scores for key-value pairs.

Result: Experiments using LLaMA and Qwen models show that OBCache's scores improve long-context accuracy over heuristic methods.

Conclusion: OBCache enhances existing cache eviction strategies by utilizing output-aware signals, offering a principled approach for efficient memory usage in LLMs.

Abstract: Large language models (LLMs) with extended context windows enable powerful
downstream applications but impose significant memory overhead, as caching all
key-value (KV) states scales linearly with sequence length and batch size.
Existing cache eviction methods address this by exploiting attention sparsity,
yet they typically rank tokens heuristically using accumulated attention
weights without considering their true impact on attention outputs. We propose
Optimal Brain Cache (OBCache), a principled framework that formulates cache
eviction as a layer-wise structured pruning problem. Building upon the Optimal
Brain Damage (OBD) theory, OBCache quantifies token saliency by measuring the
perturbation in attention outputs induced by pruning tokens, with closed-form
scores derived for isolated keys, isolated values, and joint key-value pairs.
Our scores account not only for attention weights but also for information from
value states and attention outputs, thereby enhancing existing eviction
strategies with output-aware signals. Experiments on LLaMA and Qwen models
demonstrate that replacing the heuristic scores in existing works, which
estimate token saliency across different query positions, with OBCache's
output-aware scores consistently improves long-context accuracy.

</details>


### [57] [Lossless Vocabulary Reduction for Auto-Regressive Language Models](https://arxiv.org/abs/2510.08102)
*Daiki Chijiwa,Taku Hasegawa,Kyosuke Nishida,Shin'ya Yamaguchi,Tomoya Ohba,Tamao Sakao,Susumu Takeuchi*

Main category: cs.CL

TL;DR: The paper develops a theoretical framework for lossless vocabulary reduction in auto-regressive language models, enabling model cooperation despite different tokenizations.


<details>
  <summary>Details</summary>
Motivation: Current auto-regressive language models struggle to collaborate effectively due to differing vocabularies, which hampers advances like model ensemble.

Method: It proposes a framework for losslessly converting language models to use smaller, common vocabularies without sacrificing accuracy.

Result: The framework allows models with different tokenizations to efficiently cooperate by using their maximal common vocabulary.

Conclusion: Lossless vocabulary reduction enables better collaboration between language models, improving versatility without compromising performance.

Abstract: Tokenization -- the process of decomposing a given text into a sequence of
subwords called tokens -- is one of the key components in the development of
language models. Particularly, auto-regressive language models generate texts
token by token, i.e., by predicting the next-token distribution given the
previous ones, and thus tokenization directly affects their efficiency in text
generation. Since each language model has their own vocabulary as a set of
possible tokens, they struggle to cooperate with each other at the level of
next-token distributions such as model ensemble. In this paper, we establish a
theoretical framework of lossless vocabulary reduction, which efficiently
converts a given auto-regressive language model into the one with an
arbitrarily small vocabulary without any loss in accuracy. As an application,
we demonstrate that language models with different tokenization can cooperate
with each other efficiently through their maximal common vocabulary.

</details>


### [58] [Textual Entailment and Token Probability as Bias Evaluation Metrics](https://arxiv.org/abs/2510.07662)
*Virginia K. Felkner,Allison Lim,Jonathan May*

Main category: cs.CL

TL;DR: The paper investigates comparing TP and NLI metrics for bias detection in language models, finding unique strengths and weaknesses in each.


<details>
  <summary>Details</summary>
Motivation: Existing TP bias metrics have limitations in real-world applications, necessitating exploration of NLI as a realistic alternative.

Method: Evaluation of bias through NLI metrics and comparison to existing TP metrics, analyzing correlation and sensitivity to linguistic variations.

Result: NLI metrics behave differently from TP metrics, detecting underdebias more effectively but showing brittleness to sentence wording.

Conclusion: Neither TP nor NLI is universally superior for bias evaluation; combining multiple methods is recommended for comprehensive analysis.

Abstract: Measurement of social bias in language models is typically by token
probability (TP) metrics, which are broadly applicable but have been criticized
for their distance from real-world langugage model use cases and harms. In this
work, we test natural language inference (NLI) as a more realistic alternative
bias metric. We show that, curiously, NLI and TP bias evaluation behave
substantially differently, with very low correlation among different NLI
metrics and between NLI and TP metrics. We find that NLI metrics are more
likely to detect "underdebiased" cases. However, NLI metrics seem to be more
brittle and sensitive to wording of counterstereotypical sentences than TP
approaches. We conclude that neither token probability nor natural language
inference is a "better" bias metric in all cases, and we recommend a
combination of TP, NLI, and downstream bias evaluations to ensure comprehensive
evaluation of language models.
  Content Warning: This paper contains examples of anti-LGBTQ+ stereotypes.

</details>


### [59] [Stress-Testing Model Specs Reveals Character Differences among Language Models](https://arxiv.org/abs/2510.07686)
*Jifan Zhang,Henry Sleight,Andi Peng,John Schulman,Esin Durmus*

Main category: cs.CL

TL;DR: Evaluates 12 frontier LLMs for principle contradictions using stress-testing methodology, uncovering 70,000 cases of behavioral conflicts and ambiguities.


<details>
  <summary>Details</summary>
Motivation: Addressing internal principle conflicts and interpretive ambiguities in LLM behavior outlined in specifications.

Method: Systematically stress-testing model specifications by generating scenarios with explicit value-based tradeoffs and measuring behavioral divergence.

Result: Identified over 70,000 behavioral divergence cases and numerous specification issues in multiple frontier LLMs, as well as their value prioritization patterns.

Conclusion: Behavioral divergence predicts specification issues, revealing contradictions and ambiguities that highlight the need for improved LLM guidelines.

Abstract: Large language models (LLMs) are increasingly trained from AI constitutions
and model specifications that establish behavioral guidelines and ethical
principles. However, these specifications face critical challenges, including
internal conflicts between principles and insufficient coverage of nuanced
scenarios. We present a systematic methodology for stress-testing model
character specifications, automatically identifying numerous cases of principle
contradictions and interpretive ambiguities in current model specs.
  We stress test current model specs by generating scenarios that force
explicit tradeoffs between competing value-based principles. Using a
comprehensive taxonomy we generate diverse value tradeoff scenarios where
models must choose between pairs of legitimate principles that cannot be
simultaneously satisfied. We evaluate responses from twelve frontier LLMs
across major providers (Anthropic, OpenAI, Google, xAI) and measure behavioral
disagreement through value classification scores. Among these scenarios, we
identify over 70,000 cases exhibiting significant behavioral divergence.
Empirically, we show this high divergence in model behavior strongly predicts
underlying problems in model specifications. Through qualitative analysis, we
provide numerous example issues in current model specs such as direct
contradiction and interpretive ambiguities of several principles. Additionally,
our generated dataset also reveals both clear misalignment cases and
false-positive refusals across all of the frontier models we study. Lastly, we
also provide value prioritization patterns and differences of these models.

</details>


### [60] [Large Language Models Meet Virtual Cell: A Survey](https://arxiv.org/abs/2510.07706)
*Krinos Li,Xianglu Xiao,Shenglong Deng,Lucas He,Zijun Zhong,Yuanjie Zou,Zhonghao Zhan,Zheng Hui,Weiye Bao,Guang Yang*

Main category: cs.CL

TL;DR: This paper reviews how large language models (LLMs) are used in virtual cell modeling, organizing them into Oracle and Agent paradigms, with a focus on three key tasks: cellular representation, perturbation prediction, and gene regulation inference.


<details>
  <summary>Details</summary>
Motivation: The motivation is to establish a comprehensive understanding of how LLMs can be utilized to create 'virtual cells' for modeling and predicting cellular states and behaviors.

Method: The work categorizes the use of LLMs into two paradigms: "LLMs as Oracles" for direct cellular modeling and "LLMs as Agents" for managing scientific tasks. It also reviews models, datasets, benchmarks, and challenges related to three core tasks.

Result: The paper identifies three main tasks addressed by LLMs in this domain: cellular representation, perturbation prediction, and gene regulation inference, along with highlighting critical challenges in scalability, generalizability, and interpretability.

Conclusion: The study emphasizes the transformative potential of LLMs in cellular biology while calling attention to unresolved challenges like scalability, generalizability, and interpretability.

Abstract: Large language models (LLMs) are transforming cellular biology by enabling
the development of "virtual cells"--computational systems that represent,
predict, and reason about cellular states and behaviors. This work provides a
comprehensive review of LLMs for virtual cell modeling. We propose a unified
taxonomy that organizes existing methods into two paradigms: LLMs as Oracles,
for direct cellular modeling, and LLMs as Agents, for orchestrating complex
scientific tasks. We identify three core tasks--cellular representation,
perturbation prediction, and gene regulation inference--and review their
associated models, datasets, evaluation benchmarks, as well as the critical
challenges in scalability, generalizability, and interpretability.

</details>


### [61] [Causality Guided Representation Learning for Cross-Style Hate Speech Detection](https://arxiv.org/abs/2510.07707)
*Chengshuai Zhao,Shu Wan,Paras Sheth,Karan Patwa,K. Selçuk Candan,Huan Liu*

Main category: cs.CL

TL;DR: CADET introduces a causal representation learning framework for hate speech detection by disentangling contextual factors and enabling counterfactual reasoning to enhance generalization across diverse styles.


<details>
  <summary>Details</summary>
Motivation: Hate speech detection is hindered by implicit forms such as sarcasm and stereotypes, as well as platform-specific variations, which lead to spurious correlations and poor generalization.

Method: CADET utilizes a causal graph to model factors like context, creator intent, target, and style, enabling latent representation disentanglement and counterfactual interventions to identify true hate intent.

Result: CADET outperforms existing methods by effectively isolating genuine hate speech intent and providing robust performance across stylistic variations and platforms.

Conclusion: CADET demonstrates the efficacy of leveraging causal priors to improve the robustness and generalizability of hate speech detection models.

Abstract: The proliferation of online hate speech poses a significant threat to the
harmony of the web. While explicit hate is easily recognized through overt
slurs, implicit hate speech is often conveyed through sarcasm, irony,
stereotypes, or coded language -- making it harder to detect. Existing hate
speech detection models, which predominantly rely on surface-level linguistic
cues, fail to generalize effectively across diverse stylistic variations.
Moreover, hate speech spread on different platforms often targets distinct
groups and adopts unique styles, potentially inducing spurious correlations
between them and labels, further challenging current detection approaches.
Motivated by these observations, we hypothesize that the generation of hate
speech can be modeled as a causal graph involving key factors: contextual
environment, creator motivation, target, and style. Guided by this graph, we
propose CADET, a causal representation learning framework that disentangles
hate speech into interpretable latent factors and then controls confounders,
thereby isolating genuine hate intent from superficial linguistic cues.
Furthermore, CADET allows counterfactual reasoning by intervening on style
within the latent space, naturally guiding the model to robustly identify hate
speech in varying forms. CADET demonstrates superior performance in
comprehensive experiments, highlighting the potential of causal priors in
advancing generalizable hate speech detection.

</details>


### [62] [MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation](https://arxiv.org/abs/2510.07713)
*Shuo Yu,Mingyue Cheng,Daoyu Wang,Qi Liu,Zirui Liu,Ze Guo,Xiaoyu Tao*

Main category: cs.CL

TL;DR: MemWeaver introduces a hierarchical memory framework for personalized generation by leveraging rich temporal and semantic structures within user's textual interaction history.


<details>
  <summary>Details</summary>
Motivation: The paper identifies a shift in user-internet engagement from implicit feedback signals to explicit textual interactive behaviors, necessitating deeper personalization frameworks to better capture dynamic user interests.

Method: MemWeaver constructs a dual-component memory—behavioral memory for user actions and cognitive memory for long-term preferences—to represent user history. This unified memory is integrated with large language models to enhance personalization.

Result: Experimental validation on the Language Model Personalization (LaMP) benchmark showcases MemWeaver's effectiveness in improving personalized text generation.

Conclusion: MemWeaver successfully models temporal evolution and semantic relationships in user behavior, advancing personalization capabilities of large language models.

Abstract: The primary form of user-internet engagement is shifting from leveraging
implicit feedback signals, such as browsing and clicks, to harnessing the rich
explicit feedback provided by textual interactive behaviors. This shift unlocks
a rich source of user textual history, presenting a profound opportunity for a
deeper form of personalization. However, prevailing approaches offer only a
shallow form of personalization, as they treat user history as a flat list of
texts for retrieval and fail to model the rich temporal and semantic structures
reflecting dynamic nature of user interests. In this work, we propose
\textbf{MemWeaver}, a framework that weaves the user's entire textual history
into a hierarchical memory to power deeply personalized generation. The core
innovation of our memory lies in its ability to capture both the temporal
evolution of interests and the semantic relationships between different
activities. To achieve this, MemWeaver builds two complementary memory
components that both integrate temporal and semantic information, but at
different levels of abstraction: behavioral memory, which captures specific
user actions, and cognitive memory, which represents long-term preferences.
This dual-component memory serves as a unified representation of the user,
allowing large language models (LLMs) to reason over both concrete behaviors
and abstracted traits. Experiments on the Language Model Personalization (LaMP)
benchmark validate the efficacy of MemWeaver. Our code is
available\footnote{https://github.com/fishsure/MemWeaver}.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [63] [Enhancing Maritime Object Detection in Real-Time with RT-DETR and Data Augmentation](https://arxiv.org/abs/2510.07346)
*Nader Nemati*

Main category: cs.CV

TL;DR: The paper introduces a real-time maritime object detection system leveraging RT-DETR, synthetic image augmentation, and advanced training strategies.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome challenges in maritime object detection arising from small target sizes and a lack of labeled real RGB data.

Method: The approach uses RT-DETR with multi-scale feature fusion, targeted query selection, and synthetic-real data weighting, alongside data augmentation for class balance.

Result: A robust maritime object detection pipeline achieving real-time performance even in challenging conditions, with module effectiveness confirmed through component analysis.

Conclusion: The system enhances detection accuracy while maintaining real-time capabilities and provides insights into handling extreme maritime scenarios.

Abstract: Maritime object detection faces essential challenges due to the small target
size and limitations of labeled real RGB data. This paper will present a
real-time object detection system based on RT-DETR, enhanced by employing
augmented synthetic images while strictly evaluating on real data. This study
employs RT-DETR for the maritime environment by combining multi-scale feature
fusion, uncertainty-minimizing query selection, and smart weight between
synthetic and real training samples. The fusion module in DETR enhances the
detection of small, low-contrast vessels, query selection focuses on the most
reliable proposals, and the weighting strategy helps reduce the visual gap
between synthetic and real domains. This design preserves DETR's refined
end-to-end set prediction while allowing users to adjust between speed and
accuracy at inference time. Data augmentation techniques were also used to
balance the different classes of the dataset to improve the robustness and
accuracy of the model. Regarding this study, a full Python robust maritime
detection pipeline is delivered that maintains real-time performance even under
practical limits. It also verifies how each module contributes, and how the
system handles failures in extreme lighting or sea conditions. This study also
includes a component analysis to quantify the contribution of each
architectural module and explore its interactions.

</details>


### [64] [DynamicEval: Rethinking Evaluation for Dynamic Text-to-Video Synthesis](https://arxiv.org/abs/2510.07441)
*Nithin C. Babu,Aniruddha Mahapatra,Harsh Rangwani,Rajiv Soundararajan,Kuldeep Kulkarni*

Main category: cs.CV

TL;DR: DynamicEval is a benchmark specifically designed for evaluating text-to-video (T2V) generative models under dynamic camera motion, addressing gaps in existing benchmarks like VBench and EvalCrafter.


<details>
  <summary>Details</summary>
Motivation: Existing T2V benchmarks generally focus on static camera scenes and aggregate scores into a single model-level ranking, neglecting video-level evaluation and dynamic camera motion contexts.

Method: DynamicEval utilizes a curated dataset of prompts emphasizing dynamic camera motion, paired with human annotations, and introduces new metrics for evaluating video quality, including background scene consistency and foreground object consistency.

Result: DynamicEval improves alignment with human preferences for evaluating T2V models, showing over 2% enhancement in correlation at both video and model levels compared to existing benchmarks.

Conclusion: DynamicEval proves itself as a more holistic and effective benchmark for assessing the performance of T2V models, particularly in dynamic motion scenarios.

Abstract: Existing text-to-video (T2V) evaluation benchmarks, such as VBench and
EvalCrafter, suffer from two limitations. (i) While the emphasis is on
subject-centric prompts or static camera scenes, camera motion essential for
producing cinematic shots and existing metrics under dynamic motion are largely
unexplored. (ii) These benchmarks typically aggregate video-level scores into a
single model-level score for ranking generative models. Such aggregation,
however, overlook video-level evaluation, which is vital to selecting the
better video among the candidate videos generated for a given prompt. To
address these gaps, we introduce DynamicEval, a benchmark consisting of
systematically curated prompts emphasizing dynamic camera motion, paired with
45k human annotations on video pairs from 3k videos generated by ten T2V
models. DynamicEval evaluates two key dimensions of video quality: background
scene consistency and foreground object consistency. For background scene
consistency, we obtain the interpretable error maps based on the Vbench motion
smoothness metric. We observe that while the Vbench motion smoothness metric
shows promising alignment with human judgments, it fails in two cases:
occlusions/disocclusions arising from camera and foreground object movements.
Building on this, we propose a new background consistency metric that leverages
object error maps to correct two failure cases in a principled manner. Our
second innovation is the introduction of a foreground consistency metric that
tracks points and their neighbors within each object instance to assess object
fidelity. Extensive experiments demonstrate that our proposed metrics achieve
stronger correlations with human preferences at both the video level and the
model level (an improvement of more than 2% points), establishing DynamicEval
as a more comprehensive benchmark for evaluating T2V models under dynamic
camera motion.

</details>


### [65] [Provably Accelerated Imaging with Restarted Inertia and Score-based Image Priors](https://arxiv.org/abs/2510.07470)
*Marien Renaud,Julien Hermant,Deliang Wei,Yu Sun*

Main category: cs.CV

TL;DR: RISP is an advanced algorithm for solving imaging inverse problems, offering faster convergence and high-quality images compared to RED.


<details>
  <summary>Details</summary>
Motivation: To address the need for both fast convergence and high-quality image recovery in solving ill-posed imaging inverse problems.

Method: Proposed RISP, which uses a restarting inertia mechanism for acceleration and incorporates score-based image priors, along with proof of its convergence properties and analysis of its continuous-time system.

Result: RISP achieves a faster convergence rate compared to RED and demonstrates high-quality image reconstruction across various imaging problems.

Conclusion: RISP bridges the gap between speed and quality in imaging algorithms, providing a principled and efficient solution to inverse problems.

Abstract: Fast convergence and high-quality image recovery are two essential features
of algorithms for solving ill-posed imaging inverse problems. Existing methods,
such as regularization by denoising (RED), often focus on designing
sophisticated image priors to improve reconstruction quality, while leaving
convergence acceleration to heuristics. To bridge the gap, we propose Restarted
Inertia with Score-based Priors (RISP) as a principled extension of RED. RISP
incorporates a restarting inertia for fast convergence, while still allowing
score-based image priors for high-quality reconstruction. We prove that RISP
attains a faster stationary-point convergence rate than RED, without requiring
the convexity of the image prior. We further derive and analyze the associated
continuous-time dynamical system, offering insight into the connection between
RISP and the heavy-ball ordinary differential equation (ODE). Experiments
across a range of imaging inverse problems demonstrate that RISP enables fast
convergence while achieving high-quality reconstructions.

</details>


### [66] [A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy](https://arxiv.org/abs/2510.07492)
*Guoliang Gong,Man Yu*

Main category: cs.CV

TL;DR: This paper proposes an Image Purification (IP) strategy and a Frequency-domain Flow Matching (FFM) model for effective ultra-low dose CT (uLDCT) denoising, addressing challenges due to severe noise, artifacts, and spatial misalignments.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of denoising ultra-low dose CT (uLDCT) images, which suffer from severe noise, artifacts, and spatial misalignment with normal dose CT (NDCT) images, making traditional denoising methods ineffective.

Method: The authors introduce an Image Purification (IP) strategy for generating structurally aligned uLDCT-NDCT image pairs and a Frequency-domain Flow Matching (FFM) model that synergizes with the IP strategy to ensure anatomical structure integrity during denoising.

Result: The proposed IP strategy significantly improves the performance of mainstream denoising models, while the FFM model combined with IP achieves state-of-the-art results in anatomical structure preservation on a real clinical uLDCT lung dataset.

Conclusion: The paper provides an effective solution for the data mismatch problem in uLDCT denoising, enhancing the preservation of anatomical structures and improving overall denoising performance. Code and dataset are made available for further research.

Abstract: Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but
introduces severe noise and artifacts. It also leads to substantial spatial
misalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses
challenges for directly applying existing denoising networks trained on
synthetic noise or aligned data. To address this core challenge in uLDCT
denoising, this paper proposes an innovative denoising framework based on an
Image Purification (IP) strategy. First, we construct a real clinical uLDCT
lung dataset. Then, we propose an Image Purification strategy that generates
structurally aligned uLDCT-NDCT image pairs, providing a high-quality data
foundation for network training. Building upon this, we propose a
Frequency-domain Flow Matching (FFM) model, which works synergistically with
the IP strategy to excellently preserve the anatomical structure integrity of
denoised images. Experiments on the real clinical dataset demonstrate that our
IP strategy significantly enhances the performance of multiple mainstream
denoising models on the uLDCT task. Notably, our proposed FFM model combined
with the IP strategy achieves state-of-the-art (SOTA) results in anatomical
structure preservation. This study provides an effective solution to the data
mismatch problem in real-world uLDCT denoising. Code and dataset are available
at https://github.com/MonkeyDadLufy/flow-matching.

</details>


### [67] [D2RA: Dual Domain Regeneration Attack](https://arxiv.org/abs/2510.07538)
*Pragati Shuddhodhan Meshram,Varun Chandrasekaran*

Main category: cs.CV

TL;DR: This paper introduces D2RA, a training-free attack that effectively removes or weakens watermarks embedded in generative model outputs by leveraging natural priors, highlighting flaws in current watermarking designs.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the vulnerabilities in existing watermarking methods for generative models, even when semantic watermarking is employed, as these methods fail to offer robustness against adversarial attacks in constrained scenarios.

Method: D2RA, a training-free and single-image attack approach, removes watermarks by projecting watermarked images onto natural priors using complementary representations, suppressing the watermark signal while safeguarding the visual quality.

Result: The proposed D2RA approach consistently reduces the detectability of watermarks across different schemes, proving its efficiency in overcoming the limitations of existing watermarking systems.

Conclusion: The results illustrate fundamental weaknesses in current watermarking schemes for generative models, emphasizing the need for improved designs to ensure better robustness against D2RA-like attacks.

Abstract: The growing use of generative models has intensified the need for
watermarking methods that ensure content attribution and provenance. While
recent semantic watermarking schemes improve robustness by embedding signals in
latent or frequency representations, we show they remain vulnerable even under
resource-constrained adversarial settings. We present D2RA, a training-free,
single-image attack that removes or weakens watermarks without access to the
underlying model. By projecting watermarked images onto natural priors across
complementary representations, D2RA suppresses watermark signals while
preserving visual fidelity. Experiments across diverse watermarking schemes
demonstrate that our approach consistently reduces watermark detectability,
revealing fundamental weaknesses in current designs. Our code is available at
https://github.com/Pragati-Meshram/DAWN.

</details>


### [68] [PickStyle: Video-to-Video Style Transfer with Context-Style Adapters](https://arxiv.org/abs/2510.07546)
*Soroush Mehraban,Vida Adeli,Jacob Rommann,Babak Taati,Kyryl Truskovskyi*

Main category: cs.CV

TL;DR: This paper introduces PickStyle, a framework that uses diffusion models for video style transfer guided by text prompts, achieving realistic and consistent video translations without requiring paired video data.


<details>
  <summary>Details</summary>
Motivation: A key challenge in video style transfer is the lack of paired video data for training, which makes it difficult to maintain both temporal consistency and style alignment.

Method: PickStyle incorporates low-rank adapters into pretrained video diffusion models. It uses synthetic training clips generated from paired still images and proposes Context-Style Classifier-Free Guidance (CS-CFG) to decouple style and context guidance.

Result: The approach achieves temporally coherent, style-faithful, and context-preserving video output, outperforming existing methods in both qualitative and quantitative evaluations.

Conclusion: PickStyle effectively addresses the challenges in video style transfer, demonstrating its utility for generating high-quality videos that balance temporal coherence and style adherence.

Abstract: We address the task of video style transfer with diffusion models, where the
goal is to preserve the context of an input video while rendering it in a
target style specified by a text prompt. A major challenge is the lack of
paired video data for supervision. We propose PickStyle, a video-to-video style
transfer framework that augments pretrained video diffusion backbones with
style adapters and benefits from paired still image data with source-style
correspondences for training. PickStyle inserts low-rank adapters into the
self-attention layers of conditioning modules, enabling efficient
specialization for motion-style transfer while maintaining strong alignment
between video content and style. To bridge the gap between static image
supervision and dynamic video, we construct synthetic training clips from
paired images by applying shared augmentations that simulate camera motion,
ensuring temporal priors are preserved. In addition, we introduce Context-Style
Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free
guidance into independent text (style) and video (context) directions. CS-CFG
ensures that context is preserved in generated video while the style is
effectively transferred. Experiments across benchmarks show that our approach
achieves temporally coherent, style-faithful, and content-preserving video
translations, outperforming existing baselines both qualitatively and
quantitatively.

</details>


### [69] [TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility](https://arxiv.org/abs/2510.07550)
*Saman Motamed,Minghao Chen,Luc Van Gool,Iro Laina*

Main category: cs.CV

TL;DR: The paper addresses the challenge of assessing physical realism in video through Video-Language Models (VLMs). It proposes TRAVL, a fine-tuning method, and introduces the ImplausiBench benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: Modern video generative models often produce unrealistic sequences that defy physical laws. However, there is no reliable method to quantitatively assess physical plausibility in video.

Method: The authors enhance VLMs using TRAVL, which integrates a balanced training dataset and a trajectory-aware attention module. They also introduce ImplausiBench, a benchmark specifically designed to evaluate physical reasoning while reducing linguistic biases.

Result: Existing VLMs struggle with detecting physical implausibilities. The TRAVL fine-tuning method improves motion encoding and discrimination capabilities, offering better detection of physics violations. ImplausiBench enables rigorous testing of visual-temporal understanding.

Conclusion: TRAVL and ImplausiBench collectively provide a framework to improve and evaluate physical plausibility in multimodal models, advancing research on visual-temporal reasoning in VLMs.

Abstract: Despite impressive visual fidelity, modern video generative models frequently
produce sequences that violate intuitive physical laws, such as objects
floating, teleporting, or morphing in ways that defy causality. While humans
can easily detect such implausibilities, there remains no robust method for
quantitatively assessing physical realism in video. In this work, we explore
whether Video-Language Models (VLMs) can be trained to serve as reliable judges
of physical plausibility. We find that existing VLMs struggle to identify
physics violations, exposing fundamental limitations in their temporal and
causal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe
that combines a balanced training dataset with a trajectory-aware attention
module to improve motion encoding and discrimination in VLMs. To evaluate
physical reasoning more rigorously, we propose ImplausiBench, a benchmark of
300 videos (150 real, 150 generated) that removes linguistic biases and
isolates visual-temporal understanding. Performance is reported both with
gold-standard human judgments and stricter LLM-as-judge metrics. Together,
TRAVL and ImplausiBench offer a unified framework for probing and improving
physical plausibility in multimodal models, shedding light on a challenging and
underexplored aspect of visual-temporal understanding.

</details>


### [70] [Label Semantics for Robust Hyperspectral Image Classification](https://arxiv.org/abs/2510.07556)
*Rafin Hassan,Zarin Tasnim Roshni,Rafiqul Bari,Alimul Islam,Nabeel Mohammed,Moshiur Farazi,Shafin Rahman*

Main category: cs.CV

TL;DR: The paper proposes a Semantic Spectral-Spatial Fusion Network (S3FN) that incorporates textual descriptions generated by LLMs to enhance hyperspectral imaging (HSI) classification, achieving better accuracy on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: HSI classification struggles with overfitting due to limited training samples and high dimensionality, and traditionally relies solely on spectral-spatial data, which limits performance.

Method: The paper introduces S3FN, which utilizes LLMs to generate class-specific textual descriptions and embeds these using pre-trained text encoders like BERT or RoBERTa, complementing the learning of spectral-spatial data.

Result: The S3FN model demonstrates significant performance improvements over baseline methods on three HSI benchmark datasets: Hyperspectral Wood, Hyperspectral Blueberries, and DeepHS-Fruit.

Conclusion: By integrating textual semantics with spectral-spatial data, S3FN achieves a more effective feature-label alignment, enhancing HSI classification models and setting a precedent for semantically augmented approaches in the field.

Abstract: Hyperspectral imaging (HSI) classification is a critical tool with widespread
applications across diverse fields such as agriculture, environmental
monitoring, medicine, and materials science. Due to the limited availability of
high-quality training samples and the high dimensionality of spectral data, HSI
classification models are prone to overfitting and often face challenges in
balancing accuracy and computational complexity. Furthermore, most of HSI
classification models are monomodal, where it solely relies on spectral-spatial
data to learn decision boundaries in the high dimensional embedding space. To
address this, we propose a general-purpose Semantic Spectral-Spatial Fusion
Network (S3FN) that uses contextual, class specific textual descriptions to
complement the training of an HSI classification model. Specifically, S3FN
leverages LLMs to generate comprehensive textual descriptions for each class
label that captures their unique characteristics and spectral behaviors. These
descriptions are then embedded into a vector space using a pre-trained text
encoder such as BERT or RoBERTa to extract meaningful label semantics which in
turn leads to a better feature-label alignment for improved classification
performance. To demonstrate the effectiveness of our approach, we evaluate our
model on three diverse HSI benchmark datasets - Hyperspectral Wood,
HyperspectralBlueberries, and DeepHS-Fruit and report significant performance
boost. Our results highlight the synergy between textual semantics and
spectral-spatial data, paving the way for further advancements in semantically
augmented HSI classification models. Codes are be available in:
https://github.com/milab-nsu/S3FN

</details>


### [71] [Cross-Modal Attention Guided Unlearning in Vision-Language Models](https://arxiv.org/abs/2510.07567)
*Karuna Bhaila,Aneesh Komanduri,Minh-Hao Van,Xintao Wu*

Main category: cs.CV

TL;DR: The paper introduces Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight framework addressing private data leakage in Vision-Language Models (VLMs), achieving efficient unlearning without retraining.


<details>
  <summary>Details</summary>
Motivation: To address private data leakage in VLMs during inference, particularly for tasks combining visual and textual contexts such as Visual Question Answering (VQA).

Method: Developing CAGUL, a framework utilizing visual tokens of low importance guided by cross-modal attention, thereby preventing private data leakage without altering the model's parameters.

Result: Experimental results demonstrate that CAGUL effectively prevents data leakage, performs comparably or better than finetuning methods, and avoids retraining costs.

Conclusion: CAGUL is a practical, efficient, and lightweight solution for unlearning in VLMs, safeguarding data privacy while preserving model performance.

Abstract: Vision-Language Models (VLMs) have demonstrated immense capabilities in
multi-modal understanding and inference tasks such as Visual Question Answering
(VQA), which requires models to infer outputs based on visual and textual
context simultaneously. Such inference abilities of large-scale pretrained
models are often attributed to the massive scale of pre-training data collected
across several domains. However, the models may memorize private and/or
sensitive information during training and regurgitate it in inference.
Recently, machine unlearning has been leveraged to address the leakage of
private data in LLMs. VLMs add a layer of complexity to this process, as the
visual context in the query may also contain sensitive information in addition
to the text. To address this issue, we explore unlearning for vision-language
models, specifically for the VQA task. We explore the role of visual tokens for
output generation in VLMs using cross-modal attention and utilize it to
formulate Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight and
efficient VLM unlearning framework. In contrast to computationally expensive
model finetuning methods, CAGUL utilizes external modules to encode unlearning
information in visual tokens of low importance for relevant queries. We find
that the transformed visual tokens not only prevent leakage but also retain
reference model behavior. Experimental results show that our method performs
better or on par with finetuning-based baselines without altering the
pre-trained model parameters or incurring retraining costs, making it a
practical and effective unlearning solution for VLMs.

</details>


### [72] [MaizeStandCounting (MaSC): Automated and Accurate Maize Stand Counting from UAV Imagery Using Image Processing and Deep Learning](https://arxiv.org/abs/2510.07580)
*Dewi Endah Kharismawati,Toni Kazic*

Main category: cs.CV

TL;DR: The paper introduces MaizeStandCounting (MaSC), an algorithm using UAV imagery and YOLOv9 for automated maize stand counting, achieving effective and scalable results.


<details>
  <summary>Details</summary>
Motivation: The need for accurate and efficient maize stand counts for better crop management, yield prediction, and addressing germination issues, as manual methods are laborious and error-prone.

Method: The method involves acquiring RGB imagery using low-cost UAVs, processed through a lightweight YOLOv9 model operating in two modes: image mosaics divided into patches and raw video frames aligned via homography matrices.

Result: Evaluation demonstrated strong results with R^2 values of 0.616 for image mosaics and 0.906 for raw frames, and an efficient processing speed of 83 frames in 60.63 seconds.

Conclusion: MaSC proves to be a low-cost, scalable, and accurate tool for maize stand counting, suitable for both research and production fields.

Abstract: Accurate maize stand counts are essential for crop management and research,
informing yield prediction, planting density optimization, and early detection
of germination issues. Manual counting is labor-intensive, slow, and
error-prone, especially across large or variable fields. We present
MaizeStandCounting (MaSC), a robust algorithm for automated maize seedling
stand counting from RGB imagery captured by low-cost UAVs and processed on
affordable hardware. MaSC operates in two modes: (1) mosaic images divided into
patches, and (2) raw video frames aligned using homography matrices. Both modes
use a lightweight YOLOv9 model trained to detect maize seedlings from V2-V10
growth stages. MaSC distinguishes maize from weeds and other vegetation, then
performs row and range segmentation based on the spatial distribution of
detections to produce precise row-wise stand counts. Evaluation against
in-field manual counts from our 2024 summer nursery showed strong agreement
with ground truth (R^2= 0.616 for mosaics, R^2 = 0.906 for raw frames). MaSC
processed 83 full-resolution frames in 60.63 s, including inference and
post-processing, highlighting its potential for real-time operation. These
results demonstrate MaSC's effectiveness as a scalable, low-cost, and accurate
tool for automated maize stand counting in both research and production
environments.

</details>


### [73] [Quick-CapsNet (QCN): A fast alternative to Capsule Networks](https://arxiv.org/abs/2510.07600)
*Pouya Shiri,Ramin Sharifi,Amirali Baniasadi*

Main category: cs.CV

TL;DR: Quick-CapsNet (QCN) is introduced as a faster alternative to Capsule Network (CapsNet), achieving a 5x faster inference speed with minor compromise on accuracy.


<details>
  <summary>Details</summary>
Motivation: Capsule Networks (CapsNet) outperform CNNs in certain tasks but suffer from slow training and testing, hindering their suitability for applications requiring rapid responses.

Method: Quick-CapsNet reduces the number of capsules in the network and employs an enhanced decoder for improved efficiency and performance.

Result: QCN maintains competitive accuracy while significantly improving speed, with 5x faster inference tested on datasets including MNIST, F-MNIST, SVHN, and CIFAR-10.

Conclusion: QCN is a promising framework for real-time applications requiring high-speed inference with CapsNet-like functionality.

Abstract: The basic computational unit in Capsule Network (CapsNet) is a capsule (vs.
neurons in Convolutional Neural Networks (CNNs)). A capsule is a set of
neurons, which form a vector. CapsNet is used for supervised classification of
data and has achieved state-of-the-art accuracy on MNIST digit recognition
dataset, outperforming conventional CNNs in detecting overlapping digits.
Moreover, CapsNet shows higher robustness towards affine transformation when
compared to CNNs for MNIST datasets. One of the drawbacks of CapsNet, however,
is slow training and testing. This can be a bottleneck for applications that
require a fast network, especially during inference. In this work, we introduce
Quick-CapsNet (QCN) as a fast alternative to CapsNet, which can be a starting
point to develop CapsNet for fast real-time applications. QCN builds on
producing a fewer number of capsules, which results in a faster network. QCN
achieves this at the cost of marginal loss in accuracy. Inference is 5x faster
on MNIST, F-MNIST, SVHN and Cifar-10 datasets. We also further enhanced QCN by
employing a more powerful decoder instead of the default decoder to further
improve QCN.

</details>


### [74] [Rectified-CFG++ for Flow Based Models](https://arxiv.org/abs/2510.07631)
*Shreshth Saini,Shashank Gupta,Alan C. Bovik*

Main category: cs.CV

TL;DR: Rectified-CFG++ improves text-to-image generation using adaptive guidance that combines rectified flow efficiency with geometry-aware conditioning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address severe off-manifold drift and artifacts caused by applying standard Classifier-Free Guidance (CFG) to rectified flow (RF) based models.

Method: The approach uses adaptive predictor-corrector guidance with conditional RF updates followed by weighted corrections to interpolate between velocity fields.

Result: Rectified-CFG++ demonstrates superior performance compared to standard CFG on large-scale text-to-image models across various benchmarks.

Conclusion: This method ensures stability and better results for text-conditioned inference in diffusion models, overcoming limitations of standard CFG.

Abstract: Classifier-free guidance (CFG) is the workhorse for steering large diffusion
models toward text-conditioned targets, yet its native application to rectified
flow (RF) based models provokes severe off-manifold drift, yielding visual
artifacts, text misalignment, and brittle behaviour. We present
Rectified-CFG++, an adaptive predictor-corrector guidance that couples the
deterministic efficiency of rectified flows with a geometry-aware conditioning
rule. Each inference step first executes a conditional RF update that anchors
the sample near the learned transport path, then applies a weighted conditional
correction that interpolates between conditional and unconditional velocity
fields. We prove that the resulting velocity field is marginally consistent and
that its trajectories remain within a bounded tubular neighbourhood of the data
manifold, ensuring stability across a wide range of guidance strengths.
Extensive experiments on large-scale text-to-image models (Flux, Stable
Diffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms
standard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and
T2I-CompBench. Project page: https://rectified-cfgpp.github.io/

</details>


### [75] [PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment](https://arxiv.org/abs/2510.07636)
*Shashank Gupta,Gregoire Phillips,Alan C. Bovik*

Main category: cs.CV

TL;DR: This paper introduces PIT-QMM, a novel large multimodal model for no-reference point cloud quality assessment (NR-PCQA), leveraging diverse data modalities to set a new performance benchmark and enhance explainability.


<details>
  <summary>Details</summary>
Motivation: To leverage the advancements of large multimodal models for improving the quality assessment of 3D assets, specifically in scenarios without a reference point cloud.

Method: The authors combine text descriptions, 2D projections, and 3D point cloud views within PIT-QMM, a multimodal model designed to predict perceptual quality of point clouds end-to-end. They validated its performance with extensive experimentation.

Result: PIT-QMM significantly outperforms state-of-the-art methods on popular benchmarks and achieves this with fewer training iterations. Additionally, it enables distortion localization and identification.

Conclusion: PIT-QMM marks a significant step forward in NR-PCQA by achieving better performance and introducing explainability and interactivity capabilities for 3D quality assessments, setting new standards in the field.

Abstract: Large Multimodal Models (LMMs) have recently enabled considerable advances in
the realm of image and video quality assessment, but this progress has yet to
be fully explored in the domain of 3D assets. We are interested in using these
models to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), where
the aim is to automatically evaluate the perceptual quality of a point cloud in
absence of a reference. We begin with the observation that different modalities
of data - text descriptions, 2D projections, and 3D point cloud views - provide
complementary information about point cloud quality. We then construct PIT-QMM,
a novel LMM for NR-PCQA that is capable of consuming text, images and point
clouds end-to-end to predict quality scores. Extensive experimentation shows
that our proposed method outperforms the state-of-the-art by significant
margins on popular benchmarks with fewer training iterations. We also
demonstrate that our framework enables distortion localization and
identification, which paves a new way forward for model explainability and
interactivity. Code and datasets are available at
https://www.github.com/shngt/pit-qmm.

</details>


### [76] [Dual-Stream Alignment for Action Segmentation](https://arxiv.org/abs/2510.07652)
*Harshala Gammulle,Clinton Fookes,Sridha Sridharan,Simon Denman*

Main category: cs.CV

TL;DR: The paper introduces the Dual-Stream Alignment Network (DSA Net), a new method for action segmentation in continuous video streams, leveraging two data streams and a hybrid quantum-classical approach.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve action segmentation by leveraging a second stream for action features, addressing limitations in traditional single-stream models.

Method: The proposed DSA Net includes a Temporal Context block for cross-stream information exchange and uses a hybrid quantum-classical approach. It introduces a Dual-Stream Alignment Loss composed of relational consistency, cross-level contrastive, and cycle-consistency reconstruction losses.

Result: The proposed method is tested on benchmark datasets like GTEA, Breakfast, 50Salads, and EgoProcel, showing substantial performance improvement over existing methods.

Conclusion: DSA Net advances the field of action segmentation, achieving state-of-the-art results by innovatively using dual streams and a hybrid quantum-classical framework.

Abstract: Action segmentation is a challenging yet active research area that involves
identifying when and where specific actions occur in continuous video streams.
Most existing work has focused on single-stream approaches that model the
spatio-temporal aspects of frame sequences. However, recent research has
shifted toward two-stream methods that learn action-wise features to enhance
action segmentation performance. In this work, we propose the Dual-Stream
Alignment Network (DSA Net) and investigate the impact of incorporating a
second stream of learned action features to guide segmentation by capturing
both action and action-transition cues. Communication between the two streams
is facilitated by a Temporal Context (TC) block, which fuses complementary
information using cross-attention and Quantum-based Action-Guided Modulation
(Q-ActGM), enhancing the expressive power of the fused features. To the best of
our knowledge, this is the first study to introduce a hybrid quantum-classical
machine learning framework for action segmentation. Our primary objective is
for the two streams (frame-wise and action-wise) to learn a shared feature
space through feature alignment. This is encouraged by the proposed Dual-Stream
Alignment Loss, which comprises three components: relational consistency,
cross-level contrastive, and cycle-consistency reconstruction losses. Following
prior work, we evaluate DSA Net on several diverse benchmark datasets: GTEA,
Breakfast, 50Salads, and EgoProcel. We further demonstrate the effectiveness of
each component through extensive ablation studies. Notably, DSA Net achieves
state-of-the-art performance, significantly outperforming existing

</details>


### [77] [Once Is Enough: Lightweight DiT-Based Video Virtual Try-On via One-Time Garment Appearance Injection](https://arxiv.org/abs/2510.07654)
*Yanjie Pan,Qingdong He,Lidong Wang,Bo Peng,Mingmin Chi*

Main category: cs.CV

TL;DR: This paper proposes OIE (Once is Enough), a video virtual try-on strategy that replaces clothing in the first frame and uses temporal guidance for subsequent frames, achieving high efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Current video virtual try-on methods face challenges in adapting diffusion models based on Diffusion Transformers, primarily due to high parameter dependency and lack of temporal features.

Method: OIE employs a first-frame clothing replacement via an image-based model, followed by pose and mask-guided sequential generation for subsequent frames.

Result: OIE outperforms existing methods in parameter and computational efficiency while maintaining strong overall performance in video virtual try-on tasks.

Conclusion: Using the first frame as a basis for temporal synthesis presents an efficient, effective solution for video virtual try-on challenges and improves performance metrics.

Abstract: Video virtual try-on aims to replace the clothing of a person in a video with
a target garment. Current dual-branch architectures have achieved significant
success in diffusion models based on the U-Net; however, adapting them to
diffusion models built upon the Diffusion Transformer remains challenging.
Initially, introducing latent space features from the garment reference branch
requires adding or modifying the backbone network, leading to a large number of
trainable parameters. Subsequently, the latent space features of garments lack
inherent temporal characteristics and thus require additional learning. To
address these challenges, we propose a novel approach, OIE (Once is Enough), a
virtual try-on strategy based on first-frame clothing replacement:
specifically, we employ an image-based clothing transfer model to replace the
clothing in the initial frame, and then, under the content control of the
edited first frame, utilize pose and mask information to guide the temporal
prior of the video generation model in synthesizing the remaining frames
sequentially. Experiments show that our method achieves superior parameter
efficiency and computational efficiency while still maintaining leading
performance under these constraints.

</details>


### [78] [MONKEY: Masking ON KEY-Value Activation Adapter for Personalization](https://arxiv.org/abs/2510.07656)
*James Baker*

Main category: cs.CV

TL;DR: The paper proposes an enhancement to personalize diffusion models for image generation, enabling better integration of subjects with text prompts through automated masking.


<details>
  <summary>Details</summary>
Motivation: To address the limitation in personalized diffusion models where generated images often recreate the subject without effectively incorporating text prompts.

Method: The approach leverages the IP-Adapter's capability to generate masks that segment subjects from backgrounds, applying these masks in a second inference pass to restrict token focus to the subject.

Result: The method improves alignment between the generated subject and the text prompt, notably enhancing image generation for prompts describing specific locations or places.

Conclusion: Utilizing automated masking during a secondary inference phase leads to higher fidelity in prompt and source image alignment, outperforming other personalization techniques.

Abstract: Personalizing diffusion models allows users to generate new images that
incorporate a given subject, allowing more control than a text prompt. These
models often suffer somewhat when they end up just recreating the subject
image, and ignoring the text prompt. We observe that one popular method for
personalization, the IP-Adapter automatically generates masks that we
definitively segment the subject from the background during inference. We
propose to use this automatically generated mask on a second pass to mask the
image tokens, thus restricting them to the subject, not the background,
allowing the text prompt to attend to the rest of the image. For text prompts
describing locations and places, this produces images that accurately depict
the subject while definitively matching the prompt. We compare our method to a
few other test time personalization methods, and find our method displays high
prompt and source image alignment.

</details>


### [79] [Automatic Text Box Placement for Supporting Typographic Design](https://arxiv.org/abs/2510.07665)
*Jun Muraoka,Daichi Haraguchi,Naoto Inoue,Wataru Shimoda,Kota Yamaguchi,Seiichi Uchida*

Main category: cs.CV

TL;DR: This paper evaluates automated text box placement in layouts using several models and finds that task-specific architectures like Transformers outperform Vision and Language Models, though challenges remain with small or dense layouts.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for balancing visual appeal and communication efficiency in layout designs for advertisements and web pages.

Method: It compares different model types (Transformers and Vision and Language Models) using evaluations on the Crello dataset.

Result: Standard Transformer-based models outperform Vision and Language Models, especially with richer appearance data, but struggle with very small text or crowded layouts.

Conclusion: Task-specific architectures, such as Transformers, offer benefits for automated layout design, but further improvements are needed to handle complex scenarios like small or dense text boxes.

Abstract: In layout design for advertisements and web pages, balancing visual appeal
and communication efficiency is crucial. This study examines automated text box
placement in incomplete layouts, comparing a standard Transformer-based method,
a small Vision and Language Model (Phi3.5-vision), a large pretrained VLM
(Gemini), and an extended Transformer that processes multiple images.
Evaluations on the Crello dataset show the standard Transformer-based models
generally outperform VLM-based approaches, particularly when incorporating
richer appearance information. However, all methods face challenges with very
small text or densely populated layouts. These findings highlight the benefits
of task-specific architectures and suggest avenues for further improvement in
automated layout design.

</details>


### [80] [TCIP: Threshold-Controlled Iterative Pyramid Network for Deformable Medical Image Registration](https://arxiv.org/abs/2510.07666)
*Heming Wu,Di Wang,Tai Ma,Peng Zhao,Yubin Xiao,Zhongke Wu,Xing-Ce Wang,Chuang Li,Xuan Wu,You Zhou*

Main category: cs.CV

TL;DR: The paper introduces the Feature-Enhanced Residual Module (FERM) and the Threshold-Controlled Iterative strategy (TCI) to improve deformable medical image registration, providing enhanced accuracy with a compact model size.


<details>
  <summary>Details</summary>
Motivation: Deformable medical image registration suffers from issues like propagation of anatomical misalignments and inefficient optimization iterations. This paper aims to address these limitations.

Method: The authors propose FERM, a module that refines semantic features in decoding layers, and TCI, a strategy to adaptively optimize iteration numbers based on stability and convergence measures.

Result: The TCIP model, integrating FERM and TCI, outperformed existing state-of-the-art networks in registration accuracy across brain MRI and abdomen CT datasets while maintaining competitive efficiency.

Conclusion: FERM and TCI effectively enhance model performance and generalizability, mitigating anatomical misalignments and adapting to varied deformation requirements in medical image registration.

Abstract: Although pyramid networks have demonstrated superior performance in
deformable medical image registration, their decoder architectures are
inherently prone to propagating and accumulating anatomical structure
misalignments. Moreover, most existing models do not adaptively determine the
number of iterations for optimization under varying deformation requirements
across images, resulting in either premature termination or excessive
iterations that degrades registration accuracy. To effectively mitigate the
accumulation of anatomical misalignments, we propose the Feature-Enhanced
Residual Module (FERM) as the core component of each decoding layer in the
pyramid network. FERM comprises three sequential blocks that extract anatomical
semantic features, learn to suppress irrelevant features, and estimate the
final deformation field, respectively. To adaptively determine the number of
iterations for varying images, we propose the dual-stage Threshold-Controlled
Iterative (TCI) strategy. In the first stage, TCI assesses registration
stability and with asserted stability, it continues with the second stage to
evaluate convergence. We coin the model that integrates FERM and TCI as
Threshold-Controlled Iterative Pyramid (TCIP). Extensive experiments on three
public brain MRI datasets and one abdomen CT dataset demonstrate that TCIP
outperforms the state-of-the-art (SOTA) registration networks in terms of
accuracy, while maintaining comparable inference speed and a compact model
parameter size. Finally, we assess the generalizability of FERM and TCI by
integrating them with existing registration networks and further conduct
ablation studies to validate the effectiveness of these two proposed methods.

</details>


### [81] [Controllable Video Synthesis via Variational Inference](https://arxiv.org/abs/2510.07670)
*Haoyi Duan,Yunzhi Zhang,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: This paper introduces a method for video synthesis enabling high controllability over specified elements while preserving diversity for under-specified ones, using variational inference and step-wise KL divergence minimization.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle to handle mixed user controls, such as granular trajectory data alongside coarse text prompts, necessitating a new approach for flexible controllability and diversity.

Method: The authors employ variational inference to approximate composed distributions, utilize multiple video generation backbones to manage constraints, and implement a step-wise KL divergence minimization with a context-conditioned factorization.

Result: The proposed method showcases improved controllability, diversity, and 3D consistency in generated videos compared to previous models.

Conclusion: The approach effectively combines distinct user controls and constraints for video synthesis, overcoming optimization challenges and outperforming prior methods.

Abstract: Many video workflows benefit from a mixture of user controls with varying
granularity, from exact 4D object trajectories and camera paths to coarse text
prompts, while existing video generative models are typically trained for fixed
input formats. We develop a video synthesis method that addresses this need and
generates samples with high controllability for specified elements while
maintaining diversity for under-specified ones. We cast the task as variational
inference to approximate a composed distribution, leveraging multiple video
generation backbones to account for all task constraints collectively. To
address the optimization challenge, we break down the problem into step-wise KL
divergence minimization over an annealed sequence of distributions, and further
propose a context-conditioned factorization technique that reduces modes in the
solution space to circumvent local optima. Experiments suggest that our method
produces samples with improved controllability, diversity, and 3D consistency
compared to prior works.

</details>


### [82] [Hybrid CNN-BYOL Approach for Fault Detection in Induction Motors Using Thermal Images](https://arxiv.org/abs/2510.07692)
*Tangin Amir Smrity,MD Zahin Muntaqim Hasan Muhammad Kafi,Abu Saleh Musa Miah,Najmul Hassan,Yuichi Okuyama,Nobuyoshi Asai,Taro Suzuki,Jungpil Shin*

Main category: cs.CV

TL;DR: The study introduces BYOL-IMNet, a hybrid CNN model integrated with BYOL for fault detection in induction motors using thermal images, achieving superior accuracy and inference speed.


<details>
  <summary>Details</summary>
Motivation: Fault detection in induction motors is crucial to prevent overheating, energy inefficiencies, and operational failures.

Method: The researchers used thermal image datasets and tested multiple deep learning models integrated with BYOL for fault detection. They introduced BYOL-IMNet as a custom-designed lightweight CNN model.

Result: BYOL-IMNet achieved a test accuracy of 99.89% and an inference speed of 5.7 ms per image, surpassing existing models.

Conclusion: The proposed BYOL-IMNet model provides a robust, accurate, and efficient solution for fault detection in induction motors, aiding industrial online monitoring systems.

Abstract: Induction motors (IMs) are indispensable in industrial and daily life, but
they are susceptible to various faults that can lead to overheating, wasted
energy consumption, and service failure. Early detection of faults is essential
to protect the motor and prolong its lifespan. This paper presents a hybrid
method that integrates BYOL with CNNs for classifying thermal images of
induction motors for fault detection. The thermal dataset used in this work
includes different operating states of the motor, such as normal operation,
overload, and faults. We employed multiple deep learning (DL) models for the
BYOL technique, ranging from popular architectures such as ResNet-50,
DenseNet-121, DenseNet-169, EfficientNetB0, VGG16, and MobileNetV2.
Additionally, we introduced a new high-performance yet lightweight CNN model
named BYOL-IMNet, which comprises four custom-designed blocks tailored for
fault classification in thermal images. Our experimental results demonstrate
that the proposed BYOL-IMNet achieves 99.89\% test accuracy and an inference
time of 5.7 ms per image, outperforming state-of-the-art models. This study
highlights the promising performance of the CNN-BYOL hybrid method in enhancing
accuracy for detecting faults in induction motors, offering a robust
methodology for online monitoring in industrial settings.

</details>


### [83] [Mutual Learning for Hashing: Unlocking Strong Hash Functions from Weak Supervision](https://arxiv.org/abs/2510.07703)
*Xiaoxu Ma,Runhao Li,Zhenyu Weng*

Main category: cs.CV

TL;DR: This paper introduces Mutual Learning for Hashing (MLH), enhancing hashing performance by combining strengths of pairwise-based and center-based methods.


<details>
  <summary>Details</summary>
Motivation: The need to address weaknesses in center-based and pairwise-based hashing methods for optimizing large-scale image retrieval.

Method: Developing a mutual learning framework with two branches—center-based and pairwise-based—alongside a mixture-of-hash-experts module for cross-branch interaction.

Result: MLH consistently delivers superior results compared to state-of-the-art hashing methods on various benchmark datasets.

Conclusion: By combining local similarity cues and global data structures, MLH bridges the gap between pairwise-based and center-based approaches for improved image retrieval.

Abstract: Deep hashing has been widely adopted for large-scale image retrieval, with
numerous strategies proposed to optimize hash function learning. Pairwise-based
methods are effective in learning hash functions that preserve local similarity
relationships, whereas center-based methods typically achieve superior
performance by more effectively capturing global data distributions. However,
the strength of center-based methods in modeling global structures often comes
at the expense of underutilizing important local similarity information. To
address this limitation, we propose Mutual Learning for Hashing (MLH), a novel
weak-to-strong framework that enhances a center-based hashing branch by
transferring knowledge from a weaker pairwise-based branch. MLH consists of two
branches: a strong center-based branch and a weaker pairwise-based branch.
Through an iterative mutual learning process, the center-based branch leverages
local similarity cues learned by the pairwise-based branch. Furthermore,
inspired by the mixture-of-experts paradigm, we introduce a novel
mixture-of-hash-experts module that enables effective cross-branch interaction,
further enhancing the performance of both branches. Extensive experiments
demonstrate that MLH consistently outperforms state-of-the-art hashing methods
across multiple benchmark datasets.

</details>


### [84] [RePainter: Empowering E-commerce Object Removal via Spatial-matting Reinforcement Learning](https://arxiv.org/abs/2510.07721)
*Zipeng Guo,Lichen Ma,Xiaolong Fu,Gaojing Zhou,Lan Yang,Yuchen Zhou,Linkai Liu,Yu He,Ximan Liu,Shiping Dong,Jingling Fu,Zhen Chen,Yu Shi,Junshi Huang,Jason Li,Chao Gou*

Main category: cs.CV

TL;DR: The paper proposes Repainter, a novel reinforcement learning framework for enhancing product image inpainting on e-commerce platforms, outperforming current state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Intrusive elements like watermarks and promotional text diminish the quality of product visuals on e-commerce platforms, necessitating effective inpainting solutions.

Method: Repainter leverages reinforcement learning using spatial-matting trajectory refinement and Group Relative Policy Optimization (GRPO), alongside a composite reward mechanism balancing constraints.

Result: Extensive experiments show Repainter performs significantly better than state-of-the-art methods, especially in complex scenes, and introduces new benchmarks and datasets for evaluation.

Conclusion: Repainter offers a promising solution for improving e-commerce image quality by effectively addressing visual artifacts and undesired object insertion challenges in inpainting tasks.

Abstract: In web data, product images are central to boosting user engagement and
advertising efficacy on e-commerce platforms, yet the intrusive elements such
as watermarks and promotional text remain major obstacles to delivering clear
and appealing product visuals. Although diffusion-based inpainting methods have
advanced, they still face challenges in commercial settings due to unreliable
object removal and limited domain-specific adaptation. To tackle these
challenges, we propose Repainter, a reinforcement learning framework that
integrates spatial-matting trajectory refinement with Group Relative Policy
Optimization (GRPO). Our approach modulates attention mechanisms to emphasize
background context, generating higher-reward samples and reducing unwanted
object insertion. We also introduce a composite reward mechanism that balances
global, local, and semantic constraints, effectively reducing visual artifacts
and reward hacking. Additionally, we contribute EcomPaint-100K, a high-quality,
large-scale e-commerce inpainting dataset, and a standardized benchmark
EcomPaint-Bench for fair evaluation. Extensive experiments demonstrate that
Repainter significantly outperforms state-of-the-art methods, especially in
challenging scenes with intricate compositions. We will release our code and
weights upon acceptance.

</details>


### [85] [SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction](https://arxiv.org/abs/2510.07723)
*Wenyue Chen,Peng Li,Wangguandong Zheng,Chengfeng Zhao,Mengfei Li,Yaolong Zhu,Zhiyang Dou,Ronggang Wang,Yuan Liu*

Main category: cs.CV

TL;DR: The paper introduces SyncHuman, a novel framework for photorealistic 3D human reconstruction from a single image by combining 2D multiview and 3D native generative models, overcoming challenges like ambiguity and occlusion.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address limitations in existing methods for 3D human reconstruction, such as inaccurate 3D priors and difficulties in handling challenging poses and fine details.

Method: SyncHuman integrates 2D multiview and 3D native generative models using a synchronization attention mechanism, and then refines details using a feature injection mechanism.

Result: The framework achieves robust, photo-realistic 3D human reconstructions, especially for complex poses, and outperforms baseline methods in both geometric accuracy and visual quality.

Conclusion: SyncHuman presents a promising approach for high-fidelity 3D reconstruction, combining the complementary strengths of 2D and 3D generative models for superior results.

Abstract: Photorealistic 3D full-body human reconstruction from a single image is a
critical yet challenging task for applications in films and video games due to
inherent ambiguities and severe self-occlusions. While recent approaches
leverage SMPL estimation and SMPL-conditioned image generative models to
hallucinate novel views, they suffer from inaccurate 3D priors estimated from
SMPL meshes and have difficulty in handling difficult human poses and
reconstructing fine details. In this paper, we propose SyncHuman, a novel
framework that combines 2D multiview generative model and 3D native generative
model for the first time, enabling high-quality clothed human mesh
reconstruction from single-view images even under challenging human poses.
Multiview generative model excels at capturing fine 2D details but struggles
with structural consistency, whereas 3D native generative model generates
coarse yet structurally consistent 3D shapes. By integrating the complementary
strengths of these two approaches, we develop a more effective generation
framework. Specifically, we first jointly fine-tune the multiview generative
model and the 3D native generative model with proposed pixel-aligned 2D-3D
synchronization attention to produce geometrically aligned 3D shapes and 2D
multiview images. To further improve details, we introduce a feature injection
mechanism that lifts fine details from 2D multiview images onto the aligned 3D
shapes, enabling accurate and high-fidelity reconstruction. Extensive
experiments demonstrate that SyncHuman achieves robust and photo-realistic 3D
human reconstruction, even for images with challenging poses. Our method
outperforms baseline methods in geometric accuracy and visual fidelity,
demonstrating a promising direction for future 3D generation models.

</details>


### [86] [ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes](https://arxiv.org/abs/2510.07729)
*Jian Gao,Mengqi Yuan,Yifei Zeng,Chang Zeng,Zhihao Li,Zhenyu Chen,Weichao Qiu,Xiao-Xiao Long,Hao Zhu,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: The paper proposes a novel framework, ComGS, for improving 3D object-scene composition, achieving high-quality, real-time rendering with vivid shadows and efficient editing.


<details>
  <summary>Details</summary>
Motivation: The authors identified challenges in combining objects and scenes in Gaussian Splatting due to baked appearance and shadow inconsistencies.

Method: The method involves using Surface Octahedral Probes (SOPs) for efficient lighting and shadow computation, along with environment lighting estimation using radiance field reconstruction and a fine-tuned diffusion model.

Result: ComGS achieves real-time rendering at 28 FPS, harmonious visual composition, and fast editing within 36 seconds, enabling immersive 3D object-scene integration.

Conclusion: The research demonstrates that ComGS overcomes inefficiencies and visual inconsistencies in Gaussian-based inverse rendering, providing a practical and effective solution for realistic 3D compositions.

Abstract: Gaussian Splatting (GS) enables immersive rendering, but realistic 3D
object-scene composition remains challenging. Baked appearance and shadow
information in GS radiance fields cause inconsistencies when combining objects
and scenes. Addressing this requires relightable object reconstruction and
scene lighting estimation. For relightable object reconstruction, existing
Gaussian-based inverse rendering methods often rely on ray tracing, leading to
low efficiency. We introduce Surface Octahedral Probes (SOPs), which store
lighting and occlusion information and allow efficient 3D querying via
interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x
speedup in reconstruction and enable real-time shadow computation in Gaussian
scenes. For lighting estimation, existing Gaussian-based inverse rendering
methods struggle to model intricate light transport and often fail in complex
scenes, while learning-based methods predict lighting from a single image and
are viewpoint-sensitive. We observe that 3D object-scene composition primarily
concerns the object's appearance and nearby shadows. Thus, we simplify the
challenging task of full scene lighting estimation by focusing on the
environment lighting at the object's placement. Specifically, we capture a 360
degrees reconstructed radiance field of the scene at the location and fine-tune
a diffusion model to complete the lighting. Building on these advances, we
propose ComGS, a novel 3D object-scene composition framework. Our method
achieves high-quality, real-time rendering at around 28 FPS, produces visually
harmonious results with vivid shadows, and requires only 36 seconds for
editing. Code and dataset are available at
https://nju-3dv.github.io/projects/ComGS/.

</details>


### [87] [UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes](https://arxiv.org/abs/2510.07741)
*Yuang Meng,Xin Jin,Lina Lei,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

TL;DR: The paper presents UltraLED, a method to reconstruct UHDR scenes from a single short-exposure RAW image, avoiding ghosting issues and outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of capturing ultra-high dynamic range (UHDR) scenes, particularly the difficulty in preserving highlight and shadow details in dynamic environments.

Method: UltraLED is a two-stage framework; it applies exposure correction via a ratio map to manage dynamic range and uses a brightness-aware RAW denoiser to recover details in dark areas. This approach relies solely on a single short-exposure RAW image for robustness in dynamic scenes.

Result: Extensive experiments demonstrate that UltraLED outperforms current single-frame UHDR reconstruction methods, effectively preserving details across dynamic ranges.

Conclusion: UltraLED provides a robust and superior method for UHDR scene reconstruction using a single short-exposure RAW frame, avoiding issues like ghosting, and makes its code and dataset publicly available for further research.

Abstract: Ultra-high dynamic range (UHDR) scenes exhibit significant exposure
disparities between bright and dark regions. Such conditions are commonly
encountered in nighttime scenes with light sources. Even with standard exposure
settings, a bimodal intensity distribution with boundary peaks often emerges,
making it difficult to preserve both highlight and shadow details
simultaneously. RGB-based bracketing methods can capture details at both ends
using short-long exposure pairs, but are susceptible to misalignment and
ghosting artifacts. We found that a short-exposure image already retains
sufficient highlight detail. The main challenge of UHDR reconstruction lies in
denoising and recovering information in dark regions. In comparison to the RGB
images, RAW images, thanks to their higher bit depth and more predictable noise
characteristics, offer greater potential for addressing this challenge. This
raises a key question: can we learn to see everything in UHDR scenes using only
a single short-exposure RAW image? In this study, we rely solely on a single
short-exposure frame, which inherently avoids ghosting and motion blur, making
it particularly robust in dynamic scenes. To achieve that, we introduce
UltraLED, a two-stage framework that performs exposure correction via a ratio
map to balance dynamic range, followed by a brightness-aware RAW denoiser to
enhance detail recovery in dark regions. To support this setting, we design a
9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a
corresponding dataset based on diverse scenes, using only the shortest exposure
as input for reconstruction. Extensive experiments show that UltraLED
significantly outperforms existing single-frame approaches. Our code and
dataset are made publicly available at
https://srameo.github.io/projects/ultraled.

</details>


### [88] [DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream](https://arxiv.org/abs/2510.07752)
*Junhao He,Jiaxu Wang,Jia Li,Mingyuan Sun,Qiang Zhang,Jiahang Cao,Ziyi Zhang,Yi Gu,Jingkai Sun,Renjing Xu*

Main category: cs.CV

TL;DR: This paper proposes a novel framework for reconstructing dynamic 3D Gaussian Splatting (3DGS) using low-framerate RGB videos combined with high-framerate event streams, addressing the challenges of large inter-frame motions and modal discrepancies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the difficulty of reconstructing dynamic 3DGS from low-framerate RGB videos, where large inter-frame motions can increase solution uncertainty. Event cameras, which provide robust motion data, are leveraged to reduce this uncertainty.

Method: The proposed framework combines RGB images and event data, using motion priors derived from event streams to guide the optimization of deformation fields. Key components include the LoCM unsupervised fine-tuning framework for extracting motion priors, and a geometry-aware data association method to establish motion correspondences.

Result: Experiments on synthetic and real scenes demonstrate that the method outperforms existing approaches based on either image or event data, showing effective optimization of dynamic 3DGS using the modal combination.

Conclusion: Integrating RGB videos with event camera data allows for improved reconstruction of dynamic 3DGS, overcoming large motion challenges and modal discrepancies through the proposed framework.

Abstract: Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB
videos is challenging. This is because large inter-frame motions will increase
the uncertainty of the solution space. For example, one pixel in the first
frame might have more choices to reach the corresponding pixel in the second
frame. Event cameras can asynchronously capture rapid visual changes and are
robust to motion blur, but they do not provide color information. Intuitively,
the event stream can provide deterministic constraints for the inter-frame
large motion by the event trajectories. Hence, combining
low-temporal-resolution images with high-framerate event streams can address
this challenge. However, it is challenging to jointly optimize Dynamic 3DGS
using both RGB and event modalities due to the significant discrepancy between
these two data modalities. This paper introduces a novel framework that jointly
optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event
motion priors to guide the optimization of the deformation fields. First, we
extract the motion priors encoded in event streams by using the proposed LoCM
unsupervised fine-tuning framework to adapt an event flow estimator to a
certain unseen scene. Then, we present the geometry-aware data association
method to build the event-Gaussian motion correspondence, which is the primary
foundation of the pipeline, accompanied by two useful strategies, namely motion
decomposition and inter-frame pseudo-label. Extensive experiments show that our
method outperforms existing image and event-based approaches across synthetic
and real scenes and prove that our method can effectively optimize dynamic 3DGS
with the help of event data.

</details>


### [89] [Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets and Explainable AI (XAI): A Comparative Analysis](https://arxiv.org/abs/2510.07785)
*Ming Jie Ong,Sze Yinn Ung,Sim Kuan Goh,Jimmy Y. Zhong*

Main category: cs.CV

TL;DR: This study explored the use of Explainable Artificial Intelligence (XAI) with UNet models for brain tumor segmentation in MRI images, evaluating ResUNet as the best model.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve the accuracy and trust in AI-driven brain tumor segmentation, aiding physicians in clinical decisions.

Method: Three UNet models (UNet, ResUNet, AttUNet) were tested using Gradient-weighted Class Activation Mapping (Grad-CAM) and attention-based visualization techniques. Performance metrics included Dice similarity, Jaccard similarity, accuracy, recall, and F1 scores.

Result: ResUNet outperformed other models in segmentation metrics, and XAI techniques clarified model decisions, offering visuospatial insights and attention module explanations.

Conclusion: ResUNet is recommended for automated brain tumor segmentation due to its superior performance and explainability features, with implications for future clinical use.

Abstract: The current study investigated the use of Explainable Artificial Intelligence
(XAI) to improve the accuracy of brain tumor segmentation in MRI images, with
the goal of assisting physicians in clinical decision-making. The study focused
on applying UNet models for brain tumor segmentation and using the XAI
techniques of Gradient-weighted Class Activation Mapping (Grad-CAM) and
attention-based visualization to enhance the understanding of these models.
Three deep learning models - UNet, Residual UNet (ResUNet), and Attention UNet
(AttUNet) - were evaluated to identify the best-performing model. XAI was
employed with the aims of clarifying model decisions and increasing physicians'
trust in these models. We compared the performance of two UNet variants
(ResUNet and AttUNet) with the conventional UNet in segmenting brain tumors
from the BraTS2020 public dataset and analyzed model predictions with Grad-CAM
and attention-based visualization. Using the latest computer hardware, we
trained and validated each model using the Adam optimizer and assessed their
performance with respect to: (i) training, validation, and inference times,
(ii) segmentation similarity coefficients and loss functions, and (iii)
classification performance. Notably, during the final testing phase, ResUNet
outperformed the other models with respect to Dice and Jaccard similarity
scores, as well as accuracy, recall, and F1 scores. Grad-CAM provided
visuospatial insights into the tumor subregions each UNet model focused on
while attention-based visualization provided valuable insights into the working
mechanisms of AttUNet's attention modules. These results demonstrated ResUNet
as the best-performing model and we conclude by recommending its use for
automated brain tumor segmentation in future clinical assessments. Our source
code and checkpoint are available at
https://github.com/ethanong98/MultiModel-XAI-Brats2020

</details>


### [90] [GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.07791)
*Qinghongbing Xie,Zhaoyuan Xia,Feng Zhu,Lijun Gong,Ziyue Li,Rui Zhao,Long Zeng*

Main category: cs.CV

TL;DR: The paper introduces Geo-Temporal Reasoning benchmark (GTR-Bench) to address deficiencies in Visual-Language Models (VLMs) for geographic spatial-temporal intelligence and evaluates current models, revealing significant gaps compared to human performance.


<details>
  <summary>Details</summary>
Motivation: To develop a benchmark that assesses geographic spatial-temporal intelligence of VLMs using both images/videos and graphics contexts, targeting areas like traffic management and emergency response.

Method: GTR-Bench challenges models with tasks requiring perspective switches between maps and videos, multi-view video reasoning, and inference over unobserved spatial-temporal regions. Popular VLMs were evaluated on this benchmark.

Result: Models tested, such as Gemini-2.5-Pro, showed significantly lower performance (34.9%) compared to human benchmarks (78.61%). The analysis identified three major deficiencies in current VLMs: imbalanced spatial-temporal context utilization, weak temporal forecasting, and poor map-video alignment capability.

Conclusion: GTR-Bench highlights critical areas for improvement in VLMs' geographic temporal reasoning approaches and offers directions for future research. Its release will support advancements in spatial-temporal intelligence applications.

Abstract: Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has
attracted much attention due to its importance for Autonomous Driving, Embodied
AI and General Artificial Intelligence. Existing spatial-temporal benchmarks
mainly focus on egocentric perspective reasoning with images/video context, or
geographic perspective reasoning with graphics context (eg. a map), thus fail
to assess VLMs' geographic spatial-temporal intelligence with both images/video
and graphics context, which is important for areas like traffic management and
emergency response. To address the gaps, we introduce Geo-Temporal Reasoning
benchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of
moving targets in a large-scale camera network. GTR-Bench is more challenging
as it requires multiple perspective switches between maps and videos, joint
reasoning across multiple videos with non-overlapping fields of view, and
inference over spatial-temporal regions that are unobserved by any video
context. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that
even the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags
behind human performance (78.61%) on geo-temporal reasoning. Moreover, our
comprehensive analysis on GTR-Bench reveals three primary deficiencies of
current models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by
an imbalanced utilization of spatial-temporal context. (2) VLMs are weak in
temporal forecasting, which leads to worse performance on temporal-emphasized
tasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to
comprehend or align the map data with multi-view video inputs. We believe
GTR-Bench offers valuable insights and opens up new opportunities for research
and applications in spatial-temporal intelligence. Benchmark and code will be
released at https://github.com/X-Luffy/GTR-Bench.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [91] [Adaptive Execution Scheduler for DataDios SmartDiff](https://arxiv.org/abs/2510.07811)
*Aryan Poduri*

Main category: cs.DC

TL;DR: The paper introduces an adaptive scheduler for SmartDiff that dynamically tunes execution settings to improve latency and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to optimize performance in a differencing engine, SmartDiff, by reducing p95 latency and memory usage within fixed CPU and memory budgets.

Method: The approach involves an adaptive scheduler using an in-memory and Dask-mode hybrid execution. It employs lightweight preflight profiling, an online cost model, and a guarded hill-climb policy for decision-making.

Result: The scheduler achieves a 23-28% p95 latency reduction over tuned heuristics, 35-40% over grid baselines, and reduces peak memory usage by 16-22% (or 25-32% over fixed baselines). There were no out-of-memory errors and throughput remained comparable.

Conclusion: The proposed adaptive scheduler effectively balances execution settings to achieve significant improvements in latency and memory usage, making it robust and efficient for use in tabular data processing.

Abstract: We present an adaptive scheduler for a single differencing engine (SmartDiff)
with two execution modes: (i) in-memory threads and (ii) Dask based
parallelism. The scheduler continuously tunes batch size and worker/thread
count within fixed CPU and memory budgets to minimize p95 latency. A
lightweight preflight profiler estimates bytes/row and I/O rate; an online
cost/memory model prunes unsafe actions; and a guarded hill-climb policy favors
lower latency with backpressure and straggler mitigation. Backend selection is
gated by a conservative working-set estimate so that in-memory execution is
chosen when safe, otherwise Dask is used. Across synthetic and public tabular
benchmarks, the scheduler reduces p95 latency by 23 to 28 percent versus a
tuned warm-up heuristic (and by 35 to 40 percent versus fixed grid baselines),
while lowering peak memory by 16 to 22 percent (25 to 32 percent vs. fixed)
with zero OOMs and comparable throughput.

</details>


### [92] [A Multi-Simulation Bridge for IoT Digital Twins](https://arxiv.org/abs/2510.08164)
*Marco Picone,Samuele Burattini,Marco Melloni,Prasad Talasila,Davide Ziglioli,Matteo Martinelli,Nicola Bicocchi,Alessandro Ricci,Peter Gorm Larsen*

Main category: cs.DC

TL;DR: The paper presents a software framework called DT Simulation Bridge for integrating Digital Twins (DTs) with simulation platforms, improving system design, validation, and real-time operation.


<details>
  <summary>Details</summary>
Motivation: The increasing potential of Digital Twins in IoT and IIoT requires efficient integration with simulations to optimize design, validation, and operations.

Method: The DT Simulation Bridge enables bidirectional data exchange between DTs and simulation environments, incorporating flexibility and scalability via architectural design and software components.

Result: Experimental evaluation reveals enhanced design agility, virtual commissioning, and live behavioral analysis in industrial scenarios using the DT Simulation Bridge.

Conclusion: The paper concludes that the DT Simulation Bridge effectively bridges DTs and simulations, optimizing its application across diverse industrial use cases.

Abstract: The increasing capabilities of Digital Twins (DTs) in the context of the
Internet of Things (IoT) and Industrial IoT (IIoT) call for seamless
integration with simulation platforms to support system design, validation, and
real-time operation. This paper introduces the concept, design, and
experimental evaluation of the DT Simulation Bridge - a software framework that
enables diverse interaction patterns between active DTs and simulation
environments. The framework supports both the DT development lifecycle and the
incorporation of simulations during active operation. Through bidirectional
data exchange, simulations can update DT models dynamically, while DTs provide
real-time feedback to adapt simulation parameters. We describe the
architectural design and core software components that ensure flexible
interoperability and scalable deployment. Experimental results show that the DT
Simulation Bridge enhances design agility, facilitates virtual commissioning,
and supports live behavioral analysis under realistic conditions, demonstrating
its effectiveness across a range of industrial scenarios.

</details>


### [93] [Towards Energy-Efficient Serverless Computing with Hardware Isolation](https://arxiv.org/abs/2510.08180)
*Natalie Carl,Tobias Pfandzelter,David Bermbach*

Main category: cs.DC

TL;DR: The paper explores how serverless architectures can be re-engineered to improve energy efficiency, proposing hardware isolation instead of software isolation, achieving significant energy savings.


<details>
  <summary>Details</summary>
Motivation: Traditional serverless platforms overprovision hardware and rely on expensive software isolation, leading to inefficiencies, especially in terms of energy consumption.

Method: The authors propose a hardware-centric approach where individual processors are allocated per function for serverless architectures, resulting in a system optimized for energy efficiency.

Result: Their preliminary evaluation showed that their proposed approach could decrease energy consumption overheads by 90.63% (or 70.8MW on average) in typical serverless workloads.

Conclusion: Rethinking hardware architecture to integrate hardware isolation for serverless platforms is a promising way to drastically cut energy overheads and enhance overall efficiency.

Abstract: Serverless computing provides just-in-time infrastructure provisioning with
rapid elasticity and a finely-grained pricing model. As full control of
resource allocation is in the hands of the cloud provider and applications only
consume resources when they actually perform work, we believe that serverless
computing is uniquely positioned to maximize energy efficiency.
  However, the focus of current serverless platforms is to run hundreds or
thousands of serverless functions from different tenants on traditional server
hardware, requiring expensive software isolation mechanisms and a high degree
of overprovisioning, i.e., idle servers, to anticipate load spikes. With shared
caches, high clock frequencies, and many-core architectures, servers today are
optimized for large, singular workloads but not to run thousands of isolated
functions.
  We propose rethinking the serverless hardware architecture to align it with
the requirements of serverless software. Specifically, we propose using
hardware isolation with individual processors per function instead of software
isolation resulting in a serverless hardware stack that consumes energy only
when an application actually performs work. In preliminary evaluation with real
hardware and a typical serverless workload we find that this could reduce
energy consumption overheads by 90.63% or an average 70.8MW.

</details>


### [94] [Distributed Resource Selection for Self-Organising Cloud-Edge Systems](https://arxiv.org/abs/2510.08228)
*Quentin Renau,Amjad Ullah,Emma Hart*

Main category: cs.DC

TL;DR: The paper proposes a distributed resource selection mechanism for cloud-edge environments to achieve efficient, scalable, and resilient resource allocation without relying on centralized systems.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiencies and limitations of centralized resource allocation in dynamic and complex cloud-edge environments.

Method: Introduces a consensus-based mechanism leveraging local knowledge and inter-agent collaboration for resource selection.

Result: Demonstrated rapid resource allocations without compromising optimality, achieving scalability and faster results compared to centralized approaches.

Conclusion: The proposed distributed mechanism significantly enhances resource allocation in cloud-edge environments by being efficient, scalable, and independent of central controllers.

Abstract: This paper presents a distributed resource selection mechanism for diverse
cloud-edge environments, enabling dynamic and context-aware allocation of
resources to meet the demands of complex distributed applications. By
distributing the decision-making process, our approach ensures efficiency,
scalability, and resilience in highly dynamic cloud-edge environments where
centralised coordination becomes a bottleneck. The proposed mechanism aims to
function as a core component of a broader, distributed, and self-organising
orchestration system that facilitates the intelligent placement and adaptation
of applications in real-time. This work leverages a consensus-based mechanism
utilising local knowledge and inter-agent collaboration to achieve efficient
results without relying on a central controller, thus paving the way for
distributed orchestration. Our results indicate that computation time is the
key factor influencing allocation decisions. Our approach consistently delivers
rapid allocations without compromising optimality or incurring additional cost,
achieving timely results at scale where exhaustive search is infeasible and
centralised heuristics run up to 30 times slower.

</details>


### [95] [Energy-Efficient Maximal Independent Sets in Radio Networks](https://arxiv.org/abs/2510.08244)
*Dominick Banasik,Varsha Dani,Fabien Dufoulon,Aayush Gupta,Thomas P. Hayes,Gopal Pandurangan*

Main category: cs.DC

TL;DR: This paper proposes new energy-efficient algorithms for solving the Maximal Independent Set (MIS) problem in wireless network models, minimizing energy usage critical for battery-powered systems.


<details>
  <summary>Details</summary>
Motivation: To design energy-efficient distributed MIS algorithms for wireless networks, where conserving energy is crucial due to battery limitations.

Method: Developing and presenting randomized distributed MIS algorithms for the Radio Network model, including two models: with and without collision detection. The algorithms are evaluated based on energy complexity, round complexity, and failure probability.

Result: In the CD model, the algorithm achieves optimal energy complexity $O(\log n)$, with $O(\log^2 n)$ round complexity. In the no-CD model, the energy complexity is $O(\log^2n \log \log n)$, significantly outperforming prior algorithms.

Conclusion: The study advances energy-efficient distributed computing in wireless networks by designing optimal and near-optimal algorithms, demonstrating progress in both theoretical and practical aspects of the MIS problem.

Abstract: The maximal independent set (MIS) is one of the most fundamental problems in
distributed computing, and it has been studied intensively for over four
decades. This paper focuses on the MIS problem in the Radio Network model, a
standard model widely used to model wireless networks, particularly ad hoc
wireless and sensor networks. Energy is a premium resource in these networks,
which are typically battery-powered. Hence, designing distributed algorithms
that use as little energy as possible is crucial. We use the well-established
energy model where a node can be sleeping or awake in a round, and only the
awake rounds (when it can send or listen) determine the energy complexity of
the algorithm, which we want to minimize.
  We present new, more energy-efficient MIS algorithms in radio networks with
arbitrary and unknown graph topology. We present algorithms for two popular
variants of the radio model -- with collision detection (CD) and without
collision detection (no-CD). Specifically, we obtain the following results:
  1. CD model: We present a randomized distributed MIS algorithm with energy
complexity $O(\log n)$, round complexity $O(\log^2 n)$, and failure probability
$1 / poly(n)$, where $n$ is the network size. We show that our energy
complexity is optimal by showing a matching $\Omega(\log n)$ lower bound.
  2. no-CD model: In the more challenging no-CD model, we present a randomized
distributed MIS algorithm with energy complexity $O(\log^2n \log \log n)$,
round complexity $O(\log^3 n \log \Delta)$, and failure probability $1 /
poly(n)$. The energy complexity of our algorithm is significantly lower than
the round (and energy) complexity of $O(\log^3 n)$ of the best known
distributed MIS algorithm of Davies [PODC 2023] for arbitrary graph topology.

</details>


### [96] [Investigating Matrix Repartitioning to Address the Over- and Undersubscription Challenge for a GPU-based CFD Solver](https://arxiv.org/abs/2510.08536)
*Gregor Olenik,Marcel Koch,Hartwig Anzt*

Main category: cs.DC

TL;DR: This paper introduces a repartitioning strategy to address GPU integration challenges in OpenFOAM, improving performance and resource utilization.


<details>
  <summary>Details</summary>
Motivation: To overcome performance and development effort trade-offs in integrating GPU acceleration into the OpenFOAM framework.

Method: Proposes a matrix repartitioning strategy and computational model balancing CPU matrix assembly with GPU linear solves, alongside a procedural update evaluation on large-scale simulations.

Result: The approach reduces oversubscription issues, thereby boosting solver performance and resource utilization in CPU-GPU environments.

Conclusion: The proposed framework effectively addresses limitations of plugin-based GPU integration for OpenFOAM, facilitating efficient HPC workflows.

Abstract: Modern high-performance computing (HPC) increasingly relies on GPUs, but
integrating GPU acceleration into complex scientific frameworks like OpenFOAM
remains a challenge. Existing approaches either fully refactor the codebase or
use plugin-based GPU solvers, each facing trade-offs between performance and
development effort. In this work, we address the limitations of plugin-based
GPU acceleration in OpenFOAM by proposing a repartitioning strategy that better
balances CPU matrix assembly and GPU-based linear solves. We present a detailed
computational model, describe a novel matrix repartitioning and update
procedure, and evaluate its performance on large-scale CFD simulations. Our
results show that the proposed method significantly mitigates oversubscription
issues, improving solver performance and resource utilization in heterogeneous
CPU-GPU environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [97] [Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children](https://arxiv.org/abs/2510.07320)
*Nelaka K. A. R,Peiris M. K. V,Liyanage R. P. B*

Main category: cs.LG

TL;DR: This study focuses on understanding behavioral and emotional patterns in autistic children to create evidence-based frameworks for better skill development, particularly in IT education.


<details>
  <summary>Details</summary>
Motivation: There is a critical lack of understanding of nuanced behavioral and emotional patterns in autistic children, which hinders the development of effective interventions and skill-building strategies.

Method: A longitudinal approach was used to monitor emotions and behaviors, analyzing trends over time to identify specific challenges faced by autistic students, especially in IT education.

Result: The research established foundational insights into the unique needs of autistic children and proposed a targeted framework for developing technical aids and applications to support these needs.

Conclusion: Prioritizing early identification and mapping of behavioral patterns can significantly improve the learning and developmental outcomes for autistic children, creating a more inclusive educational system.

Abstract: Autism Spectrum Disorder significantly influences the communication
abilities, learning processes, behavior, and social interactions of
individuals. Although early intervention and customized educational strategies
are critical to improving outcomes, there is a pivotal gap in understanding and
addressing nuanced behavioral patterns and emotional identification in autistic
children prior to skill development. This extended research delves into the
foundational step of recognizing and mapping these patterns as a prerequisite
to improving learning and soft skills. Using a longitudinal approach to monitor
emotions and behaviors, this study aims to establish a baseline understanding
of the unique needs and challenges faced by autistic students, particularly in
the Information Technology domain, where opportunities are markedly limited.
Through a detailed analysis of behavioral trends over time, we propose a
targeted framework for developing applications and technical aids designed to
meet these identified needs. Our research underscores the importance of a
sequential and evidence-based intervention approach that prioritizes a deep
understanding of each child's behavioral and emotional landscape as the basis
for effective skill development. By shifting the focus toward early
identification of behavioral patterns, we aim to foster a more inclusive and
supportive learning environment that can significantly improve the educational
and developmental trajectory of children with ASD.

</details>


### [98] [DUA-D2C: Dynamic Uncertainty Aware Method for Overfitting Remediation in Deep Learning](https://arxiv.org/abs/2411.15876)
*Md. Saiful Bari Siddiqui,Md Mohaiminul Islam,Md. Golam Rabiul Alam*

Main category: cs.LG

TL;DR: DUA-D2C improves overfitting in deep learning by dynamically weighting model contributions based on validation performance and prediction uncertainty.


<details>
  <summary>Details</summary>
Motivation: To address overfitting caused by data outliers, noise, and limited training data by improving upon the Divide2Conquer (D2C) method.

Method: DUA-D2C dynamically weights subset models during aggregation based on their validation performance and uncertainty, enabling better generalization.

Result: DUA-D2C significantly improves generalization performance across benchmark datasets and enhances decision boundaries and loss curves.

Conclusion: DUA-D2C is a theoretically sound and effective approach to overfitting, even when combined with existing regularization techniques.

Abstract: Overfitting remains a significant challenge in deep learning, often arising
from data outliers, noise, and limited training data. To address this, the
Divide2Conquer (D2C) method was previously proposed, which partitions training
data into multiple subsets and trains identical models independently on each.
This strategy enables learning more consistent patterns while minimizing the
influence of individual outliers and noise. However, D2C's standard aggregation
typically treats all subset models equally or based on fixed heuristics (like
data size), potentially underutilizing information about their varying
generalization capabilities. Building upon this foundation, we introduce
Dynamic Uncertainty-Aware Divide2Conquer (DUA-D2C), an advanced technique that
refines the aggregation process. DUA-D2C dynamically weights the contributions
of subset models based on their performance on a shared validation set,
considering both accuracy and prediction uncertainty. This intelligent
aggregation allows the central model to preferentially learn from subsets
yielding more generalizable and confident edge models, thereby more effectively
combating overfitting. Empirical evaluations on benchmark datasets spanning
multiple domains demonstrate that DUA-D2C significantly improves
generalization. Our analysis includes evaluations of decision boundaries, loss
curves, and other performance metrics, highlighting the effectiveness of
DUA-D2C. This study demonstrates that DUA-D2C improves generalization
performance even when applied on top of other regularization methods,
establishing it as a theoretically grounded and effective approach to combating
overfitting in modern deep learning. Our codes are publicly available at:
https://github.com/Saiful185/DUA-D2C.

</details>


### [99] [A Modality-Aware Cooperative Co-Evolutionary Framework for Multimodal Graph Neural Architecture Search](https://arxiv.org/abs/2510.07325)
*Sixuan Wang,Jiao Yin,Jinli Cao,Mingjian Tang,Yong-Feng Ge*

Main category: cs.LG

TL;DR: MACC-MGNAS introduces a novel cooperative algorithm to optimize multimodal graph neural networks for vulnerability prediction, achieving superior accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing Graph Neural Architecture Search (GNAS) methods that fail to account for modality heterogeneity while analyzing software vulnerabilities.

Method: Developed MACC-MGNAS, a modality-aware cooperative co-evolutionary framework, combined with surrogate evaluation (MADTS method) and a population diversity strategy (SPDI).

Result: On the VulCE dataset, MACC-MGNAS achieved an F1-score of 81.67%—an improvement of 8.7% in accuracy over state-of-the-art—while reducing computation costs by 27%.

Conclusion: MACC-MGNAS provides an effective and efficient solution for multimodal graph neural networks design, addressing co-exploitation attacks and improving prediction accuracy.

Abstract: Co-exploitation attacks on software vulnerabilities pose severe risks to
enterprises, a threat that can be mitigated by analyzing heterogeneous and
multimodal vulnerability data. Multimodal graph neural networks (MGNNs) are
well-suited to integrate complementary signals across modalities, thereby
improving attack-prediction accuracy. However, designing an effective MGNN
architecture is challenging because it requires coordinating modality-specific
components at each layer, which is infeasible through manual tuning. Genetic
algorithm (GA)-based graph neural architecture search (GNAS) provides a natural
solution, yet existing methods are confined to single modalities and overlook
modality heterogeneity. To address this limitation, we propose a modality-aware
cooperative co-evolutionary algorithm for multimodal graph neural architecture
search, termed MACC-MGNAS. First, we develop a modality-aware cooperative
co-evolution (MACC) framework under a divide-and-conquer paradigm: a
coordinator partitions a global chromosome population into modality-specific
gene groups, local workers evolve them independently, and the coordinator
reassembles chromosomes for joint evaluation. This framework effectively
captures modality heterogeneity ignored by single-modality GNAS. Second, we
introduce a modality-aware dual-track surrogate (MADTS) method to reduce
evaluation cost and accelerate local gene evolution. Third, we design a
similarity-based population diversity indicator (SPDI) strategy to adaptively
balance exploration and exploitation, thereby accelerating convergence and
avoiding local optima. On a standard vulnerabilities co-exploitation (VulCE)
dataset, MACC-MGNAS achieves an F1-score of 81.67% within only 3 GPU-hours,
outperforming the state-of-the-art competitor by 8.7% F1 while reducing
computation cost by 27%.

</details>


### [100] [MultiFair: Multimodal Balanced Fairness-Aware Medical Classification with Dual-Level Gradient Modulation](https://arxiv.org/abs/2510.07328)
*Md Zubair,Hao Zheng,Nussdorf Jonathan,Grayson W. Armstrong,Lucy Q. Shen,Gabriela Wilson,Yu Tian,Xingquan Zhu,Min Shi*

Main category: cs.LG

TL;DR: The paper proposes MultiFair, a new method for multimodal medical classification that addresses modality bias and demographic unfairness using gradient modulation. It outperforms existing methods in experiments.


<details>
  <summary>Details</summary>
Motivation: To tackle bias and unfairness in multimodal medical decision systems caused by uneven learning from various modalities and demographic disparities.

Method: MultiFair employs a dual-level gradient modulation process to dynamically adjust training directions and magnitudes at both data modality and demographic group levels.

Result: Experiments on two multimodal medical datasets demonstrate that MultiFair achieves superior performance compared to existing multimodal and fairness learning models.

Conclusion: MultiFair effectively improves fairness and balance in multimodal medical classification while maintaining strong performance.

Abstract: Medical decision systems increasingly rely on data from multiple sources to
ensure reliable and unbiased diagnosis. However, existing multimodal learning
models fail to achieve this goal because they often ignore two critical
challenges. First, various data modalities may learn unevenly, thereby
converging to a model biased towards certain modalities. Second, the model may
emphasize learning on certain demographic groups causing unfair performances.
The two aspects can influence each other, as different data modalities may
favor respective groups during optimization, leading to both imbalanced and
unfair multimodal learning. This paper proposes a novel approach called
MultiFair for multimodal medical classification, which addresses these
challenges with a dual-level gradient modulation process. MultiFair dynamically
modulates training gradients regarding the optimization direction and magnitude
at both data modality and group levels. We conduct extensive experiments on two
multimodal medical datasets with different demographic groups. The results show
that MultiFair outperforms state-of-the-art multimodal learning and fairness
learning methods.

</details>


### [101] [Out-of-Distribution Generalization in Climate-Aware Yield Prediction with Earth Observation Data](https://arxiv.org/abs/2510.07350)
*Aditya Chakravarty*

Main category: cs.LG

TL;DR: The paper tests the geographic generalization of two crop yield forecasting models, GNN-RNN and MMST-ViT, across 1,200+ U.S. counties, finding GNN-RNN is superior in speed and generalization under out-of-distribution conditions.


<details>
  <summary>Details</summary>
Motivation: Accurate crop yield prediction is critical for food security amidst climate change's impact on agriculture, but existing models lack evaluation for geographic and temporal robustness.

Method: The authors used leave-one-cluster-out cross-validation on the CropNet dataset (2017-2022) across seven U.S. farm regions to evaluate geographic and yearly prediction capabilities of state-of-the-art deep learning models.

Result: GNN-RNN showed better geographic transferability with stable accuracy and faster training, while MMST-ViT performed poorly under OOD conditions. Specific regions like Prairie Gateway exhibited persistently high RMSE values due to structural and climate factors.

Conclusion: Spatial-temporal alignment, rather than model complexity, drives robust generalization for agricultural forecasting, emphasizing the need for transparent out-of-distribution evaluation protocols in such models.

Abstract: Climate change is increasingly disrupting agricultural systems, making
accurate crop yield forecasting essential for food security. While deep
learning models have shown promise in yield prediction using satellite and
weather data, their ability to generalize across geographic regions and years -
critical for real-world deployment - remains largely untested. We benchmark two
state-of-the-art models, GNN-RNN and MMST-ViT, under realistic
out-of-distribution (OOD) conditions using the large-scale CropNet dataset
spanning 1,200+ U.S. counties from 2017-2022. Through leave-one-cluster-out
cross-validation across seven USDA Farm Resource Regions and year-ahead
prediction scenarios, we identify substantial variability in cross-region
transferability. GNN-RNN demonstrates superior generalization with positive
correlations under geographic shifts, while MMST-ViT performs well in-domain
but degrades sharply under OOD conditions. Regions like Heartland and Northern
Great Plains show stable transfer dynamics (RMSE less than 10 bu/acre for
soybean), whereas Prairie Gateway exhibits persistent underperformance (RMSE
greater than 20 bu/acre) across both models and crops, revealing structural
dissimilarities likely driven by semi-arid climate, irrigation patterns, and
incomplete spectral coverage. Beyond accuracy differences, GNN-RNN achieves
135x faster training than MMST-ViT (14 minutes vs. 31.5 hours), making it more
viable for sustainable deployment. Our findings underscore that
spatial-temporal alignment - not merely model complexity or data scale - is key
to robust generalization, and highlight the need for transparent OOD evaluation
protocols to ensure equitable and reliable climate-aware agricultural
forecasting.

</details>


### [102] [ConCuR: Conciseness Makes State-of-the-Art Kernel Generation](https://arxiv.org/abs/2510.07356)
*Lingcheng Kong,Jiateng Wei,Hanzhang Shen,Huan Wang*

Main category: cs.LG

TL;DR: The paper develops a pipeline to generate and curate high-quality CUDA kernels with reasoning traces, addressing the lack of open-source kernel data, and introduces KernelCoder, a model achieving state-of-the-art performance in GPU kernel generation tasks.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality, open-source CUDA kernel data limits the ability to use supervised fine-tuning in large language models for generating high-performance kernels.

Method: The authors created a pipeline for generating and curating CUDA kernels with concise reasoning traces, leading to the creation of the ConCuR dataset. They trained their model, KernelCoder, on this curated dataset to align with kernel generation tasks.

Result: KernelCoder significantly outperforms existing top-performing models like QwQ-32B and advanced frontier models such as DeepSeek-V3.1-Think and Claude-4-sonnet in the KernelBench setup.

Conclusion: The paper's methodology and insights, such as leveraging reasoning traces and reasoning-length metrics, provide tools for improving data quality and advancing kernel generation using large language models.

Abstract: GPU kernel generation by LLMs has recently experienced rapid development,
leveraging test-time scaling and reinforcement learning techniques. However, a
key challenge for kernel generation is the scarcity of high-quality data, as
most high-quality kernels are proprietary and not open-source. This challenge
prevents us from leveraging supervised fine-tuning to align LLMs to the kernel
generation task. To address this challenge, we develop a pipeline that
generates and curates high-quality CUDA kernels with reasoning traces,
motivated by a critical observation that concise yet informative reasoning
traces result in robust generation of high-performance kernels. Using this
pipeline, we construct our dataset ConCuR and introduce our model KernelCoder,
which is the first model trained on a curated dataset consisting of PyTorch,
reasoning, and CUDA kernel pairs, to our knowledge. In the KernelBench setup,
our model achieves significant improvements over the existing top-performing
model, QwQ-32B, and outperforms all open-source models fine-tuned for kernel
generation, as well as frontier models such as DeepSeek-V3.1-Think and
Claude-4-sonnet. Finally, we show that the average reasoning length can serve
as a metric to assess the difficulty of kernel generation tasks. The
observations, metrics, and our data collection and curation pipeline can help
obtain better data in the kernel generation task in the future.

</details>


### [103] [Encode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts](https://arxiv.org/abs/2510.07358)
*Yeskendir Koishekenov,Aldo Lipani,Nicola Cancedda*

Main category: cs.LG

TL;DR: The paper introduces a technique called Encode-Think-Decode (ETD) to enhance reasoning in large language models (LLMs) by iterating specific reasoning-relevant layers during mid-training and inference.


<details>
  <summary>Details</summary>
Motivation: To improve reasoning in LLMs without expanding model size, data, or altering architecture, leveraging interpretability findings that reasoning is concentrated in certain layers.

Method: ETD trains models to iterate over reasoning-critical layers during the mid-training phase while preserving the original design and introduces layer-based recurrence during inference.

Result: ETD improves reasoning benchmarks significantly, achieving +28.4% accuracy on GSM8K and +36% on MATH with the OLMo-2 1B Base model.

Conclusion: Recursive latent reasoning via targeted layer iteration is a straightforward and effective method for enhancing LLM reasoning capabilities.

Abstract: Most efforts to improve the reasoning capabilities of large language models
(LLMs) involve either scaling the number of parameters and the size of training
data, or scaling inference computation by letting models generate complex
chains of thought. Motivated by interpretability studies showing that the
crucial computation required for reasoning tasks is concentrated in a limited
range of layers, we introduce Encode-Think-Decode (ETD), a method that enhances
the reasoning capabilities of a base model by training it to iterate over a
small subset of reasoning-relevant layers during the mid-training stage. ETD
amplifies latent reasoning while preserving the original architecture,
parameter count, hyperparameters, and training data composition. When iterating
on the selected layers at inference time, ETD models yield substantial gains on
17 reasoning benchmarks, including +28.4% relative accuracy improvement on
GSM8K and +36% on MATH with the OLMo-2 1B Base model. We also explore an
adaptive depth strategy that adjusts the computation per input token. Our
results show that recursive latent reasoning offers a simple and effective path
to stronger LLM reasoning.

</details>


### [104] [FedQS: Optimizing Gradient and Model Aggregation for Semi-Asynchronous Federated Learning](https://arxiv.org/abs/2510.07664)
*Yunbo Li,Jiaping Gui,Zhihang Deng,Fanchao Meng,Yue Wu*

Main category: cs.LG

TL;DR: The paper introduces FedQS, a framework improving semi-asynchronous federated learning by balancing gradient and model aggregation strategies, achieving high accuracy and efficient training.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in optimizing gradient-based and model-based aggregation strategies seen in semi-asynchronous federated learning (SAFL) models, particularly the trade-offs in convergence, accuracy, and stability.

Method: FedQS leverages a divide-and-conquer strategy by classifying clients into four types based on data and computational profiles, and adaptively optimizing their training processes.

Result: FedQS surpasses current state-of-the-art methods across computer vision, NLP, and real-world settings in accuracy, loss minimization, and convergence speed.

Conclusion: FedQS successfully bridges the gap between different aggregation strategies in SAFL, enabling stable, accurate, and efficient federated learning.

Abstract: Federated learning (FL) enables collaborative model training across multiple
parties without sharing raw data, with semi-asynchronous FL (SAFL) emerging as
a balanced approach between synchronous and asynchronous FL. However, SAFL
faces significant challenges in optimizing both gradient-based (e.g., FedSGD)
and model-based (e.g., FedAvg) aggregation strategies, which exhibit distinct
trade-offs in accuracy, convergence speed, and stability. While gradient
aggregation achieves faster convergence and higher accuracy, it suffers from
pronounced fluctuations, whereas model aggregation offers greater stability but
slower convergence and suboptimal accuracy. This paper presents FedQS, the
first framework to theoretically analyze and address these disparities in SAFL.
FedQS introduces a divide-and-conquer strategy to handle client heterogeneity
by classifying clients into four distinct types and adaptively optimizing their
local training based on data distribution characteristics and available
computational resources. Extensive experiments on computer vision, natural
language processing, and real-world tasks demonstrate that FedQS achieves the
highest accuracy, attains the lowest loss, and ranks among the fastest in
convergence speed, outperforming state-of-the-art baselines. Our work bridges
the gap between aggregation strategies in SAFL, offering a unified solution for
stable, accurate, and efficient federated learning. The code and datasets are
available at https://anonymous.4open.science/r/FedQS-EDD6.

</details>


### [105] [Best-of-Both Worlds for linear contextual bandits with paid observations](https://arxiv.org/abs/2510.07424)
*Nathan Boyer,Dorian Baudry,Patrick Rebeschini*

Main category: cs.LG

TL;DR: The paper introduces a computationally efficient Best-of-Both-Worlds algorithm for linear contextual bandits with paid observations, achieving optimal regret in adversarial setups and poly-logarithmic regret in stochastic settings.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of minimizing loss in a linear contextual bandit setting where observation costs are incurred, offering a solution for both adversarial and stochastic environments.

Method: The authors propose a Best-of-Both-Worlds algorithm based on the Follow-the-Regularized-Leader framework and Matrix Geometric Resampling, leveraging tailored analytical techniques.

Result: The algorithm achieves $\\Theta(T^{2/3})$ minimax-optimal regret in adversarial settings and poly-logarithmic regret in (corrupted) stochastic regimes.

Conclusion: The introduced methodology effectively balances computational efficiency with regret minimization across diverse environments, providing a robust solution for the studied problem.

Abstract: We study the problem of linear contextual bandits with paid observations,
where at each round the learner selects an action in order to minimize its loss
in a given context, and can then decide to pay a fixed cost to observe the loss
of any arm. Building on the Follow-the-Regularized-Leader framework with
efficient estimators via Matrix Geometric Resampling, we introduce a
computationally efficient Best-of-Both-Worlds (BOBW) algorithm for this
problem. We show that it achieves the minimax-optimal regret of
$\Theta(T^{2/3})$ in adversarial settings, while guaranteeing poly-logarithmic
regret in (corrupted) stochastic regimes. Our approach builds on the framework
from \cite{BOBWhardproblems} to design BOBW algorithms for ``hard problem'',
using analysis techniques tailored for the setting that we consider.

</details>


### [106] [Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs](https://arxiv.org/abs/2510.07429)
*Wang Wei,Tiankai Yang,Hongjie Chen,Yue Zhao,Franck Dernoncourt,Ryan A. Rossi,Hoda Eldardiry*

Main category: cs.LG

TL;DR: BaRP, a contextual bandit approach, optimizes LLM selection by using partial feedback during training, enabling adaptable performance/cost trade-offs while outperforming offline routers and large models in deployment.


<details>
  <summary>Details</summary>
Motivation: Efficient selection of large language models for deployment is necessary to balance performance and cost, as current offline routers fail to adapt to real-world conditions.

Method: The paper introduces BaRP, a contextual bandit framework that utilizes prompt features and user preferences, simulating online feedback during training to make adaptive routing decisions.

Result: BaRP improved routing performance by at least 12.46% compared to offline routers and 2.45% relative to the largest LLM, showing robustness even for unseen tasks.

Conclusion: BaRP bridges the gap between offline training and real-world deployment, offering a tunable framework that adapts to cost-performance preferences while consistently outperforming traditional solutions.

Abstract: Efficient use of large language models (LLMs) is critical for deployment at
scale: without adaptive routing, systems either overpay for strong models or
risk poor performance from weaker ones. Selecting the right LLM for each query
is fundamentally an online decision problem: models differ in strengths, prices
fluctuate, and users value accuracy and cost differently. Yet most routers are
trained offline with labels for all candidate models, an assumption that breaks
in deployment, where only the outcome of the chosen model is observed. We
bridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach
that trains under the same partial-feedback restriction as deployment, while
supporting preference-tunable inference: operators can dial the
performance/cost trade-off at test time without retraining. Framed as a
contextual bandit over prompt features and a user preference vector, our method
simulates an online feedback setting during training and adapts its routing
decisions to each new prompt, rather than depending on full-information offline
supervision. Comprehensive experiments show that our method consistently
outperforms strong offline routers by at least 12.46% and the largest LLM by at
least 2.45%, and generalizes robustly for unseen tasks.

</details>


### [107] [SketchGuard: Scaling Byzantine-Robust Decentralized Federated Learning via Sketch-Based Screening](https://arxiv.org/abs/2510.07922)
*Murtaza Rangwala,Farag Azzedin,Richard O. Sinnott,Rajkumar Buyya*

Main category: cs.LG

TL;DR: This paper introduces SketchGuard, a decentralized federated learning (DFL) method using sketch-based compression to defend against Byzantine attacks efficiently.


<details>
  <summary>Details</summary>
Motivation: The motivation is the challenge of enabling Byzantine-robust decentralized federated learning (DFL) while minimizing high communication and computational costs associated with existing methods.

Method: SketchGuard employs Count Sketch compression to create low-dimensional sketches for neighbor screening, selectively fetching full models only from accepted neighbors, thereby reducing communication complexity.

Result: The proposed framework achieves comparable robustness to existing methods while reducing computation time by up to 82% and communication overhead by 50-70%, with advantages scaling with model dimensionality and connectivity.

Conclusion: Sketch-based compression is a practical and efficient solution to enable robust decentralized federated learning at large scales, preserving effectiveness against adversarial attacks.

Abstract: Decentralized Federated Learning (DFL) enables privacy-preserving
collaborative training without centralized servers, but remains vulnerable to
Byzantine attacks where malicious clients submit corrupted model updates.
Existing Byzantine-robust DFL defenses rely on similarity-based neighbor
screening that requires every client to exchange and compare complete
high-dimensional model vectors with all neighbors in each training round,
creating prohibitive communication and computational costs that prevent
deployment at web scale. We propose SketchGuard, a general framework that
decouples Byzantine filtering from model aggregation through sketch-based
neighbor screening. SketchGuard compresses $d$-dimensional models to
$k$-dimensional sketches ($k \ll d$) using Count Sketch for similarity
comparisons, then selectively fetches full models only from accepted neighbors,
reducing per-round communication complexity from $O(d|N_i|)$ to $O(k|N_i| +
d|S_i|)$, where $|N_i|$ is the neighbor count and $|S_i| \le |N_i|$ is the
accepted neighbor count. We establish rigorous convergence guarantees in both
strongly convex and non-convex settings, proving that Count Sketch compression
preserves Byzantine resilience with controlled degradation bounds where
approximation errors introduce only a $(1+O(\epsilon))$ factor in the effective
threshold parameter. Comprehensive experiments across multiple datasets,
network topologies, and attack scenarios demonstrate that SketchGuard maintains
identical robustness to state-of-the-art methods while reducing computation
time by up to 82% and communication overhead by 50-70% depending on filtering
effectiveness, with benefits scaling multiplicatively with model dimensionality
and network connectivity. These results establish the viability of sketch-based
compression as a fundamental enabler of robust DFL at web scale.

</details>


### [108] [Parameter-Free Federated TD Learning with Markov Noise in Heterogeneous Environments](https://arxiv.org/abs/2510.07436)
*Ankur Naskar,Gugan Thoppe,Utsav Negi,Vijay Gupta*

Main category: cs.LG

TL;DR: This paper introduces a Federated Temporal Difference (FTD) learning approach that efficiently speeds up reinforcement learning using multiple agents and achieves optimal convergence rates for Markovian data.


<details>
  <summary>Details</summary>
Motivation: Existing TD learning methods achieving optimal convergence rates require dependency on unknown problem parameters in case of Markovian training samples. This paper aims to address this gap.

Method: The paper proposes a two-timescale Federated Temporal Difference (FTD) learning algorithm with Polyak-Ruppert averaging, designed to be parameter-free and applicable to federated learning environments.

Result: The proposed method attains the optimal convergence rate of $	ilde{O}(1/(NT))$ in both average-reward and discounted settings, showcasing its effectiveness and novelty.

Conclusion: The FTD algorithm overcomes limitations of previous methods, providing a practical and scalable solution for federated reinforcement learning in heterogeneous environments with Markovian data.

Abstract: Federated learning (FL) can dramatically speed up reinforcement learning by
distributing exploration and training across multiple agents. It can guarantee
an optimal convergence rate that scales linearly in the number of agents, i.e.,
a rate of $\tilde{O}(1/(NT)),$ where $T$ is the iteration index and $N$ is the
number of agents. However, when the training samples arise from a Markov chain,
existing results on TD learning achieving this rate require the algorithm to
depend on unknown problem parameters. We close this gap by proposing a
two-timescale Federated Temporal Difference (FTD) learning with Polyak-Ruppert
averaging. Our method provably attains the optimal $\tilde{O}(1/NT)$ rate in
both average-reward and discounted settings--offering a parameter-free FTD
approach for Markovian data. Although our results are novel even in the
single-agent setting, they apply to the more realistic and challenging scenario
of FL with heterogeneous environments.

</details>


### [109] [From Tokens to Layers: Redefining Stall-Free Scheduling for LLM Serving with Layered Prefill](https://arxiv.org/abs/2510.08055)
*Gunjun Lee,Jiwon Kim,Jaiyoung Park,Younjoo Lee,Jung Ho Ahn*

Main category: cs.LG

TL;DR: The paper introduces "layered prefill," a scheduling technique to optimize LLM inference by addressing inefficiencies caused by existing methods like chunked prefill, particularly for MoE models.


<details>
  <summary>Details</summary>
Motivation: Current LLM serving systems struggle with balancing throughput, latency, energy consumption, and memory usage due to inefficiencies like redundant weight loads in MoE models.

Method: The method involves partitioning the transformer model into layer groups, enabling scheduling at the layer level rather than token level, thus interleaving prefill and decode across groups.

Result: Layered prefill reduces off-chip bandwidth demand, TTFT by 70%, end-to-end latency by 41%, and per-token energy consumption by up to 22%.

Conclusion: Shifting scheduling from tokens to layers enhances performance, efficiency, and energy-awareness in LLM serving systems, setting a new paradigm for co-located AI environments.

Abstract: Large Language Model (LLM) inference in production must meet stringent
service-level objectives for both time-to-first-token (TTFT) and
time-between-token (TBT) while maximizing throughput under fixed compute,
memory, and interconnect budgets. Modern serving systems adopt stall-free
scheduling techniques such as chunked prefill, which splits long prompt
processing along the token dimension and interleaves prefill with ongoing
decode iterations. While effective at stabilizing TBT, chunked prefill incurs
substantial overhead in Mixture-of-Experts (MoE) models: redundant expert
weight loads increase memory traffic by up to 39% and inflate energy
consumption. We propose layered prefill, a new scheduling paradigm that treats
transformer layer groups as the primary scheduling unit. By vertically
partitioning the model into contiguous layer groups and interleaving prefill
and decode across the groups, layered prefill sustains stall-free decoding
while eliminating chunk-induced MoE weight reloads. It reduces off-chip
bandwidth demand, lowering TTFT by up to 70%, End-to-End latency by 41% and
per-token energy by up to 22%. Evaluations show that layered prefill
consistently improves the TTFT--TBT Pareto frontier over chunked prefill,
reducing expert-load traffic and energy cost while maintaining stall-free
decoding. Overall, shifting the scheduling axis from tokens to layers unlocks a
new operating regime for high-efficiency, energy-aware LLM serving in
co-located environments.

</details>


### [110] [MoGU: Mixture-of-Gaussians with Uncertainty-based Gating for Time Series Forecasting](https://arxiv.org/abs/2510.07459)
*Yoli Shavit,Jacob Goldberger*

Main category: cs.LG

TL;DR: MoGU introduces a new Mixture-of-Experts (MoE) framework with uncertainty-based gating for time series regression, offering better accuracy and uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: To enhance regression tasks in time series forecasting by incorporating uncertainty quantification into predictions.

Method: MoGU represents outputs as Gaussian distributions and uses an uncertainty-based gating mechanism to weigh expert contributions.

Result: MoGU consistently performs better than single-expert and traditional MoE models, providing reliable uncertainty estimates that align with prediction errors.

Conclusion: MoGU improves predictive accuracy and reliability in time series forecasting while offering meaningful uncertainty insights.

Abstract: We introduce Mixture-of-Gaussians with Uncertainty-based Gating (MoGU), a
novel Mixture-of-Experts (MoE) framework designed for regression tasks and
applied to time series forecasting. Unlike conventional MoEs that provide only
point estimates, MoGU models each expert's output as a Gaussian distribution.
This allows it to directly quantify both the forecast (the mean) and its
inherent uncertainty (variance). MoGU's core innovation is its
uncertainty-based gating mechanism, which replaces the traditional input-based
gating network by using each expert's estimated variance to determine its
contribution to the final prediction. Evaluated across diverse time series
forecasting benchmarks, MoGU consistently outperforms single-expert models and
traditional MoE setups. It also provides well-quantified, informative
uncertainties that directly correlate with prediction errors, enhancing
forecast reliability. Our code is available from:
https://github.com/yolish/moe_unc_tsf

</details>


### [111] [metabeta -- A fast neural model for Bayesian mixed-effects regression](https://arxiv.org/abs/2510.07473)
*Alex Kipnis,Marcel Binz,Eric Schulz*

Main category: cs.LG

TL;DR: The paper introduces metabeta, a transformer-based neural network model designed to improve Bayesian mixed-effects regression efficiency by shifting computational burden to pre-training, achieving faster inference compared to MCMC methods.


<details>
  <summary>Details</summary>
Motivation: Analyzing hierarchical data often relies on mixed-effects regression, with Bayesian inference providing uncertainty estimates. However, standard methods like MCMC are computationally expensive and intractable for many cases.

Method: The authors propose metabeta, leveraging a transformer-based neural network architecture to conduct Bayesian mixed-effects regression. They use simulated and real datasets with known targets for pre-training instead of inference-time computation.

Result: Metabeta achieves comparable accuracy to MCMC methods while significantly reducing computation time for parameter estimation.

Conclusion: The paper validates metabeta as an efficient alternative to traditional computational methods for Bayesian mixed-effects regression, improving scalability and inference time handling.

Abstract: Hierarchical data with multiple observations per group is ubiquitous in
empirical sciences and is often analyzed using mixed-effects regression. In
such models, Bayesian inference gives an estimate of uncertainty but is
analytically intractable and requires costly approximation using Markov Chain
Monte Carlo (MCMC) methods. Neural posterior estimation shifts the bulk of
computation from inference time to pre-training time, amortizing over simulated
datasets with known ground truth targets. We propose metabeta, a
transformer-based neural network model for Bayesian mixed-effects regression.
Using simulated and real data, we show that it reaches stable and comparable
performance to MCMC-based parameter estimation at a fraction of the usually
required time.

</details>


### [112] [Surrogate Modeling for the Design of Optimal Lattice Structures using Tensor Completion](https://arxiv.org/abs/2510.07474)
*Shaan Pakala,Aldair E. Gongora,Brian Giera,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: This paper proposes using tensor completion as a surrogate model to enhance material design, particularly in scenarios with non-uniform training data, achieving better or comparable performance to traditional ML methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in designing materials with specific properties due to the exponential growth of the search space and the limitations of existing ML methods when data are biased or non-uniformly sampled.

Method: The authors propose using tensor completion as a surrogate modeling approach to overcome data bias in supervised learning. They compare its performance against classical ML methods like Gaussian Process and XGBoost.

Result: Tensor completion outperforms traditional ML methods with a 5% increase in performance ($R^2$) in biased sampling scenarios and gives comparable results in uniformly sampled scenarios.

Conclusion: The study concludes that tensor completion is a robust alternative for accelerating the material design process, especially in atypical data conditions.

Abstract: When designing new materials, it is often necessary to design a material with
specific desired properties. Unfortunately, as new design variables are added,
the search space grows exponentially, which makes synthesizing and validating
the properties of each material very impractical and time-consuming. In this
work, we focus on the design of optimal lattice structures with regard to
mechanical performance. Computational approaches, including the use of machine
learning (ML) methods, have shown improved success in accelerating materials
design. However, these ML methods are still lacking in scenarios when training
data (i.e. experimentally validated materials) come from a non-uniformly random
sampling across the design space. For example, an experimentalist might
synthesize and validate certain materials more frequently because of
convenience. For this reason, we suggest the use of tensor completion as a
surrogate model to accelerate the design of materials in these atypical
supervised learning scenarios. In our experiments, we show that tensor
completion is superior to classic ML methods such as Gaussian Process and
XGBoost with biased sampling of the search space, with around 5\% increased
$R^2$. Furthermore, tensor completion still gives comparable performance with a
uniformly random sampling of the entire search space.

</details>


### [113] [HEMERA: A Human-Explainable Transformer Model for Estimating Lung Cancer Risk using GWAS Data](https://arxiv.org/abs/2510.07477)
*Maria Mahbub,Robert J. Klein,Myvizhi Esai Selvan,Rowena Yip,Claudia Henschke,Providencia Morales,Ian Goethert,Olivera Kotevska,Mayanka Chandra Shekar,Sean R. Wilkinson,Eileen McAllister,Samuel M. Aguayo,Zeynep H. Gümüş,Ioana Danciu,VA Million Veteran Program*

Main category: cs.LG

TL;DR: The study introduces HEMERA, an explainable transformer-based deep learning framework that uses GWAS data to predict lung cancer risk with high accuracy (>99% AUC).


<details>
  <summary>Details</summary>
Motivation: To advance lung cancer risk assessment through genetic biomarkers, addressing the limitations of current approaches by employing a novel transformer-based deep learning model.

Method: HEMERA processes raw genotype data (GWAS data) without clinical covariates, incorporating additive positional encodings, neural genotype embeddings, and refined variant filtering. Layer-wise Integrated Gradients are used to attribute predictions to specific SNPs.

Result: HEMERA achieved excellent predictive performance with an AUC score exceeding 99%, affirming its reliability in identifying LC risk loci.

Conclusion: HEMERA offers a promising, transparent model for personalized lung cancer risk assessment, enabling early intervention and deeper insights into genetic predispositions.

Abstract: Lung cancer (LC) is the third most common cancer and the leading cause of
cancer deaths in the US. Although smoking is the primary risk factor, the
occurrence of LC in never-smokers and familial aggregation studies highlight a
genetic component. Genetic biomarkers identified through genome-wide
association studies (GWAS) are promising tools for assessing LC risk. We
introduce HEMERA (Human-Explainable Transformer Model for Estimating Lung
Cancer Risk using GWAS Data), a new framework that applies explainable
transformer-based deep learning to GWAS data of single nucleotide polymorphisms
(SNPs) for predicting LC risk. Unlike prior approaches, HEMERA directly
processes raw genotype data without clinical covariates, introducing additive
positional encodings, neural genotype embeddings, and refined variant
filtering. A post hoc explainability module based on Layer-wise Integrated
Gradients enables attribution of model predictions to specific SNPs, aligning
strongly with known LC risk loci. Trained on data from 27,254 Million Veteran
Program participants, HEMERA achieved >99% AUC (area under receiver
characteristics) score. These findings support transparent,
hypothesis-generating models for personalized LC risk assessment and early
intervention.

</details>


### [114] [DYNAMIX: RL-based Adaptive Batch Size Optimization in Distributed Machine Learning Systems](https://arxiv.org/abs/2510.08522)
*Yuanjun Dai,Keqiang He,An Wang*

Main category: cs.LG

TL;DR: DYNAMIX applies reinforcement learning to optimize batch sizes in distributed machine learning environments, improving training efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: Current batch size selection methods lack adaptability in dynamic and heterogeneous distributed systems.

Method: The paper uses Proximal Policy Optimization (PPO) in a reinforcement learning framework with multi-dimensional state representation.

Result: DYNAMIX improves model accuracy by up to 6.3% and reduces training time by 46%, maintaining scalability across cluster sizes.

Conclusion: DYNAMIX provides a robust, generalizable approach for batch size optimization, outperforming existing static methods and heuristics.

Abstract: Existing batch size selection approaches in distributed machine learning rely
on static allocation or simplistic heuristics that fail to adapt to
heterogeneous, dynamic computing environments. We present DYNAMIX, a
reinforcement learning framework that formulates batch size optimization as a
sequential decision-making problem using Proximal Policy Optimization (PPO).
Our approach employs a multi-dimensional state representation encompassing
network-level metrics, system-level resource utilization, and training
statistical efficiency indicators to enable informed decision-making across
diverse computational resources. Our approach eliminates the need for explicit
system modeling while integrating seamlessly with existing distributed training
frameworks. Through evaluations across diverse workloads, hardware
configurations, and network conditions, DYNAMIX achieves up to 6.3% improvement
in the final model accuracy and 46% reduction in the total training time. Our
scalability experiments demonstrate that DYNAMIX maintains the best performance
as cluster size increases to 32 nodes, while policy transfer experiments show
that learned policies generalize effectively across related model
architectures.

</details>


### [115] [Reinforcement Learning-based Task Offloading in the Internet of Wearable Things](https://arxiv.org/abs/2510.07487)
*Waleed Bin Qaim,Aleksandr Ometov,Claudia Campolo,Antonella Molinaro,Elena Simona Lohan,Jari Nurmi*

Main category: cs.LG

TL;DR: The paper addresses energy and computational constraints in wearable devices, proposing a Reinforcement Learning-based framework for task offloading to edge devices. It uses Q-learning to optimize offloading decisions and simulates various scenarios.


<details>
  <summary>Details</summary>
Motivation: Wearable devices face challenges such as limited battery life and computing power, while demand for high-performing applications continues to grow.

Method: The task offloading problem is modeled as a Markov Decision Process, using Q-learning to enable wearable devices to make optimal decisions without prior knowledge.

Result: The framework shows benefits in terms of energy consumption, task accomplishment time, and the percentage of tasks effectively offloaded in simulations.

Conclusion: Reinforcement Learning-based task offloading can improve wearable device performance by addressing energy and computation constraints while optimizing for user experience.

Abstract: Over the years, significant contributions have been made by the research and
industrial sectors to improve wearable devices towards the Internet of Wearable
Things (IoWT) paradigm. However, wearables are still facing several challenges.
Many stem from the limited battery power and insufficient computation resources
available on wearable devices. On the other hand, with the popularity of smart
wearables, there is a consistent increase in the development of new
computationally intensive and latency-critical applications. In such a context,
task offloading allows wearables to leverage the resources available on nearby
edge devices to enhance the overall user experience. This paper proposes a
framework for Reinforcement Learning (RL)-based task offloading in the IoWT. We
formulate the task offloading process considering the tradeoff between energy
consumption and task accomplishment time. Moreover, we model the task
offloading problem as a Markov Decision Process (MDP) and utilize the
Q-learning technique to enable the wearable device to make optimal task
offloading decisions without prior knowledge. We evaluate the performance of
the proposed framework through extensive simulations for various applications
and system configurations conducted in the ns-3 network simulator. We also show
how varying the main system parameters of the Q-learning algorithm affects the
overall performance in terms of average task accomplishment time, average
energy consumption, and percentage of tasks offloaded.

</details>


### [116] [Black-box Detection of LLM-generated Text Using Generalized Jensen-Shannon Divergence](https://arxiv.org/abs/2510.07500)
*Shuangyi Chen,Ashish Khisti*

Main category: cs.LG

TL;DR: The paper develops a novel method called SurpMark for detecting AI-generated text, focusing on practical and interpretable techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to build a practical detector for machine-generated text even when the scoring model doesn't match the text's source model and generation isn't cost-effective per input.

Method: Introduce SurpMark, a system based on summarizing token surprisal dynamics, quantizing them into states, estimating a state-transition matrix, and scoring test text using a generalized Jensen-Shannon gap between human and machine references.

Result: SurpMark outperformed or matched existing methods across various datasets and models, proving effective and validating the theoretical properties of its decision statistic.

Conclusion: SurpMark is an effective and robust tool for detecting machine-generated text, with sound theoretical underpinnings and strong empirical results.

Abstract: We study black-box detection of machine-generated text under practical
constraints: the scoring model (proxy LM) may mismatch the unknown source
model, and per-input contrastive generation is costly. We propose SurpMark, a
reference-based detector that summarizes a passage by the dynamics of its token
surprisals. SurpMark quantizes surprisals into interpretable states, estimates
a state-transition matrix for the test text, and scores it via a generalized
Jensen-Shannon (GJS) gap between the test transitions and two fixed references
(human vs. machine) built once from historical corpora. We prove a principled
discretization criterion and establish the asymptotic normality of the decision
statistic. Empirically, across multiple datasets, source models, and scenarios,
SurpMark consistently matches or surpasses baselines; our experiments
corroborate the statistic's asymptotic normality, and ablations validate the
effectiveness of the proposed discretization.

</details>


### [117] [PEAR: Planner-Executor Agent Robustness Benchmark](https://arxiv.org/abs/2510.07505)
*Shen Dong,Mingxuan Zhang,Pengfei He,Li Ma,Bhavani Thuraisingham,Hui Liu,Yue Xing*

Main category: cs.LG

TL;DR: The paper presents PEAR, a benchmark to assess vulnerabilities and performance in planner-executor multi-agent systems (MAS), focusing on robustness against adversarial attacks and providing insights for improving MAS designs.


<details>
  <summary>Details</summary>
Motivation: MAS relying on Large Language Models (LLMs) show promise in tackling complex tasks but remain vulnerable to adversarial manipulation. Current research lacks a unified framework to understand and address these vulnerabilities.

Method: PEAR is introduced as a benchmark compatible with various MAS architectures, particularly analyzing planner-executor systems through systematic utility and vulnerability evaluations in diverse scenarios.

Result: Experimental findings indicate weak planners harm task performance more than weak executors, memory modules are more vital for planners than executors, task performance trades off robustness, and planner-targeted attacks are highly effective.

Conclusion: The paper provides essential insights for MAS robustness improvement and sets the stage for developing principled defenses to fortify multi-agent systems.

Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a
powerful paradigm for tackling complex, multi-step tasks across diverse
domains. However, despite their impressive capabilities, MAS remain susceptible
to adversarial manipulation. Existing studies typically examine isolated attack
surfaces or specific scenarios, leaving a lack of holistic understanding of MAS
vulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for
systematically evaluating both the utility and vulnerability of
planner-executor MAS. While compatible with various MAS architectures, our
benchmark focuses on the planner-executor structure, which is a practical and
widely adopted design. Through extensive experiments, we find that (1) a weak
planner degrades overall clean task performance more severely than a weak
executor; (2) while a memory module is essential for the planner, having a
memory module for the executor does not impact the clean task performance; (3)
there exists a trade-off between task performance and robustness; and (4)
attacks targeting the planner are particularly effective at misleading the
system. These findings offer actionable insights for enhancing the robustness
of MAS and lay the groundwork for principled defenses in multi-agent settings.

</details>


### [118] [Efficient Generalization via Multimodal Co-Training under Data Scarcity and Distribution Shift](https://arxiv.org/abs/2510.07509)
*Tianyu Bell Pan,Damon L. Woodard*

Main category: cs.LG

TL;DR: The paper introduces a multimodal co-training framework to improve model generalization under limited labeled data and distribution shifts, validated with theoretical foundations and convergence analysis.


<details>
  <summary>Details</summary>
Motivation: To address challenges in AI systems' generalization when labeled data is limited and there's a distribution shift, using theoretical and practical insights.

Method: Multimodal co-training framework employing iterative co-training, leveraging unlabeled multimodal data, promoting agreement between modalities, and ensuring conditional view independence.

Result: Theoretical analyses show significant improvements in generalization and a novel generalization bound illustrates advantages in using multimodal co-training.

Conclusion: Multimodal co-training is effective for building robust, data-efficient AI systems that generalize well in real-world dynamic scenarios, supported by theoretical principles and analyses.

Abstract: This paper explores a multimodal co-training framework designed to enhance
model generalization in situations where labeled data is limited and
distribution shifts occur. We thoroughly examine the theoretical foundations of
this framework, deriving conditions under which the use of unlabeled data and
the promotion of agreement between classifiers for different modalities lead to
significant improvements in generalization. We also present a convergence
analysis that confirms the effectiveness of iterative co-training in reducing
classification errors. In addition, we establish a novel generalization bound
that, for the first time in a multimodal co-training context, decomposes and
quantifies the distinct advantages gained from leveraging unlabeled multimodal
data, promoting inter-view agreement, and maintaining conditional view
independence. Our findings highlight the practical benefits of multimodal
co-training as a structured approach to developing data-efficient and robust AI
systems that can effectively generalize in dynamic, real-world environments.
The theoretical foundations are examined in dialogue with, and in advance of,
established co-training principles.

</details>


### [119] [MLLM4TS: Leveraging Vision and Multimodal Language Models for General Time-Series Analysis](https://arxiv.org/abs/2510.07513)
*Qinghua Liu,Sam Heshmati,Zheda Mai,Zubin Abraham,John Paparrizos,Liu Ren*

Main category: cs.LG

TL;DR: MLLM4TS introduces a novel framework for time-series analysis, integrating visual representations with multimodal large language models to improve predictive and generative tasks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in analyzing multivariate time-series data and explore whether visual representations can enhance automated analysis.

Method: The framework integrates multivariate time-series into a color-coded line plot, employs temporal-aware visual patch alignment, and fuses numerical and visual data using multimodal large language models.

Result: Extensive experiments on benchmarks show superior performance in classification, anomaly detection, and forecasting tasks.

Conclusion: Integrating visual modalities with pretrained language models offers a robust and generalizable approach to time-series analysis.

Abstract: Effective analysis of time series data presents significant challenges due to
the complex temporal dependencies and cross-channel interactions in
multivariate data. Inspired by the way human analysts visually inspect time
series to uncover hidden patterns, we ask: can incorporating visual
representations enhance automated time-series analysis? Recent advances in
multimodal large language models have demonstrated impressive generalization
and visual understanding capability, yet their application to time series
remains constrained by the modality gap between continuous numerical data and
discrete natural language. To bridge this gap, we introduce MLLM4TS, a novel
framework that leverages multimodal large language models for general
time-series analysis by integrating a dedicated vision branch. Each time-series
channel is rendered as a horizontally stacked color-coded line plot in one
composite image to capture spatial dependencies across channels, and a
temporal-aware visual patch alignment strategy then aligns visual patches with
their corresponding time segments. MLLM4TS fuses fine-grained temporal details
from the numerical data with global contextual information derived from the
visual representation, providing a unified foundation for multimodal
time-series analysis. Extensive experiments on standard benchmarks demonstrate
the effectiveness of MLLM4TS across both predictive tasks (e.g.,
classification) and generative tasks (e.g., anomaly detection and forecasting).
These results underscore the potential of integrating visual modalities with
pretrained language models to achieve robust and generalizable time-series
analysis.

</details>


### [120] [EEG Sleep Stage Classification with Continuous Wavelet Transform and Deep Learning](https://arxiv.org/abs/2510.07524)
*Mehdi Zekriyapanah Gashti,Ghasem Farjamnia*

Main category: cs.LG

TL;DR: The paper introduces a wavelet-transform-based method for automated sleep stage classification, achieving significant accuracy (88.37%) and F1 score (73.15%) using ensemble learning and outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current sleep stage classification relies on manual annotation or basic computational methods, limiting efficiency and accuracy in addressing sleep disorders.

Method: The study uses continuous wavelet transform (CWT) to generate time-frequency maps, combined with ensemble learning algorithms for sleep stage classification. It evaluates the method using the Sleep-EDF Expanded Database.

Result: The method achieves an overall accuracy of 88.37% and a macro-averaged F1 score of 73.15, surpassing traditional machine learning techniques and rivaling deep learning approaches.

Conclusion: Wavelet-based analysis shows promise for accurate, interpretable, and clinically useful sleep stage classification, enhancing diagnostic capabilities for sleep disorders.

Abstract: Accurate classification of sleep stages is crucial for the diagnosis and
management of sleep disorders. Conventional approaches for sleep scoring rely
on manual annotation or features extracted from EEG signals in the time or
frequency domain. This study proposes a novel framework for automated sleep
stage scoring using time-frequency analysis based on the wavelet transform. The
Sleep-EDF Expanded Database (sleep-cassette recordings) was used for
evaluation. The continuous wavelet transform (CWT) generated time-frequency
maps that capture both transient and oscillatory patterns across frequency
bands relevant to sleep staging. Experimental results demonstrate that the
proposed wavelet-based representation, combined with ensemble learning,
achieves an overall accuracy of 88.37 percent and a macro-averaged F1 score of
73.15, outperforming conventional machine learning methods and exhibiting
comparable or superior performance to recent deep learning approaches. These
findings highlight the potential of wavelet analysis for robust, interpretable,
and clinically applicable sleep stage classification.

</details>


### [121] [Estimating Fair Graphs from Graph-Stationary Data](https://arxiv.org/abs/2510.07536)
*Madeline Navarro,Andrei Buciulea,Samuel Rey,Antonio G. Marques,Santiago Segarra*

Main category: cs.LG

TL;DR: The paper introduces Fair Spectral Templates (FairSpecTemp), an optimization-based method to create fair graphs from graph-stationary signals by addressing bias in connections without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Graph connections are often biased against sensitive attributes, which leads to unfair treatment in graph-based tasks. The study aims to address both group- and individual-level fairness in graphs.

Method: The authors developed FairSpecTemp with two variants: one directly constraining bias through commutativity properties, and another indirectly restricting bias in the graph spectrum for more flexibility. Bias metrics, including spectral domain measurements, are used to evaluate fairness.

Result: FairSpecTemp demonstrates its capability to construct fair graphs while maintaining accuracy, supported by evaluations on synthetic and real-world datasets.

Conclusion: The paper concludes that fairness in graphs can be achieved without compromising accuracy, showcasing FairSpecTemp as an effective and adaptable tool.

Abstract: We estimate fair graphs from graph-stationary nodal observations such that
connections are not biased with respect to sensitive attributes. Edges in
real-world graphs often exhibit preferences for connecting certain pairs of
groups. Biased connections can not only exacerbate but even induce unfair
treatment for downstream graph-based tasks. We therefore consider group and
individual fairness for graphs corresponding to group- and node-level
definitions, respectively. To evaluate the fairness of a given graph, we
provide multiple bias metrics, including novel measurements in the spectral
domain. Furthermore, we propose Fair Spectral Templates (FairSpecTemp), an
optimization-based method with two variants for estimating fair graphs from
stationary graph signals, a general model for graph data subsuming many
existing ones. One variant of FairSpecTemp exploits commutativity properties of
graph stationarity while directly constraining bias, while the other implicitly
encourages fair estimates by restricting bias in the graph spectrum and is thus
more flexible. Our methods enjoy high probability performance bounds, yielding
a conditional tradeoff between fairness and accuracy. In particular, our
analysis reveals that accuracy need not be sacrificed to recover fair graphs.
We evaluate FairSpecTemp on synthetic and real-world data sets to illustrate
its effectiveness and highlight the advantages of both variants of
FairSpecTemp.

</details>


### [122] [Targeted Digital Twin via Flow Map Learning and Its Application to Fluid Dynamics](https://arxiv.org/abs/2510.07549)
*Qifan Chen,Zhongshu Xu,Jinjin Zhang,Dongbin Xiu*

Main category: cs.LG

TL;DR: The paper proposes a numerical method to create targeted digital twins (tDT) that model core system dynamics more efficiently than full digital twins (DT), achieving computational savings.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of full digital twins, the study aims to develop a compact and targeted model focusing on specific quantities of interest (QoIs).

Method: The approach uses memory-based flow map learning (FML), applying trajectory data from full DT simulations offline to create predictive models for key QoIs. Online simulations rely exclusively on the tDT.

Result: In a case study on fluid dynamics around a cylinder, the tDT accurately predicted hydrodynamic forces without requiring detailed flow field simulations, saving computational resources.

Conclusion: The study demonstrates the viability of targeting specific dynamical aspects via tDTs, proving their efficiency for predictive simulation tasks while avoiding the complexity and computational expense of full DT systems.

Abstract: We present a numerical framework for constructing a targeted digital twin
(tDT) that directly models the dynamics of quantities of interest (QoIs) in a
full digital twin (DT). The proposed approach employs memory-based flow map
learning (FML) to develop a data-driven model of the QoIs using short bursts of
trajectory data generated through repeated executions of the full DT. This
renders the construction of the FML-based tDT an entirely offline computational
process. During online simulation, the learned tDT can efficiently predict and
analyze the long-term dynamics of the QoIs without requiring simulations of the
full DT system, thereby achieving substantial computational savings. After
introducing the general numerical procedure, we demonstrate the construction
and predictive capability of the tDT in a computational fluid dynamics (CFD)
example: two-dimensional incompressible flow past a cylinder. The QoIs in this
problem are the hydrodynamic forces exerted on the cylinder. The resulting tDTs
are compact dynamical systems that evolve these forces without explicit
knowledge of the underlying flow field. Numerical results show that the tDTs
yield accurate long-term predictions of the forces while entirely bypassing
full flow simulations.

</details>


### [123] [Some theoretical improvements on the tightness of PAC-Bayes risk certificates for neural networks](https://arxiv.org/abs/2510.07935)
*Diego García-Pérez,Emilio Parrado-Hernández,John Shawe-Taylor*

Main category: cs.LG

TL;DR: The paper introduces theoretical advancements and applications of PAC-Bayes bounds for neural networks, with a focus on optimizing risk certificates and achieving tight generalization bounds.


<details>
  <summary>Details</summary>
Motivation: To make PAC-Bayes risk certificates more useful and applicable for neural networks by improving their theoretical foundation, optimizing methodologies, and achieving credible empirical results.

Method: The authors propose new bounds on KL divergence, utilize implicit differentiation in PAC-Bayes risk certificate optimization, and design methods for non-differentiable objectives, complemented by experiments on well-known datasets like MNIST and CIFAR-10.

Result: The theoretical contributions are validated with empirical tests, achieving the first non-vacuous generalization bounds on the CIFAR-10 dataset for neural networks.

Conclusion: This work provides significant advancements in the practical usability of PAC-Bayes bounds for neural network classifiers, showing tighter risks and broader applicability.

Abstract: This paper presents four theoretical contributions that improve the usability
of risk certificates for neural networks based on PAC-Bayes bounds. First, two
bounds on the KL divergence between Bernoulli distributions enable the
derivation of the tightest explicit bounds on the true risk of classifiers
across different ranges of empirical risk. The paper next focuses on the
formalization of an efficient methodology based on implicit differentiation
that enables the introduction of the optimization of PAC-Bayesian risk
certificates inside the loss/objective function used to fit the network/model.
The last contribution is a method to optimize bounds on non-differentiable
objectives such as the 0-1 loss. These theoretical contributions are
complemented with an empirical evaluation on the MNIST and CIFAR-10 datasets.
In fact, this paper presents the first non-vacuous generalization bounds on
CIFAR-10 for neural networks.

</details>


### [124] [Phase Diagram of Dropout for Two-Layer Neural Networks in the Mean-Field Regime](https://arxiv.org/abs/2510.07554)
*Lénaïc Chizat,Pierre Marion,Yerkin Yesbay*

Main category: cs.LG

TL;DR: This paper explores the role and asymptotic effects of dropout in large two-layer neural networks, providing a phase diagram with five distinct behaviors depending on dropout rate, learning rate, and network width.


<details>
  <summary>Details</summary>
Motivation: The study aims to deepen understanding of the dropout mechanism, which is widely applied in neural network training but lacks theoretical insights, particularly in large-scale models.

Method: Analyzing large-width asymptotics of gradient descent with dropout on two-layer neural networks utilizing mean-field initialization and stochastic process techniques.

Result: A phase diagram with five behaviors, showcasing how dropout's penalty effect fades with practical learning rates, and demonstrating dropout's equivalence to random geometry techniques in specific regimes.

Conclusion: This work offers a theoretical framework to understand the dynamics of dropout in large-scale networks, suggesting implications for improved training techniques.

Abstract: Dropout is a standard training technique for neural networks that consists of
randomly deactivating units at each step of their gradient-based training. It
is known to improve performance in many settings, including in the large-scale
training of language or vision models. As a first step towards understanding
the role of dropout in large neural networks, we study the large-width
asymptotics of gradient descent with dropout on two-layer neural networks with
the mean-field initialization scale. We obtain a rich asymptotic phase diagram
that exhibits five distinct nondegenerate phases depending on the relative
magnitudes of the dropout rate, the learning rate, and the width. Notably, we
find that the well-studied "penalty" effect of dropout only persists in the
limit with impractically small learning rates of order $O(1/\text{width})$. For
larger learning rates, this effect disappears and in the limit, dropout is
equivalent to a "random geometry" technique, where the gradients are thinned
randomly after the forward and backward pass have been computed. In this
asymptotic regime, the limit is described by a mean-field jump process where
the neurons' update times follow independent Poisson or Bernoulli clocks
(depending on whether the learning rate vanishes or not). For some of the
phases, we obtain a description of the limit dynamics both in path-space and in
distribution-space. The convergence proofs involve a mix of tools from
mean-field particle systems and stochastic processes. Together, our results lay
the groundwork for a renewed theoretical understanding of dropout in
large-scale neural networks.

</details>


### [125] [Investigating Thematic Patterns and User Preferences in LLM Interactions using BERTopic](https://arxiv.org/abs/2510.07557)
*Abhay Bhandarkar,Gaurav Mishra,Khushi Juchani,Harsh Singhal*

Main category: cs.LG

TL;DR: The study uses BERTopic to model topics in a multilingual dataset containing LLM responses, aiming to link themes to user preferences.


<details>
  <summary>Details</summary>
Motivation: To understand thematic patterns in multilingual conversations and evaluate user preferences across different topics regarding LLM performance.

Method: BERTopic was applied after robust preprocessing to extract coherent themes, followed by analysis through visualizations and matrices linking topics to user preferences.

Result: 29 coherent topics were identified, such as AI and ethics, with clear trends in how specific topics align with user preferences for various LLMs.

Conclusion: Findings highlight how LLM performance varies by topic, offering insights for targeted fine-tuning and optimization to enhance user satisfaction.

Abstract: This study applies BERTopic, a transformer-based topic modeling technique, to
the lmsys-chat-1m dataset, a multilingual conversational corpus built from
head-to-head evaluations of large language models (LLMs). Each user prompt is
paired with two anonymized LLM responses and a human preference label, used to
assess user evaluation of competing model outputs. The main objective is
uncovering thematic patterns in these conversations and examining their
relation to user preferences, particularly if certain LLMs are consistently
preferred within specific topics. A robust preprocessing pipeline was designed
for multilingual variation, balancing dialogue turns, and cleaning noisy or
redacted data. BERTopic extracted over 29 coherent topics including artificial
intelligence, programming, ethics, and cloud infrastructure. We analysed
relationships between topics and model preferences to identify trends in
model-topic alignment. Visualization techniques included inter-topic distance
maps, topic probability distributions, and model-versus-topic matrices. Our
findings inform domain-specific fine-tuning and optimization strategies for
improving real-world LLM performance and user satisfaction.

</details>


### [126] [EBGAN-MDN: An Energy-Based Adversarial Framework for Multi-Modal Behavior Cloning](https://arxiv.org/abs/2510.07562)
*Yixiao Li,Julia Barth,Thomas Kiefer,Ahmad Fraij*

Main category: cs.LG

TL;DR: The paper introduces the EBGAN-MDN framework to solve multi-modal behavior cloning challenges like mode averaging and collapse, achieving superior results in synthetic and robotic tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional models struggle with capturing diverse input-output mappings in multi-modal behavior cloning, which is critical for robotics due to its need for safety and performance through valid action modeling.

Method: The proposed method combines energy-based models, Mixture Density Networks (MDNs), and adversarial training using a modified InfoNCE loss and energy-enforced MDN loss.

Result: Experimental evaluation on synthetic and robotic benchmarks highlights the superior performance of EBGAN-MDN in multi-modal learning.

Conclusion: EBGAN-MDN is an effective and efficient solution to challenges in multi-modal behavior cloning, addressing key issues like mode averaging and collapse.

Abstract: Multi-modal behavior cloning faces significant challenges due to mode
averaging and mode collapse, where traditional models fail to capture diverse
input-output mappings. This problem is critical in applications like robotics,
where modeling multiple valid actions ensures both performance and safety. We
propose EBGAN-MDN, a framework that integrates energy-based models, Mixture
Density Networks (MDNs), and adversarial training. By leveraging a modified
InfoNCE loss and an energy-enforced MDN loss, EBGAN-MDN effectively addresses
these challenges. Experiments on synthetic and robotic benchmarks demonstrate
superior performance, establishing EBGAN-MDN as a effective and efficient
solution for multi-modal learning tasks.

</details>


### [127] [Counterfactual Identifiability via Dynamic Optimal Transport](https://arxiv.org/abs/2510.08294)
*Fabio De Sousa Ribeiro,Ainkaran Santhirasekaram,Ben Glocker*

Main category: cs.LG

TL;DR: The paper tackles counterfactual identification for high-dimensional outcomes using continuous-time flows, achieving consistent and valid causal inference.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in establishing causal validity of estimates for high-dimensional outcomes, addressing the gaps in identification of counterfactual inference methods.

Method: Continuous-time flow models and dynamic optimal transport are used to ensure monotone and rank-preserving counterfactual mappings.

Result: Theory is validated through controlled experiments with counterfactual ground truth, showing improvements in counterfactual soundness on real datasets.

Conclusion: The approach provides a rigorous framework for multivariate counterfactual identification while ensuring causal validity and better real-world applicability.

Abstract: We address the open question of counterfactual identification for
high-dimensional multivariate outcomes from observational data. Pearl (2000)
argues that counterfactuals must be identifiable (i.e., recoverable from the
observed data distribution) to justify causal claims. A recent line of work on
counterfactual inference shows promising results but lacks identification,
undermining the causal validity of its estimates. To address this, we establish
a foundation for multivariate counterfactual identification using
continuous-time flows, including non-Markovian settings under standard
criteria. We characterise the conditions under which flow matching yields a
unique, monotone and rank-preserving counterfactual transport map with tools
from dynamic optimal transport, ensuring consistent inference. Building on
this, we validate the theory in controlled scenarios with counterfactual
ground-truth and demonstrate improvements in axiomatic counterfactual soundness
on real images.

</details>


### [128] [Automated Machine Learning for Unsupervised Tabular Tasks](https://arxiv.org/abs/2510.07569)
*Prabhant Singh,Pieter Gijsbers,Elif Ceren Gok Yildirim,Murat Onur Yildirim,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: LOTUS leverages Optimal Transport distances to perform model selection for unsupervised tasks like clustering and outlier detection. It identifies pipelines based on data distribution similarity.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of selecting suitable machine learning pipelines for unsupervised tasks across diverse datasets by leveraging data distribution similarities.

Method: LOTUS employs Optimal Transport distances to measure the similarity between tabular datasets and uses this information for recommending suitable machine learning pipelines for tasks such as clustering and outlier detection.

Result: LOTUS demonstrated its capability to recommend effective pipelines through experiments that outperform strong baselines in model selection.

Conclusion: LOTUS offers a promising unified approach for model selection in unsupervised machine learning applications, particularly outlier detection and clustering.

Abstract: In this work, we present LOTUS (Learning to Learn with Optimal Transport for
Unsupervised Scenarios), a simple yet effective method to perform model
selection for multiple unsupervised machine learning(ML) tasks such as outlier
detection and clustering. Our intuition behind this work is that a machine
learning pipeline will perform well in a new dataset if it previously worked
well on datasets with a similar underlying data distribution. We use Optimal
Transport distances to find this similarity between unlabeled tabular datasets
and recommend machine learning pipelines with one unified single method on two
downstream unsupervised tasks: outlier detection and clustering. We present the
effectiveness of our approach with experiments against strong baselines and
show that LOTUS is a very promising first step toward model selection for
multiple unsupervised ML tasks.

</details>


### [129] [Characterizing the Multiclass Learnability of Forgiving 0-1 Loss Functions](https://arxiv.org/abs/2510.08382)
*Jacob Trauger,Tyson Trauger,Ambuj Tewari*

Main category: cs.LG

TL;DR: The paper characterizes learnability for forgiving 0-1 loss functions in the finite label multiclass setting using a generalized Natarajan Dimension, connecting it to set-valued feedback learning.


<details>
  <summary>Details</summary>
Motivation: To understand learnability of forgiving 0-1 loss functions under multiclass classification, and establish theoretical foundations using a combinatorial framework.

Method: The authors introduce a Generalized Natarajan Dimension based on the original Natarajan Dimension and use it to analyze learnability, providing proofs and linking findings to set-valued feedback.

Result: The Generalized Natarajan Dimension is proven to be the key determinative factor for learning forgiving 0-1 loss functions. A connection with set-valued feedback learning is established.

Conclusion: Finite Generalized Natarajan Dimension characterizes the learnability of forgiving 0-1 loss functions, emphasizing the importance of combinatorial dimensions in understanding learning problems.

Abstract: In this paper we will give a characterization of the learnability of
forgiving 0-1 loss functions in the finite label multiclass setting. To do
this, we create a new combinatorial dimension that is based off of the
Natarajan Dimension \citep{natarajan1989learning} and we show that a hypothesis
class is learnable in our setting if and only if this Generalized Natarajan
Dimension is finite. We also show a connection to learning with set-valued
feedback. Through our results we show that the learnability of a set learning
problem is characterized by the Natarajan Dimension.

</details>


### [130] [Symbolic-Diffusion: Deep Learning Based Symbolic Regression with D3PM Discrete Token Diffusion](https://arxiv.org/abs/2510.07570)
*Ryan T. Tymkow,Benjamin D. Schnapp,Mojtaba Valipour,Ali Ghodshi*

Main category: cs.LG

TL;DR: The paper introduces Symbolic Diffusion, a diffusion-based model for symbolic regression that simultaneously generates all tokens of an equation, showing comparable or improved performance compared to traditional autoregressive methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address limitations of autoregressive generation—like sequential token dependency—by proposing a simultaneous token generation approach for symbolic regression.

Method: They developed Symbolic Diffusion, a discrete state-space diffusion model, and compared its performance against SymbolicGPT. Both models used equivalent transformer architectures.

Result: The diffusion-based method demonstrated comparable or, according to certain metrics, superior performance compared to the autoregressive SymbolicGPT model.

Conclusion: Symbolic Diffusion shows promise for advancing neural-network-based symbolic regression and opens pathways for further research in this domain.

Abstract: Symbolic regression refers to the task of finding a closed-form mathematical
expression to fit a set of data points. Genetic programming based techniques
are the most common algorithms used to tackle this problem, but recently,
neural-network based approaches have gained popularity. Most of the leading
neural-network based models used for symbolic regression utilize
transformer-based autoregressive models to generate an equation conditioned on
encoded input points. However, autoregressive generation is limited to
generating tokens left-to-right, and future generated tokens are conditioned
only on previously generated tokens. Motivated by the desire to generate all
tokens simultaneously to produce improved closed-form equations, we propose
Symbolic Diffusion, a D3PM based discrete state-space diffusion model which
simultaneously generates all tokens of the equation at once using discrete
token diffusion. Using the bivariate dataset developed for SymbolicGPT, we
compared our diffusion-based generation approach to an autoregressive model
based on SymbolicGPT, using equivalent encoder and transformer architectures.
We demonstrate that our novel approach of using diffusion-based generation for
symbolic regression can offer comparable and, by some metrics, improved
performance over autoregressive generation in models using similar underlying
architectures, opening new research opportunities in neural-network based
symbolic regression.

</details>


### [131] [ClauseLens: Clause-Grounded, CVaR-Constrained Reinforcement Learning for Trustworthy Reinsurance Pricing](https://arxiv.org/abs/2510.08429)
*Stella C. Dong,James R. Finlay*

Main category: cs.LG

TL;DR: ClauseLens framework enhances reinsurance treaty pricing by combining legal clauses with Risk-Aware Constrained Markov Decision Process (RA-CMDP), yielding more regulation-compliant, transparent, and risk-driven quotes.


<details>
  <summary>Details</summary>
Motivation: Current practices in reinsurance treaty pricing are opaque and difficult to audit, necessitating a transparent and regulation-compliant approach that integrates legal context into pricing.

Method: ClauseLens employs a RA-CMDP approach, embedding legal and underwriting clauses into the agent’s observations to constrain actions and generate clause-grounded explanations. It is evaluated using a multi-agent simulator calibrated to industry data.

Result: ClauseLens results in 51% reduced solvency violations, 27.9% improved tail-risk performance (CVaR_0.10), 88.2% accuracy in clause-grounded explanations, and high retrieval precision (87.4%) and recall (91.1%).

Conclusion: Embedding legal context in decision and explanation processes enhances interpretability, auditability, and compliance of reinsurance treaty pricing, aligning with global regulatory standards such as Solvency II and the EU AI Act.

Abstract: Reinsurance treaty pricing must satisfy stringent regulatory standards, yet
current quoting practices remain opaque and difficult to audit. We introduce
ClauseLens, a clause-grounded reinforcement learning framework that produces
transparent, regulation-compliant, and risk-aware treaty quotes.
  ClauseLens models the quoting task as a Risk-Aware Constrained Markov
Decision Process (RA-CMDP). Statutory and policy clauses are retrieved from
legal and underwriting corpora, embedded into the agent's observations, and
used both to constrain feasible actions and to generate clause-grounded natural
language justifications.
  Evaluated in a multi-agent treaty simulator calibrated to industry data,
ClauseLens reduces solvency violations by 51%, improves tail-risk performance
by 27.9% (CVaR_0.10), and achieves 88.2% accuracy in clause-grounded
explanations with retrieval precision of 87.4% and recall of 91.1%.
  These findings demonstrate that embedding legal context into both decision
and explanation pathways yields interpretable, auditable, and
regulation-aligned quoting behavior consistent with Solvency II, NAIC RBC, and
the EU AI Act.

</details>


### [132] [Accuracy, Memory Efficiency and Generalization: A Comparative Study on Liquid Neural Networks and Recurrent Neural Networks](https://arxiv.org/abs/2510.07578)
*Shilong Zong,Alex Bierly,Almuatazbellah Boker,Hoda Eldardiry*

Main category: cs.LG

TL;DR: This comparative review analyzes liquid neural networks (LNNs) and traditional recurrent neural networks (RNNs), focusing on accuracy, memory efficiency, and generalization ability, and identifies areas for future research.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to understand the strengths and weaknesses of LNNs compared to RNNs in handling sequential data, addressing shortcomings, and exploring their potential for broader applications.

Method: The authors performed a systematic review of existing research to analyze principles, mathematical models, characteristics, and challenges of LNNs and RNNs.

Result: LNNs display strong performance in noisy and non-stationary data handling and out-of-distribution generalization, with some variants excelling in parameter efficiency and computational speed compared to RNNs.

Conclusion: While LNNs show promise for advanced applications, RNNs continue to be foundational in sequence modeling due to their robust ecosystem. Future improvements in LNN scalability are necessary to enhance their adoption in complex scenarios.

Abstract: This review aims to conduct a comparative analysis of liquid neural networks
(LNNs) and traditional recurrent neural networks (RNNs) and their variants,
such as long short-term memory networks (LSTMs) and gated recurrent units
(GRUs). The core dimensions of the analysis include model accuracy, memory
efficiency, and generalization ability. By systematically reviewing existing
research, this paper explores the basic principles, mathematical models, key
characteristics, and inherent challenges of these neural network architectures
in processing sequential data. Research findings reveal that LNN, as an
emerging, biologically inspired, continuous-time dynamic neural network,
demonstrates significant potential in handling noisy, non-stationary data, and
achieving out-of-distribution (OOD) generalization. Additionally, some LNN
variants outperform traditional RNN in terms of parameter efficiency and
computational speed. However, RNN remains a cornerstone in sequence modeling
due to its mature ecosystem and successful applications across various tasks.
This review identifies the commonalities and differences between LNNs and RNNs,
summarizes their respective shortcomings and challenges, and points out
valuable directions for future research, particularly emphasizing the
importance of improving the scalability of LNNs to promote their application in
broader and more complex scenarios.

</details>


### [133] [gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity](https://arxiv.org/abs/2510.08450)
*Hugh Blayney,Álvaro Arroyo,Xiaowen Dong,Michael M. Bronstein*

Main category: cs.LG

TL;DR: Graph Neural Networks (GNNs) face the issue of over-squashing, where large receptive field information is compressed into fixed-size node vectors, creating bottlenecks. This paper introduces a novel GNN architecture addressing this issue, validated through synthetic and real-world tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the over-squashing problem in GNNs, where nodes fail to retain sufficient information due to limited capacity, hindering model performance.

Method: The authors propose a new GNN architecture influenced by methods like associative memories and xLSTM models, improving storage and retrieval capacity for node representations.

Result: The proposed architecture exhibits enhanced performance in synthetic tasks measuring storage capacity and real-world benchmarks.

Conclusion: Enhancing node capacity in GNNs is crucial to mitigate over-squashing, and the introduced architecture successfully demonstrates its effectiveness in various scenarios.

Abstract: Graph Neural Networks (GNNs) leverage the graph structure to transmit
information between nodes, typically through the message-passing mechanism.
While these models have found a wide variety of applications, they are known to
suffer from over-squashing, where information from a large receptive field of
node representations is collapsed into a single fixed sized vector, resulting
in an information bottleneck. In this paper, we re-examine the over-squashing
phenomenon through the lens of model storage and retrieval capacity, which we
define as the amount of information that can be stored in a node's
representation for later use. We study some of the limitations of existing
tasks used to measure over-squashing and introduce a new synthetic task to
demonstrate that an information bottleneck can saturate this capacity.
Furthermore, we adapt ideas from the sequence modeling literature on
associative memories, fast weight programmers, and the xLSTM model to develop a
novel GNN architecture with improved capacity. We demonstrate strong
performance of this architecture both on our capacity synthetic task, as well
as a range of real-world graph benchmarks.

</details>


### [134] [Expanding the Action Space of LLMs to Reason Beyond Language](https://arxiv.org/abs/2510.07581)
*Zhongqi Yue,Weishi Wang,Yundaichuan Zhan,Juncheng Li,Daniel Dahlmeier,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: Large Language Models (LLMs) extend their capabilities by interacting with external environments via an expanded action space, avoiding vocabulary-only constraints.


<details>
  <summary>Details</summary>
Motivation: LLMs face limitations in interacting with external environments when confined to language-based actions, necessitating a system to seamlessly integrate external operations.

Method: The paper introduces Expanded Action space (ExpA) to enable LLMs to switch between language reasoning and external environment-specific actions, supported by ExpA Reinforcement Learning (EARL) for efficient exploration.

Result: EARL achieves superior performance in multi-turn interaction and contingent planning tasks, with robust success in calculator-based tasks and perfect accuracy on complex sorting problems.

Conclusion: Decoupling language reasoning and external interaction via ExpA enhances the capabilities of LLMs, opening avenues for efficient multitask execution and algorithmic discoveries.

Abstract: Large Language Models (LLMs) are powerful reasoners in natural language, but
their actions are typically confined to outputting vocabulary tokens. As a
result, interactions with external environments -- such as symbolic operators
or simulators -- must be expressed through text in predefined formats, parsed,
and routed to external interfaces. This overloads the model's language with
both reasoning and control duties, and requires a hand-crafted parser, external
to the LLM. To address this, we decouple environment interactions from language
by internalizing them in an Expanded Action space (ExpA), beyond the
vocabulary. The model starts reasoning in the default language environment, but
may trigger routing actions and switch to an external environment at any time.
From there, the model can only invoke environment-specific actions, receive
feedback from the environment, and potentially route back to language as a
result. To promote effective exploration of the expanded action space and new
environments, we introduce ExpA Reinforcement Learning (EARL) with
counterfactual policy optimization. On tasks requiring multi-turn interactions
and contingent planning, EARL outperforms strong baselines with
vocabulary-constrained actions. It performs robustly across calculator-based
multi-task learning and, in the partially observed sorting problem, achieves
perfect Sort-4 accuracy while self-discovering an efficient algorithm
competitive with classical designs.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [135] [A Digital Pheromone-Based Approach for In/Out-of-Control Classification](https://arxiv.org/abs/2510.07329)
*Pedro Pestana,M. Fátima Brilhante*

Main category: cs.NE

TL;DR: The paper proposes a bio-inspired ant colony optimization approach to monitor and predict Out-of-Control (OutC) states in industrial production, using scores derived from digital ant behavior.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the necessity of real-time and reliable classification of In Control (InC) and Out of Control (OutC) states and the prediction of transitions in complex production lines.

Method: The method simulates ant colony behavior, where sequences of temperature readings act as digital ants depositing virtual pheromones, generating scores like Base Score, Threat Score, Environmental Score, combined to monitor and classify system states dynamically.

Result: The study applies the method to industrial potato chip frying systems, showing the approach effectively monitors system states and predicts transitions from InC to OutC using scores derived from ant behavior.

Conclusion: The ant colony optimization-inspired framework offers an adaptive and explainable solution for process monitoring and predictive maintenance in industrial environments.

Abstract: In complex production lines, it is essential to have strict, fast-acting
rules to determine whether the system is In Control (InC) or Out of Control
(OutC). This study explores a bio-inspired method that digitally mimics ant
colony behavior to classify InC/OutC states and forecast imminent transitions
requiring maintenance. A case study on industrial potato chip frying provides
the application context. During each two-minute frying cycle, sequences of
eight temperature readings are collected. Each sequence is treated as a digital
ant depositing virtual pheromones, generating a Base Score. New sequences,
representing new ants, can either reinforce or weaken this score, leading to a
Modified Base Score that reflects the system's evolving condition. Signals such
as extreme temperatures, large variations within a sequence, or the detection
of change-points contribute to a Threat Score, which is added to the Modified
Base Score. Since pheromones naturally decay over time unless reinforced, an
Environmental Score is incorporated to reflect recent system dynamics,
imitating real ant behavior. This score is calculated from the Modified Base
Scores collected over the past hour. The resulting Total Score - the sum of the
Modified Base Score, Threat Score, and Environmental Score - is used as the
main indicator for real-time system classification and forecasting of
transitions from InC to OutC. This ant colony optimization-inspired approach
provides an adaptive and interpretable framework for process monitoring and
predictive maintenance in industrial environments.

</details>


### [136] [Learning Neuron Dynamics within Deep Spiking Neural Networks](https://arxiv.org/abs/2510.07341)
*Eric Jahns,Davi Moreno,Michel A. Kinsy*

Main category: cs.NE

TL;DR: This paper introduces Learnable Neuron Models (LNMs), a parametric approach to enhance the performance of Spiking Neural Networks by learning neuron dynamics, achieving state-of-the-art results on various datasets.


<details>
  <summary>Details</summary>
Motivation: Current Spiking Neural Networks struggle with limited performance due to simplistic neuron models like the Leaky Integrate-and-Fire model, which fail to capture complex temporal dynamics.

Method: The paper proposes Learnable Neuron Models (LNMs), a parametric system employing low-degree polynomial parameterization to automatically learn neuron dynamics during the training of deep SNNs.

Result: State-of-the-art performance was demonstrated across datasets such as CIFAR-10, CIFAR-100, ImageNet, and CIFAR-10 DVS using the LNMs.

Conclusion: LNMs provide an effective and scalable method for improving spiking architectures, overcoming limitations of traditional neuron models like manual hyperparameter tuning and complexity issues.

Abstract: Spiking Neural Networks (SNNs) offer a promising energy-efficient alternative
to Artificial Neural Networks (ANNs) by utilizing sparse and asynchronous
processing through discrete spike-based computation. However, the performance
of deep SNNs remains limited by their reliance on simple neuron models, such as
the Leaky Integrate-and-Fire (LIF) model, which cannot capture rich temporal
dynamics. While more expressive neuron models exist, they require careful
manual tuning of hyperparameters and are difficult to scale effectively. This
difficulty is evident in the lack of successful implementations of complex
neuron models in high-performance deep SNNs. In this work, we address this
limitation by introducing Learnable Neuron Models (LNMs). LNMs are a general,
parametric formulation for non-linear integrate-and-fire dynamics that learn
neuron dynamics during training. By learning neuron dynamics directly from
data, LNMs enhance the performance of deep SNNs. We instantiate LNMs using
low-degree polynomial parameterizations, enabling efficient and stable
training. We demonstrate state-of-the-art performance in a variety of datasets,
including CIFAR-10, CIFAR-100, ImageNet, and CIFAR-10 DVS. LNMs offer a
promising path toward more scalable and high-performing spiking architectures.

</details>


### [137] [A Rotation-Invariant Embedded Platform for (Neural) Cellular Automata](https://arxiv.org/abs/2510.07440)
*Dominik Woiwode,Jakob Marten,Bodo Rosenhahn*

Main category: cs.NE

TL;DR: The paper introduces a modular, rotation-invariant platform for simulating neural cellular automata (NCA) in robotics, overcoming limitations of prior designs.


<details>
  <summary>Details</summary>
Motivation: To address limitations in prior hardware designs of physical NCAs and develop a robust modular system capable of rotation-invariance and independent operation.

Method: A symmetric, modular structure with battery-powered cells was implemented to ensure independent operation, seamless connectivity, and rotation-invariant functionality.

Result: A novel rotation-invariant NCA model for isotropic shape classification was demonstrated using the platform, with implementations openly shared.

Conclusion: This platform provides a strong foundation for physical realization of NCAs, paving the way for distributed robotics and self-organizing systems.

Abstract: This paper presents a rotation-invariant embedded platform for simulating
(neural) cellular automata (NCA) in modular robotic systems. Inspired by
previous work on physical NCA, we introduce key innovations that overcome
limitations in prior hardware designs. Our platform features a symmetric,
modular structure, enabling seamless connections between cells regardless of
orientation. Additionally, each cell is battery-powered, allowing it to operate
independently and retain its state even when disconnected from the collective.
To demonstrate the platform's applicability, we present a novel
rotation-invariant NCA model for isotropic shape classification. The proposed
system provides a robust foundation for exploring the physical realization of
NCA, with potential applications in distributed robotic systems and
self-organizing structures. Our implementation, including hardware, software
code, a simulator, and a video, is openly shared at:
https://github.com/dwoiwode/embedded_nca

</details>


### [138] [Co-design is powerful and not free](https://arxiv.org/abs/2510.08368)
*Yi Zhang,Yue Xie,Tao Sun,Fumiya Iida*

Main category: cs.NE

TL;DR: This paper explores a framework for co-designing robot morphology and control and examines when co-design is beneficial.


<details>
  <summary>Details</summary>
Motivation: To understand when the co-design of robot morphology and control is necessary for improving robotic performance in task-specific scenarios.

Method: A unified framework where morphology and control parameters are merged into a single neural network for end-to-end optimization. Case studies include tasks constrained by static obstacles to assess various performance metrics.

Result: Co-design outperforms control-only optimization in situations where the morphology is poorly matched to the task but provides no advantage when the baseline morphology is sufficient.

Conclusion: This work enhances the understanding of embodied intelligence and provides practical criteria for deciding between co-design and control-only optimization in robot design.

Abstract: Robotic performance emerges from the coupling of body and controller, yet it
remains unclear when morphology-control co-design is necessary. We present a
unified framework that embeds morphology and control parameters within a single
neural network, enabling end-to-end joint optimization. Through case studies in
static-obstacle-constrained reaching, we evaluate trajectory error, success
rate, and collision probability. The results show that co-design provides clear
benefits when morphology is poorly matched to the task, such as near obstacles
or workspace boundaries, where structural adaptation simplifies control.
Conversely, when the baseline morphology already affords sufficient capability,
control-only optimization often matches or exceeds co-design. By clarifying
when control is enough and when it is not, this work advances the understanding
of embodied intelligence and offers practical guidance for embodiment-aware
robot design.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [139] [Type, Ability, and Effect Systems: Perspectives on Purity, Semantics, and Expressiveness](https://arxiv.org/abs/2510.07582)
*Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: The paper explores the trade-offs between monads, type-and-effect systems, and capability systems in programming, proposing a synthesis for better handling purity and interaction of systems.


<details>
  <summary>Details</summary>
Motivation: Current programming paradigms need improved systems to precisely yet usability separate pure computation from impure interactions.

Method: It defines purity based on contextual equivalence and measures expressiveness via semantic completeness. Then compares and synthesizes system strengths using a formal model.

Result: Minimal effect systems and capability systems were found incomparable in expressiveness, while a synthesis combines their strengths.

Conclusion: The proposed synthesis improves upon existing systems by offering a better balance between precision, expressiveness, and usability in effect typing disciplines.

Abstract: Programming benefits from a clear separation between pure, mathematical
computation and impure, effectful interaction with the world. Existing
approaches to enforce this separation include monads, type-and-effect systems,
and capability systems. All share a tension between precision and usability,
and each one has non-obvious strengths and weaknesses.
  This paper aims to raise the bar in assessing such systems. First, we propose
a semantic definition of purity, inspired by contextual equivalence, as a
baseline independent of any specific typing discipline. Second, we propose that
expressiveness should be measured by the degree of completeness, i.e., how many
semantically pure terms can be typed as pure. Using this measure, we focus on
minimal meaningful effect and capability systems and show that they are
incomparable, i.e., neither subsumes the other in terms of expressiveness.
  Based on this result, we propose a synthesis and show that type, ability, and
effect systems combine their respective strengths while avoiding their
weaknesses. As part of our formal model, we provide a logical relation to
facilitate proofs of purity and other properties for a variety of effect typing
disciplines.

</details>


### [140] [The Functional Machine Calculus III: Control](https://arxiv.org/abs/2510.07851)
*Willem Heijltjes*

Main category: cs.PL

TL;DR: The paper introduces extensions to the Functional Machine Calculus to unify functional and imperative programming paradigms, adding support for branching and looping control flows.


<details>
  <summary>Details</summary>
Motivation: To integrate functional and imperative programming within a single unified calculus, addressing limitations in embedding imperative constructs and control flows.

Method: Extended operational semantics based on a simplified Krivine machine, with additions like multiple operand stacks and a continuation stack to model computation features.

Result: Provides a new calculus that embeds imperative language constructs while maintaining confluent reduction semantics and typed termination guarantees.

Conclusion: The developed calculus successfully models computation unifying functional and imperative paradigms, ensuring strong formal properties such as normalization and termination.

Abstract: The Functional Machine Calculus (Heijltjes 2022) is a new approach to
unifying the imperative and functional programming paradigms. It extends the
lambda-calculus, preserving the key features of confluent reduction and typed
termination, to embed computational effects, evaluation strategies, and control
flow operations. The first instalment modelled sequential higher-order
computation with global store, input/output, probabilities, and
non-determinism, and embedded both the call-by-name and call-by-value
lambda-calculus, as well as Moggi's computational metalanguage and Levy's
call-by-push-value. The present paper extends the calculus from sequential to
branching and looping control flow. This allows the faithful embedding of a
minimal but complete imperative language, including conditionals, exception
handling, and iteration, as well as constants and algebraic data types.
  The calculus is defined through a simple operational semantics, extending the
(simplified) Krivine machine for the lambda-calculus with multiple operand
stacks to model effects and a continuation stack to model sequential,
branching, and looping computation. It features a confluent reduction relation
and a system of simple types that guarantees termination of the machine and
strong normalization of reduction (in the absence of iteration). These
properties carry over to the embedded imperative language, providing a unified
functional-imperative model of computation that supports simple types, a direct
and intuitive operational semantics, and a confluent reduction semantics.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [141] [FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams](https://arxiv.org/abs/2510.07417)
*Corban Rivera,Grayson Byrd,Meghan Booker,Bethany Kemp,Allison Gaines,Emma Holmes,James Uplinger,Celso M de Melo,David Handelman*

Main category: cs.RO

TL;DR: The paper introduces FLEET, a framework that converts natural language commands into multi-robot schedules, combining an LLM front-end and formal optimization techniques for efficient operation.


<details>
  <summary>Details</summary>
Motivation: Current solutions for coordinating robot teams from natural language struggle with accuracy or require restrictive closed-world models.

Method: FLEET employs a hybrid decentralized approach where an LLM creates task structures and robot-task fitness metrics, and a formal optimization back-end uses MILP to generate schedules.

Result: FLEET outperforms generative planners in multi-robot coordination tasks, with real-world validation using heterogeneous quadruped robots.

Conclusion: Integrating LLMs with MILP-based optimizations enables robust, capable robotic teamwork for free-form tasks directed by natural language.

Abstract: Coordinating heterogeneous robot teams from free-form natural-language
instructions is hard. Language-only planners struggle with long-horizon
coordination and hallucination, while purely formal methods require
closed-world models. We present FLEET, a hybrid decentralized framework that
turns language into optimized multi-robot schedules. An LLM front-end produces
(i) a task graph with durations and precedence and (ii) a capability-aware
robot--task fitness matrix; a formal back-end solves a makespan-minimization
problem while the underlying robots execute their free-form subtasks with
agentic closed-loop control. Across multiple free-form language-guided autonomy
coordination benchmarks, FLEET improves success over state of the art
generative planners on two-agent teams across heterogeneous tasks. Ablations
show that mixed integer linear programming (MILP) primarily improves temporal
structure, while LLM-derived fitness is decisive for capability-coupled tasks;
together they deliver the highest overall performance. We demonstrate the
translation to real world challenges with hardware trials using a pair of
quadruped robots with disjoint capabilities.

</details>


### [142] [VeMo: A Lightweight Data-Driven Approach to Model Vehicle Dynamics](https://arxiv.org/abs/2510.07447)
*Girolamo Oddo,Roberto Nuca,Matteo Parsani*

Main category: cs.RO

TL;DR: The paper presents a GRU-based encoder-decoder model for predicting a vehicle's future state with high accuracy and robustness, even under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: Lack of detailed structural information about vehicles, especially in autonomous driving applications, necessitates developing data-driven models for predicting vehicle dynamics.

Method: The authors designed a lightweight encoder-decoder model based on Gate Recurrent Unit (GRU) layers to estimate future states from past states and driver control actions.

Result: The model achieves a mean relative error below 2.6% in challenging scenarios and robustly processes noisy input data across various frequency components.

Conclusion: The proposed model is data-driven, free of physical constraints, and provides physically consistent output signals like accelerations, yaw rate, and longitudinal velocity, making it suitable for vehicle modeling with limited structural information.

Abstract: Developing a dynamic model for a high-performance vehicle is a complex
problem that requires extensive structural information about the system under
analysis. This information is often unavailable to those who did not design the
vehicle and represents a typical issue in autonomous driving applications,
which are frequently developed on top of existing vehicles; therefore, vehicle
models are developed under conditions of information scarcity. This paper
proposes a lightweight encoder-decoder model based on Gate Recurrent Unit
layers to correlate the vehicle's future state with its past states, measured
onboard, and control actions the driver performs. The results demonstrate that
the model achieves a maximum mean relative error below 2.6% in extreme dynamic
conditions. It also shows good robustness when subject to noisy input data
across the interested frequency components. Furthermore, being entirely
data-driven and free from physical constraints, the model exhibits physical
consistency in the output signals, such as longitudinal and lateral
accelerations, yaw rate, and the vehicle's longitudinal velocity.

</details>


### [143] [HJCD-IK: GPU-Accelerated Inverse Kinematics through Batched Hybrid Jacobian Coordinate Descent](https://arxiv.org/abs/2510.07514)
*Cael Yasutake,Zachary Kingston,Brian Plancher*

Main category: cs.RO

TL;DR: HJCD-IK is a GPU-accelerated solver for robotic inverse kinematics that combines sampling, greedy initialization, and Jacobian polishing to improve speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address limitations in traditional IK solvers, such as computational inefficiency, local minima convergence, and restrictions on degrees-of-freedom.

Method: HJCD-IK integrates an orientation-based greedy coordinate descent initialization with a Jacobian-based polishing routine, leveraging GPU acceleration.

Result: This approach achieves superior performance compared to state-of-the-art solvers in accuracy, latency, and distribution quality of samples.

Conclusion: HJCD-IK represents a significant improvement in the field of inverse kinematics, advancing the accuracy-speed tradeoff and providing open-source tools for community use.

Abstract: Inverse Kinematics (IK) is a core problem in robotics, in which joint
configurations are found to achieve a desired end-effector pose. Although
analytical solvers are fast and efficient, they are limited to systems with low
degrees-of-freedom and specific topological structures. Numerical
optimization-based approaches are more general, but suffer from high
computational costs and frequent convergence to spurious local minima. Recent
efforts have explored the use of GPUs to combine sampling and optimization to
enhance both the accuracy and speed of IK solvers. We build on this recent
literature and introduce HJCD-IK, a GPU-accelerated, sampling-based hybrid
solver that combines an orientation-aware greedy coordinate descent
initialization scheme with a Jacobian-based polishing routine. This design
enables our solver to improve both convergence speed and overall accuracy as
compared to the state-of-the-art, consistently finding solutions along the
accuracy-latency Pareto frontier and often achieving order-of-magnitude gains.
In addition, our method produces a broad distribution of high-quality samples,
yielding the lowest maximum mean discrepancy. We release our code open-source
for the benefit of the community.

</details>


### [144] [AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation](https://arxiv.org/abs/2510.07548)
*Adam Hung,Fan Yang,Abhinav Kumar,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: The paper proposes Amortized Value Optimization (AVO) to improve the efficiency and performance of trajectory optimization for dexterous manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Current trajectory optimization methods for dexterous manipulation struggle when sub-tasks are optimized independently, leading to inefficiencies and high computational costs.

Method: AVO introduces a learned value function to guide trajectory optimization, incorporating predictions about future task performance to reduce computation and enhance optimization.

Result: AVO was validated on screwdriver manipulation tasks in both simulation and real-world experiments, showing improved task outcomes and reduced computational demands (by 50%).

Conclusion: Incorporating learned value functions into trajectory optimization can improve performance and efficiency, making dexterous manipulation tasks more feasible.

Abstract: Dexterous manipulation tasks often require switching between different
contact modes, such as rolling, sliding, sticking, or non-contact contact
modes. When formulating dexterous manipulation tasks as a trajectory
optimization problem, a common approach is to decompose these tasks into
sub-tasks for each contact mode, which are each solved independently.
Optimizing each sub-task independently can limit performance, as optimizing
contact points, contact forces, or other variables without information about
future sub-tasks can place the system in a state from which it is challenging
to make progress on subsequent sub-tasks. Further, optimizing these sub-tasks
is very computationally expensive. To address these challenges, we propose
Amortized Value Optimization (AVO), which introduces a learned value function
that predicts the total future task performance. By incorporating this value
function into the cost of the trajectory optimization at each planning step,
the value function gradients guide the optimizer toward states that minimize
the cost in future sub-tasks. This effectively bridges separately optimized
sub-tasks, and accelerates the optimization by reducing the amount of online
computation needed. We validate AVO on a screwdriver grasping and turning task
in both simulation and real world experiments, and show improved performance
even with 50% less computational budget compared to trajectory optimization
without the value function.

</details>


### [145] [Inspection Planning Primitives with Implicit Models](https://arxiv.org/abs/2510.07611)
*Jingyang You,Hanna Kurniawati,Lashika Medagoda*

Main category: cs.RO

TL;DR: The paper proposes Inspection Planning Primitives with Implicit Models (IPIM) to enable efficient inspection planning using neural Signed Distance Functions (SDFs), reducing memory usage significantly and maintaining trajectory quality.


<details>
  <summary>Details</summary>
Motivation: Infrastructure inspection is becoming increasingly critical due to aging and complexity, but efficient inspection planning often demands excessive memory, particularly for large, intricate structures. Neural SDF-based representation offers potential solutions, but current planners lack compatibility without inefficient model transformations.

Method: The authors introduce a set of primitive computations called IPIM, tailored for neural Signed Distance Functions. This enables inspection planners based on sampling to directly use implicit models throughout the planning process.

Result: Testing on three scenarios, including a real-world structure with over 92M triangular mesh faces, demonstrated that IPIM-based planners can achieve comparable trajectory quality to the state-of-the-art while consuming up to 70x less memory.

Conclusion: IPIM enables memory-efficient inspection planning using neural SDFs, avoiding explicit model transformations and ensuring high-quality trajectory generation for complex structures.

Abstract: The aging and increasing complexity of infrastructures make efficient
inspection planning more critical in ensuring safety. Thanks to sampling-based
motion planning, many inspection planners are fast. However, they often require
huge memory. This is particularly true when the structure under inspection is
large and complex, consisting of many struts and pillars of various geometry
and sizes. Such structures can be represented efficiently using implicit
models, such as neural Signed Distance Functions (SDFs). However, most
primitive computations used in sampling-based inspection planner have been
designed to work efficiently with explicit environment models, which in turn
requires the planner to use explicit environment models or performs frequent
transformations between implicit and explicit environment models during
planning. This paper proposes a set of primitive computations, called
Inspection Planning Primitives with Implicit Models (IPIM), that enable
sampling-based inspection planners to entirely use neural SDFs representation
during planning. Evaluation on three scenarios, including inspection of a
complex real-world structure with over 92M triangular mesh faces, indicates
that even a rudimentary sampling-based planner with IPIM can generate
inspection trajectories of similar quality to those generated by the
state-of-the-art planner, while using up to 70x less memory than the
state-of-the-art inspection planner.

</details>


### [146] [GATO: GPU-Accelerated and Batched Trajectory Optimization for Scalable Edge Model Predictive Control](https://arxiv.org/abs/2510.07625)
*Alexander Du,Emre Adabag,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: GATO, a GPU-accelerated batched trajectory optimization solver, achieves real-time performance for moderate batch sizes using innovative parallelism techniques.


<details>
  <summary>Details</summary>
Motivation: Real-time Model Predictive Control applications face computational challenges when dealing with moderate batches of nonlinear trajectory optimization problems.

Method: GATO integrates block-, warp-, and thread-level parallelism across algorithm, software, and hardware design to optimize solver performance.

Result: GATO achieves 18-21x speedups over CPU baselines and 1.4-16x over GPU baselines in simulated benchmarks, improves disturbance rejection, and validates its performance on industrial hardware.

Conclusion: GATO enhances throughput for moderate batch sizes, bridging a key performance gap in GPU-based MPC applications, and is open-sourced for broader use and reproducibility.

Abstract: While Model Predictive Control (MPC) delivers strong performance across
robotics applications, solving the underlying (batches of) nonlinear trajectory
optimization (TO) problems online remains computationally demanding. Existing
GPU-accelerated approaches typically (i) parallelize a single solve to meet
real-time deadlines, (ii) scale to very large batches at slower-than-real-time
rates, or (iii) achieve speed by restricting model generality (e.g., point-mass
dynamics or a single linearization). This leaves a large gap in solver
performance for many state-of-the-art MPC applications that require real-time
batches of tens to low-hundreds of solves. As such, we present GATO, an open
source, GPU-accelerated, batched TO solver co-designed across algorithm,
software, and computational hardware to deliver real-time throughput for these
moderate batch size regimes. Our approach leverages a combination of block-,
warp-, and thread-level parallelism within and across solves for ultra-high
performance. We demonstrate the effectiveness of our approach through a
combination of: simulated benchmarks showing speedups of 18-21x over CPU
baselines and 1.4-16x over GPU baselines as batch size increases; case studies
highlighting improved disturbance rejection and convergence behavior; and
finally a validation on hardware using an industrial manipulator. We open
source GATO to support reproducibility and adoption.

</details>


### [147] [Differentiable Particle Optimization for Fast Sequential Manipulation](https://arxiv.org/abs/2510.07674)
*Lucas Chen,Shrutheesh Raman Iyer,Zachary Kingston*

Main category: cs.RO

TL;DR: SPaSM is a GPU-parallelized framework for solving sequential robot manipulation tasks efficiently, achieving millisecond solution times.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the computational challenges of finding real-time, collision-free trajectories for sequential robot manipulation tasks in high-dimensional configuration spaces.

Method: The approach involves full GPU parallelization with optimized CUDA kernels, using a two-stage particle optimization strategy—initial sampling for placement constraints and subsequent trajectory optimization in joint space.

Result: SPaSM achieves a 100% success rate with millisecond-level solution times, demonstrating a remarkable 4000x speedup compared to prior methods.

Conclusion: The proposed SPaSM framework effectively leverages GPU-based acceleration for real-time performance in complex robot manipulation tasks, overcoming previous limitations of CPU-GPU communication and inefficiencies.

Abstract: Sequential robot manipulation tasks require finding collision-free
trajectories that satisfy geometric constraints across multiple object
interactions in potentially high-dimensional configuration spaces. Solving
these problems in real-time and at large scales has remained out of reach due
to computational requirements. Recently, GPU-based acceleration has shown
promising results, but prior methods achieve limited performance due to CPU-GPU
data transfer overhead and complex logic that prevents full hardware
utilization. To this end, we present SPaSM (Sampling Particle optimization for
Sequential Manipulation), a fully GPU-parallelized framework that compiles
constraint evaluation, sampling, and gradient-based optimization into optimized
CUDA kernels for end-to-end trajectory optimization without CPU coordination.
The method consists of a two-stage particle optimization strategy: first
solving placement constraints through massively parallel sampling, then lifting
solutions to full trajectory optimization in joint space. Unlike hierarchical
approaches, SPaSM jointly optimizes object placements and robot trajectories to
handle scenarios where motion feasibility constrains placement options.
Experimental evaluation on challenging benchmarks demonstrates solution times
in the realm of $\textbf{milliseconds}$ with a 100% success rate; a
$4000\times$ speedup compared to existing approaches.

</details>


### [148] [EB-MBD: Emerging-Barrier Model-Based Diffusion for Safe Trajectory Optimization in Highly Constrained Environments](https://arxiv.org/abs/2510.07700)
*Raghav Mishra,Ian R. Manchester*

Main category: cs.RO

TL;DR: This paper introduces Emerging-Barrier Model-Based Diffusion (EB-MBD), which uses barrier functions to enforce constraints while improving computation efficiency and solution quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the performance issues and computational inefficiencies caused by constraints in Model-Based Diffusion systems, particularly when using Monte Carlo approximations.

Method: The authors propose Emerging-Barrier Model-Based Diffusion (EB-MBD) which introduces progressive barrier constraints to avoid catastrophic performance degradation, without relying on expensive projection-based methods.

Result: The method demonstrates superior solution quality in 2D collision avoidance and a 3D underwater manipulator task. It also achieves significantly lower computational requirements compared to projection-based solutions.

Conclusion: EB-MBD improves solution quality and computational efficiency for constrained systems in Model-Based Diffusion, making it a viable alternative to traditional methods.

Abstract: We propose enforcing constraints on Model-Based Diffusion by introducing
emerging barrier functions inspired by interior point methods. We show that
constraints on Model-Based Diffusion can lead to catastrophic performance
degradation, even on simple 2D systems due to sample inefficiency in the Monte
Carlo approximation of the score function. We introduce Emerging-Barrier
Model-Based Diffusion (EB-MBD) which uses progressively introduced barrier
constraints to avoid these problems, significantly improving solution quality,
without the need for computationally expensive operations such as projections.
We analyze the sampling liveliness of samples each iteration to inform barrier
parameter scheduling choice. We demonstrate results for 2D collision avoidance
and a 3D underwater manipulator system and show that our method achieves lower
cost solutions than Model-Based Diffusion, and requires orders of magnitude
less computation time than projection based methods.

</details>


### [149] [Probabilistically-Safe Bipedal Navigation over Uncertain Terrain via Conformal Prediction and Contraction Analysis](https://arxiv.org/abs/2510.07725)
*Kasidit Muenprasitivej,Ye Zhao,Glen Chou*

Main category: cs.RO

TL;DR: The paper presents a probabilistically safe planning and control strategy for bipedal robots traversing rough terrains by combining Gaussian Process regression, Conformal Prediction, and Model Predictive Control (MPC).


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by uncertain terrains, ensuring dynamic feasibility and safety for bipedal robots in achieving stable locomotion.

Method: The method integrates GP and CP for terrain uncertainty modeling, contraction-based reachable tubes for state convergence, torque control law for stabilizing angular momentum, and MPC for probabilistic safe planning.

Result: The framework enables safe traversal with goal reachability guarantees, demonstrated via simulations on the Digit bipedal robot in MuJoCo.

Conclusion: The proposed approach successfully ensures centroidal robustness and dynamic feasibility in uncertain terrains, providing probabilistic safety guarantees for bipedal robots.

Abstract: We address the challenge of enabling bipedal robots to traverse rough terrain
by developing probabilistically safe planning and control strategies that
ensure dynamic feasibility and centroidal robustness under terrain uncertainty.
Specifically, we propose a high-level Model Predictive Control (MPC) navigation
framework for a bipedal robot with a specified confidence level of safety that
(i) enables safe traversal toward a desired goal location across a terrain map
with uncertain elevations, and (ii) formally incorporates uncertainty bounds
into the centroidal dynamics of locomotion control. To model the rough terrain,
we employ Gaussian Process (GP) regression to estimate elevation maps and
leverage Conformal Prediction (CP) to construct calibrated confidence intervals
that capture the true terrain elevation. Building on this, we formulate
contraction-based reachable tubes that explicitly account for terrain
uncertainty, ensuring state convergence and tube invariance. In addition, we
introduce a contraction-based flywheel torque control law for the reduced-order
Linear Inverted Pendulum Model (LIPM), which stabilizes the angular momentum
about the center-of-mass (CoM). This formulation provides both probabilistic
safety and goal reachability guarantees. For a given confidence level, we
establish the forward invariance of the proposed torque control law by
demonstrating exponential stabilization of the actual CoM phase-space
trajectory and the desired trajectory prescribed by the high-level planner.
Finally, we evaluate the effectiveness of our planning framework through
physics-based simulations of the Digit bipedal robot in MuJoCo.

</details>


### [150] [Injecting Hallucinations in Autonomous Vehicles: A Component-Agnostic Safety Evaluation Framework](https://arxiv.org/abs/2510.07749)
*Alexandre Moreira Nascimento,Gabriel Kenji Godoy Shimanuki,Lúcio Flavio Vismari,João Batista Camargo Jr,Jorge Rady de Almeida Jr,Paulo Sergio Cugnasca,Anna Carolina Muller Queiroz,Jeremy Noah Bailenson*

Main category: cs.RO

TL;DR: The study introduces a framework to assess perception failures in autonomous vehicles by simulating various hallucination types (false perceptions). Over 18,350 simulations reveal the significant safety impacts of these failures, specifically perceptual latency and drift, on collision risks.


<details>
  <summary>Details</summary>
Motivation: Perception failures in autonomous vehicles are a critical safety issue, leading to accidents. Current methods of fault injection focus on specific sensors or modules, making them narrow and isolative.

Method: The authors reframe perception failures as observable hallucinations and propose a component-agnostic hallucination injection framework for simulation. They tested six hallucination types over 18,350 simulations using an open-source platform.

Result: The study quantified the effect of various hallucination types on collisions and near misses. It found that perceptual latency and drift notably increase collision risks, validating the framework's ability to evaluate AV safety.

Conclusion: The framework is scalable, statistically validated, and agnostic to components, offering a robust tool for AV safety evaluation. It has the potential to expedite AV development and promote fault-tolerant and resilient designs.

Abstract: Perception failures in autonomous vehicles (AV) remain a major safety concern
because they are the basis for many accidents. To study how these failures
affect safety, researchers typically inject artificial faults into hardware or
software components and observe the outcomes. However, existing fault injection
studies often target a single sensor or machine perception (MP) module,
resulting in siloed frameworks that are difficult to generalize or integrate
into unified simulation environments. This work addresses that limitation by
reframing perception failures as hallucinations, false perceptions that distort
an AV situational awareness and may trigger unsafe control actions. Since
hallucinations describe only observable effects, this abstraction enables
analysis independent of specific sensors or algorithms, focusing instead on how
their faults manifest along the MP pipeline. Building on this concept, we
propose a configurable, component-agnostic hallucination injection framework
that induces six plausible hallucination types in an iterative open-source
simulator. More than 18,350 simulations were executed in which hallucinations
were injected while AVs crossed an unsignalized transverse street with traffic.
The results statistically validate the framework and quantify the impact of
each hallucination type on collisions and near misses. Certain hallucinations,
such as perceptual latency and drift, significantly increase the risk of
collision in the scenario tested, validating the proposed paradigm can stress
the AV system safety. The framework offers a scalable, statistically validated,
component agnostic, and fully interoperable toolset that simplifies and
accelerates AV safety validations, even those with novel MP architectures and
components. It can potentially reduce the time-to-market of AV and lay the
foundation for future research on fault tolerance, and resilient AV design.

</details>


### [151] [Accurate and Noise-Tolerant Extraction of Routine Logs in Robotic Process Automation (Extended Version)](https://arxiv.org/abs/2510.08118)
*Massimiliano de Leoni,Faizan Ahmed Khan,Simone Agostinelli*

Main category: cs.RO

TL;DR: This paper proposes a clustering-based method to improve routine log extraction for robotic process mining, particularly under noisy conditions, outperforming previous techniques.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the gap in extracting routine logs effectively, especially in noisy scenarios, to enable robotic process automation beyond simply capturing routine actions.

Method: The authors develop a clustering-based technique, which extracts routine logs from UI interactions and compares its effectiveness against adapted existing techniques using nine datasets with different noise levels.

Result: Experiments demonstrate that the proposed technique achieves greater accuracy in routine log extraction compared to state-of-the-art methods, particularly under conditions with added noise.

Conclusion: The clustering-based approach provides a robust solution for routine log discovery, overcoming limitations of existing techniques and setting a new benchmark in robotic process mining accuracy.

Abstract: Robotic Process Mining focuses on the identification of the routine types
performed by human resources through a User Interface. The ultimate goal is to
discover routine-type models to enable robotic process automation. The
discovery of routine-type models requires the provision of a routine log.
Unfortunately, the vast majority of existing works do not directly focus on
enabling the model discovery, limiting themselves to extracting the set of
actions that are part of the routines. They were also not evaluated in
scenarios characterized by inconsistent routine execution, hereafter referred
to as noise, which reflects natural variability and occasional errors in human
performance. This paper presents a clustering-based technique that aims to
extract routine logs. Experiments were conducted on nine UI logs from the
literature with different levels of injected noise. Our technique was compared
with existing techniques, most of which are not meant to discover routine logs
but were adapted for the purpose. The results were evaluated through standard
state-of-the-art metrics, showing that we can extract more accurate routine
logs than what the state of the art could, especially in the presence of noise.

</details>


### [152] [Trajectory Conditioned Cross-embodiment Skill Transfer](https://arxiv.org/abs/2510.07773)
*YuHang Tang,Yixuan Lou,Pengfei Han,Haoming Song,Xinyi Ye,Dong Wang,Bin Zhao*

Main category: cs.RO

TL;DR: This paper introduces TrajSkill, a framework for transferring manipulation skills directly from human demonstration videos to robots. It uses optical flow trajectories to bridge the human-robot embodiment gap without relying on paired datasets or hand-crafted rewards.


<details>
  <summary>Details</summary>
Motivation: Human-to-robot skill transfer faces challenges due to embodiment differences, and existing methods relying on paired datasets or crafted rewards face scalability and generalization issues.

Method: TrajSkill leverages sparse optical flow trajectories derived from human motion as embodiment-agnostic cues. It uses visual and textual inputs to synthesize robot manipulation videos and translate them into executable actions for cross-embodiment skill transfer.

Result: Experiments on simulation data show significant improvements over state-of-the-art methods, with 39.6% reduction in FVD, 36.6% reduction in KVD, and up to 16.7% increase in cross-embodiment success rates. Real-world tests in kitchen tasks further validate the approach.

Conclusion: TrajSkill demonstrates effective human-to-robot skill transfer, bridging the embodiment gap and improving the practical applicability of manipulation skill learning through scalable and generalized techniques.

Abstract: Learning manipulation skills from human demonstration videos presents a
promising yet challenging problem, primarily due to the significant embodiment
gap between human body and robot manipulators. Existing methods rely on paired
datasets or hand-crafted rewards, which limit scalability and generalization.
We propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment
Skill Transfer, enabling robots to acquire manipulation skills directly from
human demonstration videos. Our key insight is to represent human motions as
sparse optical flow trajectories, which serve as embodiment-agnostic motion
cues by removing morphological variations while preserving essential dynamics.
Conditioned on these trajectories together with visual and textual inputs,
TrajSkill jointly synthesizes temporally consistent robot manipulation videos
and translates them into executable actions, thereby achieving cross-embodiment
skill transfer. Extensive experiments are conducted, and the results on
simulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\% and KVD
by 36.6\% compared with the state-of-the-art, and improves cross-embodiment
success rate by up to 16.7\%. Real-robot experiments in kitchen manipulation
tasks further validate the effectiveness of our approach, demonstrating
practical human-to-robot skill transfer across embodiments.

</details>


### [153] [IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction](https://arxiv.org/abs/2510.07778)
*Yandu Chen,Kefan Gu,Yuqing Wen,Yucheng Zhao,Tiancai Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: The study introduces IntentionVLA, a framework enhancing Vision-Language-Action models for reasoning-intensive human-robot interactions, achieving superior success rates in various scenarios compared to existing baselines.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action models lack reasoning capabilities essential for implicit human intention inference in complex real-world scenarios.

Method: IntentionVLA uses curriculum training with reasoning data for intention inference, spatial grounding, and embodied reasoning, followed by fine-tuning to use reasoning outputs for action generation.

Result: IntentionVLA showed improved performance: 18% higher success with direct instructions, 28% higher with intention-based tasks, doubling success rate on out-of-distribution tasks, and achieving 40% success in zero-shot human-robot interaction.

Conclusion: IntentionVLA presents a significant advancement for human-robot interaction systems by integrating reasoning-intensive mechanisms to enable robust embodied intelligence across varying instructions and scenarios.

Abstract: Vision-Language-Action (VLA) models leverage pretrained vision-language
models (VLMs) to couple perception with robotic control, offering a promising
path toward general-purpose embodied intelligence. However, current SOTA VLAs
are primarily pretrained on multimodal tasks with limited relevance to embodied
scenarios, and then finetuned to map explicit instructions to actions.
Consequently, due to the lack of reasoning-intensive pretraining and
reasoning-guided manipulation, these models are unable to perform implicit
human intention reasoning required for complex, real-world interactions. To
overcome these limitations, we propose \textbf{IntentionVLA}, a VLA framework
with a curriculum training paradigm and an efficient inference mechanism. Our
proposed method first leverages carefully designed reasoning data that combine
intention inference, spatial grounding, and compact embodied reasoning,
endowing the model with both reasoning and perception capabilities. In the
following finetuning stage, IntentionVLA employs the compact reasoning outputs
as contextual guidance for action generation, enabling fast inference under
indirect instructions. Experimental results show that IntentionVLA
substantially outperforms $\pi_0$, achieving 18\% higher success rates with
direct instructions and 28\% higher than ECoT under intention instructions. On
out-of-distribution intention tasks, IntentionVLA achieves over twice the
success rate of all baselines, and further enables zero-shot human-robot
interaction with 40\% success rate. These results highlight IntentionVLA as a
promising paradigm for next-generation human-robot interaction (HRI) systems.

</details>


### [154] [GM3: A General Physical Model for Micro-Mobility Vehicles](https://arxiv.org/abs/2510.07807)
*Grace Cai,Nithin Parepally,Laura Zheng,Ming C. Lin*

Main category: cs.RO

TL;DR: The paper introduces a new physics-based model for micro-mobility vehicles (MMVs) called the Generalized Micro-Mobility Model (GM3) to improve simulation accuracy compared to traditional models like the Kinematic Bicycle Model (KBM).


<details>
  <summary>Details</summary>
Motivation: Current tools for modeling MMVs are limited and fail to properly account for various physical dynamics such as tire slip, load transfer, and lean dynamics, especially across diverse vehicle configurations.

Method: The proposed GM3 model uses a tire-level formulation based on tire brush representation to accommodate arbitrary wheel layouts. Additionally, an interactive simulation framework is developed for comparing models and validating results.

Result: The GM3 was validated using real-world data from the Stanford Drone Dataset, specifically in scenarios involving bikers, skaters, and carts, demonstrating its improved realism in capturing physical dynamics.

Conclusion: The GM3 represents a significant advancement in modeling MMVs, providing a unified and physics-accurate framework that enables better simulations and applications in urban traffic and autonomous vehicle systems.

Abstract: Modeling the dynamics of micro-mobility vehicles (MMV) is becoming
increasingly important for training autonomous vehicle systems and building
urban traffic simulations. However, mainstream tools rely on variants of the
Kinematic Bicycle Model (KBM) or mode-specific physics that miss tire slip,
load transfer, and rider/vehicle lean. To our knowledge, no unified,
physics-based model captures these dynamics across the full range of common
MMVs and wheel layouts. We propose the "Generalized Micro-mobility Model"
(GM3), a tire-level formulation based on the tire brush representation that
supports arbitrary wheel configurations, including single/double track and
multi-wheel platforms. We introduce an interactive model-agnostic simulation
framework that decouples vehicle/layout specification from dynamics to compare
the GM3 with the KBM and other models, consisting of fixed step RK4
integration, human-in-the-loop and scripted control, real-time trajectory
traces and logging for analysis. We also empirically validate the GM3 on the
Stanford Drone Dataset's deathCircle (roundabout) scene for biker, skater, and
cart classes.

</details>


### [155] [DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation](https://arxiv.org/abs/2510.07865)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: DM1 framework enhances the precision and efficiency of robotic manipulation policies through a novel flow matching approach, significantly reducing inference time and improving task success rates.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address representation collapse in flow-based generative models used for robotic manipulation, which leads to failures in tasks requiring precise control.

Method: DM1 introduces dispersive regularization into the MeanFlow framework, encouraging diverse visual representations across training batches while maintaining efficiency without additional network modules or specialized procedures.

Result: DM1 shows a 20-40x faster inference speed and improves success rates by 10-20% in RoboMimic benchmarks, with real-world deployment successfully transferring from simulation.

Conclusion: DM1 demonstrates that representation regularization can overcome limitations of flow-based policies, establishing an efficient and robust solution for robotic manipulation tasks.

Abstract: The ability to learn multi-modal action distributions is indispensable for
robotic manipulation policies to perform precise and robust control. Flow-based
generative models have recently emerged as a promising solution to learning
distributions of actions, offering one-step action generation and thus
achieving much higher sampling efficiency compared to diffusion-based methods.
However, existing flow-based policies suffer from representation collapse, the
inability to distinguish similar visual representations, leading to failures in
precise manipulation tasks. We propose DM1 (MeanFlow with Dispersive
Regularization for One-Step Robotic Manipulation), a novel flow matching
framework that integrates dispersive regularization into MeanFlow to prevent
collapse while maintaining one-step efficiency. DM1 employs multiple dispersive
regularization variants across different intermediate embedding layers,
encouraging diverse representations across training batches without introducing
additional network modules or specialized training procedures. Experiments on
RoboMimic benchmarks show that DM1 achieves 20-40 times faster inference (0.07s
vs. 2-3.5s) and improves success rates by 10-20 percentage points, with the
Lift task reaching 99% success over 85% of the baseline. Real-robot deployment
on a Franka Panda further validates that DM1 transfers effectively from
simulation to the physical world. To the best of our knowledge, this is the
first work to leverage representation regularization to enable flow-based
policies to achieve strong performance in robotic manipulation, establishing a
simple yet powerful approach for efficient and robust manipulation.

</details>


### [156] [USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots](https://arxiv.org/abs/2510.07869)
*Junwen Gu,Zhiheng wu,Pengxuan Si,Shuang Qiu,Yukai Feng,Luoyang Sun,Laien Luo,Lianyi Yu,Jian Wang,Zhengxing Wu*

Main category: cs.RO

TL;DR: Underwater robotics face challenges like complex hydrodynamics and scarce datasets. The authors address this via USIM, a large dataset, and U0, a model improving success and task efficiency.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous multi-task underwater robots despite challenges such as limited resources, datasets, and complex environments.

Method: Developed USIM dataset with 561K frames and 1,852 trajectories across varied underwater tasks and introduced U0 model utilizing Vision-Language-Action techniques integrated with advanced perception modules.

Result: U0 achieved an 80% success rate across various tasks and reduced target distance by 21.2% in challenging mobile manipulation tasks, demonstrating its efficacy.

Conclusion: USIM and U0 significantly advance VLA modeling for underwater robotics, promoting scalable dataset creation and enhanced task autonomy.

Abstract: Underwater environments present unique challenges for robotic operation,
including complex hydrodynamics, limited visibility, and constrained
communication. Although data-driven approaches have advanced embodied
intelligence in terrestrial robots and enabled task-specific autonomous
underwater robots, developing underwater intelligence capable of autonomously
performing multiple tasks remains highly challenging, as large-scale,
high-quality underwater datasets are still scarce. To address these
limitations, we introduce USIM, a simulation-based multi-task
Vision-Language-Action (VLA) dataset for underwater robots. USIM comprises over
561K frames from 1,852 trajectories, totaling approximately 15.6 hours of
BlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from
visual navigation to mobile manipulation. Building upon this dataset, we
propose U0, a VLA model for general underwater robots, which integrates
binocular vision and other sensor modalities through multimodal fusion, and
further incorporates a convolution-attention-based perception focus enhancement
module (CAP) to improve spatial understanding and mobile manipulation. Across
tasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,
the framework achieves a success rate of 80%, while in challenging mobile
manipulation tasks, it reduces the distance to the target by 21.2% compared
with baseline methods, demonstrating its effectiveness. USIM and U0 show that
VLA models can be effectively applied to underwater robotic applications,
providing a foundation for scalable dataset construction, improved task
autonomy, and the practical realization of intelligent general underwater
robots.

</details>


### [157] [Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track](https://arxiv.org/abs/2510.07871)
*Erjia Xiao,Lingfeng Zhang,Yingbo Tang,Hao Cheng,Renjing Xu,Wenbo Ding,Lei Zhou,Long Chen,Hangjun Ye,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: This paper introduces a novel Proactive Risk Perception Module to improve social navigation in human-populated environments, obtaining 2nd place in the IROS 2025 RoboSense Challenge.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for safe, efficient, and socially compliant navigation in dynamic indoor spaces, where autonomous agents must operate with limited sensors and avoid collisions.

Method: The method enhances the Falcon model by introducing a Proactive Risk Perception Module, which predicts distance-based collision risk scores for nearby humans, enabling better spatial awareness and collision avoidance.

Result: The approach showed improved personal space compliance and robust goal navigation in crowded dynamic scenes, achieving 2nd place among 16 teams in the Social-HM3D benchmark.

Conclusion: The integration of proactive collision risk assessment enhances social navigation, demonstrating significant progress in compliance with social norms and safety while navigating dynamic environments.

Abstract: In this report, we describe the technical details of our submission to the
IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on
developing RGBD-based perception and navigation systems that enable autonomous
agents to navigate safely, efficiently, and socially compliantly in dynamic
human-populated indoor environments. The challenge requires agents to operate
from an egocentric perspective using only onboard sensors including RGB-D
observations and odometry, without access to global maps or privileged
information, while maintaining social norm compliance such as safe distances
and collision avoidance. Building upon the Falcon model, we introduce a
Proactive Risk Perception Module to enhance social navigation performance. Our
approach augments Falcon with collision risk understanding that learns to
predict distance-based collision risk scores for surrounding humans, which
enables the agent to develop more robust spatial awareness and proactive
collision avoidance behaviors. The evaluation on the Social-HM3D benchmark
demonstrates that our method improves the agent's ability to maintain personal
space compliance while navigating toward goals in crowded indoor scenes with
dynamic human agents, achieving 2nd place among 16 participating teams in the
challenge.

</details>


### [158] [Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots](https://arxiv.org/abs/2510.07882)
*Boyu Li,Siyuan He,Hang Xu,Haoqi Yuan,Yu Zang,Liwei Hu,Junpeng Yue,Zhenxiong Jiang,Pengbo Hu,Börje F. Karlsson,Yehui Tang,Zongqing Lu*

Main category: cs.RO

TL;DR: The paper introduces DualTHOR, a simulator for dual-arm humanoid robots, and presents Proprio-MLLM, a model with enhanced embodiment awareness achieving a notable improvement in planning performance.


<details>
  <summary>Details</summary>
Motivation: Multimodal Large Language Models (MLLMs) face challenges in long-horizon tasks for humanoid robots due to lack of simulation platforms and embodiment awareness.

Method: Development of DualTHOR simulation platform and Proprio-MLLM model that uses proprioceptive information, motion-based position embedding, and a cross-spatial encoder.

Result: Proprio-MLLM demonstrated an average planning performance improvement of 19.75% compared to existing MLLMs.

Conclusion: The new platform and model advance embodied intelligence for humanoid robotics, addressing key limitations in current systems.

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have demonstrated
the ability to serve as high-level planners, enabling robots to follow complex
human instructions. However, their effectiveness, especially in long-horizon
tasks involving dual-arm humanoid robots, remains limited. This limitation
arises from two main challenges: (i) the absence of simulation platforms that
systematically support task evaluation and data collection for humanoid robots,
and (ii) the insufficient embodiment awareness of current MLLMs, which hinders
reasoning about dual-arm selection logic and body positions during planning. To
address these issues, we present DualTHOR, a new dual-arm humanoid simulator,
with continuous transition and a contingency mechanism. Building on this
platform, we propose Proprio-MLLM, a model that enhances embodiment awareness
by incorporating proprioceptive information with motion-based position
embedding and a cross-spatial encoder. Experiments show that, while existing
MLLMs struggle in this environment, Proprio-MLLM achieves an average
improvement of 19.75% in planning performance. Our work provides both an
essential simulation platform and an effective model to advance embodied
intelligence in humanoid robotics. The code is available at
https://anonymous.4open.science/r/DualTHOR-5F3B.

</details>


### [159] [Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation](https://arxiv.org/abs/2510.07975)
*Mingyang Sun,Jiude Wei,Qichen He,Donglin Wang,Cewu Lu,Jianhua Sun*

Main category: cs.RO

TL;DR: GRACE bridges the gap between semantic task planning by Vision-Language Models and precise physical manipulation by introducing a framework that combines high-level reasoning with executable analytic concepts for robots.


<details>
  <summary>Details</summary>
Motivation: The goal is to overcome the challenge of enabling robots to effectively perform manipulation tasks in unstructured environments, addressing the disconnect between VLMs' semantic capacity and physical execution.

Method: GRACE utilizes executable analytic concepts (EACs) to encode affordances, constraints, and semantics, transforming natural language and visual input into actionable robotic control plans.

Result: GRACE demonstrates strong zero-shot generalization, executing manipulation tasks across articulated objects in both simulated and real-world scenarios without task-specific training.

Conclusion: GRACE provides a unified and interpretable bridge between semantic reasoning and physical execution, enhancing robotic manipulation capabilities in diverse environments.

Abstract: Enabling robots to perform precise and generalized manipulation in
unstructured environments remains a fundamental challenge in embodied AI. While
Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
semantic reasoning and task planning, a significant gap persists between their
high-level understanding and the precise physical execution required for
real-world manipulation. To bridge this "semantic-to-physical" gap, we
introduce GRACE, a novel framework that grounds VLM-based reasoning through
executable analytic concepts (EAC)-mathematically defined blueprints that
encode object affordances, geometric constraints, and semantics of
manipulation. Our approach integrates a structured policy scaffolding pipeline
that turn natural language instructions and visual information into an
instantiated EAC, from which we derive grasp poses, force directions and plan
physically feasible motion trajectory for robot execution. GRACE thus provides
a unified and interpretable interface between high-level instruction
understanding and low-level robot control, effectively enabling precise and
generalizable manipulation through semantic-physical grounding. Extensive
experiments demonstrate that GRACE achieves strong zero-shot generalization
across a variety of articulated objects in both simulated and real-world
environments, without requiring task-specific training.

</details>


### [160] [Orientation Learning and Adaptation towards Simultaneous Incorporation of Multiple Local Constraints](https://arxiv.org/abs/2510.07986)
*Gaofeng Li,Peisen Xu,Ruize Wang,Qi Ye,Jiming Chen,Dezhen Song,Yanlong Huang*

Main category: cs.RO

TL;DR: The paper introduces a method for learning orientations in the rotation group SO(3) by addressing distortions caused by non-Euclidean geometry using the Angle-Axis Space-based orientation representation approach.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle the challenges associated with orientation learning on the rotation group SO(3), specifically distortion issues stemming from its non-Euclidean nature, and enable simultaneous incorporation of multiple local constraints.

Method: The authors propose a weighted average mechanism in SO(3) based on the angle-axis representation. Multiple trajectories are generated considering different local constraints and fused into a smooth trajectory using the weighted mechanism.

Result: Through simulations and experiments, the method successfully adapts orientations, minimizes angular acceleration, and incorporates multiple local constraints to reduce acceleration costs.

Conclusion: The proposed method mitigates geometric distortion issues, enabling standard Euclidean learning algorithms to be re-applied in SO(3), and demonstrates effective orientation adaptation and angular acceleration minimization while handling multiple constraints.

Abstract: Orientation learning plays a pivotal role in many tasks. However, the
rotation group SO(3) is a Riemannian manifold. As a result, the distortion
caused by non-Euclidean geometric nature introduces difficulties to the
incorporation of local constraints, especially for the simultaneous
incorporation of multiple local constraints. To address this issue, we propose
the Angle-Axis Space-based orientation representation method to solve several
orientation learning problems, including orientation adaptation and
minimization of angular acceleration. Specifically, we propose a weighted
average mechanism in SO(3) based on the angle-axis representation method. Our
main idea is to generate multiple trajectories by considering different local
constraints at different basepoints. Then these multiple trajectories are fused
to generate a smooth trajectory by our proposed weighted average mechanism,
achieving the goal to incorporate multiple local constraints simultaneously.
Compared with existing solution, ours can address the distortion issue and make
the off-theshelf Euclidean learning algorithm be re-applicable in non-Euclidean
space. Simulation and Experimental evaluations validate that our solution can
not only adapt orientations towards arbitrary desired via-points and cope with
angular acceleration constraints, but also incorporate multiple local
constraints simultaneously to achieve extra benefits, e.g., achieving smaller
acceleration costs.

</details>


### [161] [FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset](https://arxiv.org/abs/2510.08022)
*Kehui Liu,Zhongjie Jia,Yang Li,Zhaxizhuoma,Pengan Chen,Song Liu,Xin Liu,Pingrui Zhang,Haoming Song,Xinyi Ye,Nieqing Cao,Zhigang Wang,Jia Zeng,Dong Wang,Yan Ding,Bin Zhao,Xuelong Li*

Main category: cs.RO

TL;DR: The paper introduces FastUMI-100K, a large-scale, multimodal robotic manipulation dataset designed to address limitations in scalability and applicability in existing datasets.


<details>
  <summary>Details</summary>
Motivation: Existing robotic manipulation datasets are constrained by scalability, smoothness of trajectories, and adaptability across robotic embodiments in real-world settings.

Method: The authors propose FastUMI-100K, collected using FastUMI, a modular robotic system that integrates lightweight tracking and multimodal data streams like fisheye images and textual annotations.

Result: FastUMI-100K comprises 100K+ trajectories across 54 tasks and hundreds of object types, demonstrating high policy success rates in baselines.

Conclusion: The dataset enhances the robustness and applicability of manipulation learning systems, facilitating solutions for complex tasks in real-world environments.

Abstract: Data-driven robotic manipulation learning depends on large-scale,
high-quality expert demonstration datasets. However, existing datasets, which
primarily rely on human teleoperated robot collection, are limited in terms of
scalability, trajectory smoothness, and applicability across different robotic
embodiments in real-world environments. In this paper, we present FastUMI-100K,
a large-scale UMI-style multimodal demonstration dataset, designed to overcome
these limitations and meet the growing complexity of real-world manipulation
tasks. Collected by FastUMI, a novel robotic system featuring a modular,
hardware-decoupled mechanical design and an integrated lightweight tracking
system, FastUMI-100K offers a more scalable, flexible, and adaptable solution
to fulfill the diverse requirements of real-world robot demonstration data.
Specifically, FastUMI-100K contains over 100K+ demonstration trajectories
collected across representative household environments, covering 54 tasks and
hundreds of object types. Our dataset integrates multimodal streams, including
end-effector states, multi-view wrist-mounted fisheye images and textual
annotations. Each trajectory has a length ranging from 120 to 500 frames.
Experimental results demonstrate that FastUMI-100K enables high policy success
rates across various baseline algorithms, confirming its robustness,
adaptability, and real-world applicability for solving complex, dynamic
manipulation challenges. The source code and dataset will be released in this
link https://github.com/MrKeee/FastUMI-100K.

</details>


### [162] [Towards Reliable LLM-based Robot Planning via Combined Uncertainty Estimation](https://arxiv.org/abs/2510.08044)
*Shiyuan Yin,Chenjia Bai,Zihao Zhang,Junwei Jin,Xinxin Zhang,Chi Zhang,Xuelong Li*

Main category: cs.RO

TL;DR: The paper introduces CURE, a method to enhance the reliability of planning in robots using LLMs by effectively decomposing and estimating uncertainties.


<details>
  <summary>Details</summary>
Motivation: LLMs are capable of understanding instructions and planning but often suffer from hallucinations or unreliable predictions, posing risks to the safety and alignment of plans.

Method: CURE uses techniques like random network distillation and multi-layer perceptron to separately estimate intrinsic and epistemic uncertainties in planning tasks.

Result: Experimental validations on kitchen manipulation and tabletop rearrangement tasks showed CURE's uncertainty measures strongly corresponded with execution outcomes, surpassing other existing approaches.

Conclusion: Decomposing uncertainties enhances the reliability of embodied planning with LLMs, proving CURE to be a promising solution for safer robot planning and execution.

Abstract: Large language models (LLMs) demonstrate advanced reasoning abilities,
enabling robots to understand natural language instructions and generate
high-level plans with appropriate grounding. However, LLM hallucinations
present a significant challenge, often leading to overconfident yet potentially
misaligned or unsafe plans. While researchers have explored uncertainty
estimation to improve the reliability of LLM-based planning, existing studies
have not sufficiently differentiated between epistemic and intrinsic
uncertainty, limiting the effectiveness of uncertainty estimation. In this
paper, we present Combined Uncertainty estimation for Reliable Embodied
planning (CURE), which decomposes the uncertainty into epistemic and intrinsic
uncertainty, each estimated separately. Furthermore, epistemic uncertainty is
subdivided into task clarity and task familiarity for more accurate evaluation.
The overall uncertainty assessments are obtained using random network
distillation and multi-layer perceptron regression heads driven by LLM
features. We validated our approach in two distinct experimental settings:
kitchen manipulation and tabletop rearrangement experiments. The results show
that, compared to existing methods, our approach yields uncertainty estimates
that are more closely aligned with the actual execution outcomes.

</details>


### [163] [Beyond hospital reach: Autonomous lightweight ultrasound robot for liver sonography](https://arxiv.org/abs/2510.08106)
*Zihan Li,Yixiao Xu,Lei Zhang,Taiyu Han,Xinshan Yang,Yingni Wang,Mingxuan Liu,Shenghai Xin,Linxun Liu,Hongen Liao,Guochen Ning*

Main category: cs.RO

TL;DR: An autonomous ultrasound robot was developed for liver diagnostics, featuring an AI-powered system and a lightweight robotic arm, aiming to address expert shortages in remote areas.


<details>
  <summary>Details</summary>
Motivation: To address the shortage of expert sonographers in resource-limited regions and provide robust liver diagnostic tools in challenging scenarios.

Method: The system comprises an AI agent with multi-modal perception and memory attention for unseen target localization, integrated with a lightweight 6-degree-of-freedom cable-driven robot mounted on the abdomen.

Result: The robot autonomously achieved expert-level liver ultrasound planes and detected pathology effectively, even in high-altitude cities and during rapid motion or wilderness environments.

Conclusion: This autonomous system could revolutionize access to expert liver diagnostics, particularly in underserved or remote areas.

Abstract: Liver disease is a major global health burden. While ultrasound is the
first-line diagnostic tool, liver sonography requires locating multiple
non-continuous planes from positions where target structures are often not
visible, for biometric assessment and lesion detection, requiring significant
expertise. However, expert sonographers are severely scarce in resource-limited
regions. Here, we develop an autonomous lightweight ultrasound robot comprising
an AI agent that integrates multi-modal perception with memory attention for
localization of unseen target structures, and a 588-gram 6-degrees-of-freedom
cable-driven robot. By mounting on the abdomen, the system enhances robustness
against motion. Our robot can autonomously acquire expert-level standard liver
ultrasound planes and detect pathology in patients, including two from Xining,
a 2261-meter-altitude city with limited medical resources. Our system performs
effectively on rapid-motion individuals and in wilderness environments. This
work represents the first demonstration of autonomous sonography across
multiple challenging scenarios, potentially transforming access to expert-level
diagnostics in underserved regions.

</details>


### [164] [NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions](https://arxiv.org/abs/2510.08173)
*Haolin Yang,Yuxing Long,Zhuoyuan Yu,Zihan Yang,Minghan Wang,Jiapeng Xu,Yihan Wang,Ziyan Yu,Wenzhe Cai,Lei Kang,Hao Dong*

Main category: cs.RO

TL;DR: The paper introduces NavSpace, a benchmark designed to evaluate spatial intelligence in navigation agents, and proposes SNav, a high-performing navigation model.


<details>
  <summary>Details</summary>
Motivation: Address the gap in systematically assessing the spatial perception and reasoning abilities of navigation agents in instruction-following tasks.

Method: Created the NavSpace benchmark with 1,228 trajectory-instruction pairs across six task categories to evaluate navigation agents comprehensively, including state-of-the-art models and large language models.

Result: NavSpace revealed insights into spatial intelligence in navigation agents, and the proposed SNav model outperformed existing agents on the benchmark and real-world tests.

Conclusion: NavSpace advances the evaluation of spatial abilities in navigation systems, and SNav sets a new performance benchmark for future research.

Abstract: Instruction-following navigation is a key step toward embodied intelligence.
Prior benchmarks mainly focus on semantic understanding but overlook
systematically evaluating navigation agents' spatial perception and reasoning
capabilities. In this work, we introduce the NavSpace benchmark, which contains
six task categories and 1,228 trajectory-instruction pairs designed to probe
the spatial intelligence of navigation agents. On this benchmark, we
comprehensively evaluate 22 navigation agents, including state-of-the-art
navigation models and multimodal large language models. The evaluation results
lift the veil on spatial intelligence in embodied navigation. Furthermore, we
propose SNav, a new spatially intelligent navigation model. SNav outperforms
existing navigation agents on NavSpace and real robot tests, establishing a
strong baseline for future work.

</details>


### [165] [Evaluation of a Robust Control System in Real-World Cable-Driven Parallel Robots](https://arxiv.org/abs/2510.08270)
*Damir Nurtdinov,Aliaksei Korshuk,Alexei Kornaev,Alexander Maloletov*

Main category: cs.RO

TL;DR: This paper compares classical PID controllers and modern reinforcement learning methods for controlling Cable-Driven Parallel Robots, finding TRPO to be most effective.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve control techniques for Cable-Driven Parallel Robots (CDPRs), particularly in underconstrained and real-world environments with challenges like limited time discretization.

Method: A comparative analysis between classical PID control and reinforcement learning methods, such as DDPG, PPO, and TRPO, is carried out to evaluate performance in terms of error rates and control robustness.

Result: TRPO achieves the best performance with the lowest RMS errors, robustness to larger time intervals, and reliable control in noisy, real-world conditions.

Conclusion: TRPO proves to be a robust, computationally efficient method, offering potential for broader applications in dynamic and complex control environments.

Abstract: This study evaluates the performance of classical and modern control methods
for real-world Cable-Driven Parallel Robots (CDPRs), focusing on
underconstrained systems with limited time discretization. A comparative
analysis is conducted between classical PID controllers and modern
reinforcement learning algorithms, including Deep Deterministic Policy Gradient
(DDPG), Proximal Policy Optimization (PPO), and Trust Region Policy
Optimization (TRPO). The results demonstrate that TRPO outperforms other
methods, achieving the lowest root mean square (RMS) errors across various
trajectories and exhibiting robustness to larger time intervals between control
updates. TRPO's ability to balance exploration and exploitation enables stable
control in noisy, real-world environments, reducing reliance on high-frequency
sensor feedback and computational demands. These findings highlight TRPO's
potential as a robust solution for complex robotic control tasks, with
implications for dynamic environments and future applications in sensor fusion
or hybrid control strategies.

</details>


### [166] [Airy: Reading Robot Intent through Height and Sky](https://arxiv.org/abs/2510.08381)
*Baoyang Chen,Xian Xu,Huamin Qu*

Main category: cs.RO

TL;DR: The paper discusses "Airy," an installation where two reinforcement-trained robot arms snap a bedsheet to investigate if complex AI can be made intuitively understandable.


<details>
  <summary>Details</summary>
Motivation: To address the lack of safety, trust, and public oversight in AI systems as robots increasingly move into shared human spaces.

Method: Airy stages a bedsheet-snapping competition between two robots using principles like clear competition metrics, embodied familiarity, and visualized cooperation or conflict through weather projections.

Result: Observations from exhibitions show that audiences could interpret the robots' strategies, conflicts, and cooperation in real-time with corresponding emotional reactions.

Conclusion: The project demonstrates that sensory metaphors can effectively transform opaque AI systems into understandable and relatable public interfaces.

Abstract: As industrial robots move into shared human spaces, their opaque decision
making threatens safety, trust, and public oversight. This artwork, Airy, asks
whether complex multi agent AI can become intuitively understandable by staging
a competition between two reinforcement trained robot arms that snap a bedsheet
skyward. Building on three design principles, competition as a clear metric
(who lifts higher), embodied familiarity (audiences recognize fabric snapping),
and sensor to sense mapping (robot cooperation or rivalry shown through forest
and weather projections), the installation gives viewers a visceral way to read
machine intent. Observations from five international exhibitions indicate that
audiences consistently read the robots' strategies, conflict, and cooperation
in real time, with emotional reactions that mirror the system's internal state.
The project shows how sensory metaphors can turn a black box into a public
interface.

</details>


### [167] [Reliability of Single-Level Equality-Constrained Inverse Optimal Control](https://arxiv.org/abs/2510.08406)
*Filip Bečanović,Kosta Jovanović,Vincent Bonnet*

Main category: cs.RO

TL;DR: The paper proposes a faster and robust single-level method for solving Inverse Optimal Control (IOC), significantly reducing computation time compared to conventional bilevel approaches.


<details>
  <summary>Details</summary>
Motivation: The study aims to address inefficiencies and noise sensitivity in existing IOC methods used for human motion analysis.

Method: The authors introduce a single-level reformulation to replace the slow bilevel process, ensuring equivalent results while improving computational speed and noise resilience.

Result: Numerical experiments demonstrate the proposed method's 15-fold reduction in computation time and its ability to handle very large noise levels in a simulated human-like planar reaching task.

Conclusion: The single-level formulation is validated as a faster, noise-resilient alternative to classical bilevel IOC methods for modeling human motion.

Abstract: Inverse optimal control (IOC) allows the retrieval of optimal cost function
weights, or behavioral parameters, from human motion. The literature on IOC
uses methods that are either based on a slow bilevel process or a fast but
noise-sensitive minimization of optimality condition violation. Assuming
equality-constrained optimal control models of human motion, this article
presents a faster but robust approach to solving IOC using a single-level
reformulation of the bilevel method and yields equivalent results. Through
numerical experiments in simulation, we analyze the robustness to noise of the
proposed single-level reformulation to the bilevel IOC formulation with a
human-like planar reaching task that is used across recent studies. The
approach shows resilience to very large levels of noise and reduces the
computation time of the IOC on this task by a factor of 15 when compared to a
classical bilevel implementation.

</details>


### [168] [Validation of collision-free spheres of Stewart-Gough platforms for constant orientations using the Application Programming Interface of a CAD software](https://arxiv.org/abs/2510.08408)
*Bibekananda Patra,Rajeevlochana G. Chittawadigi,Sandipan Bandyopadhyay*

Main category: cs.RO

TL;DR: This study validates the largest collision-free sphere (CFS) of a 6-6 Stewart-Gough manipulator using CAD API automation for given orientations.


<details>
  <summary>Details</summary>
Motivation: To ensure the safe operation of spatial parallel manipulators by validating the largest collision-free area in their mechanical configuration.

Method: Utilizes the CAD software's API to modify the manipulator's position across sampled points, checking every pose's leg pairs for collision within the CFS.

Result: The method can automate the validation of precomputed CFS and estimate CFS configurations for any spatial parallel manipulator.

Conclusion: The approach provides a reliable and efficient way to verify collision safety for manipulators, enhancing operational confidence.

Abstract: This paper presents a method of validation of the size of the largest
collision-free sphere (CFS) of a 6-6 Stewart-Gough platform manipulator (SGPM)
for a given orientation of its moving platform (MP) using the Application
Programming Interface (API) of a CAD software. The position of the MP is
updated via the API in an automated manner over a set of samples within a shell
enclosing the surface of the CFS. For each pose of the manipulator, each pair
of legs is investigated for mutual collisions. The CFS is considered safe or
validated iff none of the points falling inside the CFS lead to a collision
between any pair of legs. This approach can not only validate the safety of a
precomputed CFS, but also estimate the same for any spatial parallel
manipulator.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [169] [Modeling Developer Burnout with GenAI Adoption](https://arxiv.org/abs/2510.07435)
*Zixuan Feng,Sadia Afroz,Anita Sarma*

Main category: cs.SE

TL;DR: The study examines how using Generative AI (GenAI) in software development influences developer burnout, finding that it increases job demands and burnout but can be mitigated by job resources and positive perceptions.


<details>
  <summary>Details</summary>
Motivation: To understand how adopting GenAI impacts developers' well-being, particularly regarding burnout, due to the technology's dual productivity benefits and potential new pressures.

Method: A mixed-methods approach combining a survey of 442 developers across roles and organizations, quantitative modeling with PLS-SEM and regression analyses, and qualitative open-ended response analysis.

Result: Findings indicate that GenAI adoption amplifies job demands and thereby burnout, but the effects are alleviated by job resources and favorable perspectives on GenAI.

Conclusion: While GenAI poses risks of increased burnout, proper support systems and positive framing around its use can help developers perceive its adoption more as an opportunity than a burden.

Abstract: Generative AI (GenAI) is rapidly reshaping software development workflows.
While prior studies emphasize productivity gains, the adoption of GenAI also
introduces new pressures that may harm developers' well-being. In this paper,
we investigate the relationship between the adoption of GenAI and developers'
burnout. We utilized the Job Demands--Resources (JD--R) model as the analytic
lens in our empirical study. We employed a concurrent embedded mixed-methods
research design, integrating quantitative and qualitative evidence. We first
surveyed 442 developers across diverse organizations, roles, and levels of
experience. We then employed Partial Least Squares--Structural Equation
Modeling (PLS-SEM) and regression to model the relationships among job demands,
job resources, and burnout, complemented by a qualitative analysis of
open-ended responses to contextualize the quantitative findings. Our results
show that GenAI adoption heightens burnout by increasing job demands, while job
resources and positive perceptions of GenAI mitigate these effects, reframing
adoption as an opportunity.

</details>


### [170] [HotBugs.jar: A Benchmark of Hot Fixes for Time-Critical Bugs](https://arxiv.org/abs/2510.07529)
*Carol Hanna,Federica Sarro,Mark Harman,Justyna Petke*

Main category: cs.SE

TL;DR: HotBugs$.$jar is a benchmark dataset dedicated to hot fixes, derived from 10 Apache projects, including 679 validated hot fixes and metadata.


<details>
  <summary>Details</summary>
Motivation: No existing evaluation benchmark specifically addresses hot fixes, despite their critical nature in production environments.

Method: The dataset was created by mining Apache projects, identifying hot fixes, manually validating them, and packaging them with buggy/fixed versions, test suites, and metadata.

Result: HotBugs$.$jar includes 679 manually validated hot fixes, 110 of which are reproducible, and serves as the challenge dataset for SBSE Conference Challenge Track.

Conclusion: This dataset enables research on debugging, automated repair, and resilience in software systems, potentially advancing tools for production-grade systems.

Abstract: Hot fixes are urgent, unplanned changes deployed to production systems to
address time-critical issues. Despite their importance, no existing evaluation
benchmark focuses specifically on hot fixes. We present HotBugs$.$jar, the
first dataset dedicated to real-world hot fixes. From an initial mining of 10
active Apache projects totaling over 190K commits and 150K issue reports, we
identified 746 software patches that met our hot-fix criteria. After manual
evaluation, 679 were confirmed as genuine hot fixes, of which 110 are
reproducible using a test suite. Building upon the Bugs$.$jar framework,
HotBugs$.$jar integrates these 110 reproducible cases and makes available all
679 manually validated hot fixes, each enriched with comprehensive metadata to
support future research. Each hot fix was systematically identified using Jira
issue data, validated by independent reviewers, and packaged in a reproducible
format with buggy and fixed versions, test suites, and metadata. HotBugs$.$jar
has already been adopted as the official challenge dataset for the Search-Based
Software Engineering (SBSE) Conference Challenge Track, demonstrating its
immediate impact. This benchmark enables the study and evaluation of tools for
rapid debugging, automated repair, and production-grade resilience in modern
software systems to drive research in this essential area forward.

</details>


### [171] [RustAssure: Differential Symbolic Testing for LLM-Transpiled C-to-Rust Code](https://arxiv.org/abs/2510.07604)
*Yubo Bai,Tapti Palit*

Main category: cs.SE

TL;DR: RustAssure automates the transpilation of C to Rust using Large Language Models (LLMs) and symbolic testing to ensure semantic equivalence.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable improved software security and memory safety by converting C codebases to Rust, a safer programming language.

Method: RustAssure employs prompt engineering for LLMs to write idiomatic Rust and uses differential symbolic testing to verify the semantic equivalence between C and Rust code.

Result: In testing, the system generated compilable Rust code for 89.8% of C functions, and 69.9% of those produced equivalent symbolic results.

Conclusion: RustAssure demonstrates the feasibility of leveraging LLMs and symbolic testing to translate C to Rust, improving code safety while maintaining functionality.

Abstract: Rust is a memory-safe programming language that significantly improves
software security. Existing codebases written in unsafe memory languages, such
as C, must first be transpiled to Rust to take advantage of Rust's improved
safety guarantees. RustAssure presents a system that uses Large Language Models
(LLMs) to automatically transpile existing C codebases to Rust. RustAssure uses
prompt engineering techniques to maximize the chances of the LLM generating
idiomatic and safe Rust code. Moreover, because LLMs often generate code with
subtle bugs that can be missed under traditional unit or fuzz testing,
RustAssure performs differential symbolic testing to establish the semantic
similarity between the original C and LLM-transpiled Rust code. We evaluated
RustAssure with five real-world applications and libraries, and showed that our
system is able to generate compilable Rust functions for 89.8% of all C
functions, of which 69.9% produced equivalent symbolic return values for both
the C and Rust functions.

</details>


### [172] [AppForge: From Assistant to Independent Developer -- Are GPTs Ready for Software Development?](https://arxiv.org/abs/2510.07740)
*Dezhi Ran,Yuan Cao,Mengzhou Wu,Simin Chen,Yuzhe Guo,Jun Ren,Zihe Song,Hao Yu,Jialei Wei,Linyi Li,Wei Yang,Baishakhi Ray,Tao Xie*

Main category: cs.SE

TL;DR: APPFORGE is a benchmark designed to evaluate LLMs' abilities to create entire Android applications from natural language specifications, revealing significant limitations in current model capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to evaluate whether LLMs can handle the complexity of building entire software systems, which involves reasoning over system components, lifecycle management, and maintaining states.

Method: The researchers created APPFORGE, a benchmark of 101 real-world Android app problems with automated test cases and an evaluation framework to assess LLM performance in constructing functional apps.

Result: Evaluation on 12 LLMs showed poor performance, with GPT-5 being the best but achieving only 18.8% functionally correct apps.

Conclusion: Current LLMs, including GPT-5, are not capable of handling the complexity of multi-component software engineering tasks, indicating significant room for advancements.

Abstract: Large language models (LLMs) have demonstrated remarkable capability in
function-level code generation tasks. Unlike isolated functions, real-world
applications demand reasoning over the entire software system: developers must
orchestrate how different components interact, maintain consistency across
states over time, and ensure the application behaves correctly within the
lifecycle and framework constraints. Yet, no existing benchmark adequately
evaluates whether LLMs can bridge this gap and construct entire software
systems from scratch. To address this gap, we propose APPFORGE, a benchmark
consisting of 101 software development problems drawn from real-world Android
apps. Given a natural language specification detailing the app functionality, a
language model is tasked with implementing the functionality into an Android
app from scratch. Developing an Android app from scratch requires understanding
and coordinating app states, lifecycle management, and asynchronous operations,
calling for LLMs to generate context-aware, robust, and maintainable code. To
construct APPFORGE, we design a multi-agent system to automatically summarize
the main functionalities from app documents and navigate the app to synthesize
test cases validating the functional correctness of app implementation.
Following rigorous manual verification by Android development experts, APPFORGE
incorporates the test cases within an automated evaluation framework that
enables reproducible assessment without human intervention, making it easily
adoptable for future research. Our evaluation on 12 flagship LLMs show that all
evaluated models achieve low effectiveness, with the best-performing model
(GPT-5) developing only 18.8% functionally correct applications, highlighting
fundamental limitations in current models' ability to handle complex,
multi-component software engineering challenges.

</details>


### [173] [Interleaved Learning and Exploration: A Self-Adaptive Fuzz Testing Framework for MLIR](https://arxiv.org/abs/2510.07815)
*Zeyu Sun,Jingjing Liang,Weiyi Wang,Chenyao Suo,Junjie Chen,Fanjiang Xu*

Main category: cs.SE

TL;DR: This paper introduces FLEX, a self-adaptive fuzzing framework for MLIR, which leverages neural networks and a perturbation strategy to discover bugs effectively.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in fuzzing MLIR, such as generating diverse and semantically valid test cases to uncover subtle errors in MLIR's complex code space.

Method: FLEX uses neural networks for program generation, a perturbed sampling strategy for diversity, and a feedback-driven augmentation loop to refine its test cases iteratively.

Result: FLEX outperformed four state-of-the-art fuzzers by detecting 80 unknown bugs in a 30-day evaluation and achieving superior code coverage in 24-hour comparisons.

Conclusion: The findings highlight FLEX's effectiveness in uncovering MLIR bugs, reinforcing its components like perturbed generation and diversity augmentation as critical to its success.

Abstract: MLIR (Multi-Level Intermediate Representation) has rapidly become a
foundational technology for modern compiler frameworks, enabling extensibility
across diverse domains. However, ensuring the correctness and robustness of
MLIR itself remains challenging. Existing fuzzing approaches-based on manually
crafted templates or rule-based mutations-struggle to generate sufficiently
diverse and semantically valid test cases, making it difficult to expose subtle
or deep-seated bugs within MLIR's complex and evolving code space. In this
paper, we present FLEX, a novel self-adaptive fuzzing framework for MLIR. FLEX
leverages neural networks for program generation, a perturbed sampling strategy
to encourage diversity, and a feedback-driven augmentation loop that
iteratively improves its model using both crashing and non-crashing test cases.
Starting from a limited seed corpus, FLEX progressively learns valid syntax and
semantics and autonomously produces high-quality test inputs. We evaluate FLEX
on the upstream MLIR compiler against four state-of-the-art fuzzers. In a
30-day campaign, FLEX discovers 80 previously unknown bugs-including multiple
new root causes and parser bugs-while in 24-hour fixed-revision comparisons, it
detects 53 bugs (over 3.5x as many as the best baseline) and achieves 28.2%
code coverage, outperforming the next-best tool by 42%. Ablation studies
further confirm the critical role of both perturbed generation and diversity
augmentation in FLEX's effectiveness.

</details>


### [174] [Bug Histories as Sources of Compiler Fuzzing Mutators](https://arxiv.org/abs/2510.07834)
*Lingjun Liu,Feiran Qin,Owolabi Legunsen,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: IssueMut introduces a novel approach to extract mutators from compiler bug histories, leveraging them for effective fuzzing research, uncovering numerous missed bugs in common compilers.


<details>
  <summary>Details</summary>
Motivation: Compiler bugs have severe impacts, and while mutational fuzzing is a promising detection method, it lacks techniques that derive mutators from previous bug histories.

Method: IssueMut mines bug histories to extract mutators hinting at program elements causing bugs, then integrates these mutators into fuzzers to explore new vulnerabilities.

Result: By mining 587 mutators from GCC and LLVM bug reports, IssueMut identified 28 new bugs in GCC and 37 in LLVM, of which 60 were confirmed or fixed.

Conclusion: Bug histories are a valuable resource for compiler fuzzing, and utilizing them, as demonstrated by IssueMut, improves bug detection significantly by finding previously missed issues.

Abstract: Bugs in compilers, which are critical infrastructure today, can have outsized
negative impacts. Mutational fuzzers aid compiler bug detection by
systematically mutating compiler inputs, i.e., programs. Their effectiveness
depends on the quality of the mutators used. Yet, no prior work used compiler
bug histories as a source of mutators. We propose IssueMut, the first approach
for extracting compiler fuzzing mutators from bug histories. Our insight is
that bug reports contain hints about program elements that induced compiler
bugs; they can guide fuzzers towards similar bugs. IssueMut uses an automated
method to mine mutators from bug reports and retrofit such mutators into
existing mutational compiler fuzzers. Using IssueMut, we mine 587 mutators from
1760 GCC and LLVM bug reports. Then, we run IssueMut on these compilers, with
all their test inputs as seed corpora. We find that "bug history" mutators are
effective: they find new bugs that a state-of-the-art mutational compiler
fuzzer misses-28 in GCC and 37 in LLVM. Of these, 60 were confirmed or fixed,
validating our idea that bug histories have rich information that compiler
fuzzers should leverage.

</details>


### [175] [An AUTOSAR-Aligned Architectural Study of Vulnerabilities in Automotive SoC Software](https://arxiv.org/abs/2510.07941)
*Srijita Basu,Haraldsson Bengt,Miroslaw Staron,Christian Berger,Jennifer Horkoff,Magnus Almgren*

Main category: cs.SE

TL;DR: The paper investigates security vulnerabilities in System-on-Chip (SoC) frameworks within automotive environments aligned with AUTOSAR standards, analyzing 180 vulnerabilities to identify root causes, affected modules, and patching delays.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the increasing vulnerabilities in SoC platforms used in safety-critical automotive systems and the lack of systematic analysis of these risks within AUTOSAR-aligned architectures.

Method: The study examines 180 publicly reported automotive SoC vulnerabilities, maps them to an AUTOSAR-aligned architecture model, and identifies root causes and patching patterns.

Result: The analysis identified 16 root causes, 56 affected software modules, and critical modules with prolonged mitigation delays, uncovering vulnerability patterns.

Conclusion: The findings provide actionable insights for enhancing security in automotive SoC platforms with improved detection and mitigation strategies tailored to AUTOSAR-aligned architectures.

Abstract: Cooperative, Connected and Automated Mobility (CCAM) are complex
cyber-physical systems (CPS) that integrate computation, communication, and
control in safety-critical environments. At their core, System-on-Chip (SoC)
platforms consolidate processing units, communication interfaces, AI
accelerators, and security modules into a single chip. AUTOSAR (AUTomotive Open
System ARchitecture) standard was developed in the automotive domain to better
manage this complexity, defining layered software structures and interfaces to
facilitate reuse of HW/SW components. However, in practice, this integrated SoC
software architecture still poses security challenges, particularly in
real-time, safety-critical environments. Recent reports highlight a surge in
SoC-related vulnerabilities, yet systematic analysis of their root causes and
impact within AUTOSAR-aligned architectures is lacking. This study fills that
gap by analyzing 180 publicly reported automotive SoC vulnerabilities, mapped
to a representative SoC software architecture model that is aligned with
AUTOSAR principles for layered abstraction and service orientation. We identify
16 root causes and 56 affected software modules, and examine mitigation delays
across Common Weakness Enumeration (CWE) categories and architectural layers.
We uncover dominant vulnerability patterns and critical modules with prolonged
patch delays, and provide actionable insights for securing automotive CPS
platforms, including guides for improved detection, prioritization, and
localization strategies for SoC software architectures in SoC-based vehicle
platforms.

</details>


### [176] [Past, Present, and Future of Bug Tracking in the Generative AI Era](https://arxiv.org/abs/2510.08005)
*Utku Boran Torun,Mehmet Taha Demircan,Mahmut Furkan Gön,Eray Tüzün*

Main category: cs.SE

TL;DR: The paper proposes an AI-powered bug tracking system using large language models (LLMs) to automate bug reporting, refinement, reproduction, and resolution, aiming to reduce human effort and speed up fixes.


<details>
  <summary>Details</summary>
Motivation: Traditional bug tracking systems are slow, manual, and require significant coordination between technical and non-technical stakeholders, which hampers efficiency and frustrates users.

Method: The proposed framework uses LLMs to automate bug reporting, classify issues, refine reports, reproduce bugs, and generate candidate patches. It integrates automation across all phases of the bug tracking and resolution process.

Result: The framework enables faster response times, reduces human effort, and enhances collaboration between users and developers by leveraging AI at each stage of bug management.

Conclusion: The AI-driven approach improves bug tracking efficiency and software maintenance practices, creating a faster, more user-centered process for identifying and resolving issues.

Abstract: Traditional bug tracking systems rely heavily on manual reporting,
reproduction, triaging, and resolution, each carried out by different
stakeholders such as end users, customer support, developers, and testers. This
division of responsibilities requires significant coordination and widens the
communication gap between non-technical users and technical teams, slowing the
process from bug discovery to resolution. Moreover, current systems are highly
asynchronous; users often wait hours or days for a first response, delaying
fixes and contributing to frustration. This paper examines the evolution of bug
tracking, from early paper-based reporting to today's web-based and SaaS
platforms. Building on this trajectory, we propose an AI-powered bug tracking
framework that augments existing tools with intelligent, large language model
(LLM)-driven automation. Our framework addresses two main challenges: reducing
time-to-fix and minimizing human overhead. Users report issues in natural
language, while AI agents refine reports, attempt reproduction, and request
missing details. Reports are then classified, invalid ones resolved through
no-code fixes, and valid ones localized and assigned to developers. LLMs also
generate candidate patches, with human oversight ensuring correctness. By
integrating automation into each phase, our framework accelerates response
times, improves collaboration, and strengthens software maintenance practices
for a more efficient, user-centric future.

</details>


### [177] [Building Whitespace-Sensitive Languages Using Whitespace-Insensitive Components](https://arxiv.org/abs/2510.08200)
*Alexander Hellwig,Nico Jansen,Bernhard Rumpe*

Main category: cs.SE

TL;DR: The paper addresses the challenge of reusing modular language components for both whitespace-sensitive and -insensitive languages by pre-processing artifacts before parsing.


<details>
  <summary>Details</summary>
Motivation: In Software Language Engineering, there is a need to improve reusability when integrating whitespace-sensitive and whitespace-insensitive languages as current methods are inconsistent and hinder reuse.

Method: The authors propose a technique that preprocesses artifacts from modular, whitespace-insensitive language modules to construct whitespace-sensitive languages.

Result: The technique is evaluated by reconstructing a simplified version of Python, demonstrating the feasibility of the approach.

Conclusion: The paper concludes that the proposed solution enhances reusability of language components, thereby reducing development time and improving software language quality.

Abstract: In Software Language Engineering, there is a trend towards reusability by
composing modular language components. However, this reusability is severely
inhibited by a gap in integrating whitespace-sensitive and
whitespace-insensitive languages. There is currently no consistent procedure
for seamlessly reusing such language components in both cases, such that
libraries often cannot be reused, and whitespacesensitive languages are
developed from scratch. This paper presents a technique for using modular,
whitespaceinsensitive language modules to construct whitespace sensitive
languages by pre-processing language artifacts before parsing. The approach is
evaluated by reconstructing a simplified version of the programming language
Python. Our solution aims to increase the reusability of existing language
components to reduce development time and increase the overall quality of
software languages.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [178] [Beyond Grid-Locked Voxels: Neural Response Functions for Continuous Brain Encoding](https://arxiv.org/abs/2510.07342)
*Haomiao Chen,Keith W Jamison,Mert R. Sabuncu,Amy Kuceyeski*

Main category: q-bio.NC

TL;DR: The paper introduces the Neural Response Function (NRF), a framework for modeling fMRI-measured brain responses as continuous functions over anatomical 3D space rather than flat voxel vectors.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional neural encoding models that ignore spatial anatomical structure by flattening 3D fMRI data into 1D vectors.

Method: The NRF framework models fMRI activity as a continuous implicit function that predicts brain responses at spatial coordinates in standardized MNI space. It leverages properties like local smoothness and cross-subject alignment.

Result: NRF demonstrated superior performance in encoding brain responses both within individual subjects and across subjects, reducing required training data size dramatically.

Conclusion: NRF provides the first anatomically aware framework for encoding brain responses, moving beyond traditional flat voxel representations to exploit spatial and anatomical structures.

Abstract: Neural encoding models aim to predict fMRI-measured brain responses to
natural images. fMRI data is acquired as a 3D volume of voxels, where each
voxel has a defined spatial location in the brain. However, conventional
encoding models often flatten this volume into a 1D vector and treat voxel
responses as independent outputs. This removes spatial context, discards
anatomical information, and ties each model to a subject-specific voxel grid.
We introduce the Neural Response Function (NRF), a framework that models fMRI
activity as a continuous function over anatomical space rather than a flat
vector of voxels. NRF represents brain activity as a continuous implicit
function: given an image and a spatial coordinate (x, y, z) in standardized MNI
space, the model predicts the response at that location. This formulation
decouples predictions from the training grid, supports querying at arbitrary
spatial resolutions, and enables resolution-agnostic analyses. By grounding the
model in anatomical space, NRF exploits two key properties of brain responses:
(1) local smoothness -- neighboring voxels exhibit similar response patterns;
modeling responses continuously captures these correlations and improves data
efficiency, and (2) cross-subject alignment -- MNI coordinates unify data
across individuals, allowing a model pretrained on one subject to be fine-tuned
on new subjects. In experiments, NRF outperformed baseline models in both
intrasubject encoding and cross-subject adaptation, achieving high performance
while reducing the data size needed by orders of magnitude. To our knowledge,
NRF is the first anatomically aware encoding model to move beyond flattened
voxels, learning a continuous mapping from images to brain responses in 3D
space.

</details>


### [179] [Monkey Perceptogram: Reconstructing Visual Representation and Presumptive Neural Preference from Monkey Multi-electrode Arrays](https://arxiv.org/abs/2510.07576)
*Teng Fei,Srinivas Ravishankar,Hoko Nakada,Abhinav Uppal,Ian Jackson,Garrison W. Cottrell,Ryusuke Hayashi,Virginia R. de Sa*

Main category: q-bio.NC

TL;DR: This paper introduces a framework using macaque spiking datasets to decode how visual information is processed by the primate ventral visual stream, reconstructing visual perceptions and their attributes.


<details>
  <summary>Details</summary>
Motivation: To understand how the primate brain transforms complex visual scenes into coherent perceptual experiences, and to link neural activity to perceptual processes.

Method: They use spiking data from macaque V1, V4, and IT cortex along with encoding and decoding models. Linear models are employed to decode neural activity into visual spaces, reconstruct images with generative models, and visualize preferred neural stimuli.

Result: The study demonstrates that low-level and high-level features of visual stimuli can be reconstructed from neural activity, with spatiotemporal activity reflecting hierarchical processing in the ventral stream. Temporal dynamics reveal evolving feature selectivity.

Conclusion: The findings illustrate the feasibility of reconstructing perceptual representations from neural recordings and highlight the hierarchical and dynamic nature of feature encoding in the ventral visual stream of primates.

Abstract: Understanding how the primate brain transforms complex visual scenes into
coherent perceptual experiences remains a central challenge in neuroscience.
Here, we present a comprehensive framework for interpreting monkey visual
processing by integrating encoding and decoding approaches applied to two
large-scale spiking datasets recorded from macaque using THINGS images (THINGS
macaque IT Dataset (TITD) and THINGS Ventral Stream Spiking Dataset (TVSD)). We
leverage multi-electrode array recordings from the ventral visual
stream--including V1, V4, and inferotemporal (IT) cortex--to investigate how
distributed neural populations encode and represent visual information. Our
approach employs linear models to decode spiking activity into multiple latent
visual spaces (including CLIP and VDVAE embeddings) and reconstruct images
using state-of-the-art generative models. We further utilize encoding models to
map visual features back to neural activity, enabling visualization of the
"preferred stimuli" that drive specific neural ensembles. Analyses of both
datasets reveal that it is possible to reconstruct both low-level (e.g., color,
texture) and high-level (e.g., semantic category) features of visual stimuli
from population activity, with reconstructions preserving key perceptual
attributes as quantified by feature-based similarity metrics. The
spatiotemporal spike patterns reflect the ventral stream's hierarchical
organization with anterior regions representing complex objects and categories.
Functional clustering identifies feature-specific neural ensembles, with
temporal dynamics show evolving feature selectivity post-stimulus. Our findings
demonstrate feasible, generalizable perceptual reconstruction from large-scale
monkey neural recordings, linking neural activity to perception.

</details>


### [180] [State-dependent brain responsiveness, from local circuits to the whole brain](https://arxiv.org/abs/2510.07956)
*A. Destexhe,J Goldman,N. Tort-Colet,A. Roques,J. Fousek,S. Petkoski,V. Jirsa,O. David,M. Jedynak,C. Capone,C. De Luca,G. De Bonis,P. S. Paolucci,E. Mikulan,Pigorini,M Massimini,A. Galluzzi,A. Pazienti,M. Mattia,A. Arena,B. E. Juel,E. Hagen,J. F. Storm,E. Montagni,F. Resta,F. S. Pavone,A. L. Allegra Mascaro,A. Dwarakanath,T. I. Panagiotaropoulos,J. Senk,M. Diesmann,A. Camassa,L. Dalla Porta,A. Manasanch,M. V. Sanchez-Vives*

Main category: q-bio.NC

TL;DR: The paper reviews how the brain's responsiveness to stimulation varies by its state across scales and focuses on using quantitative methods like the PCI to analyze this relationship.


<details>
  <summary>Details</summary>
Motivation: To explore the relationship between brain state and responsiveness, and how this is manifested across different scales of brain activity.

Method: The authors review physiological data and computational models to examine state-dependent responsiveness at various scales, comparing multiple methods like the Perturbational Complexity Index (PCI).

Result: The paper provides a systematic comparison of data and models across cellular, mesoscale, and macroscale levels, shedding light on mechanisms explaining state-dependent brain responses.

Conclusion: The research highlights how brain states influence responsiveness and emphasizes the utility of quantitative tools in understanding these relationships.

Abstract: The objective of this paper is to review physiological and computational
aspects of the responsiveness of the cerebral cortex to stimulation, and how
responsiveness depends on the state of the system. This correspondence between
brain state and brain responsiveness (state-dependent responses) is outlined at
different scales from the cellular and circuit level, to the mesoscale and
macroscale level. At each scale, we review how quantitative methods can be used
to characterize network states based on brain responses, such as the
Perturbational Complexity Index (PCI). This description will compare data and
models, systematically and at multiple scales, with a focus on the mechanisms
that explain how brain responses depend on brain states.

</details>


### [181] [Optimizing BCI Rehabilitation Protocols for Stroke: Exploring Task Design and Training Duration](https://arxiv.org/abs/2510.08082)
*Aniana Cruz,Marko Kuzmanoski,Gabriel Pires*

Main category: q-bio.NC

TL;DR: The paper explores optimizing BCI-based stroke rehabilitation by simplifying motor imagery tasks and investigating session duration, demonstrating improved performance and shorter training efficacy.


<details>
  <summary>Details</summary>
Motivation: Stroke recovery methods are limited, and BCIs show promise, but clinical outcomes are inconsistent due to a lack of optimal rehabilitation protocols.

Method: The study tested an alternative BCI motor imagery task (affected hand vs. rest) on data from healthy individuals and stroke patients, evaluating performance with FBCSP and EEGNet while analyzing session duration effects.

Result: The proposed motor imagery approach and shorter training sessions improved BCI performance in both datasets.

Conclusion: Simplified motor imagery and shorter training sessions significantly enhance BCI-based stroke rehabilitation outcomes.

Abstract: Stroke is a leading cause of long-term disability and the second most common
cause of death worldwide. Although acute treatments have advanced, recovery
remains challenging and limited. Brain-computer interfaces (BCIs) have emerged
as a promising tool for post-stroke rehabilitation by promoting
neuroplasticity. However, clinical outcomes remain variable, and optimal
protocols have yet to be established. This study explores strategies to
optimize BCI-based rehabilitation by comparing motor imagery of affected hand
movement versus rest, instead of the conventional left-versus-right motor
imagery. This alternative aims to simplify the task and address the weak
contralateral activation commonly observed in stroke patients. Two datasets,
one from healthy individuals and one from stroke patients, were used to
evaluate the proposed approach. The results showed improved performance using
both FBCSP and EEGNet. Additionally, we investigated the impact of session
duration and found that shorter training sessions produced better BCI
performance than longer sessions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [182] [Evaluating and Learning Optimal Dynamic Treatment Regimes under Truncation by Death](https://arxiv.org/abs/2510.07501)
*Sihyung Park,Wenbin Lu,Shu Yang*

Main category: stat.ML

TL;DR: The paper addresses challenges in evaluating Dynamic Treatment Regimes (DTRs) in critical care, proposing a robust estimation method based on principal stratification.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for evaluating DTRs fail in critical care due to truncation by death, which obscures potential outcomes.

Method: The authors develop a semiparametrically efficient, multiply robust estimator focusing on always-survivor value function using principal stratification.

Result: The proposed method is empirically validated and applied to electronic health records, showcasing its robustness and efficiency in personalized treatment optimization.

Conclusion: The approach provides a robust and efficient tool for optimizing multi-stage DTRs, particularly useful in critical care contexts.

Abstract: Truncation by death, a prevalent challenge in critical care, renders
traditional dynamic treatment regime (DTR) evaluation inapplicable due to
ill-defined potential outcomes. We introduce a principal stratification-based
method, focusing on the always-survivor value function. We derive a
semiparametrically efficient, multiply robust estimator for multi-stage DTRs,
demonstrating its robustness and efficiency. Empirical validation and an
application to electronic health records showcase its utility for personalized
treatment optimization.

</details>


### [183] [From Data to Rewards: a Bilevel Optimization Perspective on Maximum Likelihood Estimation](https://arxiv.org/abs/2510.07624)
*Abdelhakim Benechehab,Gabriel Singer,Corentin Léger,Youssef Attia El Hili,Giuseppe Paolo,Albert Thomas,Maurizio Filippone,Balázs Kégl*

Main category: stat.ML

TL;DR: The paper explores aligning generative models using a bilevel optimization framework without explicit reward signals, addressing limitations of Maximum Likelihood Estimation and Policy Gradient methods.


<details>
  <summary>Details</summary>
Motivation: Generative models face challenges with generalization and catastrophic forgetting under Maximum Likelihood Estimation, and reward-dependent Policy Gradient methods require explicit signals often unavailable.

Method: The authors propose a bilevel optimization framework where the reward function is optimized in an outer problem, while an inner problem uses a policy gradient objective. The theoretical analysis validates its effectiveness.

Result: Insights derived from the framework were confirmed in settings like tabular classification and model-based reinforcement learning.

Conclusion: The proposed method successfully aligns generative models using high-quality datasets and solves issues with current training paradigms. Code has been made publicly available for further research.

Abstract: Generative models form the backbone of modern machine learning, underpinning
state-of-the-art systems in text, vision, and multimodal applications. While
Maximum Likelihood Estimation has traditionally served as the dominant training
paradigm, recent work have highlighted its limitations, particularly in
generalization and susceptibility to catastrophic forgetting compared to
Reinforcement Learning techniques, such as Policy Gradient methods. However,
these approaches depend on explicit reward signals, which are often unavailable
in practice, leaving open the fundamental problem of how to align generative
models when only high-quality datasets are accessible. In this work, we address
this challenge via a Bilevel Optimization framework, where the reward function
is treated as the optimization variable of an outer-level problem, while a
policy gradient objective defines the inner-level. We then conduct a
theoretical analysis of this optimization problem in a tractable setting and
extract insights that, as we demonstrate, generalize to applications such as
tabular classification and model-based reinforcement learning. We release the
code at https://github.com/abenechehab/nll_to_po .

</details>


### [184] [A Honest Cross-Validation Estimator for Prediction Performance](https://arxiv.org/abs/2510.07649)
*Tianyu Pan,Vincent Z. Yu,Viswanath Devanarayan,Lu Tian*

Main category: stat.ML

TL;DR: The paper proposes an improved method for estimating prediction model performance, using hierarchical and empirical Bayes estimators to outperform conventional cross-validation.


<details>
  <summary>Details</summary>
Motivation: Standard cross-validation is critiqued for not estimating the future-use model's performance accurately.

Method: They introduce hierarchical Bayesian and empirical Bayes estimators within a random-effects model framework to refine performance estimation.

Result: The proposed estimators outperform naive and conventional cross-validation methods in simulations and real-world data.

Conclusion: The study highlights the advantages of the new estimators in achieving better performance assessments of predictive models.

Abstract: Cross-validation is a standard tool for obtaining a honest assessment of the
performance of a prediction model. The commonly used version repeatedly splits
data, trains the prediction model on the training set, evaluates the model
performance on the test set, and averages the model performance across
different data splits. A well-known criticism is that such cross-validation
procedure does not directly estimate the performance of the particular model
recommended for future use. In this paper, we propose a new method to estimate
the performance of a model trained on a specific (random) training set. A naive
estimator can be obtained by applying the model to a disjoint testing set.
Surprisingly, cross-validation estimators computed from other random splits can
be used to improve this naive estimator within a random-effects model
framework. We develop two estimators -- a hierarchical Bayesian estimator and
an empirical Bayes estimator -- that perform similarly to or better than both
the conventional cross-validation estimator and the naive single-split
estimator. Simulations and a real-data example demonstrate the superior
performance of the proposed method.

</details>


### [185] [When Robustness Meets Conservativeness: Conformalized Uncertainty Calibration for Balanced Decision Making](https://arxiv.org/abs/2510.07750)
*Wenbin Zhou,Shixiang Zhu*

Main category: stat.ML

TL;DR: The paper introduces a new distribution-free framework with finite-sample guarantees for robust optimization, addressing limitations in previous approaches that fix robustness levels arbitrarily.


<details>
  <summary>Details</summary>
Motivation: Traditional robust optimization often relies on ad hoc robustness level choices, which can lead to either over-conservatism or insufficient protection in decisions under uncertainty.

Method: The authors employ a framework that estimates the miscoverage-regret Pareto frontier to provide guarantees on both miscoverage and regret for robust policies, offering guidance for the selection of cost-risk trade-offs.

Result: The proposed method achieves sharper finite-sample performance compared to existing techniques and provides tools for calibrating robustness levels based on practitioners' preferences.

Conclusion: This framework is user-friendly, broadly applicable, and enables principled robustness calibration for high-stakes decision-making, improving upon existing robustness selection methodologies.

Abstract: Robust optimization safeguards decisions against uncertainty by optimizing
against worst-case scenarios, yet their effectiveness hinges on a prespecified
robustness level that is often chosen ad hoc, leading to either insufficient
protection or overly conservative and costly solutions. Recent approaches using
conformal prediction construct data-driven uncertainty sets with finite-sample
coverage guarantees, but they still fix coverage targets a priori and offer
little guidance for selecting robustness levels. We propose a new framework
that provides distribution-free, finite-sample guarantees on both miscoverage
and regret for any family of robust predict-then-optimize policies. Our method
constructs valid estimators that trace out the miscoverage-regret Pareto
frontier, enabling decision-makers to reliably evaluate and calibrate
robustness levels according to their cost-risk preferences. The framework is
simple to implement, broadly applicable across classical optimization
formulations, and achieves sharper finite-sample performance than existing
approaches. These results offer the first principled data-driven methodology
for guiding robustness selection and empower practitioners to balance
robustness and conservativeness in high-stakes decision-making.

</details>


### [186] [Surrogate Graph Partitioning for Spatial Prediction](https://arxiv.org/abs/2510.07832)
*Yuta Shikuri,Hironori Fujisawa*

Main category: stat.ML

TL;DR: The paper tackles spatial prediction challenges using graph partitioning for interpretability, proposing a mixed-integer quadratic programming approach with an efficient approximation scheme.


<details>
  <summary>Details</summary>
Motivation: Industries requiring interpretability in spatial prediction struggle with adopting complex black-box models; the paper aims to bridge this gap.

Method: Proposes a graph partitioning approach to create spatial segments minimizing prediction variance, formulated as mixed-integer quadratic programming and supported by an efficient approximation.

Result: The efficient approximation scheme demonstrates computational feasibility in identifying spatial segments, addressing scalability concerns.

Conclusion: The study introduces a viable method for enhancing interpretability in spatial prediction, balancing computational complexity and precise segmentation.

Abstract: Spatial prediction refers to the estimation of unobserved values from
spatially distributed observations. Although recent advances have improved the
capacity to model diverse observation types, adoption in practice remains
limited in industries that demand interpretability. To mitigate this gap,
surrogate models that explain black-box predictors provide a promising path
toward interpretable decision making. In this study, we propose a graph
partitioning problem to construct spatial segments that minimize the sum of
within-segment variances of individual predictions. The assignment of data
points to segments can be formulated as a mixed-integer quadratic programming
problem. While this formulation potentially enables the identification of exact
segments, its computational complexity becomes prohibitive as the number of
data points increases. Motivated by this challenge, we develop an approximation
scheme that leverages the structural properties of graph partitioning.
Experimental results demonstrate the computational efficiency of this
approximation in identifying spatial segments.

</details>


### [187] [On the Optimality of Tracking Fisher Information in Adaptive Testing with Stochastic Binary Responses](https://arxiv.org/abs/2510.07862)
*Sanghwa Kim,Dohyun Ahn,Seungki Min*

Main category: stat.ML

TL;DR: The paper addresses efficient adaptive testing and estimation of a continuous ability parameter using sequential binary responses, optimizing query selection while ensuring result accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve adaptive testing and preference learning by minimizing the number of queries needed to estimate a continuous ability parameter within a desired accuracy margin.

Method: An algorithm is proposed that maximizes Fisher information for adaptive query selection, estimates via method-of-moments, and uses a novel statistic to certify accuracy, backed by theoretical proofs in different regimes.

Result: The proposed Fisher-tracking strategy achieves optimal performance, overcoming challenges in fixed-budget settings and providing rigorous support for efficient procedures.

Conclusion: The study validates a theoretically-grounded and efficient approach for adaptive estimation, significantly enhancing methodologies for adaptive testing and preference learning.

Abstract: We study the problem of estimating a continuous ability parameter from
sequential binary responses by actively asking questions with varying
difficulties, a setting that arises naturally in adaptive testing and online
preference learning. Our goal is to certify that the estimate lies within a
desired margin of error, using as few queries as possible. We propose a simple
algorithm that adaptively selects questions to maximize Fisher information and
updates the estimate using a method-of-moments approach, paired with a novel
test statistic to decide when the estimate is accurate enough. We prove that
this Fisher-tracking strategy achieves optimal performance in both
fixed-confidence and fixed-budget regimes, which are commonly invested in the
best-arm identification literature. Our analysis overcomes a key technical
challenge in the fixed-budget setting -- handling the dependence between the
evolving estimate and the query distribution -- by exploiting a structural
symmetry in the model and combining large deviation tools with Ville's
inequality. Our results provide rigorous theoretical support for simple and
efficient adaptive testing procedures.

</details>


### [188] [On the Optimality of the Median-of-Means Estimator under Adversarial Contamination](https://arxiv.org/abs/2510.07867)
*Xabier de Juan,Santiago Mazuelas*

Main category: stat.ML

TL;DR: The paper analyzes Median-of-Means (MoM) estimators under adversarial contamination for various distribution classes, finding it optimal in some scenarios and sub-optimal in others.


<details>
  <summary>Details</summary>
Motivation: To understand the robustness and limitations of the MoM estimator under adversarial contamination, beyond traditional i.i.d. and Gaussian cases.

Method: Establishing upper and lower error bounds for the MoM estimator across classes of distributions: finite variance distributions, infinite variance with finite absolute $(1+r)$-th moment distributions, and light-tailed distributions.

Result: MoM is found to be minimax optimal under finite variance and certain infinite variance distributions, while sub-optimal for light-tailed distributions.

Conclusion: The study refines understanding of the MoM estimator's performance under various contamination scenarios and establishes its limitations.

Abstract: The Median-of-Means (MoM) is a robust estimator widely used in machine
learning that is known to be (minimax) optimal in scenarios where samples are
i.i.d. In more grave scenarios, samples are contaminated by an adversary that
can inspect and modify the data. Previous work has theoretically shown the
suitability of the MoM estimator in certain contaminated settings. However, the
(minimax) optimality of MoM and its limitations under adversarial contamination
remain unknown beyond the Gaussian case. In this paper, we present upper and
lower bounds for the error of MoM under adversarial contamination for multiple
classes of distributions. In particular, we show that MoM is (minimax) optimal
in the class of distributions with finite variance, as well as in the class of
distributions with infinite variance and finite absolute $(1+r)$-th moment. We
also provide lower bounds for MoM's error that match the order of the presented
upper bounds, and show that MoM is sub-optimal for light-tailed distributions.

</details>


### [189] [Stick-Breaking Mixture Normalizing Flows with Component-Wise Tail Adaptation for Variational Inference](https://arxiv.org/abs/2510.07965)
*Seungsu Han,Juyoung Hwang,Won Chang*

Main category: stat.ML

TL;DR: The paper proposes StiCTAF, a novel method to improve posterior approximations using normalizing flows by introducing a stick-breaking mixture base with tail adaptation to handle multimodal and heavy-tailed distributions.


<details>
  <summary>Details</summary>
Motivation: Normalizing flows struggle to approximate complex posterior distributions with multimodality and heavy tails, prompting the need for improvements.

Method: StiCTAF is introduced, which first uses a flexible stick-breaking mixture base for better mode representation, estimates local tail indices, and refines components with tail transforms to model anisotropic tails.

Result: The experiments show improved mode coverage and tail recovery for synthetic posteriors compared to benchmarks, alongside practical advantages in real-data posterior inference.

Conclusion: StiCTAF is effective at addressing the limitations of normalizing flows in Bayesian inference, achieving both accurate mode and tail approximation while maintaining computational efficiency.

Abstract: Normalizing flows with a Gaussian base provide a computationally efficient
way to approximate posterior distributions in Bayesian inference, but they
often struggle to capture complex posteriors with multimodality and heavy
tails. We propose a stick-breaking mixture base with component-wise tail
adaptation (StiCTAF) for posterior approximation. The method first learns a
flexible mixture base to mitigate the mode-seeking bias of reverse KL
divergence through a weighted average of component-wise ELBOs. It then
estimates local tail indices of unnormalized densities and finally refines each
mixture component using a shared backbone combined with component-specific tail
transforms calibrated by the estimated indices. This design enables accurate
mode coverage and anisotropic tail modeling while retaining exact density
evaluation and stable optimization. Experiments on synthetic posteriors
demonstrate improved tail recovery and better coverage of multiple modes
compared to benchmark models. We also present a real-data analysis illustrating
the practical benefits of our approach for posterior inference.

</details>


### [190] [Beyond Real Data: Synthetic Data through the Lens of Regularization](https://arxiv.org/abs/2510.08095)
*Amitis Shidani,Tyler Farghly,Yang Sun,Habib Ganjgahi,George Deligiannidis*

Main category: stat.ML

TL;DR: This paper introduces a theoretical framework to balance synthetic and real data for optimal generalization, validated empirically on CIFAR-10 and clinical brain MRI datasets.


<details>
  <summary>Details</summary>
Motivation: Existing challenges in using synthetic data arise from potential distribution mismatches, which can degrade model performance.

Method: The authors use algorithmic stability to derive generalization error bounds, integrating the Wasserstein distance for quantifying synthetic and real data trade-offs.

Result: The study predicts a U-shaped test error behavior based on synthetic data proportion and demonstrates performance improvements in domain adaptation scenarios.

Conclusion: Carefully balancing synthetic and real data can optimize learning and generalization in both in-domain and cross-domain settings, providing practical insights.

Abstract: Synthetic data can improve generalization when real data is scarce, but
excessive reliance may introduce distributional mismatches that degrade
performance. In this paper, we present a learning-theoretic framework to
quantify the trade-off between synthetic and real data. Our approach leverages
algorithmic stability to derive generalization error bounds, characterizing the
optimal synthetic-to-real data ratio that minimizes expected test error as a
function of the Wasserstein distance between the real and synthetic
distributions. We motivate our framework in the setting of kernel ridge
regression with mixed data, offering a detailed analysis that may be of
independent interest. Our theory predicts the existence of an optimal ratio,
leading to a U-shaped behavior of test error with respect to the proportion of
synthetic data. Empirically, we validate this prediction on CIFAR-10 and a
clinical brain MRI dataset. Our theory extends to the important scenario of
domain adaptation, showing that carefully blending synthetic target data with
limited source data can mitigate domain shift and enhance generalization. We
conclude with practical guidance for applying our results to both in-domain and
out-of-domain scenarios.

</details>


### [191] [High-dimensional Analysis of Synthetic Data Selection](https://arxiv.org/abs/2510.08123)
*Parham Rezaei,Filip Kovacevic,Francesco Locatello,Marco Mondelli*

Main category: stat.ML

TL;DR: The study examines the impact of synthetic data properties on classifier performance, finding that covariance matching is key, not mean matching.


<details>
  <summary>Details</summary>
Motivation: The motivation is to clarify which specific properties of synthetic data influence the generalization error of classifiers, given the uncertainty in current heuristic principles.

Method: The study theoretically explores high-dimensional regression, focusing on linear models to analyze covariance and mean shifts. These insights are extended to deep neural networks and generative models, with empirical validation.

Result: The research finds that covariance shifts influence generalization error, while mean shifts do not. Covariance matching is empirically shown to outperform other synthetic data selection methods across multiple scenarios.

Conclusion: Matching covariances between synthetic and target data distributions optimally improves classifier performance, applicable across various models and datasets.

Abstract: Despite the progress in the development of generative models, their
usefulness in creating synthetic data that improve prediction performance of
classifiers has been put into question. Besides heuristic principles such as
"synthetic data should be close to the real data distribution", it is actually
not clear which specific properties affect the generalization error. Our paper
addresses this question through the lens of high-dimensional regression.
Theoretically, we show that, for linear models, the covariance shift between
the target distribution and the distribution of the synthetic data affects the
generalization error but, surprisingly, the mean shift does not. Furthermore we
prove that, in some settings, matching the covariance of the target
distribution is optimal. Remarkably, the theoretical insights from linear
models carry over to deep neural networks and generative models. We empirically
demonstrate that the covariance matching procedure (matching the covariance of
the synthetic data with that of the data coming from the target distribution)
performs well against several recent approaches for synthetic data selection,
across training paradigms, architectures, datasets and generative models used
for augmentation.

</details>


### [192] [PAC Learnability in the Presence of Performativity](https://arxiv.org/abs/2510.08335)
*Ivan Kirev,Lyuben Baltadzhiev,Nikola Konstantinov*

Main category: stat.ML

TL;DR: The paper explores the issue of performativity in machine learning, where shifts in test data distribution due to model deployment affect performance. Using the PAC learning framework, they present a performative risk minimization approach for binary classification.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problem of performative shifts—changes in test data distribution caused by deployed models—which lead to degraded performance in machine learning applications.

Method: The authors propose a performative empirical risk function that accounts for the performative effects in data. They utilize this function in a PAC learning framework to minimize performative risks.

Result: They show that any PAC-learnable hypothesis space remains learnable under performative effects. Experimental evaluations demonstrate the effectiveness of their proposed method on both synthetic and real datasets.

Conclusion: Performative risk minimization can address shifts in test distribution effectively and ensures PAC learnability for binary classification in performative settings.

Abstract: Following the wide-spread adoption of machine learning models in real-world
applications, the phenomenon of performativity, i.e. model-dependent shifts in
the test distribution, becomes increasingly prevalent. Unfortunately, since
models are usually trained solely based on samples from the original
(unshifted) distribution, this performative shift may lead to decreased
test-time performance. In this paper, we study the question of whether and when
performative binary classification problems are learnable, via the lens of the
classic PAC (Probably Approximately Correct) learning framework. We motivate
several performative scenarios, accounting in particular for linear shifts in
the label distribution, as well as for more general changes in both the labels
and the features. We construct a performative empirical risk function, which
depends only on data from the original distribution and on the type
performative effect, and is yet an unbiased estimate of the true risk of a
classifier on the shifted distribution. Minimizing this notion of performative
risk allows us to show that any PAC-learnable hypothesis space in the standard
binary classification setting remains PAC-learnable for the considered
performative scenarios. We also conduct an extensive experimental evaluation of
our performative risk minimization method and showcase benefits on synthetic
and real data.

</details>


### [193] [Optimal Stopping in Latent Diffusion Models](https://arxiv.org/abs/2510.08409)
*Yu-Han Wu,Quentin Berthet,Gérard Biau,Claire Boyer,Romuald Elie,Pierre Marion*

Main category: stat.ML

TL;DR: The paper identifies issues in Latent Diffusion Models where final diffusion steps degrade sample quality. Lower latent dimensions benefit from early stopping, while higher ones require later termination.


<details>
  <summary>Details</summary>
Motivation: Understanding the degradation in sample quality caused by final steps in Latent Diffusion Models. Offers deeper insights into early stopping and its impact on generative quality.

Method: Analyzed Gaussian framework using linear autoencoders to characterize latent dimension-stopping time interaction. Conducted experiments with synthetic and real datasets to validate findings.

Result: Lower-dimensional latent spaces benefit from earlier diffusion stopping, while high-dimensional latent spaces require later stopping. Established latent dimension interplays with hyperparameters.

Conclusion: Early stopping is a crucial hyperparameter for improving sample quality in latent diffusion models, influenced by latent dimension and other factors.

Abstract: We identify and analyze a surprising phenomenon of Latent Diffusion Models
(LDMs) where the final steps of the diffusion can degrade sample quality. In
contrast to conventional arguments that justify early stopping for numerical
stability, this phenomenon is intrinsic to the dimensionality reduction in
LDMs. We provide a principled explanation by analyzing the interaction between
latent dimension and stopping time. Under a Gaussian framework with linear
autoencoders, we characterize the conditions under which early stopping is
needed to minimize the distance between generated and target distributions.
More precisely, we show that lower-dimensional representations benefit from
earlier termination, whereas higher-dimensional latent spaces require later
stopping time. We further establish that the latent dimension interplays with
other hyperparameters of the problem such as constraints in the parameters of
score matching. Experiments on synthetic and real datasets illustrate these
properties, underlining that early stopping can improve generative quality.
Together, our results offer a theoretical foundation for understanding how the
latent dimension influences the sample quality, and highlight stopping time as
a key hyperparameter in LDMs.

</details>


### [194] [Accelerated Aggregated D-Optimal Designs for Estimating Main Effects in Black-Box Models](https://arxiv.org/abs/2510.08465)
*Chih-Yu Chang,Ming-Chung Chang*

Main category: stat.ML

TL;DR: The paper introduces A2D2E, a method designed to address limitations in explaining black-box models by leveraging experimental design principles for efficient and robust estimation of input variable effects on predictions.


<details>
  <summary>Details</summary>
Motivation: Explaining black-box model behaviors is challenging due to issues like scalability, sensitivity to sampling, and instability under feature correlations.

Method: The authors propose A2D2E, an estimator using Accelerated Aggregated D-Optimal Designs, supported by theoretical guarantees and experimental validation.

Result: The method demonstrates improved efficiency, robustness, and variance reduction through simulations, with real data case studies showcasing its applicability.

Conclusion: A2D2E improves main effect estimation for black-box models while addressing common limitations, with promising applications in language models.

Abstract: Recent advances in supervised learning have driven growing interest in
explaining black-box models, particularly by estimating the effects of input
variables on model predictions. However, existing approaches often face key
limitations, including poor scalability, sensitivity to out-of-distribution
sampling, and instability under correlated features. To address these issues,
we propose A2D2E, an $\textbf{E}$stimator based on $\textbf{A}$ccelerated
$\textbf{A}$ggregated $\textbf{D}$-Optimal $\textbf{D}$esigns. Our method
leverages principled experimental design to improve efficiency and robustness
in main effect estimation. We establish theoretical guarantees, including
convergence and variance reduction, and validate A2D2E through extensive
simulations. We further provide the potential of the proposed method with a
case study on real data and applications in language models. The code to
reproduce the results can be found at https://github.com/cchihyu/A2D2E.

</details>


### [195] [Permutation-Invariant Spectral Learning via Dyson Diffusion](https://arxiv.org/abs/2510.08535)
*Tassilo Schwarz,Cai Dieball,Constantin Kogler,Kevin Lam,Renaud Lambiotte,Arnaud Doucet,Aljaž Godec,George Deligiannidis*

Main category: stat.ML

TL;DR: The paper proposes the Dyson Diffusion Model that uses Dyson's Brownian Motion to enhance generative modeling for graphs by focusing on spectral dynamics instead of relying solely on inductive bias in architectures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing graph diffusion models, which struggle to distinguish graph families without additional ad hoc features due to reliance on the learning architecture's inductive bias.

Method: The authors use random matrix theory to derive spectral properties of the diffusion process and introduce the Dyson Diffusion Model. This model employs Dyson's Brownian Motion to focus on spectral dynamics while preserving other graph information.

Result: The Dyson Diffusion Model accurately learns graph spectra and achieves better performance compared to existing graph diffusion models.

Conclusion: By shifting the inductive bias from the architecture to the dynamics, the proposed Dyson Diffusion Model offers a more effective approach to generative modeling for graphs, addressing key limitations of prior methods.

Abstract: Diffusion models are central to generative modeling and have been adapted to
graphs by diffusing adjacency matrix representations. The challenge of having
up to $n!$ such representations for graphs with $n$ nodes is only partially
mitigated by using permutation-equivariant learning architectures. Despite
their computational efficiency, existing graph diffusion models struggle to
distinguish certain graph families, unless graph data are augmented with ad hoc
features. This shortcoming stems from enforcing the inductive bias within the
learning architecture. In this work, we leverage random matrix theory to
analytically extract the spectral properties of the diffusion process, allowing
us to push the inductive bias from the architecture into the dynamics. Building
on this, we introduce the Dyson Diffusion Model, which employs Dyson's Brownian
Motion to capture the spectral dynamics of an Ornstein-Uhlenbeck process on the
adjacency matrix while retaining all non-spectral information. We demonstrate
that the Dyson Diffusion Model learns graph spectra accurately and outperforms
existing graph diffusion models.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [196] [Self-replication and Computational Universality](https://arxiv.org/abs/2510.08342)
*Jordan Cotler,Clément Hongler,Barbora Hudcová*

Main category: nlin.CG

TL;DR: This paper investigates the relationship between Turing-universal computation and self-replication in physical systems, challenging the assumption that the former guarantees the latter.


<details>
  <summary>Details</summary>
Motivation: To understand how self-replication emerges dynamically in physical, non-equilibrium systems and test the hypothesis that any Turing-universal system can support self-replication.

Method: The authors clarified the definition of Turing-universality in physical systems, constructed a cellular automaton to test self-replication, and analyzed dynamics like transcription and translation.

Result: They found that a constructed Turing-universal cellular automaton could perform computation but not sustain self-replication, challenging the conventional hypothesis.

Conclusion: The findings highlight the significance of computational complexity in translating between physical dynamics and symbolic computation for understanding self-replication and suggest necessary conditions for defining living systems.

Abstract: Self-replication is central to all life, and yet how it dynamically emerges
in physical, non-equilibrium systems remains poorly understood. Von Neumann's
pioneering work in the 1940s and subsequent developments suggest a natural
hypothesis: that any physical system capable of Turing-universal computation
can support self-replicating objects. In this work, we challenge this
hypothesis by clarifying what computational universality means for physical
systems and constructing a cellular automaton that is Turing-universal but
cannot sustain non-trivial self-replication. By analogy with biology, such
dynamics manifest transcription and translation but cannot instantiate
replication. More broadly, our work emphasizes that the computational
complexity of translating between physical dynamics and symbolic computation is
inseparable from any claim of universality (exemplified by our analysis of Rule
110) and builds mathematical foundations for identifying self-replicating
behavior. Our approach enables the formulation of necessary dynamical and
computational conditions for a physical system to constitute a living organism.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [197] [pyGinkgo: A Sparse Linear Algebra Operator Framework for Python](https://arxiv.org/abs/2510.08230)
*Keshvi Tuteja,Gregor Olenik,Roman Mishchuk,Yu-Hsiang Tsai,Markus Götz,Achim Streit,Hartwig Anzt,Charlotte Debus*

Main category: cs.MS

TL;DR: The paper introduces pyGinkgo, a Python interface to the Ginkgo library, enabling high-performance sparse linear algebra with compatibility across multiple hardware platforms.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of high-performance sparse linear algebra in Python, especially on modern CPU and GPU architectures.

Method: Developed pyGinkgo, a Python wrapper for the Ginkgo library using Pybind11, with compatibility for NumPy and PyTorch, and benchmarked its performance against leading Python libraries.

Result: pyGinkgo outperformed Python libraries such as SciPy, CuPy, PyTorch, and TensorFlow in sparse operations, while maintaining parity with Ginkgo C++ backend on various hardware platforms.

Conclusion: pyGinkgo effectively bridges the gap between Python usability and high-performance sparse linear algebra, making it a valuable tool for machine learning and scientific computing.

Abstract: Sparse linear algebra is a cornerstone of many scientific computing and
machine learning applications. Python has become a popular choice for these
applications due to its simplicity and ease of use. Yet high performance sparse
kernels in Python remain limited in functionality, especially on modern CPU and
GPU architectures. We present pyGinkgo, a lightweight and Pythonic interface to
the Ginkgo library, offering high-performance sparse linear algebra support
with platform portability across CUDA, HIP, and OpenMP backends. pyGinkgo
bridges the gap between high-performance C++ backends and Python usability by
exposing Ginkgo's capabilities via Pybind11 and a NumPy and PyTorch compatible
interface. We benchmark pyGinkgo's performance against state-of-the-art Python
libraries including SciPy, CuPy, PyTorch, and TensorFlow. Results across
hardware from different vendors demonstrate that pyGinkgo consistently
outperforms existing Python tools in both sparse matrix vector (SpMV) product
and iterative solver performance, while maintaining performance parity with
native Ginkgo C++ code. Our work positions pyGinkgo as a compelling backend for
sparse machine learning models and scientific workflows.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [198] [Rotated Mean-Field Variational Inference and Iterative Gaussianization](https://arxiv.org/abs/2510.07732)
*Yifan Chen,Sifan Liu*

Main category: stat.CO

TL;DR: The paper introduces mean-field variational inference (MFVI) in rotated coordinates using PCA to reduce variable correlations for more accurate approximations with minimal cost.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and efficiency of MFVI by addressing correlations between variables.

Method: The authors propose using PCA-based rotation and iterative transformations to make MFVI closer to Gaussian while avoiding large-scale optimization.

Result: Their method provides higher accuracy than standard MFVI and greater computational efficiency compared to normalizing flows.

Conclusion: Rotated MFVI offers an effective, efficient framework for Bayesian inference, bridging accuracy and computational cost.

Abstract: We propose to perform mean-field variational inference (MFVI) in a rotated
coordinate system that reduces correlations between variables. The rotation is
determined by principal component analysis (PCA) of a cross-covariance matrix
involving the target's score function. Compared with standard MFVI along the
original axes, MFVI in this rotated system often yields substantially more
accurate approximations with negligible additional cost.
  MFVI in a rotated coordinate system defines a rotation and a coordinatewise
map that together move the target closer to Gaussian. Iterating this procedure
yields a sequence of transformations that progressively transforms the target
toward Gaussian. The resulting algorithm provides a computationally efficient
way to construct flow-like transport maps: it requires only MFVI subproblems,
avoids large-scale optimization, and yields transformations that are easy to
invert and evaluate. In Bayesian inference tasks, we demonstrate that the
proposed method achieves higher accuracy than standard MFVI, while maintaining
much lower computational cost than conventional normalizing flows.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [199] [When Light Bends to the Collective Will: A Theory and Vision for Adaptive Photonic Scale-up Domains](https://arxiv.org/abs/2510.08072)
*Vamsi Addanki*

Main category: cs.NI

TL;DR: The paper addresses the challenges of chip-to-chip silicon photonics in collective communication by proposing adaptive programmable photonic interconnects to optimize performance despite reconfiguration delays.


<details>
  <summary>Details</summary>
Motivation: To overcome bottlenecks in collective communication for scalable systems using silicon photonics, and to exploit adaptive, dynamic reconfiguration capabilities for optimizing high-bandwidth optical paths.

Method: Introduces a theoretical framework connecting Birkhoff--von Neumann decomposition, maximum concurrent flow, and the $
alpha$-$eta$ cost model to manage reconfiguration delays effectively.

Result: The framework explicitly showcases the trade-offs in performance gains against reconfiguration delays and lays the groundwork for a research agenda in related algorithm design and systems integration.

Conclusion: Adaptive reconfiguration can significantly improve communication in silicon photonics, but its trade-offs must be analyzed through insights provided by the proposed theoretical connections.

Abstract: As chip-to-chip silicon photonics gain traction for their bandwidth and
energy efficiency, collective communication has emerged as a critical
bottleneck in scale-up systems. Programmable photonic interconnects offer a
promising path forward: by dynamically reconfiguring the fabric, they can
establish direct, high-bandwidth optical paths between communicating endpoints
-- \emph{synchronously and guided by the structure of collective operations}
(e.g., AllReduce). However, realizing this vision -- \emph{when light bends to
the collective will} -- requires navigating a fundamental trade-off between
reconfiguration delay and the performance gains of adaptive topologies.
  In this paper, we present a simple theoretical framework for adaptive
photonic scale-up domains that makes this trade-off explicit and clarifies when
reconfiguration is worthwhile. Along the way, we highlight a connection -- not
surprising but still powerful -- between the Birkhoff--von Neumann (BvN)
decomposition, maximum concurrent flow (a classic measure of network
throughput), and the well-known $\alpha$-$\beta$ cost model for collectives.
Finally, we outline a research agenda in algorithm design and systems
integration that can build on this foundation.

</details>


### [200] [BlockSDN: Towards a High-Performance Blockchain via Software-Defined Cross Networking optimization](https://arxiv.org/abs/2510.08139)
*Wenyang Jia,Jingjing Wang,Ziwei Yan,Xiangli Peng,Guohui Yuan*

Main category: cs.NI

TL;DR: The paper introduces BlockSDN, an SDN-integrated architecture for blockchain, which significantly reduces block synchronization time.


<details>
  <summary>Details</summary>
Motivation: To address the scalability limits of blockchain systems caused by inefficient P2P broadcasting, particularly those overlooked at the physical network layer.

Method: BlockSDN integrates an SDN-based design, using a distributed control plane for network oversight, a graph engine for clustering, and a hybrid neighbor selection mechanism with hierarchical broadcasting.

Result: BlockSDN achieves a 65% and 55% reduction in global block synchronization time compared to Gossip and Mercury approaches, respectively, as per simulations.

Conclusion: The findings demonstrate that SDN-enabled cross-layer coordination can significantly improve blockchain scalability and performance.

Abstract: The scalability of blockchain systems is constrained by inefficient P2P
broadcasting, as most existing optimizations focus only on the logical layer
without considering physical network conditions. To address this, we propose
BlockSDN, the first SDN-based integrated architecture for blockchain. BlockSDN
employs a distributed control plane for a global network view, a graph engine
for hierarchical clustering, and a hybrid macro-micro neighbor selection with
hierarchical broadcasting. A dedicated simulation platform shows that BlockSDN
reduces global block synchronization time by 65% and 55% compared to Gossip and
Mercury, respectively.These results highlight the potential of SDN-enabled
cross-layer coordination to significantly enhance blockchain scalability and
performance.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [201] [Autoencoding Coordinate Sequences from Psychophysiologic Signals](https://arxiv.org/abs/2510.07415)
*Timothy L. Hutcheson,Anil K. Raj*

Main category: eess.SP

TL;DR: The paper introduces a technique to map 24 channels of psychophysiological data into 3D coordinates to estimate cognitive states.


<details>
  <summary>Details</summary>
Motivation: The goal is to effectively interpret psychophysiological data to understand participants' involvement in specific tasks and cognitive states.

Method: Utilizing EEG, ECG, EDA, and RR data, the method transforms time-series inputs into trackable 3D coordinates.

Result: The approach successfully maps psychophysiological metrics into 3D space, facilitating the estimation of specific tasks and cognitive states.

Conclusion: The work provides a novel way to analyze complex psychophysiological data and track cognitive and task-related states.

Abstract: We present a method for converting 24 channels of psychophysiologic time
series data collected from individual participants via electroencephalogram
(EEG), electrocardiogram (ECG), electrodermal activity (EDA), respiration rate
(RR) into trackable three dimensional (3D) coordinates sufficient to estimate
participation in specific task and cognitive states.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [202] [Beyond independent component analysis: identifiability and algorithms](https://arxiv.org/abs/2510.07525)
*Alvaro Ribot,Anna Seigal,Piotr Zwiernik*

Main category: math.ST

TL;DR: This paper extends Independent Component Analysis (ICA) by introducing pairwise mean independence, which generalizes independence notions and enables identifiable recovery of latent variables.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from addressing limitations in classical ICA that strictly relies on full independence, aiming to relax this condition while preserving identifiability properties.

Method: The authors introduce pairwise mean independence as a relaxed identifiability criterion and propose an algebraic recovery algorithm based on least-squares optimization over the orthogonal group.

Result: Simulations show that enforcing full independence can lead to estimation errors, whereas pairwise mean independence improves stability and robustness in recovering latent variables.

Conclusion: The study extends ICA with pairwise mean independence, offering a rigorous framework for blind source separation beyond full independence while highlighting the advantages of relaxed independence conditions.

Abstract: Independent Component Analysis (ICA) is a classical method for recovering
latent variables with useful identifiability properties. For independent
variables, cumulant tensors are diagonal; relaxing independence yields tensors
whose zero structure generalizes diagonality. These models have been the
subject of recent work in non-independent component analysis. We show that
pairwise mean independence answers the question of how much one can relax
independence: it is identifiable, any weaker notion is non-identifiable, and it
contains the models previously studied as special cases. Our results apply to
distributions with the required zero pattern at any cumulant tensor. We propose
an algebraic recovery algorithm based on least-squares optimization over the
orthogonal group. Simulations highlight robustness: enforcing full independence
can harm estimation, while pairwise mean independence enables more stable
recovery. These findings extend the classical ICA framework and provide a
rigorous basis for blind source separation beyond independence.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [203] [Platform-Agnostic Modular Architecture for Quantum Benchmarking](https://arxiv.org/abs/2510.08469)
*Neer Patel,Anish Giri,Hrushikesh Pramod Patil,Noah Siekierski,Avimita Chatterjee,Sonika Johri,Timothy Proctor,Thomas Lubinski,Siyuan Niu*

Main category: quant-ph

TL;DR: The paper introduces a modular architecture that enhances quantum computing benchmarking by separating problem generation, circuit execution, and results analysis into interoperable components. It supports diverse benchmarks and integrates with multiple APIs, validating its flexibility and extensibility.


<details>
  <summary>Details</summary>
Motivation: The increasing fragmentation in quantum computing benchmarking highlights the need for a standardized and interoperable system to reduce inefficiencies and enable diverse workflows across incompatible frameworks.

Method: They designed a platform-agnostic modular architecture that integrates several benchmarking variants and APIs (e.g., Qiskit, CUDA-Q, Cirq), validated its extensibility via new benchmarks, and demonstrated interoperability through integration with tools like pyGSTi and CUDA-Q.

Result: The architecture successfully reduced ecosystem fragmentation, supported 20+ benchmarks, integrated with various frameworks, and introduced new benchmarks like quantum reinforcement learning. It validated its flexibility through real-world implementations.

Conclusion: Standardized modular interfaces enable a more unified quantum computing benchmarking ecosystem, preserving optimization flexibility while enhancing interoperability. This framework is a significant enhancement to QED-C's benchmarking suite.

Abstract: We present a platform-agnostic modular architecture that addresses the
increasingly fragmented landscape of quantum computing benchmarking by
decoupling problem generation, circuit execution, and results analysis into
independent, interoperable components. Supporting over 20 benchmark variants
ranging from simple algorithmic tests like Bernstein-Vazirani to complex
Hamiltonian simulation with observable calculations, the system integrates with
multiple circuit generation APIs (Qiskit, CUDA-Q, Cirq) and enables diverse
workflows. We validate the architecture through successful integration with
Sandia's $\textit{pyGSTi}$ for advanced circuit analysis and CUDA-Q for
multi-GPU HPC simulations. Extensibility of the system is demonstrated by
implementing dynamic circuit variants of existing benchmarks and a new quantum
reinforcement learning benchmark, which become readily available across
multiple execution and analysis modes. Our primary contribution is identifying
and formalizing modular interfaces that enable interoperability between
incompatible benchmarking frameworks, demonstrating that standardized
interfaces reduce ecosystem fragmentation while preserving optimization
flexibility. This architecture has been developed as a key enhancement to the
continually evolving QED-C Application-Oriented Performance Benchmarks for
Quantum Computing suite.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [204] [Near-optimal Rank Adaptive Inference of High Dimensional Matrices](https://arxiv.org/abs/2510.08117)
*Frédéric Zheng,Yassir Jedra,Alexandre Proutiere*

Main category: cs.IT

TL;DR: The paper proposes rank-adaptive algorithms to estimate high-dimensional matrices from linear measurements, optimizing sample efficiency by adaptively selecting effective matrix ranks through singular value analysis.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently estimating high-dimensional matrices, where an optimal balance must be struck between precision in singular value estimation and approximation costs, particularly in noisy scenarios and limited samples.

Method: Introduces an adaptive algorithm combining a Least-Squares estimator with singular value thresholding, supported by enhanced theoretical analysis and finite-sample error bounds.

Result: The proposed algorithm achieves performance close to theoretical lower limits of sample complexity and demonstrates effectiveness for multivariate regression and linear dynamic system identification tasks.

Conclusion: The study provides robust algorithms and insights into effective rank selection for matrix estimation, offering optimal trade-offs between sample efficiency, rank adaptivity, and noise resilience.

Abstract: We address the problem of estimating a high-dimensional matrix from linear
measurements, with a focus on designing optimal rank-adaptive algorithms. These
algorithms infer the matrix by estimating its singular values and the
corresponding singular vectors up to an effective rank, adaptively determined
based on the data. We establish instance-specific lower bounds for the sample
complexity of such algorithms, uncovering fundamental trade-offs in selecting
the effective rank: balancing the precision of estimating a subset of singular
values against the approximation cost incurred for the remaining ones. Our
analysis identifies how the optimal effective rank depends on the matrix being
estimated, the sample size, and the noise level. We propose an algorithm that
combines a Least-Squares estimator with a universal singular value thresholding
procedure. We provide finite-sample error bounds for this algorithm and
demonstrate that its performance nearly matches the derived fundamental limits.
Our results rely on an enhanced analysis of matrix denoising methods based on
singular value thresholding. We validate our findings with applications to
multivariate regression and linear dynamical system identification.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [205] [Transferable Generative Models Bridge Femtosecond to Nanosecond Time-Step Molecular Dynamics](https://arxiv.org/abs/2510.07589)
*Juan Viguera Diez,Mathias Schreiner,Simon Olsson*

Main category: physics.chem-ph

TL;DR: This paper presents a deep generative modeling framework to accelerate molecular dynamics simulations by four orders of magnitude while maintaining physical realism.


<details>
  <summary>Details</summary>
Motivation: The goal is to overcome the limitations of conventional molecular dynamics simulations, which are restricted by their femtosecond time steps and thus cannot access slower, functionally important molecular dynamics.

Method: The authors implemented a deep generative modeling framework to enhance sampling of molecular dynamics, achieving faster yet physically accurate simulations for small organic molecules and peptides.

Result: The framework quantitatively characterizes equilibrium ensembles and dynamical relaxation processes on extended time scales, generalizes across chemical compositions, and scales to larger peptides than those used for training.

Conclusion: This approach enables deeper understanding of molecular behavior by bridging multiple time scales without compromising atomistic resolution, facilitating exploration of conformational landscapes, thermodynamics, and kinetics in chemistry and biophysics.

Abstract: Understanding molecular structure, dynamics, and reactivity requires bridging
processes that occur across widely separated time scales. Conventional
molecular dynamics simulations provide atomistic resolution, but their
femtosecond time steps limit access to the slow conformational changes and
relaxation processes that govern chemical function. Here, we introduce a deep
generative modeling framework that accelerates sampling of molecular dynamics
by four orders of magnitude while retaining physical realism. Applied to small
organic molecules and peptides, the approach enables quantitative
characterization of equilibrium ensembles and dynamical relaxation processes
that were previously only accessible by costly brute-force simulation.
Importantly, the method generalizes across chemical composition and system
size, extrapolating to peptides larger than those used for training, and
captures chemically meaningful transitions on extended time scales. By
expanding the accessible range of molecular motions without sacrificing
atomistic detail, this approach opens new opportunities for probing
conformational landscapes, thermodynamics, and kinetics in systems central to
chemistry and biophysics.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [206] [Decentralised Blockchain Management Through Digital Twins](https://arxiv.org/abs/2510.07901)
*Georgios Diamantopoulos,Nikos Tziritas,Rami Bahsoon,Georgios Theodoropoulos*

Main category: cs.CR

TL;DR: The paper proposes a dynamic and decentralized blockchain management system using digital twins to address the trade-off between control and decentralization in blockchain governance.


<details>
  <summary>Details</summary>
Motivation: Current blockchain systems face a dilemma between maintaining decentralization and enabling effective governance and dynamic management.

Method: The mechanism uses multiple digital twins controlled by stakeholders. These twins operate within a secondary blockchain for decentralized decision-making, reaching consensus and propagating decisions to a primary blockchain.

Result: Simulation evaluations showed the proposed mechanism can rapidly reach consensus and reconfigure the primary blockchain with minimal overhead.

Conclusion: The proposed digital twin-based system effectively manages blockchain systems without necessitating centralized governance.

Abstract: The necessity of blockchain systems to remain decentralised limits current
solutions to blockchain governance and dynamic management, forcing a trade-off
between control and decentralisation. In light of the above, this work proposes
a dynamic and decentralised blockchain management mechanism based on digital
twins. To ensure decentralisation, the proposed mechanism utilises multiple
digital twins that the system's stakeholders control. To facilitate
decentralised decision-making, the twins are organised in a secondary
blockchain system that orchestrates agreement on, and propagation of decisions
to the managed blockchain. This enables the management of blockchain systems
without centralised control. A preliminary evaluation of the performance and
impact of the overheads introduced by the proposed mechanism is conducted
through simulation. The results demonstrate the proposed mechanism's ability to
reach consensus on decisions quickly and reconfigure the primary blockchain
with minimal overhead.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [207] [Multi-level informed optimization via decomposed Kriging for large design problems under uncertainty](https://arxiv.org/abs/2510.07904)
*Enrico Ampellio,Blazhe Gjorgiev,Giovanni Sansavini*

Main category: eess.SY

TL;DR: The paper addresses challenges in optimizing complex engineering designs under uncertainty by proposing a scalable, Kriging-based surrogate model offering higher accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Conventional methods struggle with scalability and precision in optimizing large, complex engineering designs under uncertainty.

Method: The authors developed a multi-level approach using Kriging-based surrogates and hierarchical decomposition for efficient optimization within high-dimensional spaces.

Result: The proposed method outperformed state-of-the-art techniques in both speed and accuracy during statistical comparisons on an analytical testbed.

Conclusion: The approach is a robust and scalable solution for accurately optimizing complex engineering problems under uncertainty, requiring minimal resources.

Abstract: Engineering design involves demanding models encompassing many decision
variables and uncontrollable parameters. In addition, unavoidable aleatoric and
epistemic uncertainties can be very impactful and add further complexity. The
state-of-the-art adopts two steps, uncertainty quantification and design
optimization, to optimize systems under uncertainty by means of robust or
stochastic metrics. However, conventional scenario-based, surrogate-assisted,
and mathematical programming methods are not sufficiently scalable to be
affordable and precise in large and complex cases. Here, a multi-level approach
is proposed to accurately optimize resource-intensive, high-dimensional, and
complex engineering problems under uncertainty with minimal resources. A
non-intrusive, fast-scaling, Kriging-based surrogate is developed to map the
combined design/parameter domain efficiently. Multiple surrogates are
adaptively updated by hierarchical and orthogonal decomposition to leverage the
fewer and most uncertainty-informed data. The proposed method is statistically
compared to the state-of-the-art via an analytical testbed and is shown to be
concurrently faster and more accurate by orders of magnitude.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [208] [SEPhIA: <1 laser/neuron Spiking Electro-Photonic Integrated Multi-Tiled Architecture for Scalable Optical Neuromorphic Computing](https://arxiv.org/abs/2510.07427)
*Matěj Hejda,Aishwarya Natarajan,Chaerin Hong,Mehmet Berkay On,Sébastien d'Herbais de Thun,Raymond G. Beausoleil,Thomas Van Vaerenbergh*

Main category: cs.ET

TL;DR: SEPhIA introduces a scalable photonic-electronic architecture for optical spiking neural networks, emphasizing realistic constraints and achieving high classification accuracy on specific tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current research on optical spiking neural networks, such as constraints in optical power, crosstalk, and scalability.

Method: SEPhIA uses microring resonator modulators (MRMs) combined with multi-wavelength sources and validates the architecture through co-simulation and a trainable optoelectronic SNN model.

Result: Achieved over 90% classification accuracy on a spike-encoded dataset, demonstrating a realistic and effective neuromorphic photonic computing solution.

Conclusion: SEPhIA provides a scalable, trainable, and physically grounded architecture for advancing optical SNNs with practical implementation feasibility.

Abstract: Research into optical spiking neural networks (SNNs) has primarily focused on
spiking devices, networks of excitable lasers or numerical modelling of large
architectures, often overlooking key constraints such as limited optical power,
crosstalk and footprint. We introduce SEPhIA, a photonic-electronic,
multi-tiled SNN architecture emphasizing implementation feasibility and
realistic scaling. SEPhIA leverages microring resonator modulators (MRMs) and
multi-wavelength sources to achieve effective sub-one-laser-per-spiking neuron
efficiency. We validate SEPhIA at both device and architecture levels by
time-domain co-simulating excitable CMOS-MRR coupled circuits and by devising a
physics-aware, trainable optoelectronic SNN model, with both approaches
utilizing experimentally derived device parameters. The multi-layer
optoelectronic SNN achieves classification accuracies over 90% on a four-class
spike-encoded dataset, closely comparable to software models. A design space
study further quantifies how photonic device parameters impact SNN performance
under constrained signal-to-noise conditions. SEPhIA offers a scalable,
expressive, physically grounded solution for neuromorphic photonic computing,
capable of addressing spike-encoded tasks.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [209] [Spike-frequency and h-current based adaptation are dynamically equivalent in a Wilson-Cowan field model](https://arxiv.org/abs/2510.08436)
*Ronja Strömsdörfer,Klaus Obermayer*

Main category: nlin.AO

TL;DR: The study explores how two different neural adaptation mechanisms influence traveling waves during slow-wave sleep, using a model of neuronal activity.


<details>
  <summary>Details</summary>
Motivation: To understand how internal adaptation mechanisms, such as spike-frequency adaptation and hyperpolarization-activated currents, modulate traveling waves during slow-wave sleep.

Method: A spatially extended two-population Wilson-Cowan model was used, incorporating local spatial coupling and adaptation mechanisms applied to excitatory neural populations. Dynamical equivalence and state-space shifts were analyzed under varying conditions.

Result: The mechanisms were shown to be mathematically equivalent with compensatory input, and strong adaptation was required to induce traveling waves. Adaptation modulates temporal/spatial frequencies and wave speed, with variations leading to different effects on wave propagation.

Conclusion: While the mechanisms are mathematically equivalent, differences in feedback strength affect wave propagation, providing insights into the modulation of brain rhythms during slow-wave sleep.

Abstract: During slow-wave sleep, the brain produces traveling waves of slow
oscillations (SOs; $\leq 2$ Hz), characterized by the propagation of
alternating high- and low-activity states. The question of internal mechanisms
that modulate traveling waves of SOs is still unanswered although it is
established that it is an adaptation mechanism that mediates them. One
mechanism investigated is spike-frequency adaptation, a hyperpolarizing
feedback current that is activated during periods of high-activity. An
alternative mechanism is based on hyperpolarization-activated currents, which
are positive feedback currents that are activated in low-activity states. Both
adaptation mechanisms were shown to feature SO-like dynamics in neuronal
populations, and the inclusion of a spatial domain seems to enhance observable
differences in their effects. To investigate this in detail, we examine a
spatially extended two-population Wilson-Cowan model with local spatial
coupling and the excitatory populations equipped with either one of the two
adaptation mechanisms. We describe them with the same dynamical equation and
include the inverse mode of action by changing the signs of adaptation strength
and gain. We show that the dynamical systems are mathematically equivalent
under a compensatory external input, which depends on the adaptation strength,
leading to a shift in state space of the otherwise equivalent bifurcation
structure. Strong enough adaptation is required to induce traveling waves.
Additionally, adaptation modulates the properties of the spatio-temporal
activity patterns, such as temporal and spatial frequencies, and the speed of
the traveling waves, all of which increase with increasing strength. Though
being dynamically equivalent, our results also explain why location-dependent
variations in feedback strength cause differences in the propagation of
traveling waves between both adaptation mechanisms.

</details>
