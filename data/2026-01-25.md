<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 9]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 10]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.SE](#cs.SE) [Total: 7]
- [q-bio.NC](#q-bio.NC) [Total: 7]
- [stat.ML](#stat.ML) [Total: 6]
- [cs.NI](#cs.NI) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models](https://arxiv.org/abs/2601.15305)
*Alfred Shen,Aaron Shen*

Main category: cs.AI

TL;DR: The paper introduces Gated Sparse Attention (GSA), which combines strengths from sparse attention and gated attention models, achieving efficiency and quality improvements in long-context language models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of high computational costs in long-context language models, improving both efficiency and stability by integrating sparse attention with gated attention mechanisms.

Method: The proposed GSA architecture includes a gated lightning indexer, an adaptive sparsity controller, and dual gating mechanisms. It optimizes token selection, modulates sparsity based on uncertainty, and enhances stability.

Result: GSA achieves 12-16x efficiency speed-ups at 128K context, decreases perplexity from 6.03 to 5.70, nearly doubles RULER scores, minimizes attention to initial tokens, and reduces training loss spikes by 98%.

Conclusion: GSA effectively balances efficiency and stability, overcoming the limitations of existing sparse and gated attention methods, making it highly effective for long-context language models.

Abstract: The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.

</details>


### [2] [Uncovering Latent Bias in LLM-Based Emergency Department Triage Through Proxy Variables](https://arxiv.org/abs/2601.15306)
*Ethan Zhang*

Main category: cs.AI

TL;DR: The paper addresses bias in large language model (LLM)-based AI systems for emergency department triage and finds discriminatory behaviors influenced by proxy variables.


<details>
  <summary>Details</summary>
Motivation: To investigate hidden biases in LLMs applied to clinical decision-making, specifically in emergency department triage, to ensure patient equity.

Method: The study evaluated proxy variables using both public and restricted MIMIC datasets and analyzed LLM behavior in representing patient acuity with respect to specific contextual tokens.

Result: Findings revealed systematic biases in how LLMs modify perceived patient severity, mediated by proxy variables, leading to discriminatory behavior.

Conclusion: The study underscores that LLMs are still imperfect at representing real patient acuity and stresses the need for improved training and safeguards for AI deployment in clinical contexts.

Abstract: Recent advances in large language models (LLMs) have enabled their integration into clinical decision-making; however, hidden biases against patients across racial, social, economic, and clinical backgrounds persist. In this study, we investigate bias in LLM-based medical AI systems applied to emergency department (ED) triage. We employ 32 patient-level proxy variables, each represented by paired positive and negative qualifiers, and evaluate their effects using both public (MIMIC-IV-ED Demo, MIMIC-IV Demo) and restricted-access credentialed (MIMIC-IV-ED and MIMIC-IV) datasets as appropriate~\cite{mimiciv_ed_demo,mimiciv_ed,mimiciv}. Our results reveal discriminatory behavior mediated through proxy variables in ED triage scenarios, as well as a systematic tendency for LLMs to modify perceived patient severity when specific tokens appear in the input context, regardless of whether they are framed positively or negatively. These findings indicate that AI systems is still imperfectly trained on noisy, sometimes non-causal signals that do not reliably reflect true patient acuity. Consequently, more needs to be done to ensure the safe and responsible deployment of AI technologies in clinical settings.

</details>


### [3] [DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey](https://arxiv.org/abs/2601.15307)
*Guo-Biao Zhang,Ding-Yuan Liu,Da-Yi Wu,Tian Lan,Heyan Huang,Zhijing Wu,Xian-Ling Mao*

Main category: cs.AI

TL;DR: The paper introduces DeepSurvey-Bench, a benchmark designed to evaluate the academic value, including informational, communication, and guidance aspects, of generated surveys.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to effectively assess the deep academic value of generated surveys, focusing only on surface metrics and unreliable ground truth datasets.

Method: The authors propose an evaluation criteria covering informational value, scholarly communication, and research guidance, along with constructing a reliable dataset annotated with academic value for evaluation.

Result: Experimental results show that DeepSurvey-Bench is consistent with human evaluations in assessing the academic value of generated surveys.

Conclusion: DeepSurvey-Bench addresses flaws of prior benchmarks by emphasizing deep academic value, providing a comprehensive tool for evaluating survey quality.

Abstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep "academic value", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.

</details>


### [4] [Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases](https://arxiv.org/abs/2601.15476)
*Alex Dantart*

Main category: cs.AI

TL;DR: The paper focuses on improving the reliability of large language models for legal tasks by reducing hallucinations through retrieval-augmented generation (RAG) systems.


<details>
  <summary>Details</summary>
Motivation: Legal work demands high reliability as AI hallucinations can lead to serious consequences. The paper explores frameworks to minimize hallucinations in AI systems used for high-stakes legal tasks.

Method: The authors evaluate 2,700 answers from 12 LLMs across 75 legal tasks using metrics False Citation Rate (FCR) and Fabricated Fact Rate (FFR), and compare three AI paradigms: standalone generative models, basic RAG systems, and advanced RAG systems.

Result: Standalone generative models showed high errors (FCR >30%), while basic RAG minimized errors but still had notable misgrounding. Advanced RAG systems using embedding fine-tuning, re-ranking, and self-correction reduced fabrication to below 0.2%.

Conclusion: Trustworthy legal AI requires rigor-driven, retrieval-based, and verification-focused architectures. The framework proposed is applicable to other high-risk domains.

Abstract: This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models ("creative oracle"), (2) basic retrieval-augmented systems ("expert archivist"), and (3) an advanced, end-to-end optimized RAG system ("rigorous archivist"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.

</details>


### [5] [Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.15311)
*Mustafa Arslan*

Main category: cs.AI

TL;DR: Aeon introduces a neuro-symbolic cognitive operating system addressing LLM limitations like high computational cost and information disorganization in expanded contexts.


<details>
  <summary>Details</summary>
Motivation: To overcome LLM constraints such as inefficiencies in self-attention computation and the degradation of reasoning abilities when dealing with large context windows, as well as the inadequacies of existing memory systems.

Method: Developed Aeon with a structured "Memory Palace" and "Trace," using Atlas for efficient memory retrieval and the Semantic Lookaside Buffer for high-speed caching.

Result: Aeon achieved sub-millisecond retrieval latency and maintained state consistency for conversational tasks, supporting structured and persistent memory use.

Conclusion: Aeon presents a significant advancement in LLM memory management, enabling efficient, hierarchical, and temporally aware memory systems tailored for autonomous agents.

Abstract: Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the "Lost in the Middle" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily "Flat RAG" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to "Vector Haze", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.

</details>


### [6] [The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15316)
*Wei Ai,Yilong Tan,Yuntao Shou,Tao Meng,Haowen Chen,Zhixiong He,Keqin Li*

Main category: cs.AI

TL;DR: This paper reviews the evolution of multimodal fake news detection (MFND) through large vision-language models, which improve semantic understanding and text-image interaction, addressing the gap in systematic surveys.


<details>
  <summary>Details</summary>
Motivation: The motivation is to fill the gap by systematically documenting the evolution and contributions of large vision-language models (LVLMs) in improving multimodal fake news detection, which lacks thorough surveys.

Method: The authors analyze prior methods and present a historical evolution, propose taxonomy framework for architectures, datasets, and benchmarks, evaluate challenges, and outline future research directions.

Result: The survey consolidates advancements in MFND using LVLMs and highlights challenges like interpretability, domain generalization, and remaining technical gaps for further exploration.

Conclusion: This is the first comprehensive survey detailing the transformative role of LVLMs in combating multimodal fake news, presenting taxonomy, challenges, and directions for future improvement.

Abstract: In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.

</details>


### [7] [Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents](https://arxiv.org/abs/2601.15322)
*Raffi Khatchadourian*

Main category: cs.AI

TL;DR: The paper addresses LLM agents' difficulty in reproducing consistent audit trajectory decisions and proposes a DFAH framework to measure determinism and faithfulness. Benchmark results highlight variability across model sizes and configurations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in regulatory audit replay for LLM agents, seeking consistent and evidence-faithful reproductions of flagged transaction decisions.

Method: The paper introduces the DFAH framework to evaluate determinism and evidence-conditioned faithfulness across various model configurations and agents using tools in financial services.

Result: Results indicate smaller models achieve higher determinism with fewer runs, while tool-use introduces variability. A positive correlation between determinism and faithfulness was discovered. Benchmarked models align with audit replay standards under certain conditions.

Conclusion: Tier 1, schema-first LLM architectures provide reliable audit replay determinism, supporting improved financial regulatory compliance and faithfulness in tool-using applications.

Abstract: LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.
  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.
  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.

</details>


### [8] [Prometheus Mind: Retrofitting Memory to Frozen Language Models](https://arxiv.org/abs/2601.15324)
*Mark Wind*

Main category: cs.AI

TL;DR: The study presents a reversible memory extension method, Prometheus Mind, for frozen language models like Qwen3-4B using modular adapters without altering the model weights.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance pretrained language models with memory capabilities using an approach that avoids modifying the original model weights or architecture, ensuring reversibility.

Method: They use 11 modular adapters with a focus on four technical challenges: extracting semantic directions via Contrastive Direction Discovery (CDD), stage-wise training for modular components, adapting mappings from existing model parameters, and addressing hidden state collapses through projection.

Result: Prometheus Mind achieves 94.4% retrieval accuracy on clean cases but is less effective (19.4%) on informal inputs. The bottleneck lies in relation classification, which records 47.3% accuracy.

Conclusion: The approach demonstrates the feasibility of adding memory to frozen models; however, reliability declines in informal contexts, demanding improvements in relation classification and robustness.

Abstract: Adding memory to pretrained language models typically requires architectural changes or weight modification. We present Prometheus Mind, which retrofits memory to a frozen Qwen3-4B using 11 modular adapters (530MB, 7% overhead) -- fully reversible by removing the adapters. Building this system required solving four problems: (1) Extraction -- we develop Contrastive Direction Discovery (CDD), which finds semantic directions via minimal pairs without labeled data. (2) Training -- end-to-end optimization collapses; stage-wise training of each adapter on simple proxy tasks succeeds. (3) Injection -- learned encoders fail to generalize; we find that lm_head.weight rows already provide the mapping we need, requiring no training. (4) Hidden state collapse -- transformers make ``wife'' and ``brother'' 0.98+ similar; we train projections to recover distinction (0.98 $\rightarrow$ 0.09). On PrometheusExtract-132 (132 cases), the system achieves 94.4% retrieval on clean inputs (n=54, 95% CI: [84.9%, 98.1%]), degrading to 19.4% on informal inputs with ellipsis, filler words, or implicit subjects (n=36). The primary bottleneck is relation classification (47.3% accuracy), responsible for most extraction errors.

</details>


### [9] [Logic Programming on Knowledge Graph Networks And its Application in Medical Domain](https://arxiv.org/abs/2601.15347)
*Chuanqing Wang,Zhenmin Zhao,Shanshan Du,Chaoqun Fei,Songmao Zhang,Ruqian Lu*

Main category: cs.AI

TL;DR: This paper introduces the concept of 'knowledge graph network' and its applications in medicine and healthcare, highlighting its systematic development and experimentation.


<details>
  <summary>Details</summary>
Motivation: Traditional knowledge graph research lacks sufficient application of advanced logic reasoning, AI techniques, and multi-graph cooperation, especially in healthcare.

Method: The authors develop a systematic framework for 'knowledge graph network,' addressing various conditions like uncertainty and multimodality, supported by real data and experiments.

Result: The paper demonstrates the definition, computation, reasoning, and applications of knowledge graph networks in healthcare using examples and experimental results.

Conclusion: The research concludes with a focus on innovation, highlighting advancements in knowledge graph network techniques for healthcare applications.

Abstract: The rash development of knowledge graph research has brought big driving force to its application in many areas, including the medicine and healthcare domain. However, we have found that the application of some major information processing techniques on knowledge graph still lags behind. This defect includes the failure to make sufficient use of advanced logic reasoning, advanced artificial intelligence techniques, special-purpose programming languages, modern probabilistic and statistic theories et al. on knowledge graphs development and application. In particular, the multiple knowledge graphs cooperation and competition techniques have not got enough attention from researchers. This paper develops a systematic theory, technique and application of the concept 'knowledge graph network' and its application in medical and healthcare domain. Our research covers its definition, development, reasoning, computing and application under different conditions such as unsharp, uncertain, multi-modal, vectorized, distributed, federated. Almost in each case we provide (real data) examples and experiment results. Finally, a conclusion of innovation is provided.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design](https://arxiv.org/abs/2601.15710)
*Jiahao Zhang,Zifan He,Nicholas Fraser,Michaela Blott,Yizhou Sun,Jason Cong*

Main category: cs.AR

TL;DR: FlexLLM is a High-Level Synthesis (HLS) library for building domain-specific LLM accelerators, enabling rapid, efficient, and low-bit inference acceleration with improved performance and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for efficient and rapid development of domain-specific hardware accelerators for large language model (LLM) inference, where standard devices struggle with performance efficiency and long-context processing.

Method: FlexLLM introduces an HLS library with customizable architectural parameters for LLM inference stages, a quantization suite for low-bit operation, and a Hierarchical Memory Transformer (HMT) plug-in to optimize long-context processing.

Result: FlexLLM's implementation on an AMD U280 FPGA achieves significant performance improvements compared to an NVIDIA A100 GPU, including up to 1.29x speedup, 1.64x decode throughput, 3.14x energy efficiency, and remarkable long-context processing capabilities with HMT, further scaling on advanced FPGA nodes.

Conclusion: FlexLLM simplifies the creation of high-performance LLM accelerators, combining advanced algorithmic innovations and hardware customization with minimal effort, enabling both fast deployment and superior computational efficiency.

Abstract: We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\times$ end-to-end speedup, 1.64$\times$ higher decode throughput, and 3.14$\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\times$, 6.55$\times$, and 4.13$\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\times$ and extends the context window by 64$\times$, delivering 1.10$\times$/4.86$\times$ lower end-to-end latency and 5.21$\times$/6.27$\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.

</details>


### [11] [A Case for Hypergraphs to Model and Map SNNs on Neuromorphic Hardware](https://arxiv.org/abs/2601.16118)
*Marco Ronzani,Cristina Silvano*

Main category: cs.AR

TL;DR: The paper proposes hypergraph-based techniques for mapping Spiking Neural Networks (SNNs) onto neuromorphic hardware, aiming to improve efficiency compared to current state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: SNNs mapping is challenging due to its NP-hard nature, scaling complexities, and the need to optimize spike movement and core usage on neuromorphic hardware.

Method: The authors raise the abstraction level from graphs to hypergraphs for SNNs, redesigning mapping techniques and analyzing the benefits of hyperedge properties to improve partitioning and placement algorithms.

Result: Hypergraph-based mapping techniques demonstrated better performance than state-of-the-art methods across various bio-plausible SNNs and execution time scenarios.

Conclusion: A promising set of algorithms using hypergraph insights effectively achieves better mappings for SNNs on neuromorphic hardware, catering to scalability and execution efficiency.

Abstract: Executing Spiking Neural Networks (SNNs) on neuromorphic hardware poses the problem of mapping neurons to cores. SNNs operate by propagating spikes between neurons that form a graph through synapses. Neuromorphic hardware mimics them through a network-on-chip, transmitting spikes, and a mesh of cores, each managing several neurons. Its operational cost is tied to spike movement and active cores. A mapping comprises two tasks: partitioning the SNN's graph to fit inside cores and placement of each partition on the hardware mesh. Both are NP-hard problems, and as SNNs and hardware scale towards billions of neurons, they become increasingly difficult to tackle effectively. In this work, we propose to raise the abstraction of SNNs from graphs to hypergraphs, redesigning mapping techniques accordingly. The resulting model faithfully captures the replication of spikes inside cores by exposing the notion of hyperedge co-membership between neurons. We further show that the overlap and locality of hyperedges strongly correlate with high-quality mappings, making these properties instrumental in devising mapping algorithms. By exploiting them directly, grouping neurons through shared hyperedges, communication traffic and hardware resource usage can be reduced be yond what just contracting individual connections attains. To substantiate this insight, we consider several partitioning and placement algorithms, some newly devised, others adapted from literature, and compare them over progressively larger and bio-plausible SNNs. Our results show that hypergraph based techniques can achieve better mappings than the state-of-the-art at several execution time regimes. Based on these observations, we identify a promising selection of algorithms to achieve effective mappings at any scale.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Entropy-Tree: Tree-Based Decoding with Entropy-Guided Exploration](https://arxiv.org/abs/2601.15296)
*Longxuan Wei,Yubo Zhang,Zijiao Zhang,Zhihu Wang,Shiwan Zhao,Tianyu Huang,Huiting Zhao,Chenfei Liu,Shenao Zhang,Junchi Yan*

Main category: cs.CL

TL;DR: The paper introduces Entropy-Tree, an advanced decoding approach for large language models that selectively expands search trees based on uncertainty, improving accuracy and uncertainty estimation in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing decoding strategies in large language models either lack direction (random sampling) or are inefficient with redundancies (independent multi-sampling). A method that balances exploration efficiency and uncertainty estimation is needed.

Method: The proposed method, Entropy-Tree, uses entropy measures to guide branching decisions in a tree-shaped decoding process, focusing on areas where the model's uncertainty is high.

Result: Entropy-Tree outperforms existing methods like Multi-chain in terms of pass@k accuracy and shows superior uncertainty estimation metrics, including better AUROC values.

Conclusion: Entropy-Tree improves reasoning accuracy and calibration in large language models while integrating structured exploration and reliable uncertainty estimation into a unified process.

Abstract: Large language models achieve strong reasoning performance, yet existing decoding strategies either explore blindly (random sampling) or redundantly (independent multi-sampling). We propose Entropy-Tree, a tree-based decoding method that exploits entropy as a signal for branching decisions--expanding the search tree only at positions where the model exhibits genuine uncertainty. Entropy-Tree shows superior accuracy and calibration in reasoning tasks: it achieves better pass@k than Multi-chain across multiple models and datasets, and its predictive entropy demonstrates better AUROC compared to several traditional metrics. Entropy-Tree unifies efficient structured exploration and reliable uncertainty estimation within a single decoding procedure.

</details>


### [13] [AfriEconQA: A Benchmark Dataset for African Economic Analysis based on World Bank Reports](https://arxiv.org/abs/2601.15297)
*Edward Ajayi*

Main category: cs.CL

TL;DR: AfriEconQA is a new dataset focused on African economic analysis with 8,937 QA instances from 236 World Bank reports for benchmarking information retrieval systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in specialized datasets for African economic analysis, enabling advancements in domain-specific information retrieval and reasoning systems.

Method: They created a high-quality dataset consisting of QA pairs requiring numerical reasoning and temporal analysis, benchmarking it against zero-shot and Retrieval-augmented Generation (RAG) models in 11 experiments.

Result: The results highlight the difficulty of the task, as zero-shot models fail over 90% of queries, and RAG systems achieve limited precision.

Conclusion: AfriEconQA provides a robust challenge for IR and RAG systems, underscoring the need for tailored methods in underrepresented domains. The dataset will be publicly available.

Abstract: We introduce AfriEconQA, a specialized benchmark dataset for African economic analysis grounded in a comprehensive corpus of 236 World Bank reports. The task of AfriEconQA is to answer complex economic queries that require high-precision numerical reasoning and temporal disambiguation from specialized institutional documents. The dataset consists of 8,937 curated QA instances, rigorously filtered from a pool of 10018 synthetic questions to ensure high-quality evidence-answer alignment. Each instance is composed of: (1) a question requiring reasoning over economic indicators, (2) the corresponding evidence retrieved from the corpus, (3) a verified ground-truth answer, and (4) source metadata (e.g., URL and publication date) to ensure temporal provenance. AfriEconQA is the first benchmark focused specifically on African economic analysis, providing a unique challenge for Information Retrieval (IR) systems, as the data is largely absent from the pretraining corpora of current Large Language Models (LLMs). We operationalize this dataset through an 11-experiment matrix, benchmarking a zero-shot baseline (GPT-5 Mini) against RAG configurations using GPT-4o and Qwen 32B across five distinct embedding and ranking strategies. Our results demonstrate a severe parametric knowledge gap, where zero-shot models fail to answer over 90 percent of queries, and even state-of-the-art RAG pipelines struggle to achieve high precision. This confirms AfriEconQA as a robust and challenging benchmark for the next generation of domain-specific IR and RAG systems. The AfriEconQA dataset and code will be made publicly available upon publication.

</details>


### [14] [Embedding Retrofitting: Data Engineering for better RAG](https://arxiv.org/abs/2601.15298)
*Anantha Sharma*

Main category: cs.CL

TL;DR: This paper proposes a data engineering framework to improve word embedding retrofitting by addressing knowledge graph artifacts caused by poor text preprocessing, showing preprocessing quality outweighs algorithm differences.


<details>
  <summary>Details</summary>
Motivation: To address the degradation in the quality of retrofitted word embeddings caused by low-quality knowledge graphs, which often arise from textual annotation artifacts.

Method: The study evaluates the impact of text preprocessing quality on retrofitting algorithms by reducing annotation artifacts, notably hashtag-induced knowledge graph density, and measuring retrofitting performance.

Result: On noisy knowledge graphs, retrofitting techniques degraded performance by 3.5%â€“5.2%, while preprocessing improved performance with EWMA retrofitting achieving a 6.2% improvement, and performance excelled in specific tasks like quantitative synthesis questions (+33.8%).

Conclusion: Preprocessing quality, particularly removing annotation artifacts, is the most important determinant of successful retrofitting, more impactful than differences between retrofitting algorithms.

Abstract: Embedding retrofitting adjusts pre-trained word vectors using knowledge graph constraints to improve domain-specific retrieval. However, the effectiveness of retrofitting depends critically on knowledge graph quality, which in turn depends on text preprocessing. This paper presents a data engineering framework that addresses data quality degradation from annotation artifacts in real-world corpora.
  The analysis shows that hashtag annotations inflate knowledge graph density, leading to creating spurious edges that corrupt the retrofitting objective. On noisy graphs, all retrofitting techniques produce statistically significant degradation ($-3.5\%$ to $-5.2\%$, $p<0.05$). After preprocessing, \acrshort{ewma} retrofitting achieves $+6.2\%$ improvement ($p=0.0348$) with benefits concentrated in quantitative synthesis questions ($+33.8\%$ average). The gap between clean and noisy preprocessing (10\%+ swing) exceeds the gap between algorithms (3\%), establishing preprocessing quality as the primary determinant of retrofitting success.

</details>


### [15] [MALTopic: Multi-Agent LLM Topic Modeling Framework](https://arxiv.org/abs/2601.15299)
*Yash Sharma*

Main category: cs.CL

TL;DR: This paper introduces MALTopic, a multi-agent LLM-based topic modeling framework that enhances survey text analysis by integrating structured data and improving topic coherence, diversity, and interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional topic modeling methods focus solely on free-text responses, ignore structured or categorical data, and often require significant human interpretation due to abstract results. This work aims to overcome these limitations.

Method: The authors propose MALTopic, a framework that uses LLM agents for specialized tasks: enriching text with structured data, extracting latent themes, and deduplicating results to refine topics.

Result: The MALTopic framework improves topic coherence, diversity, and interpretability compared to existing methods like LDA and BERTopic when tested on a survey dataset.

Conclusion: MALTopic successfully bridges the gap between structured and textual data in topic modeling, offering a more contextual and human-readable analysis for complex survey data.

Abstract: Topic modeling is a crucial technique for extracting latent themes from unstructured text data, particularly valuable in analyzing survey responses. However, traditional methods often only consider free-text responses and do not natively incorporate structured or categorical survey responses for topic modeling. And they produce abstract topics, requiring extensive human interpretation. To address these limitations, we propose the Multi-Agent LLM Topic Modeling Framework (MALTopic). This framework decomposes topic modeling into specialized tasks executed by individual LLM agents: an enrichment agent leverages structured data to enhance textual responses, a topic modeling agent extracts latent themes, and a deduplication agent refines the results. Comparative analysis on a survey dataset demonstrates that MALTopic significantly improves topic coherence, diversity, and interpretability compared to LDA and BERTopic. By integrating structured data and employing a multi-agent approach, MALTopic generates human-readable topics with enhanced contextual relevance, offering a more effective solution for analyzing complex survey data.

</details>


### [16] [Intelligence Degradation in Long-Context LLMs: Critical Threshold Determination via Natural Length Distribution Analysis](https://arxiv.org/abs/2601.15300)
*Weiwei Wang,Jiyong Min,Weijie Zou*

Main category: cs.CL

TL;DR: The paper identifies and analyzes catastrophic performance degradation in Large Language Models (LLMs) when processing long contexts, particularly beyond critical thresholds. It provides new insights and strategies to overcome these limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate and address the significant performance degradation observed in LLMs when dealing with long contexts, which poses a major challenge for their applications in long-context scenarios.

Method: Three contributions were made: (1) analyzing natural length distributions to isolate context length effects, (2) determining critical threshold contexts for Qwen2.5-7B, and (3) developing a unified framework to explain and mitigate shallow long-context adaptation.

Result: For Qwen2.5-7B, the critical threshold was identified at 40-50% of maximum context length, beyond which F1 scores drop significantly, exhibiting up to 45.5% degradation.

Conclusion: The study systematically highlights the issue of shallow long-context adaptation in LLMs and proposes a unified framework, paving the way for developing robust mitigation strategies for long-context performance degradation.

Abstract: Large Language Models (LLMs) exhibit catastrophic performance degradation when processing contexts approaching certain critical thresholds, even when information remains relevant. This intelligence degradation-defined as over 30% drop in task performance-severely limits long-context applications. This degradation shows a common pattern: models maintain strong performance up to a critical threshold, then collapse catastrophically. We term this shallow long-context adaptation-models adapt for short to medium contexts but fail beyond critical thresholds. This paper presents three contributions: (1) Natural Length Distribution Analysis: We use each sample's natural token length without truncation or padding, providing stronger causal evidence that degradation results from context length itself. (2) Critical Threshold Determination: Through experiments on a mixed dataset (1,000 samples covering 5%-95% of context length), we identify the critical threshold for Qwen2.5-7B at 40-50% of maximum context length, where F1 scores drop from 0.55-0.56 to 0.3 (45.5% degradation), using five-method cross-validation. (3) Unified Framework: We consolidate shallow adaptation, explaining degradation patterns and providing a foundation for mitigation strategies. This work provides the first systematic characterization of intelligence degradation in open-source Qwen models, offering practical guidance for deploying LLMs in long-context scenarios.

</details>


### [17] [Can We Trust LLM Detectors?](https://arxiv.org/abs/2601.15301)
*Jivnesh Sandhan,Harshit Jaiswal,Fei Cheng,Yugo Murawaki*

Main category: cs.CL

TL;DR: The paper evaluates the limitations of current AI text detection methods and proposes a supervised contrastive learning (SCL) framework to address brittleness under distribution shifts, unseen generators, and stylistic perturbations.


<details>
  <summary>Details</summary>
Motivation: Growing adoption of large language models (LLMs) increases the need for robust AI text detection, but current methods often fail outside predefined benchmarks.

Method: The paper systematically evaluates training-free and supervised detection paradigms and introduces a supervised contrastive learning (SCL) framework that learns discriminative style embeddings.

Result: Supervised detectors are effective in-domain but degrade significantly out-of-domain, while training-free methods are highly sensitive to proxy choices. Results highlight challenges in designing domain-agnostic detectors.

Conclusion: Fundamental challenges in AI text detection require innovative frameworks, like SCL, to enhance robustness against stylistic and domain shifts.

Abstract: The rapid adoption of LLMs has increased the need for reliable AI text detection, yet existing detectors often fail outside controlled benchmarks. We systematically evaluate 2 dominant paradigms (training-free and supervised) and show that both are brittle under distribution shift, unseen generators, and simple stylistic perturbations. To address these limitations, we propose a supervised contrastive learning (SCL) framework that learns discriminative style embeddings. Experiments show that while supervised detectors excel in-domain, they degrade sharply out-of-domain, and training-free methods remain highly sensitive to proxy choice. Overall, our results expose fundamental challenges in building domain-agnostic detectors. Our code is available at: https://github.com/HARSHITJAIS14/DetectAI

</details>


### [18] [ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation](https://arxiv.org/abs/2601.15330)
*Zhebo Wang,Xiaohu Mu,Zijie Zhou,Mohan Li,Wenpeng Xing,Dezhang Kong,Meng Han*

Main category: cs.CL

TL;DR: The paper addresses a conversational flaw in Large Language Models (LLMs) where they struggle to recover from initial ambiguous inputs and proposes a new training framework, ICPO, to improve interactions.


<details>
  <summary>Details</summary>
Motivation: LLMs often fail in multi-turn conversations due to difficulties in managing ambiguous or incorrect initial prompts under current training techniques.

Method: The ICPO framework trains LLMs using prompts that highlight ambiguity and rewards models for asking clarifying information or expressing uncertainty, based on user intent.

Result: ICPO showed an average improvement of 75% in handling multi-turn conversations while maintaining performance in single-turn tasks.

Conclusion: ICPO advances conversational AI by encouraging thoughtful interactions, reducing overconfidence, and improving collaborative dialogue capabilities.

Abstract: Large Language Models (LLMs) in multi-turn conversations often suffer from a ``lost-in-conversation'' phenomenon, where they struggle to recover from early incorrect assumptions, particularly when users provide ambiguous initial instructions. We find that standard post-training techniques like Reinforcement Learning with Verifiable Rewards (RLVR) exacerbate this issue by rewarding confident, direct answers, thereby inducing overconfidence and discouraging the model from seeking clarification. To address this, we propose Illocution-Calibrated Policy Optimization (ICPO), a novel training framework that sensitizes the model to instruction ambiguity. ICPO augments the training corpus with underspecified prompts and conditions the reward signal on the user's illocutionary intent, rewarding the model for expressing uncertainty or asking for clarification when faced with ambiguity. Experiments demonstrate that ICPO fosters appropriate humility, yielding a substantial average improvement of 75\% in multi-turn conversation, while preserving robust performance on single-turn benchmarks. Our work presents a practical path toward more robust and collaborative conversational AI that can better navigate the nuances of human interaction.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [19] [AI-Based Culvert-Sewer Inspection](https://arxiv.org/abs/2601.15366)
*Christina Thrainer*

Main category: cs.CV

TL;DR: The paper explores automated defect segmentation in culverts and sewer pipes using limited annotated data, proposing methods to enhance segmentation performance by improving training data or modifying model architectures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of identifying structural defects in culverts and sewer pipes due to the scarcity of annotated data, which poses risks to public safety and the environment.

Method: Three approaches were used: preprocessing strategies (data augmentation and dynamic label injection), a novel architecture (FORTRESS) combining advanced computational techniques, and application of few-shot semantic segmentation using bidirectional prototypical networks with attention mechanisms.

Result: The proposed approaches improved defect segmentation metrics (IoU and F1 scores), achieved state-of-the-art performance with reduced computational costs, and demonstrated effectiveness in data-scarce environments.

Conclusion: The paper concludes that enhancing training data and modifying architectures can significantly improve defect segmentation in culverts and sewer pipes, even with limited annotation data.

Abstract: Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.
  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.

</details>


### [20] [Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition](https://arxiv.org/abs/2601.15406)
*Hatef Otroshi Shahreza,Anjith George,SÃ©bastien Marcel*

Main category: cs.CV

TL;DR: This paper evaluates the performance of state-of-the-art Multimodal Large Language Models (MLLMs) in heterogeneous face recognition (HFR) and identifies significant performance gaps compared to classical face recognition systems under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore the potential of MLLMs in biometric applications, particularly for heterogeneous face recognition across different sensing modalities.

Method: The authors conducted systematic evaluations of MLLMs across various cross-modality scenarios (e.g., VIS-NIR, VIS-SWIR, VIS-THERMAL) using biometric protocols and performance metrics such as Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR).

Result: Despite advances in MLLMs, the results showed that their performance lags behind traditional face recognition systems, particularly under cross-spectral conditions.

Conclusion: The study concludes that current MLLMs exhibit limitations for heterogeneous face recognition tasks and emphasizes the need for rigorous biometric evaluations when considering their application in face recognition systems.

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.

</details>


### [21] [DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models](https://arxiv.org/abs/2601.16073)
*Hanwen Zhang,Qiaojin Shen,Yuxi Liu,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: The paper introduces DSFedMed, a dual-scale federated framework that enhances medical image segmentation using knowledge distillation between a foundation model and lightweight client models.


<details>
  <summary>Details</summary>
Motivation: To address computational demands, communication overhead, and inference costs associated with deploying foundation models in federated medical image segmentation tasks.

Method: The authors propose a dual-scale framework using mutual knowledge distillation between a centralized foundation model and client models. They employ high-quality synthetic medical images and a learnability-guided sample selection strategy to optimize efficiency.

Result: DSFedMed improves segmentation performance by 2% in Dice score while reducing communication costs and inference time by nearly 90% compared to existing methods.

Conclusion: DSFedMed demonstrates improved performance and significant efficiency, making it suitable for resource-constrained federated medical imaging applications.

Abstract: Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.

</details>


### [22] [CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation](https://arxiv.org/abs/2601.15408)
*Pablo Messina,AndrÃ©s Villa,Juan LeÃ³n AlcÃ¡zar,Karen SÃ¡nchez,Carlos Hinojosa,Denis Parra,Ãlvaro Soto,Bernard Ghanem*

Main category: cs.CV

TL;DR: CURE addresses inaccuracies in visual grounding and factual consistency in medical vision-language models by using error-aware curriculum learning without needing additional data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the weak visual-text alignment and inconsistent factual grounding in radiology report generation models.

Method: CURE employs a fine-tuning framework focusing on error-aware curriculum learning through three phases: phrase grounding, grounded report generation, and anatomy-grounded report generation.

Result: The framework boosts grounding accuracy (+0.37 IoU), report quality (+0.188 CXRFEScore), and reduces hallucinations by 18.6%.

Conclusion: CURE improves both visual grounding and the reliability of radiology reports efficiently, providing a notable advancement in automated report generation leveraging public datasets.

Abstract: Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure

</details>


### [23] [DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction](https://arxiv.org/abs/2601.15416)
*Cuong Tran Van,Trong-Thang Pham,Ngoc-Son Nguyen,Duy Minh Ho Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: DuFal is a new framework that improves sparse-view Cone-Beam Computed Tomography reconstruction by integrating frequency and spatial processing, significantly outperforming existing methods in preserving high-frequency anatomical details.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the difficulty of reconstructing fine-grained anatomical details in sparse-view CT imaging, as traditional CNNs focus on low-frequency data and struggle with high-frequency details.

Method: The proposed method, DuFal, uses a dual-path architecture combining spatial and frequency-domain processing with innovations like the High-Local Factorized Fourier Neural Operator, Spectral-Channel Factorization, and a Cross-Attention Frequency Fusion module.

Result: The method shows superior performance in retaining high-frequency features compared to state-of-the-art methods, validated by experiments on LUNA16 and ToothFairy datasets under sparse-view conditions.

Conclusion: DuFal effectively improves sparse-view CT reconstruction by addressing the shortcomings of conventional methods and integrating advanced frequency and spatial strategies.

Abstract: Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.

</details>


### [24] [Class Confidence Aware Reweighting for Long Tailed Learning](https://arxiv.org/abs/2601.15924)
*Brainard Philemon Jagati,Jitendra Tembhurne,Harsh Goud,Rudra Pratap Singh,Chandrashekhar Meshram*

Main category: cs.CV

TL;DR: This paper introduces a class and confidence-aware loss re-weighting method for addressing imbalances in long-tailed distributions, showing improved performance across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Long-tailed data distributions worsen the performance of deep neural networks due to head classes dominating the training data and tail classes having limited examples. There is limited focus on leveraging confidence and class-frequency differences during optimization.

Method: The authors propose a novel loss-level re-weighting scheme using the Î©(p_t, f_c) function. This function modulates contributions during training based on confidence values and relative class frequency, complementing existing logit adjustment methods.

Result: Results from experiments on datasets such as CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 validate the method's effectiveness, showing improvements under various imbalance conditions.

Conclusion: The proposed class and confidence-aware re-weighting scheme effectively addresses class imbalances, providing complementary benefits to logit-based methods and achieving notable performance improvements in long-tailed learning scenarios.

Abstract: Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an Î©(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.

</details>


### [25] [DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection](https://arxiv.org/abs/2601.15453)
*Morteza Poudineh,Marc Lalonde*

Main category: cs.CV

TL;DR: This paper tackles the Few-Normal Shot Anomaly Detection (FNSAD) problem by proposing a deviation-guided prompt learning framework that combines vision-language models and statistical scoring, achieving superior performance in pixel-level anomaly detection.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of limited supervision and diverse defects in anomaly detection with only a few normal samples, and improve the weak discriminability of existing vision-language prompt learning methods.

Method: The proposed approach involves replacing fixed prompt prefixes with learnable vectors, using class-aware prompts for alignment, and introducing a deviation loss with Top-K Multiple Instance Learning (MIL) for patch-level anomaly scoring based on statistical deviations.

Result: Experimental evaluation on MVTecAD and VISA datasets demonstrated superior performance in detecting anomalies at the pixel level, outperforming PromptAD and other baseline models.

Conclusion: Integrating learnable prompts with deviation-based scoring and Top-K MIL effectively improves anomaly detection performance, providing better localization, interpretability, and discriminability.

Abstract: Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.

</details>


### [26] [Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events](https://arxiv.org/abs/2601.15475)
*Yunshan Qi,Lin Zhu,Nan Bao,Yifan Zhao,Jia Li*

Main category: cs.CV

TL;DR: The paper proposes a NeRF-based framework for creating sharp HDR novel view synthesis from single-exposure blurry LDR images and event data, overcoming sensor-physics mismatches and achieving impressive results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving sharp and HDR 3D novel view synthesis from blurry, low dynamic range (LDR) images with extreme lighting, a common issue in real-world scenarios, while also improving upon existing methods that inadequately handle mismatches between sensor outputs and physical scene radiances.

Method: The authors propose a sensor-physics grounded NeRF framework that models HDR 3D scene dynamics and incorporates two alignment techniques: a pixel-wise RGB mapping field to align rendered HDR values with input LDR images, and an event mapping field to bridge scene dynamics with outputs from event sensors. Both mapping fields are jointly optimized using spatial and temporal data.

Result: The proposed method demonstrates significant improvements in HDR novel view synthesis, producing sharp deblurring results on both public and collected datasets, outperforming prior approaches.

Conclusion: By grounding NeRF in sensor physics and leveraging event data, the paper showcases a robust solution for achieving high-quality, sharp HDR and 3D scene representations from single-exposure blurry images.

Abstract: Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.

</details>


### [27] [Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis](https://arxiv.org/abs/2601.15490)
*Jobeal Solomon,Ali Mohammed Mansoor Alsahag,Seyed Sahand Mohammadi Ziabari*

Main category: cs.CV

TL;DR: This paper evaluates using Vision Transformer backbones instead of U-Net convolutional encoders in chest X-ray classifiers to mitigate demographic bias while preserving diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: To address bias in chest X-ray classifiers caused by demographic shortcuts such as sex- and age-related attributes, which lead to underdiagnosis in minority groups.

Method: A Vision Transformer (ViT) backbone was implemented in the Attribute-Neutral Framework and trained on the ChestX-ray14 dataset, assessing its performance in reducing attribute leakage and maintaining diagnostic accuracy.

Result: The ViT neutralizer reduced demographic attribute leakage more effectively than the original framework's U-Net encoder, with reduced patient sex-recognition AUC and similar diagnostic accuracy metrics.

Conclusion: Vision Transformer backbones can reduce bias in chest X-ray classifiers more effectively than convolutional encoders, presenting a fairer alternative without compromising clinical utility.

Abstract: Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.

</details>


### [28] [Controllable Layered Image Generation for Real-World Editing](https://arxiv.org/abs/2601.15507)
*Jinrui Yang,Qing Liu,Yijun Li,Mengwei Ren,Letian Zhang,Zhe Lin,Cihang Xie,Yuyin Zhou*

Main category: cs.CV

TL;DR: The paper introduces LASAGNA, a novel framework for layered image generation with controllable editing and realistic visual effects, along with new datasets and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in image generation models, consistent and controllable edits for specific image elements remain challenging, especially for layered representations with realistic effects like shadows and reflections.

Method: The authors propose LASAGNA, a framework that simultaneously generates images and their composing layers, with inputs including text prompts, mask locations, and foreground/background content, utilizing a new dataset and benchmarking tool.

Result: LASAGNA efficiently produces photorealistic backgrounds and transparent foregrounds with coherent compositions and realistic effects, significantly improving controllability and usability for real-world applications.

Conclusion: LASAGNA enhances image editing by providing controllable, consistent layered results and sets a foundation for future research with its novel dataset (LASAGNA-48K) and benchmark (LASAGNABENCH).

Abstract: Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [29] [Securing LLM-as-a-Service for Small Businesses: An Industry Case Study of a Distributed Chatbot Deployment Platform](https://arxiv.org/abs/2601.15528)
*Jiazhu Xie,Bowen Li,Heyu Fu,Chong Gao,Ziqi Xu,Fengling Han*

Main category: cs.DC

TL;DR: This paper presents a case study of an open-source platform allowing small businesses to deploy customized LLM-based chatbots efficiently and securely, addressing cost, complexity, and security challenges.


<details>
  <summary>Details</summary>
Motivation: Small businesses face difficulties in deploying LLM-based question-answering systems due to infrastructural costs, engineering challenges, and security concerns, especially in RAG-based scenarios.

Method: An open-source, no-code multi-tenant platform is developed using lightweight distributed k3s clusters on low-cost machines with encrypted networking, container isolation, and tenant-specific data controls. The platform also incorporates defenses against prompt injection attacks without requiring model retraining.

Result: The platform was evaluated in an e-commerce setting, demonstrating secure, cost-efficient chatbot services suitable for small businesses' operational and security constraints.

Conclusion: Customized LLM-based chatbot services can be achieved for small businesses by leveraging lightweight infrastructure, enhanced security mechanisms, and simplified workflows.

Abstract: Large Language Model (LLM)-based question-answering systems offer significant potential for automating customer support and internal knowledge access in small businesses, yet their practical deployment remains challenging due to infrastructure costs, engineering complexity, and security risks, particularly in retrieval-augmented generation (RAG)-based settings. This paper presents an industry case study of an open-source, multi-tenant platform that enables small businesses to deploy customised LLM-based support chatbots via a no-code workflow. The platform is built on distributed, lightweight k3s clusters spanning heterogeneous, low-cost machines and interconnected through an encrypted overlay network, enabling cost-efficient resource pooling while enforcing container-based isolation and per-tenant data access controls. In addition, the platform integrates practical, platform-level defences against prompt injection attacks in RAG-based chatbots, translating insights from recent prompt injection research into deployable security mechanisms without requiring model retraining or enterprise-scale infrastructure. We evaluate the proposed platform through a real-world e-commerce deployment, demonstrating that secure and efficient LLM-based chatbot services can be achieved under realistic cost, operational, and security constraints faced by small businesses.

</details>


### [30] [Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search](https://arxiv.org/abs/2601.15633)
*Enzo Meneses,Hugo Bec,CristÃ³bal A. Navarroa,BenoÃ®t Crespin,Felipe A. Quezada,Nancy Hitschfeld,Heinich Porro,Maxime Maria*

Main category: cs.DC

TL;DR: The paper introduces three methods to improve particle FRNN physics simulations using RT Cores, including a BVH update optimizer, a neighbor-list-free RT Core approach, and periodic boundary condition compatibility, achieving notable speed and efficiency gains.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance the performance, scalability, and adaptability of particle FRNN physics simulations on RT Cores in the context of intensive computational challenges like bounding volume hierarchy management and periodic boundary conditions.

Method: The authors introduce (i) an optimizer for BVH update/rebuild ratios, (ii) two variants of an RT core method eliminating the need for neighbor lists, and (iii) an RT core-based technique supporting periodic boundary conditions.

Result: Experiments using the Lennard-Jones FRNN model showed the BVH optimizer achieved up to 3.4x speed improvements, while the neighbor-list-free RT Core variants increased speedup and energy efficiency by up to 2.0x and enabled memory-intensive cases. The periodic boundary condition technique performed effectively without significant overhead.

Conclusion: The proposed methods effectively improve the speed, efficiency, and memory handling of particle FRNN simulations on RT Cores. They also provide insights into scenarios favoring RT Cores versus regular GPU computation, underlining their strengths and limitations.

Abstract: In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\sim 3.4\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\sim1.3\times$ at small radius to $\sim2.0\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Empowering LLMs for Structure-Based Drug Design via Exploration-Augmented Latent Inference](https://arxiv.org/abs/2601.15333)
*Xuanning Hu,Anchen Li,Qianli Xing,Jinglong Ji,Hao Tuo,Bo Yang*

Main category: cs.LG

TL;DR: The paper proposes ELILLM framework to enhance LLMs capabilities for Structure-based Drug Design by systematic exploration in latent space and knowledge-guided decoding.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of LLMs in understanding protein structures and generating molecules for Structure-based Drug Design.

Method: ELILLM redefines LLM generation as encoding, exploring latent embedding space using Bayesian optimization, and decoding with chemical validity constraints.

Result: ELILLM demonstrates strong controlled exploration and high binding affinity scores on the CrossDocked2020 benchmark, outperforming seven baseline methods.

Conclusion: ELILLM effectively improves LLMs' application to drug design by enabling systematic search and chemically valid molecular generation.

Abstract: Large Language Models (LLMs) possess strong representation and reasoning capabilities, but their application to structure-based drug design (SBDD) is limited by insufficient understanding of protein structures and unpredictable molecular generation. To address these challenges, we propose Exploration-Augmented Latent Inference for LLMs (ELILLM), a framework that reinterprets the LLM generation process as an encoding, latent space exploration, and decoding workflow. ELILLM explicitly explores portions of the design problem beyond the model's current knowledge while using a decoding module to handle familiar regions, generating chemically valid and synthetically reasonable molecules. In our implementation, Bayesian optimization guides the systematic exploration of latent embeddings, and a position-aware surrogate model efficiently predicts binding affinity distributions to inform the search. Knowledge-guided decoding further reduces randomness and effectively imposes chemical validity constraints. We demonstrate ELILLM on the CrossDocked2020 benchmark, showing strong controlled exploration and high binding affinity scores compared with seven baseline methods. These results demonstrate that ELILLM can effectively enhance LLMs capabilities for SBDD.

</details>


### [32] [Language Models Entangle Language and Culture](https://arxiv.org/abs/2601.15337)
*Shourya Jain,Paras Chopra*

Main category: cs.LG

TL;DR: This paper investigates if LLMs provide fair responses across different languages, finding that lower quality answers are given in low-resource languages and exploring how language affects cultural context in responses.


<details>
  <summary>Details</summary>
Motivation: To ensure users interacting with LLMs receive responses of similar quality regardless of the language they use and to understand the entanglement of language and cultural context in LLM outputs.

Method: The researchers analyzed responses to open-ended real-world questions from the WildChat dataset, employed LLM-as-a-Judge to analyze cultural context in responses, and evaluated LLMs using a translated subset of the CulturalBench benchmark across multiple languages.

Result: LLMs often deliver lower quality responses for low-resource languages and language choice significantly impacts cultural context used in answers.

Conclusion: Language can disadvantage users in LLM applications due to varying answer quality and cultural context, suggesting the need for improvements in multilingual language model fairness and cultural understanding.

Abstract: Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.

</details>


### [33] [Improving MoE Compute Efficiency by Composing Weight and Data Sparsity](https://arxiv.org/abs/2601.15370)
*Maciej Kilian,Oleg Mkrtchyan,Luke Zettlemoyer,Akshat Shrivastava,Armen Aghajanyan*

Main category: cs.LG

TL;DR: The paper introduces a novel method for enhancing computational efficiency in Mixture-of-Experts models by combining weight sparsity and data sparsity using null experts, ensuring causality in autoregressive models and achieving better training loss and downstream performance.


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE) layers aim to optimize compute efficiency by processing only subsets of tokens, but autoregressive models face causality issues with traditional expert-choice routing methods.

Method: The paper proposes causal token-choice MoE routing integrated with null experts, which serve to ensure data sparsity without consuming compute and uphold autoregressive causality.

Result: The approach achieved improved compute efficiency and performance compared to weight sparsity alone, showing gains on training loss and downstream tasks, while implicitly allocating tokens based on modality.

Conclusion: The study demonstrates that combining weight and data sparsity leads to more efficient and effective model training, especially in heterogeneous data contexts like vision-language tasks, without requiring explicit modality routing.

Abstract: Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.

</details>


### [34] [You Need Better Attention Priors](https://arxiv.org/abs/2601.15380)
*Elon Litman,Gabe Guo*

Main category: cs.LG

TL;DR: This paper proposes GOAT, a generalized attention mechanism using Entropic Optimal Transport and trainable priors, improving upon standard attention by addressing limitations like attention sinks.


<details>
  <summary>Details</summary>
Motivation: To address limitations of standard attention mechanisms, such as their naive assumptions on the implicit uniform prior and issues with representational trade-offs, while retaining compatibility with optimized computation.

Method: Introduce Generalized Optimal Transport Attention (GOAT), incorporating trainable, continuous priors into the attention mechanism, integrating spatial information for improved extrapolation.

Result: The proposed GOAT mechanism avoids representational trade-offs, provides solutions for attention sink issues, and combines flexibility of learned embeddings with generalization abilities of fixed encodings.

Conclusion: GOAT enhances attention mechanisms by optimizing prior assumptions and incorporating spatial information, offering improved performance and addressing common attention-related issues.

Abstract: We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.

</details>


### [35] [FedUMM: A General Framework for Federated Learning with Unified Multimodal Models](https://arxiv.org/abs/2601.15390)
*Zhaolong Su,Leheng Zhao,Xiaoying Wu,Ziyue Xu,Jindong Wang*

Main category: cs.LG

TL;DR: This paper introduces FedUMM, a federated learning framework for unified multimodal models (UMMs) to enable efficient training in privacy-sensitive and distributed scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of UMMs, which are typically trained in centralized settings, making them unsuitable for scenarios with privacy concerns or geographically distributed data.

Method: The proposed FedUMM framework uses parameter-efficient fine-tuning where clients train lightweight LoRA adapters, freezing foundation model parameters. Updates are aggregated at the server to optimize communication efficiency.

Result: Experiments on VQA v2 and the GenEval benchmarks under non-IID data scenarios demonstrate that model performance remains competitive with centralized training despite increasing heterogeneity and client counts. Adapter-only federation significantly reduces communication costs by over an order of magnitude.

Conclusion: The study shows that the FedUMM framework is effective for privacy-preserving, federated training of UMMs, making it a practical solution for distributed and sensitive data scenarios.

Abstract: Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datasets are gathered in a central server, limiting the deployment in privacy-sensitive and geographically distributed scenarios. In this paper, we present FedUMM, a general federated learning framework for UMMs under non-IID multimodal data with low communication cost. Built on NVIDIA FLARE, FedUMM instantiates federation for a BLIP3o backbone via parameter-efficient fine-tuning: clients train lightweight LoRA adapters while freezing the foundation models, and the server aggregates only adapter updates. We evaluate on VQA v2 and the GenEval compositional generation benchmarks under Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight degradation as client count and heterogeneity increase, while remaining competitive with centralized training. We further analyze computation--communication trade-offs and demonstrate that adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning, enabling practical federated UMM training. This work provides empirical experience for future research on privacy-preserving federated unified multimodal models.

</details>


### [36] [Attention-Informed Surrogates for Navigating Power-Performance Trade-offs in HPC](https://arxiv.org/abs/2601.15399)
*Ashna Nawar Ahmed,Banooqa Banday,Terry Jones,Tanzima Z. Islam*

Main category: cs.LG

TL;DR: The paper presents a framework using embedding-informed surrogate models and Bayesian optimization to improve HPC job scheduling by balancing runtime and power concerns.


<details>
  <summary>Details</summary>
Motivation: To address challenges in HPC scheduling, optimizing both performance and power, while maintaining data efficiency and stability.

Method: The authors propose a surrogate-assisted multi-objective Bayesian optimization framework utilizing attention-based embeddings of job telemetry and an intelligent sampling strategy.

Result: The approach outperformed baselines by identifying superior runtime-power trade-offs and reducing training costs while enhancing stability in results.

Conclusion: This is the first application of embedding-informed surrogates within MOBO for HPC scheduling, showing significant promise in optimizing production workloads.

Abstract: High-Performance Computing (HPC) schedulers must balance user performance with facility-wide resource constraints. The task boils down to selecting the optimal number of nodes for a given job. We present a surrogate-assisted multi-objective Bayesian optimization (MOBO) framework to automate this complex decision. Our core hypothesis is that surrogate models informed by attention-based embeddings of job telemetry can capture performance dynamics more effectively than standard regression techniques. We pair this with an intelligent sample acquisition strategy to ensure the approach is data-efficient. On two production HPC datasets, our embedding-informed method consistently identified higher-quality Pareto fronts of runtime-power trade-offs compared to baselines. Furthermore, our intelligent data sampling strategy drastically reduced training costs while improving the stability of the results. To our knowledge, this is the first work to successfully apply embedding-informed surrogates in a MOBO framework to the HPC scheduling problem, jointly optimizing for performance and power on production workloads.

</details>


### [37] [Ambient Dataloops: Generative Models for Dataset Refinement](https://arxiv.org/abs/2601.15417)
*AdriÃ¡n RodrÃ­guez-MuÃ±oz,William Daspit,Adam Klivans,Antonio Torralba,Constantinos Daskalakis,Giannis Daras*

Main category: cs.LG

TL;DR: The paper introduces Ambient Dataloops, an iterative method for refining datasets to enhance the performance of diffusion models by progressively improving data quality and model capabilities.


<details>
  <summary>Details</summary>
Motivation: To address challenges in training models on heterogeneous datasets, which often include samples of varying quality, leading to suboptimal model performance.

Method: The approach involves a dataset-model co-evolution process where the dataset quality and model simultaneously improve. At each iteration, noisy samples get slightly less noisy and improved through Ambient Diffusion techniques.

Result: Ambient Dataloops achieve state-of-the-art results in image generation (unconditional and text-conditional) as well as protein design.

Conclusion: The proposed iterative framework effectively enhances both datasets and models, offering theoretical justification and achieving strong performance across diverse applications.

Abstract: We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [38] [Ternary Spiking Neural Networks Enhanced by Complemented Neurons and Membrane Potential Aggregation](https://arxiv.org/abs/2601.15598)
*Boxuan Zhang,Jiaxin Wang,Zhen Xu,Kuan Tao*

Main category: cs.NE

TL;DR: This paper proposes the Complemented Ternary Spiking Neuron (CTSN) model and Temporal Membrane Potential Regularization (TMPR) method to address limitations in existing ternary spiking neural networks, achieving performance improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the biological plausibility and information capacity of spiking neurons while addressing issues like information loss, gradient vanishing, and irregular membrane potential distributions in ternary spiking neuron models.

Method: The authors develop CTSN, a ternary spiking neuron model with a learnable complementary term for storing historical input information. Additionally, TMPR, a novel training method based on the temporal evolution of membrane potentials, is introduced to enhance training through time-varying regularization.

Result: Through extensive experiments on various datasets, the proposed CTSN and TMPR methods achieved significant performance improvements compared to existing models.

Conclusion: The study successfully enhances the biological plausibility, adaptability, and training efficiency of spiking neural networks, validating the effectiveness of CTSN and TMPR in improving both neuron dynamics and overall model performance.

Abstract: Spiking Neural Networks (SNNs) are promising energy-efficient models and powerful framworks of modeling neuron dynamics. However, existing binary spiking neurons exhibit limited biological plausibilities and low information capacity. Recently developed ternary spiking neuron possesses higher consistency with biological principles (i.e. excitation-inhibition balance mechanism). Despite of this, the ternary spiking neuron suffers from defects including iterative information loss, temporal gradient vanishing and irregular distributions of membrane potentials. To address these issues, we propose Complemented Ternary Spiking Neuron (CTSN), a novel ternary spiking neuron model that incorporates an learnable complemental term to store information from historical inputs. CTSN effectively improves the deficiencies of ternary spiking neuron, while the embedded learnable factors enable CTSN to adaptively adjust neuron dynamics, providing strong neural heterogeneity. Furthermore, based on the temporal evolution features of ternary spiking neurons' membrane potential distributions, we propose the Temporal Membrane Potential Regularization (TMPR) training method. TMPR introduces time-varying regularization strategy utilizing membrane potentials, furhter enhancing the training process by creating extra backpropagation paths. We validate our methods through extensive experiments on various datasets, demonstrating remarkable performance advances.

</details>


### [39] [LLM-Assisted Automatic Dispatching Rule Design for Dynamic Flexible Assembly Flow Shop Scheduling](https://arxiv.org/abs/2601.15738)
*Junhao Qiu,Haoyang Zhuang,Fei Liu,Jianjun Liu,Qingfu Zhang*

Main category: cs.NE

TL;DR: The study introduces LLM4DRD, a framework utilizing an LLM to design dynamic scheduling rules for multi-stage, multi-product systems, achieving superior scheduling performance compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in coordinating dynamic scheduling and supply decisions in multi-product processing and assembly systems due to hierarchical constraints and evolving uncertainties.

Method: LLM4DRD employs a heterogeneous graph to transform supply decisions and uses an elite initialization mechanism, dual-expert interaction (LLM-A for rule generation, LLM-S for evaluation), along with dynamic feature-fitting and hybrid evaluations to evolve adaptive scheduling rules.

Result: Experiments show LLM4DRD outperforms existing methods, achieving 3.17-12.39% better average tardiness across test cases and a 11.10% improvement in diverse scenarios with robust performance.

Conclusion: LLM4DRD demonstrates its capability to generate generalizable scheduling rules efficiently, offering a promising solution for complex dynamic scheduling problems in hybrid manufacturing environments.

Abstract: Dynamic multi-product delivery environments demand rapid coordination of part completion and product-level kitting within hybrid processing and assembly systems to satisfy strict hierarchical supply constraints. The flexible assembly flow shop scheduling problem formally defines dependencies for multi-stage kitting, yet dynamic variants make designing integrated scheduling rules under multi-level time coupling highly challenging. Existing automated heuristic design methods, particularly genetic programming constrained to fixed terminal symbol sets, struggle to capture and leverage dynamic uncertainties and hierarchical dependency information under transient decision states. This study develops an LLM-assisted Dynamic Rule Design framework (LLM4DRD) that automatically evolves integrated online scheduling rules adapted to scheduling features. Firstly, multi-stage processing and assembly supply decisions are transformed into feasible directed edge orderings based on heterogeneous graph. Then, an elite knowledge guided initialization embeds advanced design expertise into initial rules to enhance initial quality. Additionally, a dual-expert mechanism is introduced in which LLM-A evolutionary code to generate candidate rules and LLM-S conducts scheduling evaluation, while dynamic feature-fitting rule evolution combined with hybrid evaluation enables continuous improvement and extracts adaptive rules with strong generalization capability. A series of experiments are conducted to validate the effectiveness of the method. The average tardiness of LLM4DRD is 3.17-12.39% higher than state-of-the-art methods in 20 practical instances used for training and testing, respectively. In 24 scenarios with different resource configurations, order loads, and disturbance levels totaling 480 instances, it achieves 11.10% higher performance than the second best competitor, exhibiting excellent robustness.

</details>


### [40] [Neural Particle Automata: Learning Self-Organizing Particle Dynamics](https://arxiv.org/abs/2601.16096)
*Hyunsoo Kim,Ehsan Pajouheshgar,Sabine SÃ¼sstrunk,Wenzel Jakob,Jinah Park*

Main category: cs.NE

TL;DR: The work introduces Neural Particle Automata (NPA), extending Neural Cellular Automata (NCA) from static grids to dynamic particle systems for modeling continuous particle dynamics.


<details>
  <summary>Details</summary>
Motivation: To address limitations of Neural Cellular Automata in modeling dynamic systems such as particle-based scenarios and provide a scalable framework for learning self-organizing behaviors in such settings.

Method: The authors propose a Lagrangian formalism for NCA by representing cells as particles with continuous positions and internal states, updated using neural network rules. They integrate Smoothed Particle Hydrodynamics (SPH) for differentiable local interactions, made computationally efficient via CUDA-accelerated kernels.

Result: The method successfully demonstrates NPAâ€™s capabilities across diverse tasks like morphogenesis, point-cloud classification, and texture synthesis while preserving core NCA properties like robustness and adding new capabilities specific to particle systems.

Conclusion: NPA serves as a compact, efficient neural model, advancing the field of self-organizing particle dynamics and extending NCA functionality to dynamic, heterogeneous systems.

Abstract: We introduce Neural Particle Automata (NPA), a Lagrangian generalization of Neural Cellular Automata (NCA) from static lattices to dynamic particle systems. Unlike classical Eulerian NCA where cells are pinned to pixels or voxels, NPA model each cell as a particle with a continuous position and internal state, both updated by a shared, learnable neural rule. This particle-based formulation yields clear individuation of cells, allows heterogeneous dynamics, and concentrates computation only on regions where activity is present. At the same time, particle systems pose challenges: neighborhoods are dynamic, and a naive implementation of local interactions scale quadratically with the number of particles. We address these challenges by replacing grid-based neighborhood perception with differentiable Smoothed Particle Hydrodynamics (SPH) operators backed by memory-efficient, CUDA-accelerated kernels, enabling scalable end-to-end training. Across tasks including morphogenesis, point-cloud classification, and particle-based texture synthesis, we show that NPA retain key NCA behaviors such as robustness and self-regeneration, while enabling new behaviors specific to particle systems. Together, these results position NPA as a compact neural model for learning self-organizing particle dynamics.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [41] [Sawtooth Wavefront Reordering: Enhanced CuTile FlashAttention on NVIDIA GB10](https://arxiv.org/abs/2601.16032)
*Yifan Zhu,Yekai Pan,Chen Ding*

Main category: cs.PF

TL;DR: The paper proposes a new technique, Sawtooth Wavefront Reordering, to improve cache performance in CuTile-based Flash Attention on NVIDIA GB10 architecture, achieving up to 60% higher throughput.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address performance limitations caused by L2 cache misses in CuTile-based Flash Attention, which are critical for Large Language Models.

Method: The authors analyze the memory behavior on NVIDIA GB10 architecture to identify L2 cache miss causes and introduce Sawtooth Wavefront Reordering programming technique to optimize cache interactions.

Result: The proposed technique reduces L2 cache misses by 50% or more and improves throughput by up to 60% in both CUDA and CuTile implementations.

Conclusion: Sawtooth Wavefront Reordering demonstrates significant improvements in cache performance and throughput, making it suitable for optimizing Flash Attention in Large Language Models on GB10 architecture.

Abstract: High-performance attention kernels are essential for Large Language Models. This paper presents analysis of CuTile-based Flash Attention memory behavior and a technique to improve its cache performance. In particular, our analysis on the NVIDIA GB10 (Grace Blackwell) identifies the main cause of L2 cache miss. Leveraging this insight, we introduce a new programming technique called Sawtooth Wavefront Reordering that reduces L2 misses. We validate it in both CUDA and CuTile, observing 50\% or greater reduction in L2 misses and up to 60\% increase in throughput on GB10.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [42] [Remarks on Algebraic Reconstruction of Types and Effects](https://arxiv.org/abs/2601.15455)
*Patrycja Balik,Szymon JÄ™dras,Piotr Polesiuk*

Main category: cs.PL

TL;DR: The paper revisits Pierre Jouvelot and David Gifford's 1991 work on a type-and-effect reconstruction algorithm, identifying subtle variable binding bugs in their higher-rank polymorphism approach.


<details>
  <summary>Details</summary>
Motivation: To examine and critique the original 1991 type-and-effect reconstruction algorithm, especially focusing on its challenges related to higher-rank polymorphism and identifying any issues.

Method: Analyzes the original type system and reconstruction algorithm by Jouvelot and Gifford, specifically investigating variable binding problems in handling higher-rank polymorphism.

Result: The authors discover and describe subtle bugs related to variable binding in the original algorithm, adding insights into its limitations.

Conclusion: While Jouvelot and Gifford's work is foundational, their handling of higher-rank polymorphism contains subtle implementation issues that require careful attention in future improvements.

Abstract: In their 1991 paper "Algebraic Reconstruction of Types and Effects," Pierre Jouvelot and David Gifford presented a type-and-effect reconstruction algorithm based on an algebraic structure of effects. Their work is considered a milestone in the development of type-and-effect systems, and has inspired numerous subsequent works in the area of static analysis. However, unlike the later research it spawned, the original algorithm considered a language with higher-rank polymorphism, a feature which is challenging to implement correctly. In this note, we identify subtle bugs related to variable binding in their approach to this feature. We revisit their type system and reconstruction algorithm, and describe the discovered issues.

</details>


### [43] [Prioritizing Configuration Relevance via Compiler-Based Refined Feature Ranking](https://arxiv.org/abs/2601.16008)
*Federico Bruzzone,Walter Cazzola,Luca Favini*

Main category: cs.PL

TL;DR: The paper introduces RustyEx, a tool leveraging graph-based analysis and centrality measures to prioritize and generate valid configurations for Rust programs, addressing configuration explosion challenges.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome challenges in exploring the combinatorial explosion of software configurations in Rust, which hampers program analysis, optimization, and testing.

Method: The authors extract intermediate representations, create graph-based structures, rank features using centrality measures, refine rankings based on code impact, and validate configurations using a SAT solver.

Result: RustyEx effectively generates specified configurations for open source Rust projects within bounded resources, ensuring soundness and practicality.

Conclusion: Centrality-guided prioritization facilitates efficient exploration of large configuration spaces, offering a foundation for future research on configuration-aware software analysis.

Abstract: Modern programming languages, most notably Rust, offer advanced linguistic constructs for building highly configurable software systems as aggregation of features -- identified by a configuration. However, they pose substantial challenges for program analysis, optimization, and testing, as the combinatorial explosion of configurations often makes exhaustive exploration infeasible. In this manuscript, we present the first compiler-based method for prioritizing configurations. Our approach consists of four main steps: 1. extracting a tailored intermediate representation from the Rust compiler, 2. constructing two complementary graph-based data structures, 3. using centrality measures to rank features, and 4. refining the ranking by considering the extent of code they impact. A fixed number of most relevant configurations are generated based on the achieved feature ranking. The validity of the generated configurations is guaranteed by using a SAT solver that takes a representation of this graph in conjunctive normal form. We formalized this approach and implemented it in a prototype, RustyEx, by instrumenting the Rust compiler. An empirical evaluation on higher-ranked open source Rust projects shows that RustyEx efficiently generates user-specified sets of configurations within bounded resources, while ensuring soundness by construction. The results demonstrate that centrality-guided configuration prioritization enables effective and practical exploration of large configuration spaces, paving the way for future research in configuration-aware analysis and optimization.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [44] [Designing Persuasive Social Robots for Health Behavior Change: A Systematic Review of Behavior Change Strategies and Evaluation Methods](https://arxiv.org/abs/2601.15309)
*Jiaxin Xu,Chao Zhang,Raymond H. Cuijpers,Wijnand A. IJsselsteijn*

Main category: cs.RO

TL;DR: The paper reviews social robots in health behavior change, identifying behavior change strategies and evaluation methods in 39 studies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limited actionable knowledge guiding the design and evaluation of social robots for health behavior change interventions.

Method: The study conducted a systematic review by analyzing 39 studies through database and manual searches to identify behavior change strategies and evaluation practices.

Result: The analysis revealed four categories of strategies: coaching, counseling, social influence, and persuasion-enhancing. Key evaluation practices, such as study designs and outcome measures, were also identified.

Conclusion: The findings emphasize the unique roles of social robots in health behavior change and offer design recommendations, proposing future research directions in HRI studies.

Abstract: Social robots are increasingly applied as health behavior change interventions, yet actionable knowledge to guide their design and evaluation remains limited. This systematic review synthesizes (1) the behavior change strategies used in existing HRI studies employing social robots to promote health behavior change, and (2) the evaluation methods applied to assess behavior change outcomes. Relevant literature was identified through systematic database searches and hand searches. Analysis of 39 studies revealed four overarching categories of behavior change strategies: coaching strategies, counseling strategies, social influence strategies, and persuasion-enhancing strategies. These strategies highlight the unique affordances of social robots as behavior change interventions and offer valuable design heuristics. The review also identified key characteristics of current evaluation practices, including study designs, settings, durations, and outcome measures, on the basis of which we propose several directions for future HRI research.

</details>


### [45] [Preparation and Motion Study of Magnetically Driven Micro Soft Robot Mimicking the Cownose Ray](https://arxiv.org/abs/2601.15349)
*Jiaqing Chang,Song Gao,Chaowei Dong,zhaobang Li,Yang Liu*

Main category: cs.RO

TL;DR: This paper proposes a magnetically driven, cownose ray-inspired micro soft robot that demonstrates efficient swimming performance using wireless power supply in underwater environments.


<details>
  <summary>Details</summary>
Motivation: To improve swimming performance and applicability of micro soft robots in narrow underwater spaces, where internal powering is challenging and flexible movement is critical.

Method: The robot is constructed using NdFeB and PDMS materials, designed based on cownose ray mechanics. Swimming experiments are conducted using a 3D Helmholtz coil to generate a harmonic magnetic field, varying magnetic field parameters to study their effects.

Result: The robot achieves its fastest swimming speed at 5 mT and 11 Hz, with a motion speed of 5.25 mm/s (0.5 body lengths/second), and demonstrates different swimming patterns like straight, turning, and directional swimming.

Conclusion: The study develops a magnetically driven micro soft robot, supporting its potential use in underwater environments such as monitoring and medical procedures, and provides insights for trajectory control through stepwise adjustments.

Abstract: In narrow, unstructured underwater environments such as environmental monitoring and minimally invasive medical procedures, micro soft robots exhibit unique advantages due to their flexible movement capabilities and small size. At the same time, applying bionic technology to the structural design of micro soft robots can significantly improve their swimming performance. However, limited by their miniaturization, these robots are difficult to power internally and usually adopt a wireless power supply method. This study designs and fabricates a magnetically responsive, cownose ray-inspired micro soft robot based on the swimming principle of the cownose ray. The robot is made of a certain proportion of NdFeB and PDMS. Then, a three-dimensional Helmholtz coil is used to generate an oscillating harmonic magnetic field to conduct swimming experiments on the robot, exploring the influence of magnetic field parameters on the robot's swimming performance. The experimental results show that the swimming speed is the fastest at B = 5 mT and f = 11 Hz, reaching 5.25 mm/s, which is about 0.5 body lengths per second. In addition, by adjusting the current direction and frequency of the coil, the robot can perform different swimming modes such as straight swimming, turning swimming, and directional swimming. By employing a stepwise adjustment method, the impact of response errors on the robot's trajectory can be effectively reduced. This study demonstrates a method for magnetically driven micro soft robots, laying a foundation for the application of wireless-driven robots in underwater narrow spaces.

</details>


### [46] [Learning a Unified Latent Space for Cross-Embodiment Robot Control](https://arxiv.org/abs/2601.15419)
*Yashuai Yan,Dongheui Lee*

Main category: cs.RO

TL;DR: Introducing a scalable framework for humanoid robot control that employs a shared latent representation to unify motion across diverse humanoid platforms for seamless motion retargeting and robust, adaptable control.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of achieving consistent and agile motion control across humanoid robots with varied morphologies and embodiments, reducing the need for robot-specific adaptions.

Method: Develop a shared latent space using contrastive learning to unify partial motion patterns across robots, create custom similarity metrics, and train goal-conditioned control policies within this latent space using human data through a conditional variational autoencoder.

Result: The trained policies achieve embodiment-agnostic control and direct deployment across multiple humanoid robots, supporting the rapid inclusion of new robots by utilizing lightweight, robot-specific embedding layers.

Conclusion: The framework provides robust, scalable, and flexible control systems for humanoid robots, facilitating enhanced alignment and transferability across diverse embodiments and reducing adaptation efforts.

Abstract: We present a scalable framework for cross-embodiment humanoid robot control by learning a shared latent representation that unifies motion across humans and diverse humanoid platforms, including single-arm, dual-arm, and legged humanoid robots. Our method proceeds in two stages: first, we construct a decoupled latent space that captures localized motion patterns across different body parts using contrastive learning, enabling accurate and flexible motion retargeting even across robots with diverse morphologies. To enhance alignment between embodiments, we introduce tailored similarity metrics that combine joint rotation and end-effector positioning for critical segments, such as arms. Then, we train a goal-conditioned control policy directly within this latent space using only human data. Leveraging a conditional variational autoencoder, our policy learns to predict latent space displacements guided by intended goal directions. We show that the trained policy can be directly deployed on multiple robots without any adaptation. Furthermore, our method supports the efficient addition of new robots to the latent space by learning only a lightweight, robot-specific embedding layer. The learned latent policies can also be directly applied to the new robots. Experimental results demonstrate that our approach enables robust, scalable, and embodiment-agnostic robot control across a wide range of humanoid platforms.

</details>


### [47] [Neural Collision Detection for Multi-arm Laparoscopy Surgical Robots Through Learning-from-Simulation](https://arxiv.org/abs/2601.15459)
*Sarvin Ghiasi,Majid Roshanfar,Jake Barralet,Liane S. Feldman,Amir Hooshiar*

Main category: cs.RO

TL;DR: This paper presents a framework integrating analytical modeling, simulation, and machine learning to enhance the safety and efficiency of robotic arms in laparoscopic surgery by addressing collision detection and minimum distance estimation.


<details>
  <summary>Details</summary>
Motivation: To address critical challenges in robotic arms' collision detection and distance estimation in laparoscopic surgery, ensuring their safe and efficient operation in intricate surgical environments.

Method: The paper combines analytical modeling for theoretical distance calculation, real-time 3D simulation of robotic arms to generate diverse datasets, and training of a deep neural network for accurate distance prediction based on robotic configurations and spatial inputs.

Result: The trained deep neural network achieved a mean absolute error of 282.2 mm and an R-squared value of 0.85 in estimating distances, demonstrating high accuracy and generalization of spatial relationships.

Conclusion: The study effectively demonstrates that combining analytical precision with advanced machine learning techniques enhances the safety, precision, and operational reliability of robotic systems in laparoscopic surgery.

Abstract: This study presents an integrated framework for enhancing the safety and operational efficiency of robotic arms in laparoscopic surgery by addressing key challenges in collision detection and minimum distance estimation. By combining analytical modeling, real-time simulation, and machine learning, the framework offers a robust solution for ensuring safe robotic operations. An analytical model was developed to estimate the minimum distances between robotic arms based on their joint configurations, offering precise theoretical calculations that serve as both a validation tool and a benchmark. To complement this, a 3D simulation environment was created to model two 7-DOF Kinova robotic arms, generating a diverse dataset of configurations for collision detection and distance estimation. Using these insights, a deep neural network model was trained with joint actuators of robot arms and relative positions as inputs, achieving a mean absolute error of 282.2 mm and an R-squared value of 0.85. The close alignment between predicted and actual distances highlights the network's accuracy and its ability to generalize spatial relationships. This work demonstrates the effectiveness of combining analytical precision with machine learning algorithms to enhance the precision and reliability of robotic systems.

</details>


### [48] [A Universal Large Language Model -- Drone Command and Control Interface](https://arxiv.org/abs/2601.15486)
*Javier N. Ramos-Silva,Peter J. Burke*

Main category: cs.RO

TL;DR: The paper introduces a universal interface for integrating large language models (LLMs) with drone command and control.


<details>
  <summary>Details</summary>
Motivation: To simplify and make efficient the integration of LLMs with drones for advanced control by overcoming current cumbersome methods.

Method: Developed an LLM and drone-agnostic interface using the new Model Context Protocol (MCP) standard with integration to the Mavlink protocol for universal control.

Result: Demonstrated real-time flight control on an actual UAV, and extensive navigation and control capabilities in a simulated environment using Google Maps for real-time data.

Conclusion: The paper establishes a new universal, versatile, and simple integration paradigm for drones and AI leveraging modern AI capabilities effectively.

Abstract: The use of artificial intelligence (AI) for drone control can have a transformative impact on drone capabilities, especially when real world information can be integrated with drone sensing, command, and control, part of a growing field of physical AI. Large language models (LLMs) can be advantageous if trained at scale on general knowledge, but especially and in particular when the training data includes information such as detailed map geography topology of the entire planet, as well as the ability to access real time situational data such as weather. However, challenges remain in the interface between drones and LLMs in general, with each application requiring a tedious, labor intensive effort to connect the LLM trained knowledge to drone command and control. Here, we solve that problem, using an interface strategy that is LLM agnostic and drone agnostic, providing the first universal, versatile, comprehensive and easy to use drone control interface. We do this using the new model context protocol (MCP) standard, an open standard that provides a universal way for AI systems to access external data, tools, and services. We develop and deploy a cloud based Linux machine hosting an MCP server that supports the Mavlink protocol, an ubiquitous drone control language used almost universally by millions of drones including Ardupilot and PX4 framework.We demonstrate flight control of a real unmanned aerial vehicle. In further testing, we demonstrate extensive flight planning and control capability in a simulated drone, integrated with a Google Maps MCP server for up to date, real time navigation information. This demonstrates a universal approach to integration of LLMs with drone command and control, a paradigm that leverages and exploits virtually all of modern AI industry with drone technology in an easy to use interface that translates natural language to drone control.

</details>


### [49] [CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation](https://arxiv.org/abs/2601.15541)
*Heng Zhang,Wei-Hsing Huang,Qiyi Tong,Gokhan Solak,Puze Liu,Sheng Liu,Jan Peters,Arash Ajoudani*

Main category: cs.RO

TL;DR: The paper introduces CompliantVLA-adaptor to enhance Vision-Language-Action models for contact-rich robotic tasks using variable impedance control informed by vision-language models.


<details>
  <summary>Details</summary>
Motivation: Existing VLA systems lack force-aware adaptation, resulting in unsafe or failed interactions during contact-rich tasks, requiring a safer and more effective robotic manipulation approach.

Method: The proposed model leverages vision-language models (VLM) to interpret task contexts and adapt impedance control parameters such as stiffness and damping, ensuring safer operations using real-time force/torque feedback.

Result: The proposed method outperformed baselines in both simulations and real hardware, with success rates improving from 9.86% to 17.29% and reduced force violations.

Conclusion: CompliantVLA-adaptor significantly improves the safety and success rates in contact-rich manipulation tasks, demonstrating its potential to advance VLA systems.

Abstract: We propose a CompliantVLA-adaptor that augments the state-of-the-art Vision-Language-Action (VLA) models with vision-language model (VLM)-informed context-aware variable impedance control (VIC) to improve the safety and effectiveness of contact-rich robotic manipulation tasks. Existing VLA systems (e.g., RDT, Pi0, OpenVLA-oft) typically output position, but lack force-aware adaptation, leading to unsafe or failed interactions in physical tasks involving contact, compliance, or uncertainty. In the proposed CompliantVLA-adaptor, a VLM interprets task context from images and natural language to adapt the stiffness and damping parameters of a VIC controller. These parameters are further regulated using real-time force/torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms the VLA baselines on a suite of complex contact-rich tasks, both in simulation and on real hardware, with improved success rates and reduced force violations. The overall success rate across all tasks increases from 9.86\% to 17.29\%, presenting a promising path towards safe contact-rich manipulation using VLAs. We release our code, prompts, and force-torque-impedance-scenario context datasets at https://sites.google.com/view/compliantvla.

</details>


### [50] [A Mobile Magnetic Manipulation Platform for Gastrointestinal Navigation with Deep Reinforcement Learning Control](https://arxiv.org/abs/2601.15545)
*Zhifan Yan,Chang Liu,Yiyang Jiang,Wenxuan Zheng,Xinhao Chen,Axel Krieger*

Main category: cs.RO

TL;DR: The paper introduces a compact, low-cost mobile magnetic manipulation platform using Deep Reinforcement Learning for precise control of magnetic robots targeting drug delivery in the gastrointestinal tract.


<details>
  <summary>Details</summary>
Motivation: Current magnetic systems for targeted drug delivery in the GI tract face challenges in control due to either limited workspace (for stationary systems) or time-consuming model calibration (for mobile systems).

Method: A UR5 collaborative robot featuring a compact four-electromagnet array is paired with a Soft Actor-Critic-based control strategy trained through a sim-to-real pipeline.

Result: The DRL-based controller achieved precise trajectory control of a 7-mm magnetic capsule with RMSE of 1.18 mm (square path) and 1.50 mm (circular path), within a clinically relevant workspace of 30 cm * 20 cm.

Conclusion: The system offers a rapidly deployable, model-free framework for accurate magnetic manipulation in large GI tract workspaces, validated using a 2D GI phantom.

Abstract: Targeted drug delivery in the gastrointestinal (GI) tract using magnetic robots offers a promising alternative to systemic treatments. However, controlling these robots is a major challenge. Stationary magnetic systems have a limited workspace, while mobile systems (e.g., coils on a robotic arm) suffer from a "model-calibration bottleneck", requiring complex, pre-calibrated physical models that are time-consuming to create and computationally expensive. This paper presents a compact, low-cost mobile magnetic manipulation platform that overcomes this limitation using Deep Reinforcement Learning (DRL). Our system features a compact four-electromagnet array mounted on a UR5 collaborative robot. A Soft Actor-Critic (SAC)-based control strategy is trained through a sim-to-real pipeline, enabling effective policy deployment within 15 minutes and significantly reducing setup time. We validated the platform by controlling a 7-mm magnetic capsule along 2D trajectories. Our DRL-based controller achieved a root-mean-square error (RMSE) of 1.18~mm for a square path and 1.50~mm for a circular path. We also demonstrated successful tracking over a clinically relevant, 30 cm * 20 cm workspace. This work demonstrates a rapidly deployable, model-free control framework capable of precise magnetic manipulation in a large workspace,validated using a 2D GI phantom.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [51] [ToolCaching: Towards Efficient Caching for LLM Tool-calling](https://arxiv.org/abs/2601.15335)
*Yi Zhai,Dian Shen,Junzhou Luo,Bin Yang*

Main category: cs.SE

TL;DR: The paper introduces "ToolCaching," a caching framework designed for large language model (LLM) tool-calling systems to address inefficiencies like redundant tool requests, improving cache hit ratios and latency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the efficiency of LLM tool-calling, which often suffers from repeated or redundant requests. Traditional techniques face challenges due to heterogeneous request semantics, dynamic workloads, and freshness requirements.

Method: The authors propose ToolCaching, a caching mechanism that combines semantic and system-level features. The introduced VAAC algorithm employs bandit-based admission and value-driven, multi-factor eviction to optimize caching.

Result: ToolCaching demonstrates significantly improved performance, achieving up to 11% higher cache hit ratios and 34% lower latency in tool-calling workloads compared to standard caching policies.

Conclusion: The proposed approach successfully improves LLM tool-calling efficiency, making it better suited for real-world applications by optimizing cache management in dynamic and complex workloads.

Abstract: Recent advances in Large Language Models (LLMs) have revolutionized web applications, enabling intelligent search, recommendation, and assistant services with natural language interfaces. Tool-calling extends LLMs with the ability to interact with external APIs, greatly enhancing their practical utility. While prior research has improved tool-calling performance by adopting traditional computer systems techniques, such as parallel and asynchronous execution, the challenge of redundant or repeated tool-calling requests remains largely unaddressed. Caching is a classic solution to this problem, but applying it to LLM tool-calling introduces new difficulties due to heterogeneous request semantics, dynamic workloads, and varying freshness requirements, which render conventional cache policies ineffective. To address these issues, we propose ToolCaching, an efficient feature-driven and adaptive caching framework for LLM tool-calling systems. ToolCaching systematically integrates semantic and system-level features to evaluate request cacheability and estimate caching value. At its core, the VAAC algorithm integrates bandit-based admission with value-driven, multi-factor eviction, jointly accounting for request frequency, recency, and caching value. Extensive experiments on synthetic and public tool-calling workloads demonstrate that ToolCaching with VAAC achieves up to 11% higher cache hit ratios and 34% lower latency compared to standard policies, effectively accelerating LLM tool-calling in practical applications.

</details>


### [52] [Lost in Transcription: How Speech-to-Text Errors Derail Code Understanding](https://arxiv.org/abs/2601.15339)
*Jayant Havare,Ashish Mittal,Srikanth Tamilselvam,Ganesh Ramakrishnan*

Main category: cs.SE

TL;DR: The paper introduces a multilingual, voice-based framework for code understanding to increase accessibility, particularly in non-English speaking regions, and demonstrates its effectiveness using LLM-guided refinement.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current code understanding tools that are primarily designed for English-speaking users using keyboard interaction, and to create a more inclusive solution for voice-driven, multilingual contexts, especially in regions like India.

Method: The framework leverages multilingual Automatic Speech Recognition (ASR), large language models (LLMs) for refining ASR outputs, and integrates code-specific benchmarks (e.g., CodeSearchNet, CodeQA) to evaluate code-related tasks like question answering and retrieval.

Result: Significant improvement in transcription accuracy and downstream code understanding tasks when employing LLM-guided ASR refinement. Challenges with domain-specific vocabulary and code-adaptive speech transcription were also addressed.

Conclusion: The study highlights the potential of building robust, multilingual, and voice-driven programming tools by incorporating code-sensitive adaptations and LLM-guided refinements, improving accessibility for diverse linguistic and interaction contexts.

Abstract: Code understanding is a foundational capability in software engineering tools and developer workflows. However, most existing systems are designed for English-speaking users interacting via keyboards, which limits accessibility in multilingual and voice-first settings, particularly in regions like India. Voice-based interfaces offer a more inclusive modality, but spoken queries involving code present unique challenges due to the presence of non-standard English usage, domain-specific vocabulary, and custom identifiers such as variable and function names, often combined with code-mixed expressions. In this work, we develop a multilingual speech-driven framework for code understanding that accepts spoken queries in a user native language, transcribes them using Automatic Speech Recognition (ASR), applies code-aware ASR output refinement using Large Language Models (LLMs), and interfaces with code models to perform tasks such as code question answering and code retrieval through benchmarks such as CodeSearchNet, CoRNStack, and CodeQA. Focusing on four widely spoken Indic languages and English, we systematically characterize how transcription errors impact downstream task performance. We also identified key failure modes in ASR for code and demonstrated that LLM-guided refinement significantly improves performance across both transcription and code understanding stages. Our findings underscore the need for code-sensitive adaptations in speech interfaces and offer a practical solution for building robust, multilingual voice-driven programming tools.

</details>


### [53] [A Prompt-Based Framework for Loop Vulnerability Detection Using Local LLMs](https://arxiv.org/abs/2601.15352)
*Adeyemi Adeseye,Aisvarya Adeseye*

Main category: cs.SE

TL;DR: The study proposes a prompt-based framework utilizing local Large Language Models (LLMs) for detecting loop vulnerabilities in Python 3.7+ code, focusing on control errors, security risks, and resource inefficiencies. Results show Phi LLM outperforming LLaMA.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional static analyzers in detecting semantic loop vulnerabilities and to leverage local LLMs for enhanced security, privacy, and offline analysis.

Method: A prompt-based framework was designed for local LLMs, guiding them through iterative prompting to detect loop-related issues. It was tested with LLaMA and Phi models, using various safeguards like code awareness and hallucination prevention.

Result: Phi outperformed LLaMA in detecting vulnerabilities, achieving higher precision, recall, and F1-scores when validated against a manually created baseline.

Conclusion: Effective prompt design is essential for optimizing local LLMs in secure and accurate detection of code vulnerabilities.

Abstract: Loop vulnerabilities are one major risky construct in software development. They can easily lead to infinite loops or executions, exhaust resources, or introduce logical errors that degrade performance and compromise security. The problem are often undetected by traditional static analyzers because such tools rely on syntactic patterns, which makes them struggle to detect semantic flaws. Consequently, Large Language Models (LLMs) offer new potential for vulnerability detection because of their ability to understand code contextually. Moreover, local LLMs unlike commercial ones like ChatGPT or Gemini addresses issues such as privacy, latency, and dependency concerns by facilitating efficient offline analysis. Consequently, this study proposes a prompt-based framework that utilize local LLMs for the detection of loop vulnerabilities within Python 3.7+ code. The framework targets three categories of loop-related issues, such as control and logic errors, security risks inside loops, and resource management inefficiencies. A generalized and structured prompt-based framework was designed and tested with two locally deployed LLMs (LLaMA 3.2; 3B and Phi 3.5; 4B) by guiding their behavior via iterative prompting. The designed prompt-based framework included key safeguarding features such as language-specific awareness, code-aware grounding, version sensitivity, and hallucination prevention. The LLM results were validated against a manually established baseline truth, and the results indicate that Phi outperforms LLaMA in precision, recall, and F1-score. The findings emphasize the importance of designing effective prompts for local LLMs to perform secure and accurate code vulnerability analysis.

</details>


### [54] [Testing Deep Learning Libraries via Neurosymbolic Constraint Learning](https://arxiv.org/abs/2601.15493)
*M M Abid Naziri,Shinhae Kim,Feiran Qin,Marcelo d'Amorim,Saikat Dutta*

Main category: cs.SE

TL;DR: The study introduces Centaur, a neurosymbolic technique for testing deep learning (DL) library APIs using learned input constraints, showing improved bug detection and code coverage compared to prior methods.


<details>
  <summary>Details</summary>
Motivation: Testing DL libraries is challenging due to their complexity and lack of proper API input specifications. Existing methods fail to model these constraints effectively, leading to missed opportunities for bug detection or false positives.

Method: Centaur uses a neurosymbolic approach that combines dynamically learned API input constraints, Large Language Model (LLM)-assisted rule generation, and SMT solvers for generating valid and diverse test inputs. It employs a novel grammar for representing constraints and prunes spurious rules through a custom refinement strategy.

Result: Centaur achieved high precision (94%) and recall (94%) for learned constraints, and demonstrated superior code coverage over existing state-of-the-art tools like TitanFuzz, ACETest, and Pathfinder. It also discovered 26 new bugs in PyTorch and TensorFlow, with 18 confirmed.

Conclusion: Centaur advances the testing of DL library APIs by learning and leveraging precise input constraints, resulting in more effective and diverse input generation. This approach is both practical and impactful for improving the reliability of DL libraries.

Abstract: Deep Learning (DL) libraries (e.g., PyTorch) are popular in AI development. These libraries are complex and contain bugs. Researchers have proposed various bug-finding techniques for such libraries. Yet, there is much room for improvement. A key challenge in testing DL libraries is the lack of API specifications. Prior testing approaches often inaccurately model the input specifications of DL APIs, resulting in missed valid inputs that could reveal bugs or false alarms due to invalid inputs.
  To address this challenge, we develop Centaur -- the first neurosymbolic technique to test DL library APIs using dynamically learned input constraints. Centaur leverages the key idea that formal API constraints can be learned from a small number of automatically generated seed inputs, and that the learned constraints can be solved using SMT solvers to generate valid and diverse test inputs.
  We develop a novel grammar that represents first-order logic formulae over API parameters and expresses tensor-related properties (e.g., shape, data types) as well as relational properties between parameters. We use the grammar to guide a Large Language Model (LLM) to enumerate syntactically correct candidate rules, validated using seed inputs. Further, we develop a custom refinement strategy to prune the set of learned rules to eliminate spurious or redundant rules. We use the learned constraints to systematically generate valid and diverse inputs by integrating SMT-based solving with randomized sampling.
  We evaluate Centaur for testing PyTorch and TensorFlow. Our results show that Centaur's constraints have a recall of 94.0% and a precision of 94.0% on average. In terms of coverage, Centaur covers 203, 150, and 9,608 more branches than TitanFuzz, ACETest and Pathfinder, respectively. Using Centaur, we also detect 26 new bugs in PyTorch and TensorFlow, 18 of which are confirmed.

</details>


### [55] [FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation](https://arxiv.org/abs/2601.15687)
*Khusrav Badalov,Young Yoon*

Main category: cs.SE

TL;DR: The paper introduces FARM, a system for generating complete, executable applets for TAP platforms like IFTTT by solving ingredient-to-field binding issues.


<details>
  <summary>Details</summary>
Motivation: Prior efforts in TAP automation left users needing manual adjustments due to incomplete or non-executable applets generated from natural language input.

Method: FARM employs a two-stage architecture: Stage 1 uses schema-enriched dual encoders to retrieve candidates, and Stage 2 employs a multi-agent pipeline with LLM-based intent analysis, selection, and verification.

Result: FARM achieves 81% joint accuracy on function-level applet generation, significantly outperforming baseline methods in generating executable configurations.

Conclusion: FARM is a promising solution for fully automated and executable applet generation in TAP platforms, improving efficiency and accuracy in Web of Things automation.

Abstract: Trigger-Action Programming (TAP) platforms such as IFTTT and Zapier enable Web of Things (WoT) automation by composing event-driven rules across heterogeneous services. A TAP applet links a trigger to an action and must bind trigger outputs (ingredients) to action inputs (fields) to be executable. Prior work largely treats TAP as service-level prediction from natural language, which often yields non-executable applets that still require manual configuration. We study the function-level configuration problem: generating complete applets with correct ingredient-to-field bindings. We propose FARM (Field-Aware Resolution Model), a two-stage architecture for automated applet generation with full configuration. Stage 1 trains contrastive dual encoders with selective layer freezing over schema-enriched representations, retrieving candidates from 1,724 trigger functions and 1,287 action functions (2.2M possible trigger-action pairs). Stage 2 performs selection and configuration using an LLM-based multi-agent pipeline. It includes intent analysis, trigger selection, action selection via cross-schema scoring, and configuration verification. Agents coordinate through shared state and agreement-based selection. FARM achieves 81% joint accuracy on Gold (62% Noisy, 70% One-shot) at the function level, where both trigger and action functions must match the ground truth. For comparison with service-level baselines, we map functions to their parent services and evaluate at the service level. FARM reaches 81% joint accuracy and improves over TARGE by 23 percentage points. FARM also generates ingredient-to-field bindings, producing executable automation configurations.

</details>


### [56] [Evaluating and Achieving Controllable Code Completion in Code LLM](https://arxiv.org/abs/2601.15879)
*Jiajun Zhang,Zeyu Cui,Lei Zhang,Jian Yang,Jiaxi Yang,Qiang Liu,Zilei Wang,Binyuan Hui,Liang Wang,Junyang Lin*

Main category: cs.SE

TL;DR: The paper introduces C3-Bench, a benchmark evaluating LLMs' instruction-guided code completion abilities, highlighting performance gaps between open-source and proprietary models. It proposes supervised fine-tuning with synthesized data to enhance instruction-following capabilities, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Recent improvements in LLMs for code completion lack corresponding advancements in evaluation methods, particularly in assessing instruction-following capabilities during code completion.

Method: The study develops C3-Bench, a benchmark analyzing instruction-guided code completion for 40+ LLMs. In addition, it creates high-quality instruction-completion pairs using Qwen2.5-Coder and applies Supervised Fine-Tuning (SFT) to enhance performance.

Result: The paper exposes significant performance disparities in instruction-following abilities between open-source and advanced proprietary models. Qwen2.5-Coder-C3 fine-tuned with synthesized data achieves state-of-the-art results in code completion tasks.

Conclusion: The research establishes benchmarks and methods to improve LLMsâ€™ instruction-following abilities. Open-sourcing resources encourages replication and future advancements in code completion capabilities.

Abstract: Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.

</details>


### [57] [The Role of Cognitive Abilities in Requirements Inspection: Comparing UML and Textual Representations](https://arxiv.org/abs/2601.16009)
*Giovanna Broccia,Sira Vegas,Alessio Ferrari*

Main category: cs.SE

TL;DR: This paper evaluates the impact of using UML sequence diagrams with text-based requirements in inspection tasks, showing that cognitive abilities like working memory and mental rotation significantly affect performance.


<details>
  <summary>Details</summary>
Motivation: The study was motivated by the need to understand whether using UML diagrams alongside textual requirements improves inspection accuracy and how cognitive abilities influence this process.

Method: A crossover experiment with 38 participants was conducted comparing two treatments: text-based requirements alone vs. text with UML diagrams. Linear mixed-effects and generalized linear models analyzed the data.

Result: The study found a significant interaction between representation type, working memory, and mental rotation ability, showing that individuals with high cognitive abilities had varied performance depending on the task type.

Conclusion: UML diagrams' effectiveness for requirements inspection is not uniform; high cognitive abilities enhance reasoning accuracy but may reduce violation detection accuracy when using UML.

Abstract: The representation of requirements plays a critical role in the accuracy of requirements inspection. While visual representations, such as UML diagrams, are widely used alongside text-based requirements, their effectiveness in supporting inspection is still debated. Cognitive abilities, such as working memory and mental rotation skills, may also influence inspection accuracy. This study aims to evaluate whether the use of UML sequence diagrams alongside text-based requirements improves the accuracy of requirements inspection compared to text-based requirements alone and to explore whether cognitive abilities are associated with differences in performance across the two treatments (text vs text with UML support). We conducted a crossover experiment with 38 participants to assess the accuracy of requirements inspection under the two treatments in terms of issues found and justifications provided. Linear mixed-effects and generalized linear models were used to analyse the effects of treatment, period, sequence, and cognitive abilities. The results indicate a significant three-way interaction between representation type, working memory capacity, and mental rotation ability. This finding suggests that the effectiveness of UML support is not uniform across individuals: participants with high scores in both cognitive abilities experienced reduced performance when using UML for violation detection. Conversely, the same cognitive profile was associated with improved justification accuracy under UML-aided inspection, indicating that higher cognitive abilities may support deeper reasoning processes when dealing with multi-modal information, i.e., diagrams and text.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [58] [Mind the Gap: Why Neural Memory Fails Under Semantic Density](https://arxiv.org/abs/2601.15313)
*Matt Beton,Simran Chana*

Main category: q-bio.NC

TL;DR: The paper addresses AI's challenge in storing episodic facts without affecting general semantic knowledge through a novel bicameral memory architecture, inspired by brain mechanisms.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current AI memory systems that fail to separate episodic storage from semantic understanding, leading to issues like memory collapse and inaccurate retrieval over time.

Method: The authors analyze the stability of current AI memory systems, formalize the Orthogonality Constraint, and propose Knowledge Objects (KOs) as discrete memory units to achieve a complementary learning system.

Result: They identify the 'Stability Gap' and provide evidence of memory collapse in neural mechanisms depending on semantic density and number of facts. Schema drift and ambiguity highlight inefficiencies in current methods.

Conclusion: A reliable AI memory system requires a design based on complementary learning architectures, integrating Knowledge Objects alongside neural weights for enhanced performance.

Abstract: The brain solves a problem that current AI architectures struggle to manage: storing specific episodic facts without corrupting general semantic knowledge. Neuroscience explains this through Complementary Learning Systems theory - a fast hippocampal system for episodic storage using pattern-separated representations, and a slow neocortical system for extracting statistical regularities. Current AI systems lack this separation, attempting both functions through neural weights alone. We identify the 'Stability Gap' in online neural memory: fast-weight mechanisms that write facts into shared continuous parameters collapse to near-random accuracy within tens of semantically related facts. Through semantic density (rho), we show collapse occurs with as few as N=5 facts at high density (rho > 0.6) or N ~ 20-75 at moderate density - a phenomenon we formalise as the Orthogonality Constraint. This failure persists even with perfect attention and unlimited context, arising from write-time interference when storage and retrieval share the same substrate. We also identify schema drift and version ambiguity as primary failure modes in production systems, observing 40-70% schema consistency and 0-100% clean correction rates. Context-based memory incurs 30-300% cost premium over selective retrieval. We propose Knowledge Objects (KOs): discrete, typed memory units with controlled vocabularies and explicit version chains. Paired with neural weights, KOs enable a true complementary learning architecture, suggesting reliable AI memory may require this bicameral design.

</details>


### [59] [Beyond the Einstein-Bohr Debate: Cognitive Complementarity and the Emergence of Quantum Intuition](https://arxiv.org/abs/2601.15314)
*Lalit Kumar Shukla*

Main category: q-bio.NC

TL;DR: The paper discusses quantum complementarity as an epistemic principle and introduces the concept of cognitive complementarity, linking quantum measurement theory with cognition.


<details>
  <summary>Details</summary>
Motivation: To challenge traditional ontological interpretations of quantum complementarity and explore its implications for cognition and reasoning under uncertainty.

Method: The authors reinterpret the Einstein-Bohr debate and propose a framework of cognitive complementarity, integrating quantum principles with cognitive science.

Result: The study establishes quantum intuition as a testable cognitive capacity, suggesting parallels between quantum principles and decision-making under uncertainty.

Conclusion: The research bridges quantum theory and cognitive science, reframing historical debates and offering new empirical directions for understanding reasoning in uncertain conditions.

Abstract: Recent high-precision experimental confirmations of quantum complementarity have revitalized foundational debates about measurement, description, and realism. This article argues that complementarity is most productively interpreted as an epistemic principle--constraining what can be simultaneously accessed and represented--rather than as an ontological claim about quantum reality. Reexamining the Einstein-Bohr debate through this lens reveals a persistent tension between descriptive completeness and contextual meaning, a tension experiments clarify but do not dissolve. Building on this analysis, we introduce cognitive complementarity as a structural principle governing reasoning under non-classical uncertainty, where mutually constraining representations cannot be jointly optimized. Within this framework, we propose quantum intuition as a testable cognitive capacity: the ability to sustain representational plurality, regulate commitment timing, and resolve perspective-incompatibilities in a context-sensitive manner. Formulated as a naturalistic construct grounded in shared informational constraints, quantum intuition offers a principled bridge between quantum measurement theory and cognition. This work reframes the historical debate, extends epistemic lessons from quantum foundations into cognitive science, and outlines empirical pathways for studying decision-making in contexts of irreducible uncertainty.

</details>


### [60] [Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles](https://arxiv.org/abs/2601.15319)
*Francesco Chiappone,Davide Marocco,Nicola Milano*

Main category: q-bio.NC

TL;DR: This study evaluates the capability of Large Language Models (LLMs) to simulate neurodevelopmental traits by comparing their outputs to real psychometric responses from structured interviews.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs can accurately model neurodevelopmental characteristics (e.g., ADHD, ASD) through structured interviews, enabling potential advances in psychometric research.

Method: The study utilized two LLMs, GPT-4o and Qwen3-235B-A22B, which simulated psychological profiles based on interview data and completed questionnaires for comparison with human-derived psychometric scores.

Result: Both LLMs showed performance above random responses, with GPT-4o achieving higher accuracy and consistency. While results were reliable for most tests, limitations were noted in specific areas such as the AQ subscale on Attention to Detail.

Conclusion: LLMs can serve as effective synthetic participants in early psychometric trials but require further refinement for certain domain-specific traits.

Abstract: Adult neurodivergence, including Attention-Deficit/Hyperactivity Disorder (ADHD), high-functioning Autism Spectrum Disorder (ASD), and Cognitive Disengagement Syndrome (CDS), is marked by substantial symptom overlap that limits the discriminant sensitivity of standard psychometric instruments. While recent work suggests that Large Language Models (LLMs) can simulate human psychometric responses from qualitative data, it remains unclear whether they can accurately and stably model neurodevelopmental traits rather than broad personality characteristics. This study examines whether LLMs can generate psychometric responses that approximate those of real individuals when grounded in a structured qualitative interview, and whether such simulations are sensitive to variations in trait intensity. Twenty-six adults completed a 29-item open-ended interview and four standardized self-report measures (ASRS, BAARS-IV, AQ, RAADS-R). Two LLMs (GPT-4o and Qwen3-235B-A22B) were prompted to infer an individual psychological profile from interview content and then respond to each questionnaire in-role. Accuracy, reliability, and sensitivity were assessed using group-level comparisons, error metrics, exact-match scoring, and a randomized baseline. Both models outperformed random responses across instruments, with GPT-4o showing higher accuracy and reproducibility. Simulated responses closely matched human data for ASRS, BAARS-IV, and RAADS-R, while the AQ revealed subscale-specific limitations, particularly in Attention to Detail. Overall, the findings indicate that interview-grounded LLMs can produce coherent and above-chance simulations of neurodevelopmental traits, supporting their potential use as synthetic participants in early-stage psychometric research, while highlighting clear domain-specific constraints.

</details>


### [61] [On Brain as a Mathematical Manifold: Neural Manifolds, Sheaf Semantics, and Leibnizian Harmony](https://arxiv.org/abs/2601.15320)
*Takao InouÃ©*

Main category: q-bio.NC

TL;DR: The paper proposes using sheaf theory to model brain functions and pathologies, emphasizing global coherence and integration.


<details>
  <summary>Details</summary>
Motivation: To establish a conceptual framework to better understand brain functions and pathologies using advanced mathematical tools.

Method: Introduces sheaf theory to model local and global cognitive functions; global coherence as global sections and pathologies as obstructions using sheaf cohomology.

Result: Provides a formal framework inspired by neuroscience and philosophy to conceptualize brain function and dysfunction.

Conclusion: This is a conceptual proposal to propose new insights into brain modeling through sheaf theory, bridging mathematics, neuroscience, and philosophy.

Abstract: We present a mathematical and philosophical framework in which brain function is modeled using sheaf theory over neural state spaces. Local neural or cognitive functions are represented as sections of a sheaf, while global coherence corresponds to the existence of global sections. Brain pathologies are interpreted as obstructions to such global integration and are classified using tools from sheaf cohomology. The framework builds on the neural manifold program in contemporary neuroscience and on standard results in sheaf theory, and is further interpreted through a Leibnizian lens \cite{Churchland2012, Leibniz1714, MacLaneMoerdijk, Perich2025}. This paper is intended as a conceptual and formal proposal rather than a complete empirical theory.

</details>


### [62] [Analysis of the Ventriloquism Aftereffect Using Network Theory Techniques](https://arxiv.org/abs/2601.15321)
*Sayan Saha*

Main category: q-bio.NC

TL;DR: The study explores how ventriloquism after-effect alters unisensory auditory localization and its temporal processing in the brain, analyzing EEG networks.


<details>
  <summary>Details</summary>
Motivation: To understand how sustained exposure to the ventriloquist illusion recalibrates auditory localization and the temporal progression of this neural change.

Method: The research employs network analysis, non-stationary time series analysis, and multivariate pattern classification to examine EEG data and track changes in the auditory processing pathway.

Result: Recalibration of auditory processing occurs early in the pathway and diminishes over time after exposure to the illusion.

Conclusion: Ventriloquism after-effect is driven by early changes in the auditory processing pathway, with its effect fading over time post-exposure.

Abstract: Ventriloquism After-Effect is the phenomenon where sustained exposure to the ventriloquist illusion causes a change in unisensory auditory localization towards the location where the visual stimulus was present. We investigate the recalibration in EEG networks that causes this change and the track the timeline of changes in the auditory processing pathway. Our results obtained using network analysis, non-stationary time series analysis and multivariate pattern classification show that recalibration takes place early in the auditory processing pathway and the after-effect decays with time after exposure to the illusion.

</details>


### [63] [Learning Discrete Successor Transitions in Continuous Attractor Networks: Emergence, Limits, and Topological Constraints](https://arxiv.org/abs/2601.15336)
*Daniel Brownell*

Main category: q-bio.NC

TL;DR: This paper evaluates the ability of Continuous Attractor Networks (CANs) to acquire stable state transitions without external displacement signals and compares different topologies regarding their stability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand whether attractor-based circuits can achieve reliable recurrent dynamics for state transitions in the absence of explicit external displacement signals or if other strategies prevail.

Method: An experimental framework was designed to train CANs for successor-like transitions without external signals. Variations in temporal stability evaluation and network topologies (circular ring and folded snake manifold) were implemented to assess outcomes.

Result: For short evaluation periods, networks defaulted to associative solutions without persistent attractor dynamics. Long evaluation windows showed genuine attractor-based dynamics but required explicit stability constraints. Moreover, the continuous ring topology reliably achieved stability, whereas the folded snake topology faced limitations at discontinuities.

Conclusion: Attractor dynamics demand constrained conditions and do not emerge as a default. Topology plays a crucial role, with continuous structures like the ring supporting stability better than folded structures.

Abstract: Continuous attractor networks (CANs) are a well-established class of models for representing low-dimensional continuous variables such as head direction, spatial position, and phase. In canonical spatial domains, transitions along the attractor manifold are driven by continuous displacement signals, such as angular velocity-provided by sensorimotor systems external to the CAN itself. When such signals are not explicitly provided as dedicated displacement inputs, it remains unclear whether attractor-based circuits can reliably acquire recurrent dynamics that support stable state transitions, or whether alternative predictive strategies dominate.
  In this work, we present an experimental framework for training CANs to perform successor-like transitions between stable attractor states in the absence of externally provided displacement signals. We compare two recurrent topologies, a circular ring and a folded snake manifold, and systematically vary the temporal regime under which stability is evaluated. We find that, under short evaluation windows, networks consistently converge to impulse-driven associative solutions that achieve high apparent accuracy yet lack persistent attractor dynamics. Only when stability is explicitly enforced over extended free-run periods do genuine attractor-based transition dynamics emerge. This suggests that shortcut solutions are the default outcome of local learning in recurrent networks, while attractor dynamics represent a constrained regime rather than a generic result.
  Furthermore, we demonstrate that topology strictly limits the capacity for learned transitions. While the continuous ring topology achieves perfect stability over long horizons, the folded snake topology hits a geometric limit characterized by failure at manifold discontinuities, which neither curriculum learning nor basal ganglia-inspired gating can fully overcome.

</details>


### [64] [A Dual-Head Transformer-State-Space Architecture for Neurocircuit Mechanism Decomposition from fMRI](https://arxiv.org/abs/2601.15344)
*Cole Korponay*

Main category: q-bio.NC

TL;DR: The paper introduces a methodology called Neurocircuit Mechanism Decomposition (NMD) to improve the utility of fMRI in precision psychiatry through biomechanism analysis for enhanced treatment strategies.


<details>
  <summary>Details</summary>
Motivation: Precision psychiatry aims to discern brain biomarkers for better disease risk assessment and treatment design, but current fMRI biomarkers often fail to offer actionable insights beyond descriptive neurobiological details.

Method: The study develops NMD, which combines a graph-constrained, lag-aware transformer for pathway-specific signal estimation with a measurement-aware state-space model that models hemodynamic responses and intrinsic brain circuit dynamics.

Result: NMD provides interpretable circuit parameters that enable more precise connections between brain activity observed via fMRI and potential therapeutic interventions.

Conclusion: By providing a deep biomechanistic understanding of brain circuits, NMD enhances the capability of fMRI for guiding personalized treatment strategies in psychiatry.

Abstract: Precision psychiatry aspires to elucidate brain-based biomarkers of psychopathology to bolster disease risk assessment and treatment development. To this end, functional magnetic resonance imaging (fMRI) has helped triangulate brain circuits whose functional features are correlated with or even predictive of forms of psychopathology. Yet, fMRI biomarkers to date remain largely descriptive identifiers of where, rather than how, neurobiology is aberrant, limiting their utility for guiding treatment. We present a method for decomposing fMRI-based functional connectivity (FC) into constituent biomechanisms - output drive, input responsivity, modulator gating - with clearer alignment to differentiable therapeutic interventions. Neurocircuit mechanism decomposition (NMD) integrates (i) a graph-constrained, lag-aware transformer to estimate directed, pathway-specific routing distributions and drive signals, with (ii) a measurement-aware state-space model (SSM) that models hemodynamic convolution and recovers intrinsic latent dynamics. This dual-head architecture yields interpretable circuit parameters that may provide a more direct bridge from fMRI to treatment strategy selection. We instantiate the model in an anatomically and electrophysiologically well-defined circuit: the cortico-basal ganglia-thalamo-cortical loop.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [65] [Robust X-Learner: Breaking the Curse of Imbalance and Heavy Tails via Robust Cross-Imputation](https://arxiv.org/abs/2601.15360)
*Eichi Uehara*

Main category: stat.ML

TL;DR: The paper proposes the RX-Learner to improve the estimation of heterogeneous treatment effects (HTE) in contexts with extreme data challenges like imbalance and outliers.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of the X-Learner framework, particularly its vulnerability to 'Outlier Smearing' due to extreme observations in datasets, which are common in industrial applications like advertising technology and healthcare.

Method: The proposed RX-Learner integrates Î³-divergence objectives and stabilizes optimization using Majorization-Minimization (MM) principles, thereby enhancing robustness in handling highly imbalanced and heavy-tailed outcome data.

Result: The RX-Learner framework significantly improves estimation accuracy, evidenced by a 98.6% reduction in the PEHE metric on a semi-synthetic dataset.

Conclusion: The RX-Learner is effective in decoupling stable and volatile data populations, making it better suited for industrial applications requiring robust HTE estimation.

Abstract: Estimating Heterogeneous Treatment Effects (HTE) in industrial applications such as AdTech and healthcare presents a dual challenge: extreme class imbalance and heavy-tailed outcome distributions. While the X-Learner framework effectively addresses imbalance through cross-imputation, we demonstrate that it is fundamentally vulnerable to "Outlier Smearing" when reliant on Mean Squared Error (MSE) minimization. In this failure mode, the bias from a few extreme observations ("whales") in the minority group is propagated to the entire majority group during the imputation step, corrupting the estimated treatment effect structure. To resolve this, we propose the Robust X-Learner (RX-Learner). This framework integrates a redescending Î³-divergence objective -- structurally equivalent to the Welsch loss under Gaussian assumptions -- into the gradient boosting machinery. We further stabilize the non-convex optimization using a Proxy Hessian strategy grounded in Majorization-Minimization (MM) principles. Empirical evaluation on a semi-synthetic Criteo Uplift dataset demonstrates that the RX-Learner reduces the Precision in Estimation of Heterogeneous Effect (PEHE) metric by 98.6% compared to the standard X-Learner, effectively decoupling the stable "Core" population from the volatile "Periphery".

</details>


### [66] [Non-Stationary Functional Bilevel Optimization](https://arxiv.org/abs/2601.15363)
*Jason Bohne,Ieva Petrulionyte,Michael Arbel,Julien Mairal,PaweÅ‚ Polak*

Main category: stat.ML

TL;DR: SmoothFBO is a novel algorithm designed for functional bilevel optimization (FBO) in online, non-stationary scenarios, featuring a time-smoothed hypergradient estimator for stable updates with sublinear regret.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current FBO methods, which perform poorly in online, non-stationary environments due to their dependence on static offline settings.

Method: SmoothFBO employs a time-smoothed stochastic hypergradient estimator with a window parameter, reducing variance for stable updates and ensuring theoretical guarantees. The framework generalizes to classical parametric bilevel cases.

Result: SmoothFBO empirically outperforms existing FBO methods in tasks like non-stationary hyperparameter optimization and model-based reinforcement learning, showcasing its effectiveness in practical applications.

Conclusion: SmoothFBO provides a theoretically grounded and practical framework for addressing online, non-stationary FBO challenges, establishing a new standard for hierarchical learning in dynamic environments.

Abstract: Functional bilevel optimization (FBO) provides a powerful framework for hierarchical learning in function spaces, yet current methods are limited to static offline settings and perform suboptimally in online, non-stationary scenarios. We propose SmoothFBO, the first algorithm for non-stationary FBO with both theoretical guarantees and practical scalability. SmoothFBO introduces a time-smoothed stochastic hypergradient estimator that reduces variance through a window parameter, enabling stable outer-loop updates with sublinear regret. Importantly, the classical parametric bilevel case is a special reduction of our framework, making SmoothFBO a natural extension to online, non-stationary settings. Empirically, SmoothFBO consistently outperforms existing FBO methods in non-stationary hyperparameter optimization and model-based reinforcement learning, demonstrating its practical effectiveness. Together, these results establish SmoothFBO as a general, theoretically grounded, and practically viable foundation for bilevel optimization in online, non-stationary scenarios.

</details>


### [67] [Low-Dimensional Adaptation of Rectified Flow: A New Perspective through the Lens of Diffusion and Stochastic Localization](https://arxiv.org/abs/2601.15500)
*Saptarshi Roy,Alessandro Rinaldo,Purnamrita Sarkar*

Main category: stat.ML

TL;DR: The paper studies Rectified Flow (RF) for efficient sampling, proving its effectiveness with low intrinsic dimensionality and introducing a stochastic RF sampler for better adaptability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how RF adapts to target distributions with low-dimensional support and improves sampling efficiency.

Method: The authors analyze RF's iteration complexity, connect it to DDPM via stochastic localization, and design a new stochastic RF sampler with specific time schedules.

Result: The new samplers, including stochastic RF, demonstrate improved performance in both synthetic data and text-to-image experiments.

Conclusion: This study confirms RF's adaptability to dimensionality, enhances its sampling efficiency, and broadens its application with novel methods.

Abstract: In recent years, Rectified flow (RF) has gained considerable popularity largely due to its generation efficiency and state-of-the-art performance. In this paper, we investigate the degree to which RF automatically adapts to the intrinsic low dimensionality of the support of the target distribution to accelerate sampling. We show that, using a carefully designed choice of the time-discretization scheme and with sufficiently accurate drift estimates, the RF sampler enjoys an iteration complexity of order $O(k/\varepsilon)$ (up to log factors), where $\varepsilon$ is the precision in total variation distance and $k$ is the intrinsic dimension of
  the target distribution. In addition, we show that the denoising diffusion probabilistic model (DDPM) procedure is equivalent to a stochastic version of RF by establishing a novel connection between these processes and stochastic localization. Building on this connection, we further design a stochastic RF sampler that also adapts to the low-dimensionality of the target distribution under milder requirements on the accuracy of the drift estimates, and also with a specific time schedule. We illustrate with simulations on the synthetic data and text-to-image data experiments the improved performance of the proposed samplers implementing the newly designed time-discretization schedules.

</details>


### [68] [On damage of interpolation to adversarial robustness in regression](https://arxiv.org/abs/2601.16070)
*Jingfu Peng,Yuhong Yang*

Main category: stat.ML

TL;DR: This paper explores the generalization and adversarial vulnerabilities of interpolating estimators in nonparametric regression, showing that perfect fitting compromises robustness under future attacks.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks generalize well despite interpolating training data but are highly vulnerable to adversarial perturbations in inputs, raising concerns about their use in predictive tasks under future attacks.

Method: The paper theoretically analyzes the adversarial robustness of interpolating estimators within a framework of nonparametric regression and supports findings with numerical experiments.

Result: Interpolating estimators are shown to be suboptimal under adversarial attacks, and achieving perfect fitting harms their robustness. A unique 'curse of simple size' phenomenon is discovered in the high interpolation regime.

Conclusion: Perfect fitting in models sacrifices adversarial robustness, highlighting trade-offs between interpolation and security, with theoretical and numerical evidence presented.

Abstract: Deep neural networks (DNNs) typically involve a large number of parameters and are trained to achieve zero or near-zero training error. Despite such interpolation, they often exhibit strong generalization performance on unseen data, a phenomenon that has motivated extensive theoretical investigations. Comforting results show that interpolation indeed may not affect the minimax rate of convergence under the squared error loss. In the mean time, DNNs are well known to be highly vulnerable to adversarial perturbations in future inputs. A natural question then arises: Can interpolation also escape from suboptimal performance under a future $X$-attack? In this paper, we investigate the adversarial robustness of interpolating estimators in a framework of nonparametric regression. A finding is that interpolating estimators must be suboptimal even under a subtle future $X$-attack, and achieving perfect fitting can substantially damage their robustness. An interesting phenomenon in the high interpolation regime, which we term the curse of simple size, is also revealed and discussed. Numerical experiments support our theoretical findings.

</details>


### [69] [Synthetic Augmentation in Imbalanced Learning: When It Helps, When It Hurts, and How Much to Add](https://arxiv.org/abs/2601.16120)
*Zhengchi Ma,Anru R. Zhang*

Main category: stat.ML

TL;DR: The paper explores synthetic data augmentation in imbalanced classification, analyzing when it helps and determining optimal synthetic sample size.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiency of standard training procedures in accurately classifying rare classes in imbalanced datasets and resolve uncertainties regarding synthetic augmentation's effectiveness.

Method: It develops a statistical framework for understanding synthetic augmentation, categorizes conditions (local symmetry/asymmetry) under which synthetic data affects performance, and proposes VTSS for determining optimal synthetic sample size.

Result: Synthetic data is not always beneficial; its impact depends on generator accuracy and bias alignment. VTSS improves synthetic augmentation by tuning the sample size through validation loss.

Conclusion: Synthetic augmentation can enhance learning in imbalanced classification, but requires careful size tuning based on generator and dataset dynamics to ensure effective results.

Abstract: Imbalanced classification, where one class is observed far less frequently than the other, often causes standard training procedures to prioritize the majority class and perform poorly on rare but important cases. A classic and widely used remedy is to augment the minority class with synthetic examples, but two basic questions remain under-resolved: when does synthetic augmentation actually help, and how many synthetic samples should be generated?
  We develop a unified statistical framework for synthetic augmentation in imbalanced learning, studying models trained on imbalanced data augmented with synthetic minority samples and evaluated under the balanced population risk. Our theory shows that synthetic data is not always beneficial. In a ``local symmetry" regime, imbalance is not the dominant source of error near the balanced optimum, so adding synthetic samples cannot improve learning rates and can even degrade performance by amplifying generator mismatch. When augmentation can help (a ``local asymmetry" regime), the optimal synthetic size depends on generator accuracy and on whether the generator's residual mismatch is directionally aligned with the intrinsic majority-minority shift. This structure can make the best synthetic size deviate from naive full balancing, sometimes by a small refinement and sometimes substantially when generator bias is systematic. Practically, we recommend Validation-Tuned Synthetic Size (VTSS): select the synthetic size by minimizing balanced validation loss over a range centered near the fully balanced baseline, while allowing meaningful departures when the data indicate them. Simulations and a real sepsis prediction study support the theory and illustrate when synthetic augmentation helps, when it cannot, and how to tune its quantity effectively.

</details>


### [70] [Beyond Predictive Uncertainty: Reliable Representation Learning with Structural Constraints](https://arxiv.org/abs/2601.16174)
*Yiyao Yang*

Main category: stat.ML

TL;DR: This paper proposes a framework for learning reliable representations in machine learning by incorporating representation-level uncertainty and structural constraints.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the implicit assumption that learned representations are inherently reliable, introducing uncertainty estimation at the representation level.

Method: The researchers propose uncertainty-aware regularization in the representation space, leveraging structural constraints like sparsity and relational structures.

Result: The framework enables representations that are predictive, well-calibrated, robust, and stable, even in noisy or perturbed conditions.

Conclusion: This approach broadens the scope of uncertainty estimation in machine learning and can be applied universally across different representation learning architectures.

Abstract: Uncertainty estimation in machine learning has traditionally focused on the prediction stage, aiming to quantify confidence in model outputs while treating learned representations as deterministic and reliable by default. In this work, we challenge this implicit assumption and argue that reliability should be regarded as a first-class property of learned representations themselves. We propose a principled framework for reliable representation learning that explicitly models representation-level uncertainty and leverages structural constraints as inductive biases to regularize the space of feasible representations. Our approach introduces uncertainty-aware regularization directly in the representation space, encouraging representations that are not only predictive but also stable, well-calibrated, and robust to noise and structural perturbations. Structural constraints, such as sparsity, relational structure, or feature-group dependencies, are incorporated to define meaningful geometry and reduce spurious variability in learned representations, without assuming fully correct or noise-free structure. Importantly, the proposed framework is independent of specific model architectures and can be integrated with a wide range of representation learning methods.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [71] [Performance Evaluation of LoRa for IoT Applications in Non-Terrestrial Networks via ns-3](https://arxiv.org/abs/2509.02811)
*Alessandro Traspadini,Michele Zorzi,Marco Giordani*

Main category: cs.NI

TL;DR: This paper explores the feasibility of using LoRa for large-scale IoT connectivity through LEO satellites, focusing on its performance and potential limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable reliable IoT connectivity in remote areas via the integration of IoT and Non-Terrestrial Networks, using LoRa for its long-range, energy efficiency, and adaptability.

Method: A new ns3-LoRa-NTN simulation module was developed to enable full-stack end-to-end simulation of LoRa networks communicating with LEO satellites.

Result: The study found that LoRa can support direct communication from the ground to LEO satellites, but network optimization is necessary to address collision issues caused by overlapping Spreading Factors (SFs).

Conclusion: LoRa shows promise for satellite IoT applications, but careful optimization of the network is essential to realize its full potential in such scenarios.

Abstract: The integration of Internet of Things (IoT) and Non-Terrestrial Networks (NTNs) has emerged as a key paradigm to provide connectivity for sensors and actuators via satellite gateways in remote areas where terrestrial infrastructure is limited or unavailable. Among other Low-Power Wide-Area Network (LPWAN) technologies for IoT, Long Range (LoRa) holds great potential given its long range, energy efficiency, and flexibility. In this paper, we explore the feasibility and performance of LoRa to support large-scale IoT connectivity through Low Earth Orbit (LEO) satellite gateways. To do so, we developed a new ns3-LoRa-NTN simulation module, which integrates and extends the ns3-LoRa and ns3-NTN modules, to enable full-stack end-to-end simulation of satellite communication in LoRa networks. Our results, given in terms of average data rate and Packet Reception Ratio (PRR), confirm that LoRa can effectively support direct communication from the ground to LEO satellites, but network optimization is required to mitigate collision probability when end nodes use the same Spreading Factors (SFs) over long distances.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [72] [Statistical Reinforcement Learning in the Real World: A Survey of Challenges and Future Directions](https://arxiv.org/abs/2601.15353)
*Asim H. Gazi,Yongyi Guo,Daiqi Gao,Ziping Xu,Kelly W. Zhang,Susan A. Murphy*

Main category: stat.AP

TL;DR: The paper discusses the challenges of deploying reinforcement learning (RL) in practical settings, introduces a three-component process for practical RL application, and reviews recent advances and future research directions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap between RL research and practical deployment across diverse domains by addressing real-world challenges like limited interaction opportunities and environmental changes.

Method: The paper frames RL application into three components: online learning during deployment, offline analyses between deployments, and iterative deployment cycles. It reviews recent advances in statistical RL and methods for improving RL system efficiency and continual development.

Result: The paper provides insights into optimizing data utility, enhancing sample efficiency, improving continual learning, and achieving impactful real-world applications of RL systems.

Conclusion: The proposed framework and reviewed advances aim to directly inform the design and iterative improvement of RL systems in practical, real-world applications.

Abstract: Reinforcement learning (RL) has achieved remarkable success in real-world decision-making across diverse domains, including gaming, robotics, online advertising, public health, and natural language processing. Despite these advances, a substantial gap remains between RL research and its deployment in many practical settings. Two recurring challenges often underlie this gap. First, many settings offer limited opportunity for the agent to interact extensively with the target environment due to practical constraints. Second, many target environments often undergo substantial changes, requiring redesign and redeployment of RL systems (e.g., advancements in science and technology that change the landscape of healthcare delivery). Addressing these challenges and bridging the gap between basic research and application requires theory and methodology that directly inform the design, implementation, and continual improvement of RL systems in real-world settings.
  In this paper, we frame the application of RL in practice as a three-component process: (i) online learning and optimization during deployment, (ii) post- or between-deployment offline analyses, and (iii) repeated cycles of deployment and redeployment to continually improve the RL system. We provide a narrative review of recent advances in statistical RL that address these components, including methods for maximizing data utility for between-deployment inference, enhancing sample efficiency for online learning within-deployment, and designing sequences of deployments for continual improvement. We also outline future research directions in statistical RL that are use-inspired -- aiming for impactful application of RL in practice.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [73] [KnowTeX: Visualizing Mathematical Dependencies](https://arxiv.org/abs/2601.15294)
*Elif Uskuplu,Lawrence S. Moss,Valeria de Paiva*

Main category: cs.HC

TL;DR: KnowTeX is a tool that visualizes conceptual dependencies from LaTeX sources to create graphs, bridging the gap between informal and formal mathematical representations.


<details>
  <summary>Details</summary>
Motivation: The difficulty in moving between informal mathematical texts and formal proof libraries due to hidden and overly detailed dependencies.

Method: KnowTeX uses a 'uses' command within LaTeX to extract hierarchical relationships, generating graphs in DOT and TikZ formats.

Result: KnowTeX generates clear dependency graphs that aid in understanding mathematical texts, aligning informal and formal formats, and supporting education.

Conclusion: Dependency graphs should be a standard feature in mathematical writing to benefit both human comprehension and automated systems.

Abstract: Mathematical knowledge exists in many forms, ranging from informal textbooks and lecture notes to large formal proof libraries, yet moving between these representations remains difficult. Informal texts hide dependencies, while formal systems expose every detail in ways that are not always human-readable. Dependency graphs offer a middle ground by making visible the structure of results, definitions, and proofs. We present KnowTeX, a standalone, user-friendly tool that extends the ideas of Lean's Blueprints, enabling the visualization of conceptual dependencies directly from LaTeX sources. Using a simple "uses" command, KnowTeX extracts relationships among statements and generates previewable graphs in DOT and TikZ formats. Applied to mathematical texts, such graphs clarify core results, support education and formalization, and provide a resource for aligning informal and formal mathematical representations. We argue that dependency graphs should become a standard feature of mathematical writing, benefiting both human readers and automated systems.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [74] [Scaling Sample-Based Quantum Diagonalization on GPU-Accelerated Systems using OpenMP Offload](https://arxiv.org/abs/2601.16169)
*Robert Walkup,Juha JÃ¤ykkÃ¤,Igor Pasichnyk,Zachary Streeter,Kasia Åšwirydowicz,Mikko Tukiainen,Yasuko Eckert,Luke Bertels,Daniel Claudino,Peter Groszkowski,Travis S. Humble,Constantinos Evangelinos,Javier Robledo-Moreno,William Kirby,Antonio Mezzacapo,Antonio CÃ³rcoles,Seetharami Seelam*

Main category: cs.ET

TL;DR: The paper focuses on enhancing quantum-classical hybrid workflows by optimizing classical diagonalization tasks on GPU systems, achieving a 100x speed boost on node basis.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of hybrid quantum-HPC methods by speeding up the classical diagonalization step, which is a computational bottleneck in quantum chemistry simulations.

Method: Developing and testing GPU-based diagonalization systems using the Davidson algorithm, utilizing offload strategies, code transformations, and data movement. Measurements were performed on the Frontier supercomputer and other GPU-accelerated systems.

Result: Results show that GPU optimization offers a per-node performance improvement of around 100x, dramatically reducing classical processing time from hours to minutes.

Conclusion: GPU-enabled diagonalization significantly improves hybrid quantum-HPC workflows, enabling faster and more efficient computation for quantum chemistry applications.

Abstract: Hybrid quantum-HPC algorithms advance research by delegating complex tasks to quantum processors and using HPC systems to orchestrate workflows and complementary computations. Sample-based quantum diagonalization (SQD) is a hybrid quantum-HPC method in which information from a molecular Hamiltonian is encoded into a quantum circuit for evaluation on a quantum computer. A set of measurements on the quantum computer yields electronic configurations that are filtered on the classical computer, which also performs diagonalization on the selected subspace and identifies configurations to be carried over to the next step in an iterative process. Diagonalization is the most demanding task for the classical computer. Previous studies used the Fugaku supercomputer and a highly scalable diagonalization code designed for CPUs. In this work, we describe our efforts to enable efficient scalable and portable diagonalization on heterogeneous systems using GPUs as the main compute engines based on the previous work.
  GPUs provide massive on-device thread-level parallelism that is well aligned with the algorithms used for diagonalization. We focus on the computation of ground-state energies and wavefunctions using the Davidson algorithm with a selected set of electron configurations. We describe the offload strategy, code transformations, and data-movement, with examples of measurements on the Frontier supercomputer and five other GPU accelerated systems. Our measurements show that GPUs provide an outstanding performance boost of order 100x on a per-node basis. This dramatically expedites the diagonalization step-essential for extracting ground and excited state energies-bringing the classical processing time down from hours to minutes.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [75] [Algebraic Statistics in OSCAR](https://arxiv.org/abs/2601.15807)
*Tobias Boege,Antony Della Vecchia,Marina Garrote-LÃ³pez,Benjamin Hollering*

Main category: stat.CO

TL;DR: The paper introduces the AlgebraicStatistics section of the OSCAR computer algebra system, outlining its key features and functionalities.


<details>
  <summary>Details</summary>
Motivation: To enhance algebraic statistics tools within the OSCAR computer algebra system by adding advanced features and improving data sharing and computational capabilities.

Method: The paper discusses the extensible design of the system, serialization for sharing data, creation of databases, and development of implicitization algorithms.

Result: The AlgebraicStatistics section within the OSCAR system offers state-of-the-art tools for algebraic statistics, simplifying data handling and computation.

Conclusion: The paper presents the advantages of integrating AlgebraicStatistics into the OSCAR system, emphasizing its extensible and high-performance features for researchers in the field.

Abstract: We introduce the AlgebraicStatistics section of the OSCAR computer algebra system. We give an overview of its extensible design and highlight its features including serialization of data types for sharing results and creating databases, and state-of-the-art implicitization algorithms.

</details>
